[Music] [Applause] [Music] okay you wanna get started cool we're live all right hello everyone my name is anthony campolo i'm here from quick node and we're going to be talking about scaling aetherium and various different layer 2 scaling solutions including side chains zk proofs and optimistic roll-ups just a little background about myself i'm fairly new to uh professionally working in the web 3 space i've been very interested in it since around 2017 is when i first kind of became hip to ethereum and and what was going on but i didn't really learn to code until around like 2018 2019 and came through a traditional web dev sort of boot camp experience and learned javascript and react and stuff like that and i have been getting more into actually coding web 3 stuff over this last year and started working at quick node which is a node provider and this talk isn't really about quick node at all though may the chains we'll be talking about are available if you want to host a node 4 on quick node but um i actually got hired and then the week before i started they said hey you want to speak at eath amsterdam so my first kind of two or three weeks at the company have just been ramping up and creating this talk and when i was talking to my co-worker about kind of what the the topic should be it's like well you do like you know the nft apis like everyone likes nfts nfts are fun and mt's are very fun but i was like now i want something a little like meteor so i picked a very dense technical topic just to challenge myself and this is something that if you have been following the scaling story over the last four or five years you're probably gonna be familiar with a lot of these terms and a lot of these concepts um if not this is gonna be a really great overview of all the different things and all the work that's gone into scaling ethereum and the first thing we should talk about is like what is the problem here like why do we need to scale ethereum at all and this is because the ethereum network creates a new block every 12 to 14 seconds so there's a certain bounded limit to just how much throughput the actual chain the main chain can have and the idea of how to scale it is kind of split into two big buckets you can think of there's the layer two ski layer one scaling and the layer two scaling so layer one is how do we make ethereum faster how do we get more transactions to go through ethereum and or larger transactions because this is why i use the the block instead of the transaction some people usually say you have this many transactions per second but that doesn't really tell you all the information you need because the transaction could be a lot of different things based on the size and what's actually happening within the transaction but you have solutions that try and scale layer one so this is things like sharding or previous things like casper and you have layer two solutions which is what we'll really be talking about today is gonna be layer two which is the idea that what if we offload computation and take it off of ethereum and do it somewhere else so that question of what is the somewhere else's is the big question here and we're going to look at lots of different somewhere else's but most of them are going to be a blockchain that's going to be the somewhere else but we'll get into that as we go so first thing that's kind of interesting and worth mentioning is that a lot of these ideas were kind of circling around even before they were actually implemented so there's a really interesting interview with carl floss on the the bankless podcast where he tells the story of optimism and this is kind of a spoiler alert but optimism once they kind of crack the code and figure it out they went and told vitalik and he's like oh that sounds a lot like that shadow chain thing that i came up with back in 2014 and this isn't like as that story illustrates this is not really work that optimism was based on they weren't even aware of it but after the fact they realized oh this was already prior work leading to it and so we see here we have the main line state which is the main chain and then we have the shadow chain and so kind of what is that shadow chain is the other layer two that we're gonna talk about and so along with the shadow chain idea there's also the bitcoin lightning network and this is the same idea of how do you scale bitcoin because bitcoin has many of the same limitations in terms of throughput and scalability that ethereum has and the bitcoin lightning network is the same idea you want to offload computation off of the main chain now what's different though is that the lightning network was not a separate blockchain it was another network with all sorts of mechanisms involved which isn't super important but the idea is that it's a separate thing offloading computation and this work kind of came together and merged with plasma and so plasma you now have joseph poon who did the lightning one of the authors of the lightning paper and the vitabuderin who already had the shadow chains idea and with plasma this is the the abstract of the paper it's a proposed framework for incentivized and enforced execution of smart contracts scalable to a significant amount of state updates per second potentially billions and enables the blockchain to represent a significant amount of decentralized financial applications worldwide so this was kind of the vision of plasma but if you actually read the plasma paper it's not exactly an implementation it's more of a theory and a system and it's important to figure out what that theory and that system is and then we can kind of talk about how it became reified so again we have a child chain and then a root chain and so lots of different term terminology and mostly it's just there's always a layer one in a layer two kind of what terms they use to represent layer one or layer two tends to shift depending on who's writing the paper or implementing the thing but here we have the communication between the chains is secured by fraud proofs and this is really the key idea that we're going to build on throughout this talk is the fraud proof what happens if someone tries to propose a block that's different or fraudulent or gives themselves a million dollars when they didn't have a million dollars before this is a key problem that we need to solve here and so you have each child chain has its own mechanism for validating the blocks and the different solutions we'll be looking at today will have different mechanisms for doing that also different consensus algorithms so the different fraud proofs can be built on different consensus algorithms and depending on which consensus algorithm you use you will have the different trade-offs that go along with that now let's look at the pros and cons of this idea the first pro is that the layer two lets you have lower fees and faster computation this is the core id and why we want a layer two in the first place and why all the things we're gonna be looking at are all different layer twos it also reduces the amount of data processing that happens on layer one that's kind of a consequence or the first part is kind of a consequence of the second part and then you can create compatible layer one scaling solutions like sharding which i talked about in the beginning you have things to scale layer one and hopefully those can be compatible with the way you're scaling layer two as well what are the cons though now the cons is that as i said the paper is more of a system and a theory than actual implementation and this is why plasma was really confusing for a lot of people for a while when you talk about plasma you'd have people not be entirely clear what they're talking about because plasma is not a single thing it was a group of many different implementations that were based on this first core paper so you had plasma mvp plasma cache plasma debit there's even a separate plasma mvp called more viable plasma so very confusing and you then had these tool chains being built around it that have pretty much been deprecated now it's you could think of it like oh no plasma was a failure they scrapped all that work but that's not really what happened the people who are working on plasma are the same people who work on the later solutions that we're going to look on here like optimism and the funds could only be withdrawn after a lengthy waiting period this is another idea that's going to re-emerge throughout this talk and it's almost a philosophical point of do you want your system to be set up in a way where you can stop and have a waiting period so you think of something like the dow hack we needed time to actually coordinate and fork the chain to make it correct so sometimes having actually a period of time where something can be challenged that itself is a key part of the security mechanism so you sometimes can't really get away from the waiting period and some of the solutions we'll be looking at will have say a seven day waiting period where your funds are going to be locked up and you can't withdraw until that seven days is over and whether you're okay with that is kind of whether you believe in this idea that that's a built-in almost social mechanism to allow coordination in the worst case where you need to coordinate a new chain essentially next thing we're going to look at are side chains with side chains it's the same idea where you have layer 1 and layer 2 but you're being very explicit in that that layer 2 is a brand new block chain that we're creating and this block chain will have all the properties of block chains that we're used to in terms of needs a consensus mechanism and has a linked list that's append only and can't be tampered with all the things that we you know love about blockchains and you're gonna have the separate layer two running parallel to layer one and for all of these that we're going to talk about they're going to be specifically ethereum is going to be the layer one but really these ideas could be transferred to almost any chain you can think of if it's a chain that has the same sort of scalability problem as ethereum and then you have a two-way bridge that connects the two now it'll also have its own consensus algorithm and block parameters that go along with that the pros and cons of this is that first pro is that it's an established technology we already buy into the fact that blockchains work the way that we we think they do and so if we know a blockchain works then it you know makes sense that we would use it as a solution for the same problem and then another thing that is general property of a blockchain like ethereum it supports general computation so if you're going to offload computation from the main chain you need to make sure you're offloading into something that can actually do that so it needs to be turned complete it needs to have a programming language it needs to be compatible with whatever computation is going to happen on the layer one now the cons is that it's less decentralized and this is an interesting point because it's not really like a theoretical limit to side change there's no really inherent reason why a side chain needs to be less decentralized it's more of just like an empirical fact because ethereum itself has been around for years it has a very good decentralized network of nodes already so you have this bootstrapping problem of if you're going to have a whole separate blockchain that's going to have a whole separate stack and it's going to require a whole separate set of validators and nodes together then there's kind of a cold start problem there and that if you want to have it be as decentralized as ethereum is anyway you need to start up this whole blockchain with all these people so in practice side chain implementations tend to be less decentralized at least in the beginning and need to grow to become as decentralized as something like ethereum is you then end up with a separate consensus mechanism that is not secured by the layer one and this will introduce additional complexity because if you already know you have a sound consensus mechanism in your layer one then you may be wary of well what is this other chain doing and how do i know this other chain isn't vulnerable to like a 51 attack or all these other sorts of attacks that blockchains can potentially be vulnerable to and then you also have a quorum of validators which can commit fraud you can think of this kind of like an off-chain 51 attack so if you're relying on this second chain and you have a network that's not very decentralized then it can be easy for enough nodes in that chain to get 51 percent and then break that consensus algorithm all right now we're actually start looking at some of the implementations here so so far we had plasma and plasma led to a lot of implementations that didn't really pick up but the ones we're looking at now are actually in practice and people are using them and any of them can be accessed through an rpc provider like quick node so polygon is a side chain and it's a clone of the layer one chain that supports transferring assets to and from layer one to layer two so that should make sense all the things we've already been talking about throughout this talk you have one chain and you got another chain two chains talk to each other now the layer two is a new blockchain with its own consensus mechanism for creating blocks so polygon is not an exact copy of ethereum it's a new chain itself this leads to other ideas that are not necessarily going to be a separate blockchain with its own consensus algorithm but we're going to now see things called zk rollups which are using zero knowledge proofs and zero knowledge proofs are very mathematically kind of dense idea but the simplest way to describe it is that it's about verifying a secret without sharing it and there's an interesting thought experiment that i actually found helped me kind of understand this is you can imagine you have a approver and a validator so approver has to prove that they are not colorblind and the validator has to validate whether that is true or not even though that validator themselves is colorblind and the way this works is imagine the validator has two beans one in each hand one is red and one is blue so you can think of this kind of like the matrix got the red pill i got the blue pill and they are going to have them behind their back and then they're gonna show them they're going to either switch the two in their hands or they're going to keep them in the same hand and then they're going to show it to the prover and then the prover will say whether it was switched or not so the prover can see whether it's switched or not and they can verify that and they can do that without needing to actually tell the validator what the colors are because they know whether they switched or not and then when they show it that person can validate that and that doesn't require actually knowing the colors themselves and this is really useful because it allows the person to validate without actually sharing that secret but the thing is that it's kind of probabilistic if you think about it because what if you just guessed the first time and you said it was switched and you happened to be right so there's a 50 50 chance you actually knew that or not you're actually proving it and then you do it again then you prove it a second time and then it's you know slightly more likely you know 75 and then as you do it over and over and over again if you continue to prove it then the probability that you're actually proving it goes up and up and up and up and it's an interesting question to ask yourself what percentage would you be comfortable with like is 99 enough what about 99.9 99.99 but eventually it gets the point where you can say okay yes you have actually proved this and that is what a zero knowledge proof is now if we look at how this then factors into all this other stuff we've been talking about you have your layer two scaling solution and then you have your layer one computation ecp performed on layer two and then for every roll-up block this is on the layer two you have a state transition zero knowledge proof which is generated and verified by the layer one so they will create this proof which is going to prove that they have actually done all the transactions correctly and then this allows having a lot of transactions all kind of rolled up together and then that combination of transactions can then be put back on to layer one and you can kind of think of it like it's being compressed we have a lot of information a lot of computation that's happening off chain but then they can kind of roll it all up and then put it back on the layer one chain now with zk roll ups we don't actually have real implementations running on mainnet right now um zk sync is pretty close they're on test net and they have a lot of funding and they are kind of the closest release that i'm aware of that are about to be on mainnet but there's no implementation right now for zk roll ups and that's why there is going to be sidechain implementations and there's going to be optimistic role of implementations that we're going to look at but the zk roll ups are still kind of in process but i thought they are worth mentioning because they're very important to the development of the next thing that we're going to look at which are the optimistic roll-ups but first let's look at the pros and cons of zk roll ups you have reduced fees per user transaction and this is true of many of the solutions that we're going to be looking at here and then you have less data contained in each transaction because they're being all rolled up together and then put on to layer one and then it doesn't require uh fraud game verification this is what makes the zk roll ups different from optimistic roll ups which we're going to look at after this which require fraud proof verifier the cons is that computing zero knowledge proofs requires data optimization for maximum throughput so basically it requires a very specialized algorithms that are written by people with very specialized phds and this kind of stuff and this in one sense is good it's it's a pro because you know you know that's being built on like really solid long-term research that's been done for a very long time but there's also like maybe a hundred people in the world who like really really fully wrap their mind around this stuff so if you find this interesting this is a good thing to kind of get into because they need they need more people and help but the the security scheme it assumes a level of unver verifiable trust because you're kind of trusting that the algorithms are sound and that the fraud proof is correct and you really need to audit that and know that it works because you aren't really having a kind of fraud way to kind of call fraud on it you're just saying like well you can't defraud this in the first place because it's fundamentally built in a way that can't be defrauded with optimistic rollups though we're going to have fraud proofs and we'll get into that as we go now with zk rollups you start by proving to ethereum that the transactions are valid whereas with optimistic rollups you assume the transactions are valid and then leave room for others to prove that fraud and this is why it's optimistic because it's kind of like if you've ever heard the term optimistic ui optimistic uis when you have fire requests off to the server and then you give a response back to the client you assume that it worked and it's the same thing awesome as optimistic roll-ups you're going to assume the transactions are valid and correct and you're going to leave the window open for anyone to say this is not valid i want to call fraud on this and so there's a mechanism built in to do that and there's going to be two separate different mechanisms though that we're going to look at to do that the pros and cons is that it's compatible with evm this is really important for the two implementations we're going to look at which are arbitrary and optimism they both say that we want to be compatible with the evm and so that means that you could have computation that would run on the evm and work on optimism or arbitrarum and you don't need to necessarily change the code hopefully the stack should be basically identical and that means that it's more flexible than zk roll ups because it's basically just like running almost a clone of ethereum in that sense and then you have all the data is available and secured on chain now the cons is that um it has more limited throughput compared to zk roll ups because zk roll-ups you can really take a ton of transactions and smoosh them all together and it requires an honest majority of ethereum validators and at least one aggregator that does not censor the transactions because you actually have to have someone to call fraud and activate that fraud proof and it only requires one person though so as long as you feel that there's one trusted node within this quorum then you can feel confident that they're going to call fraud on it the first implementation we'll look at is arbitram and this little thing here so this is from the white paper and the first time i look at this i was like what the heck is going on here but it actually makes a lot of sense if you just kind of look at the the middle pieces you have the challenge and then when you have the challenge it goes to bisected bisected goes up to waiting and then we'll check whether it is confirmed or not and then it goes to pending and you either re-challenge and then you do the loop again or you say okay this is good and then you exit and this will then kind of slowly chunk it down little by little so if you think of like a binary search it's a little bit like that you can think of alice and bob engaging in a back and forth and then this is refereed by a layer one contract to resolve that dispute and that's being resolved through the bisection and this is based on the section of the dispute which is what the the image is showing now the next one is optimism and with optimism we don't have the same bisection idea with optimism instead we have a challenge window and so with this you have a period of time which you need to wait and say okay throughout this period of time we're going to allow anyone to verify whether this is actually valid or not and if not then they can challenge it and then the computation is rerun and verified whether it actually worked or not and if it goes unchallenged for the duration of the challenge window that is considered final so in optimism you have a seven day window and then once the commitment is considered final the layer one smart contract can accept it and then you're all good to go okay and then these are the citations of all of the stuff that i talked about and these are links for quick note if you're curious so the three different implementations we looked at are all available if you want to spin up a note on quicknote and connect to it check out our twitter we have some events and we have an event in two days at 7 30. so feel free to come hang out at our house hacker house and then we're also hiring so i am still fairly new to the team but we're hiring in lots and lots of different areas and parts of the company and we also have a discord so feel free to check out our discord yeah that is the whole talk thank you [Applause] and does anyone have any questions we've got a couple minutes here if not that is totally fine appreciate you all being here i hope that all made sense this is a very interesting dense technical topic and that was kind of a whirlwind overview but this is work that is very consequential and important to ethereum i think most people around like 2017 when crypto kitties happened everyone was like oh wow this is like really slowing down the network and then for a while people were like uh it's not really that big of a deal because she had the whole crash and then now over this last two years now it's really a problem i mean i can say personally for myself i have an ens domain i don't know if you have a dot eth kind of domain and when i bought it it was 15 for the domain and 200 for the so this is definitely a serious problem and it's great that there's lots of projects out there trying to address it and hopefully this gave you a bit of an idea of what is out there and available you can check out yeah yeah but obviously there's these different methods that people are proposing do you have any predictions about which ones are going to be more successful or more adopted or like if there's going to be tests or kind of uh potential issues with any particular method that you talked about yeah sure so the the three that were kind of like the implementations for polygon arbitrum and optimism they're all running on mainnet right now and you can like put money into those things and they are running and you can treat them like a real blockchain that you can invest in so with those the proof is in the pudding in the sense that they are already operating and you want to be able to separate between what is actually a scaling solution that you can look at and that you can interact with today versus ones that are still kind of in the works so that's where like the zk roll up stuff is still kind of in the works and it's running on test net and it's still unclear what form that's really going to take so you can look at something like zk sync and you can say okay this is on test net it's probably going to be on main net in a similar form but you can't really say whether it actually works or not versus things like arbitrary optimism and polygon those are running and you can say okay this is something i can point to and say is working so i think that's the main thing is that i feel fairly confident that we're going to continue to see arbitrary and optimism and polygon continue to run and operate and they seem to be stable and working and then the zk roll up stuff is still a bit more experimental and theoretical and that we're not entirely sure how that's going to pan out yeah yeah all right cool well thank you all so much everybody [Applause] [Music] [Applause] [Music] you 