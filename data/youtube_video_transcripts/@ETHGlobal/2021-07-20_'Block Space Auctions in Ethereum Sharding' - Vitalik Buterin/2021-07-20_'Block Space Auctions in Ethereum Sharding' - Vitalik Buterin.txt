hello yes okay great um so to so i'm going to talk about this proposal that i think phil mentioned earlier on today which is basically block space uh auctions and um an integrated uh market that kind of separates the function of uh block builders that come to get that uh actually package and come up with proposals from blocks from the consensus function of blog proposing and our proposal to add that into the ethereum sharding design uh so a quick uh summary of the ethereum sharding design uh so there's a beacon chain uh there's a bunch of shark chains uh currently 64 but that amount is likely to increase over time and the shard chains just have data as so they're just a data availability space and the reason why these jar chains exist is basically to provide space for roll-ups to publish their data and have it be guaranteed by yet consensus through data availability sampling that this data actually is available right so projects that are currently roll-ups with the data on mainnet could instead be rollups with their data on chart chains and this would uh increase their scalability by something like a factor of 100. now the ch the question is like what actually is the fee market for uh putting data into these shards going to look like right um so basically the uh a challenge here is um that like we're looking at these data blobs and it will mostly be a roll-up project providing these large data blobs but conceivably there could be multiple roll-ups that provide multiple blobs that get included into the same shard block and uh the job is to just make it easy for block proposers to choose like which one of uh like what data to include and get paid for it um so this is the proposal for a market for choosing block proposals so there's a few steps but i'll go through it step by step so we have a class of actor called block builders and the block builders propose block bodies and think of the block body as being just a lump of data so it has no meaning within the protocol itself but rollups can refer to it right rollups will be able to make proofs that show that hey this particular piece of data actually is available because it actually was published in a shard block at some previous point in time uh so the block body you can compute a polynomial commitment and that can go into a header and so there is this concept of a block body header a header contains the commitments of the block body and it also contains a signature from the block builder that produced it together with a number that represents what fee the block builder is willing to pay so we have a bunch of block builders they create a bunch of bodies and uh from those bodies you can create the headers so the header is going to be like at most like maybe 100 bytes maybe a little more all together and this all gets stuff published into a special peer-to-peer network that we call the proposal header subnet uh so then there is the blog proposer right so vlog proposers are these actors that are actually the ones that are supposed to have kind of ultimate choice about what gets included and blog proposers are going to look at all of these headers and they are going to take the one that has the highest fee and um oh this is a mistake that should be 0.50 and instead of 0.13 they're going to take the the header that has to be the highest fee not the lowest and they're going to sign it and they're going to uh republish the signs header into the into the proposal header subnet and the block builder that actually created the winning a header is going to see this right so step one block builders create body submit headers with fees block proposer chooses the highest key that they find they sign it and step three the block builder sees this and at that point it's up to the block builder to publish the body once the block proposer has submitted the header the block proposer does not do when he needs to do anything more they have no further choice about what happens so notice that at the time that the blog proposer makes their only decision um they do not see the body right so yeah so this makes the fee market very efficient for the blog proposal they don't have to do any kind of complex math all they have to do is see a bunch of headers and choose the one with the highest fee and they have no ability to censor because they have no ability to see contents now this gets published back to the network and then the block builder is the one that publishes the block body they have to publish a block body whose commit where if you calculate the commitment that actually matches up with the header and so the block builder does not have a choice of what to publish they have to actually publish the original block body that they started with and then they published this into the main chart subnet and then you have the attesters who are basically the same attesters that are currently participating in these 2.0 consensus and instead of voting on like basically either there being a new block or they're not being a new block they end up voting on one of three choices uh one choice is that the body is available so everything is available the second is that the header is available but the body is not available and the third is that the the header is unavailable right so if the header is unavailable that basically means that well this basically means the signed header is unavailable right so it means that the proposer kind of neglected their duties the proposal was offline if the header is available but the body's unavailable then that means the block builder neglected their duty so in this case what happens is that the fee from the block builder to the proposer still goes through but the body is counted as not being included and then if the body is available then the body is counted as being included um so the goal is to basically replicate this kind of free market design sort of similar to what flashbacks does with a single bundle with this nice privacy property but do it all in protocol in a way that's kind of very efficient has that very light uh uh trust assumptions so properties right so block proposer does not know any contents during step one by the time the blog proposer signs the header which is step two after they do this after they publish they cannot prevent publication of the block body and the final actor in publication actually is the block builder but they have no choice of what to publish um i made an east research post about this some i it may have been linked somewhere but you can easily find it it's like block builder proposer separation as some just search for those words that you research and you'll find it and it talks about the rationale of kind of all of these uh design decisions but the goal is to basically allow block builders to be this specialized actor that can understand so kind of where to grab different pieces from roll-up projects and other projects to come up with a block body and and then the block the function of walk builders is fairly specialized and the function of log proposer is this very easy uh function that's very adminible to decentralization which is they just see a bunch of headers and they pick the one with the highest price um so this potentially even allows the blog proposer to be mpc for example right which could be useful for staking pools because then and it actually provides an advantage for decentralized taking polls because decentralized staking polls are the only ones that can credibly not be a kind of siphoning off mev to themselves by side channels because they could have multiple participants that are running this algorithm and they and they would have to all agree on like which uh which of these headers is the height has the highest view um so this can be combined together with flashbots so phil also talked about the possibility of combining together the sgx based approach with the economic approach what i talk about here is purely economic right there's the theater and the incentive for the block builder to publish is that if they don't publish they have to pay the fee but they don't get the benefit of their block body being included and and the incentive against censorship here is basically that if there is something like if there is some underlying lump of data that pays a fee then if the block builders try to censor it they'll just get outbid by block builders who don't censor it and if the censoring block builders suggest uh start like pushing their bids higher then they'll have to basically pay higher bids than what they can actually get in revenue and so they'll have to keep losing money for a very quickly they lose money once every block forever until eventually they have to stop censoring right so but these are all economic arguments um but what you can also have is you can have this market going from searchers to real layers right so block builders in this case like we could call them real layers this would be the central actor and then they themselves would be listening to a flashback style market where you have a lot of different searchers searchers can specialize they can focus on individual roll-ups for example and so searchers are this function where you can do something useful even if you're very small even if you're an individual hacker for example and then relayers would be responsible for just doing all this aggregating between the searchers and you can kind of slot this into flashbots and even the kind of flash flashbots with mavs gx almost as it exists today except instead of the contents of a body being unlocked when uh the miner gets the proof of work the contents of the body would be unlocked when the real layer gets back the header signed by a proposer um so there's this nice kind of opportunity to create this three-layer market that basically kimbap has a kind of very decentralized a market of searchers doing all sorts of things to a fairly small but still competitive group of these more professionalized failures and then going to uh block proposals so that's all for that that's it for me and uh yeah hope you enjoy the presentation hope you go read these research posts 