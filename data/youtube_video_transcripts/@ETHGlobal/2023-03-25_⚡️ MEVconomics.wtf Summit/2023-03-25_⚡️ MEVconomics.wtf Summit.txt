foreign [Music] [Music] foreign [Music] thank you [Music] foreign [Music] thank you [Music] foreign [Music] [Music] thank you [Music] foreign [Music] [Music] foreign foreign [Music] [Music] [Music] thank you [Music] all right everyone welcome to the economics uh we've got a ton of great research topics set up here at the intersection of Mev ethereum all things economics um first shout out to eat Global and flashbots for putting all this together especially Tina and Sarah um and thank you again Tina for informing me that I volunteered to be gross Master for this um we've got a I'm sorry sorry um so just a quick scheduling note all the presentations here we'll have some q a at the end um so please submit any questions you've got um if you're on the live stream we will be checking them I'll prioritize the spicy ones so please know softball questions um with that I will get into the first presentation there we go um so this will just be a very broad overview to kind of set the stage for all of the other talks today this is basically the State of Affairs for everything Mev right now questions so basically as everyone kind of knows uh merge happened at the end of last year completely changed the block production supply chain this is broadly what it looks like right now um this picture comes from Barnaby who's obviously working on a lot of the stuff over at the theorem Foundation um so broad transaction supply chain users submit Transit actions um Searchers will take a lot of those transactions make specific bundles they send them to builders those go through relays and eventually validators will take those blocks um that filters make for them and propose those to the network um this kind of separates that specialized task of actually building blocks um so the validators can have permissionless access to Meb and not have decentralizing pressures of needing to be sophisticated actors who need to do that more complicated task so this is what the Builder Market has looked like since the merge um as you can see it has Diversified a lot um since the start of it or right around the time of the merge it was rather centralized in the beginning where there just weren't a whole lot of parties there yet um since then it has broken up very very nicely um such that you have a lot of different participants who are all taking you know 10 to 15 20 market share and then a long tail of a much smaller ones as well um so as you can see kind of in this graph below at the bottom of the table below at the bottom here some of these Builders are able to take a nice margin on that for some of the bigger ones these are the profits that you know some of them have picked up over the last week for some of the big Builders some of them do run in a profit like as you can see with some of the bigger ones like Beaver here rsync uh bulb2069 and others just run at flat where they pass everything through which is the illuminate democratized distribute that one's flashbots that's what they do one of the other exciting things with Builders as well is we'll start to see going forward hopefully is their ability in their kind of role here to add a lot of additional features um going forward which I would recommend you to see Alex stokes's talks on those topics because that is a very long topic and it ended up itself um so the big question here is that everything looks pretty good um from like the start of the merge I think most people would agree that we're pretty happy with the way that the Builder Market is shaped out that there's a good amount of decentralization now in that there's a lot of different participants in there who are acting pretty competitively um so I would say it's a good start um but there's obviously concern that there are very natural very strong centralizing tendencies in this over time um so that's where it comes in trying to build an actually fully decentralized block building Network um that kind of plugs in here and is able to have many many participants who actually contribute to even building a single block and that is exactly what Suave is as I'm sure everyone here has heard of by now um here's just a very quick overview of it uh so moving on down to the next step of the supply chain um relays is basically kind of just the the pipes in the middle between the builders and the validators who are sending those blocks along um again you see the very similar Trend where at the beginning it was very centralized it was basically everyone was using flashbots for the most part um and since those first couple of months we have seen a lot of diversification in there which has been really nice um and the biggest one now is actually not flashbots um and it is I believe last I checked it is the ultrasound relay um especially after they launched very recently I believe a week ago they're optimistic relays so that is kind of a New Concept where you start to realize that there's a meaningful latency Advantage if you cut out that kind of Step of not requiring the Val uh not requiring the relays to actually validate the block that they are sending along um to the proposers that cuts off you know 100 200 milliseconds of latency and that does add a meaningful benefit um so it does come along with some risks because now they are no longer checking the blocks that Builders are sending them um to the full extent um but they're also collateralized by these Builders so Builders who want to be able to get that extra latency Edge and give their blocks a little higher chance of getting accepted what they'll effectively do is they will just put up some form of collateral and you know hey if my payment that I said that I was going to give the proposer doesn't go through for some reason if something was screwed up then you could take my collateral basically give it to them and I get kicked off of that relay so there's some element of trust in there but there is also this fallback option to make sure that proposers don't just completely get rugged on it and recommend checking out the med boost Community call there's been a couple of them now where a lot of this stuff gets discussed so the general trend on this I'm sure everyone has seen this drop by now um and this goes kind of hand in hand with a lot of that diversification that we saw throughout the relay and Builder Market is early on um the large majority of relay and Builder adoption that we saw was primarily coming from ones who filter out ofac transactions which obviously got a lot of people worried in the earlier parts of the Year particularly if that started to continue to Trend up you'd potentially have to wait a rather long time if you were one of those sanctioned listed addresses thankfully a lot of the related versification and build a diversification has come from parties who are not engaging in that ones like ultrasound relay who are not filtering those types of transactions out so the the big question kind of here becomes is the ethereum protocol is very kind of simple today and most of these types of actors are sitting outside of the purview of the core protocol itself so the perfect example of that is something like imp uh in protocol proposer Builder separation so as of right now we need those relays because there isn't this notion of that market structure and allocation mechanism for for PBS actually built into the protocol itself um so we do rely on those other actors and similarly with Concepts like restaking is it possible to bring those kind of commitments into the protocol such that the protocol is actually aware of those things and that becomes a very big question of what is it actually the ethereum protocol's job to guarantee um should it be bringing all of these things in if this is if we see this as a really important and potentially dangerous role that is sitting outside the protocol that has a lot of power something like Builders should we bring that into the Proto all itself something like in protocol PBS the nice thing about that is it removes the need for relays so you get rid of one of these vectors for censorship in the middle you remove another party you make that kind of Engagement between the proposal and the Builder more trustless that you know that they won't get paid um but it does also mean that you are now enshrining additional complexity into the protocol and there's a possibility that you know you find out a year or two later oh maybe there's this thing that we haven't thought of on the incentive compatibility on the economics of this structure um so there has to be a pretty high bar to like actually and try on something and it probably looks like something like enshrining PBS will take some time if it is going to happen um so this is kind of the core idea between uh behind a lot of what Barnaby has spoken about in the past year this concept of Pepsi which he has uh mentioned before which is effectively making the protocol aware uh it's it's an idea it's not a specific proposal or anything that's like an EIP or anything um but it's an exploratory idea at least of what if we just make the protocol more flexible and understanding these kind of external commitments um as opposed to trying to enshrine everything uh into the protocol itself like a very specific uh proposer Builder separation model um and this kind of relates to restaking a lot um because we're starting to have this idea where proposers can potentially opt into external commitments outside of the protocol that hey I'm saying I'm going to secure this other application but the protocol is not aware of that at its core um so there are other obviously a lot of risks that come along with restaking that people are concerned about but the reality is it's here and we just need to understand how to build with it responsibly um and one potential uh kind of answer to that is potentially just making the protocol more aware of it and being actively flexible as opposed to enshrining very very specific and complicated mechanisms into the ethereum protocol so looking at the validator market here um I'm sure a lot of people have seen this before but you see the similar trend of we kind of realized that a handful of large operators do control a very very large amount of the stake um and while we have high Economic Security we also would like to much more decentralize the actual stake behind that such that there are many operators behind it and one of the things there is obviously liquid staking providers they have a number of operators who are underneath the hood of something like Lido something like rocket pool and they obviously offer a lot of nice benefits to users they're effectively a backdoor way of smoothing rewards for users it gives them a liquid token um so there's going to be a lot of development here on things like as Lido talked about with their staking router being able to bring in things like distributed validator technology to hopefully decentralize kind of uh The Operators under the hood they're even more than they are today so one of the big trends that I we will definitely hear about later today um and probably a lot for the next year is that users have mostly been not acknowledged a whole lot in a lot of the stuff that has been built um in regards to Med over the past year or two um there aren't a lot of options to really give users back a ton of value today for the most part you're going to send something to the men pool hopefully you set your slippage right such that you know you don't get sandwiched but there's not generally this kind of competitive process where people are bidding you back the actual value of what your order is creating and that's kind of the core concept behind order flow auction such as what flashbots has proposed with Med share early on there's a number of others who are building very similar Solutions whether that be block native and others um and this is basically that competitive process where Searchers can say hey I realize that I can capture a hundred dollars if I have the right to backrun this trade so I should be able to bid in this auction I'll bid 99 for the right to do that and that kickback can then go to the user returning them some of that value it becomes difficult to do this in a trustless manner and that's where a lot of the kind of programmable privacy Tech that you hear about that is going to need to be layered on over time such that you're removing trusted components like something like the person who's running the auction uh effectively in Med share early on or not technically running the option but the Matchmaker who's kind of doing this you are trusting them and so removing those trusted parties is going to be a big part of flushing out these kind of systems another trend is going to be wallets um they're they've been a bit of the sleeping giant for a while here where they're in a very powerful position of aggregating user order flow and sending that through the supply chain like that is effectively where everything comes from so the simplest thing for them to do is you strike private deals you do payment for word of flow I hey I'll only send it to this Builder you execute these for me you kick me back some money um the threat of that obviously is it becomes very centralizing um that gets back to the centralizing forces that we talked before with Builders um so you would ideally like to see a much more permissionless in the open system where they're not doing that otherwise we've effectively just created a worse version of tradify um if you kind of play that out at the application layer this will be another Trend that you start to see over the year is most of the general purpose environments like let's say uniswap or some decks or whatever on a general purpose environment most of the value from that is generally going to go to the ethereum validators and Searchers and Builders Etc but realistically applications want to be able to capture that value from for themselves because that's one of the core values that they do create um so you see very custom uh ideas around this stuff like osmosis is implementing a module where their validators will just automatically do these arbitrages and capture that revenue for themselves to be distributed as they want it's much easier to do in an app game something like that where you can just actually customize a consensus becomes more difficult than a general purpose environment um and then lastly here we have l2s so you'll hear a lot of this for uh for jasus and through in stock later on um but Mev is obviously going to be a big problem on layer twos it doesn't the problem doesn't just go away once we kick it up there and it already even is a problem today even with a centralized sequencer um so even with out decentralizing at all there are meaningful problems um that do need to be addressed significantly better because the naive implementations of this something like a first come first serve uh you just end up with these latency games and as tarun will explain later uh the idea of fair ordering is a bit questionable um so decentralizing it will make it even harder um and you introduce a lot of new complexities and hey how do we try to maintain a private mental anymore now that we have more participants in here stuff like that um and the reality is I think it's more important to deal with the mbv effects of something like first come first serve than it is probably to decentralize the sequencers particularly in the short term because the whole point of Roll-Ups is if they're implemented correctly where you remove the upbringing Keys you have permissionless proving Etc um they're safe anyway and users should be able to exit from them and now you have this whole new idea also popping up of shared sequencers there's a number of variations there which we'll talk about later today um one of them is what Justin has proposed called base Roll-Ups which is kind of an idea based on what uh Metallica described a couple of years ago is total Anarchy which is basically letting the layer one itself sequence for all of the Roll-Ups that are on top of it comes up with a number of trade-offs kind of on ux at a lot of very different economics which are very interesting and then there are also other chains which are looking to provide similar Services ones like espresso and Metro um who will be offering this kind of shared sequencing layer that many Roll-Ups on top of them can plug into um and they will be providing that sequencing service for all of them and that gives you a lot of exciting benefits it also again raises a lot of very complex questions around the economics and the trust in these systems Etc um and those ones also do give you a bit of an easier route to one of the nicest things that people like about rollups as a user is you want those really fast pre-conformations that's really difficult to do possibly not practical at all to do with something like a base roll-up where you probably are going to fall back to the L1 block times um so these are all very interesting ideas which we're going to hear a lot more about later on today and that is all from it were there any questions no it does not look like there are any questions all right so I'll transition it over a couple minutes early here then we are going to have Elaine speaking next on Mev activity since the merge strategy Searchers and Builders during Market panics hello can you hear me yep we can hear you can go okay cool just checking if the audio is working right so in the next 15 to 20 minutes I'm going to go through uh some of the Mev activities since the merge I'll touch a bit on everything including uh the strategy the Searcher strategy the proposes the Builder competition market and specifically touching to recent stress or Market crisis events um yeah so let's get started so before I go into the data I'd like to highlight the concept of nav um because it's a theoretical value that sometimes it's difficult to assess and obtain all the data on chain to do this so therefore we introduced a concept of realized extractable value so rev is the actual value that we can observe on chain that's from the Mev opportunities so before the merge we've done something similar by collecting the Rev from The Proposal balance change or the minor balance change and after the merge we are collecting the same data so here we're using the ease send to the proposal as an approximation for the total rev but bear in mind the Rev Pi which is only a portion of the Meb be theoretical pie also contains different parts for example we only look at the atomic arbitrages the Meb opportunities we don't look at any kind of arbitrages outside one block there are also revs on other domains on Cross domains and also on centralized exchange which currently we don't have too much data on these so the numbers that we see currently in the ecosystem is mostly focused on Atomic Arbitrage or Mev opportunities on chain and some of you might be aware of one of the products that we have from flash volts which is called in math Explorer and there have been many quotations of the numbers from mavix poor but just to highlight that MAV Explorer only represents a small portion of the pie so when you look at the data from map Explorer just bear in mind that doesn't tell you the whole picture okay so let's have a look at the nav activities since the merge on layer one um back in 2021 we have produced similar numbers uh using MAV Explorer here we're showing on the right hand side a 12 months coverage of Mev activities from liquidations and arbitrages and that's in total 475 million and bear in mind that the the dollar value is based on the East price at the time uh at the end of 2021 which is probably much higher than where we are now on the left hand side is what I'm going to show you um in the following slides this is the six months since the merge and in total we've seen the total is sent to the validators around 146 000 East that's equivalent to around 260 million in dollar value today um just looking at this number again just to highlight that we are using the East pay to proposes as an approximation for the total Mev and we've seen this number have been accumulating since the merge but uh you can see two distinct bar charts on the right hand side here where the weekly numbers are actually much higher than the other weeks so these are the two events that I will talk about later on in my slides so these are the two Market crisis events happened recently one in November the FTX collapse one in March not too long ago the bank run on svb and also the shortly depacking of the stable coins um just to show you all the parties involved in the supply chain we're talking about only the ease paid to proposes which is only one component or one a party involved in the supply chain and if we make a simple assumption to say the other parties other than the proposed and the Searcher receives no Mev which is not exactly accurate at the moment for example the builders some Builders take do take profit from building the blog but let's just assume the breakdown is only between searches and proposes we see a 48 and maybe go into the Searcher and 52 going to the proposal so the numbers I showed in the previous slides only represent that 52 roughly on on average okay so since the merge what's happening uh with the relay so we see the relay competition uh has been increasing there are more relays in the ecosystem now than since the merge um at the beginning we have just flash votes um and block um and and no other relays in the in the ecosystem now we have multiple relays and you see flash volts um slot shares has has been decreasing since the beginning of this year at the bottom here you can also see the ease paid from flashbots relay versus the others you also see there used to be a gap where flash was really pays more than the others but now this Gap is closing rapidly looking at the Builder space um also at the beginning we have um at the beginning since the merge we have flashbots and also Builder 0x69 um dominating the market but then the share from flashcards Builder also uh starts to decrease since the beginning of this year and now we have shown just the top 10 builders in the space that we can identify but at the same time there are many uh other smaller Builders I haven't included which I will touch base on the later slides so latency so what do we mean um by latency usually um among all the parties involved in the supply chain we look at when the block submission happens so this slide shows the time the validator um asks for the get header from the Builder so this is around 400 milliseconds after this the slot starts so after this 400 milliseconds the validator starts to ask for the bid from the blog Builder and when was the block ready on average here we see that the average winning blocks winning blocks means the blocks that actually landed on chain on average they arrive 149 milliseconds before the slot starts I'm just going to show you the winning blocks um also the proposed of the landed blogs versus all the blogs submitted to a slot so this chart shows you actually um a very interesting uh picture that most of the blocks actually submitted in the last two seconds of the slot this is what we are talking about in terms of latency so there is a delay from each involved party to submit the block as late as possible to the end of the slot so they can include as much transaction as possible to increase the total profit so here this picture just shows you that most of the blocks that landed on chain arrives two seconds before and also some of them are too late two seconds after the slow ends but at the beginning let's say the the first 10 seconds they're not that many blocks being submitted so high latency in Block submission could cause some major issues for example the delay in the attestation and also the congestion in the last few seconds to the network I believe there are speakers talking specifically about the latency problem in the later sections so I won't touch too much details in this so now um zooming in to the two reason Market Panic events um one happened on the 9th of November when FTX and three hour Capital collapse and the other one uh very fresh in in your mind probably just a few weeks ago that we saw the svb bank run and the stable coins such as usdc and um uh and die Deepak shortly so here is a chart showing the Bitcoin price and The Ether East price during these two crises uh interestingly you see the the first crisis actually lasted much longer all the way until almost the beginning of February whereas the recent event recovers very swiftly so why do we want to look into these two events and what is particularly interesting um so here we can see the daily Mev pay to the proposals have reached all-time high since the merge in a recent event and that's like nine times more than the average daily Mev and that's due to the the usdc depict events shortly happen after the svb bank run whereas before in the FTX collapse event we see a slightly less severe in terms of the Mev sent to proposes but still six times more than the average um also looking into the Mev from a searcher's perspective we collected the data from eigenfi this is showing the Arbitrage sandwich and liquidation specifically during these two events you can also see the same pattern can be observed from this chart where the Arbitrage volume on that day skyrocketed and interestingly looking at a cross domain so the cross domain shows a slightly different story um the you you can see the FTX event is has more impact on a cross domain whereas the recent Bank Run svb Bank Run and the Deepak event is more contained within the ethereum network so interest interesting data from semiotics Cross chain Mev um specifically we are going to look at how much ease were paid on these two days two proposes and what's the Builder competition look like during these two days and also the latency in Block submission and these are on ethereum and we're also going to look at the cross-domain strategies on these two days so on the 11th of March the when the bank run happened we we see that 6 000 East has been sent to the proposals so this is equivalent to or even more than the average weekly total ease paid to proposes so at the bottom here you see the weekly average is around four thousand where on that single day on 11th of March you see 6 000 paid similarly on the 9th of November when FTX collapsed you see around 4 000 is paid um so these are extremely large single day Meb activities that I'm highlighting here um on the right hand side I um I have a breakdown by the builders in terms of the total ease paid to proposes so in back in November we see that the top Builders flash was um Beaver build and Builder 69 dominates the market taking about uh over 60 of the of the market but in a recent event the top Builders portion have actually decreased we see more Builders coming into the space and the non-top builders or the smaller Builders actually takes 60 of the total is paid to proposes so you see a kind of um decentralization happening or diversification in the Builder Market from the recent event compared to back in November now what about the search strategies so we also look at these two days so on the left hand side you can see the Arbitrage on these two days and on the right hand side you can see the liquidation so bear in mind I mentioned the map inspect pipe which Powers the map Explorer dashboard is only counting liquidation and Arbitrage so it's only a small portion of what's happening on that day just to highlight the eigenfi also provides this data and they also cover a large a wider spectrum of the MAV types they also cover sandwiches and also uh probably more Protocols are included but here I'm just sampling the data on these two days from Mev explore you can see that most of the Arbitrage happen among ethereum Bitcoin and stable coins so the the in terms of the token that's been upped they're very similar between these two events but in terms of the liquidation it's more event based so as you can see the the recent event at the bottom here we see the the curve pool actually have gained a lot more portion in in the whole pie whereas the November event is also simply arbing through across different uh tokens okay um in terms of the search strategy by protocol uh this is also looking into math explore data where um we have included nine different protocols in this data um just comparing the two days we see the same story basically back in November when FTX collapsed most of the arbitrages happened on unit swap V2 and V3 whereas the recent event we see an increase in the percentage in arbing on the curve stable coin pool and in terms of liquidation the recent event has um not much in terms of dollar value of liquidation or that's kind of obvious because the the recent event is not um highly correlated to a liquidation event whereas the FTX has a wider effect in the market where more liquidations were happening okay in terms of latency um here on the left hand side we see uh the November FTX event and then on the right hand side we see the recent event very interestingly the chart uh on the right hand side is very similar to the overall chart that I've shown at the beginning of this presentation and it's probably showing that the latency game is more severe um as compare if we compare it to the beginning of the merge because most of the blocks if you look look back to November are submitted quite evenly across different time frames of the 12 seconds whereas now most of the blocks are submitted in the last two seconds so just in a short spam of four months we see the latency game is more severe so this is probably not um not related to the market Panic events but more of a Time comparison in terms of the relay and the builders getting more sophisticated and competing on Landing the block now looking at the crosstalking we have um two different types of uh events happening also on the cross domain um the top Arbitrage tokens are very similar to the single domain or only theorem you can see most of the the hopping paths happened or initiated from either the stable coins or Bitcoin or east so this is also very similar in terms of the percentage so it's probably not that different across different Market events just one thing to highlight again the cross domain mov um in terms of US dollar value is much higher back in their FTX collapsing events um but the recent event has a much lower Mev value looking at the the chains and the and the particular paths we see that ethereum and BSC are are the most frequently armed chains and also one thing to notice is back in November we also see polygon and arbitrum whereas in the recent event it's mostly between ethereum and BSC okay lastly just a little bit more detail on the top 10 arbitrages by profit we have shown the top 10 arbs uh conducted across different domains so this is the exact path the the Bots have simulated where the Bots can detect any Arbitrage opportunities just to bear in mind that the data provided here is not actual cross-chain Mev realized data it's based on simulations across different eight different chains and around 40 different decentralized exchanges and 60 tokens so there might be inaccuracy in the simulation where the missing Arbitrage opportunities or the missing data could cause the value to be even higher so just to summarize I've talked a lot about different activities Mev Trends in L1 in Cross domain since the merge so what we can summarize is that we know that Mev estimation is very difficult we have only limited data from ethereum and own chain data there is a lack of data in in the estimation of nav in the centralized exchange activities we also observe like more relays and Builders coming into the ecosystem since the merge and the space is more Diversified the top Builders market share interestingly during the high Mev periods have actually decreased so this is also a sign showing that the Builder Market is more Diversified than since the merge in terms of search Arbitrage strategies we don't see too much of a difference between different Market crises most of the Arbitrage strategies focus on are being across ethereum Bitcoin and stable coins the liquidity the liquidation strategies are actually event based so if it's more of a um a crisis that's related to liquidation of certain assets then you will see the liquidation number in terms of Mev goes up but in the recent event this is not the case course domain strategies also have the same pattern there is the Mev activity spiked during the crisis but in terms of the magnitude on average it's much higher than the daily average probably because a lot of the Cross domain arbitrages on a daily basis are not as much as on ethereum but when major events happen the Bots are searching everywhere for opportunities so that's why you see much more of a spike during these events and also the strategies differs from different Market crises it's event driven and lastly I just want to mention that because of the lack of data we have a lot more room to improve in terms of the estimation of crossover domain Mev the centralized exchange to decentralized exchange Mev and there's still a lot of work to be done to collect the accurate Mev activities and total values so here is a list of the references and also some of the interesting readings that I recommend lastly a special thanks to Tina from flashcards and I can find block Sprout metrica semi-autics and block native for helping me presenting preparing these slides and contributing to the charts and the data okay I think that's the end of my presentation I can take any questions if there is any I think we're running a little bit tight on time ran a couple minutes there uh so we're going to turn it over to Phil for the next talk but I did see there were some questions in the chat um flashbots has also posted um a link in their Research Forum um so you can go there with any questions for follow-ups on this talk or any other talks um and we'll check in there as well after the fact so we'll bring them till now all right hey everyone can you hear me all right I'm going to assume yes and kick off my timer so today I'm going to be talking to you all about hypermevization or information is the new money or why everything is Mev and it always has been um or why geography and privacy these are the two memes I want you to walk away with run the world or why your project is now an Mev project which if you've been developing in the space you may have noticed uh in the last year or so um so having heard my talk title Shin on uh on Twitter got really excited and said man will we see a complete theory of why information is the new money tomorrow and Phil Diane's talk um and maybe that could have been the talk but today it's not um sadly I didn't have time to quite polish all the ends on a complete Theory so instead I'm going to try to zoom out like 10 levels from our previous talk which contained a lot of amazing data and talk about some high level Concepts around Mev information Theory uh privacy decentralization and all of these things that we're trying to build together you should see this talk as like some informed trolling which I think should be super fun for you um so take off your skeptic a hat and put on your fun hat and let's get started um okay so what are we going to try to do in this talk we're going to try to marry these two different areas of study these two different areas of research and modalities we've seen in Academia and when I was putting together these slides I realized I don't really Vibe with the institution of traditional marriage so I at least need to use like uh you know a same-sex marriage at the very least to represent this uh but even so you know not fully on board with this meme but anyway on the left here we have um kind of information based fields of studies or or technical Fields based of study where you reason about entropy bits of information um kind of computational algorithms on top of this Etc things like information Theory cryptography and privacy and on the right we have economics which traditionally has been also a modeling based field but in a kind of very different direction attempting to model human interactions attempting to organize human systems uh much more political kind of bent and here we have like the study of economics cryptocurrency and also Mev auctions so we're going to try to marry these two disparate Fields today with a single unifying set of abstractions that'll make both able to talk to each other better and hopefully we'll have good communication and a lifelong relationship so why do we think this is possible well here I stole the Wikipedia descriptions of both information Theory and money um which are two things that I want to draw some very high level connections today and apply to Mev so information Theory according to Wikipedia is the scientific study of the quantification storage and communication of information at the intersection of probability Theory statistics computer science statistical mechanics information engineering and electrical engineering like we just said all very kind of technical or logical Pursuits whereas money is this kind of nebulous economic concept uh it's any item or verifiable record notice the ore here so that's a very interesting or that is generally accepted as payment for goods and services or repayment of debts and generally people kind of flag these three properties as being interesting medium of exchange unit of account and store of value but you could argue that all of these three things medium of exchange unit of account and store value as well as payment are really just quantification right quantification is unit of account storage um uh store of value and communication AKA medium of exchange of information which here is called payments so you really see some like direct correlations between reasoning about information and reasoning about money that become particularly Salient once you apply them to blockchains so to unify these two things it would be nice to have a clean mathematical transform where reasoning about money can be translated into information theoretic models Frameworks and results and vice versa I'm not at all claiming this is original many people have been trying to do that for a long time and I'm going to talk about a little bit the history of these mathematical transforms and how we can apply these to Mev and maybe get somewhere um so in this talk I'm going to argue and this is really the outline for the rest of the talk that the key to unifying these disparate Concepts lies in both Mev and specifically in privacy Primitives useful in Mev options um so in how specifically we Define the Mev option that we're running on chain and in kind of the real world that is going to allow us to use both information Theory as well as money to create the best systems for users um so let's talk about the history of these attempts and what's happened in the past so one kind of uh strong existing attempt at this I think my camera probably crashed did it did it not oops don't want to show The Green Room either it's okay we'll continue without a camera YOLO um so um exchange attempts um sorry existing attempts have kind of all been under the umbrella of this thing that we call mechanism design um which is kind of a 1960s field that was started by this person called Leonid herzwick when he was kind of thinking about policy how should Central planners reach decisions on policy when the quality of the decision kind of relies on these information silos right and this kind of aggregation of information into a single output is what we call mechanism design it studies the economic as well as the the flows of incentives and information to make these kind of decisions um for everybody so some fun conclusions that underpin this whole field of mechanism design Etc um so first one is this thing called the gippered sather white I'm probably butchering this because you know uh it's been a while since I've taken a class on this theorem uh from 1973 and 1975. and this is two kind of philosophers as well as an uh sorry one philosopher and one Economist that were attempting to study the same problem of allocation and voting and making decisions based on voting systems and they basically in their own model of voting systems noticed that there was a tension between three properties where kind of all three properties seem undesirable to a voting system so the first property is if the rule is dictatorial so basically if we're all deferring to one distinguished voter if there's one voter whose input completely determines the outcome of the vote that seems super bad right there's this critical point you can imagine like the one swing voter in the U.S election that ultimately chooses the outcome we're giving this node a lot of power and that's surely seems undesirable specifically to decentralization Etc um so that's bad let's uh let's say that's bad the second tension is between the rule limiting the possible outcome to only two Alternatives so in their model you can get past number one and three if you only consider two Alternatives obviously seems bad right we live in a very complex world and only kind of eliciting information or choices between binary Alternatives is super limiting and the last one which kind of underpins and is the fundamental result of mechanism design is if you have if you kind of want something that's not the tutorial with more than two Alternatives you're always going to be subject to Tactical voting so in many cases a voter will not be incentivized to vote their sincere ballot they'll be incentivized to strategically play based on their model of what other people are playing AKA play the metagame As we call it in crypto and for voting systems this is super undesirable because you collapse into this world where everyone is reasoning about the metagame people are never truthful and it's very hard to think about what actual outcome the vote is eliciting if everyone's playing strategically rather than giving input that's correlated to themselves um so a fun result that actually underpins the fundamental mechanism design studies of like the 70s and 80s is that if you just add one property here you add utility functions to these individual agents and you allow the mechanism that you're designing to do transfers um over these kind of uh this kind of shared set of integers you can do things like model human rationality you can build much more complex mechanisms using things like auctions and assumptions about how people will behave against these transfers that allow you to bypass many of the results of classic social Choice Theory so basically what this says is that by adding transfers by adding money we can do much more than we can with pure voting we can do much more than we can just with information elicitation if we don't have money and that kind of kicked off the whole field of mechanism design so money is a very powerful primitive that being said around the 80s and 90s people actually started questioning this result and thinking can we actually take half a step back here and rather kind of going then going pedal to the metal on money and adding rationality to our life maybe we can look at other models that allow these nuanced uh kind of transfers of information that let us achieve the same results as money does and bypass all these classical voting and possibilities um but do so in a different model so this is one paper that kind of looks at this I think the kind of paper itself is mostly irrelevant but I thought the intro was like a very good phrasing of the problem uh specifically this paper looks at continuous mechanisms which is one modification you can do to the model rather than looking at discrete utilities kind of allow continuous functions Etc same for transfers um again that's kind of a sidebar to what we're talking about but the core of what they were saying in this intro is like let's look at mechanisms without money and tweak them and see if we can still do things without at least classically what's considered money this transfer of like a shared unit of account uh in this exchange right um so what did they say they basically said in many contexts these monetary transfers simply aren't available and we still want to solve mechanism design problems so specifically they were looking at things like delegation within a single firm where you can't like transfer really money within a single firm or incentives aren't as clean or more political political science so direct politics incentives were often money is viewed as corrosive or regulation where like money transfers may be prohibited in certain contexts among others and they actually found that basically um if you tweak uh the the model slightly where you basically have principles um commit to the principle of a mechanism commit to a rule and then the agent report uh private information that executes on this rule commitment you can basically achieve a lot of the same properties as you can with money so they're saying even if you don't have money as long as there's a commitment to a rule that determines the outcome and then there's information fed in that is kind of acting on this commitment by the agents we can still get a lot of the same beautiful properties as if we have money uh so this is kind of what I'm going to be talking about for the rest of the talk how does this reasoning about commitment and information relate to money as we view it in cryptocurrency how does it relate to unchain tokens as we call them money and how does that relate to kind of like the bull monetary case uh for cryptocurrency itself a fun another connection which is in the cs700 UPenn lecture dope class highly recommend you check it out if you have some free Cycles it talks about how you can actually use differential privacy in mechanism design so by tweaking the model to allow for differentially correct outputs and differentially private inputs into these commitments and into this information you can also achieve like a whole class of different mechanisms that aren't possible without differential privacy um so basically what I'm saying is information theory is this framework that's tying together um partially through mechanism design which is one application of information but also in in all these connections with privacy and with economics and with money trying to tie together reasoning about these different uh contexts and these same outcomes we want to achieve um so how does this make sense for crypto well it actually gives us a roadmap for crypto like we need to marry information Theory with money through commitments and privacy it's a clear roadmap of what's happened in the past what's been successful in like reasoning and we need to Port this over in a more clear formal way into the systems we're building in cryptocurrency um so this is a little bit of the trolling I'm doing here I have a lot more slides that I'm not going to get through so I'm going to go through it slow because I think this is actually one of the fun ones um this is presented without proof so shout out to roon who's probably uh triggered by this without proof part and also you know maybe you can help help us prove all this um there's a few kind of intuitive results that seem obvious once you start thinking this way one is that if you have trusted private smart contracts so let's assume we have this magic privacy box that we all trust that gives you strictly more expressive power than public smart contracts from a mechanism design framework from a modeling framework of like how powerful is our model which is what we've been talking about with these various Frameworks you can prove this really easily so look at just instantiating like let's say a vcg seal bid second price auction in crypto if you don't have uh trusted privacy it's impossible to do this securely with two untrusted parties let's say Ethan Solana and uh Avalanche want to run a sealed bid option together with like the output of the validator set being the bids you could see how you could build this with private smart contracts but it's very difficult to imagine building it securely without unless you have centralization which is which is also private smart contracts but in a much more ad hoc way what does this mean this means that once you have privacy you can build money more auctions so you can make auctions more secure and more powerful with privacy as a tool not only that but there are many auction structures for example the one I just mentioned with vcg not all auction structures but many that require this privacy interface to be able to be trusted to be executed on a chain um so private smart contracts are strictly more powerful otherwise known as informationally better money they can give us more options to approximate more properties of more powerful money than we can without privacy assuming the Privacy comes with no downside so this is the big asterisk here if the Privacy centralizes our whole stack and self-defeats its own goals this doesn't work if privacy can't be built and comes with like a huge cost this is much less clear so we're assuming here we have this magical privacy primitive but assuming we have this the world becomes amazing and the last thing is that if we build systems that are interoperable and have Global reach we have more efficiency right this is the basic premise of decentralization the pi is bigger when more people participate even if a system becomes less efficient the economic value of participation shines through and gets born out so thus the best efficiency feedback loop we can build is this uh loop of interoperability and trust and Global reach um proof sketch this is Trivial so if people don't participate in mechanisms they don't trust obviously a global mechanism is more valuable than a locally um so we're not in such bad shape here because crypto actually already has transfers and transfers in crypto are fully trustless whereas privacy and crypto is not right now so we're in kind of this gray Zone where there's a few classes of mechanisms we can build anything that requires public transfers that can be satisfied by the blockchain but there are many things that we can't build the ones that require privacy the auctions that are really more efficient if you have privacy and this is what's causing externalities in the economics of our systems today because many of our mechanisms require trust so we're seeing this privacy because this privacy is economically useful it lets us build more mechanisms but because the Privacy is not trustless we're resulting in trust and we're resulting in externalities um so how can we solve privacy what is the abstraction boundary here what does privacy actually mean and how the hell do we solve it that's kind of the focus of the next um part of my talk so first giving you the bull case let's say we do solve privacy and we come up with this magic black box that I just showed and we have Universal trusted privacy amazing um what will we gain if this happens well first of all powerful Network effects for this better class of mechanisms I just talked about this is going to embarrass the crap out of tradvice so tradfi doesn't have Universal trusted privacy that's why you see regulation that's why you see broker dealer conflicts that's why you see these auctions that nobody knows how they execute because you don't have this concept of universal trusted privacy so we should make this look embarrassing that's my opinion as a space this is our mission the other thing is we should embarrass classical mechanism design because a lot of the results even regarding transfers assume you have a trusted Auctioneer to do these transfers and if you have privacy and blockchain with some asterisks that we can get into in a separate talk um you can kind of go beyond what's possible there that would create a clear adoption case for crypto right because many markets would just start working better than they do otherwise better for users with better commitments and better mechanisms and more efficient apps and you know that's how crypto eats the World by building privacy that is actually better for users and Universal um so where are we today we have this kind of Meb supply chain we're going to talk about various components here including Mev share and Mev boost both of these components rely on privacy so the reason we can separate these roles and gain the efficiency of separation of Duties and keep the decentralization uh of the validator set of the Builder set of the Searcher set Etc is because we're able to have these abstraction boundaries uh MAV share and math boost being two examples of money reasoning abstraction boundaries we have today but the Privacy here is not perfect the Privacy here is flashbots just saying trust us and that makes it super sub-optimal and you can see that if you look even at the Mev uh Mev uh uh not Mev boost any of you share flow here which kind of shows you the transactions that a user makes which represent commitments and the selective data sharing which allows them to Define privacy this is how we build a more efficient Mev mechanism that we believe is best to redistribute value back to users and this is the what we believe is the economically best way however it relies on these two very centralized Concepts to achieve this economic output we want so from the information theoretic point of view it's super subpar and from the kind of decentralized system reasoning it's extremely centralizing which makes it very sub-optimal um okay so here I talk a little bit about how to use various techniques to go in depth and get out of this including committees sgx and tex crypto economics MPC fhe and the centralized systems that we're seeing today our belief is that you layer all these things together in a maximally decentralized way to truly achieve decentralization so I'm going to skip this part of the slide which kind of talks about how to use tees how to use economics how to combine them and get to this world where basically we're going Beyond sgx and we're achieving decentralized privacy I will leave you with this to say that this is the goal of our Suave project it's to achieve this defense in depth on this boundary between the ideal mechanism design for Mev extraction and allowing users to have these abstractions to control their privacy that can then be implemented into cryptography that can then be implemented into differential privacy into information theoretic reasoning Frameworks directly um so we see the Suave Primitives the very formal core of suave as being essentially the translation layer where expressing your transactions through these Primitives allows for a direct translation to and from mechanism design and reasoning about the class of mechanisms we can build the types of money we're building as a community and where the basis of this money comes from and how that relates to Mev and in the other direction uh translating into information and privacy and asking the question of how do we implement this and how do we protect this information Mev share and MAV boost is kind of the Solo Cup to our Holy Grail it's good enough for now it does the job but it's centralized so it tastes a little bit like plastic right so it allows us to start experimenting with these translations but it does need to be replaced eventually with the Holy Grail and that's what we want your help with that's what we want you to help us with at flashbots and that's what we're doing so this is real we're already experimenting with uh introducing more decentralized techniques and more decentralized guarantees including economics including sgx into our existing mark Market come join the conversation we believe this forms the bull case for all of crypto where Nev feeds into these Suave Primitives people start expressing and leveraging their privacy in negotiations this fuels developments of sgx2es and encryption Technologies like fully homomorphic encryption eventually cryptography wins and beats uh tees because it's better and I was criticized for putting the AGI bullet point last because I was told AGI feeds Mev and not the other way around but there's your bull case for the economics of growth in the future join us at flashbots.net and I am overtime so that is all the time I have today thank you very much awesome thank you very much uh so we have a couple minutes before vitala comes on uh one question I wanted to ask you earlier on is like what are kind of your initial intuitions on you like you were describing how it's strictly better if we assume that we have this magical privacy thing that comes with no costs and it works perfectly and everything um but particularly based on where we are and where development like looks like it's headed like what what do you think is the kind of practical state of that is it it's good enough but it has some cost associated with it um that's outweighed Etc I think it's an extremely nuanced question and I would love to give a talk on this this is kind of the extreme version of like the reasoning uh but I think that what I would say today is like the pressure for privacy like the economic side is really powerful like people want to build the types of systems that can only be built with privacy that's why math boost exists that's why MF share exists that's why custom dap layer centralized order flow auctions are being deployed um and and that's why people are talking about things like Builder features and using that for for PBS and bank sharding and things like that at its core is because like privacy and commitments together enable like new classes of mechanisms and people really want this um and so what we've seen today is the centralized stuff has kind of been good enough quote unquote for now like it hasn't collapsed it's somewhat economically Diversified but it is a huge centralizing pressure on our systems and fundamentally I don't think that's good enough um adding sgx I think is a better step in that it kind of shackles these actors a little bit more and restricts what they can do um and you know limits them but it's also not a silver bullet because it's still centralized it still relies on Intel Etc adding committees or crypto economics similarly has like its own downsides so I think we need to like do a combination of these things that just maximally decentralizes every single Technique we can use and then put the max decentralization pressure on because the economics is not going away so if we don't build like the decentralized mechanism for that to express itself we're going to end up in track five so that was like a very compressed range but that's kind of the state of things today in my opinion one of the other slides you were sailing through because we thought we were chat on time uh it's talking about kind of the post relay world and how we have to remove all the centralized components part of that starts to become kind of out of the hands of uh people like yourselves where let's say that ethereum doesn't actually end up implementing for example in protocol PBS um like how bad would that be quote unquote in your mind slash or are there just other ways that we would have to figure out how to how do we decentralize that role in a better way if it's not built into the protocol itself and we keep the protocol super simple yeah yeah so I mean I think in many Futures it's not critical that ethereum Foundation puts hard pushes hard on this maybe the community is just aligned enough around making it like decentralized enough and assuming we can decentralize things like much more than they are today and predictably like it will play out okay for the L1 maybe it's okay for EF to take a step back and we're not like totally screwed um so maybe we can do a lot of this without the EF in terms of removing the relay building decentralized Alternatives building like you know sharing Primitives for users and things like that that being said I think if the EF in practice doesn't have that in its back pocket as like a nuclear option things get really bad because like the worst case would be some angle of centralization in trenches here and we're like in a textbook moloch and so that's why I think PBS is super useful because like many things the EF can do will like reset that moloch and I do think it's their a little bit their obligation to stay on top of that just because otherwise their own objectives will be defeated I do agree with you it's beyond my pay grade in this case so like I can say what I think is the right thing to do but ultimately like it's up to the community and we'll try to maximally decentralize whatever is left basically awesome thank you very much uh we will be moving on now so we have vitalik up next he is going to be speaking on protocol economics uh the ethereum protocols perspective okay thank you guys are you uh here are you guys hearing me well yep we can hear you okay perfect um so what I wanted to give to uh today is a brief overview of uh like what protocol economics looks like in general and focusing on ethereum and uh some of the changes that have happened to both the ethereum protocol and and to the ethereum application ecosystem over time and touch on you know some of the issues of uh how that interacts with uh some of the things that might happen with both ethereum protocol and the ethereum application ecosystem in the future um so some of the protocol economics questions that matter this is pretty basic uh so one is uh what are the incentives facing users uh two is what are the incentives that are facing consensus actors so meaning minors and validators um what is the behavior of the chain uh both average case behavior and extreme case behavior and how does the behavior of the chain affect incentives at the application layer so for example if an application depends on the Chain working in a particular way for its incentives then if the chain ends up Suddenly working some different way because of an attack then that might cause the application to have some problems and how do applications affect any of the incentives inside those basic users inside the space consensus actors incentives to do things that might break the chain and all of that stuff and so there's some pretty complex interactions between all of these right so users sends transactions to the chain users read data from the chain uh the the state of applications lives on the chain and so you can say the state effects say yeah or the chain effects of state of applications applications give incentives to users and applications are the reason why incentive uh your users sends transactions that have particular strings of data instead of other particular strings of data of consensus actors maintain the stability of the chain and the chain provides some intentional in protocol incentives to consent attackers so this includes mining rewards validating rewards various fancy and protocol defined rewards it also includes penalties and applications uh also provide more incentives right so there's in protocol incentives and then there's also extra protocol incentives and ultimately this whole Mev situation is a situation about extra protocol incentives that arise mostly accidentally and how they end up affecting some of the incentives to run the chain so some of the risks to watch out for are I mean one I think and the one that we've focused on for the longest is just the incentive for validators to act correctly instead of acting incorrectly and in particular the incentive to keep advancing the chain instead of trying to 51 attack or for example trying to censor the chain right and the kind of in uh battle between the yeah users incentive which is just is the transaction fee and now just the priority fee to get included in the Chain versus potential bribes the sensor and you know might bribes the sensor happen accidentally or bribes the sensor or something it might happen intentionally I'm like trying to set economics so that the user incentive has the upper hand and so that's side incentives uh don't have the upper hands right like that a lot of that was a big part of the discussion and things like PBS censorship resistance um but aside from that there is also economies of scale in incentives and one example is that if user incentives are complex then that might lead to wallet centralization right so for example pre-eip1559 uh you the incentives facing users in terms of how the user sets the gas brace we're pretty complicated and a user got a pretty complicated trade-off curve between getting their transaction at good quickly and waiting a very variable and unknown amount of time for their transaction to get included and this both caused a huge amount of dead weight loss because a lot of users waited for for their transaction to get included for no good reason and it also I think made it harder to create wallets and probably yeah ended up contributing somewhat to dissuading new wallets from getting created post eip1559 incentives for users were generally a lot simpler you just set a reasonably High match base fee and then you set the priority feed to some probably a constant number or if there is an active an active situation going on you could set it to something higher using a much Dumber algorithm and you get penalized much less right but generally the situation is import for users we also see economies of scale for consensus actors right and that leads to consensus centralization so if the strategy is that consent to sectors see are something that's constantly complex and shifting and especially requires access to proprietary information and all of these things then that's gonna be really bad for solar staking it's going to be really bad for decentralized pooling even it's even going to be really bad for small centralized pools and so it's just going to lead to big pool domination right and I think we even saw in near the end of the proof of work era ethereum centralized pool domination was something that started to get pretty bad right we got to the point where like two mining pools were enough to get over 50 percent and uh Bitcoin is like a little better than ethereum proof of work but it wasn't that much better than ethereum proof of work and so like I think we want to really make sure that proof of stake goes in a healthy direction as well um and then finally incentives to have low latency are also a different kind of centralization incentive because they are a geographic concentration incentive they're also a cloud computing incentive and both of those things actually yeah contribute to jurisdiction risk which is uh and other really important and big risk to the credible neutrality of a blockchain as I think we've seen over the past year um so a lot of this started basically with the rise of defy right so uh burst Mev meaning uh a kind of these opportunities to grab up a large amount of money by being the final actor that constructs a Blog a lot of those um actually existed even all the way back in 2017. so there were these pretty famous icos that happened in 2017 the yes picture on the left is uh a graph uh graphing the gas price and it got to an extremely high number and there was just like a huge amount of deadweight loss um I think this was specifically during the basic attention token Ico and had there been um so like every view collection infrastructure at the time that could have just been collected by um you know whatever mining pool was uh sophisticated enough to have that infrastructure right but realistically at the time there were just not enough of these incidents to probably justify here paying the fixed costs of trying of uh you know building the infrastructure to try to capture all this meth but then in 2019 2020 we had defy summer and um at that time we had uh you know basically a kind of the start of essentially permanent uh combination of uh kind of ongoing Mev yeah that would happen pretty much every block and even bigger and uh even crazier spikes than we saw in uh 2017 right and that created economy is a scale for consensus actors which uh ended up creating the necessity for things like PBS to try to kind of split up the economies of scale heavy role so that the remaining world would not be so economies of scale heavy and the chart on the right is uh daily extracted mbvu gross profit which shows how Mev basically you know is a constant but at the same time it is uh very spiky um efd1559 right so that greatly reduced waiting times for users and it also greatly reduced the incentive to use complex gas price strategies as I mentioned which reduces barriers to entry for wallets the merge actually yeah reduced waiting times for users even further and I think there's going to be some research on that coming out fairly soon um and uh you know that's something that's going to improve uh further over time um the merge actually does a lot of strange things at the same time right so the merge first of all it completely changes the class of participants maintaining consensus right which means that the side incentives of people maintaining consensus change a lot before it was minors now it's youth stakers miners tended to be a group that was kind of off in their own corner and uh often not even very loyal to ethereum because they would jump between ethereum and other protocols now there were some miners that were definitely very loyal to ethereum but there were also others that were that were not um and it's uh people that had access to Hardware whose main expertise was in things like Hardware meanwhile each stakers on the other hand are a subset of etholders and if holders tend to have less sophistication and Hardware but they um you know also tends to be more integrated into the ethereum ecosystem in other ways and you know there was complicated effects I think from the change of uh like what kinds of people were doing the mining to what kinds of people are doing beef staking on average right um so I know a lot of miners um actually ended up becoming Youth stakers and so a lot of actors carried over from one side to the other but I mean on average the composition did change a lot um the merge also made a reorg attacks vastly harder so there's this uh uh piece from myself and Georgios from uh almost two years ago that talked about how reorgs after the merge are going to get much harder um and then just also a very different incentive landscape and sort of things like centralization and joining a staking pool and what kind of pools are even possible and all of these issues right so the merge changed a lot and uh you know I think the mortgage is one of those things where even 10 years from now uh you know if someone asks you know what kinds of effects did the merge have you know as the apocryphal quote about the French Revolution goes I mean or the correct response is going to be it's too early to tell um so that's uh existing changes now upcoming changes withdrawals are going to get enabled in the next hard Fork then stuff going beyond that single secret leader elections different types of in protocol PPS and finally yeah single slot finality so which draws basically two consequences right people can withdraw and sell people will will withdraw and that reduces the total amount of each state but then also a people can restrain so people become more confident that if they stake they'll get their money back which leads to more is getting staked which one of these dominates um you know we have no idea could go either way though ultimately I think the main practical consequence of this is going to be a change to the total amount of staking and possibly also a change to the composition of stakers um and uh I mean my hope is that for solar stickers things are going to become more friendly and which draws are going to kind of reduce the premium that pools have because currently pools have this premium that they're the only way to get liquidity and with withdrawals and are you going to be able to withdraw and get your money back in a couple of days uh so it'll be interesting to see what kinds of consequences that has um but single secret leader elections these are interesting right so only the proposer will know that they are the proposer before the block is produced and though the proposal could voluntarily reveal that info and even prove that they're going to be the proposer so the intended effect of all this is to reduce the Dos risk against proposals right if no one knows who you are until the block is created then there's nothing that can be dust and once you reveal Who You Are by creating the block well the Block's already out there and so even if everyone does as you well I'm you know the block is already out there and it is going to join the chain but there's this interesting research question of like are there under unintended effects and in particular especially in the combination of PBS like would there be incentives for proposers to pre-reveal their identity to anyone um I'll skip over like PBS for now because I feel like everyone is going to be talking about PBS and it's probably the kind of primary subject over the next few years in some sense uh but but it's uh you know a fascinating rabbit hole and then a single so what finality right so singles award finality basically means instead of uh blocks finalizing after two epochs every slot is just going to immediately finalize a yeah block with the exception of uh these special cases where you have it in activity League Sega sword finality means even more reworked security it might make ethereum more Bridge friendly um but uh well as we'll get into with layer 2 pre-confirmations single Squad finality will not be de facto single slot for a lot of users and I think that's going to be an interesting nuance so finally yeah let's switch to Layer Two and protocol economics right there so how does the rise of lawyer tours affect the incentives facing uh specifically actors in The protocol at layer one um there a lot of it depends on like how how the whole Layer Two thing plays out right so one is sequencing style so uh on each research uh Justin Drake made this post about what he calls based Roll-Ups uh basically Roll-Ups that just uh do uh letlier one directly to the sequencing for them versus some kind of lawyer two controlled sequencing where you have a lawyer to Define sequence or auction um and um all of these things what percent of layer 1D file will move to layer twos if the answer is zero Layer Two is half no impact if the answer is a hundred percent then everything depends on how the lawyer twos work and then the answer could be somewhere between zero and a hundred percent fraud proof windows and censorship that's a pretty yeah simple one uh pre-conformations that we'll talk about and then this kind of final question which is will the switch where a lot of activity moves to all your twos make a layer one economics more boring possibly by insulating the lawyer one from the side effects of some activity or will it make it less boring by creating more complicated interaction effects so based Roll-Ups right so uh when I made my post on Roll-Ups about two years ago one of the things that I assumed in on is basically this question of uh in a roll-up you have these batches that be kind of uh take a lot of transactions and commit them into the lawyer one and the question is like who has the right to submit a batch option one is total Anarchy anyone can submit a batch at any time and here I said this is the simplest approach but it has important drawbacks particularly there's a risk that multiple participants will generate and attempt to submit batches in parallel only one of them will get included this weed store wasted effort in generating proofs and wasted gas and Publishing batches to chain with PBS a lot of these costs really get reduced especially the gas costs get reduced right because you don't you can make the Gas payment of a batch conditional on that batch get included and to get incorrectly processed and this is something that you will have to work through the Builder ecosystem in order to do but if the Builder exists then you can do it and so suddenly the total what I called the total Anarchy approach or what Justin calls the base roll-up approach suddenly becomes viable but then there's these also other approaches right centralized sequencers you could do SQL Server auctions you could do a dpos voting you could do random selection from a set you can combine sequence or options with DP OS voting to remove the sequencer if they're abusive in some way that layer one doesn't detect but still really matters to users right and so there's a lot of options that are not based and there's an interesting question of like do you grow Ops want to be based or not critically or twos absorb complex layer one unit Mev if Roll-Ups are based then they can't right because layer 2ab just becomes layer one Mev because the layer 1 uh PBS I'm a system just does and like sequencing for the layer twos if a large part of layer 1 D5 stays on layer 1 then also no if there are other complicated interaction effects so like for example if rapidly or one transaction inclusion helps lawyer to Arbitrage and whoever the first transaction is to get included ends up just like becoming the first one that successfully can do a particular way you're too Arbitrage or like if there's other interaction effects then maybe no and otherwise um you know maybe right and this is one of those interesting research questions fraud and censorship pretty easy if you could censor a chain for a week and that chain doesn't social or reward you then you can steal from optimistic Roll-Ups but I would argue this is not too interesting because there's like much shorter term censorship attacks that are more realistic that can seriously hurt D5 right if he can only censor for an hour then you can do a huge amount of nav extraction on all kinds of D5 projects like Roll-Ups or I'm sorry or like uniswap like systems uh potentially yeah even liquidations you might even be able to force liquidate people and prevent them from filling up their uh CDP is so you could do all um all kinds of stuff right and like that's also something that's worth thinking about but that's like not a way or two specific problem right it exists somewhere one or a layer two pre-confirmations so users generally want faster confirmations than even 12 seconds and a lot of Roll-Ups are not even doing 12 seconds because they want to save on fees and also might end up increasing the SWAT time maybe from 12 seconds to 32 seconds um and so what uh a lot of Roll-Ups are thinking about is uh doing these uh pre-conformations right so you have an off-chain pre-confirmation protocol that pre-confirms the way or two blocks and then eventually you have a batch that kind of submits the roll-up walks and then possibly some like proof a compact proof of the pre-confirmation into a layer one block all of that gets verified and then that actually solidifies the block out layer one right and so with this even if you have single slot finality earlier one out like the user's experience is that step one they uh accept a their transaction gets accepted into a layer two block step two of the lawyer two big buckets pre-confirmed and then finally step three gets confirmed on uh layer one um so arguably this is a point in favor of ssf because uh if lawyer to everyone's using Layer Two is an order twos have free confirmations then it's okay for layer one to have vloggers a lot of times and the loggers a lot of times generally make ssf easier but still you have two tiers of Confirmation and I think the two tiers of confirmation thing might be fundamental because layer one is not willing to centralize as much as Layer Two And so where your two confirmations are always going to be faster like there's just always going to be some kind of uh curve where a user's knowledge of how confirmed their transaction is doesn't just jump from zero to 100 immediately it kind of goes up over time and so you have some notion of free confirmation that's uh inevitable now notice that when you're too pre-confirmations are incompatible with based Roll-Ups right a roll-up cannot be based because if it has pre-confirmations because well the submission protocol has to force the system to respect to the pre-confirmation mechanism and a set medical or two blocks that does not respect pre-conformations it has to just reject right and uh ultimately like it would be the layer two in the pre-confirmation system that collects most of the mvv instead of layer one right so uh based Roll-Ups cannot reconfirm which is uh I think an important thing unless you do some kind of layer one universal bricky Ferber which could be interesting will your one be more boring or less boring I mean My Hope what I would love to see is for layer one to be more boring but this is not guaranteed right Warrior Two could absorb functionality and risk or Layer Two could create complicated interaction effects between layer 1 and Layer Two And I think ultimately um you know we need more analysis of a fully post way or two world and I think um you know like Mev economics with way or two is in mind or just a yeah a really fascinating research topic that I mean we did it we'll probably continue to need a lot a lot more people thinking about for years so thank you awesome thank you vitalik for the great talk and that actually perfectly segues into our next talk by hasu um who is going to be talking about decentralizing sequencers um and PBS in the context of Roll-Ups so also you can come on up uh Hey Hey thank you vitalik thank you John um hey I lost weight I currently uh lead strategy at flashbots and I will talk to you about decentralizing sequences uh wait it's all PBS yeah uh always has been so in this talk um I will convince you roll ups are in the process of decentralizing in their sequencer so every roll up um has a decentralization roadmap and all of these roadmaps have some plans for decentralizing the sequencer however these uh the way that sequencing works and they are too it actually combines uh what is the equivalent of a layer 1 proposal and layer one block builder in the same role and this creates a variety of problems unless we start to address it by separating these two roles on Layer Two just like we did on layer one in addition to that even if we do PBS on layer 2 there are novel challenges involved around privacy cross-domain mov and lower latency than we are used to on layer one and so we will get into these in the talk as well um and we will see PBS is essential but it is in itself not enough we also need to decentralize the Builder role itself so decentralizing roll up sequences what is a sequencer there are basically four steps involved so as a user you send your transactions to a layer to sequencer who orders them according to some policy and then gives the user some receipt to the user so italic was talking about pre-confirmations that is step step number three here and then the sequencer sends the ordered batch to the DA layer where it basically becomes finalized from the perspective of the layer 2. so this particular diagram is taken from stagnet but the above is how it works in pretty much every um every Layer Two so there might be some small differences for example um in stock net a the sequence size also the prover so they have even more responsibility but all of these can be stripped out and would be stripped out over time so why is decentralizing layer 2 sequences important well because as vitalik was alluding to because roll ups are becoming more and more important and more and more volume and liquidity is moving over to this new execution layers and um I don't think we have even started to imagine the end state of um of how blockchains would scale as we are kind of seeing with these different Roll-Ups uh roll-up providers who are now not even just focused on building their own rollups no they are focused on they've all pivoted to kind of Roll-Ups as a service and um I think it's quite possible that we will see a future where um spinning up a new blockchain is is kind of like spinning up a new smart contract today and so we will have many Roll-Ups and these roll ups all will have sequences so we left many sequences and today all of these sequences are centralized and centralized sequences um Can censor the user they can go offline they can charge arbitrary prices they are regulatory choke point and they can see the user transaction before it gets confirmed and they can do various things with this transaction that is hard to attribute and so while rollups have these great guarantees so we can get always a transaction Mind through the layer one and the sequence I cannot force an invalid State transition and so on then there's still a lot of problems with them and that's why we need to decentralize them there are various proposals for how to decentralize sequences um one is the layer one or quote-unquote other chain sequenced approach so that includes the base sequencing that vitalik was talking about they're also other approaches for example about shared sequencing so that's when one blockchain um sequences many other blockchains so shout out to Espresso for example who have put forward this Vision um you have the optimism style sequencer slash block auction um where the right to propose the next block or an X series of blocks is auctioned off every now and then um and then what stock stock net is planning random selection from a proof of stake set so in that case you have um you have basically consensus mechanism on top of your of your roll up and a proposal is selected randomly and and they get to propose the block this is very similar to how it works in in the layer one and then finally you can have various committee based Solutions and the most common one would probably be first come first serve um this is where Committee just looks uh what transaction um the different notes are first and then come to some kind of consensus on that and um yes we were saying already in the uh in the summary in the layer 2 right now uh what we understand as the term sequencer actually combines um the equivalent of a layer one proposal and the layer one Builder um especially in the leader election mechanism so that is The Proposal Part and then the ordering mechanism that is the building part and so if we go back to these different um proposals that have been put forward so they actually have different leader election mechanisms different ordering mechanisms but what I would like you to take away from this um from this slide is only that the innovation that is inherent to these proposals is almost entirely in the leader election mechanism so nobody has any credible plans about the ordering mechanism right so they are only focused on the leader election mechanism today and that is a problem because if you do layer twos without proposal Builder separation then what you do then you have the two roads bonded like they used to be bundled in ethereum layer one before Mev was a big deal but now Mev is a big deal and what would happen is you recreate the Dynamics of pre-pbs days from ethereum layer 1 on these roll ups and what you get you get priority gas auctions again you get front running you get failed transactions and a clocked up P2P layer from the Searcher strategies who are being forced to compete in inefficient Mev auctions for um for the Meb and because the option is less efficient you will also get a lower proposal Revenue that leads to a threat of vertical integration between searches and block Builders so all of these problems that we talked about two years ago on ethereum boom they are back now on Layer Two making their comeback and so that's why I would actually say when we talk about decentralizing sequencer to date sequencer as a service I think it would be more accurate to actually talk about proposal as a service because that's what it really is right it's we are only looking at the proposal today so we need to start separating these two roads on layer 2 just like we did on layer 1. here's a quick refresher for you what PBS actually does so the effect that PB has on ethereum is it Shields the proposer role from the centralizing effects of Mev by making the most valuable block available to all proposers equally and you don't whether it's a small Solo validator or it's a big staking pool like Lido or coinbase they all make the same from Mev and that is a huge huge achievement it also unlocks competition on features from block builders that don't require any protocol changes to ethereum and this has created more privacy for bidders more expressivity for bidas and TI would particularly highlight the introduction of the bundle by flashbots that decoupled the position of the transaction from the price because in the public mempool you were only able to express basically these two Dimensions uh through a single unit which which was the gas price um the removal of negative externalities from the chain that were the result of these uh these bad Searcher strategies and um maximizing revenue for the seller and so we can see this map one to one to the problems that we laid out earlier um and PBS solves them all which is amazing so we need PBS and explicit Mev options to maximize protocol revenue and minimize negative externalities on these layer 2 networks and their users but PBS only A2 also faces novel challenges that were not necessarily here when we designed PBS on on layer one so a big one the arguably the biggest one is privacy so all of these centralized sequences have gotten the user very addicted to the idea of quote unquote easy privacy right and um now I think it's a very hard Choice like do you remove privacy when you decentralize the Builder or do you try to decentralize it I think both paths are very very hard and it's it's not clear that either of them um is necessarily better or easier than the other and um and I I think this this kind of speaks to um the difficulty of Roll-Ups just taking these shortcuts through a centralized sequencer in providing the users better ux and now it's it's going to be very hard to either move away from that ux or um decentralize it so how do we recreate privacy in a decentralized way so um one is maybe you should have centralized block Builders but at least these centralized block Builders are competing with each other in a in a sort of competitive market right but the downside here is that these Builders all have different they all have their own own unique form of trust and so as a user or as a blockchain you need to decide which Builder do you trust which build do you not trust and this really risks enshrining some Builders over others and it also creates a big incentive for vertical integration between different supply chain participants because you're always more willing to trust yourself than you are willing to trust someone else and so this option I would argue it's very centralizing second one is again the formation committee based Solutions like threshold encryption or first come first serve these models work together and then option three which is having one homomorphic so I mentioned the second one because this is basically a way of privacy and easy mode if you will because it's much easier to do I stand under the idea of an encrypted mempool or a any system really where um audio back okay any any system really um where different parties share the same type of privacy where if you trust the the Privacy Zone then you can also trust everyone within it and so um for example what this would mean you send an encrypted transaction to a mempool and you know it's going to be kept private no matter who interacts with it going forward and so this is what we're working on um Suave but it's also super hard foreign just when you think you're out they pull you back in um even if you have competition between the centralized Builders you still have the problem of cross domain Med the Builder Market is already naturally very centralizing um due to economies of scale um but layer twos uh just because of the existence they make the problem even worse because the more fragmentation the more domains they are the more fragmented liquidity and and trading volume is the more cross domain there will be and this creates a strong centralizing pressure on the Builder oil through demand for Atomic transactions and then finally there is latency and this I would put in the same category as as privacy so roll ups um they all wanted to have or at least a doctor when they wanted to have um based sequencing originally but then um it turned out that users liked faster confirmation so much that they have had to switch to a centralized sequencer with faster pre-conformations um and this lower latency this is a big friction with decentralization we need geographically decentralization for our neutrality and decentralization and other things but for that we need our systems to be insensitive to latency um and if you lower the block time too much then what you get is you encourage vertical integration geographical integration between proposals and block Builders and to discourage participation from anyone who is not able to vertically integrate who is not able to co-locate in that same geographical area and so that's why PBS is essential but it's not enough we also need to look into decentralizing the Builder role itself so that's what we're trying to do with Suave um there are other proposals as well so but the idea is building the best Mev auction in making it decentralized um because that's that's what allows us to do the heavy lifting on ordering for these other chains removing all elements of trust in flashbots and Searchers and block Builders from the system making it private so that participants have the ability to trust the auction and Trust others and don't have this need to vertically integrate um with others in the supply chain and making it scalable because by connecting many different domains we allow for the expression of cross domain preferences as well so John charbone our host uh has this great um diagram here that actually shows the roll-up transaction supply chain in the future uh possibly and it shows that the sequencer really becomes the proposer right and you can have proposed us that that basically sequence many chains but you still need the block Builder and because you you have these centralizing effects on the block beta you also need the block Builder to be decentralized so in the roll up end game we're gonna need all three we're gonna need the decentralized sequencer gonna need the decentralized Builder and we're going to need the private mempool uh otherwise it's it's not going to work out and um dear roll ups please make Mev a more central element of your decentralization roadmaps remember sequencer equals proposal plus Builder don't ignore the last part we need to decentralize both so come reach uh reach out to us to learn more about Suave and come to our Forum collective.flashboards.net um that's it from me um thank you very much and um yeah I'm happy to to take any questions thank you uh we're running a couple minutes over so we're not gonna actually have any time for questions on this one um but we are going to be moving over this transitions well into our next talk we'll be bringing on Davide Barnaby and Justin to be talking about economics meets web economics and we'll be starting with the presentation hello can you hear us yeah um so um we are kind of um gonna have like a fireside chat discussion on delay of the land more broadly like we start with like um a brief um Pitch introduction from me then Justin then Barnaby and then probably gonna have like a longer q a uh so I can go to next slide and I'm going to start um at the high level uh defining um uh Mev flows kind of from first principle so if we go to the next slide foreign so essentially um what I want to start is that these Mev flows are actually a property not of the blockchain itself but of the economy that we build on top of blockchain so like the system of value transfers uh which includes also like the economic and financial gains that people play um uh on top uh and so they kind of affect uh the ethereum protocol but also the in trustless infrastructure or hopefully trustless infrastructure on the ethereum protocol and then like any other protocols that user may use um so essentially um when we think about um how can we make ethereum protocol Mev resilient um which is an idea that uh I put a few days ago and we actually need to think about how the whole system can be any resilient right and these properties uh which I call blockchain SEC properties so security equity and cost efficiency like some of them are inherited directly from the blockchain but some of them relate to the broader system right um so essentially like um what are these properties like the first one is economic security which in the context of Mev uh we know that it changes the incentives from validators to behave honestly so we want to build uh incentives that are robust to Mev with like some of the solutions we are developing is essentially to make these incentives uh robust to this uh process that uh arose um and that now it's a constant because people are playing these defy games continuously uh the second property is uh Equity so here we think about Equity both um in um access so essentially like uh people should not be censored but also equity in distribution uh people should um have a fair game and um have a third distribution uh of value like uh comparable to the the externalities they introduce into the system so we want to minimize the losses for all honest user and then the third one is cost efficiency which uh affects a fourth property which is the utility uh of using the blockchain and and here essentially we want unethical user user that are just there to like play the game or extracting rent to have minimal runs so this is kind of uh the high level goal we are trying to achieve and and I'll argue there is like four ingredients to achieve this so the first one is we need to reduce Mev with clever design so essentially um both at the game level so the economic apps that we build uh should be cleverly designed to account for Med and also at the metagame level at the transaction inclusion game also that one should be cleverly designed to account for Mev and that's what most people in this group are working on um then the second step is extraction so like there is a potential MV in the system uh this uh can cause instabilities to incentives so we need an efficient technology for extraction today this is Searchers and Builder um then the third one is uh capture so once the Mev is extracted we need to make sure that uh this is captured and this needs to be captured by trustless protocol it doesn't need to be entirely the ethereum protocol it can be order flow auctions or other protocol uh that are in the broader infrastructure but it does need to be trustless because um otherwise uh there is going to be capture and like um this mechanism will not be credible and then finally any via location is very important because essentially like uh once we capture this nav we essentially need to make sure that um this flows back to the users or essentially maybe we can talk about this a little bit more what should be the goal of allocation of enemy so if we go to next slide so here is a cartoon view of uh the block supply chain and which kind of includes if you started it for a little bit all the elements that we are seeing today uh including the bad elements exclusive order flow vertical integration between Searcher and Builder uh horizontal competition a different player maybe Stronger versus not stronger there could be segmentation um and then finally math take can differ at different layers so if you look at this you realize that although we would want a clean supply chain we actually have a supply network where like there is several connections across layers so we kind of need to be aware of this um and uh and essentially if we go to next slide uh What uh we started doing is like looking closer uh into all the important metrics of the supply network we launched a rig open problem um which is uh essentially a way to advance research around like more complex properties of this supply network so essentially like what you are going to try to do is create an open source framework to both Define and measure these most more complex metrics that are related to inclusion delays take rates for different uh segments of the market validator the deviations from honest behavior and uh our goal is to bring together multiple parties in the ecosystem we already have people from flashbots block native again Phi units for clubs that are looking with us at uh what's happening concretely into the market in order to then use this data to like design better protocols and systems and this is all for me I believe uh Justin is up next okay great thanks David so today I want to talk about Mev precedence which I guess I just named today which is what is the the ordering in which um you know Mev to which part is the Mev should flow to like who should have presidents uh on the Mev over over parties and basically um in my mind um we have uh we have two types of residence list we have the the President's List that we want and the the Precedence list that we have and those those are actually different today so in terms of what we want I think is we want to give users precedence over all other types of of participants over the dmev so if they create Mev they should be the ones that capture it through this concept of rebates you know called Mev share and things like that and then my claim is that the the next entity in the the Precedence list should be uh holders the token holders and the way that they receive these these flows is through a burn so we already have uh part one of the burn which is eip1559 you know I think of the base fees as actually part of Mev right it's it's uh it's not fancy or sophisticated mov but it is uh extractable value and it turns out to be extracted by by if holders and there's this other idea called Mev burn which is kind of part two of the burn uh where all the Mev not just one part of it goes to the holder so already from these these two items in the Precedence list kind of the the way that I I see where we want to be moving towards is users capturing all the Mev that they they initiate and then all the rest or almost all the rest going to to to to the holders um and now the reality is that there is a whole you know Mev pipeline or chain or network whatever you want to call it of of um intermediaries and you know these NC series will will capture a little bit of Mev but um they will capture a very small minority and you know in the competitive market these these uh should the the inefficiencies should go should go to zero um and the reason why I ordered um these uh these entities in in the way here is basically the closer you are to the user um the the higher the Precedence you should have so if you're an interface for example you're very very close to the user you you provide value by you know bringing the user onto ethereum and so you should have precedence over the Searcher the Searcher is doing you know complicated stuff in terms of of building the bundles and that's that's maybe more valuable than than the Builder which is just bundle merging and then the relay is doing something even less valuable and then the proposal is just doing almost nothing like the the only thing that they're doing is that they're receiving a few bids and then they're picking the highest bid and they're signing it um they're doing almost almost no work um now it turns out in terms of what we have today for the users I put the the skull emoji here that the users are being uh are being you know not receiving the flows and so the the the the they're not being prioritized in in this precedence list at all um and in position number one what we have is the proposer so the proposer who you know should be in my opinion the very bottom of the President's List is right now at the very top of the Precedence list and that that is something that uh that I think uh needs to be fixed and um one of the the the the the cool things of of of this kind of model is that we have a clean definition of of a toxic Nev and it's basically this idea that um it's Mev that doesn't respect the President's List um or at least the the ideal precedence list so in this case any Mev that should be going to the users but instead is going to the proposer is toxic immunity so you know things like things like front running and and and and sandwiches um and and you can argue that maybe uh proposers you know uh receiving receiving Mev that doesn't originate from the users is also toxic um and I'll I'll talk about that later because it leads to all sorts of um economic distortions that that manipulate the the incentive that we we we've tried to to bake into the protocol as designers um and then in terms of what we have again we have these these more commoditized entities that will receive a small portion of the of the Mev uh in in a competitive market okay now I want to talk about maybe what is the most controversial thing of this this whole diagram which is you know why should holders have precedence over the the the the the the proposes and this goes back to this idea of um these economic distortions that happens when you give the Mev to the proposer and so um the way that we give Nev to the to the whole business through me V burn that won't explain how it works that's for another talk but I'll explain some of the the advantages of it so on the left uh we'll have distortions that happen if you give the dmev to the proposal so the very first one is this idea of reworks short-term Rio so if you have a very valuable block 100 if a thousand Eve ten thousand if a hundred thousand if who knows you know in a single in a single block then really you're you're putting a lot of pressure on the chain to do these short term reorgs and we haven't seen that right you know yet but it it it might happen uh you know with as the market becomes more more sophisticated and more mature so that's the first thing that is not good when you give the Mev to The Proposal um the second thing which is not great when you give Mev to the proposer is that you're basically introducing a lottery all the proposers are Lottery participants that you have you have lottery tickets and as in you know most lotteries uh almost everyone is a loser right almost everyone doesn't win the lottery and then there's like the one person the one proposer who wins the jackpot at the expense of everyone else and in order to correct for this fact that almost everyone is a proposed is a loser the proposals need to pool and that creates you know centralization vectors it goes against solo validating and things like that so that's the second economic Distortion that we would avoid if we were giving the Mev to the holders as opposed to the proposals um the the third economic Distortion is is what I call Rock pools so it's pooling is rugging the pools um and here the the observation is that even if you have a pooling system it it it's still not incentive aligned and the reason is that um the the the operator of a of a validator that's part of a pool has an incentive to to basically not give the the the Mev to the pool if the Mev is large enough specifically is larger than their collateral or their reputation so there's a massive block that comes in ten thousand each block and you know they only operate one validator with 32 if you know they're happy to to say okay the pool you can keep the 32 if as my as my collateral but I'm gonna run away I'm gonna rock pool uh with the 10 000 if and so um if you're even if you like the concept of pools you know like like rocket pool or Lido um Mev burn actually makes these pools uh better because it removes the the rug pooling and then the the the the the the fourth uh economic Distortion that I want to highlight is this idea of drought so there's there's two use cases for if right there's the the staking which is economic security and then there's this other very important use case which is economic bandwidth so for example we're going to need decentralized stable coins that are backed by pristine collateral by if and if we're in a in a bull market let's say where you know Mev has just gone crazy then what's going to happen is that the the the the the incentive to become a Staker are going to grow and all the if is going to be sucked into staking and then that's going to dry up the the if for the economic bandwidth and it's basically going to to fight the ethereum economy um so in a way the the chain is kind of too successful and it's it's overpaying for security and that leads to less economic um bandwidth for for the rest of the ecosystem and then another thing that that's not on this slide but I I just kind of remembered is that if you're giving the the Mev to the proposes you're kind of incentivizing people to use um derivatives as as collateral and my my personal belief is that if derivatives um are are risky to use as collateral because they can lose all their value from one day to another through slashing and so they're not high grade collateral and ideal ideally they should be avoided if you're building things like decentralized stable coins and in terms of the the good things if you give the Mev to the holder you have basically more scarcity you know you you increase the shelling point that if is is money for the internet and that ethereum is the settlement layer for the inside of value um but ironically you you're also increasing the the rewards uh for for stakers even though you're removing the Mev portion and um the reason is that the the rewards are going to tend to the cost of value in either case but for most people you know because of this uh this variance around the the the reward most people are going to be earning less than the cost of value so actually most people are going to be losing money staking and a few people are going to be making money um uh staking so that makes everyone earn kind of the same the the same rewards and also from a USD denominated perspective um you know presumably if there's more scarcity the price of ethis is worth more and so you actually have more USD denominated Rewards even though the if denominator rewards are the same you also have more economic efficiency in the sense that you don't need as much uh issuance to pay for Economic Security and the the reason is that the amount of issuance grows uh with uh the the total amount of stakers and so we can we can reduce the amount of stakers and and therefore have less assurance and still have enough Economic Security to for ethereum and then the final thing is that it's kind of related to the scarcity um you know even though the total amount of if staked uh you know will go down it's possible that counter-intuitively you'll have more Economic Security and the reason is that again the price of if might be worth more and so if I'm a USD denominated standpoint you might have more Economic Security so all of this to kind of to justify this this new precedence list that I think we should be uh targeting for when we uh redirect Mev to various participants and now on to Barnaby I believe thank you Justin uh hi everyone I want to talk a bit about uh protocol credibility and principle agent problems um I'm told that there's an echo maybe okay so can you move on to the next slide please in October I released a post on embeddling PBS with something I call Pepsi protocol and first proposal commitment it allows the proposal to basically commit to any kind of executable contract and the commitment is fully defended by the protocol in the sense that block validity depends on the satisfaction of The Proposal commitment in particular you can write PBS in in Pepsi but you can also do much more than that so it's it's much more General and Pepsi came from a feeling that I was somewhat unsatisfied let's say I thought we don't really have a theory of protocol upgrades and so we might as well give ourselves as much optionality as we can and Pepsi is really maximum optionality in terms of what the proposal can commit to that's not to say that we shouldn't do protocol upgrades I think we have a good intuition for them we've done them in the past and they worked pretty successfully and there's something to be said for being iterative Community Driven this kind of diffuse pragmatic approach to to protocol development but the Arc of ethereum is the Arc of any complexifying economy so processes get most sophisticated you have a division of labor that we call modularization in in blockchain slang if you can move on to the next slide Justin and so this division of labor it fractures basically trust domains it it more and more things get delegated you have delegation and if you have delegation you have principal agent problems with paps and if you have principal agent problems you have potentially misaligned incentives and so we can always choose to protocolize systemic paps principle agent problems but then we have to Define what the system is and in this conversation I think we often led to ask what is the role of a protocol in that larger system of protocol and infrastructure and I don't think we can have this conversation properly without having a bit more theoretical background on on basically protocol credibility and so with this question on credibility is really taking off at the moment along with commitments to me the groundbreaking work is Virgil's ethereum is game changing technology literally but recently Sheen from flashbots has revived his conversation he has a much more holistic theory of credibility and commitment and I've been in parallel very much in the Zeitgeist you're starting to see much more results on credibility in auctions and credibility in mechanism design where you don't have let's say a central controller and so what this conversation tells me is that there is this hierarchy of commitments and the reason why we want to put things in protocol is because we see that as being more credible and so I want to just go a bit into why we might think that in and basically because the protocol gives us a shelling fence the validator set says anything we put behind the fence and if we put in the protocol is going to be defended with a whole might of a validator set but the protocol can only defend what it sees so most of the upgrades that we do we're really done with the aim of extending the vision of the of the protocol and I call it seeing like a protocol we want to make legible certain outcomes and certain pieces of infrastructure so that the protocol can control and defend them and so VA means to get a credible signal for instance if we see a safety fault in the FFG Gadget if we have a base B these are all credible signals of things that are happening in the protocol next slide please and so this protocol credibility to me is the sum of two things is this first protocol introspection and also protocol agency so the more introspection we have the more control and the more we can have actionable responses to events that are happening so sometimes we don't want to go through the diffuse process of community government if there's a fault that we can't control for instance people are driving base B to zero it's not something that the protocol can see as a fault and we need to act as a community to to defend against that but there are faults that we can deal with pretty much automatically like slashing people who are finalizing and doing safety Folds but the trade-off here is that when we build in this protocol credibility when we extend the boundary of a protocol it means that we also need to become opinionated about outcomes that we want and so there is a higher risk that we log in kind of local Optima or mechanisms that are sub-optimal but more importantly when we do that we also start overloading the validator set with the duty to defend more and more things and we can do so but then we must be convinced that we as a community are going to defend these mechanisms basically with the protocol's life for whatever Force under its zone of controls for instance we commit to forking out validators who subvert the censorship resistance Gadget that we build in the med burning the finality gadgets all of these gadgets and to me this feels like it gets harder as time goes on because there are other domains like Suave or like eigen layer that may directly shift the incentives of a validators such that honest behavior and rational Behavior start deferring more and more especially when the protocol has locked in sub-optimal outcome maximizing because when this is the case the welfare Gap is basically a subsidy for bribing validators into into doing the the wrong thing and so back to Pepsi to mediate it was nice because it creates this idea of programmable protocol credibility but it may be at odds with the goals of zeroing in on the minimal set of gadgets that we are willing to defend with basically the protocol's life and that question is is still open in my mind so yeah I think we're getting closer to his theory of protocol upgrade uh I think commitments credibility both have to do with that and I'm excited to to see what's next I think we'll beat over time but we should jump to the questions awesome great presentations guys uh so we will have a brief kind of question section here before we move on to the next one um the first one I'll go with will be kind of basically getting uh David and Barnaby your reactions a bit on kind of the Meb allocation and distribution type of convo for the most part um I know Davide like you've written previously as far as about allocation the goal is basically distribute the captured Mev to participants in a way that is most beneficial to the protocol um so that last part is really kind of what hones in on the question of how do we Define what is that objective function of what is beneficial to the protocol that we're trying to optimize here um whether that be smoothing burning um in particular because website does address some of those points um that Justin had spoken about as far as the rear resistance and some of that kind of stuff um so like how you guys kind of think about what you're optimizing for here yeah good question so I think the answer is nuance and it ties to like uh parts of the presentation of Justin and Barnaby as well so maybe I'll start and then they can continue so like in the ideal world like what doesn't most beneficial for the protocol means I mean the protocol is just like organizing all the participants like users and like service providers validators in this case right so like on the user side like the ideal allocation is essentially like a fair tax so essentially like the Mev you contribute to the system and the Mev you get allocated like if you subtract those they should be proportional to the externality that you bring to the on the other side you also need to use like part of this Mev to make sure that like the validator incentives are not uh distorted behaving honestly so I think this ties to like uh the kind of Precedence hierarchy that Justin was talking about and yeah so I'm not sure if he wants to comment next also because it doesn't need the allocation doesn't need to be monetary could be purely fiscal but it could also be monetary in the case of burn so what do you mean by fiscal in the sense uh it's a trans you could you could organize direct transfers right for example in the guys in the kind in the type of um allocation that an OFA system would do uh there is a direct transfer back to the user so essentially the user is getting that Mev directly not through um inflation or deflation deflation in this case yeah right I mean I you know there's a separation of the consensus layer and the execution layer right and like I guess what you're saying is that the execution layer is the fiscal layer and I mean I don't think users would would get issuance right that's kind of the wrong layer because insurance is at the consensus player right okay yeah I mean Johnny made a good point around moving you know potentially solving some of some of these issues um smoothing doesn't solve the the economic bandwidth drought basically it's just the staking sucking um yeah and it's also unclear in terms of the the Rio even the Rio like one of the ideas is that we have some sort of a small committee that is in charge of the the smoothing but the small committee is maybe not representative of the whole value data sets or maybe it is I don't know um but um there's there's some Nuance there a little bit um yeah on that point if I can comment I think sometimes we also get the monetary policy that we can defend and if we can't defend things like net burn or things like Maps moving because they rely at the end of the day on this on this majority of committee to to defend these things um yeah it might be something that we we can't get but um I wanted to also mention some research that is done by Anders in our group on the dynamic yield curve and notably the idea of capping the size of a validator set and potentially even auctioning of there's lots of validators and that's I would say another way of internalizing that Mev allocation in the protocol and in particular if you get rewards from like eigen layer as a validator you're going to be willing to pay more at this option and so you can even internalize more than the execution layer Mev via this mechanism or you could do both I think that would be the ideal I mean one of the things that you mentioned is that any of the burn was moving requires an honest majority I I do want to highlight that eip1559 also requires Anonymous majority um and just for people to to understand this basically if you have a if you have a dishonest majority um they control the full Choice Rule and then what they can do is that they can Fork any any block that increases the base fee and so basically the basically is like decreasing only and it goes to zero and then you've just essentially deactivated the ip1559 and actually there's a lot of parallels between the ip1559 and Mev brand is just essentially the same mechanisms the same Dynamics it's just on the different side of the coin very good uh so moving on to one of the other questions that we had uh this one will be for you Justin it relates to several of the earlier presentations that referenced it um but your recent post on based Roll-Ups where the basic idea is letting the layer one kind of do the sequencing for these Roll-Ups potentially um and tldr a very simple Mev implication here is if you implement it in that simple form um basically the Mev that would normally accrue to roll up sequencers would start going to the L1 itself um so the question here is kind of why is that such a desirable or undesirable feature of this and then in particular is there kind of a tension here between now all the value goes to ethereum and maybe this is kind of the dominant solution to them and compared to roll up should have their own incentives to continue to innovate and continue to grow over time is that kind of in any way a rejection of like the roll-up Center of roadmap where we want this private Market of people to be incentivized to go make these things right I mean on the on the topic of incentivization I'm not at all worried about the rolled up teams right there's like 10 Billion Dollar Plus kind of market caps I think they're doing very very well and you know for good reason because um you know they're doing something very very complicated that requires you know very big teams it requires taking a lot of risk um and and also the the fact that we have these tokens is kind of fuel to kind of almost disrupt a little bit the layer one network effects and create new network effects at Layer Two so um I'm I'm I'm all for like this uh you know this capitalism as it were at the roller player uh but I think what will happen is that you know what is capitalism in the short and medium term might become you know commoditized eventually it will be just like running water and electricity um and you know there will be less and less of a need for for incentivization um I mean even the roll-up teams themselves right they want to be credibly neutral they want to be maximally decentralized and so they go for example from centralized sequencer to decentralized sequencer they go from governance you know token governance for upgrades and then you know they might remove the governance and become like uni swap you know fully trustless um and I think you know the move from being layer 2 sequence to being layer one sequence is part of this natural progression of of progressive decentralization and then once you've reached the the base roll up you know the the ultimate maybe goal uh is to become an enshrined roll-up and um you know I'm I'm talking to the to the scroll Founders and you know that that's one of the things that they're thinking about like how could scroll be coming and try and roll up so there's kind of this the spectrum of of Roll-Ups um and by the way my definition of a roll up is you know when there is sufficient data on chain that you can recompute this the the state and so you know some of the rollups that people don't talk about is centralized rollups so for example optimism today is a centralized roll up it's a rollout because you can recompute the states but it's it's centralized and I think another type of roll-up that people uh um you know not looking into but I'm I'm relatively optimistic about this sgx rollups um where the security engine is is sgx but I think the what we what we're doing is we're progressively fleshing out this this whole design space of of roll ups with and with rediscovering you know and try and based blah blah blah um in terms of the advantages I think you know they really do lean highly towards credible neutrality right so it's like maximum security maximum liveness maximum decentralization maximum Simplicity and so all of these things are you know tending towards the the mature side of the road so I what I expect will happen is that um you know the dominant roll ups in the short and medium term will be the non-based one and then eventually you know maybe a a based one and uh actually one of the ideas that we that we've been thinking within the ultrasound team is like should we launch an ultrasound roll up and if we were to do such a thing it necessarily must be you know a base roll up because that's kind of like the the natural shelling point um now one of the things that I think will happen with with Roll-Ups is that the it's kind of winner take most because of the network effects um and so you know Roll-Ups need to be very strategic right on the on the one hand they need to have a you know they need to move extremely fast have the first move Advantage have a lot of of token fuel to do the boosting but at the same time they need to have this credible story that eventually you know they'll become you know maximally incredibly neutral and I think uh and adding becoming a base roll up as one of the line items in the roadmap um could be could be a very powerful strategic move uh for for for them and you know one of the things that was uh you know highlighted about baseball Labs is that it's a little unclear how you get some of the services like pre-confirmations um but I'm actually uh you know relatively optimistic that uh you know with things like eigenlayer uh that we we can actually have uh fast bit confirmation so we can you know have our cake and eat it to have like all the advantages of non-based roll apps and all the advantages of baseball Labs um so yeah the future is a price for rollaps for sure foreign options um I think we have we'll squeeze it in one more since we started this a few minutes late um this one kind of gets to the question of what is really the protocol's job to kind of provide um to everyone who's kind of using this ecosystem um so it's a question that I had actually asked Phil earlier as well um so curious to hear your guys take um it's regarding and trying PBS um and is this something that we actually need to do um is this something that actually needs to be enshrined in the protocol at any point um it's clearly not a near-term action item that's happening imminently um but there is that real risk of you and trying something a very specific structure and crypto has a strong tendency of finding new existential risks every couple of years and you might find a new one of those a couple of years later and realize you wanted more flexibility um so what are the downsides if we were not to do that and kind of how do you think about that um starting with barnabay on this one sure um yeah I would feel sensor was quite interesting to say that uh basically our job is to do research on providing nuclear options or almost like deterrence from for people to to not do bad things and for the market to stay clear um I actually I'm pretty hopeful about let's say the development of PBS Justin can probably talk more about this but lately there's been like a lot of innovation like optimistic reeling towards this roadmap to PBS so to me the risk was not to say that okay PBS is a bad idea clearly I think you will want this clean interface of a protocol to communicate with what's outside of it I think PBS is is probably part of the right one but I did want to see more data more experimentation more research and it feels like we're at this point where okay every day there's like a new paper not necessarily about DBS but that can be applied to PBS or new data sets new new findings uh and then yeah but that makes me fairly confident about figuring out that nuclear option like the shape of it before we actually need to to use it yeah I mean my take is that um you know with Android PBS is going to take a long time you know relative to how fast the blockchain space you know moves it will take let's say two three years at the minimum and in in two three years the Mev space is just gonna dramatically change like every week you know the mov space dramatically changes we're just speed running the whole thing and I'm hopeful that we'll have some sort of reasonable equilibrium and let's say one one and a half years or two years and then that will be you know close to the end game of you know the non-ins trying PBS and then we'll be able to make an informed decision as to whether or not we really want and trying PBS but intuitively as a roll-up operator uh I'd say sorry as a Relay operator I'd say we we really do want and trying PPS like being a Relay operator is not it's not good like um on the one hand you know this is kind of a lot of work uh and you know there's a lot of pitfalls but one of the things that I've been thinking about is okay you know how can the role of the relay be abused like what happens if someone hacks our relay and I believe that if someone hacks a relay they can steal millions of dollars so basically it's it's it's it's a high kind of trust environment you know you could do it with the builders you could do it with the validators there's all sorts of attacks many of which are not documented and uh you know really you just want to remove this this distrusted entity uh as quickly as possible um but yeah it is possible that there's some very very clever technique that allows us to remove the trusted entity without entering PBS and I think flashbots for example is working on using sgx you know to remove the relay and I think you know as well not be mentioned there's this whole roadmap that we have now that's you know been written by Mike noodle basically documenting um you know a progression for optimistic relaying which incrementally brings us towards and trying PBS and I I yeah I encourage you to read it because it it it kind of shows you almost how natural the the enshrine PBS uh should be in uh in in a few years awesome this was a lot of fun I don't want to keep uh Patrick waiting any longer um so thank you again all of you guys this one was really really great um and we will now transition on to the next talk uh we're gonna have Pat McCrory coming on um and he's going to be talking about a closer look at the sequencer's role in a meeting good morning everyone says trying to all mute myself I was enjoying that by the way I was happy to be delayed that was great um okay I'll just get started then so today what I'm going to talk about is layers of layers of layers you know hopefully taking inspiration from layer one on a planet to Layer Two and what we're mostly going to look at is the sequencer and how we have we can think about muv around the sequencer suppose we know you know just a bit of background in Mev land there's typically three agents that we care about one is the honest user all the honest user wants to do is buy and sell their boom cap we have a proposer on the proposer's job is to take a list of pending transactions and decide the final ordering of those transactions and third we have the Searcher you know those Mev Bots those people living in their bedrooms where they're looking for these Mev opportunities they bundle it up then they pay the proposer to include this bundle in the list of transactions and the Order of the transactions so does Mev land in a nutshell so let's begin with ethereum mean that and let's look at the life cycle of a transaction and let's see what we can extract from this scenario so we have a user Alice again who wants to buy and sell mooncats and we have the proposers so the question is how does the user get their transaction and communicate it to the proposer so we have to consider the communication Channel by default we will use a gossip protocol or the peer-to-peer Network you know all this will send a transaction to appear they'll take the transaction and pass it on to their peers and eventually within one to two seconds every appear in the network will get a copy of this transaction including the proposer they'll take this transaction and hopefully include it in their block now the issue is that it's a peer-to-peer Network so anyone could be on it including a Searcher so they could listen out for the user's transaction inspect it find an OP you know Mev opportunity and then front run the user and steal the profit so it's a bit like a dark forest in a way and there could be you know not just one Searcher for many Searchers that are all competing for exactly the same you know Mev opportunity the CM profit and this leads to something called priority gas auctions which I'm sure people have spoke about already where the Searchers will bid each other up you know transaction fee one two three four five six and as up to the proposer to pick the transaction with the largest fee that pays them the most money and decide who wins the auction now I stole this from a blog post by tobler soda.eth uh I probably you know didn't butcher that name a little bit but you can see here in the in the graph within a 10 second period there's basically like hundreds of transactions being sent then eventually Mr Blue wins and he wins the auction he gets uh gets the MAV opportunity now there's two problems of this approach one is wasteful of gas you have one transaction that's successful followed by a list of field transactions and that's a waste of block space and two it's unrestricted Mev you know you're taking the user's transaction throwing it to the wolves I'm just hoping it gets to the other side okay because it's completely unrestricted what these Bots can do so a few years ago a wonderful company called flashback you know I wonder who they are they came along and tried to solve the problem for ethereum you know we give the transaction directly to the flashbots and they'll give it directly on to the block proposer so then the mbv Bots or the Searchers they can't find the opportunity and they can't you know extract value from the user's transaction so stepping back a bit you know what can we extract from this scenario one we have to consider the proposer's ordering policy and this key as they're picking the transactions based on the fee highest fee first lowest fee at the bottom and then around 12 seconds to do this you know as we saw with the priority gas auction what we need to consider as well is the communication Channel how does the block proposer learn about the transaction and how do the Searchers find it as well um finally you know sometimes we forget about the pure little user but we have to think about the user experience you know how long does it take for a user to be informed that their transaction is confirmed on how it was executed because at the end of the day they want to execute and you know participate on this network took some cool ideas from that so now let's move on to Layer Two And Roll Upland and in rule up land there's typically three actors The Honest user the sequencer and the executor now for once we're not going to talk about the executor they have very little to do with Mev we care about the sequencer who's basically the proposer and gets to decide those list of transactions and they're ordering they have everything to do of Med and the the life cycle is pretty similar all this gives their transaction to the sequencer the sequencer will have a list of pending transactions they'll run some ordering policy they'll decide The Ordering of the transactions then they'll post that onto ethereum into an inbox and then picked up by the bridge now in this scenario we have this direct communication Channel there's no gossip protocol all us can communicate directly with the sequencer and what we need to be concerned about is you know when does the uh when does Alice get a response from the sequencer on what type of response do they get and that's really going to depend on how the sequencer decides the order of these transactions then of course what we care about is the sequences ordering policy you know they can keep the transactions private for you know as long as they want decide the ordering and then eventually make that public once they've made their decision so what ordering policies could the sequencer Implement so we're going to go through three the first is has extraction first the second is has fee first and the third is first come first serve so that's just Dive Right In and see what we come up with has extraction first um basically when you talk about sequencers in mbv and Layer Two this is the first ordering policy that everyone talks about you know and the reason is the sequencer has ample time to extract value they could wait you know a minute or hours before they have to publish that transaction publicly so in practice both arbitrary and optimism they typically publish these transactions in about 20 or 30 seconds but according to the smart contracts you know arbitrum can hold it up to something like 24 hours and the optimism I couldn't actually find the value someone could probably drop it in the chat but I'm pretty sure it's like 24 hours they could keep these transactions private forward so if you can hold a transaction you know pending transactions for three hours let's say well you know you have to you have this big basket of transactions you have your Mev extraction algorithm you run it on the batch and then you can order your transactions according to you know the ones that you can extract and boost value from now in terms of user experience there is a benefit to this we can have a Robin Hood style experience the user could transact for free because later on their transactions use the compute some Mev reward so from the user's perspective is a free transaction but they're really paying for it through Mev now the issue with this approach is really the long delay if we allow the sequencer to extract value for two to three hours well that sucks for the user you know think of Bitcoin you have to wait 10 20 minutes and Bitcoin and everyone hits that it's a terrible user experience so I'm also waiting two hours just so someone can go extract value from your transaction you know it's not a great user experience now you could solve this problem you know a lot of people think okay sequencers can extract Mev let's solve this and stop that from happening so they can Implement some Fair ordering protocol maybe the sequencer can't see the transaction content maybe you introduce a consensus protocol what does fairy mean in this context I don't really care it's not important for this talk I think tarun's going to talk about it later it's going to be pretty spicy but for now I'm going to argue differently I'm going to argue we don't need to solve the problem because if you look at proven stick ethereum today you have this open market of Searchers You Know PBS and you have the stickers you're basically competing a lottery you know the Searchers do all the hard work they extract the muv they pay a bribe to the proposer the proposer gets a transaction fee then they include the bundle and so there's a very good chance that a sequencer can make more money by having an open market of Searchers do the hard work as opposed to trying to extract the Mev themselves and I've asked the key is then their financial incentive is not to extract the value but allow someone else to do it so we don't have to worry about the fair ordering problem for now and I've asked the key is well that means the sequencer's ordering policy is the order of transactions by transaction fee whoever pays the has fee will get ordered first in the list and so you know this is basically payment for order flow maybe some Mev people hit me for saying that but it's basically that anyway you know the sequencer will get the list of pending transactions and give it to the Searchers The Searchers can Crunch and crunch and crunch extract as much nvv as they want and then eventually send a bundle to the sequencer with a payment and they'll say sequencer if you include my bundle you make this amount of money then the sequencer takes that bundle they take the payment and of course they order it according to according to the payments they receive very much similar to what happens on ethereum today you know you're sharing the Mev between the sequencer and the Searchers and again user experience not very different to the previous case you know users could still have free transactions you know because the the transaction fee is actually the Mev that's extracted but again this could have a long delay you know users don't want to and at least I don't want to build a layer two where users are waiting for two to three hours for their transaction to be confirmed so this is why most rule apps Implement first come first serve because they want to prioritize the user experience what do I mean by this well the user will send their transaction to the sequencer the sequencer will timestamp this and then simply order the transactions according to the timestamp and this has a wonderful user experience it's a bit like transacting on coinbase you know you send your transaction to the service provider and they return back in response to say it's confirmed this is how it's executed you know under a second typically when people talk about first come first serve they say oh well there's no Mev here you know hiding under the covers well well of course of course there's Mev there's Mev everywhere uh you'll never get rid of muv um so anyway how do we have Mev so what happens in practice at least with arbitrim and I assume is the CM for optimism when a user gives their transaction to the sequencer the sequencer will create a little block within 250 milliseconds and then publish that off at a feed now the reason they have a sequencer feed is because external service providers like etherscan coinbius or infuria they could pick up the block and present it to the user so a user could send the transaction to the sequencer then look at ether scan and say oh my transactions are confirmed according to etherscond now what happens when you have a data feed that's released in data about transactions in real time sergers connect to it they listen for the transaction if there's an Mev opportunity they'll background it and send it immediately to the to the sequencer and this leads to the rise of latency games because you don't end up with one Searcher you end up with 150 000 web socket connections because whoever gets the data first he ever gets that transaction data first is the one who wins the Mev opportunity and can back run the user and you know this is in theory this happened the arbitrim and I guess it's probably still happening I don't know you know they have this post where they had a hundred thousand to 150 000 connections because the Mev Bots are competing on a latency game to extract Med on these background transactions the short-term solution proposed was to implement Oshkosh so you create 50 dedicated connections the Searchers do proof of work you ever asked the Lewis nuns wins one of those connections and so then you know the Searchers game is to have do proof of work as opposed to creating the thousands and thousands of web connectors or websocket connections now that is a solution but I think there's a more interesting solution that comes out of this and sort of motivated by Ed Felton's proposal which we'll talk about in a second what did you combine first come first serve with the hatsby first what do I mean by this so basically similar to before the user gives her transaction to the sequencer the sequencer May collect one or two transactions and then pass it on to the Searchers The Searchers will take the small bundle extract response Mev as they can participate in an auction and then submit their bundle with a payment to the sequencer the sequencer will take whatever bundo pays the most money and then confirm that now the point here is that this bundle auction is configurable the auction window could be really short it could be 500 milliseconds 2 milliseconds five five seconds whatever you know a really short period of time and the sequencer can constrain the Mev the sequencer could say oh I'll only let you front run transactions or background transactions or sandwich transactions are all of the above so what's cool about this is one if you have these short rapid auctions then we're gonna fast confirmation for users transactions you know I submit my transaction within three seconds I'm told that it's confirmed I made a front runner background runs sure but my transaction was confirmed two it was open market for Searchers admittedly as smaller bundles that they're working with so you are constraining you know what extraction they can do but again you still have this open market where they can probably find the best extraction finally is configurable you know the sequencer can decide what mevs allowed as opposed to having this crazy Dark Forest that's pretty unconstrained now this is sort of the idea coming out of I don't want to say sort it's basically the idea coming out of Ed Felton's post about time boost here I basically described it as having a queue you can immediately front run or background or of course soundwatch in this proposal you could convince the sequencer you know with a payment to say well here's a transaction ordered 500 milliseconds in the past and then you could you know depending on the timestamps of the transactions you go front runner background so you're paying for time as opposed to a position in the queue but my main point is this is a really cool research question a really cool open problem and it's sure to get the best of both worlds fast confirmations but still having this open market for Searchers and so I guess that's my last slide I guess I'm just within time but I just want to present this idea because I've been thinking about it a lot I think it's a really cool idea so yeah GG or GM I guess GM awesome thank you so much it was a great talk um we will now be moving on to our next one we're gonna have John Adler come on and he's going to be talking about meconomics uh for modular blockchain steps hi can you guys hear me yep we got you you're good to go awesome okay so let's get started uh so today I guess I'll be talking about Mev economics uh for modular blockchain stocks uh and I'm going to start with what appears to be blockchain 101 uh but if you bear with me it'll become obvious why we kind of build intuitions by construction starting from the beginner uh so what are some components of any blockchain protocol not just a modular blockchain one it has a leader selection algorithm uh and these are some of these things uh will you know will you'll have heard of them from previous talks uh especially the ones that we're going to Deep dive in today uh so this is basically another one that tells you uh who is permitted to actually produce new blocks physical blockchain that can't account of new blocks added to it isn't particularly useful uh then tying into this you need a simple resistance mechanism uh uh in any kind of decentralized blockchain you need some decentralized permissionless blockchain you need some mechanism to prevent someone from just spamming a bunch of potential candidate leaders uh and that's going to be the Civil resistance mechanism so things like proof of work groups like Etc then you have a block validity function or a state transition function uh effectively the rules around the validity of the execution of a block independently of who got to produce it and then finally uh to wrap all of these up together you have a fortress rule which uh which allows you well by you I mean the nodes the distinguish between two otherwise valid chains so two chains that have the correct leader that has sufficient civil resistance like that amounts whatever you want to call it and the blocks are all valid uh you need some way of distinguishing between these two otherwise completely valid Chinese all right and we're going to focus primarily on the first and the fourth fourth one of these uh because things like block validity yes the specifics of the blockability function determine what Mev may be extractable for instance Bitcoin may have less extractable Mev than for instance that you're in complete blockchain like ethereum but given that just a few days ago we saw reorg on bitcoin because of an entertainment maybe that's that's not actually the case anymore uh the leader selection and the Fortress ruler definitely things that come into play uh with Meb so uh this is the only slide that will contain theorems and crawler is uh so please bear with me but the first thing around is that in any decentralized blockchain it's required that leader cannot hold the chain on their own uh statistically of course you know if we're talking about proof of work uh if one person gets really really lucky they can just produce every block forever uh or censor the whole chain Etc uh why why do we want this to be the case well because of one party acting alone can completely and permanently halt the chain uh then they effectively have the ability to destroy it and if they can destroy it then they can they it's isomorphic to having control over it uh as a coach from doing I would say uh uh now uh corollary to this is that if you're given some some if a actor is given access to sufficient civilizer sufficient of like the simple resistance mechanism amount uh they should be able to become a leader for this one block in some finite time again statistically uh and as long as this is true then you know the system is permissionless so decentralized permissionless is all it's all very good the general way this is done is I mean this leaderless protocols like Avalanche but you know here we're not going to into too many specifics the general way it is done is that if the assigned leader doesn't act within some finite time bound then another entity becomes a leader okay so why this important uh the reason it's important is that the leader can capture Mev either directly or indirectly uh through things like Arbitrage funds running sandwich attacks Etc now generally it's hard to manipulate who's going to be the leader because the leader is tied into a civil resistance mechanism for instance proof of work you can't just like you know show up and say hey I'm the leader uh you kind of have to do a whole bunch of work similarly improve aesthetic you know various proof of stake protocols like tender bench or gastro whatever have pre-assigned leaders you can't really manipulate the leader in any meaningful or cheap way but there's all the things you can manipulate which is going to be the Fortress rule which works in tandem with the leader selection algorithm and it could be easier to manipulate the four Choice rule than it is to manipulate the leader selection algorithm and manipulation of this could lead to things like time-banded attacks Etc and of course you know once once you've rewarded the chain then you're also the leader of this block so then you can also engage in front running etc etc within a single block or a sequence of or sequence of blocks uh now these may not be that much of an issue in an L1 context but we'll soon see why there's such a concern and why there's so much so much more flexible in a modular context so what does the modular world uh we're moving towards a multi-d chain and specifically modular world where you have shared data layers that provide a very high amount of data bandwidth such as Celestia or these two I guess ethereum nowadays uh and then a number of execution layers or roll up Sovereign optimism start where scroll Austria just to name a few there's there's countless of these already in development many already deployed et cetera now uh in this multi-chained world or rather if we're really if we're really being General this multi-domain world uh a lot of the current concern is around cross domain or horizontal as I like to call it Mev or you have MAV that comes from Arbitrage and other opportunities within two different execution layers within two different Roll-Ups or any domains really uh but and this is obviously you know a very important problem Etc but it's not going to be the topic of today's talk or to I didn't talk I guess is 15 minutes we're going to be talking about and exploring what I would call vertical Med in the modular blockchain stack which is the med relationship between the rollup and the data layer and this could of course extend to multiple Roll-Ups on top of the roll up Etc et cetera but this is like a vertical relationship instead of a horizontal relationship uh and now you might be looking at this and saying well there shouldn't be any relationship between these two because you know there's two execution layers trades are being done on one that might create Arbitrage opportunities on the other you know you can atomically execute transactions on each etc etc uh but if you have an execution layer and a data layer the data layer itself doesn't have any Arbitrage opportunities because it doesn't execute anything so like how why would there be a relationship between these two and this is why this is an interesting problem because it's not cross it's not cross domain Mev uh in the same way that this horizontal MEP is it's very different uh and for this we're going to keep in the back of our minds the tongue-in-cheek first law of crypto dynamics that Mev cannot be added or removed it can only be moved from one layer to another uh or put another way maybe it can't be you know destroyed it can only be changed from moved around from one layer to another this is of course not actually like a formula it's just a tongue-in-cheek observation uh when dealing with vertical Meb specifically not cross-domain and maybe so uh when it comes to building a execution layer building a roll up on top of a data layer you need to have some mechanism just like an end blockchain to produce new blocks selection algorithm right uh but you would also like to leverage the data layer for security because if you don't leverage it for security then you're kind of losing half of what you get out of the modular blockchain right you get you can get the ordering and you also get availability uh be nice if you can you know use use the blockchain also for ordering uh because if you only use it for availability now you have like these synchronization issues between you know one block change consensus and another blockchain's consensus you would like ultimately all consensus to be done ultimately only on the base data layer and have everything else use some mechanism of merge consensus instead of their own completely independent consensus protocols because then if they use a completely independent consensus protocol you'd have synchronization issues so what do you want out of a leader selection algorithm for one of these Roll-Ups on top of the data layer or you would like to decentralize the sequencers the views of one sequencer you know the censorship concerns there's potential for mbb Monopoly and Meb extraction regulatory concerns Etc also decentralizing sequencers is a very convenient and easy way to introduce a token right just have a token some state-weighted mechanism round robin you know leader selection just like tenderbend has uh it's a great way of introducing that uh what else you want do you want strong labus guarantees you don't want it so that a single sequencer going down for instance means that the roll-up goes down forever uh you'd also like to minimize trust assumptions here uh you don't want the trust that a single party is operating or you know small number of parties are operating correctly for the chain to operate correctly uh and also you want to minimize waste of the data layers block space and waste can happen if there's some sort of contention Etc where two transactions uh show up and one of them becomes useless and it just wastes the data layers block space you you like to avoid that uh you what you don't want though is you don't want the nav to bleed down to the data layer and I gave a talk at I think that some maybe MAV Summit back in back back a while ago quite a few months ago it's linked in the last slide of this so if you're if you have access to the slides it's a the YouTube link on the last slide here uh that kind of Dives deeper into this so I won't dive too too deep but the intuition is that depending on the leader selection algorithm you can have Mev bleeding down to the base layer and why do you not want that because uh now you have a situation where you know if you have a bunch of Meb bleeding down to the data layer you have all of this Mev to be captured that instead of being isolated to the individual Roll-Ups is all going down to the data layer and all that mab to be extracted means there's some centralization pressure on the validators of the data layer to actually extract that MEP and we would prefer that made be isolated as much as possible to the individual execution layers that create the Meb now there's a problem here and this is somewhat of a I don't want to say open resource problem but rather still an open challenge uh that we would definitely if you're an Meb researcher or et cetera if you're interested in uh modular blockchains uh please absolutely reach out to us uh so we can you know brainstorm on on this problem one thing we can't guarantee is that Roll-Ups don't necessarily care and aren't necessarily incentive aligned with the best interest of the data layer they have their own interests their own economic interest Etc uh and they may just do something bad for for the data there we can we can't exactly guarantee that they won't do that okay uh so let's maybe go over a few uh examples of potential leader selection algorithms slash four Choice rules that you could have for one of these roll ups on top of a data layer uh one of them is first come first serve uh so the intuition here is basically a bunch of people just post blobs that are that have a particular namespace uh or that have a you know let's say this blob is for this particular role basically when that's what I mean by namespace a bunch of people who post blobs completely permissionless permissionlessly the first person who posts The Blob to this last year blog for that namespace is the one that is decided is selected to be the leader exposed facto uh what else am I choose here uh it means that the leader is ultimately decided by an auction on the base layer uh and auctions as as we know in the blockchain context uh well you either have to trust the auctioneer or if you know the auctioneer is permissionless uh then options can be manipulated but I guess options can be manipulated at night okay so just a trust assumption rather than a permissionless assumption uh options can be manipulated so that's not great so this has the issue that it introduces uh PJ uh for who has to be the leader and therefore bleeds the Mev that comes from that auction down to the data layer on top of that it also wastes a bunch of space uh block space because all these other transactions that aren't the first still go on train they just don't do anything for the application so they're useless like they pay fees yes but it means now more blog space is being used without providing value to end users uh another example uh tenderbend style leader rotation this one is fairly straightforward is basically you take just the leader selection protocol tendermint not like the whole Quorum and the voting and rounds and all like the P2P stuff just the leader selection protocol which is like I don't know 20 lines of code it's very it's very straightforward uh and then based only on the state within the roll up so you don't use anything about the ordering it's last year or anything of that or the data layer rather more generally use only the state in the rollout uh and the Fortress rule is Trivial because well temperaments work for you now those issues here which is that as a safety rather than liveness performing protocol this kind of scheme makes it a bit more challenging to have good liveness because as if you use tender mint on an L1 you can increment rounds very quickly but if you're on a roll-up potentially you have to wait one or more data layer blocks uh in order to say okay this this the leader for the slot didn't actually post their block therefore we will you know up increment around and rotate the leader permissionlessly uh another one is highest Priority First this was a proposal that was floating around uh just a few days ago it would require a protocol change to Celestia or the data layer it's basically the intuition is introducing a field a priority field that can be malluated by the blood producers the data layer block producers uh and that serves to order the blobs instead of just the fees for instance or the position in the block uh now the issue is that it actually degenerates to being isomorphic to first come first serve except the auction instead of being done on chain is done out of band and opaque which may be even worse uh last example before I close it off uh is base Roll-Ups our base Roll-Ups bass are cringed or cringe uh so the intuition here is base Roll-Ups are based on uh work that I did uh minimal viable merge consensus from 2019 so like basically four years ago almost uh combined with Meb boost or some PBS time then maybe the Boost is used to prevent waste of course you introduce additional trust assumptions since we know Meb boost isn't completely trustless so this is maybe a side grade rather than a strict upgrade over the work that I uh published four years ago uh assumptions what are the issues you still haven't mitigated the use of an auction so select the leader so Mev still lays down okay uh I was asked by Tina to add a slide on how this ties into me Mev economics uh someone will have to come after this and tell me specifically and like concretely and completely MAV economics is but I guess this ties into Mev and the economics of Mev bleeding across a vertical blockchain stock uh a lot of the work ideas thoughts proposals that I listed here were didn't come for me they actually came from members of the Celestia team and others I'll list some of them here non-exhaustively so there's Conor rutual Evan Callum and Gabriel uh they worked very hard on these kind of things if you have access to the slides there's link to their socials uh if you click on the bubbles here so please follow these people on Twitter GitHub or whatever they're very bright and they're the ones who did a lot of the work on a lot of the ideas and proposals here and if you want to do further reading as a tradition uh in the blockchain space there's lots of prior art so I posted a bunch of links here with variety of proposals intuitions counter points Etc okay uh that's about it sorry for being like a minute over we're running a couple behind no worries uh this is significantly better than we're usually running on these so we'll take it uh thank you for the talk so we're going to move on now to James who's going to be speaking on the meth economics of bridging how's it going okay you're on mute there we go there we go thanks John and thanks other John before that I guess uh yeah I'm gonna be talking about uh a specific Mev mitigation within the context of bridging uh I think I was required by Tina to put the word metaconomics in the title of the talk so uh what we're really going to be talking about is using the structure of a bridge and partially ordering messages in order to mitigate Mev in the question context uh so I'm famous I've been working on bridging and Mev since late 2017 early 2018. I've been around the block a few times uh you can find me on Twitter and GitHub uh generally speaking when we talk about a bridge we're talking about two one-way communication channels between blockchains you know from chain a to chain b and a separate system to go back from chain B to chain a General pattern that these work in is chain a will dispatch a message some off-chain actors will pick up that message and deliver it to chain B chain B will do authentication and validation of the message and then deliver it to the application that receives it and handles it so if you put it on a timeline it looks a little bit more like this each of these steps is atomic on chain or takes a very tiny amount of off-chain time and then there's these big big in computer time gaps between the actions uh you know it sometimes takes tens of seconds for a off-chain actor to pick up a transaction from the blockchain or they might wait several minutes for it to finalize in the proof of stake consensus so these like gaps between the origination of a message and its handling by the Austrian actors can be minutes and the gap between the off-chain actors getting the message and its confirmation on the receiving checking can be several minutes as well uh as we probably know Mev is a result of uh you know time gaps between committing to what you want to do and that being executed it's a result of you know people being able to insert reorder or sandwich your transactions after you've committed before their process and so cross chain bridging has a lot of opportunity for Mev to sneak in because there is such a long time Gap from the perspective of each chain so Mev in a sense is about early access to information and cross chain bridging like that information is public so long before it gets acted on on the remote chain so everyone has you know 10 minutes of warning on what this message does and what it's going to do when it hits the remote chain and how to front run that message most effectively so From perspective of the remote chain the receiving chain a message and a txr equivalent the message comes into the chain someone off chain submits it and it kicks off some amount of computation or smart contract running or whatever so a message and a transaction can be included in the block essentially the same as transactions reordering and sandwiching for questioning messages work just the same but again you have 10 minutes advanced warning instead of seconds so Mev pushes The Ordering of a blockchain towards the maximally extractable ordering the ordering in the block which provides the most value to the Searcher Builder proposer supply chain so everything gets shuffled Mev transactions get inserted and the proposer ends up making a bunch of money so what we want to do is mitigate the impact that reordering and sandwiching have on cross-chain messages because cross-chain messages are more vulnerable than the average transaction so we're going to change the message flow a little bit that I talked about earlier rather than immediately dispatching to the uh to the other chain we're going to accumulate on the first chain on the sending chain a batch of messages and this should be a batch of messages that touches the same application at the same state and rather than committing each in each individual message we want to commit to a batch uh so you take all of your cross-chain messages and you put them into a single unit uh and then you dispatch that batch and process the batch on the other end so what does this get us as a mechanism why do this at all uh when you are extracting value from the remote chains block ordering putting it in a batch preserves the relative ordering of those messages it prevents the minor or the Searcher or whoever from extracting value by reordering the batch and it prevents the you know Searcher from extracting value by inserting within the batch so this is a change to the semantic message handling of the bridge that mitigates the ability to extract me Z by preventing insertion and reordering for specific transactions and we do this by changing the bridge and the way it handles messages this usually introduces a small amount of latency but as a result you can't be extracted from it easily so we can extend this to reorder batches upon receipt so when a batch fits the remote chain the destination we can reorder that batch on chain uh using a deterministic but difficult to predict ordering this can be as simple as just shuffling based on the Block hash of the destination chain uh usually you don't want to use a block hash for entropy because it is in some sense predictable and manipulatable however we're not trying to prevent manipulation we're trying to make it more expensive so what this does is uh instead of delivering all messages in the order they were dispatched we think you can shuffle all the messages using pretty bad randomness and then make sure that the messages are delivered in the shuffled order and you might be thinking can't a Searcher just continually reshuffle until they get a good ordering that's more extractable and yeah they can but you know we have a word for that it's called proof of work so the Searcher now has to iteratively repeat some Shuffle hash function and then try to extract upon that blog and it has to repeat that many times trying to find the best shuffling uh and so searching on these shuffled batches becomes a proof of work problem uh it increases the cost to the Searcher in a way that uh mitigates the amount of Mev that the Searcher can extract it is less profitable to do this because the batches can be shuffled by the remote chain and because all we're trying to do is increase the cost we don't need good Randomness to do so we can rely on bad Randomness that the Searcher might be able to manipulate because manipulating the randomness is equivalent to doing the proof of work it increases the cost of extraction so there are a few pros and cons trade-offs to this it's easy to implement and very little on-chain overhead it mitigates specific extraction from cross-chain messages uh like I said it's the Band-Aid we're increasing the cost we're not preventing it it's a mitigation not a fix and in order to be effective it requires significant throughput if all of the messages touch state then shuffling the order has no effect on the outcomes this only works if you have several messages a significant number that touch the same state so that shuffling changes the outcome of those messages uh the other like major con is that some applications want a specific ordering they want to know that messages are received in the order they're dispatched and this does not preserve that property so this is kind of a quick noodle on potential bridging mechanisms for me view mitigation it is not a full solution and it does not work for every use case but it's still interesting and fun to think about so that brings me about to the end of my slides uh there are a few things that I wanted to talk about but this is a very short time slot so I'm not going to get to uh one of the things I wanted to say is that cross train Meb is almost exclusively statistical mdv it has risk involved because it plays out over multiple transactions and Mev is the cosmic background radiation in Cross chain comes and that it is going from everywhere to everywhere all at once every chain is being statistically armed against every other chain in every sex at the same time uh but you can't use that communication to carry any useful message I really wanted to make a joke about ordering discretion and ordering indiscretions so when you're ordering a block you have discretion and Tina politely asked me to say the word meconomics towards the end of the talk so this is me saying the word meconomics and that is the end of my slides awesome thank you very much for that um I think we have a minute before sriram is coming on uh something vaguely related uh I'm curious for your take on the kind of conversation around it's been a lot in the last week about Roll-Ups and bridges are they really defined by the bridge or is the role of this completely separate thing from the bridge and it's all just chains communicating with each other hahaha so this is something that I fight about a lot uh a roll ups like definitionally includes a bridge to the layer one the question that we fight out about is whether the bridge determines the correct state of the roll-up or whether the notes determine the correct state of the roll-up and I land very firmly on the Node side of things uh I think that the bridge cannot force the nodes to accept any invalid block but the nodes can force the bridge to accept and as a result I think that the nodes decide what the state of a roll-up is um right in the context of okay no keep going sorry oh I can keep talking if we're still waiting uh I'm really good at having opinions um in in the in the context of bridging in this sense uh Roll-Ups are special because they have this built-in bridge and roll ups are a subclass of sovereign Roll-Ups which is a roll-up without that built-in Bridge so you take the Sovereign Row up you add an enshrined bridge and you have a Roll-Up uh we just came up with the terminology for these backwards so the terminology is a little funky Sovereign relapse are the general type roll ups are the specialized type with the built-in Bridge um just in time he threw him did just join the room uh since we are running a few minutes behind we will go over to him uh really appreciate the talk this was great as always um and we are now going to bring entree on to the next talk and he will be talking about restaking an Mev thank you foreign [Music] let me get my screenshot started um Okay window okay can you see the yep we got it now you're good to go okay perfect okay um hi everybody I'm sriram uh I'm a faculty member of the University of Washington Seattle also uh working on this project called eigen layer today I'm here to talk about uh how Mev intersects with restaking and the subtitle of the talk is mycelial networks at the root of the Dark Forest so Mycelia are these like fungi that help connect and interconnect the roots of various trees basically helping them pass along nutrients to each other so we think of risk taking as uh basically something like that and in this talk we're going to kind of examine how Mev kind of Mev intersects with restraking and how we can think about it Okay so what is Mev right one way to define maybe there are many different generalizations of this but one way to define Mev is it is it arises from the freedom of block proposals to order transactions so the protocol uh for example in ethereum the consensus protocol constrains various actions of block proposals but the there is still a remarkable amount of Freedom when it comes to transaction ordering and this gives rise to both markets as well as some negative externalities and one example of a clearly negative externality is Rears so for example if there's a block proposer and they see there is some like valuable transactions in a previous blog they may want to reorg the chain because of this and um while something like this is uh much more likely with proof of stake and much more difficult with proof of proof of much more much easier approval work and more difficult with proof of stake because there is a certain amount of voting accumulating in for example in ethereum today uh one way to mitigate this is by using uh things like single slot finality so in proof of stake what happens is like once you have enough votes cost on a given block then you can actually move on uh and finalize that block and any finality which breaks if two blocks are proposed with different orders then the the block proposals can be slashed so one way of thinking about it is the mitigation is possible the mitigation is possible because there is a certain amount of staking and there is a certain amount of slashing and one way we can think about this is if we want to generalize this so what is happening here is single sort finality is basically a commitment from block proposals to hold a certain rule even though they have a certain ordering Freedom they cannot exceed a certain Bond on the ordering freedom because they've made a certain commitment in the case of finality a commitment not a re-arc and a commitment not to reorg is enforced because of slashing and essentially one way of thinking about how to generalize this is if there were a mechanism to do programmable staking and slashing then block proposals can commit to more such rules and this gives rise to new kinds of credible commitments that block proposals can make that gives rise to more interesting uh mechanisms to manage me so before we go and explore some of these interfaces I just want to give a high level overview of what is restaking through the lens of uh eigenlayer the first three staking protocol so what happens in eigen layer is if you're an ethereum Shaker you know block proposes are ethereum stakers so if you're an ethereum Staker you stake in ethereum and then you make a credible commitment on the eigenlayer protocol it's just a series of smart contracts on which you can set your withdrawal credentials and in when you set your withdrawal credentials you can basically uh opt in into new commitments with the same stake so you could be running metal wires but you could also run more particularly commitments on how to run your transaction ordering preferences and here uh you know just to give a kind of mental model for how this uh this Collective works is it's a really a two-sided Market one side is ethereum stakers who opt in uh into securing these new services are to holding by the ordering commitments each middleware contains like an off chain container that you download and run and it also contains a new uh smart contract that you have to write eigenlet itself is just a smart contract on ethereum which which is a container which is holds your withdrawal credentials and um the eigenet contracts can talk to any middleware contract and uh enforce the series of like registration payment and slashing conditions so that's the Primitive of restaking it's called restaking because you're taking in ethereum core as well as uh opting into other kinds of commitments okay so what I'm going to do with the rest of the talk is give like a few examples of you know what are the possibilities at this interface I'll also talk a little bit about what are some negative externalities that we want to think about and mitigate in this uh interface one example is uh event driven Activation so what is going on here is imagine that there is a landing protocol or there is let's say cryptokitties and if you're a crypto Kitty there's you know there's this concept called the birth of a cryptocurity in a learning protocol there is a concept of either refueling your collateral or liquidating summary and these are essentially event-driven actions even driven actions basically means if the state of the ethereum protocol is something then uh the ethereum blockchain is something then a certain event needs to be triggered this is like a Cron job in uh Linux and right now we do not have a native blockchain framework to do this even driven activations all happen potentially off-chain and what you can do with eigen layer is if like block proposals are restaked on eigen layer then they can opt in to fulfill certain event driven activation services and basically what and if the transaction that you need to trigger is is a function purely of the ethereum state then what happens is if the uh since the for example in this case the uh collateral refueling or repayment or any of these actions is actually a deterministic function of the ethereum state then the block proposer who's creating this block actually has made a credible commitment on this event driven activation service then essentially they have to include that liquidation or cryptocurities birth or whatever that is otherwise the proposal can get slashed so you know what is happening here is uh there's already markets for this for example there is gelato and keeper and chain link Keepers and so on which actually do services like this but there is no tethering between that service and the block proposer and so what happens is there is a non-attributability if a gelato triggers a transaction and the transaction did not get included it's not attributable whether gelato actually triggered the transaction and like the proposal didn't include it or something else happened in the middle because things are not attributable you do not get like strong crypto economic guarantees of these actions but once you have strong crypto economic guarantees of these actions you actually change the structure of the protocol quite a bit because now for example what happens is like you let's say if you know that like people can set up like collateral refueling within a few blocks because there is some significant fractional proposes opted into this Market then what could happen is uh for uh margin lending or for stable coins or any other protocol the over collateralization ratio is basically just the time to liquidation so because you can have very tight time to liquidation you can actually build much more efficient Financial protocols if you had event driven Activation so this is one example of how Mev and something like Risk taking can interact with each other um I'll give another example which I've talked about before called partial block auctions uh we called it also Mev boost plus plus and the core idea here is uh Mev boost is a mechanism to uh to sell your entire block to a block Builder and instead you can think of we can think about whether it is possible to sell only portions of the block to the block Builder and the main reason why only the entire block is sold in the Meb boost Market is if a block proposal double signs a block header then they will get slashed so by having them commit to signing a block header we get a credible commitment that they will actually include that entire block but by opting into something uh like eigenlayer a block proposal can express much more fine-grained degrees of freedom for example saying that I will include this uh this a chunk with this hash as the prefix of my block and the remaining I have freedom to fill in myself so that's the high level idea I mean I can just dive into like one level more detail here the idea is that block Builders can send a certain uh portion of the block which is we can call it the Builder part and uh a block uh proposer basically includes the Builder part at the front of the block and the remaining is a Freedom From the Block proposal to include whatever transactions they want at the end of the block so instead of the block proposal selling the entirety of the blog they're only selling one portion of the block and they have freedom to fill in whatever they want at the end of the block and why is a mechanism like this interesting is because now blog proposers can express their own agency in figuring out how to fill the rest of the block and uh this you know if you look at the whole Meb boost architecture it is tailored around actually making sure that blog proposals remain decentralized and but if the blood proposals are decentralized but have no agency in expressing any transactions then it's not clear what what benefit that is but in a mechanism like this if the block proposals can express their agency then you actually get something uh quite powerful okay so there's a lot more details that I don't want to get into but I just want to keep this conversation at a high level of what are possible synergies of once you have block proposals in ethereum L1 restrict what other things can you start doing okay so another thing that you could do is you basically have a bunch of nodes participating in eigen layer like let's say a block proposal has opted into this uh threshold cryptography then what could happen is uh transactions that uh that come in come encrypted to a threshold cryptography method where there is a there is a secret share of the keys among the many nodes uh which which are operating which are also restaked on eigen layer and essentially what they can do is they um the client sends in transactions which are encrypted to the uh threshold group and once the transactions are committed with the and encrypted with the threshold group uh an eigen layer block proposal can take in and sign off that they're going to include only a decrypted version of these transactions and once they receive the decrypted version of these transactions from the once the keys uh the decryption key is available you can actually go in and include it uh include the right version of transactions a Blog proposal that commits to an encrypted transaction but doesn't include the decrypted transaction could get slashed to here there is an honesty assumption on this like keeper group The Keeper group is majority honest basically it is possible that the decrypted version of transactions is not included so one way to protect against this is you would say that uh if you did not get the decrypted transactions you have to just waste your block space for that portion of gas that you sold for the encrypted Lane and so this helps protect against like even if a majority of these are dishonest and do not reveal the threshold key then the blog proposal just proposes an empty block and so the so is just there are a lot of ways to tune the incentives here but high level idea is because the block block proposal is committing to include the threshold and crypto transactions they don't include the decrypted ones then they can get slashed Okay so that's another example uh I'll give one more example here before switching to uh any mitigations uh one example is you know is it possible to sell block space Futures so blobfish features is like I want to reserve a portion of blog space from this validator in the future and the idea here is you send your transactions to like uh kind of like a super fast Mev chain which would be something like Suave and uh you know this chain needs some amount of reorg resistance and some amount of censorship resistance and you buy block space a priority and uh you send your transactions to this like super fast chain and if the transaction gets included in this super fast chain then a block proposal who has opted into this block switcher actually has to include these transactions in the block and so essentially blog proposals can make credible commitments to include transactions from the future block if they get included into the MAV chain so this is another example of like what what we can build and it's possible to take all these different modules and compose them to build a block where some aspect of the blocks come from even driven transactions some aspects come from auctions sold to builders some aspects of the transactions come from threshold encrypted Lane some other aspects come from the proposer having their own freedom so this is in some sense like a modular roadmap for how Mev can be built on on ethereum by using something like recycling there are also some risks I want to just spend a last minute on this uh one risk is because you may need proposals to be more sophisticated it may need lead to more centralization if you have mechanisms like this but one way to contain this centralization is by basically if there is a enough of a rich language for the proposal to express their constraints to the builders and to the relays then the bill doesn't release can actually satisfy all these constraints follow the proposals um there are also other really interesting things that happen with restaking is because in some of these Services uh Services building on restrict eigenair Services can only request the more decentralized committees to participate it's possible for uh promoting actually decentralization in ethereum itself there is the possibility of cross domain Mev because the same set of nodes now participate in many different uh many different domains or services and again the solution here is to opt in to like Rich uh general purpose cross domain Mev markets like uh swap to actually uh build these things and finally there is risk of actually potentially building uh censorship markets on something like aguiler I think this is something we can only keep vigil socially and use social consensus to deal with situations like this so uh that's a high level tour of that I think there are interesting intersections between uh something like eigenlayer in or a general restaking and mov and look forward to having discussing with some of you thank you awesome thank you so much and this is going to segue perfectly so you can hang on here uh we're going to have a panel now on meconomic security in liquid and restaking protocols so it'll be by self-moderating and we're going to have firm um and then we're gonna be bringing on tarun Ben and Holly for this one how's it going everyone hi yo all right sounds all working uh so your last slide actually segues perfectly into the thing that I wanted to ask um for the first question um and it is around that centralization risk um both which exists for both um for both restaking and liquid staking worth of both of the topics here um liquid staking so far has been uh somewhat limited um in the amount of operators that are generally like actually working under the hood of some of these liquid thinking protocols um so there's going to be a challenge there in you know Lido has talked about in their V2 things like the staking router starting to use DVT to try to decentralize under the hood um and then similarly for restaking um there is that centralization risk of ideally we would like to have these super horizontally scalable things like eigen da that are easy for does to run um but someone could also show up and put Solana on restaking um and now there's effectively a massive incentive out there to be a really sophisticated restaker um so where it's supposed to take advantage heterogeneity does it just then incentivize that kind of lower bound to actually just move up um so I'll turn it over to you sure I'm to kind of continue where you left off on the last one to start yeah no I think uh if you look at like aspects of decentralized trust there are two distinct aspects of decentralized trust one is just coming from the amount of economics and the other one is it comes from a certain amount of decentralization our collusion resistance and I think different Services rely on different kinds of uh security like for example if all you want is for a for a you know for for example for certifying that a certain execution condition is valid and you want somebody to underwrite this risk as long as there is you know a significant amount of Economics underpinning it it is enough like if you had one billion dollar restate on like a validity condition which is checkable on chain and slashable then it doesn't really matter whether that one billion dollar comes from one node or it comes from like thousands of notes I'm talking about this from the point of view of services building on top of wine layer and there for those Services we would expect basically like they would you know there is a significant benefit to centralization so there is like a small centralized committee or just like coinbase or some like few nodes which can just opt in and provide these Services which are primarily just economic in nature and uh there is a another set of services which simply do not uh you know it does not suffice to have Economic Security you need decentralization-based security and some examples there are you want to have an ordering layer which has censorship resistance uh you want to have a secure multi-party computation where you're splitting up some secret and information into different nodes and the core assumption is that these different nodes don't collude with each other uh there are other examples like uh you know the threshold cryptography we saw earlier where like you need to split up a key into group into secret chairs um even things like data availability Reliance to some extent on honest majority or like honest certain fraction assumption and you know these examples all demand a decentralized quorum so what we expect to happen is on more expressive risk-taking platforms like eigenlayer people will specify that they only want a decentralized quorum for certain tasks and this can promote an additional yield to a crew onto the decentralized Quorum and you know basically instead of all of us just you know uh talking about the ideology of decentralization there is an actual value of decentralization and if the users can just pay up for it that's really the best way to promote decentralization foreign wanted to turn to you on the kind of first half of that question curious for your take on uh the liquid taking tokens and kind of the underlying operators of those um particularly because it's unlikely that we're going to have a million different types of liquid saving tokens so it is important if we want to do centralized validators most likely that kind of the underlying components of Lido rocket pool Etc um that they are able to more sufficiently decentralize that validator set underneath um and kind of how you see that landscape and how you see that flying out going forward yeah great question John um I I think that um first of all it's I agree it's super important that um the liquid staking um provide us uh decentralized because there is a lot of centralizing pressure on them because of the network effect of the underlying token and we have seen um a range of different designs from completely centralized providers um like coinbase on the one end to arguably very decentralized providers but less scalable like rocket pool on the far end and then Lido maybe to the slide left of Rocket pool making another trade-off in more scalability but um yeah but but less decentralization and so um it comes down to a trade-off and the question is um will those trade-offs remain or can we have the best of all words right can we have scalability and decentralization and I think um the answer is mostly yes so there might always be some the need for some amount of governance that cannot be removed but um so my favorite mental model for thinking about this is basically um thinking of a staking pool as another variant of a landing Market where um you take money from uh from stakers and you you basically give it to node operators right and and there's different types of of lending um there's one that's secured by reputation um and then you don't need a lot of collateral or I you anybody basically can can borrow and and then you do need more collateral to secure it and um so I think um whenever you want to have scalability um you basically can't have over collateralized Landing only and so you need some way of dealing with this reputation Challenge and I think it would be interesting to see how well we can bring these kind of you know attestations to someone's reputation um how how where we can make that trust us how where we can bring that on chain entirely um for example through the use of uh inspecting you know performance stats uh building reputation over time but also minimizing uh the harm that a faulty node operator can do through techniques such as DDT and then in the center I see something like um the staking router that you alluded to in in Lido V2 that is basically the central Hub where anybody can um where anybody can add new modules um and new combinations of uh um of node operators and collateral and reputation and whatnot and then there's some Central mechanism to allocate those and uh to to allocate the Eve that comes into to those modules and I would be very interested um Toyota rune's talk because I think he's uh what everyone thinks about this because he's one of the most one of the leading experts on on lending markets and um and parameterizing parameter parameters um basically the the relationship between uh you know the collateral and the borrowers and so on um yeah I mean I uh I I yeah I kind of think the Lido model is the the end game this the the router model is sort of the the end game for this um you know it's funny you actually you mentioned um hey you can Implement Solana on top of a restaking mechanism well I don't think running Layer Two sequencers is that much less difficult than keeping up a Solana node if we're going to be completely honest if you look at all the arbitrim posts if you look at all it's like not that different uh and I think in the roll up world sequencers and providers to roll ups are going to have to do a lot of operation have a lot of operational overhead so I think the centralization aspect of like oh I don't know how to correctly restake or allocate my assets is a problem you will have generically in a decentralized sequencer roll-up world like those users are going to have to be just as sophisticated so I'm I guess I see it as less of a concern from from the perspective of like we're already moving to this kind of extremely complex ecosystem of chains and uh in that world you're inevitably going to have to do solve these allocation problems no matter no matter what participant you are um and you know based on that I think yeah figuring out what the collateralization looks like will change over time especially if you know a lot of Lido node operators are restaking to also provide sequencing services for you know particular uh particular Roll-Ups um and you know I think of course Ben has a a pretty wrote a nice post uh about kind of opinions on that and how to Think Through you know how the Strategic decisions when you're in a decentralized sequencer so yeah I just one thing to add about the I mean the question about so uh you know what if a protocol like Solano which favors centralization were to run on an argument or whether that favors centralization of of e-sticking I think it really depends on the uh relative you know fraction of uh you know of of value that's accruing to staking youth from participating in ethereum itself versus another protocol so it would only really become an issue for centralization if the value of like value coming from Solana was like much greater than the value or as equal to or greater than the value coming from participating in the theorem itself um so it's I I mean yeah that's just a very important part to uh to take to take into account uh Ben do you want to say something about like how lightweight decent light sequencing can be I think someone's saying basically decentralized sequencing will be interesting yeah I think I think this is sort of the I mean I think that it's anyways a deciderata of centralizing sequencers to make it lightweight so that it can be you know fully decentralized I think that's a bit of a different question from the concern that something like Solana could also come onto E3 staking platforms but yes um you can def like you can definitely try to make decentralized sequencing as lightweight as possible so that um and that's what the E3 stakers would be participating in like E3 stickers would not be participating in complex roles like proving they would be participating in the sequencing you don't need to have Easter stakers participating and proving if they are participating in the sequencing yeah I think the even more uh specifically I think the idea that you don't even need to hold the state of the roll up right like that you don't even need to participate in execution just ordering and data just ordering and ordering is a stateless operator so you just order a bunch of like transactions and as long as you're collecting the fee for ordering the transactions those transactions are kind of like forced to be included so that's how you can scale sequencing I I mean the reason I would like slightly push back on this is like looking at say the arbitrum example of like the sheer amount of network bandwidth that the sequencer had to handle it very much reminds me of the Solana DDOS level bandwidth like you're talking about people sending like 100 gigabits a second to a single single entity so it there is we still haven't gotten to that point where I I would say I believe that they're they're they're that different to a node operator I'm sure you know hetzner probably can which is like a big data center operator I'm sure they have the exact statistic on like how much bandwidth arbitrum has had to deal with versus Solana and like uptime statistics and I bet you that's not that crazily different I I especially after the airdrop I think they're they're not so different in terms of operational overhead that like which is that's sort of my main point right now no I I think that there may be one thing to it but I I want to just say that like there is a the problem of not having me re auctions is what has contributed to that and uh you know to to segue into your next talk and as long as the decentralized sequencing system has you know reasonable Mev management which Ben has taught a lot about uh I think you know you would not have a problem like that I also wonder how much that needs to correlate with the uh the value of staking like you could also have as a public good a uh you know infrastructure or server that's doing a lot of a lot of work in terms of routing messages but the but in order to participate in stake in the protocol you you uh you you just need to do lightweight operations and so in some sense that public service is uh real real it's just a public service that's being subsidized by everyone participating uh and it's not clear that you need to like by speaking more you can then run that service and therefore profit more so I think it depends on the um relationship between staking and and participating in different roles yeah yeah I was gonna say the 1.0 pushed back on a little bit from earlier is in regards to how much the rewards actually need to be for something like this to create a centralizing force um where it should be equivalent to or possibly greater than the type of rewards you would get on ethereum um I would say that doesn't have to be the case I mean this is exactly what we see with Mev boost and you can kind of think of any additional restaking Revenue to be viewed as a form of Mev to any kind of potential rest acre and we see that with no boost where hey if I could earn four percent or five percent of my own but I can earn six percent with my Boost 95 of people are going to run that thing um and they don't even necessarily need to create that value particularly if that like this restates Solana if they say they have an inflationary token that they're giving to the Reese takers and that subsidized it's like a high yield to them those still exist um somewhat in my mind I don't think there's I don't think there is a perfect answer to it um one of the interesting things anniversary we've talked about before is actively incentivizing um decentralized committees with higher rewards uh most decentralized ones um such that there's actually an explicit Advantage which is a very difficult problem to do but theoretically could be done um and it's a very interesting thing to play around with uh I would have a follow-up question to that so how do you uh how do you identify decentralized uh committees reliably yeah I think this is going to be a whole fun another like service which would basically do it and you know just to clear our perspective on this is eigenlayer wants to be uh you know the one of the paradigms that we lean heavily into is intersubjectivity intersubjectivity is that like most subjective decisions are made at the edges not at the platform so we don't want to make a judgment saying somebody is more decentralized and somebody else is less decentralized and each service needs to consider its own like requirements when making these decisions and these could be supported by decentralization oracles which then like steer people in in those directions and for the particular needs of each of the services like a secret sharing is an example you know if you sent this or you know and and shares of the secret to the same person you basically can end up getting no guarantee even if those those set of people are geographically distributed there are other use cases where you know for censorship resistance or legal and other purposes you want people from different jurisdictions to participate that's a different kind of like a requirement so each service will have its own requirements and they will lean on a new ecosystem of oracles which just try to do this thing and there are you know it's hard to identify decentralized notes it's easy to identify the centralized nodes we will know what the exchange wallets are we'll know what the some of the biggest operators are and so on and just excluding them just still gives us a long tail of notes to work with so there are all kinds of ways that Services can uh can do this and there's no one universal correct good answer and it's going to depend very much on the service so actually one thing I I've kind of wondered as a sort of a research direction that towards this that you know I don't think we can do right now but but maybe you know in a couple years uh we'll be at the place to do this is you know there's this line of research on doing Secret leader election and prehistake with threshold homomorphic encryption and there's a question that I've always wondered is of like can you do private elections of committees not just single leaders for this type of thing if you had say threshold fhe and like would that be sufficient for the type of thing you're talking about because I because I actually think you could generalize their method to multiple participants but no one's done that yet and that I think that would like be like the foolproof obviously you know relying on fhe for anything is always scary but yeah there's two aspects to this why if you start with a centralized taking pool and just uh choose like even many leaders randomly still not going to do any benefit because all leaders are basically the same person so there is the kind of Baseline of decentralization that you absolutely sure sure which you need like uh multiple secret leader election to to make it even more protected yeah yeah sorry I I agree I'm not I wasn't definitely you need to kind of have the state distribution be you know more uniform over unique identities not just addresses but I I guess there's a lot of this other stuff that I think has been lying in the shadows around this type of stuff that you know uh maybe we'll see live in one to two years hopefully turning directions a little bit um I just got a rather spicy question someone sent me uh just a few minutes ago during the talk um it really relates to so I know true you've spoken in the past um how eigenlayer could particularly later on be viewed and restaken broadly as some kind of like free market upgrade to ethereum where particularly as it gets more difficult to upgrade ethereum um validators Can effectively opt into it as a free market and do it themselves um the inverse side of that I wanted to ask everyone's takes on is does this give a kind of additional unearned governance power to validators um where for example if ethereum developers in the community there's some upgrade that they view as rabbit risky this is not something that we should be doing um but the restakers say hey I can make a bunch of money from this thing and we're all going to opt into this um where do you kind of see that kind of tension between that governance power which they may not we may not want them to have versus also that flexibility is a positive thing of being able to kind of free market upgrade over time I think the the real answer to this lies in the full notes okay so you know as we all know like there is a power in the full node which is not fully specified in just the internal nodes which is that if there was an invalid State transition the full nodes can reject it and imagine now there is an upgrade on top of like you know which comes from something like eigenlayer which like everybody opts in and all the validators are doing it but at the full node level so buying opt-in for example at the full node level gives you like a significantly boosted security Beyond The Economic Security that you can get by the stakers opting in and you know it is at that threshold that really the buy-in from the rest of the community is needed and I have some like uh ideas you know untested ideas on how a new service for example might be able to do it for example a service May commit to burn eat if like all the light nodes upgrade or like you know full nodes upgrade to you know in in when receiving the fees is not fees is not just distributed to the stickers some fraction of the fees just burnt as a compensation for the externality to the full nodes which then need to upgrade to not only check the validity condition on the core protocol but also on this protocol extension so there are all kinds of interesting things that can be done but also the flip side is for example if there is a censorship Market built on restaking where you just say that like hey I'm gonna pay to like censor somebody's liquidation transaction so that I can earn the money I think this can only be dealt with socially by like whatever slashing these uh social slashing these stickers and so on so it does become more complicated one thing I guess I would say is like in what world have you know if we view sort of restaking as sort of Bridging the Gap between like clients full nodes and archive nodes like you know each of those levels had different governance rights in some sense in almost every Network all you're doing is you're taking like this discrete type node types and like making a continuous Spectrum um out of it and in some sense light nodes never really had governance rights before right they they sort of were just forced to to eat whatever Merkle trees shoved at them and uh I I think I think like you know it's sort of the same all you're doing is just like widening that Spectra from these like three Delta functions I would also note that uh validators do already have the uh the the the the agency to run another protocol uh E3 staking is subsidizing that agency right it's saying you already have East State so now it's less costly for you to run this additional protocol but they already have the agency to run other protocol and it's also not clear that by running another protocol this necessarily is is equivalent to an upgrade because it's it would only be if that they were running that to the exclusion of running ethereum itself so it's a it's a protocol add-on and you could think of like if everybody's running it you could think of it as a protocol add-on and that's where I was going with but full nodes don't check for the validity of this protocol add-on and so you could ask them to check for it by like uh you know doing this uh fee burn you know there are some fans of 1559 and fever here uh so that's basically a compensation for the externality to the full nodes for having to check that you know they're continuing to do that right uh another question I know that a lot of people have kind of had uh trying to get their head around is this idea of Leverage quote unquote in staking and restaking noting the clear difference in that um obviously you know you take on a leverage trade or something like that like you might get liquidated on that due to just market conditions if you're restaking and you're honest you shouldn't be slashed assuming everything operates correctly but you are increasing the leverage in the sense that you potentially have a very high ratio of value secured relative to the amount of like stake actually securing that thing which is moving in this direction of old security um and that works primarily under the assumption where this increases the value of something like with the value that it's occurring because more value goes to it um but the other side of it is something like restaking one of the interesting benefits is you can do dual staking uh tokens with it where there's some other kind of token that captures value so do those those things kind of become at odds with each other at a certain point where now eth isn't necessarily capturing that value which accurately compensates it for the additional risk that it's taking on economically where some of that value starts going so the second token more and more and they want to kind of move in that direction okay so I think you know there are really different so you know like like you said from the Staker point of view if they're honest and if you the protocol code is correct they'll never get slashed so let's keep the sticker out of the picture from the service point of view is there like are you increasing additional profit from corruption by like adding your services on top I think that's the high level question and I think there is a lot more nuanced question and I don't think we'll get to this uh thing here the idea is you know if you look at ethereum you know it's securing 400 500 billion dollars of value with 25 billion dollars of stake and like first we have to get to the bottom of why is it okay to do this it's okay to do this because you know we are the the total value secured is not what is protected by what a state the total value transferable out of the system within a short period of time within the attack duration is what is being protected by staking and by having this mental you know this crypto economic model correctly programmed across various Services you can actually get very strong probably correct guarantees on what uh the crypto economic conditions are for safety um John I think another way of looking at this uh that issue that you described as leverage so just from a different angle is that uh because uh an E3 Staker is now participating in in you know many different sources of revenue right for an individual uh chain that it's securing uh the value on on uh the value that being generated by that chain is low relative to the to the value of the East overall being restaked because it has many other sources of revenue and so this becomes an issue of the fact that the uh if the the relative value of this one chain that it's securing is low relative to all the other Chains It's securing then the marginal utility is too small for it to care that much about that one chain so in essence like it's from the perspective of the change the risk arises which um and dual staking can lower the Leverage of the uh of the E3 stakers right and it can uh it then shifts the security onto another source when whoever is what this other token is and uh and so that is helpful for the chain itself I mean it's one different is that it's I think it's the same issue as being over leveraged but just from a different angle yeah I I would just add I like thinking of it as you know even though I I'm sure by mentioning this word people will suddenly get scared but I like viewing it as like the sort of like tranche style Insurance where like the second token is taking the equity style losses Beyond a certain point and the eth is not which is that's sort of this over leveraged piece like there's not going to be some like huge eth restaked eat liquidation versus like some fraction of that gets absorbed by this other token now the question is can the other token ever get to a market cap close enough for it to be meaningful relative to eth and like that's that's a different question like will there be enough liquidity for it to actually be this sort of like lost asset um but but in theory that's sort of what what you're kind of hoping for is kind of this split I would um I would also point out that you can't really prevent um validate us from taking on additional Leverage so for example we've seen the rise of liquid staking tokens and those they could staking tokens are now the primary form of collateral used in ethereum lending markets in order to borrow anything and so we already have an implicit amount of Leverage in the system today and so people are blaming liquid sticking tokens for that which is I mean it's just inevitable but so to those people I would say even before that improve of work there was already a lot of Leverage in staking in mining that is because all of the the the big mining Farms were all deeply deeply leveraged so they were buying all of their machines um actually on credit and they were they were borrowing against their machines and they were using their machines to collateralize as well as their future cash flows uh to buy more machines and so um this is something that all protocol designers have to keep in mind that you just can't um control what people do outside of your protocol and um that's why you have to operate with big margins of safety like screen was saying like ethereum protocol does not secure all of the money that is uh sort of part of the ethereum state somewhere it's it's not even all of the money that is currently being transacted it's only that which can be controlled by the ordering basically because not even 100 malicious uh well let us could force an invalid State transition um so yeah um that's why I think yeah the gist is basically Leverage is always possible there's no way to prevent it and you just have to design around it curious quick follow-on uh for anyone if you and I would agree on this that like with something like liquid sticking tokens were inevitable in hindsight um our liquid restaking tokens inevitable looking forward um where you have a position saying that yeah I have my eth stake but I'm also restaking to these four other things and this like gives me the right to the associated cash flows of those and there's all these different liquid restaking tokens now with an additional layer I I think you know it's much more uh likely that liquids taking protocols will integrate some of the services under the hood which means existing liquid sticking tokens kind of eat the additional reward by just integrating the under the hood for that certain Services which are highly yield barring rather than you having to go and create a whole you know liquid staking token for risk taking so I I guess I I sort of think of it as like if restaking is successful in the sense that it has hundreds of applications using it um there will inevitably be some notion of like etfization where people want different classes of ethiode they'll be like the the most high grade eth yield but it's just pure staking there's like the slightly riskier ethio which is like maybe staking plus submitting Oracle updates that there's maybe the slightly higher risky version like data availability plus Oracle updates like this all depends on what the slashing rates are so I'm so the relative ordering is not is it's not perfect but I can imagine people basically like fractionalizing like what level of ethrisk do you want um yeah I would agree with both of those I I think that um the liquid staking protocols were internalize uh restaking if it turns out to be profitable and useful and I think also down the line maybe in a few years um those same liquid sticking protocols were come out with different tranches of of Eve that give different levels of risk and seniority um to invest as I think that's one of the main things that we may remains in the design space for these protocols after after stuff like the liquid liquid sticking router and DVT has been figured out I think yeah okay so one of the applications uh that you guys are very familiar with um that will be secured potentially with reseaking and has been talked about a lot in some of the previous talks with something like a shared sequencer layer something like espresso has talked about in the past week um even simple forms of a shared sequencer AKA effectively based Roll-Ups where you're using the L1 as a shared sequencer one of the interesting points in the value flow of Mev as Justin was speaking a lot about earlier is that in the simplest case that value now effectively goes to the L1 itself so in base Roll-Ups it would go to eth in a shared sequencer the value from the Roll-Ups Mev that they would normally get from their own sequencers in the simplest case would go to the shared sequencer layer itself um and so the question then becomes kind of around the incentives for Roll-Ups to use those things if they want to keep that kind of value for themselves um and more value should be created in this system because as I know sure I'm added some great tweets about this the other day is like kind of the Baseline is what they would have already had is Mev and now you should have an additional Mev that's opened up by the interoperability between all of them so in theory of more value-graded um but that kind of incentive alignment issue of like how do we return that value to Roll-Ups in some form such that they're incentivized to opt into this thing um and it's not just nobody wants to opt into the thing that will optimize everyone because I want to keep most of the value for myself so like how we kind of think about those trade-offs and the value flows uh I can take a stab at this uh if I um to go first I think there's a lot of analogies here too you know like you have a music festival and you have all the bands showing up to the music festival and the music festival is selling tickets right and how do you allocate how do you pay the dance to show up and it's also not a One-Shot Music Festival it's a repeated game So eventually if the bands realize oh the music festival organizer is taking all the profit uh we're not we're just gonna run our own music festival so I think that before uh we can talk about the economics of shared sequencing and how and to whom the value accrues it's important to identify like who are the players right um and I think Barnaby has like a very nice post on this for Roll-Ups uh which distinguishes three parties like users a roll-up operator the bass player um and here with shared sequencing we have another party right we have the shared sequencer um and the node's operating it but I think we should also separate at the roll-up operator level between the provers who are who are actually incurring the operating costs and the governance of the roll-up right that has the power to decide two things right how do you define the VM right and uh and which sequencing layer is the roll-up going to run on and importantly not only do they have the power to decide that initially the governance has the power to decide to move away from the sequencing layer and that gives the governance the role of Leverage that governance could be encapsulated well it could be a Central Central company uh or it could be encapsulated in some token I mean let's just call out the governance um so base fees covering the operating costs like proving uh plus some Epsilon profit that can be easily directly programmed into the VM and they need to do that because they need to get previous to run um but obviously that baseview doesn't capture Mev uh and so then there's it's a negotiation between the role of governance and the sequencer the roll-up governance wants to know you know am I going to make more running on this shared sequencer then I would make running my own sequencing layer right and obviously there's a I mean there's a there's a cost of building your own one-time Capital cost of building your own sequencing layer But ultimately you want to know that you'll be making more you know going to the festival rather than running your own show um and so I I think it comes down to a negotiation and and that can manifest in in many different ways um one way is if uh if the average Mev that the roll-up would make on its own can somehow be estimated or simulated or it can be adjusted dynamically based on historical average then that can be like hard-coded into the base fee of the role of VM um but I think that's quite hard and that basically would not be covered by users it would be subsidized by the by the sequencer nodes maybe they go into a deficit sometimes but on average they would be able to cover the cost and and take the Surplus that comes from shared sequencing another way would be probably the ideal option uh but but maybe challenging to perfectly achieve is if the allocation algorithm okay the the mechanism that the sequencer is running is transparent in other words if there's transparency in the Mev auction or the block building process and if it can calculate and verifiably Report what is the marginal contribution of each roll-up to the total nav and it distributes that proportionally to each roll-up you know giving a cut taking a cut for itself then that would Foster the strongest trust and incentive alignment between the roll-up governances and the shared sequencer but I think that's like an important research question and um like that that I think is like the ideal option but it may be hard to achieve I think it's some inspiration from the atom uh uh atom World orientation security here which is uh basically like doing dropouts like you know randomly dropping out like one of the chains and then calculating what the average Builder value is and then like you can use that to uh to calculate or estimate what the uh uh looking at the chat here estimate what the uh marginal value is for the different ones another way is that having actually like combinatorial auctions and then now I have to segue to the room that sounds very expensive for the participants but yes this is possible so I I think that's one um option that we haven't discussed it which is just allowing um Roll-Ups not not enshrining a single um sequencer layer but just having the ability to profit switch between different ones um and it's a it's a funny thought how sequencing layers could kind of um be destroyed and be created on like a per block basis uh so I don't know if that's viable or not um but I think definitely in the future of of multiple sequencing layers um nothing speaks about speaks against these sequencing layers competing on a per block basis for the um Mev and just building up um the value paid to to the proposal uh or to the governance as it would be in that case right so um yeah I think I think that's another option I probably got in trouble for asking one more question because we're running late but I want to ask it um I'm curious for thoughts regarding uh it's something that I talked a bit about earlier as well um and Barnaby has discussed in the past of his idea of Pepsi on the idea of like the protocol being able to enforce different proposal commitments and like the first part of that abstraction that he's talking about is this kind of idea of an in in protocol eigen layer um where you can make the protocol aware of these commitments such that you don't end up with this problem where the protocol is not aware that hey maybe this person is slashed and is like not really aware of what's going on with its own security at that point um so I'm curious uh as to any of your thoughts on is it helpful to kind of build these things in more to a greater extent to be able to just give the protocol more awareness of like what's actually going on with its own security um or are there other externalities to think about and they're where they may not be attractive yeah I think you know the first part to that is how to make ethereum aware of a slashing event on something like eigen layer I think is the answer to that is actually simple and thankfully coming up uh in an upgrade called smart contract triggered withdrawals right now withdrawals are validator triggered not con you know withdrawal credential triggered and once you change that the awareness can be kind of instantaneous as soon as the contracts aware that there is a slashing it immediately like goes and triggers the thing from uh ethereum so that's the answer to the first part but there is a second part of the benefit which I think your crew by having it be uh enshrined as opposed to being on a kind of like a secondary layer like eigen layer is the benefit is that you know suppose somebody makes a proposal commitment that I will only include the decrypted version of these encrypted transactions and in the block and but they actually don't they actually don't include the decryption on eigen layer all that we can do is to slash them for 32e so the maximum liability is like bounded by 32e in either layer whereas in if you make it in protocol I think this was mainly Barnaby's idea the idea is that if you make it in protocol it can be part of the validity condition of the block itself that the proposal met the condition or not this is just a much higher path there are ways to I think therefore if if and when such a thing is possible it's actually beneficial but the problem is that you know uh if somebody writes a bad like opt-in condition which had some smart contract bug you know you could lead to mass slashing events and things like that and the only protection we have right now is based on on eigen layer is based on a subjective slashing veto and I don't think ethereum would want to internalize you know such kind of subjective committees so uh the evolution that I imagine is going to be that initially people play with a lot of these models on a opt-in layer like Eisen layer and once that is much more like robust and stable some aspects of it can be basically just enshrined default into ethereum there we go awesome uh we will cut it there thank you everyone for this this was a lot of fun um and again we will perfectly transition we've got tarun who will be staying on for the next one who in his own words is going to be talking about how to kill Fair ordering sorry um no I'm very happy about killing it hey sorry for that I'm just sharing my slides will take one second generosity to allow stream yard to do this so sorry about that give me great um hopefully it shows up in a second um cool um cool so uh yeah first off I uh I wanna want to talk a little bit about the flaws of fair ordering um and this is not meant as a uh attack on any of the people who who worked on it before I think like it's a really interesting idea uh in Social to take some of the ideas from social Choice Theory and apply them to uh to try to make blockchains more fair but uh I think there's there's no way of doing it in the way that people have tried there might be some other stuff hopefully we'll end with some positive notes but uh before that let's bring out the hatchet um so Tina as as previous speakers have also mentioned um forced us to to say the word Mev economic somewhere um in in my case I I unlike John and James uh kind of sort of wrote a slide uh and the idea is that I and this is a quote from one of the papers on Fair ordering which is fair ordering guarantees specific ordering in a finalized Ledger on how transactions arrive to a network to reduce minor extractable value yet virtually all fair ordering mechanisms say nothing about the economics of what they do they they guarantee that like some subsets of orderings will be respected other ones will not ever happen but they never tell you the cost of that or whether certain payoff functions change with that so one question is if we're putting all this extra onus and extra work on top of validators what's the real economic value of it is there economic value is it harmful and that's sort of the main thing we'll be covering so um you know what is fair ordering so fair ordering is a mechanism that coerces on its validators to respect a particular set of orderings or transactions I.E first come first serve block order fairness uh block batch order fairness um all the papers are not duplicitous they do all point out this fact that it's actually impossible to attain perfectly uh Kenneth Farrow Munda Nobel prize in economics for proving this which is that in in ranked Choice voting so you can think of A ranked Choice vote as a validator giving you an order permutation of sequence of transactions uh and then sort of aggregating the route together to to construct a final outcome that it's impossible to get something that doesn't centralize that doesn't have what's sort of known as a dictatorship so instead Fair ordering tries to you know make algorithms that are approximately the average or majority vote or some type of some type of property for most ordering so you know here I put two different definitions the other thing that's kind of funny about all these papers is they all have slightly different definitions of whether a batch is fair or not um and they all involve sort of extra data like some notion of time stamps and some assumptions of honesty amongst the validators um thank you to John for making this meme um but there's something weird about these definitions right these definitions are properties of partial orders they don't say anything about the economic value per unit partial order yet on the other hand if you think about Mev Mev is actually an economic profit a rent that is extracted that rent has a size that has a magnitude um and I think you know the the three main papers on Fair ordering system is uh aquitous and uh quick block ordering um they had like good intentions but they completely ignore the economics and they're like hey you do this thing it's this combinatorial thing um we restrict the sets of orderings uh and it gets rid of Mev and it's it's kind of a crazy statement to make because again you don't quantify the actual amount of Med or the value and your agnostic to all applications which doesn't really make much sense right there's a reason that certain applications generate you know very regular Mev like amm some generate very spiky Mev like liquidations or nft auctions those are very different things you can't really tell me that this ordering is universal to all of them and so that's the thing we're going to kind of explain today to try to dismantle this kind of shambolic industry of of hoping that we can uh you know violate the the natural laws of social Choice Theory so you know is it really a good idea to cause these you know to to have validators have to do all this extra work when it it doesn't really have an economic payoff and hopefully you'll be convinced at least somewhat you know the full paper will be out soon so you can see the proof but uh that it's actually harmful in some cases so I want to also give a bit of uh a bit of historical context from other fields so other fields that that use social trust Theory especially things in decision Theory things in AI have studied order fairness in different contexts especially with regards to model quality or like how much you you mutate a model by giving some fairness guarantees and they get these impossibility results the difference is model quality is is a bit like mev's economic value there's still these kind of continuous magnitude objective functions depend on the input data depend on the particular thing you're inferring but they all get these impossibility theorems so why should you not expect that for fair ordering okay so you know I've had my my uh my fun but now now we actually have to go into to what it is about Fair ordering that's so weird so to try to dismantle uh Fair ordering what we do is we consider a two-player game um one player is the fair ordering protocol that is proffering a set of orderings of transactions that are that are allowed to be executed and the other you can think of as an adversarial defy developer so someone who is making a protocol that's trying to cause the fair ordering to be give a worse value for the the users of the protocol the the rough game mechanics are nature draws a set of transactions so that's users generating sets of transactions the developer plays first and constructs a protocol so think like a D5 protocol um adapted to the set of transactions and the protocol is a payoff function so we'll talk a little bit about what those are in a second but you can think of the payoff function as the expected value for the user then the second player the fair ordering protocol draws a set of permutations from the symmetric group that's SN that's a set of permutations uh and it draws it from a distribution p um and so one reason that you have this distribution is of course in in most Fair ordering protocols there's some extra metadata like the timestamps of when people different um validators receive certain transactions that adds some Randomness and that Randomness means that there's not a unique ordering again errors and possibility theorem sort of guarantees that so there's a sort of set of orderings and it'll turn out that the size of the set of orderings controls uh how well the the the the fair ordering protocol can win in this two-player game and finally a value V is realized uh based on these so without getting too too in the weeds uh to the right you'll see something that defines the value of a protocol PA which you know we can talk about by its distribution of uh orderings it generates and we can we consider a min max loss so if you've seen sort of classical classical Game Theory classical Von Neumann Morgenstern stuff this is just traditional mean math and there's a loss function which takes in an ordering that's a permutation Pi in a symmetric group uh and a uh payoff function f and this loss function is meant to measure some notion of fairness and we'll talk about how they're different different loss functions will give you different Notions of fairness but you sort of want this game to qualitatively have the same winner like usually the D5 developer wins for many loss functions even if I perturb it and so in this game the value of the game if the game is positive the adversary the D5 protocol developer wins if the value is negative the fair ordering protocol wins and of course this will depend on loss function but we'll talk a little bit about that so why does this represent favoriting credit calls you know abstractly again you can view a fair ordering protocol as taking each validator sets of transactions and some metadata Associated to those transactions like timestamps and the fair ordering protocol outputs a single permutation with those transactions in the set of allowable transactions h um but the thing is there's many of these Age and and the key uh thing we'll find out is that fair ordering protocols don't make H that small and that's where the adversarial developer can take advantage of them again the randomness comes from Network latency user demand Etc and you know how do we how do we bound V this value of this game well there's combinatorial constraints that arise from the set age another way of looking at this is given enough noise or entropy into sets of permutations you're allowed you can almost surely construct a payoff where if I restrict to those sets I get a worse value than if I am unrestricted and basically what the proof does is we explicitly construct a D5 protocol whose rules for when liquidations are allowed to happen uh exactly is optimized to be bad on these permutations generated by the fair order and protocol um in fact the liquidation rules draw a lot of inspiration from a protocol many of you might know which is liquidy Luc they have a staking pool that auto liquidates uh LSD uh issued assets and the only difference is that instead of giving Parada rewards we get fixed rewards but it it's a little bit more complicated to describe and what is what this sort of says is fair ordering preferences particular application so this application that looks like a default protocol that has particular liquidation rules uh it has a worse value for users under Fair ordering which means that other protocols that have the same value uh you know are our sort of preferenced and so there's there's sort of this very interesting thing that a fair ordering protocol is is sort of picking winners and losers implicitly so now the next question is what is this notion of a payoff so a payoff represents the economic value each user represent uh gains from a particular set of transactions a user payoff is a function from the symmetric group to the reals you can decompose any payoff into this kind of sum over indicator functions and this you know some of the end results we'll talk about at the end which I'll just sort of give a preview of rely on sort of some of the decomposition properties there so what are examples of payoffs uh one example is just an amm payoff and in in this paper earlier uh we analyzed sort of how Mev payoffs for sandwich attacks look under permutations and then sort of get some bounds there the other example is liquidations and you can think of a liquidation as really an indicator function that's parametrized on the price and a threshold so if the value of the assets below some threshold that's the indicator function um then you you know you realize a profit and if it's not it's zero and you can sort of look at this as a barrier option as well so the real question is how do you choose this loss function well there's a couple of different ways to Define sort of fairness one one version of fairness is the extremes what's the difference between the best case ordering like the the one that maximizes social welfare and the worst case ordering the one that minimizes social welfare that should say best not base um the other is sort of mean so what's the deviation of a given ordering away from from sort of the average like is is a particular you know how much does do the orderings fluctuate around the average we want to choose L this loss function in such a way to be robust to perturbations and uh an interesting thing is that um you know for you know finite groups there's actually ways to generate kind of lower bounds on things like V like this value of this game that we talked about um these are uncertainty principles they're very they're sort of these finite group uncertainty principles are similar but quite different to the uncertainty principle you might have learned in in a physics class or a real analysis class um but the interesting thing is that you may you may now ask this question is there an uncertainty principle for for Mev here um and there's a very interesting you know that sort of the the final thing we'll see is that there's sort of this trade-off between how much you restrict the set of orderings and how manipulable a payoff function is and that that trade-off you you have sort of this lower bound and that's sort of this this hidden hidden cost and sort of you know complexity costs in MVP which is is the reason these Fair riding things sort of can fail uh so now let's find out how much orderings cost so the claim is suppose I have the distribution P that generates orderings and suppose that with very high probability the size of H is Omega of n factorial that means it's it's a percentage of the total number of permutations and if L is deviation from worst case so the extrema then you can show the value of the game is bounded by a positive constant which in word says if the value if the number of fair orderings is sufficiently large then this Minimax value is positive and this sort of D5 protocol will be constructed always sort of does worse when you use Fair routing another way of viewing this is fair ordering is actually discriminatory to particular protocols so you might say okay well is it really realistic that a fair ordering thing generates Omega of n factorial permutations shouldn't it just be generating like constant in the number of transactions um and where you would where this logic sort of falls apart is um if you actually look at random ordered elections or random elections and this was studied in 19th century uh by some some actually French sort of philosophers who also did social Choice Theory uh and this guy Gilbert who kind of proved this very Bizarro formula that in a random ranked election with three candidates there is a 91 chance uh that you don't have a condorcet paradox a condor say paradoxes candidate a beats candid B on a pairwise election basis can it be beats candid C on the pairwise election basis but candid C beats can to a on a pairwise election basis um and so when you have those types of Loops then you have no perfect ranked ordering and that's sort of like the simplest version of this and you know again to the credit of of Thanos and Aquarius and all these papers they admit that this is a flaw of theirs the problem is such a thing guarantees that under and with enough Randomness your set of orderings is still Omega of n factorial it's still a percentage of the the permit of the symmetric group and so that sort of says the the hypotheses here are pretty likely to happen unless your time stamp distribution is really really degenerate okay so I've given you all this bad news uh and you know a lot a lot of it involves like you know Game Theory math whatever is there anything you should be happy about and and what I would say you know maybe the the positives you should take out there are probably ways to do application specific order preferences so there are two results in this uh frame of mind um the first one which uh it released Yesterday by Theo and Guillermo is basically they looked at the kind of arbitrum first come first serve mechanism and they only looked at the top transactions so they don't care about the whole ordering they care just about the top transaction and they they were able to write a linear program to be able to bound its Gap and they were able to get a lower bound uh on the ratio of the price from um the optimal uh ordering versus the sort of like first come first server ordering and show a lower bound so that it's always worse uh under some conditions and so that's a kind of very nice simple version it doesn't capture all Mev but it kind of gets you at least you know kind of the minimum viable example and then this these papers from us um that basically construct an uncertainty principle and these uncertainty principles give you this lower bound the uncertainty principles actually show you things akin to the following uh where you can bound sort of the the worst case part of the Minimax loss by lower Bound by a polynomial in something known as the Fourier degree of the payoff function uh and also upper bound it in terms of Fourier degree so what this says is you can actually control the fairness it measured in these in sort of like the deviation from worst case um by the is related how how well you can actually achieve some fairness is related to the sort of inherent computational complexity of the payoff function specified by your program uh and you know I think that's actually you know a really interesting thing because it basically says hey there's a reason people optimize smart contract programs for being less complex they actually have less of this worst case unfair bound the upper Bound in this case so with that probably did go over time so uh we'll answer some questions in the chat we are running a bit over time um so I'll again just direct people um you can send your questions after the conference on that flashpot's Forum link um that we had posted earlier um and we'll all be checking that after but otherwise thank you very much darun I'm sure no one will ever try to do Fair ordering again or call it that um and now we're going to bring on Tom for his next talk cool hi everyone can you hear me yep great sweet um hi everyone uh my name is Tom I'm a general partner at dragonfly we're a global crypto Venture fund um we've done a number of Investments and and sort of been writing about Mev for a while we're early investors in flashbots and um you know I sort of want to walk through I I guess maybe more of a VC's perspective on what's happening in the Mev space I think um there's been a lot of discussion a lot of research put into um how do we design um fair you know order flow auctions um how do we incentivize people to um you know uh extract Med in a way that is like not malicious um and I think the the mechanism design research has been going on has been great I think that the problem of course is is how do you make sure that the market recognizes maybe the outcome that we want the the mechanism that we want um and versus uh sort of you know having the market and sort of what the market decides be sort of be sort of sub-optimal and so um when we started talking about is um sort of the different actors that are circling on different parts of that maybe stack and how we might go about sort of preventing any single actor from uh maybe seizing a part that we don't really want them to so um I think so far everything we all this sort of discussion around me deal and researcher around Mev has been sort of related to sort of the world that we live in right now where um you know users sign a transaction it hits the public mempool um Searchers uh uh uh you know CDs transactions they assemble bundles send them to Builder and then it eventually gets validated um and and I think unfortunately like that sort of state of the world is is slowly sort of changing over time um this is really good uh tweet from block native um where they basically try to drill down and figure out um well you know what percentage of total uh uh transactions that we see actually aren't going through a public mempool they're going through some other method um that sort of breaks the our mental model of the world um such as a private mempool that would sort of necessitate a change in how Med an Med Supply Chain is is being structured um and so we found it at 3.8 this is older I think it's actually much higher and I'll be talking about that a little bit later um and so you know the the problem is like we can design really great options um but if we don't have incentives to and encourage people to participate in the options they kind of end up going to waste um and so this sort of gets back to this topic of like um uh I guess block centralization and and um uh Builder centralization and um actually John has this really good uh flow chart um talking about you know so that inherent centralization and sort of positive flywheel um of uh um private order flow and you know I would argue this is already kind of happening um already as we'll get into a little bit um and I think so far again kind of the discussion in the Mev Community has so far has been uh sort of finger wagging oh this is bad oh we shouldn't you know um have private order flow we shouldn't have this sort of centralization of Builders um and instead I'd like to sort of you know think about it more in the sense of well how do we encourage and incentivize the people who maybe are privatizing order flow today to um make it public um and and sort of get back to this sort of censorship resistant um decentralized open world that we want to sort of live in um and so uh this is you know very somewhat famously classically um stephon's uh uh Mev supply chain um going from a user to wallet to search or to build a validator um but I would argue this again is like almost too abstract and this part right here is actually kind of where all the activities is happening right now in the Meb space um and one thing you might notice from the from the supply chain is that basically the further back you go in the supply chain um the more leverage that party has so um you know the the um while it actually has a lot more leverage over the Searchers and the user actually has a lot more leverage over the wallet um and so it's this isn't quite as simple as sort of how we think about um transactions historically um and so to get into this we'll sort of break down you know uh classically how does having a transaction get formed um so you have some sort of UI um they send a signature request to the wallet to the web food provider um the wall then ask the user to sign um you get one of these little dudes um you click OK um the users have the transaction um the wallet then sends this signed transaction to an RPC endpoint um and then um that enters sort of this public mempool it gets right across all these different nodes eventually you know either through the traditional means of just you know validator finding it including it in its block or a search or sending it to a builder and then getting it included in the Box it somehow gets validated and then the DAP also updates UI um but you know all these sort of different components of this of of the of this life cycle they all want to increasingly snapshot maybe for themselves and this flow sort of I sort of sort of started to break down um and so uh you know you got some memes that kind of you know describe you know what's going on a little bit it's got to get these out of the way unfortunately hmm and so um you know there's a couple different camps here and you can see why every different Camp every different stack um will sort of conceptualize why they actually think they deserve um the order flow and why they're actually trying to go after it so you know I think right now a lot of people think of order flow they think of RPC providers and there's been a lot of effort and a lot of development being put into having a sort of custom RPC provider um uh which we'll get into some examples in a little bit um I think in practice this is like a really tough really tough entry point for um new companies and new projects um a you know a lot of walls won't even let you choose your default RPC provider um this was not the case for metamaster recently um it's it's super clunky users don't really understand what's going on I think there's also even questions about how this actually monetize and how this actually gets uh paid but you know in concept one sort of way of entering the supply chain is basically um well when the wall actually sends the signed transaction to an RPC I'm not gonna just gonna send it to sort of the conventional uh you know uh you know um uh mempool I'm actually gonna you know sort of replicate what that interface looks like and then do something kind of funky with with the transaction um so one angle here is um you sort of convince people to use your RPC because you sell them on some sort of security angle so Harpy um which is a dragonfly portfolio company um is going down this route so um you can send them all of your transactions um you can set your RPC to them they intercept and scan every single transaction and depending on the level of security that you choose with them um they can either ask you for a two pack for every single transaction so they'll email you or they'll text you um and have you approve the transaction or if it's something that their simulation picks up is potentially you know uh scammy or malicious um then only then will they actually you know um you know ask you for two facts and so it's sort of this nice little add-on or module into how people use RPC today which is again just sort of uh you you blast it into the mempool and sort of hopefully the best um and I suspect this is kind of the path that like some of these other simulation tools are so kind of going to go down up basically hey I'm sort of like Norton Antivirus send me all your transactions before they hit the public event pool I'll make sure they look good and then we'll sort of send them on um another way to sort of think about this is um rpcs that go after this sort of ease of use so it's all these usability ux wins that um in aggregate just make it you know uh strictly Superior to just sending it to inferior or someone like that so um example would be gas Hawk um they uh uh that basically um try to model out gas price fluctuation and then send your transaction when gas prices are lower and and sort of save you money and sort of pocket some of the spread um it's even something like flashbots protect or the one or one inch Rabbit Hole um you know they wait to make sure that you're not paying for sales transactions you can pay in you know arbitrary fees Etc both this and the previous category in theory should be you know strictly Superior and everybody should use them just using a standard you know inferior for example RPC endpoint um but for a lot of that I mentioned earlier it's just really really hard to get people to switch over if they can't even switch over at all um and so this sort of goes back to this other option which is well instead of you know having users switch their RPC over what if we just um sell directly to Watts right because wallets again they're further up the supply chain they have more leverage um and then we'll integrate with them directly so um you know in concept this is great for wallets right It's A New monetization Path because you can sort of uh benefit from some of the Mev that your users are generating um you can Outsource the med extraction to somebody else so um you know maybe it's a bit simpler for you logistically or from engineering perspective or maybe you know users actually even get uh some of the med directly as well and sort of create stickiness and so we've seen some development down that path as well um but ultimately this all is sort of again downwind of of wallets wallets really have a lot of Leverage over um which RPC endpoint um an order is going to get sent to where transaction is going to get sent to um while it's obviously they need to monetize you not everyone can have a mirror mask swap style money printer and so they're all looking for ways to sort of think about alternate sources of monetization um we'll also have really great order flow not all order flow is created equal um right like order flow from a um bot Med or order flow that is toxic order flow from a from a professional Trader that might have some sort of alpha um you know that's not something where there might be that much maybe that you can extract um but if you're uh you know catering to people who are less price sensitive less time sensitive um that is actually a order flow that can generate a lot of Mev and therefore is is you know pound per pound more valuable than other types of transactions that you see um so again you know this is a sample for metamask where um they're even starting to pursue this method of um sort of swallowing up the RPC providers instead of having their own layer on top of them where they can you know again provide a lot of these ux Security benefits as well but also potentially extract Mev so before stuff you hits the public mempool um they can sort of do whatever they want with it um you know they can do it themselves um so yeah it's like a tweet interaction between Bert and uh Mike from from rainbow from a while ago discussing the idea of hey what if rainbow actually you know just extracted uh nav from user spots itself um or you know um uh uh partner with somebody else um or potentially auction it off which is I think how a lot of people conceptualize it the question is like you know what reason do they have to do that um and how do you sort of make those economics who actually work for wallets um you have to actually um cater them and sell them a product that they want because they have so much leverage in the ecosystem um I think the Silver Lining here is um I think uh while they're actually finding that just charging on this is one of these swap products is actually a much better monetization monetization strategy than Mev extraction um and so it might just be that you know they might sort of leave some some you know Juicy Fruit sitting on on the table instead of trying to you know extract it themselves because there's so many other uh ways for them to actually make money uh I mean again all the sort of discussion so far has been sort of presupposing the current sort of uh stack right like uh the RPC providers sort of assume that users will be able to switch their RPC endpoint um and wallets are sort of assuming that well I will actually be able to receive transactions from users um and that of course is also changing over time as we're seeing um that like sort of the conventional way people sign and create orders um is actually changing so um this whole sort of bit is is kind of what I want to drill into a little bit um so dapps really you know kick out this whole process of a user click something or tap something on a UI um and then the DAP says hey you know wallet please sign this this transaction um but the transaction itself is going to be a conventional ethereum transaction it can be an intent-based transaction right can be like a openc order or like a Xerox order um or this is the case from Sushi where you're basically signing a hash that represents your intention and then that doesn't even enter the wallet right there's no transaction to um relay that just goes back to the tap and then the DAP can do whatever they want with it so in this scenario um Sushi is a partnership with manifold where um they have this product called Sushi guard where basically uh when you swap on Sushi you sign this intent it goes to Sushi guard they can extract an EV from it um and then they can go back to Sushi so the wallet doesn't actually see the transaction in this flow and therefore the RPC providers don't even see this transaction um I think this is kind of not optimal um because it's really confusing for users you don't really know what you're signing you're signing this text which can obviously as we've seen as sort of the permit style attacks and other types of attacks um actually being you know malicious in some scenarios so people have been sort of conditioned to be on guard when they're signing a transaction um less so when they're sending a message but I'm message can you know given certain conditions also be be kind of malicious um and of course you know this also sort of depends on the DAP um so um a lot a lot of uh uh you know daps obviously we're talking about the protocol layer as well as maybe the UI layer um it really depends on what percentage of your volume or percentage of your activity is going through your first party interface um or versus third-party interfaces so if the majority of your volume is going through an aggregator or a third party or maybe just Bots um You probably don't actually have that much leverage in terms of how do transactions actually get generated whereas if you have you know a great um site or maybe you are solely vertically integrating and creating your own wallet or creating your own app um you actually have a lot more leverage in terms of uh you know again choosing um uh where orders are being routed and sort of where order flow is being generated so uh this is like a screenshot from um uh Vicky thought who's one of the founders of friction basically looking at the percentage of toxic order flow on on uniswap which you can think of as sort of a a rough proxy for the percent of volume that is going through uh or not going to the university why and instead being generated programmatically or going through some sort of third party um so I I think you know zooming out a little bit um all this is to say that um uh the the sort of the the land um underneath uh Med world is Shifting pretty rapidly um and so you know building a product that assumes that people will be able to find a transaction in a public mempool um and then you know engage in some sort of uh uh searching process for it I think is is not um probably gonna be long for this world so the question is you know how do we make it uh EV positive for these different players to um cooperate and participate in a open auction um that is fair to users as opposed to relying on their uh benevolence to um not generate malicious Med not not you know not self-extract um you don't really want a world where you know the majority of uh transactions are going through metamask metamaster self-extraction and then they'd also maybe you know uh become become a builder and it becomes all vertically integrated which is um kind of the trend that we're seeing um I think there's also a question of like you know where will that Med actually end up obviously everybody's trying to go after it um but there's also a question of you know why why does others end up with with users there are efforts to push the Meb that is being generated back to users versus going back to an RBC endpoint going back to wallet or going back to adapt um but I think that ultimately relies on competitive pressures across these different players to to do so no one would willingly do it but it's sort of like credit card points where everybody everybody else is giving some sort of Kickback you're sort of forced to do so as well even though um it sort of hurts your bottom line um and then uh you know I'm also I've a research piece coming out soon um that sort of has more data on um trying to map out and and quantify the public mempool or the private mempool problem um but if you want to collaborate uh feel free to DM or email me but I think in my mind we're sort of in a state similar where we were maybe two years ago before me the Explorer where we know there's a problem and we have some sort of little Snippets you know three different papers or through one-off research pieces but we don't really continuously monitor or track this and you know based on um some of the anecdotes as well some initial data um it seems like it's a growing problem if we want to live in a world where um you know there aren't all transactions are not just going through a private mempool effectively um so ultimately I think when we sort of look ahead the big takeaway here is the people further up the supply chain um have more power um and so um naturally everybody's trying to you know move further up um but It's Tricky given given some interests are sort of already entrenched and so um you know everybody's acting really rationally nobody is trying to be malicious here um and so we can't just really finger wag and say hey you should not be privatizing order flow you should not be self-extracting um instead we should make it economically uh reasonable and rational for them to give up their order flow and participate in an open auction um and I think um that's where I'd like to see more more sort of discussion happening versus um just sort of assuming that it will happen um I think things like maybe share are a great step um and obviously make it more accessible for people to participate in an open auction but I think ultimately question we need to make sure we're answering is um whether or not it makes sense for um these different actors to open up uh uh order flow um to to an open auction versus again sort of keeping it private so um that was it kind of quick um happy to answer any questions um if people um have them actually I don't I really don't even have uh I had to close the tabs I don't get Echo but happy to answer questions [Music] uh but thank you uh the viewers who are confused by Jordan's equations appreciated the break on this one uh and we will turn it over to quintus now who's going to talk about cats I think can can people see me can people hear me am I going now you're all good okay cool okay however I heard there's no q a uh because we're running short on time so I can say whatever I want uh so hi my name is quintus um from The Flash plus research team today I'm talking about auctions privacy and a cat and my motivation is um sort of to take this discussion on Mev and the discussion about Distributing Amud back to users and grounded in like a mechanism or like a class of mechanisms that people have been talking about uh implementing um and yeah and so the real point I'm trying to make is that the design space is bigger than we we realize and there's um much more to explore and I'll sort of keep a direction in which we could uh go further but first like a brief a brief overview of what I what I mean when I say a mechanism for distribution okay so um starting from Basics yeah the user trying to interact with the blockchain or blockchains in this case I just put ethereum because it's easy to think of um right and so the user has some intent they're trying to something they're trying to accomplish from the chain but they don't understand how the chain Works um most of us don't understand the intricacies of it and understand what the best way of of uh interacting with the like all the different smart contractors etc etc um similarly the mempool is very um the hard to to pass many different transactions in there it's pretty much only like searches and like a couple of like very like technical parties that actually I can reason about this um well in the the time frame required so the user's confused uh that's like me and you um and here the idea of auction comes in nicely I am just giving it my own definition which is a mechanism for Price Discovery through competition and then once you think about it for a little bit it doesn't have to be price Discovery it can just be Discovery you know um like in a reverse auction or something it doesn't have to be a price people are competing on and that makes a lot of sense here because uh we actually you know need some Discovery because we don't actually know how to do what we're trying to do and so these sort of beautiful end case would be to have this magical auction in between the happy user and the blockchain and solving all of our problems and that's the direction we're going um well I'm proposing we go at least right and just to be clear when people hear auctions and maybe they think of PBS usually um that's not what I'm talking about right this PBS and such the Searcher like build a relationship and all these things um that's like it takes part of what you can construct it's like one large auction where the beneficiary is a proposal but really like the um the mental model I'm working with is like a an auction that logically precedes all of this where the benefactors the is the user and of course like in implementation um this could these could be much more integrated and and you know maybe like if you really want to reason about this um perfectly then you should think of the whole system as a whole but for now we're just going to focus on the order for auction um which is the green bar on the left if you haven't caught on yet um and so just to give you like a a rough idea I took a slide from a presentation I gave in December um with like one way one taxonomy you could give um breaking up the different archetypes of auctions and like this really isn't like super rigorous and you can definitely propose a different way of dividing things up and and maybe it'll make more sense but just to give a rough idea you can have these kind of like um auctions which I just labeled here as our priori where the winner is the term and Aquarium Advance um and resembles sort of like Trad five people um these kind of things um what I've called auction house because I don't have a better name for it but you know there's not an official name and there's actually different variations where people bid either on single items right and these items are gonna usually be like users intents right there the um among other things but like it's it's the items all related to the users intents because the user's trying to execute something from chain and that's what we care about right um if you have combinatorial auctions where people can bid on um you know subsets of of uh of items um and batch auctions or so much somewhat of a continuation of this where the batch is fixed um you can't like pick and choose a subset um of course with some caveats and then Dutch auctions um and like I guess English auctions and whatnot all um you'll run over time um so anyway so this is like one way of dividing things up and I've explainable to you and thank you for listening but that's not what we're doing today um so the point I'm trying to make is that there's an argument oh so there's a dimension um that is missed in in this taxonomy um that is quite interesting um and that you know we should be thinking more about I think there's a like a large design space we haven't really explored much um and that's kind of like control over memory um what does that mean well if we think about uh that's not good um so they're supposed to be some little robots uh over there but I guess they're not showing uh I guess maybe it's because you're making me run Chrome anyway just imagine at the end of these bubbles there are a bunch of robots um uh the point I'm trying to make here is that um in an auction um in an auction the idea is to communicate that an item is being sold to as many bidders as possible because we're trying to get up the competition and of course you they need to know about the item being sold to to bid on the item but I'm going to argue that this is actually not desirable and also not the only thing that we need to do necessarily or the only thing that we can do to like you know have this discovery through competition mechanism um oops Yeah so in order to do this I'm introducing uh another character or I guess the first character in the story uh schroding is other cat right uh with half a sibling an expectation I guess um and again we have the same setting with the bidders which are now invisible but I promise they're there and um an item which is being auctioned over um but I'll introduce as well um a cat and a box and the the cat here acts as a acts as a Auctioneer and the as you can tell by its like hat and stick it is a magical cat and so the cat controls all of the actions taken in the box and more importantly it also controls the perimeter of the box so what this means is that the cat can control what information leaves and enters the Box so what can we do with this um and again you can't see it but there are a bunch of bidders outside the box I promise um and their bid is inside the box as well which you can't see um but the idea is basically that uh the cap can allow bidders to um submits like emissaries copies of themselves right if we're talking about Bots or some some version like that that um that can do the bidding on the auction for them and so the idea is that you inform all of these emissaries on the inside but not necessarily the like uh Putters on the outside what's happening in the auction if the boundary of the box is fully transparent the outcome is that at the end of the auction all of the bidders outside of the box are fully aware of um what transpired in the auction what the item was perhaps even who who won the item on what the largest bid was who knows but there's like um and at the very least they're aware that this item was auctioned off um but this may not be desirable and there's some other outcomes which we could have as well like for example we could have an outcome where only the winning bidder is aware of um the item which was auctioned off right or going even further no one is aware of of who won the item until like some other point in the future and the actions taken on the item um or or taken within the Box um no that's very like abstract so to make it a bit more concrete um let's imagine we have a we're auctioning off a big trade someone's trading a very large amount at the ship coin they're going to move the price um they're like different variations of how we could do this but let's imagine we have some sort of RFQ system where bids come in the form of like commitments to giving a certain price right um and so these Emma series within the Box um offer their different prices um and uh well what happens if the the barrier of the box is transparent well at the end of the day the winning bidder now has to execute this order um but the knowledge of this order will be executed is now like you know public or at least many entities are aware of it um and so there's competition to execute this order if they're going to move the price maybe some someone else is trying to price this in and it's not desirable or it's not like the best conditions for this for this bidder um and so they might actually want this information um they'll have the price in this competition when they give their bid knowing that when they're trying to execute this order um someone else might have already tried to price it in um if we change the the setup so that only the winner is aware um that they won the auction now they're the only one aware there's bigger one that needs to be executed and then the situation is much more favorable for them and that should mean that they are able to bid higher the auction um but let's imagine that dokwon is actually swapping on uniswap and he's asking to be background only you know that's like one of his conditions of the auction or maybe that's what the auctioneer wants um you know I'm leaving aside the question that's actually like the setup we want well in this case the cat has no control over what's actually happening outside of the auction so it's very hard to see if the bidder is actually what the bid is going to do about the auction um however if the the outside um bottle better is never made a way or it's only made a way at some point in the future um what the outcome of the auction was and the action taken on on the order like the formation of the the bundle in which the order is um executed happens inside of the box then the cat has control and we can make sure that the bidding is like you know um along the like the lines it uses utility function or whatever it is uh you know to put it like plain in this example there's no uh front running right um similarly you can think of an oracle update um you can imagine many people are lined up for liquidations and the oracles are like on the cliff we don't know if it's going to be liquidated or not um if the auction is running in the clear all of the bidders are aware of of this Oracle update whether the people being liquidated or not um but the um but only the winner is able to like background it for example right um however if we make this barrier you know um less uh less transparent somewhere else on the permeability like spectrum and only the winner is aware of this they now have incentive to bid more um for this information as well because it gives them an edge in the market they know something that their competitors don't they're competing Traders don't um right and so I'm I'm running out of time so just to briefly go over like the the kind of things that I covered here what we can do with information uh one is we can select for desirable competition competition that drives up Revenue but not competition that you know is uh detrimental to the users that yeah the users wishes or their intent right um monetization of information uh like theoracal example covers that quite clearly I think and then like control over action taken on information right if we have control over what happens inside of the box but not outside um if we can keep sensitive information within the box then um we have um we can stop things that which we don't want from happening which we don't want to happen from happening right um and I get a very specific instantiation of of like The Cat In The Box um but actually like the point I'm trying to make is that they you know we can combine many boxes we can combine many cats um you can get dogs you can get triangles you can go crazy um my point is that there's like a lot we can do with privacy that um and like how information flows and how we can draw things um the cryptography and distributed systems have allowed us to do and we haven't really like I don't think we've like exploded full design space and so it'd be interesting to see people come up with um yeah so that's why I always say it's not if you know you know you should ask yourself what action can I take on what information um and that's pretty much it um over time I think and so just highlighting a special course a lot of homeless cats out there do your best to donate or adopt cool I think that that's it um there we go awesome thank you uh we're still waiting for Danny to join in just a minute yes so you get you probably get one or two uh one of them that was asked is um how do you control information access here um without leaking uh yeah so I think that actually the answer which um I guess so here's like one very basic example but you could just have a bunch of people submitting scripts which are like encode their bids right you take as input the item you give as output the the bit and now they're running the server um and you know some trusted party calculates the bids for these people I was like one way of doing it um a lot of trust assumptions there and then you can scale up um like flashbots exploring the sgx route um either by like one like you know Central uh sgx I don't you know I'm not a photographer I'm not going to talk about the across assumptions there um but then um you know moving outside of that maybe you can have multiple parties running these as checks so maybe you can have the bidders themselves running at different instances um yeah that that's I'll leave you on today before I say something that doesn't make sense well that's perfect timing uh Danning just joined in now uh so thank you again and we will go over to the next one thank you thanks everyone and next we have Danning who's going to be talking about uh quantification of Mev sharing design um and Order flow options hey everyone um just give me one second to find the right link for okay great cool my sharing screen oh okay so hi everyone my name is Danny I'm a data scientist at Xerox and we are a Dax aggregator platform where basically we handle like users intent to trade across decks and access the am liquidity which is where Mev happens across the decks or me order flow and that that's why this is like very interesting um how to say topic for us and this presentation is more of like an open research in collaboration with flashbots and a lot of data are coming from flashbots and also I can find so shout out to them and so for my agenda today I want to share more like about um first of all what's a Mev sharing workflow look like um let me check the chat okay um how does the workflow look like and what are the rows there and what are the values there that we try to identify first of all and then it's like how is the Meb Revenue spell it across each row today and we will review some numbers that can potentially have an overlap with what Ellen shared earlier so I'll go quick there and then next is that looking at this funnel of this Revenue spill it where can we potentially quantify and extract the user share back um potentially and we will share two approach here today one is by looking at the Searchers on gym profits the other one is by looking at the validators bit Surplus or sorry this is actually a type of Builder's bid Surplus so um this is a how to say like a diagram of common workflow of OFA today that we learned about um so basically we have this user have an intent behind a dab or a wallet or you can say also RPC provider can be also the entry point and then they want to make a trade that's accessing amm liquidity and then there potentially will be this OFA or like a Meb share service potentially we can call it Matchmaker or can or solver or resolver and then they will send to a pool of Searchers and the Searchers will try to bid for this transaction to basically send a bundle with bids and so to win this transaction and if there is no um search or respond with this transaction potentially this service will just fall back it uh back to the mempool so to make sure the execution quality um and then once the Amity share of service figure out which is the highest abundance which is the highest debate for this transaction from one of the Searchers they will send bundles towards block Builders and then there is this normal flow of block Builders waiting for a block towards the validators through a mini boost so um there's a bunch of like variables or like numbers we need to identify first so first of all there is this now we're talking about how much potentially revenue for user can be from this whole Meb flow and potentially in the future we will see probably a movie share services as a middle layer will take potentially a fee as of Bachelor of Revenue and then we will see revenue from searches that kept to themselves and for this part potentially we can see some of them that's on chain but a lot of them can be potentially unknown or off-chain like for example like across centralized exchange index and then next is like how much revenue Builders will keep and then we will see relay but most of them basically as of today there is no revenue for them and then there will be the revenue for validators so to summarize this whole thing into like one table we have see this like table here basically to summarize all the rows and um it's sort of like a comparison of like how it was in the past and then right now with me boost as present and potentially in the future with mu boost and also OFA sharing design in the future um there you see like more and more people are taking this piece of cake and we will see how we can potentially split further and this is on top is a simplified how to say workflow that's from the order flow auction article from Frontier research so um one thing very clear here is that basically searching now is how to say um segmented or separated from directly accessing wallet or Builders but um through the order flow auction um yeah um so to look at the numbers or basically trying to quantify the scale or total values we want to look at this funnel chart here that I'm trying to propose for framing all this thing um so first of all What's the total Mev here um potentially there is unknown part that option we won't see ever um that's that's kept to Searchers and then the visible Parts own chain potentially we can see is what we'll call it like on-chem profits from the revenue uh for Searchers and then after that the next step in the funnel is how much they given out um to Builder and validators all together we will basically have that data from mme boost proposer payouts rewards uh data and then the next is after the Builder layer there's another another layer of bidding markets which is bidding for Block the previous one was for bidding for Bondo and the next is so uh we don't know for sure how much Builders want to keep or given out but as of today it seems like most of the builders will be competing to argue most of the rewards to validators and potentially there will be could be a extra amount that's outside of this whole Mev value that can be coming from Builder as a subsidy um so this is a founder of how this value actually is spill it uh step by step or extract it step by step um so let's look at how big are these bars are today um so quick revisit on some numbers that um uh Elian mentioned earlier and this is also a call out to her slides here which is basically emphasizing again that we will not have a way to pretty much know how much is happening or centralized exchange or decks um what we can see is how much on decks and uh across tax platforms and centralized to centralized exchange is also another sort of Meb but it this is outside of the scope of our discussion today because we're talking about how much we can potentially give back for Dex users and the centralized sanctions change Mev is and evolved with any intent from the Dex users so yeah um next data is coming from eigenfi and this is a pretty nice breakdown of different type of Meb we can see that like on this scale we can see roughly like around um daily of 50k to 100K of Mev only chain profits that can be attributed to the searcher's addresses and apparently this is definitely will be under indexed uh compared to what's really happening in the reality because um based on all the heuristics um it can only index for example like Atomic um so there there can be a lot more to be on uh to be indexed but next is like so we want to look at how is the stability across Builder and validator and it could be pretty tricky because of different type of behavior patterns so Builders so I think a normal flow as how we are looking at on the data of the Builder's behavior is that a lot of times Searchers will put um the coinbase transfer which is basically a payout of the Meb address to builders and then Builder as the person who is basically the fee recipient on the Block is like um basically the fee recipient of Gatsby's and then basically Builders can potentially collect all of them but then decide how much of them out of all the total they want to transfer to validators so we will see a transfer usually at the end of the blog which is the payouts Builder goes to validators um so with this approach uh we can potential index but there are some sort of like different like Jose um behaviors for Ensemble of the builders for example we could see some zero profit Builders like they just don't take anything and usually sometimes Searchers can also skip Builders they just put the values via recipient directly in the transfer in the bundle and Builder will also sometimes just put the free recipient of the blog as the validators address so in this case Builder is not even shown in those block production but actually they were actually you know handling the block there could be also Builders that's actually subsidizing um for example they they can just give the gas fee um set it to um a proposers fee for recipient address and even extra transfer at the end of the block um so yeah um and so I I was gonna say basically it's pretty tricky to spill it the um how to say the spell it between Builder and bad leaders so we will basically just look at them together as a whole and look at the scale um so this is like just like generally when we talk about like a daily total of how much proposer Revenue look like and it seems like there was some interesting um difference uh compared to end of last year which is the first three months of um I mean boost alive and then also the first three months of this year um The Daily was around 600k and now it jumps to um like a 1 million per day um and then I did some cross check with the data on I maybe boost um it seems like they also have this same scale shown um and then so using another piece of data from Mev inspect where we can clearly see how much is a um transfer out is versus how much they keep um it's also interesting to see like this clear time um timestamp cut off where like in the last uh three months of last year um the spirit was more of like a 70 to the Searcher and 30 to the um proposers uh as a whole and then starting from beginning of this year it totally changed to another like basically shifted towards um much less for Searchers but much more for Builder and proposal together um so these are some basic numbers um that we know as of today um next question is like okay now as all the application teams and wallets who have the direct access of users autoflow um thinking about entering and potentially take more autonomy on user's order flow where and how much like where can we even extract users Kickback um and where how much would that be um so thinking around these questions I think there's a few things to point out um that can be potentially the data that's also accessible that for us to be to be able to quantify um looking at this flow again one thing we can know is that like when the order flow option take the order information from the application team or the users they will be able to also know maybe how much the user already requests or they want to be shared back so user can potentially share a request of how much they want to be shared back and deliver that information to the build the bidders or basically searches here and so to see how you know this whole thing play out um so with one thing they can account is basically if they are able to see all the online profits from The Searchers then they can potentially even say hey I'm an exclusive order flow so like it's only how to say A Plus value for you so can I just get like 99 back it's possible um so we're estimating more like an upper bound here um and then the next place where we can see actually some sort of number that we know for sure that can be potentially enforced is at the um block bidding Market which is when the block Builders are bidding across competition to get validators get their blocks and maybe boost actually have this open data set from the relay that they know what's the submission of all the block Builders look like and one thing we one thing we can know is that like basically the first bidder and the second highest bidder the difference we can call it potentially as like a bidding Surplus where if we subtract that part the top Builder is still the highest value validator can access and they will still accept the block and so that's a potential you know a value that we can basically cut and give back to users um so to how to say to fit the ideas into this funnel chart we can basically see like one thing is OFA infra can potentially allow users to request some share that's visible on chain when they are giving out the order flow information the other is like I mean we share uh infra oh sorry that's actually I mean boost infra can basically cut and return the surplus of the high speed that's above the second highest um so yeah um so now we have these two approach to potentially quantify how much and maybe we share can come back for user um first thing let's look at this search engine profits data it doesn't really matter the scale here because as we said it can it can potentially be under indexed as well um but when we look at the margin on all these um how to say Arbitrage profits out of the total trading volume we actually are able to quantify like say how much how profitable per Mev Mev attacked transaction or Mev victim transaction can be or basically how much Mev can be extracted on a margin like bips based per transaction and we can see actually like it's interesting to see up trash in general is can be potentially more uh let's say higher margin than sandwiching and but only like a median stats but then when we're looking at like about the higher Bond of 95 percentile and 99 percentile there is seems to be coming into the same Trend and these numbers can be like huge variants for how to say um for example like the median looks like around its three Pips and 95 Tower is around a few hundred Pips and then 99 percentile which is basically the upper Bond or like the max number is around like a thousand ish and then if we look at the breakdown of the margin distribution we actually could see like at least clearly half of the trades that were attacked by basically Mev um Bots can potentially produce zero to five which is like we know the median is three and so in fall into this bucket around like five Pips at most um and so Tina asked us to post our number here so I'm just putting some both numbers here um if we think about concretely say now the wallets are actually entering the game and based on their base volume per month if we look at this like two biggest players um for example if you have like around 200 million volume per month um user can potentially expect a total of like say 100K on based on the five Pips Point data set um and potentially maybe 2 million as like a really huge like upper bound of upper bound kind of estimate using the 100 bips that we saw on the 55 95 percentile and if you're a Jose platform has like a 1.2 billion volume per month um then the number can go up to like 600k um on five bips estimate and 12 million or 100 bips like bold estimate so um but caveat here is that this is like a totally an upper bound because it's not all the base trading volume will be maybe extractable because like say it's like super small trade of like two dollars and I don't think that can really produce like even five bits of Mev value so yeah this is the upper bump um the second approach we want to check is basically the block bidding Surplus and on the right side this diagram shows like what that means basically on each slot or like per block all the builders um they have like the the latest bid and um we will check like what's the highest paid per across the builders and what's the second highest bit Builder and then the difference between them as we described earlier is considered as like a bidding Surplus basically if you extract any percentage out from the high speed it will still be the high speed and validators had no other choice but to accept it as it's still providing the highest value for them so um this is a methodology we will we will check the fresh freshest bid from each Builder and then take the highest two bids and take the uh how to say the difference and that's like a estimation of the Surplus for users Kickback and then um caveats is that it's definitely a lower Bound for example um why we could say like why not just take a higher percentage of all the payouts of for the proposer and second of all is like it's probably a very practical and enforceable portion um that can be implemented through any boost um so yeah I'm trying to look at like how much is that babe Surplus look like um so I've encountered some limitations on working on this data set because it's too big so I have to actually take only uh every time a slice of data of like a day um but then like I checked across time frame and it seems to give like a pretty consistent number here so averagely actually uh we can see like per block the reshare estimate which is basically the Surplus amount between the first and second bid is around 0.003 East which is five dollars that as the price of today um and the median can vary how does it vary around like a dollar ish um and then if we look at like a hundred Jose a thousand blocks it will be around two e's and then if we just do some paper math it will be like 25k per day um it seems like not that you know exciting but it's again it's like a lower Bond estimate um another way to look at this is basically okay what super percentage of this Surplus outside of the you know the total amount of the high speed which leaves the answer around like a three percent ish or like a two percent to five percent um Range uh seems like out of the total bidding amount um so yeah it seems like they're giving us a sort of like a pretty consistent um result um so what do you what what does that mean here so if you want to put some concrete number here if we look at like a DEX trading volume um total flow and say just quantifying by the percentage of the Dex market share you earn um say if we look at the total bit Surplus per month basically across all the blocks in that month the total will total up to like 750k and if you own five percent of the Dex market share you will get like 37.5 okay and if you own like 50 it will be having having like 300 uh 375k um again it's a lower Bond it's the enforceable part that's coming from the block beating layer um so putting them all together as a full picture um is um this is what I'm like thinking in my head or to share here um when we look at that and maybe a revenue split chart again we saw that shifting of the 30 for uh proposal to 70 of today and if we just put that five percent of surplus bidding um as a perfect part we kind of like can spill it then further into um the numbers that we can quantify here um you will see that the bidding Surplus as a lower bound estimate for daily will be 25k and then monthly will be like 750k and if we really be really um and then on the Builder proposal Revenue part I put like a red X percent question so that is like the part that we don't know how much we can request but potentially you can put number there um but then the search on own chain profits part is like um through the OFA design we know that users can already specify how much percent they want to give it back so so that's like a green X percent Mark there I put um if we total them all together as the only chain visible and maybe Revenue you can see a daily of 1.4 million as a upper Bond um and then a monthly total of 42 million as an upper Bond nice number um and then another thing to point out is definitely the iceberg part that's below the water or basically off chain um we will never know how much that will be um I think there are some assimilation um potential um sort of how to say service that can potentially round across a centralization centralizing exchange versus Dex and so to tell you potentially how much that is um but I I don't think that data is like going to be very widely accessible and near term um so I put a question mark there and one thing to keep in mind is that like potentially with this OFA Design coming up this um how to say Iceberg percentage versus the one that's above the water can be changed changing as well like shifting up and down um so yeah that's the full picture um of the numbers that I I've studied on this topic um and last notes is thanks to I can find stata flashbots data and Tina elen Sarah shinyu and Robert and Tom for the help um this Meme here I just want to show that like I think from the users perspectives like this is our Meb um we want to get a percentage back potentially because we are the value originator um thank you love it thank you so much um again we're not going to have time for questions so we are going to move right on to the next presentation um we're going to have shitija talking about staking lending and Meb excellent I will just uh can you see what I'm sure yep you're good now uh great so uh yeah so today I wanted to talk about uh sort of this idea of Mev redistribution uh and what it means for the economic incentives of validators I wanted to present sort of a very simple model that might help us think about uh where this mbv Revenue should go and and and how it should be redistributed to validators so roughly the uh you know the the model is that we have served these validator pools or you know sets of validators that are facing uh different sources of return you know from where from uh where they can earn money uh for example they can uh you can they can earn rewards from staking they can uh they can participate in lending of course there's Med which is sort of the topic of this uh of this conference uh they can engage in uh sticking derivatives there's this whole variety of of places that they can collect returns from and the frame that I want to sort of uh have for this talk is that they're going to view uh how much they participate in each of these different sources uh as a portfolio so they're going to hold a portfolio over these different sources of uh of return uh and in effect what this means is that there's competition between these different sources uh and so this is not a new idea this is sort of going back to uh sort of work on uh a security of proof of stake protocols maybe all the way back to 2018 or 2019. uh but there's this idea that uh these different sources of returns are competing uh with each other uh based on the portfolios that uh that these validators are constructing but sort of the the really important thing to note here is that one of these sources of returns is not like the others uh because staking uh serves sort of a dual purpose that in that it's uh it's it's a source of return for the validators but it's also uh securing the chain uh it's a you know the uh staking sort of acts as a privileged source of return because it controls the economic security of the underlying chain and at the same time as competing with other sources of appearance for the validators so I always want to have this frame in mind that uh we have this portfolio but sort of one part of this portfolio uh is important in a way that the other parts are not because it's it's uh it's a security change and uh yeah so there's this question which is can we use uh Mev as an incentive to in in some sense improve uh the security of the chain or you know other you know does it does it error that we might have uh and can mvv act as some kind of an economic lever to induce security uh and can it be used as a way to get validators to avoid some kind of some bad economic equilibria that might arise so we want to answer this question uh and uh so we should just set some terminologies we're going to have sort of this very standard model of of a proof of State protocol which which is going to hold some some token Supply denoted by sft uh it's going to have some uh reward inflation schedule given by rft and then it's going to hold some kind of uh Frozen liquidity you might think of this as a liquidity that's either protocol owned if this Epsilon is positive or it's burned if Epsilon is negative uh and then you have a very simple sort of uh update for how the supply changes over time which is just you take the supply last time step you add however much reward got inflated uh and then you uh add or subtract the protocol or burn liquidity and the perspective we're going to have from the validator the validator side is that again they're viewing this as a portfolio optimization problem right they're they're thinking about I can collect returns from all of these different sources what should I do uh this is again a sort of a well studied problem they're going to do some kind of a mean variance trade-off right they're going to look at the sources of return that they can get so suppose those sources of return are I can earn from staking or I can earn from lending uh they're going to compute a mean return for each of those you know each of those sorts of course this could include other things like derivatives if they're if they're engaging in those um and then they're going to trade it off against trade off the return against the uh variance of these of these assets uh and then here of course you have a risk parameter that tells you how you want to trade these two off uh but the point is that this is a uh this is an optimization problem that you can solve if you have good estimates of the mean returns and of the variances sort of the important thing here is how does this how does this choice of portfolio actually update the returns that that uh that you're accruing from staking lending MVD Etc uh so here's a simple sort of model uh that's been uh uh we've been looking at recently which is uh that every validator is going to have some staking yield that's going to come from two sources so one source is just the natural uh inflation of the rewards and another source which I think is of interest uh you know for uh for this event is is what we're calling sort of this Mev bonus okay this is like any additional value that comes from the application layer uh that is then redistributed back to the validators and this is going to be redistributed back in sort of a pro out of fashion relative to the portfolios that they're holding so I look at uh all of the rewards that come from uh uh just from the inflation of the token I look at all of the med that I'm trying to redistribute and I send it back to the validators proportional to their portfolios and then you have a uh analogous sort of lending yield so you have yield that just comes from uh from your you know favorite lending protocol which takes on sort of this uh form that depends on the utilization so you look at how much uh borrowed demand there is and then you uh scale It Up by sort of some risk-free rate and you update the uh the The Lending yield at time T sort of according and so these are some are some sort of uh staking lending dynamics that you can try to analyze of course uh here I'm just modeling sort of the deterministic part of these Dynamics I'm not actually modeling uh the randomness uh but you can sort of add that in and what we're trying to understand is how do the portfolio decisions of these validators uh namely by solving that that optimization problem in the last line uh and the fact that you have some control over this parameter of how much Med you want to redistribute back into the system how does that affect the incentives here uh and in particular does it help us avoid some bad uh outcomes that we might otherwise you know have if we didn't do this if we didn't redistribute this this evening back so let me just sort of do the first the first part which is it's it's been known since sort of around 2019 and 2020 that staking which is this privileged sort of source of return that is securing the chain is competing with other sources of returns like lending and derivatives so uh in particular uh this this paper in 2019 uh uh by tarun showed that if you want to avoid uh sort of uh staking uh percentages going very low or below some threshold you really need uh these inflationary block reward schedules in order to achieve this if you just have sort of constant block rewards uh and certainly if you have uh deflationary reward schedules uh then you're going to end up in situations where uh all of this uh all of the validators are roughly you know moving to uh to lending and you no one's taking so the the security of the network sort of breaks down uh and this implies that sort of proof of State protocols need to actually incentivize validators to keep staking and avoid looking for other sources of returns uh they might look for them of course they might look for them off chain but we're not modeling that right now we're just modeling sort of how much they can get from activities that we can we can actually monitor and this is also uh turns out to be true for uh staking versus uh sort of uh derivatives uh turns out that there's certain kinds of face transitions where the validators are uh the the derivatives are in fact parasitic on the on the on the proof of stick system and so this sort of sets the stage for well if we want to solve this of course one one way to solve this is just to inflate the rewards and then just you know keep uh uh keep you know validators taking that way but we have sort of another lever available to us now uh which is uh Mev uh and the question is can this Mev bonus in fact act as a way of bypassing this problem of uh having inflationary reward schedules or having to inflate your token too much in order to secure uh in order to secure the network and so the question is how does this you know sort of the mathematical question is how do these Dynamics sort of depend on this parameter Alpha and on this uh schedule uh on this med extraction schedule queue so one assumption that you can try to make is that this Q this this Mev extraction schedule is in some sense dependent on the lending on the yields that you're getting from lending and why do what is the reason to believe this might be the case well because lending yields are roughly correlated with the rates at which liquidations are happening uh and you might want to say something like nav in the future is is going to be correlated with the number of liquidations that are happening in the past you can sort of try to put these modeling assumptions and actually if this this did turn out to be the case during say the terror collapse in 2022. but we want to understand how these Dynamics behave as a function of this Alpha and one can do sort of similar kinds of experiments and sort of theory uh but the upshot here is that you can show that if this Alpha is sufficiently large if you're if you're actually doing this redistribution back to validators you can avoid having to do this inflationary sort of Highly inflationary reward schedule and in fact you can get away with things like constant uh inflation and even in some cases deflationary rewards goes to something like a Bitcoin reward schedule uh which uh the staking sort of uh and lending papers are for 2019 set would not work so if basically validators are allowed to be incentivized to state from sort of this application layer value that's collected by the protocol then you can sort of massively reduce the requirements for uh necessary to incentivize validators to stake and this is sort of uh very interesting because uh otherwise you would have to you know you would have to find some way of keeping them uh keep keep keeping them engaged and trying to maintain security so what I want to say now is that Mev redistribution sort can act as an incentive mechanism to prevent people from uh from for pulling stake away from uh from from the network and in some senses acting uh as a mechanism more generally uh uh to incentivize can be used as a as an a mechanism to incentivize good behavior in proof-of-stake protocols so usually we think of Med as sort of this parasitic uh sort of thing but it it has these use cases as trying to incentivize uh staking for example but of course this is not the end of the story right because uh all I've said is that if you if you redistribute this thing back it might sort of help you know help secure the network uh but the way that you do the redistribution and sort of the mechanics of this redistribution actually matter matter a Time uh so uh you know it's it was noted that if you have these validator pools that are uh that are uh doing this uh smoothing or redistribution where they're uh paying out immediate rewards uh in some proportion to the amount that everyone has staked this can actually reduce the variance of the returns that validators are collecting uh and uh so basically the smoothing is is allowing validators to have some kind of predictable recurring revenues uh and this also because if your calls are the way we'd set up this portfolio optimization problem they're trading office reward uh it might sort of have uh it might cause validators to have less propensity to shift their portfolio allocations radically or they they might have more stable allocations over time and this might be a desired for various reasons uh not including you know just you know you want you don't want them to move around their stake too much so there's this predictability problem which is you know how do you how do you how do you do uh redistribution to reduce variance but uh I now just at the end just want to end on some uh future directions that uh redistribution sort of uh needs to think about or redistribution research needs to think about which is the first one that immediately comes to mind is what are well what are the centralizing Tendencies of this redistribution if I'm if I'm taking this this Revenue that's uh that's being collected by the protocol I'm just passing it back to the validators does it set up some kind of uh bad feedback loops that uh sort of uh end up centralizing stake uh or end of centralizing you know a certain validator rules and I think this is a very important problem that uh I think should be studied a lot more and then the next kind of class of questions uh that I uh I was thinking about for future future directions is how can we use sort of these existing transaction fee mechanisms uh that that are already present uh in perfect Networks they could be that so they can be modified to implement redistribution and practice and why does this matter well you need if you're doing this redistribution you need actually some way to ensure things like off-chain Agreements are not happening uh off chain agreements between uh validators the sales and or validators and uh and you know and and other parties uh and sort of this is roughly the question like what is the Cooperative game theory of validator pools do they have an incentive to actually stay in the pool and collect these Rewards or do they have an incentive to defect uh and collect rewards in some other way and not do the redistribution so I think uh uh the game theory is sort of the Cooperative game theory of these validator bosses uh is sort of relatively understudied and it would be very interesting to see how this redistribution actually affects people's propensity to participate in the in these polls and then broadly uh can we prove things like uh sort of the closed loop incentive compatibility of this mechanism that I'm going to do this redistribution after already collecting Mev revenue from uh from users uh what is sort of the closed loop incentive structure of this of this process uh I think that would also be very exciting to study uh so yeah so roughly what I wanted to end on is Mev redistribution can act as a way of incentivizing good behavior at least from a security from an economic security perspective uh improve the state protocols uh and can sort of allow you to avoid these really bad inflationary sort of exponential inflationary reward schedules and make them sort of better uh and perhaps even constant or deflationary in certain environments um however this comes at a pretty you know potentially comes at a big cost which is you might end up you know centralizing uh uh the validators and also there might be other kinds of game theoretic phenomena that are happening like off-chain agreements between validator pools or between members of a validator pool to defect from the redistribution that they're uh earning so with that I will uh just end here and if there's any questions or if we have time for questions awesome thank you so much um we don't have any extra time for questions so we are going to move on now to our final panel um we've got a stacked panel for the last one we've got Eric buddish Tyrone Phil and the other Michael Jordan who will be talking about latency for this one thanks John um hey everybody I'm Michael Jordan no relation in relation to the Berkeley Professor basketball player or uh actor and so uh my parents just have a sick sense of humor we're lucky to have three awesome people on this panel today Troon Phil and Eric rudish I think Turin and Phil have spoken to this you have a context but I know enough people know but Eric is kind of the Michael Jordan of hft latency and Market design uh you wrote an incredible paper that I highly recommend and we will crib a lot of this out of um but I wanted to start on this concept of eliminating the latency Wars uh Eric I'd love to hear the intuition of of how you started thinking about this problem that the latency Wars is not in the execution it's not in the trade tape it's actually hidden and that the first step is to understanding it if seeing it so um I wanted to start there yeah great and and no one's ever called me Michael Jordan of anything before so I'll I'll take it um I think what got me started on this intellectual Journey was almost 15 years ago at this point but was reading about some of the very early investments in uh in speed technology and fast connections from one geography to another geography like the Chicago New York uh high-speed fiber optic cable that's become kind of famous um and and genuinely not understanding uh how such a small amount of time could be worth so much money we're talking fractions of the time it takes to Blink your eye what what gave my collaborators and I the idea for the the paper you're referencing that used message data and this is the the work with the UK Financial conduct Authority with researchers there was once we once I figured out in that earlier work how to think about why milliseconds or really like millions of seconds or finer are are valuable in modern financial markets you realize a lot of a lot of the value is around race-based Arbitrage is around arbitrages that are sometimes very simple to the naked eye but the comp the the complexity is capturing them quickly um and then realizing that well geez traditional Financial Market data doesn't actually let you see races that's part of why it took some Theory to uncover this this issue is you can't actually see that Phil tarun Eric and Michael Jordan are all racing for the same Arbitrage opportunity within uh a sub thousandth of a second of each other so what we got in that England study in that in that UK Financial conduct Authority study was the the full back and forth message traffic between Market participants and The Exchange in this case the London Stock Exchange so this is just very cool kind of data that no one had ever studied before that lets you see um in its in all its Glory just just trading races so you can't count them up how off how frequent are they how much money is at stake uh so it was a just a a multi-year um uh multi-year project and I was at the I'm sorry to be a little rambly but the thing that was most striking to me in that study I I had a prior that a lot of races would be for small amounts of money and they're actually smaller than I would have guessed but the the the volume of races was just astonishing it was like 20 of all trading volume um and this is in the UK equities Market uh was trading races the modal race the difference in time between Phil and tarun would be five millionths of a second in in crypto trading I I would I would get I don't have the data I'd love to speculate with this group what kind of data you'd find but my prior is it could be even higher just because the the the the shenanigans you can do with reordering time in crypto markets Phil and I are going to get philosophical about time that's promised uh it's just it's it's even more Stark than in traditional Finance so it's a long-winded answer to a great question let me stop there yeah well I wanted um maybe let tarun take a stab at a translating between the the message and execution tape of a market to what we see in crypto what's what's lit and what's unlit like where where are we seeing evidence of the the burden that these latency games exist and where they they stress the systems I think um recently we saw this with arbitrary sequence there was a place that I think they had the flag that this was happening but no one was really aware of it other than people running the infra so I would love to hear turn take a stab at translating Eric's message analogy into the crypto world yeah I mean I think obviously I think the Solana World probably is where we saw the most uh latency games obviously they made particular choices and how their protocol works such that it prioritized you know low block times and kind of effectively forcing all the validators to be in hetzner or some very like high-end Data Center um I think the on-chain stuff is still less than um lit U.S exchanges or your ex by like maybe three to four orders of magnitude in terms of message volume um but you know I think it definitely can get there uh the centralized crypto exchanges are a little bit more interesting because it's um you know most of them are actually Cloud hosted they're not actually hosted in on-prem like on a fixed Data Center and so you have all these games of people like starting 500 nodes at a particular AWS Zone pinging the the um matching engine endpoint seeing which node would have the fastest relay time and then submitting your transaction which is very similar to what happened in the arbitrary case but the um the the this type of uh cloud-based endpoint is actually ironically the way the normal Market is working so most exchanges are actually moving away from on-prem hosted data centers except like CME because basically Futures Trading is much more latency sensitive in the US equities trading people already internalize all the order flow so it's sort of like this like very weird not as latency sensitive like like it started very latency sensitive and then once wholesalers bought all the order flow it became like way less latency sensitive so um the long story short is I think on-chain stuff is still a bit far I mean you can do a lot more things but it's less about pure latency games it's a little more like online ad auctions where yes there's like a pretty frequent auction frequency like tens of milliseconds um but the dimensionality of the objects you're bidding on those very high relative to to other games yeah Phil I'll let you respond to either anything that uh Eric return said but I would like to add a question which is you're designing a lot of these systems and I know a founding tenant of flashbot is eliminating um such that we can understand what's actually happening and so how do you kind of think about elimination when you're designing some of these ideas like the fact that you what pieces of data do you need to be able to see and what piece of data do you not want to be able to see such that we understand really where these games are being played yeah no great question so maybe I want to riff a few things that have been said which are just kind of random trolling slash high level fuel on the fire here um agree with a lot of the points that have been made so far um I think the interesting thing to me one point that hasn't been made is like latency games can kind of be roughly divided into two categories one that are kinds of uh Winner Takes all kind of race style Arbitrage games where you have a market structure that incentivizes along like a certain axis of economic activity low latency and then kind of determines outcomes of like kind of rent games based on on this axis so this is kind of the most extreme form of latency game it's what we see in Trad Phi and it's uh you know what Eric's paper talks about uh mostly there are a lot of other more subtle kind of latency games and interactions even in kind of the solutions that are proposed to the latency games like batch options so in Eric's paper there there's kind of like an analysis in the discussion of the latency games that happen at the end of the interval for for each batch which still exists the argument being that like the economic value of like those latency games at the end of the interval is is much less than like this extreme kind of Winner Takes all um form of game so I think when we're in crypto it's really important to think about is specifically what type of latency gain and what type of latency activity are you are you optimizing for I think the winner takes all race model is by far the most centralizing because it means like physical infrastructure participants that have microwave Towers like take all the arbs and we've seen in tradfi and we know the market dynamics of that market are super centralized um so riffing then onto where this shows up in crypto I think the case that this shows up the most in crypto is two places today number one is if you have a first come first serve sequencer um so if you have a first come first serve sequencer there are kind of two options either you have low fees or you have high fees um so if you have super low fees like it's basically free to make a transaction then your zero-sum Arbitrage game moves into spam basically um at that point latency is not actually profitable because if you just Spam you'll always beat someone who sends like an actual latency sensitive transaction because you've sent like an uninformed packet like a minute ago and that spam will just be like ahead of them in the queue so what that does is it pushes more ARB Logic on chain and pushes the like actual economic auction into into like the spam externality bucket on the other hand if the fees are high doing the spam is just unprofitable so you'll never actually do it and that's when you see like these latency participants have like the ARB Edge from like reacting first and this is what you see in sequencers like like arbitrum for example which have like relatively High thieves compared to something like polygon um I believe it was where we saw more of a Spam attack or Avalanche which also had very low fees we saw spam attacks and then raise their fees in solid latency games so this is like kind of the trade-off landscape higher low fees spam spam or latency um but I do think it's important to to also make that zero-sum distinction we don't see it a lot in eth because of the size of the batch because eth has been kind of intentionally designed to allow for This Global slow Mev auction underlying its block that being said there's still an edge game so in the in the relays our latency game article or whatever it really like quantifies this Edge game which is again important to note very different from like a winner takes all game that you see in traffic so that's my full rant and that's where we see them oh one more thing so that's one place which is fcfs sequencers the second place is sexist which has already been mentioned by tarun I think there's a fun corollary here which is in the cross domain Mev work and what that work says is basically like the bad drives out the good in these markets so if you have super latency sensitive systems that are very centralizing they actually also impose centralizing pressure on decentralized systems because they add latency edges to whatever latency game is there through cross domain Mev fun fact we already see this with centralized exchanges so there are certain you know entities that have very efficient routing between centralized exchanges and already play that latency game and because of for example where our infrastructure is on like a cloud service provider having some overlap there we're already seeing Even in our decentralized systems or or permissionless Mev markets like this this latency competitive pressure from existing Trad fire routes so it's not like a theoretical issue it does kind of creep into decentralized systems how do we think about Illuminating it do research do data have conversations and keep kind of putting out philosophical pieces so I have one on Geographic decentralization I really want to plug I think that's like an important piece of the latency discussion um and kind of trying to come up with definitions and data around that as we speak so sorry that was a very long ramble but you know all the fuel I have to add to that fire the funny thing is the common thread of all three Rambles was this um non-agreement on the nature of time you know we have discrete time continuous time we have a relative time we have absolute time uh and Eric has this incredible point about the the the challenge of discrete time is that you always have a relative Edge there is there or continuous time trading and so I wanted to start there when it comes to time Eric I'll let you start on your what you observed in traditional markets between continuous and discrete time and then also feel free to comment on what we see in these systems that are designed without the kind of concept of relevant excuse me relative time which is actually the underlying latency games across different data centers and other things we see yeah great let me let me answer that question and also I want to respond to something yeah please Phil said that I think it's really terrific that I want to Riff on so the the in a continuous time Market there if if there are Arbitrage opportunities uh in a continuous time Market that will vary naturally incentivize competition on speeds that's a very relativistic kind of competition competition to be faster than uh faster than uh the next guy um in this in this empirical study with the FCA researchers who Quantified it on the order of millions of seconds of time difference between winners and losers actually in a chunk of races about four percent of races this bill will get kick out of this the difference in time between the winner and loser of the race was negative because it's like you show up at two you know two different chips and my chip happens to process a little bit faster than yours uh so I win the race even though I kind of got to the Finish Line uh after you um so there's a tendency towards uh relativistic relative competition it's about relative time rather than absolute time being important so we're fighting over tinier and tinier and tinier slivers of time um I like Phil's point that that that can have a tendency towards winner take all competition uh we do find that in the empirical study the the top three firms in our data I don't know the identities of the firms to be clear that's that's Anonymous uh but we can we can link firms over over over the data in the top three firms who win over 50 percent of races the top six firms win about 85 percent of races so you're relatively uh centralized to to use the terminology here um the the so the the idea of frequent batch auctions that I've I've advocated for in the context of traditional financial markets that cow swap that I advise is working on in uh in defy the idea there is to make time make time discreet and then that enables re-engineering the nature of the competition to be competition in a more productive Dimension so competition on in the case of an auction competition on price instead of competition on speed um and that you can kind of go through the economic chain of why that leads to a more liquid Market of less wasteful rent seeking activity yeah I wanted to Riff Off in Phil's comment that I thought was very interesting was this idea of some kinds of latency games might be winner take all whereas other kinds of latency games might not be that might be a different different parties might discover tiny Corners uh you know tiny little Arbitrage quarters and that for whatever reason reminded me of the real estate industry so so the real estate industry is another industry with tons of rent seeking you know the fact that it's five percent or six percent of your house value to to sell a house from one person to another is a ridiculous rent seeking activity and the way that plays out economically is you have lots of small real estate brokers each fighting for their little uh their little scrap but there is a centralizing tendency in that market which is the real estate lobbies and the real estate lobbies help preserve the kind of flawed economic arrangement of um of needing a broker on the buyer and the seller side and all the little subtle stuff that kind of Keeps Us stuck at five or six percent fees so that was just one that struck me as like yeah do we we do see some winner take all red seeking which naturally creates a concentrated dispersed dynamic in the Manser Olsen sense but even in these cases where we see very dispersed rent seeking like in real estate sometimes the dispersed rent Seekers find a way to uh to band together nonetheless is there Eric um is there a good place to look at research on the consolidation you know it started as a broader more competitive market and then once we had discrete changes in the rules the technology Stacks whatever it meant that it was returned to sophistication returned to Capital different types of forces of consolidation forces as Turin Phil think about kind of the design space it's good to understand what type of technical shifts Drive consolidation and centralization yeah the best scholarly work is by Donald McKenzie who's a sociologist based in Edinburgh and he he's over decades made a um made a mark studying the sociology of Finance so he did this amazing study on the Chicago Mercantile Exchange and he put out a book I think a year or two ago at this point called trading at the speed of light so that's the best uh that's the best um scholarly work and the best line is in Flash Boys by Michael Lewis where he said I used to worry you know about a thousand guys making a million dollars a year and now I worry about one guy making a billion dollars a year and I can't remember there was some stereotype baked into that quote that I don't remember and won't try to recall but that was sort of the best uh uh the best anecdotal nugget on that Center on that tendency just described and tarun Phil as you've seen the block kind of production supply chain get much more sophisticated how have you seen the the number of actors whether they've grown or Consolidated and how do you think that's related to whether it's latency or any other type of like technical sophistication Edge foreign happy to to take a bat on this first um I think you know I have this troll theory of markets which is like any competitive market will will land in some degree of oligopoly and it's about like managing the externalities of this and this is my like my troll um I've never seen a counter example as many times as I've trolled like economists with this um but you know we do we do see that to an extent in Mev um I think the dangerous thing at least the way I reason about it is like how much consolidation pressure are we talking about and like how many counter forces are there for actually maintaining like The Meta properties that allow this thing to exist so like Geographic diversity you know decentralization of capital whatever not like a plutocratic staking set things like that um I don't think we can ever remove these centralization pressures entirely um that being said you know I think the Mev Market is super interesting so there are certainly a few large firms and entities that do very well on specific Mev vertical especially when you look at certain types of text designs like amm's there's a few kind of centralizing or consolidation pressures we see there a big one and I think like the most powerful in today's market is the ability to take risk and this kind of looks like traditional Market making activities in Trad Phi because a lot of it relates to like sex Arbitrage that's like one of the number one fire hoses right now um people want to make trades on a decks but like the liquidity on decks is inherently update slowly so how do you bridge the gap between this like sex liquidity that's gated by like kyc and Regulation and you know custom like custody rules that people may not trust to this interface that people actually want to trade on that's just where we've seen the most activity and the most maturity just because of the volumes it's not inherent to Mev or anything it's just what people use ethereum for today and there you see this like ability to take risk as for sure like a powerful centralizing pressure um before that we saw gas golf thing as another fun one so the early generation of the flashbots auction intentionally kind of gave you a huge boost if you were able to be more gas efficient than other bidders to try to drive down kind of the network externalities of the auction in terms of like how much block space was being used by the auction and this led to people kind of really looking into Super Arcane tricks to save a little bit of gas because the gas game kind of became almost a winner take all style but instead of competing on latency they were competing on like gas used in the system so that was another example early on although I say that one has kind of faded away with maturity of the market and now we're looking at much more things like ability to Warehouse risk another category is latency for sure especially in sex sex Arbitrage complex sex decks Arbitrage or arving against like domains that are latency sensitive like Solana or arbitrum we've seen that one less I would say just anecdotally my my intuitions in the market so far just because a lot of the Mev today is on the less latency sensitive markets like eth but we're starting to see that kind of creeping at the edges and also compound with the risk pressure and like uh together form a centralizing force so for us at flashbots is a question of how do we kind of minimize the impact of those on the decentralized system yeah so you wrote up a good point which is about the adapt adaptive nature of the network uh Eric I looked through some of your prior research and showed in the 90s a lot of the electronic a lot of the benefit of tighter spreads and stuff came through just general electronification and now we're at this like you know Peta bits of of War um how have you seen markets evolve where they go from this big step function and then they the the competition gets very tight because we get smaller and smaller and that that becomes a bit of a centralizing factor I think that's an important just fact to understand about the evolution of traditional financial markets so let me just make sure the fact is clear and then we can block it and we can just discuss it which is that there is this transition that played out and again I'll point to Donald McKenzie's work as kind of the scholarly work to to look at there's this transition that played out from the 90s 90s into the early 2000s when a lot of financial markets I still had a large role for human participants so think uh pit traders in Chicago this is the thousand people making a million dollars a year that Michael Lewis was joking about were Specialists on the New York Stock Exchange for um running so humans uh facilitating trades running a market running a limit order book the same way that a limit order book in a way was run since the 18th century under the Buttonwood tree uh with with aided by computers but but fundamentally a big role for humans and then there was a transition in the U.S stock market it was really facilitated by regulation National Market System which I can grab a copy of I got a big I got a copy of my office um but that facilitated the transition to electronic markets and in the U in the in the fully electronic trading in the U.S stock market and it's it's pretty clear if you look at the historical data that the transition from human-based Trading to electronic trading and meaningfully improved uh liquidity and efficiency there's a famous study of this by uh Hendershot Jones and mengfeld that's very widely cited but the the as my read of the data and I have a lot of references to this in my work is that you can see this kind of Step change as we went from humans to computers computers are a good idea but then all of this additional relativistic competition on speed has has moved the needle precisely zero on the Market's cost of liquidity so if you look over like the last 15 17 years of of financial Market data it's hard to see you know the there are big gains in the transition from humans to computers and then it kind of flatlined um what that implies for crypto markets I honestly don't have a confident view it's a it's a newer uh a newer domain but that's what we saw in traditional Finance yeah turn you always talk about how Reagan Ms and nvbo really change Equity Market structure and how the whether it's order rules those are regulations but we have things like regulations which are hey there's a new amm launches with very specific rules or Curves um and we see that those things really change Market structure so I think it'd be interesting to comment on what Eric just said around the actual material performance layer in this added co-evolving nature of the regulatory landscape and kind of bring it over to crypto where we're seeing we went from this very naive unsophisticated world to things like SCP wintermute much more sophisticated on chain people and it's just really changing the market yeah I mean I think the weirder part about crypto maybe versus traditional markets is like um traditional markets actually had a contraction in number of venues since uh Regan Ms because a lot of venues like basically couldn't couldn't make the economics work given their volume like places like Philadelphia Stock Exchange which you know you'll still have to connect to but like basically doesn't do anything um crypto actually has the exact opposite problem where there's constantly new venues of different types and there's new venues because there's new chains so there's new Roll-Ups so it's creating more cross-domain Arbitrage um centralized exchanges obviously have had huge calamitous changes in the last six months which have created a bunch of uh Arbitrage opportunities certain things that were collateral that was not expected to to move out of a particular range in terms of its value in numerator terms um you have things happen like that so there's sort of a very different dynamic in the sense that there's this this like kind of proliferation of venues in crypto that where versus the like contraction in venues that you had in the normal world that does change the Dynamics a little bit and this is one of the reasons I actually am I I would say like the last year of crypto has made basically made me more convinced that this actually looks a lot is going to end up in a different state than um traditional lit exchanges where you kind of had the like reg nms happen then the financial crisis happened and so like a lot of conference a lot of how good trading firms did well then and then you have the kind of like boom up to 2012 then after 2012 it got extremely competitive and people realize like buying order flow was better than becoming faster alone and so then you kind of had this like the number of competitors decrease after that point I think crypto is actually kind of weird because the number of products is growing really fast like the number of different places you can trade the same product um you know just think about every new roll up think about the arbitrem launch right like when the coin came out that just like basically dosed a bunch of venues um and so there's sort of this this interesting thing where this is the reason I make more of a comparison to an ad auction in that you have you know on the x-axis a number of different assets that are growing on the y-axis the number of venues that the assets traded at um and then sort of on the the you know between the two of those those are actually like growing like the density of those is growing at the same time and it means sort of that the the dimensionality of the products is higher uh it really does mean that like arbitrary trading Arbon arbitrum versus trading Arbonne eth versus trading Arbon op they're actually kind of not fungible products because the amount of Leverage you can get on each chain is different the amount of security you're willing to take when you go across the bridge is different and the Mev auctions are different in terms of like how much you have to bid the gas rules and that that that sort of high dimensionality for products that are sort of fungible creates a huge sort of Arbitrage opportunity where people hold portfolios of the of what's supposed to be the same asset but then they kind of like you know they're doing this like weird risk warehousing statistical Arbitrage type of stuff and yeah that's where I see the crypto markets actually deviating quite a bit from traditional lit markets and it's going to be kind of interesting to see that Evolution I think that kind of just yeah sorry good in response I think that's fascinating and you know the it sort of relates to what I was initially so confused about in 10 13 years ago or so at this point which is you from a distance think of how arbitrageurs and Quant trading firms is doing something really fancy and sophisticated if because we have a as a as economists or Finance researchers think that there shouldn't be um Technical inefficiencies and financial markets you've all heard of the efficient market hypothesis to beat the market you have to know something the rest of the market doesn't know economists are very dismissive of technical uh technical trading opportunities but then you look you just look at what arbitragers are actually doing a lot of it is like the same trades on lots of different venues are when the prices get out of whack and it's actually like Grandma could notice the pricing you know can notice the pricing discrepancy all the complexities just in technologically exploiting the thing so then if Tyrone if you're right and there's this just proliferation of venues and very closely related ways of trading the same risk exposure that's an arbitrageur Feast yeah and and it seems to only be growing because like token issuances like sort of still growing even in the the bear Market I think yes and no I don't think it's an arbitrary feast in the same way you think about like hft sniping or something like that to me this looks much more like Market making and competition on risk which is actually what you probably want in like these are markets so it it is a very different nature I think there's two interesting effects here I also want to point out that do kind of aggregate add to this and make it very different from tradfi number one is like the way you can interact with these venues so in tradify execution relies on trust and it's kind of like a human based or or a technologization of like a human based process right so like if I want to place an order that's like a combinatorial order of like I either want like these three orders to execute on like the New York Stock Exchange or like take this other one like to this other venue or Etc it's kind of very hard to do those orders we're on a blockchain you have like more programmatic ways to express more complex preferences and like this something like using two different venues at the same time is not mutually exclusive if you can like structure the right aggregation or meta transaction cow swap is like in some ways an attempt to do this so that is I think one difference um and the other difference I think is like the profiles of the people taking the risk so like assuming this is true what I said and we have competition on risk and we want that I was having an interesting conversation recently about like the the stable coin depex uh uh kind of last week during all the bank failures and things like that um it turns out during these stable coin depegs all the sophisticated actors and the and the and the active liquidity even though there was a ton of money to be made and so many arbs across these combinatorial venues basically you know a lot of them sat a lot of these opportunities out if you look at the chain and if you look at where the Mev distribution actually went like passive LPS and these protocols made a lot of money um because like the active LPS disappeared and this was like the backstopper right and who are these passive LPS they're like d-gens who are taking like they have their own risk profile it's highly bespoke it maybe isn't even rational to like a market-making participant which is why those participants did the rational thing and backed off but it kind of does add this like robust liquidity backstop because you have like a risk segmented rather than a zero-sum game that makes the system more robust in downturns so anyway that's my rant about how maybe things are different yeah Eric do you have any comments or thoughts on that kind of resilience and resiliency and robustness question I know in the US we've had valmageddon and some other things where there's different causations but a lot of what happens is too much of liquidity has the same kind of profile and same type of structure and strategy they're running status meaner version when it gets to Sigma they just turn off whatever the thing is it can cause Market fragility so any thoughts there I think that's a big topic it's a it it some to some extent when when I see a flash crash I think prices change to some extent I think markets could be more robust um uh markets could be more robust if design if designed differently so I worry I can make I mean let me kind of walk back something I just said I think was a little bit precise the the thing I kind of worry about I use the phrase um uh fast crowded fast and thin crowding out smart and and deep are slow and deep it's probably a better a better way of putting it so I worry that if if you have a market that in in good times or in relatively stable times is it's very profitable if you're technologically sophisticated picking up nickels but and that that in a way crowds out deeper liquidity but that is less technologically sophisticated um so this probably makes the most sense thinking about the treasury market if you have some firms that are are very sophisticated technologically at proprietary trading um and that and they're just they're faster and that crowds out some of the um some deeper pocketed um slightly less technologically sophisticated investment Banks um but if if if when the goes down pardon I shouldn't keep cursing I'm a professor but when if when there's when was the role professors aren't allowed to curse me just checking I try to be dignified right but I I worry that that you know it is the is the deeper but slightly less technologically sophisticated liquidity there there in a crisis I have made the argument that discrete time batch auctions slightly slower Market design Level Playing Field that those kinds of properties could have a robustness benefit it's honestly very hard to model in a way that's that's convincing to skeptical economists it's just it's a hard nut to crack and my guess is someone will figure out a way to to study it using a diverse set of tools I think it's a good uh I I sort of sympathetic to the hypothesis I haven't figured out how to prove it all right Eric it seems like you have a good question for Turin before he has to go I know turns yeah right question kind of for it so this is this might be and Phil Phil prefaces every question with this is a trolley question so I think this is a draw of life which is at some level if you read Nakamoto um the problem it's solving is a Time stamping problem is how do you like compare Nakamoto to Haber stornetta in the 90s doing a you know blockchain like data structure where they were using hashes of uh hashes to link to link data over time and ensure that data hasn't been tampered with um part part of what Nakamoto was solving relative to what Haber and stornetta did is we'll it is is creating an effect a decentralized timestamper but suppose you had I kind of wonder if a lot of Mev manifests in the trading of crypto Financial assets if there was if there was a trusted time Stamper and you there and there was a a um a decentralized exchange that made reference to a credible trusted timestamps I think that might be or that's the trolley premise of the question could you solve a lot of Mev problems just wait you can't rearrange transactions because we we know what it means to rearrange transactions is to  with the guy the time stamps and you gotta process transactions in order and and then you could talk about what do we batch process do we make time to screed blah blah blah but the fact that you can rearrange the sequence it's what is what's so appalling to me as a scholar Financial Market I mean I mean I guess I'll give you the the the sort of Economics version of this which is you know the decentralized thing doesn't work because of like you know the errors and possibility theorems of the world right I have many validators having to pick an ordering and agree on it and obviously the only sort of social Choice function that works in all cases ends up being dictator and so this sort of this these kind of like very weird social Choice things around like you know if if if we do really want to be decentralized then you run into these kind of like problems that come from just the nature of you know the symmetric group uh though those are very very kind of you can't get around them but in the centralized case of course like it doesn't matter the arrows and possibility theorem says it's a dictator so all you're doing is picking the dictator in some sense so so would time stamping alone that would that would uh I don't I don't think it would remove everything for for the record um because like it's not just time saving it's also like adding and removing around particular transactions which is a Akin to the like adding a ton of cancels in front of a big order that's going to run you over so like there's still some of the the adding and removing part that I think it still probably is is an issue not just the the ordering and are you assuming that time is a zero bound that you can't do any speculative transactions I know in in the Europe European markets there's a thing called speculative triggering that they're they actually start firing orders off of messages so fast that they're just starting to see the first bit string and they're sending out messages on the other side before they've even confirmed that this is in strategy and they start canceling them creates this insane amount of load on the centralized server and they've actually been the the the issue of speculative triggering meaning that they're sending pre-confirmed transactions it's insane oh man I have so much trolling to do here I want to let tarun hop into his training I'm gonna line up the baseball bat and and let her rip um all right uh all right thanks everyone yeah amazing thank you so much tarun that was super fun I'm glad to set you up with uh you you look so so happy I'm so happy I'm so happy and we met on a panel you you asked me towards the end if we could get philosophical about times this is my this is I know we need like we need like hours though you know we need hours but uh but anyway so I will say a few things that are that are that are trolley responses to your trolley question first of all I think let's assume you had the world's most perfect time stamp system in the universe and it had no problems that I could possibly complain about there's still like the fundamental physical limit of uh you know the special theory of relativity so you know in the early 1900s there was this idea that if two things happen um at the same time the the the observation of like a third Observer of like the order in which these two things happen depends on like the reference frame of the of the Observer right so even if we had a perfect timestamp system um the best we can possibly do is approximate fcfs which as we've said either creates like a Spam this is what Michael Jordan was just talking about these pre-load transactions is like some form of spam um or or a latency race right but then who wins this latency race who wins this winner take all game depends on like the third party Observer which here in theory is like a decentralized protocol that's supposed to be fair and Universal right so like if these two people legitimately send their transaction there are at the same time and they're not in the same place in physical space the network still has to sequence that and like one of those two people is going to be feeling left unfair right because oh we sent it at the same time and because of the Network's reference frame this person's transaction got in before mine um so that's like even if you had a perfectly synchronized like to the absolute physical limit set of clocks you wouldn't achieve fairness it's like the best you can do is approximate fcfs against some reference frame um you know for tradfi this is like they they kind of wave around this by saying okay our reference frame is like this address in New Jersey and like the game is like build a cable there um but you know that doesn't really work as like a reference frame for like a distributed um permissionless Network um so one more thing I have to say this is like a trolley idea that I had many many years ago when I first started studying Mev that I've talked about to a bunch of people but I feel like it might be worth mentioning publicly in the context of this um this was one of my first intuitions when I when I came into Mev was like wouldn't clock synchronization just solve this okay how do we build decentralized clock synchronization okay let's assume we have sgx let's like give ourselves as much ammo as we can can we still build something useful here um so I actually had this idea of a DEX that works based on like this sgx nuclear football so you can imagine basically a a portable like in a briefcase sgx machine that has like a long-standing key that can't be rotated so this machine has to stay up for the exchange to uh kind of uh work then you travel from place to place around the world to all the participants in your Market maybe once a month you synchronize their local sgx with the football right so like if you put the football physically right next to their sgx you can get tighter clock synchronization bounds because the latency between this trusted football clock and your local chip is like super low then you're kind of approximating as close as you can within like real physical constraints uh this time stamping thing still doesn't solve fairness still doesn't solve Mev what does it actually give you it gives you basically a shorter block time right um it lets you have this uncertainty in the same way that kind of fair ordering protocols do with a smaller window which in your batch model is the same as like shortening the batch essentially so all a trusted timestamp would allow you to do in a decentralized context is shorten the batch is shortening the batch desirable now it depends on like The Meta argument of like the rent extraction against the batch how meaningful is the end of the batch how much does latency matter what's like the actual participant distribution Etc um so tldr is it useful I think it's interesting like certainly you can tweak some parameters and like building this sgx football thing I think is kind of fun maybe if I had like infinite time I wouldn't be against like just YOLO and trying it and seeing what properties we get at the same time like solving Mev to me like Mev is intentionally unsolvable so like it's very hard to solve and maybe I'm wrong about this this is like an open hypothesis it's like why one of the reasons that I like that other people are studying this because maybe there is like a best design but my intuition is this is not a solve because like someone's still going to be left like feeling the unfairness and like whether or not that's decentralizing depends on like the power dynamics of your system basically um so sorry for the the long troll but can I answer with just an open question which I think I so I think of the solving Mev as something you could try to do at call it the application layer uh or at the protocol layer and at the application layer I think frequent batch auctions properly implemented solve Mev um we can debate that and let's let's take that offline I think the really cool open question is whether you can quote solve Mev at the protocol layer uh is there some is there is there some tweaked uh to blockchain design uh that Pro and and my intuition is it comes from just not being able to rearrange stuff but then that ways between the two of you you invoked arrow and Einstein so I got to think a little bit harder about it but uh um but my ultimate question is like who gets the rent you know like that eliminate the rent that's why I mentioned the real estate agents why why who gets the rent why not shrink the rent no no I think shrinking the rent is great and I think that is where frequent bash auctions are super useful but I don't think they eliminate it because there's still kind of how do these batch auctions interface with the rest of the world which is like you know the the the part that's like really hard to model especially if there are a lot of like continuous venues I think this is where like a lot of the crypto research and like the global solve needs to go um specifically for example like I don't think frequent batch options solve Mev even for dexes for one like very simple reason and I say this as someone who was in 2018 like uniswap guys frequent batch auctions solves Med for Dex is why you're not using this I think people do actually some people want uh things that can only come with reordering so like many people want the ability to do complex Dynamic transactions that depend on like the specific State a transaction is being executed on in our conditioned by this one example might be like a protocol that wants to like liquidate tokens on uniswap and then do something else in the same like call frame atomically and maybe this gives them some security property they otherwise wouldn't wouldn't have or something like that so as long as you have these people they can't use the batch because they're like requirements are simply incompatible with like waiting for it um and I think that solving Mev for that class of users looks very different as solving Mev for the people who can wait two seconds or something where I agree like a batch is totally fine and like a great way to reduce so I think all these like plug away at ways to reduce Med but the reason they can't happen at the protocol is because of like rap wrangling with all these fundamental limits my last piece of trolling is I think the flashbotch auction I remember I said this last time we had a panel and I got so much pushback but I'm going to say it again because I think it's true is actually a frequent batch auction solve of uh you know Mev at the protocol layer assuming a lot of timing things about the ethnetwork it's at least kind of looks like that if you squint hard enough um is that not an important assumption the Assumption because you wrote about the importance of geography diversity Geographic diversity is that antagonistic to fairness because you're just increasing the actual amount of time for the network to coordinate literally like if we're all in one box in ny4 then our coordination time is very different than if we're designing a system such that the replication is happening across the physically large areas yeah this is why I hate fairness as a word because everyone has their own pet fairness definition like for some people I have to wait a second longer because like some guy in Australia wants to trade with me that's like super unfair it's like man if I just cut that guy out like I don't care about his economic activity you know it would be way better for everyone I care about if like we could just trade instantly and like not care about that guy from Australia whereas like if you had that the guy from Australia would be like this is super unfair there's like this cartel that I just can't like access without like submitting myself to like the US government you know hegemony or whatever um so yeah I agree with you that like there is a tension there with fairness which is why I think Geographic diversity is like I'm not implying it's more or less fair for exactly that reason like I don't know whether people will agree with that um my argument for for why that's good is much simpler it's like if we don't have that we will just be like subject to like a very single specific set of regulations and like you know the rent game will look exactly like track five that's uh my two cents but yeah no I agree like uh even parameterizing this time right like do we do nine seconds or ten seconds do we do 10 or 15 like there are fairness arguments there that like various people make because the impact is like on the actual distribution of the rents which is like where people actually like feel the fairness right like ultimately fairness to me is like are the rents like sanely distributed and there's no clear way to define that really Eric did you do any work on conditional orders there's an in hft there's a concept called ISO orders which is you can sweep across multiple venues with one order and you're able because you do have to trade within the bid ask spread legally if you trade outside of it you could find that actually moves across venues now the venues may all physically be in the same place in crypto our venues are just you know kind of different a very different concept from what we're describing but I know your LLC work was done just on the LLC have you done any of this kind of cross venue latency considerations it's a great question um the the closest I've done is in this work with um Robin Lee and John shim where we've put pulled together a bunch of empirical facts um on competition in the U.S equities Market and the the way I think about the use of iso orders this inter-market sweep order is if you're most of the time if you're trading if you're if you're trading not in a race in response to some algo signal but because you happen to yourself just want to trade some stock but you don't want to leave an algo signal for others to exploit a way to trade you want to trade across multiple venues all at once so that responders to your trade trade after you and isos are one way so sweeping the market is one way to do that there's also a reg nms compliance issue which is super in the in the regulatory weeds which is a reason why um some firms use use isos um so that's the that's the closest I I've gotten I think the conditionality is very interesting I kind of take Phil's point that at the application layer for certain kinds of trading needs frequent batch auctions eliminates Mev but they're more conditional more conditional trading needs which intrinsically need to rearrange staff or condition I do a only if B and C happen that that strikes me as more of an open question so let me kind of put two open questions out to the audience which is what can you solve at the application layer and and what if anything can you solve with the protocol where go first I think we're also seeing this super interesting uh effect in this direction which is a lot of uh so there's two Theses in crypto one is like the general or at least in smart contract chains and I'm like being very high level here but like do you have a general purpose chain which tries to optimize for like anything anyone might ever want to do and like have some reasonable compromise for the consensus across all these use cases or do you couple your consensus much more closely with like the actual economic activity that's happening on the chain and then make more like activist design decisions in like the consensus itself um I think this is kind of a version of that same question so like in in the in the chain world you could have like a batch auction chain which like only purely allowed for batch transactions and then there would be very clean abstraction boundary to reason about the rest of the world you'd still have to reason about it and like you still in my opinion need to to do some reasoning to fully solve Mev because there's still some Mev that's introduced by like arbing you across other venues but there's like a very clear boundary that the the Dex itself can control whereas when you look at something like eth something like cow swap trying to do these dap uh kind of interventions on a more general purpose chain that bound becomes like the flashbots auction basically which is like trying to stuff everything but the kitchen sink into like the block production and then it's a question of like how do you align your thing with this block production algorithm so that you still get the properties you want um I think which approach it makes more sense is like a religious question as to ruin put it a few months ago which I really enjoyed but there's certainly a lot of experimentation to happen on both and my rough trolling is like I consider batch options in like the general purpose if model um almost like a cartel and I say this not in a negative way to me cartel is not always negative but it's like a union it's like a union of users that are like we will kind of execute together and like throw our full economic weight behind this system that works a certain way and Define a boundary to like the rest of your option so that like the same way we would normally Define one against like the rest of the World by having events it's like a cartel with the sign flip that's people getting together saying let's collectively not get screwed collective bargaining yeah that's what I'm saying I don't say cartel is like a negative thing like so it's my troll pitch for flashbots for like the first two months was like let's make a cartel of people who work in Mev who like aren't profiteering so like it doesn't all go to crap but uh then I I realize maybe I shouldn't pitch like a company I'm making like using the word cartel because like certain people have very strong reactions to that um but you know yeah uh Union I think is a better better framing for sure um how do you think about the fact that the more complex application and kind of protocol logic we want kind of the more exotic and expensive Hardware we might need um and so there's a lot of different things of a centralization causes of hardware and in the hft world we have fpgas that can do SUB 50 nanosecond full you know trigger to Target type loops and that's very expensive and complicated to design and so if we want to do some of this more rich and interesting uh State Transformations we might create uh cartel that's the hardware developer and they might be able to create some type of thing and so how do you think about the hardware layer when you're making these decisions this is this is one of the trillion dollar questions of like the next 10 years of crypto Finance I think it goes very deep I think you're totally right um in that like in some ways just the complexity and the dimensionality that tarun mentioned is already enough even without throwing more fuel on the fire that like this effect exists um especially if you push the latency it like makes it way worse right like if you look at Solana for example you need to use an fpga on Solana to extract Mev which like most people don't even understand but it's true and like the strategies you can do are much simpler and there's a huge Hardware Edge if you have like a slightly better fpga you can do more complex strategies include more transactions and directly kind of winner takes all the Mev game um I think I think that it's a it's a very complex space so I won't give any like prescription I think there's a few things that we need to be mindful of number one is like make sure the hardware is doing something that is actually valuable and like indispensable in a way so like if if the fpga's centralization force is just there because like some protocol parameter like kind of induces it is it really needed you know is this Hardware Edge for these manufacturers really needed to make the system work probably not um that being said for some Mev systems maybe there's like a fundamental trade-off of like the state space is just really complex the more decentralized we make it the more complex it gets there's a hardware edge here um if that's the case then like how can we decentralize this I think that's what we're thinking about with Suave it's like in some ways a decentralized computation Network for Mev where we're going to try to distribute this like as much geographically as possible um I think this is like an activist thing though and it's like very much an open question right so if you like play out one obvious roadmap of suave it could be that like Intel suddenly is the Meb company and Intel like wakes up and they're like why are we making processors that are like for general purpose if we just use all these nice Fabs to like make better Mev Hardware will be like a trillion dollar company rather than like a hundred billion dollar company or whatever um so yeah that's that's a risk and I think the the crypto Community needs to be really careful and probably like actively invest in like Distributing Hardware supporting Alternatives making protocol choices that make sure like one Hardware Mafia isn't principled over or privileged over like another um I don't think there's like an easy fix there other than be super mindful make sure the hardware is doing something actually useful and then try to actually decentralize that if it does need to be there and there's no other way um but yeah it is it is risky and I have this troll uh this super troll which is that uh Mev turns any proof of stake system into proof of work um which is very depressing but you know noodle on that for a second uh which is kind of a corollary to your question proof of work was the most extreme version of this world I'm not fast enough to figure out that fortune cookie but I didn't want to yeah it's because because you need to optimize this combinatorial problem in real time it's very computationally complex and if users want to push the latency bounds like lower it gives edges to people who have Hardware that like optimizes this better and they need to be doing like a lot of also not useless it's less useless I think it's like more proof of useful work because it's actually doing something useful but it's still like a grind you still need to grind all these like uncertain orderings and like run these like combinatorial Algos and uh so you know um and there's Hardware externalities there um and when we saw this in proof of work like proof of work the Asic manufacturers were such a political force they basically dominated the politics so there is a risk in the future that like Mev A6 exists and like Mev Asic companies are like a super politically important force in Mev um I think we can mitigate a lot of that with like sane parameterization but yeah the fundamental force is there and there's no clear answer to say like that won't matter or that's not going to happen right now I wanted to just Echo your point about useful versus non-useful competition I think profits incentivize all kinds of innovative Behavior um the I I always have emphasized and and deeply believe that should try to align private and private profit incentives with what socially socially useful for for markets for society so that my criticism of latency Arbitrage is it's kind of socially wasteful or neutral to negative and then there's a lot of private incentive to capture it uh but Hardware Innovation isn't per se positive or negative it's a in service of what so if it's in service of rent seeking or socially negative activity that's one thing if it's in service of curing diseases or making the world a better place you know that's that's that's quite a different thing so I I like to focus on what is the source of the Innovation Innovative incentive and making sure that that's a a positive uh not not zero to negative Force so the the alignment of private and social is kind of a an economist riff I think we can also achieve that with Mev so like the ideal system in my mind under this model is like you know we like living in fantasy world it's like one where you get paid to deploy Hardware into places that like aren't already Hardware rich and that deployment directly benefits the user like commensurate with the amount of money you're spending so it's not like an adversarial situation where you're you're like spending it and they're getting like exponentially screwed the more you spend uh it's more like spending it and you're you're relatively aligned with your outcomes and maybe you make more rent from the protocol because like you're optimizing user outcomes better there's a fee for you that's sane and like this is all aligned with like the actual social welfare function we're trying to optimize um how do we get there I actually think batch options are a fun key to this so if you take the framing of like unions uh that we had earlier like I do think we need to build protocols that support the ability of like various geographies to like unionize and the protocol still works and is in fact more effective when this happens rather than less effective this is like my G Geographic decentralization trolling and push because the worst case is we centralized to like the us or Europe or something like that I you know but like the best case is like there actually should be computation in New Zealand right like if their users making transactions there and the system doesn't predatorily Advantage those transactions being optimized in New York then why not do the computation where the transaction is actually happening so I think this is where the care has to be taken make sure the system doesn't push those unions to like the central place and instead allows them to like exist in like their natural local geographies um yeah Eric I think it'd be interesting to use that as a comment on what happened with fiber I think a lot of people became aware of hft's investment when they started to see physical places coming between Chicago and Secaucus or wherever the the you know the amount of fiber that was laid and then we went to the the more advanced stuff that even I know that bird droppings had issues um in the McKenzie book he comments that uh but the world does need fibers it'd be interesting to hear what your thoughts on how was that useful at all was the fiber late for this or is it literally just for latency Edge hey when I'm open-minded that there can be unanticipated benefits that the the the um Innovation incentivized by high frequency trading activities might have some spillover benefit but I I haven't seen any specific reason to to see I have this this time-lapse data that uh it's a very dedicated research assistance build where you could see I called it the latency Arbitrage triangles you could you could see using um uh Federal I think FCC um Federal Communication Commission applications for microwave licenses you could see the line from Chicago to the New Jersey Equity markets and also from both of those locations to Washington DC get fat get straighter and straighter and straighter so DC but classy your eyes raise like it's the government data that moves financial markets gets released in a data center on K Street I mean where all the lobbyists are just by coincidence um so there's this latency Arbitrage triangle that you can see in the microwave applications data and I straightening the thing by a few extra fee I mean it doesn't cost that much money so like I don't want to get up in arms about it but also to find social value in that I think is nuts the fiber in transatlantic fiber or transoceanic Fiber that seems useful um I don't know where what the Delta is but if there weren't an incentive from latency Arbitrage it's a it does come it does come maybe to this idea of um the social welfare function being a reflective of the fact that speed in this world was so relative if Phil has a four second connection and Eric has a three second connection you'll pay infinite dollars for that one second package and so there is no like socialization of the welfare function it goes all to that one sport yeah very relative and then so the social value is probably based on the absolute you know getting from five microseconds to four microseconds is it's hard to see a ton of social value in that but you know again I it's not like I have a a proof it's just a strong economic intuition I feel the same way I would like gut gut check it at like five percent benefit versus like 95 percent burned for zero sum uh and like like my guess is like had you emulated the latency game in like a simulation Universe where they could all like invest this money and like get the same Edge um like there would be there would be little little difference actually um and again this is just like a gut gut feeling um I do think there's some utility to speed I think the tricky thing is like the types of links they're incentivized to build like don't have high throughput and they're not like usable by the public so you know that's like where the where the winner takes all thing comes from um so yeah I see a little bit of social utility and certainly there's externalities of like if you get a better routing technique maybe that propagates to like other switches and like Google's data centers or something so you know very hard to like throw the full baby out with the bath water when you have like thousands of Engineers working on any Pursuit um that being said like I don't know if it's like super aligned with like the actual outcome of efficient markets which is like and that's I think where the zero-sum thing really like takes it from being much more aligned to like much less aligned if there was a way for us all to like improve the latency of markets together and like you know we actually each were able to express our individual welfare on that in like a mechanism that then optimized the social welfare I think you'd get to like a better social welfare with less investment um so like I see it as like a coordination failure basically that like a lot of that money was burned but that's just my own political opinion and again this is this gets highly controversial with hft people I've certainly had you know face turning blue in conversations about this before so uh you kind of have to make your own judgment there I think the relevance to crypto to me is just like not even asking the value question it's just like do we want a global system or not and like how do we want the rents to be distributed in this system let's just like come up with something that we don't put value judgments on is like a much easier place to go than like oh this thing is bad so like therefore let's like moralize it I think we also see a lot of that in Mev and like the truth is usually a little more subtle there are trade-offs like we should actually engage with like the subtlety of like how much money is going where and why like that to me is like the question that a lot of these things sweep under the rug by like popping to these different layers of abstraction where you lose like the core thing of like where's the money actually going follow the money is a good good general purpose instinct so where's the Mev money gone that's why I want to I mean we have data on this um the Mev money is going largely to validators right now um certainly there's some going back to users in the form of like transactions being mined because of Mev that otherwise wouldn't have been I would expect that to increase as users get more sophisticated and kind of leave Less on the table in the transactions they're making to be extracted by The Searchers and the validators we're seeing Trends in like all sorts of Dex designs for that um some of it is going to the Searchers but on net I would guess the Searchers and this is not data so this is just my gut feeling it may even be a negative sum uh game where like most Searchers are losing money and like their the new entrants are like subsidizing like the profits of like the few that are are making some Edge which is not great but you know um it's not like the Searchers are like raking home bags full of cash unless they have like a substantial Edge on sexes in which case they'd probably be raking the same bags of cash regardless so that's kind of where the the Mev money is going today a lot to validators some to Searchers some to users the middlemen like flashbots some of them take some rents today we don't so it's a pretty race to the bottomy market and I don't think anyone's really making a killing there or like extracting huge rent or anything Eric that's a great thing to end it on follow the money um I think that's uh that if we had to say something about today's uh incredible uh incredible Summit uh follow the money I know that the ethereum um rig group is focused on some of this stuff and I think that's going to be some really cool stuff to come out of that so hopefully next time we talk we can do that this is the last Talk of the day everybody uh Eric thank you so much really appreciate it that was awesome Phil good to see you um Everybody these are going to be recorded chopped up and put on the internet and uh we'll see you soon can't wait to collaborate more on this stuff see you all soon 