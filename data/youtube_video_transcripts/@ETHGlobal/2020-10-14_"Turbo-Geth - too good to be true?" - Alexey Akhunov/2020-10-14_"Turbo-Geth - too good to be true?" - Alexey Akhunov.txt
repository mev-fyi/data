and uh next up we have alexi talking about trouble again and uh he'll just be covering sort of all the the latest improvements and performance updates that have been added to the triple get project and uh he is uh in this room so i'll let him introduce himself and kick off with the presentation welcome alexi hello hi kartik thank you very much for the introduction and yeah so i will let me just start the screen share i will um sort of say a couple of words about let me just figure it out so okay so first um a little bit about myself i basically um i am currently i've been the programmer for pretty much all my professional life and uh for almost last three years i was working on this particular software just called turboget and what the turboget is is basically it started as a fork um um as a fork uh the um of the go ethereum and sort of experimental software uh with certain goals to basically make it a bit more performant if possible and but now it's pretty it's it's becoming more more different and we're going to see in the in our talk uh how and why and the the title basically is too good to be true is this addressing the the um something that i haven't thought about until very recently is that whenever we present some kind of performance numbers for turboget most of the time the first question i get where's the trade-off like what's the catch what is what what is the you know because people think well if we do something better then we might we have to do something worse and so basically that's what i want to address so here's the picture about the um so in the databases i look at the first of all i look at ethereum as a database essentially it's a it's a distributed uh database with certain properties um but in the databases essentially you have usually like three points between which you're making trade-offs um so update efficiency space efficiency and access efficiency and sometimes of course uh the the space efficiency helps all others and yeah you can kind of move around uh and and trade one for another usually you might have seen those triangles as well uh in different areas and so you know people normally think about moving in this little plane which is the the circle underneath um and so this is where you're going to get trade-off like oh if i get more efficient in access i must be losing something in some other places but what actually happens in technology all the time and if you look at it there are there's always technologies which are strictly improving right we we have a we have seen this many times for example move from ssd to hdd first of all the ssd was were more expensive but now they're ssds for for considerable volumes they're not so expensive anymore and they're pretty much better at everything so nobody's asking what is a trade-off between ssd and hdg um at least not anymore so this happens all the time in technology and usually the the way i see it is that the previous technology is a bit further um further um down further apart away from the from this efficient frontier where you can do trade-offs and so usually you want to get closer to the efficient frontier and that normally requires some kind of paradigm shift and the paradigm shift is usually also kind of goes against the uh the common intuition um so we're gonna look at this so what kind of the paradigms do we did we have to shift here to uh which i basically i posit that we did get closer to the efficient frontier so there are a little bit of trade-offs but not many so next time you know that's why every time people ask me the questions what is the trade-off i this is what i'm bringing up so so we're going to be talking about the trade the paradigm shift and the data model where we replace trees with the flood data we're not going to be talking about uh history uh the data metal for history because it's another complex topic and i'm not going to cover it today but we are going to be talking about other things like concurrency and architecture so um with the concurrency is essentially uh whether you do homogeneous concurrency essentially when you're trying to run many things at once but they're pretty similar or do you want to run like lots and lots lots of different things at once that's what i call the heterogeneous concurrency and in terms of architecture it's the here you basically the shift is from a monolithic system with a lot of cross-cutting concerns done in the name of optimization we're moving towards the modular system with the separate separate concerns which i posit as well is more optimal um so let's start with the data model and so with so kind of i thought about this for a very long time like where does this come from where does this data model come from but everybody is using and i some time ago i saw that it actually probably comes from the yellow paper and again it comes from a sort of common intuition that yellow paper essentially calls this particular way of storing data that most implementations do right now a sensible implementation or a reasonable implementation and it says that it's minimizing the function c which is essentially function c is mapping the the um the nodes in that that sort of state try into their sort of hashes or something like that um but actually it might it should probably be the inverse so although this particular paragraphs these paragraphs are not normative they already create this sort of they basically based on somebody's intuition or some sort of common uh kind of expectation and therefore everybody assumes uh i guess everybody assumes this is how we should implement this and this is where it goes so back in 2017 when i just started this project i started with profiling go ethereum and i bumped into this thing which was 20 of my profile and this is exactly accessing this memoized function inverse memoized inverse of a function c so essentially given the hash you need to figure out what is the node um the the tree node is so basically that's the so in in in the previous slide we were talking about recommendation to memoize this function and here we're actually using this minimization and that was the from my point of view that was a performance bottleneck and this is where all that started so and this is the picture you probably saw many times some of you so on the left is basically is my very simplistic representation of the state try where you have um some elements which are v1 v2 v3 and so forth and they are built up into this kind of sophisticated i would say state try and the the the the point sorry the arrows the black arrows in that the tree on the left are the um those uh function like memorizations of this inverse function so given the root hash i can get this root element and so forth and i can navigate through so example for example if we wanted to find element v five so we need to basically access this memoization one two three four times so that's the basically for access to the database if you use this memoization as a data model and what i noticed straight away is that i couldn't parallelize this because there is a data dependency there so you don't know you cannot do the you know you can do don't do the second arrow until you resolve the first one so there is necessarily um data dependency and no um possible parallelization and on the right you can see how this minimization kind of is laid out in a database if it's a key value store so yeah and so here's another point is that you're essentially the the deeper the deeper the tree the you know the the worse the problem is becoming but why why does it why is this uh you know why do we actually have to do this at all and it turns out this is all about computing the state state root hash which is which we need to either if you are a minor you have to place it in in header or if you are just a verifier of the block you have to compare what you've computed with what um you've got in the header so all this complexity is simply to verify the state with hash and this is actually one of the interesting innovation of ethereum compared to bitcoin which essentially commits every every block we commit to the uh to the state uh uh to the state route and this is actually quite a good interesting feature but it has a certain cost and apparently so my my posit that the the cost of implementing it in the old way is actually very high and it's unnecessary high so what does the um so now uh what does the evm think about it so if we look at it from the evm point of view so if anybody if you looked at the like what is the evm up codes are they actually have no idea about to try so they don't actually look at the try they look at the account addresses and they want to pitch balances by codes or storage and stuff like this and also some instructions for storage loading they again they look at addresses they look at storage location there is no localization here at all so um so basically what uh the hypothesis was that okay the having everything stored is having the state stored in a radix tree as a primary structure it tries to satisfy this sort of computation of the state route which i think is quite a minimal thing and but it sacrifices the efficiency of evm access as you we saw before it requires the uh sequential data dependent access so is the sacrifice worth it so essentially we sacrifice evm efficiency and to be able to compute these state root hashes quickly and that's what yellow paper calls sensible or reasonable implementation so yeah that's what i've been working on for uh last two years uh is that trying to figure out if this is a sensible or reasonable or could it could it be that this this is a false trade-off and now i am convinced that this is a false trade-off because it is possible to do both things efficiently to compute the state route and have efficient avm access okay so but how do we do that how so we know how if we have a flood data structure it's i don't need to explain you how to provide efficient status access from evm because it's clear like if you if you go back to the slide if this is exactly how we store the data evm simply accesses it in in this form so i don't need to tell you how this work but i do want to tell you how the other part works so given the flat structure how do we calculate the um the state route so yeah the if we know the structure of the tree then we can do that and you know of course you can try to build up the tree and then compute the root but obviously that is going to be uh quite memory intensive what you could do better is that you can start um some kind of stack and then you need to calculate the tree let's say from the left and then you um you know you only keep the stack of one two three four five hashes and then um every time you move on you just append like you are accumulating sort of um the hashes on a certain level so you can basically compute this whole thing with a very limited memory so with about five um five accumulators for hashes and so forth but that's still quite large i mean um i can tell you that for example it says in ssd it might take you i don't know half an hour or something like this or sometimes an hour to get depending on the speed to essentially iterate through entire state and compute the state route so although it's sort of relatively quick but it's not as quick as you can you can't do it for every single block right it's too slow and so the problem then becomes is that okay what if we um we had a root uh calculated and we had maybe we cached the part of that that state tree in memory but because we don't store it in database you know once we flash the memory it's gone and then we needed to mod in our block we needed to modify this green thing and then sort of like how do we actually calculate the new state route how do we do that so in originally this is how i was doing it i would essentially load the entire region of the flat structure the entire one in anna would uh calculate the route and that was well that kind of worked as long as your um is my kind of cache of the state was large enough and in the beginning it was terribly slow like on a startup and but then it was also if star if if the if the block started to hit some cold spaces cold spaces in the state it was quite slow um but it i i sort of thought okay i'm gonna solve this problem later so i was running with that i was saying okay i know that it's really crap right at the moment but i i know we're gonna solve this uh because i had this idea how to solve it so if this next slide shows how we're going to discard this data because we just used it to compute the state root of that little subtree and then we discard it okay so the solution that i was thinking about all the time but i just didn't have time to implement it but now it's all implemented by the way so now it's all done and dusted so we going to store the what i call the intermediate hashes um so it's sort of you might think that oh but this is actually going back the direction of these um you know trees actually no because in the tree that the yellow paper suggests to do uh we are mapping the hashes into the um the nodes but here the mapping is kind of opposite to not opposite but slightly in reverse so we're mapping the prefixes in the trees to the hashes so essentially the function that this particular structure serves is simply to have those hashes and we store them in the database yeah and the storage is also um i would say that it has a nice properties because the let's go to the data locality and things like this and it's it's also pretty small um so now if we need to solve the exactly the same problem if we have this green thing that we modified and we don't have you know the the pieces of the trees around it in our cache so we can use this structure new structure that we introduced to help us to bridge the bridge the gaps and so that's how we do it we load only those uh things that i pointed out with the arrows from the database instead of the entire region and here we go and we can basically compute our new state route so this has been implemented this year um quite recently uh i think about april or something or may um and it has been optimized multiple times so now i show you um like what is the cost of that right um and also i'm going to show you like the how our storage looks like so the the structure the the the additional structure that i was talking about uh just now is called intermediate hashes it's currently for the like a recent reasonably recent state is about two um two something gigabytes so actually it's it's not a bad price to pay for this right and it is the structure which essentially stores all the intermediate hashes for all the levels of this tree and stuff like that and so you can also notice that there are two structures for the state and that's another innovation which we introduced quite recently so we state we store the state twice but why so first we store the state as a plane structure which is currently 9.9 gigabyte 9.09 gigabytes maybe maybe a bit more right now and then we have a hash state so some of you may know that in order to keep the kind of the the tree balance that we need to apply some hashes to the keys before we put in a structure so do we separate these two things so evm actually works for the plane state and then in order to produce the state route again only then we use the hash state so we convert it to the hash state and then we compute the hash there and but we have managed to get rid of the there was another table here which is called pre um so because we have the two states we actually donated pre-images and there are some other nice properties about it for example it allows us to withstand certain dose attacks easier than other implementations so yeah um so as i said i'm not going to touch on the history storage but it's also pretty efficient and so all in all um the efficiency in storage actually translates uh into efficiency of access as well because um things are simply smaller and and then they're actually more kind of standard so you can look them up quicker so now we're going to the second paradigm which is the the concurrency and so here is the something which kind of was discovered again this year is that uh in on the on the top you basically could see how the the things were processed before in goalie serum and entrepreneurs before that time and you can see that when we download the blocks uh there are let's say some sort of stages of processing for example we do recover signatures we recover senders from the signature so forth and we do we do certain things like run evm through it and compute some storage route or whatever it is and so usually this happens sort of in parallel with this kind of staggered staggered concurrency and lots and lots and things happening in parallel and so what we did is that we decided to split them up in like this and of course intuition would tell you that we tell you that this is actually slower if you just look at this graph diagram it's like oh no no this is definitely going to be slower actually uh it it probably was a bit slower in the beginning but then the the because we managed to split them into a more homogeneous pieces we were able to actually make the individual pieces are faster and in the end everything was much faster maybe like 10 times again this is another paradigm shift which you go into the counterintuitive direction and then you arrive at the kind of next level result and so this is the stage sink so what did what basically happens here is that we are trying to process as especially when we do the initial sync we try to process things in large batches as large as possible so for example if i would think right now my first batch would be more than 11 million blocks so i first put this 11 million blocks through the first stage and then another million in the whole batch through the second stage and so forth and so because we do that and so in the red i just put here um some text is that because we're putting most of these stages are actually essentially data transformation they take the data from one table and database they do something with it and put it in another table of the database that's why i'm calling ethereum a database so but with the databases that we use is that it's actually easier it's actually much more faster to to put that stuff in the database if it's pre-sorted so in other words database are pretty uh you should not use them for for sorting um so there are much better sorting algorithms and and so what what that basically graph means that we can use that properly to speed up the the sync quite a lot and so but if you let's say process one block at a time then the the time is not going to be so impressive but it would be very impressive if you start like processing millions block millionaire block at the time and this also explains why our implementation actually uh could work with um um hdd sync which hard drives actually one is currently just on a finishing line right now i started almost 30 years ago sort of 20 years 30 days ago sorry and it's today is is finally catching up with the with the tip of the chain but when it catches up it's not going to be able to process every single block through the stages what it will do which i tested before is that it will so you see this intersection here it's for for example for my hdd it will be about 15 blocks it means that it cannot process you know one single block quicker than 13 seconds but it can process 15 blocks approximately at the same speed as these 16 blocks are produced on the main net and so this is where we find the intersection so my hd sync would be always having a lagger of about 15 blocks but for some application it's probably all right i mean but as i say that because ssds are so much easier now so much faster and then because we don't require you to have five terabytes so you can actually buy a pretty cheap ssd and run it so you don't need hdd so it's pretty much for academic purposes at the moment and i think by the time uh our size grows to two two terabytes you're probably going to be able to afford ssd the same price as it it's now for one terabyte or something like that so anyway so that then we go to the third part third paradigm shift is the architecture so i put the note here about core dev calls in july 2020 some of you might have listened to those calls so for three calls in a row we decided to not discuss the ips for berlin but talk about other issues about kind of wider issues and we talked about the developer burnout we talked about development process and things like this about the client diversity and so maybe some people think it was completely useless but actually i thought it was really really good and i learned i got basically i uh i found it very informative and i made some conclusions out of it and so one thing we debated on the call is that whether the modularization and splitting things in component would actually help and so there were opinions on the call with saying that if you start splitting things up um you know you lose the optimizations because there was assumption that optimizations have to be cross-cutting and they basically like tie everything together and then eventually becomes a big spaghetti but my experience is actually at the opposite and there was a uh one person on the call actually agreed with me and so this is where we are on the journey to make this happen and so as you can see here that one thing is already done two things in progress one thing will be started and actually on the right here is that the stage sink is also part of that um so it's also kind of complementation so and one of the interesting things about this architecture is that it hopefully will allow us to bring more developers into the into the project and into the core developer the development in general it does actually also lead to certain sort of updation of power from the core developers but that's a it's a bit larger topic and i would probably discuss it in somewhere else in kind of more long form whatever chats and podcasts and stuff like that um then the last slide which is a bit of a you probably heard about uh or seen me talking about uh 10x improvement in ethereum so and today i just realized that oh you know why is it can't we go from one ethereum one x to a theorem 10x or some x maybe 2x so and so the if we if we actually take it seriously then uh there are from my point of view three main challenges here uh so it's a nice goal to have of course to to to do 10x on l on layer one uh but um there are three main challenges which i sort of described here i'm probably not going to unpack all of them for you but they're the dos attacks and there are certain those attacks we know about and some of them are because of the state access but some of them are computational some of them are related to pre-compiles and that's thanks to vitalik to point pointing out to me so we're currently actually assessing those uh things uh i mean i know that some groups are doing the same thing but we're assessing it in the context of turbo gas we are finding more attacks and we're trying to figure out how to protect against them and so we just recently started this effort and then there are other things that there are other two things that people are worried about but i think they are also solvable so on this point i think i'm going to end my presentation and i don't know if you have any time left sorry guys uh thank you so much alexi we uh do have a few minutes for questions and we actually have a lot of questions coming in so we'll do is i'll just kind of ask a few here and if you end up running out of time i'll just ask you to uh join the live chat on life.ethonline.org and you can just answer them directly there so painful clarifications a handful of broader bigger questions uh the first one being does double hashing attempt to address any long-term state degradation related issues i'm not sure about the double hashing but uh i don't know i'm not sure what it means uh what what is meant by double hashing to be honest sorry i think this will this comment will reach the audience in about 30 seconds so i'll just have them see and clarify this but in the meantime i'll just move on to the next question and that is uh so it's i mean you kind of talked about that it's taken about three years um to kind of get where you are uh purely out of curiosity what was like the most surprising breakthrough um in that time from kind of your perspective i think the stage sync was the most surprising thing because i kind of thought i discovered very early on that data model would be beneficial it's just that with the data model essentially the reason why it took so long is because um it's sort of like in engineering you solve one problem but then you get another two problems to solve and then you solve those two and then there's another three and so i wasn't sure whether this sort of i will be able to to tie all the loose ends in the end and then i only figured out like last year that i will be able to tie all the loose ends and it will work but the generally i thought it would be it would work but with the stage thing for example or now with a competent com component come up components that was quite surprising to me that such simple things actually make such a huge impact that's awesome i guess maybe a follow up on that how how many people are currently kind of helping you with being a contributor to turboget right so it's actually the team has grown quite quickly and uh so specifically this year so we currently have about 13 people who are basically uh working on it like 10 full time three part time and i'm most of the time i'm losing count because i'm actually inviting people all the time and the reason i'm inviting people more all the time because i do want to test this idea that if we if we do things in components we can have many more contributors which are sort of they're not just waiting for the pull request to be merged but they are there to develop their components maybe on multiple different implementations of the same component um and so forth so i'm i'm actually up for testing the the the theory that we could have a much wider uh contributions no it's awesome it's a it's a wonderful a wonderful problem to have when you're uh scrolling too fast and you get a track who uh who's fully contributing so that's wonderful um another question that um sort of came from the answer you just gave and that is what was the actual original intuition that sort of uh made you even try stage thinking in the first place was just experimentation or it was essentially um yeah that's a good question because what i was so basically i was profiling to brigade all the time at some point i got to the to the place where the the right rights to the disc were the bottleneck at some point and obviously as i normally do that i started to uh split um like luckily the database we had allowed us to um look at the like a right short like i call the right chord and how much data you write for each table rather than on the entire database and i noticed certain tables were were very intensive in terms of writing and so those were i i saw that those were the ones that um they had a very sort of randomized keys so for example the transaction lookup one where if you have a transaction hash you can figure out which block this transaction and obviously the keys being the transaction hashes are pretty randomized and so we were inserting those randomized hashes within the middle of the table and i was scratching my head thinking maybe if we try to to pre-sort them like if we if we just take the whole bunch of them like the like 900 million of them pre-sort from them by hash and insert them that way is it going to be faster and it turned out to be faster and this whole thing generated the essentially the stage sync it required a bit of a bravery because in order to push it through we had to delay the release and we had to rip everything apart and then change the architecture entirely into about two months it was actually kind of risky thing to do because some people said no you shouldn't do that it's just like release and then change it but no no if i release i'm never gonna change this i have to do it right now and i and i think it sort of paid off um absolutely no iris guy reward and uh glad that you you did last question for today before we went to our next talk and that is um i mean it's a pretty big core value in our space that uh people should be able to run um and and kind of individually run uh and validate notes and uh and especially from like the the genesis block so what are you kind of thoughts on on this uh as a norm um and is kind of is one of the motivations behind turbo get to actually make this world a lot more possible or or is it purely it i mean it wasn't started for this reason i mean i simply wanted to create a better technology i don't really like i don't know if it's going to be the norm because maybe if we do like let's say it's 10x maybe we will make it impossible again to essentially we improve the technology but we use it for not to uh let everybody run a node on their uh on a raspberry pi but maybe we're gonna use it to expand the the boundaries of the system instead right so i don't know it's not for me to uh to kind of make this sort of into ideology i think i'm just want to make a better technology and we'll see how this is going to what it's going to do is it going to make people run raspberry pi's or is it going to make ethereum 10 times bigger right so it's wonderful i think you're taking the stance of being a technology maximalist and uh that's and that's awesome um so there's a few more questions that are in in the chat so i'd encourage you to just kind of reply in the chat directly and i want to thank you again for uh doing this talk and answering all of our questions and uh a lot of excitement on the chat so thank you so much alexa thank you bye 