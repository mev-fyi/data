[Music] [Music] [Music] so this so all right welcome everybody my name is kartik i'm one of the co-founders of eats global and i want to welcome everybody to day five of hack defense judgings lack of this is a hackathon that we're partnering up with protocol labs on z global and we're super excited to kick off a whole week of judging and showcasing all of our amazing projects from this event so for those of you who are going to be watching this later on as a video over the past month we've had 470 hackers from 50 different countries working across 19 different time zones and they've been working on playing with what's possible with the world of ethereum and protocol live small coin and ipfs and as of last thursday we've had 132 projects that have come out of this event and we're using this entire week to showcase every project and having them be known to the rest of the world so before we move on to the demos for today i want to quickly go over the logistics of how the event itself was set up and how today is going to work uh we're going to have about seven teams today uh they'll be presenting to our judges and each team will have four minutes to demo and we'll have a four minute q a session and to minimize any technical difficulties we've asked all our teams to pre-record their demos and they'll be playing them as they come on and kind of briefly everything that we're going to see today was uh either a set of individual or a few members up to five members that worked on a project and everything you're going to see today was worked on over the course of the last four weeks and the only criteria we had for all of these projects was that everything we're going to see today um was not only just built over the course of the hackathon but also must incorporate the tools and technologies from the protocol labs and the ethereum ecosystem so we're super excited to see a lot of interesting mashups that bring the best of decentralized storage in smart contracts and as for how judging itself is going to work our judges are going to be rating our projects on how technical original practical and usable they are and we understand and realize that those four categories are not enough for for everything so we also have a general catch-all that we like to call the wow factor to incorporate anything that we may have missed on our criteria so before i really go into the demos i want to make sure i emphasize that this is not a competition this event is here for all of our hackers to learn and see what's actually possible they're very much here to share their excitement and the judges are here primarily to give feedback and offer their suggestions and comments on how you can take this thing further and make this even better than what you originally intended and not everybody's trying to become a business so while we'll see certain things that are ready to be uh commercialized after this event uh the goal for whole hack fest is to very much promote experimentation and this is kind of the theme and our idea for this event so with that i want to really kick off our our judging day five we're gonna see these seven teams present today and uh doing the hard job or three judges someone welcome indra hill from textile pj shot from protocol labs and kyle tut from pinata will be here with us for the next hour um talking to the teams and uh and giving their comments on all these projects so without further ado i'd like to call up our first demo for the day and that is team climate data pool and i welcome them to share their demo and take it from here so as a note kit we'll be uh talking over the video so whenever he's ready we'll kick this off hi welcome today we're doing a demo of climate data pool this heck fs project is a proof of concept for decentralized global data storage our use case stems from the paris climate accord in that agreement all the nations of the world united to tackle climate change climate data pool intends to unite the reporting of the nations on a blockchain each signatory country is responsible for submitting reports on a scheduled basis in our contact with the united nations we learned that having one single canonical file as a data source would be a real advantage well that's easy with ipfs we wrote two user stories for the hackathon one is sam the civil servant based in singapore who's responsible for uploading reports the other story involves riley a climate researcher who could be based anywhere let's start with sam when sam logs in he lands in the overview screen of his country's document storage you can see from the url that we're using fleek hosting so all of this is on ipfs we're using fleek extensively maybe you recognize its user interface we're imagining that fleeq offers a white label service which can be branded so we fork the ui into a document management app in storage jargon each country and climate data pool has its own bucket with its own environment on the left is a country selector and a menu with overview files listing upload and team the ui presents a number of tasks that sam could engage and shows a short list of recently uploaded documents sam has to upload a report so he goes to the upload screen he selects a file this is a document entitled energy usage analysis of decentralized data storage he gets the title on his clipboard paste it in then he selects the document type and he clicks upload voila the feedback tells sam his upload succeeded now let's look at what's going on underwater we wrote scripts which use a fleek sdk i'm going to run one called get lists js this will show us a list of files in singapore's bucket and there's the new file a listing like this would be used to populate the front-end interface now we're going to switch roles to riley the researcher when searching she'll see a screen similar to the overview that sam sees but with limited menu items on the left and more metadata about the documents instead of mana's actions the screen will have download buttons riley searches again and there's the new file rowdy clicks the download button and the document loads at this point she's done and we're done with our demo there's a team page with information about us thanks to ets global and the sponsors for running a superbly organized and inspiring hackathon awesome well thank you so much for that demo and now with that i'd like to kick off our q a with uh our judges so any comments if you bad welcome thanks i have a question shoot were you using uh were you using the directed graph to do any inter-document linking and can you explain that if so no we're not using a a dag that's that's something we considered but the uh the document linking at this point is only envisioned we haven't actually implemented anything uh you said you had done um some conversations with uh the un or the paris climate accord um can you tell me a little about that research sure we participated in i.t challenges that the un runs on a regular basis and in one of them in 2017 we proposed uh storing the climate data documents from the paris accord on a blockchain and we used ipfs those were early days so we were doing command line stuff and we've since then taken that concept further and we've used it for this hackathon to take advantage of the yeah the wonderful developing ecosystem so during the challenge we also got to meet the judges and they are the ones who told us about the requirement and things about the canonical files and all that so kit actually went to new york and met the judges and they actually uh spoke about their requirements and stuff i have a question as well which is if you uh if you had had more time to work on this project what would you have implemented next well we we originally wanted to implement the the underwater stuff so that it was all real the uh the scripts that we wrote they actually work but they're not hooked up to the web user interface and that was the what the the complicated aspect that we had to try as out of the proof of concept great i think uh one of our judges just had some navy issues so we'll uh wait a few seconds for him to come back on kyle are you back i'll just feel really quick that i think this is an awesome area to be working so i'm i'm really excited to see your submission thanks and all right uh this is a this is a great great cause to uh to use ipfs for so um it's great to see yeah ipfs is really perfect when you want to have a canonical file which is really important for the researchers it's just built in i also really really like that you um took the approach of kind of fleshing out the ui because it does help um kind of share the vision for what this sort of thing could be and um i can totally imagine showing this to someone who works uh you know in in that realm or at the un and like having someone really buy into how they could use this in their workflow because you have a really cool demo of it so um i think it's a great first step and uh excited to see where you take it next thanks yeah we enjoyed it abby maybe you want to talk about our next step yeah basically one other next step after the ui and backend linking we also wanted to uh build a small middleware component which will serve both the web 2 and 3 interfaces so if you have a front end that supports only web 2 we'll still support it but we want to move to a three thing so we want to uh uh kind of enable people to move from web 2 to 3 through our system so we are investing middleware comprehend that will probably do that super cool well uh congrats on demoing and building this amazing project we hope you continue uh working on it and i want to thank you for uh coming on first and with that i'd like to move on to our second demo for the day and that is team zentavise so adam maddie mott feel free to kick off with your demo bye everybody um okay so let's start the video hi we're team identifies and we're doing incentivized machine learning where you can provide rewards for models train on your data and earn rewards for training models a lot of small companies have very valuable user data but they can't extract any value from it because ml engineers are very hard to hire there is a need for a data marketplace where someone with limited expertise can publish a data set and a problem definition then others can come and attempt to create models that solve those problems and be paid based on their model performance the basic way that our system works is that a user who holds the data but doesn't have ml experience can submit a proposal for model training that proposal comes with some reward bounty other users then submit their trained models against that proposal's training set the accuracy is self-reported for each model and a reward is proportionally dispensed at the end of the proposal according to each model's self-reported accuracy if other users believe that accuracy is incorrect on a model they can dispute it which will trigger a verifiable resolution process that reaches out to a compute oracle now let's hop over to a demo to show you how it works first let's say i'm a project manager and i want to submit proposal we want to figure out the lifetime value of our users um you know we have the data and we want people to write models that will figure that out um so this is a regression uh type you know they could be worth zero or hundred dollars over their lifetime or more and this is important so tight timeline let's say we give people a month to submit solutions uh also a high bounty because this is an important problem to our company now let's upload the training and validation data so training feature data and this is going to ipfs obviously and you can upload this through our cli script as well as well as the evaluation script that will load the actual model that you submit and run it so this is going to ask me for my stake which is 10 ether cool now let's switch over to a different user who wants to submit a solution cool and we see this proposal it's worth 10 awesome so we're going to upload a an onyx model here um um as well you can upload a pre-processor script this will allow the oracle to pre-process the data and you want to guarantee verifiability um as well as an accuracy score and it's self-reported and people can dispute that let's say 90. so let's um submit that it's going to ask for gas confirm cool so now um let's go select another user a third user and we can see that this user is going to get the entirety of the reward because nobody else has submitted a solution however i think that this reward maybe this person was lying i ran their their model and it didn't run so i'm gonna actually dispute that and that's gonna do is if i win the dispute i get all the money um from here here confirm that and um i won the dispute so i will receive the funds from this this person was lying so there you go thanks for watching the basic architecture of our system is entirely based around a front end and a smart contract the front end stores a lot of its data in the smart contract and also uses ipfs to store larger data set information such as training and test data and the proposal metadata there is a cli script we provide which users can use to upload their training and test data using powergate this ensures that the data is available and persisted globally we use fleeq to host the front end we believe that ml engineers should have the ability to upload preprocessor scripts which can output transformed data improving the training accuracy of models any model that is trained using an uploaded preprocessor script will share its reward with the original author of the preprocessor we also want to finish implementing the use of a computational oracle in order to verifiably resolve disputes against the self-reported accuracy of the trained models thank you for watching our presentation awesome i'll uh let the judges ask questions okay i'll jump in again um uh so i saw a few really cool kind of moving pieces a little hard to tell which pieces were done i think maybe one cool way to like wrap that up what do you think the hardest piece that you built uh was like what do you think the hard part that we should focus on uh is oh you mean in terms of like developer experience yeah yeah where did you find like difficult pieces to put together that you feel like you you had some achievement there um to share with us well i think that like all of us were coming into this without any experience at all in like doing any kind of like web 3d development so all of this was new to us but the biggest thing like i guess the biggest account departure from what we've done before was writing smart contracts um just because of sort of like the way you have to like shift your mental model of like how how things work um and like i'm primarily like a javascript developer so there's like a lot of like there's a pretty steep learning curve and trying to figure out how like solidity works and all the sort of like drawbacks it has and like the limitations it has still because it's kind of like i guess like a little bit like immature sort of so um so there was a little bit of like working around to do there so that's probably that was probably like the biggest uh thing that was like pretty yeah like um trying to associate um sort of to be able to have like a um a sort of data model where you can associate a list of solutions with a certain um proposal and to be able to like have different users be able to see those lists of solutions and then um you know have the calculation that shows okay there's a reward that's distributed in a way this amount among users is kind of like almost making a database within like solidity kind of hackily so yeah cool um could you say a little bit more about how you landed on the particular ski the scheme that you're using for reward distribution like maybe what other models you investigated there and why you landed on on the one that you ended up with yeah so like the um the kind of like primary goal behind how we dispense the rewards is that we didn't want anybody to have like completely wasted work if because like training a model is usually like a pretty expensive like computational operation like you gotta have like a giant cluster and like spend a bunch of money on running those computers so we wanted to make sure that like you would you would at least have like some guarantee that you would get something if you submitted a model and it might be that like if a lot of people were interested in that reward then like the like the reward would kind of get split proportionally in a way that might be like unfavorable to you but it also means that like everybody that did some work will still get like some compensation um and i guess like the hope is that in general you'll like you'll get more back that you might have like put in in terms of like your time and your your resources so that's kind of why we landed on that approach uh the guest question i have is do you know of anything similar to this out there or uh i guess what what was your experience that led you to come up with this idea yes the most similar thing that we looked at and this was actually suggested in one of the like feedback sessions was um this service called numerix um and they basically run this thing where like every week they have sort of like a training competition against like stock market data and it's basically like a prediction problem of like uh like stock market data goes in like give us like sort of like predictions coming out and then um it's kind of similar to our thing where like you get rewarded and like paid based on um like how good your your like uh prediction is but in that case it's kind of like one specific problem and also like you don't give them like a model to run against like any arbitrary data you give them sort of like the predictions in the csv file so it's like it kind of like benefits only like numerai itself rather than like the general public because like in our in our sort of system like you can just like download the model directly off the interface and you can like just run it against anything right so it kind of creates like a bit more like a public good i guess um well i think this is a really really cool project and it's pretty impressive what y'all were able to accomplish in in just a month and especially being new to web3 it's really really cool it seems like you've touched a lot of um broad ranging technologies including decentralized compute and you know powergate so ipvs and biocoin and everything so really excited to see what you all have already accomplished thank you totally plus one that yeah uh i already have uh many thoughts on uh how we would use it and um i think it's it's right right along the the right path so great job great thank you very much thank you awesome well thanks so much for uh for demoing and uh also doing this thing across multiple time zones i feel like this team is all over the place um so uh with that we'll uh move on to our third demo for today and i'd like to welcome robin from multiverse i'm so robin please uh feel free to kick off with your demo [Music] so we have a crypto domain uh thanks to unstoppable domain extension we will go back to the place where we are hosting the website a multiverse is a decentralized social network you can upload text or links and chat we post text is stored in three box storage photos through textile buckets likes and comments through three box thread post visibility meaning we can see the post where all the read and write requests goes through the cache lab all the right requests made within 30 seconds are batched up and sent as one single request to three box each post is encrypted using a symmetric key for public post the key is shared in the post metadata for post that is meant for friends this key is encrypted using the encryption key of that user that user and all the friends or his or her friend have access to that key so that's the need for the friend request so that the encryption key can be shared let's create an account uh now we have a couple of meta marks and three box signing need to give a couple of permissions now we have to select a username and our profile is created let's send a friend request we copy the address put it in the third bar and you find the user and let's send a friend request and it instantly appears in the friends account we can accept or reject let's say we reject and it's already added to a block list uh we can see a notification that our request has been denied let's remove the unblock right now and the friend request is back again we accept we get notification that my project accepted your friend request let's upload a pic and profile pic is also updated we can also share a profile with various social media websites like twitter facebook and so on so for chat we both the people has to be friends let's open the chat window we can see the friend let's send the message a notification popped up saying that a new message is coming we can see the message let's type a reply and it instantly appears now the chat is working through three block persistent thread and the thread is private between these two users we can also have other chat session with other users now i'll show you how to create a post click on the button uh now you can have a rich text editor you can select the text you want uh you can select the post visibility meaning who can see the post let's make it private now uh let's upload images images will be stored in textile buckets and the post metadata which is encrypted will be stored in three box storage so a post is created you can see the time and the status of private uh let's see in this account the same profile but they can't access the private post we have a like action do a like the like instantly appears in the other account you can see who like the post another like you can write a comment you can see the comment also we have photo albums thanks for watching awesome thank you so much for that really cool demo robin and i'll uh i'll let our judges ask uh any questions hi i remember the super cool demo um i i wasn't sure if i heard it correctly but it sounded like you said that um the images are stored in textile buckets but the post metadata is stored um somewhere else it sounded like it would said three bucks um is that correct and and if so could you share a little bit more about um how you architected the system to have different okay so each port has a submitted key which are used to encrypt the data post data metadata in portrait where the images are stored immediately stored in text packet that is also encrypted and then we have timestamp the poster post visibility active course content itself and that the metadata of this will be stored and either public space publicly box storage of the user or private storage based on the visibility if it is private visibility to restore and private storage if it is public or friends will be stored in public but if it is friend visibility then the whole metadata uh so it's already encrypted with the key so if it is front i encrypt it again with the encryption key of the poster and store it and public reboot yeah super floored by how much of the user interface you completed that is pretty amazing it's pretty amazing so i'm curious i just kind of want to back out from that a little bit and maybe add on to where pujo is headed when you first started thinking about building this how did you conceptualize this this uh architecture this data model were there a lot of iterations in there or did you kind of know how to piece this together what was that process like a lot of iteration the design i actually thought of keep changing evolving a lot of components actually want to build and halfway through i realized i've never had time to finish it so the a lot of things that happened at the one month plus i could done for a lot of things yeah uh my question is do you think there's more of an advantage for the users to um hold and control their data or do you think more of the advantages on the um basically obscuring it from from other people what do you where do you think kind of um the best advantages for this over traditional social media sites i actually wanted people to have control of their own data so they can decide where they want to store the data what data they want to store so people can actually use the data for some other malicious purposes or ads so we hear a lot of data leaked these days and if you control your data then if something happened it's your responsibility uh i just want to try and see whether such a model can actually be built or not [Music] well thank you so much robin i want to just also congratulate you on doing all this thing by yourself um one man show so uh this is really impressive and uh with that i'd like to move on to our next demo and that is uh mapping the network and cost of uh feel free to share your screen and present the video all right hey guys my name is costco and i'll be presenting this little analytics tool and call it toothless analytics because i definitely didn't get to work on it too much because i literally lost it to during the hackathon but uh it breaks down in these four main steps so just remember them because the demo kind of goes by really quickly data generation storage on pinata dashboarding and then sharing it all right hopefully this works can you guys hear that uh we cannot cost everything you have to share again and make sure the audio checkpoint because damn got that again thank you karthik okay go and this is the demo for um falcoin visualizations so what we do is we get peering information from the falcoid network using this node.js script that use thanks for that uses textiles powergate um and actually so the hosted node defense that network and you can see this is kind of the data that comes in it's written into a json file and then the json file is uploaded to both github and we also use pinata and pinata is actually really fast so that will come through in real time and boom we have that uploaded so next step is actually creating these dashboards actually having a place where you can play with the data and figure out how you want to display it so we did that here um for a second i think the video is being downgraded to uh 360 or some other performance so we're not able to read anything um let me know if uh if it's if it's the same video and this is the demo also the same quality i think um okay give me one you're able to share the video with me i can play it on my side yeah we'll make sure that the quality is high sure and uh grab the link for you just put it on the zoo and this is the demo for i'll just do it here it's kind of hard to pop out the link one sec [Music] all right go and okay now can you paste it on the zoom chat yep ah great i need a copy and just light it up all right so just give me one second and i'll play it here here's a slide if anyone just wants to look at the notes this is a demo for um all coin visualizations so what we do is we get current information from five using this node.js script that uses textiles target um it actually uses the host node to test that network and you can see this is kind of the data that comes in it's written into a json file then the json file is uploaded to both up and we also use pinata and he got really fast this should be shared with everyone let's use that most ta-da get these nice little map visualizations and we do load it from miata i just directly go to the gateway for it since that way i don't need to upload my um api key on a static side or anything and so i created a few short visualizations there's another one on a different notebook which should have a picture in the repo and so once these have been created they can actually get exported as static sites which is super cool um we can open this in the vega editor and while that loads let me actually pull up the site over here oh there we go so that's json and you should find the site and generates the static site and the static site source basically just takes the spec and loads it that's all there is to it thanks but i think maybe it'll be helpful because if you can just also just go over some of the the reasons behind why you wanted to work on this thing so we can kind of get the remaining context because now we have to merge the slideshow yeah um basically kind of we go through these steps right and the why is being able to share these dashboards and being able to demo kind of an example is in the states um with the covet data it's not actually immutable and on top of that the public can't actually use that data um when it's just like uploaded right actually making the dashboards actually making the data consumable in an easy way is kind of important and so what i tried to do was basically create a pipeline that would automatically generate these websites given data and you could use these dashboards and just go into a jupyter notebook play with it and then figure out what kind of dashboards worked so that's kind of why and uh yeah i like slipped and filled so got some teeth so didn't really get a chance to really flesh out the full pipeline mainly got to the website um the notebook and so that i ran that for a week and this is all online this is actually in the repo and you guys can kind of look at it if you want as well awesome we'll let our judges take it from here um this is really cool is the in the um demo was that are the visualizations are they somehow connected to a node and getting data live or was that data certainly no it didn't get a chance to finish that part but they are interactive you can like hover over the little images and stuff awesome so it seems like you wanted to go kind of um into other data types like you mentioned the covet use case yeah trying to pipe some of these things through there what what are the ones to do in mind where do you want to where do you want to land with this well so i wanted to actually have like a generalized solution and there's actually a lot of stuff out there in the framework that let you pretty much pipe and build in essence a kind of like lightweight analytics solution on top of this technology and it can just be a static site right and then this sort of like data generation that lets you just upload the data in a mutable fashion onto ipfs through something and i wanted to look at covert data i wanted to look at actually more data like deals data using the hosted node network but i didn't get a chance to do that and like use your data on ethereum net 2. yeah i i don't really have a question but um comment wise um a lot of a lot of the use cases that we see being built on pinata i feel like this could add a lot of value to them just with their their data sets that they have so great job on that front thank you for pinata well if you don't have any other questions we can move on to our next demo but uh thank you so much gossip and i hope uh you keep building thank you karthik good luck guys see ya and now what that would like to call up our next team and that is team not pied piper so i'll let them kick off with their demo hey guys our pack is called not pipe piper and what we tried to do with not piper is basically we wanted to allow users to upload videos and other large files onto these public distributed networks like ipfs while allowing them to still have control over who can see their files and you know i noticed that encryption schemes like pgp did not work too well with very large files so we came up with this entirely new encryption scheme that is not even based on cryptography but uh rather based on neural networks we're using um cyclogan to basically create a sort of mask that is applied to a video and this mask makes the video unrecognizable to another ai and it can be used to obfuscate videos and you know store them anywhere and basically without this mask you won't be able to restore the video to its original form so it can be thought of as a kind of key um although it has nothing to do with cryptography and this works well with large files so the way it works is a user will generate an encrypted version of their video as well as a mask and they can upload this video to ipfs they can store the mask on ipfs as well and this mask can only be accessed by authorized people including the user himself and then the user will you know fetch this mask from ipfs whenever they want to watch this video and they'll get the encrypted video and they can just use the mask to decrypt the video so let's take a look at how it works we have this um this video here of a couple and some apples and we are going to create an encrypted version of this video and then we're also going to create the mask so now we have our obfuscated video where uh the couple has become zebras as you can see their identities are protected so from a privacy standpoint this is a this is pretty useful okay so now we want to generate our mask and upload this mask to ipfs so let's go ahead and do that upload ipfs get our hash and now we can fetch it from ipfs at any time and this is the ipfs main net great so we're going to use this mask now to restore this obfuscated video and back into the original video and there you have it that's it hey guys our pack is called not pipe amazing um i'm sure everybody has a lot of questions so i'll let them uh go here this is yeah so interesting um is this a well i guess i have two questions one is like is this a i've never seen this form of um like encryption before so i guess i'm curious uh what um what's the like context around it and what do you envision are the main use cases for it yeah sure so um i guess really the idea was you know to think about you know if if public key cryptography didn't exist or you know how we saw it's not it's not necessarily good enough for encrypting large video files how would we kind of reimagine it from scratch right as to people who don't really know anything about cryptography using cycle again was very interesting from the perspective of protecting user privacy because it actually you know it doesn't just randomly alter the video it actually tries to alter the video to the extent that like another ai cannot recognize it right so in theory right if it's good enough it's rendering it unrecognizable to to another person right so that was the basis of it from like a privacy perspective and you know we just wanted to kind of uh you know do this this alternative version of cryptography and you think like um for anything that's like stored in the public domain right or something that um you know maybe you don't trust like pgp or you think public key cryptography is is uh is gonna be hacked by quantum computers uh this is a way for which you know your your content can still be out there right in the public domain or even if you know it's it's stored somewhere encrypted people can you know find it and they still won't be able to reconstruct the original video it is um extremely computationally expensive to you know reconstruct this video to its original form without this um without these steganographic masks because they're actually much more complex in their structure than just like a cryptographic key yeah i think it's extremely creative so uh really cool thanks yeah i uh i love how you came at it with um thinking about a way with without doing encryption um i think that's a super unique um mind frame to come from so that's super impressive um and so one of the things that i'm thinking about is uh there's a couple like nft kind of projects out there that are trying to use multiple nfts to kind of um overlay um images or videos over top of each other so that you can change them um so i'm just kind of thinking how you could uh actually leverage that um kind of with nfts if you have the nft then you have access to the um uh to the the ai uh decryption key i guess if you will um so yeah super super interesting and i think there's going to be a lot of opportunities to keep working on this thanks yeah i don't i don't really have many questions i guess it's really far out it's very very cool idea so i guess that is my question is like what led you to this idea specifically that algorithm thinking that you could apply it and then like hide in the open the way that the ipfs network works like how did you kind of arrive to that yeah so i it started with the idea of just like um staginography uh and saganography is basically you know where you have an image and you know you have applied some kind of mask to it and you know such that unless you have this mask you can't get back the original image this is like a very kind of old concept right of hiding things in plain sight and you know not being able to you know reconstruct the original thing without i guess knowing what pattern to use so we started with this idea and uh you know sid is kind of um an ml wizard right so he was like hey you know what if we use cyclogan it's specifically designed to you know uh create you know things like like realistic humans is designed to fool other ais right so this we thought it would be the most modern kind of uh incarnation of of this very old concept of steganography and we thought that you know maybe we deserved a second look um compared to all of these you know cryptography based approaches to to securing files so yeah i just started with this kind of old idea of saganography one uh one quick question for me how how does this decryption happen does it require pre-processing or post-processing the whole thing or can you do that frame by frame yeah so decryption is um it is frame by frame right so that if um you know you can correct me if i'm wrong but basically what happens is yeah yeah so you got to split it up frame by frame and you know there's different matrices sparse matrices that are applied to every single frame this is really cool because uh kind of just jumping off of calzady on nft side you can actually do the key decryption client side with just mp4 filters on top of um kind of a canvas so you can actually use decrypt client side if it's frame by frame and i'll just be even cooler because you can store an encrypted video just be faster and they can be decrypted per user who has access really cool idea thanks awesome well with that i will move on to our second last demo for uh for the day and that is team fractal so i think jacob and the team is already here i'll let them uh turn the video on and share their video hey everyone we're team fractal fractal is a decentralized ethereum accounting tool that preserves user privacy and provides insight into interactions with d5 protocols d5 is poised to become the new paradigm in online finance usage of lending and trading protocols has exploded in recent times as retail users are discovering the new world of peer-to-peer finance yet the accounting tooling in the space is very undeveloped despite the open and permissive nature of ethereum transaction data is still proprietary and expensive for retail users to access currently there exists no platform to categorize and compile transaction data while preserving user anonymity centralized account based solutions exists however they are a point of failure in preserving your identity we believe that profiling users is the wrong approach to accounting on ethereum the emergence of d5 is only set to exacerbate this problem fractal is a privacy preserving ethereum accounting tool that gives insights into ethereum erc20 and d5 protocol transactions fractals reports turn these basic transactions into enriched categorized transactions that provides its users direct value most importantly fractal respects user's privacy by enabling them to remain pseudonymous while aggregating and sorting their ethereum transactions all user data is encrypted and stored on ipfs data that only the user can unlock all of fractal's infrastructure is deployed on fleek we created and deployed our own ipfs node for hosting encrypted user data with asymmetric encryption giving users a completely secure experience it is only the users themselves who have access to the keys to unlock the ipfs hosted data once the computations are complete users are able to download their attach reports from ipfs using flex space statement or download directly to their home computer getting started with fractal is easy and fast here's how it works fractal supports both new and returning users returning users can reuse the ipfs content hash to render their previously generated fraction reports let's check out the report generation flow first users firstly enter their ethereum addresses that they wish to aggregate there is no limit to the number of addresses a user can enter then users can select d5 protocols they would like to gain insights into fractal currently supports unity swap and compound with dydx and rv coming soon protocol is onboarded by building in-house algorithms using graphql on the ethereum blockchain users then select the date range they would like the report to cover once these parameters are finalized the user is presented with the cost of the service to uphold transparency fractal displays the formula that we use to calculate this feedback once accepted the user is showing the ipfs content hash which can be used later to access the report users are prompted to pay and upon confirmation the report will begin generation this process involves our back-end trolling over the entire ethereum dataset to provide clear categorized transactions in usd for all ethereum and erc20 transactions our graph algorithms enrich the data further to provide protocol specific statistics when the report is generated is encrypted and hosted on ipfs our front end then fetches the data using the user's key and compiles the content of the dashboard with webassembly the dashboard provides a user holistic view of their addresses with the date period selected which includes the total transactions made usd in and out and their gas feed spin these statistics are then broken down into a month-on-month basis since uni swap and compound were selected we can visualize all of their transactions that were made within them like here we can see transfers between uh general ethereum addresses moving on we can see liquidity transfers uni swaps so here we can see the total amount length borrowed as well as repaid between compound protocol here we can see quite a big swap menu swap between uh wrapped ethe this is last last month once finalized users can download a complete csv or month by month csv of all their attractions in historical order reverting back to the alternative flow we can use the content hash we previously stored to view the report we paid for again this can also be used for handing the report over to accountants for other trusted advisors fractal is for all users of ethereum simply enter the addresses associated with your business or personal interactions and the tool will compile and categorize your transaction data while preserving your anonymity awesome let our judges ask any questions yeah i think um it's like a really polished demo it's really really awesome um i'm not as familiar with um with the ethereum ecosystem so i'm kind of curious from your perspective what was the the need that drove wanting to create this sort of tool after reaching out to several liquidy miners node providers as well as general ethereum and d5 users it was very clear that 80 of the problem is that the data is actually not there and then beyond that most people rather use their own accountant rather than a profiling online tool that will um identify them and aggregate and store their transactions so there was a huge demand that we found through discourse for having this decentralized uh private accounting tool that will show them uh aggregation and categorization of all their d5 and ethereum [Music] interactions [Music] can you explain a bit more about um like what are the privacy preserving steps how are people remaining synonymous in this is it just because it runs primarily locally or is there something is there something more layered in there yeah so i guess the encryption was one of the big privacy preserving steps um and on top of that rather than having like traditional accounting tools you create an account you then have like your id mapped to all these addresses or exchanges that you own we kind of obfuscate all of that like all we need is an address you pay using cryptos we have a payment smart contract um and then once it's generated like there's no data stored on our end we just upload it to ipfs and then it's essentially all yours after that point um so that's the main the main steps taking all the data away from us and giving it back to the user um maybe i missed it but uh how are you thinking of sharing it with the accountant yeah so the i guess the encryption we had a few different issues that's kind of still a work in progress but the original idea was using public private crypto on the client side and then generating a symmetric key on the back end um so then the client would have that symmetric symmetric key decrypted so then i guess essentially you'd have to share both the symmetric key and the ipfs content hash um so that's where that kind of like plug in your ipfs cache flow comes into play but yeah it needs to be ironed out a little bit for that encryption step great thank you did the nice weather and uh and birds chirping help you put together such a nice ui is there some connection there that's just put a good backdrop on this is just every day yeah really cool stuff amazing um no this was really cool so if there are no more questions i will uh we'll end this here and uh i've been notified that the last team is having internet issues so what we'll do is we'll just give them a minute or two to see if they can rejoin if not we'll just end our session here today and we'll reschedule this team to uh go in the next slide so maybe this uh this is a good opportunity for our judges to gather their thoughts take a break stretch a little and we'll just try to come back in two minutes looks like we'll uh we'll have to move the last team to a different judging slot so we'll uh we'll just end it here all of that said i want to thank all of our judges for giving us the hour today and uh sharing their amazing feedback and comments so uh if all of the judges can quickly check their email after this uh session ends they'll be appreciated but uh thanks again uh for giving us your time and thanks to all the teams from all over the world demoing today uh and showing what they did to all of us and uh we'll resume judging tomorrow thanks everybody thank you thanks thank you everyone 