[Music] good morning good afternoon and good evening everybody my name is kartik one of the co-founders of beef global i want to welcome all of you to the polygon zk uh you're all watching this thing on eve global.tv so just a quick reminder for everybody who's coming in for the first time we encourage all of you to log in and uh say hi to all of us uh this is the live platform we're going to be using for the rest of the day and if you have any questions for our speakers or if you have any comments if you want to relate to any of our audience or our speakers you can write them in the chat there and i will be able to relate them directly to our speakers and also for everybody who logs in you will be getting a nft proapp token so stay tuned for that after the end of the summit so this event is brought to you by eve global and for those of you who don't know if global is an organization with a very simple goal our goal is to get more web3 developers into this ecosystem and we do this by running hackathons and summits and this is our last event of the year and we're super excited to be hosting this day talking about all things ck so let's get into what today is going to look like uh so we've an amazing day planned we're going to talk about a handful of things we're going to do a quick uh update on what is happening in the polygon ecosystem and with respect to zero knowledge uh world uh there's gonna be an exciting announcement from the polygon team which uh i can't wait for uh everybody to also find out about uh then we're gonna go into a few demos and talks on how polygon nightfall maiden and hermes are set up so we're going to go into the the polynomial identity language uh by the zkbm team and then lastly we're going to have a panel uh discussion with all of our speakers and metallic veteran to uh just cover what we can do to move this entire ecosystem forward so without further ado let's jump right in i want to invite sandeep genti and mihailo the founders of polygon to come on stage and uh share some of our their thoughts and without further ado let's bring all of them on to uh the screen welcome everybody thank you hello everyone everyone hello also so yeah let's begin with a small intro you know from all of us so i'm sandeep i am one of the co-founders at polygon and i uh look at the business side of things uh that means the you know the business development marketing and you know all the things related to jd hey uh gently here founder of polygon started out of medic here um started tech and looking after the tech and researcher nowadays yeah hello everyone i'm mikhail one of the co-founders of polygon i'm really interesting interested in scaling in our research efforts strategic type of things and uh yeah thank you so much for attending this event in such a large number we are very humbled and honored to see you guys here yep amazing so uh you know i uh you know let's start this event off with uh basically brief about the purpose of polygon and then we can move on to um you know different things we'll walk you through our timeline and you know about our zika things and and things like that so uh but but you know like the talk or the discussion about polygon cannot start without the discussion of uh ethereum right and uh you know this is so i you know we wanted to discuss about first uh the what is the core thesis of polygon here what what exactly uh polygon is is trying to achieve here right so uh you know basically this this whole web 3 space and and what blockchains are trying to do is to provide uh uh basically this internet of value right web 2 was internet of information and web 3 is basically internet of value and our strong belief is that you know ethereum has strongly emerged as the value settlement layer which is highly decentralized highly secure environment for the value to trans value to transact globally and probably you know even beyond the globe once uh human human rights human race starts going multi-planetary and things like that so but but our belief is very strong that ethereum is that settlement layer and then on top of that once uh web3 starts expanding into uh you know into the mass adoption we believe that uh you know uh the the layer two solutions or secondary layer solutions on top of ethereum will will be the avenues where the end users will interact where the business activity will will happen but all of these different avenues will connect back into ethereum and that's the core thesis of polygon and it includes other uh you know ecosystem projects of of ethereum also uh but at polygon we want to you know truly uh be one of the most dominant player in the transaction layer uh similarly you know many a time people ask that okay you know what what solutions polygon is providing right and and we say that uh or and you know you might have heard that debate of like whether it's a layer two it's this and that first of all like most of the people try you know whenever they are talking about this they are talking about one or the other solution of of polygon right but as a whole what exactly is polygon is that if you see what we believe is that ethereum scaling is a spectrum we don't believe in a narrow definition of layer two uh you know we believe that all the different kinds of different flavors of solution add value to ethereum add scalability to ethereum and add network effects to ethereum so this is a spectrum that we that we uh you know uh we believe that you know ethereum uh scalability can happen and you can see across the spectrum on the left extreme you see a stand-alone change where the security by ethereum is very limited and on the right extreme you see fully secured layer two chains which have the both their data as well as uh you know dispute resolution everything going back to ethereum which is mentioned in this uh error with d a and d dispute resolution and data availability so polygon wants to be present across this uh you know solution spectrum on the left extreme you have polygon sdk which provides you capability to have these evm compatible sovereign chains then you have polygon pos uh which is like you know kind of in the middle it is actually uh you know a chain proof of stake chain built on top of ethereum and uh you know it is currently uh you know one of the most popular chains and many of you guys might know that you know uh polygon solutions have the dau even higher than the currently the ethereum main chain and many of the users are using and it's it's undoubtedly one you know not the highest used platform globally right now and then you know as i was talking about the spectrum on the right spectrum you have the zero knowledge solutions zero knowledge roll-ups and uh evm compatible roll-ups which are being built by you know various different uh solutions and then polygon avail as a data availability solution you know which eat 2.0 data shards also are going to be when it comes they can give birth to validium solution so with that uh you know with that base in mind that what exactly is polygon trying to achieve let's go through the journey of polygon and you know i would request jd to take us all through the journey of polygon uh and then move move forward from there jd yeah thanks so certainly started with the idea of idea to scale ethereum and later on 17 when is when i was exploring multiple projects and the same cryptocurrency happened the same time and then we realized that uh there is a need for the scaling solutions for the ethereum and at the same time we started uh instead of coding about the medic and founded betting um 2018 around 2018 we um we made it official sandeep and i made it official that we are going to kind of build a medic uh evm based scaling solutions uh at the time like there were a few scaling solutions but um they were so complex that people the developers not going to use that uh properly or if they use that there will be there won't be a tools after the production of the maintenance right so we thought evm based chain would be better suited here for the developers to use and move from each other to um the specific chain uh in five or five minutes or ten minutes you know simple uh easy easy to migrate um and then we we kind of floated our io in 2019 on finance uh it was very successful io so far on the finance as you can see um june uh after multiple iterations devnet and definitely we launched mainly in early june of uh 2020 uh and since then uh we have been grown so so well that top games on board at the end of 2020 in february in between uh late 2020 is the co-founder um polygon uh pneumatic polygon uh with with with idea uh with a vision to have a multiple kind of solutions instead of one monolithic kind of pos chain solution uh and you can see like you know after 2021 uh a major d5 protocols as well um came on on polygon um uh sushi swap uh and multiple others right um in may 2021 mark cuban uh invested in us and we reached 10 billion mark in in a market cape um 2021 uh june 231 sdk uh we launched studios studio has been successful uh in our journey and uh we invested like many nft projects gaming projects so far it helped them to launch them uh and it it has been so successful so far uh in august 2020 we announced hermes um network uh and they are building um evm based ck roll-ups uh we also launched one billion grant for one billion commitment from our side for the ck based technologies scaling solution solutions um and q3 ey came on board with nightfall uh everything enterprises in mind and drivers in mind uh and q4 2021 uh we just announced like a few days back we announced uh my name uh polygon mirren uh with start-based uh evm solutions ck solutions uh and one more thing we are going to do that uh but after some time wait for that yeah you do me hello thanks jd and thanks sandeep yeah so um let me briefly cover um what we call the polygon zk thesis uh here and this the the this these is basically the block that covers it in more depth uh is published several months ago and basically we keep uh publishing it with every zk related announcement and i guess we can share it also uh after the after the event or during but uh let's just briefly uh briefly cover it and one of the things that are part of this some of the things that are part of the ziki this is are relevant to what san diego and um jd said so i'll just try to run through these things uh as quickly as i can so as sandeep said uh rightly of course we believe ethereum is our best chance to build this uh global network of value and web free we are fully convinced in that right now and we are considering polygon an integral part of the of the ethereum ecosystem of course and this whole promise of this global permissionless internet of value is really amazing and really supposed to complement this internet of information that we have been creating for the past decades and creates a huge value-added basically for for for the humanity if you will and we are fascinated by that idea and we decided to pursue it and commit all of our resources to to help reach that amazing and grand uh vision uh that being said uh again ethereum is very equal swept free in our in our minds but we need to scale it first of all so um let us first cover where polygon was like polygon beginnings and polygon where it is now basically in the wider context of the of the industry i guess when when we started versus where we are now so when we started as i said there was this pressing need to scale the the ethereum minute was heavily contested ethereum user base started to kind of deteriorate in a sense that alternative chains started popping up with some normally uh trade-offs and offering maybe additional throughput and users started leaving and what not so we understood there is this really really pressing need to scale ethereum at the same time scaling scaling ethereum was kind of a niche topic which was very relevant in the ethereum community but in the wider context of the crypto industry it wasn't that visible or premium at least and there were a lot of opinions maybe even coming from vcs or influential people that it is not really fair favorable to focus on scaling ethereum and positioning yourself as a project on top of kind of ethereum instead of maybe competing with ethereum and it was kind of accepted opinion that that can bring bigger benefits in terms of i don't know value capture and things like that um on top of that it's really important to understand that it is very hard to build quote unquote perfect solutions for ethereum scaling because ethereum as a community really does not accept compromises when it comes to these core values or core engineering principles which are mainly decentralization and security first of all um on top of it we have evm as the standard that is kind of already established all these things or requirements combined together make it pretty challenging to build kind of perfect or ideal solution or solutions uh on top of that polygon when we started we have really uh modest resources as jd already said there was a small crowdfund crowdfunding campaign and even though the treasury was was managed really well um we still basically because market cap was very low we had really modest resources at our disposal and where we are now like 10 months later i guess uh or so since we announced polygon we are now by far the most adopted scaling platform in the industry and uh moreover as we are apart from ethereum we are very likely to the best of our knowledge the most adopted blockchain network or platform in the world apart from ethereum which is i guess we all agree a huge huge success in such a short period of time one additional thing that significantly changed since since then is that scaling now is kind of a mainstream and everyone's aware of it then fortunately everyone's aware of polygon now and we think with polygon we managed to set an important precedent in a sense that i think we kind of proven that we managed to prove that it's possible to build on ethereum with ethereum on top of ethereum and still capture value and still add value back to ethereum instead of competing kind of editorial so we think this is one very important thing that will maybe set a a a an example for future founders to think about when they're starting their projects um in as a consequence i guess of this high level of adoption that we have seen so we right now have more than three thousand applications the deployed on polygon we have more than one hundred million user wallets we have processed more than one billion transactions we have more than five or six billion even now and user funds completely organic without any incentives from our side this huge level of adoption has subsequently led to the growth of the price of the token we're not here to talk about price of course but i'm just saying that subsequently increased dramatically resources financial resources that we have at our disposal and that of course put us in a much better position for what we are about to do in the next chapter of polygon and now we can send if you can go to the next slide please so we can actually get to to um to the thing so how did we achieve how do we achieve this success and this growth in such a short period we had a clear understanding and we still here that we have two major focuses that are driving us driving everything we do at polygon one is shipping and the second one is innovation innovation because uh we are still in very early stage when it comes to building uh uh infrastructure for web3 and this internet of values so right now in ethereum we have millions of users but we really want to onboard one billion plus or even the whole world and which means we need to scale 1000 x from from this point without again as i said sacrificing these major principles major design or engineering principles of ethereum to do that a lot of innovation it still has to be done a lot of work on that front is ahead of us and we understood it and we are willing to um accept it as one of our focuses uh and the other one is shipping for us we are all i guess engineers by education and training and uh we are shipping oriented and for us unfortunately innovation without shipping is just an academic basically exercise so yes uh starting from there and realizing this pressing need to scale we realize that we for now at least have to be very focused on shipping in this first period uh we as jd mentioned we delivered polygon pos which turned out to be a huge success we intentionally uh offered it as a solution with that he has certain trade-offs no not a single solution is perfect of course but we just realized that the community really needs solution today evm compatible solution today without any delays that was a very good decision uh in hindsight and after all this growth and considering where we are now which i just covered we believe at polygon now we need to focus even more and commit we are in a great position to commit much more resources all sorts of resources financial human resources etc uh to innovation i think something happened with slice if we can just go back maybe to to where we were so yes uh we are focusing back on innovation without losing uh focus on shipping shipping is still one of our of course major major focuses but we just believe that we are now in a very good position and we are thankful for that and we want to commit significant uh resources basically to uh 2d innovation activity happening on polygon if you can just get back the slides that would be great i think something happened just a moment i thought just give me one sec there is this seems to be some issue with my um this one's yes okay sorry for this disruption guys okay i'll be back guys can you see my screen yes yes okay okay awesome uh me hello you can proceed uh just a moment i tried to open the presentation myself okay uh yes excellent okay sorry for this guys okay so speaking of innovation and our focus now on it they are as many of uh you will know there are basically two major families of innovative solutions built on top of ethereum which are commonly called layer two solutions one uh one group are fraud tool-based solutions which are the most prominent ones currently at least are optimistic roll-ups and plasma is also part of that same family of solutions the other one are are the second group are zk based solutions uh as jd said like we all started uh researching and experimenting with these solutions pretty early in 2017 i guess ever since plasma was introduced and that's basically how jd and i met in the first place um and throughout all this time we analyzed all these solutions and we came basically to a conclusion that zk-based solutions are probably the the i don't want to say the way to go but we definitely became biased towards this this family of solutions because of several major reasons we don't have i guess the capacity or the time to go into into all the details and compare these two families but in a nutshell basically zk uh based solutions offer significant uh advantages in terms of scaling first of all in terms of security first of all formal security provable security and in terms of user experience and these three things are as we i guess all can agree extremely extremely important or critically important when building scaling solutions that's why we decided that zk basically is a strategic resource for us moving forward and that we want to significantly focus and commit serious or significant funds from our treasury to to experiment with these exciting solutions so zkp's or zero zero knowledge proofs or zero knowledge uh cryptography is basically a i would say a fascinating field of cryptography it was introduced in mid 80s and it didn't really have a lot of applications until blockchains came by uh zero knowledge crypto cryptography sorry uses very interesting uh um cryptography primitives and methods to prove certain statements uh uh normally without revealing most of the parts of the statement and normally or very often in a very succinct way so it's possible to with very little computation potentially approve huge amount of computation or or large amount of statements or transactions if you will and it it became obvious along the way when uh blockchains were introduced and when the scaling uh uh pressing needs to scale was became obvious it became also kind of obvious that zk and and their special properties might be a really really interesting uh tool basically to to address these major challenges that's why we have seen in the past i guess two years we have seen a true cambrian explosion of innovation in the zk field a lot of new protocols a lot of new primitives a lot of new projects and we are very happy to see that then we are even more happy to be able now to seriously join this this uh um this evolution basically of z case as i said we decided zkp is our strategic resource for us and we decided to support this effort innovation in this field with one billion fund uh or one billion commitment from from our treasury um if we can just go to the next slide um okay and just i want to just briefly briefly cover uh what we have done so far before i hand it over to sandeep again in our ziki arsenal we already have so we announced the zika this is i believe three or four months ago something like that and in the meantime we are very proud and happy that we have already significant results from this commitment and uh so far we have three active projects three very exciting projects under the polygon umbrella uh first of first one is polygon hermes as jd mentioned it is basically evm compatible roll-up uh one interesting thing or specific or let's say two interesting things about hermes out of many uh are that hermes is the first decentralized roll-up so roll-ups are currently normally centralized sense they normally have centralized operator hermes team has built the first and only to the best of not our knowledge at this moment decentralized roll-up uh and they are now building uh zkevm which is basically the the reimplementation of the ethereum virtual machine in a zk friendly manner so to say and we are very excited of course about this effort polygon nightfall is the second project it is done in collaboration with people from evi it is basically a continuation of their work on nightfall the the library for uh privacy that introduces privacy in uh on ethereum mainly targeted towards enterprises with them we are now building this uh interesting hybrid roll-up basically which is a hybrid of uh optimistic and zika roll up which is privacy focus so this is the first application uh for of z case targeted towards privacy so that's the main reason i guess why we're excited about this and the third one that we have announced so far is polygon maiden polygon maintenance is a start-based effort which is basically a generic zk or start-based roll-up which is going to be ethereum compatible so uh um in a sense you can draw some parallels i guess with starkware effort between these projects the difference that this project is uh fully open source it's mainly community driven and there are some of course differences in the architecture which we find very interesting but uh we don't really have the time to cover uh those differences now uh what is very important for us like critically important that as you can see these they are already free projects here uh and some might ask okay how will these projects interact will be will they be competing with each other what is your idea there for us we we are extremely happy that we managed to establish a really really good atmosphere with this between these projects we are all aware that this is again a very early stage of innovation these projects are working very closely together they're constantly touched i mean we are all constantly in touch we are exchanging knowledge updating each other helping each other and we are moving towards to uh together towards this uh hopefully brighter future of of scaling uh that being said this arsenal is already very powerful i would say um but it might even be better or might even complete the puzzle if we just can add something more to it um so what can we possibly add i will leave it now to sandy but can we imagine that we have the fastest zk scaling technology in the world that in some way would probably uh complete or really add a lot of value to this this arsenal that we have now and um yeah with this i will just leave it to to something please yep i think you want to bring up you know your part of the presentation for the announcement at least the placeholder so yeah i mean guys while uh karthik is bringing that up uh you know so basically as uh mihailo mentioned about our polygon you know zika thesis that uh you know and why we we feel that the announcement that we are going to do today is is very very big is that there are there are you know uh you know a few z zk solutions out there but one of the biggest problems with zk solutions have been always about performance and the second thing is that because uh because it requires a large amount of resources for uh you know zk provers to you know create uh you know gk proofs uh it makes it essentially very hard to decentralize right so imagine like as mihaela was saying that you know if we had a very you know extremely fast probably world's fastest uh you know zk recursive proving technology and extremely efficient also which could run there where you could run zk proofs uh you know on on a simple machine like a laptop that would be uh you know the game changer for this industry where a large number of uh you know nodes can be run in a network which can validate the transactions and things like that which will actually truly truly make uh you know us us or truly enable us to create decentralized uh layer 2 network uh for for polygon and and you know like for that uh you know i'm extremely happy to announce on behalf of uh you know our entire team everybody is that uh you know polygon has uh polygon is very excited to welcome uh you know uh polygon excited to welcome polygon zero uh which is uh which will be led by uh you know a really brilliant team of uh you know cryptographers and engineers uh primarily brandon and daniel and they were previously doing it as me protocol uh which has now uh decided to join uh polygon and focus on zk scaling previously they were building uh like a zk uh you know enabled chain but now therefore they have decided to join polygon and build a decentralized uh zk roll up on top of polygon uh you know so with that i will request brandon and daniel to you know take this stage and uh you know take us through uh but yeah for polygon community like you know we are extremely happy uh that uh you know this essentially now puts polygon right in the lead uh in this entire zika scaling effort we truly believe that zk is going to be is the ultimate frontier which can bring in internet level scale to blockchains and uh you know this with with me team also joining already uh an amazing suit of uh you know solutions on polygon this will this puts polygon in the lead uh globally in terms of the zk solution providers so over to you uh brandon and daniel uh hey everyone um i'm brendan i'm the uh the one of the co-founders of mir uh you you may not have heard of us but hopefully we can give uh some detail on what we've been working on um and so i i just like to say that uh we're really really excited to be joining polygon um obviously polygon has had incredible success in attracting users and developers um but i think what's really impressed us is their vision for the future and their commitment to uh zk scaling um and so over the past few months we've had the privilege of working with um with hermes and with maidan and it's already been a really fruitful uh collaboration and we're really really excited to uh to be a polygon hero um i think on behalf of of me and daniel um we're also really really excited to be coming back to the ethereum community um ethereum is how we both got into crypto uh in the first place and um you know it's it's a privilege to work alongside so many great teams uh to to scale ethereum um so i'm going to give a little overview of what we've been working on and then i'm going to hand it over to to daniel and some of our team members uh to give a little bit a little bit more technical detail um but i'd like to open uh with this so in the history of computing there's this thing that happens which is when you increase computing speed by 10x you unlock radically new applications so this has been true across platforms so for the pc for gaming for mobile every time you you deliver processors that are 10x faster the sk the space of things that you can build and the possibilities um increases dramatically and so i guess from that perspective we view uh zkp's as sort of a new computing platform and we'd like to pose a question to the ethereum community um which is uh what can you do with zkp's that are 100x faster specifically recursive ekps that will allow us to build things like more decentralized more performant roll-ups new ways to provide privacy to users and so we're just going to talk a little bit about about how we've done this what we mean by recursive uh zero-knowledge proofs and uh sort of go from there so uh recursion is is really important um for uh for for scalable zk roll ups because um as we know uh the premise behind a zk roll up is that we can take a large number of transactions that would be too expensive to verify on the ethereum main chain and instead we can process them off chain and generate a succinct proof that shows that all of those transactions are valid we can provide sort of the necessary state updates or you know depending on the data availability scheme and that allows us to to scale uh transaction throughput while still maintaining um the properties that we love about ethereum that it's decentralized and that's secure um so the issue with zika rollups right now is that it's actually really expensive to uh to generate a proof showing that a large number of transactions are valid um this is especially true for zk roll ups that support general applications and uh even more so for for uh zika rollups that sort of support evm compatibility and so what recursion allows us to do is instead of taking a single proof that verifies uh say 10 000 transactions we can instead generate 10 000 proofs to each verify one transaction we can do all this in parallel and then we can recursively aggregate um and so we're able to generate the final proof that we post to ethereum uh more efficiently so uh so we view sort of this breakthrough and in recursive proof uh generation as as being really important for the future of of zk rollups on ethereum um so for some context on how we got here um recursive proofs have only been available in practice since about 2014. before that they existed only in academic papers [Music] so when we started mir in 2019 it took two minutes to generate a recursive proof and so we realized very early that uh this would never work like we if we wanted to provide uh high throughput and and something that could scale we needed to fundamentally improve proof generation time for for recursion um because even if we were sort of taking advantage of parallelism that only gets us so far this still represents way too much latency for for the performance characteristics that we'd like so uh in 2020s two things happened so the brilliant team at aztec developed the first implementation of recursion on ethereum uh based on uh planck and the kcg polynomial commitment scheme um so proof times were about 60 seconds on a desktop um and so this was a huge leap forward for for roll-ups at the same time we developed a plot uh plonky which is a combination of plonk and halo and it allowed us to achieve 15 second proving times but the problem was is that uh the problem was that uh this wasn't ethereum compatible so we couldn't use these faster provers uh on ethereum and so now i guess uh we uh we're really happy to announce um plonky ii which is a new proving system based on uh plonk fry and uh some wizardry from our own team um and so i think it's really important to to sort of give credit where credit's due um and we uh this advances is based uh a lot on um really brilliant work from from zach and ariel uh uh at aztec um and who developed plunk and then um the extremely talented scientists at starkware who developed fry um and so this represents a 100x speedup for recursive proofs on i3 so plonky 2 is fully transparent there's no trusted setup no toxic waste um it's natively ethereum compatible so it takes about a million gas to verify monkey two proof and this is a constant like we could have a recursive proof that verifies a million transactions it still only costs one million gas to verify in ethereum and so i think what we're really excited about is that this is a big step forward for the space because now we're measuring recursive proof generation time not in seconds but in milliseconds and so what we have achieved is 170 millisecond recursive proof generation on a macbook pro so on my macbook air it's a little bit slower it's about 300 milliseconds but this is still a huge step forward for the space it's the fastest implementation of recursive proofs ever and uh it works natively on ethereum and so actually i think that i can show this um we'll see on zoom if it slows it down a little bit but um yeah so about uh 355 milliseconds um right now so yeah we're proud of that and so all of this is sort of uh in service to developing polygon zero uh which is the most scalable zkevm uh powered by plunke2 um and so we believe that zk rollups are going to compete on speed and on cost and we believe that we're building the most scalable evm compatible zk layer 2. so the goal is to allow developers to compile their existing solidity code to run effectively unchanged on a zkvm and we think that this is really important for ethereum and for the crypto space in general because our goal is to provide scalability without compromising on the things that make ethereum so important and so special for the space so we we view this as as sort of a route to scaling throughput without compromising on decentralization uh or on security um so we're really excited uh to talk a little bit more about this in greater technical detail and so i'm going to hand it over to daniel okay thanks brendan so why are we so focused on performance here generating these proofs can be very expensive especially if we're proving something like an evm program which is a really conventional design it wasn't designed for snarks so it has features like arbitrary control flow random access and all kinds of binary arithmetic these aren't features that are natively supported by snark primitives we can simulate them but there's quite a bit of overhead so on the right here i took this benchmark from one of the old tiny ramp papers tinyram is this virtual machine that can be can be we can prove the execution of and the author has measured basically the simulated clock rate of this vm and as you can see it took about 33 seconds just to prove one cycle of execution um so if we compare this to a real hardware cpu which would typically run at around 3.3 gigahertz this is a performance gap of 11 orders of magnitude uh now to be fair this is an older paper and we do have some modern techniques that can help to narrow this gap but this is still a big challenge to say the least um especially with the evm because uh the this was a 16-bit machine whereas the evm is fundamentally a a 256-bit machine so when we started this project we wanted to collect some real data about what sort of competitions we're dealing with so we spun up an ethereum node and we recorded a bunch of blocks and we looked at basically what's going on in terms of that for example we found that the average transaction executes about 3 400 instructions in total which doesn't sound like that much but some of these constructions are relatively expensive so for example we have the average transaction executes 88 of these and instructions this is a bitwise ant involving 256 bits and on hardware this would be pretty trivial but in in snarks since it's not natively supported we have to either do it bit by bit or we can use lookup tables or other techniques but there's still a lot of overhead either way the other thing that concerned us a bit here is this sha-3 instruction the straw three is this ketchup hash and it involves hundreds of thousands of bit operations just to evaluate a single hash and the average transaction does 13 of them so this might sound pretty negative so far but it's okay we can still make this fast we just have to be incredibly focused on performance so with that in mind let's look at some of the proof schemes that we could consider using we basically considered three options the first is schemes based on kcg like planck and kcg is this polynomial commitment scheme based on pairings it has some nice properties it has very small opening proofs so a whole argument can be less than a kilobyte uh it's easy to verify on ethereum however the challenge here is that it's hard to do recursion with pairings there are a few different approaches here but none of the results look look really good for performance so next we considered these schemes based on halo halo is this idea that came out a couple of years ago and it provides this really clever way to do recursive proofs without pairings using elliptic curves that are not pairing friendly um and we actually implemented this as brenton mentioned in our library plunky and we were able to get decent prover speed and a decent recursion threshold this way but the problem as brendan mentioned is that halo takes linear time to verify so it just really isn't practical to verify on ethereum unless we combine this with other techniques and finally we have schemes based on fry like starks and these are particularly interesting because there isn't really one set of performance numbers for these proofs it really depends on the settings that we use them with so fry has this parameter called the blow up factor which is basically a measure of how much redundancy we add to a polynomial before we generate the commitment to it and if we want a really fast prover we can use a small blow up factor which means less redundancy so we have less data that we're committing to and proving will be faster however the caveat here is that a smaller blow factor i mean reduces the security of the fry protocol and we have to compensate for this by running more queries and this increases proof size so we have this dilemma where we can either choose a fast prover or small proofs but we can't really have both at the same time luckily recursion helps us here so with recursion we can take a larger proof and we can shrink it by wrapping it in a in a proof in a recursive proof with a larger block factor and that's exactly what we do in our zika rollup so we start with transaction proofs that we want to be as fast as possible and here we use a blow factor of two to to really maximize the prover speed and at first we have these really large proofs because of that but that doesn't really matter because we they just stay on my computer we don't send them to anybody instead we immediately shrink them by wrapping them in a recursive proof with a larger blog factor of eight this brings the size down to about 115 kilobytes which is more manageable but if we're going to submit a proof to ethereum which charges 16 gas per byte then we'd still like them to be smaller so before sending approved to ethereum we apply the same technique but more aggressively with a blob factor of 256 and this brings the size down to about 45 kilobytes which is great these proofs do take a bit longer to generate but it doesn't really matter because um we only have a small number of proofs that have to be sent to ethereum okay uh so the other thing we can do to really maximize the prover speed is instead of using a 256-bit field like most snarks do we can encode the witness in a 64-bit field um and this is much faster especially for field multiplication because typically quadratic algorithms are used there so we can make it about 30 40 times faster by using a smaller field uh there are a couple of complications here one is that the fry protocol assumes a larger field of at least a hundred something bits um so here we borrow this idea from heath stark which is we include the witness in the small field and then when we run the fire protocol we run it in an extension field of this 64-bit field uh and that way we can get the same security of having a larger field it's just not a prime field but that's fine fry doesn't require that uh there are also a couple of complications with the planck protocol where certain in certain places plonk assumes a larger field for security so we could apply the same technique and run those protocols in this extension field in some cases we do that but in other cases we use another workaround which is to keep using the small field but we repeat some of the checks in the point protocol in order to boost soundness and that's it for me now i'll head it to william okay thanks daniel so yeah i'll talk about the recursive circuit optimizations we use in in planky 2. so first a quick intro on proof recreation uh the basic idea is that in a proof system we have both approver and a verifier and for proof recursion what we do is we write the verifier inside the circuit and that allows us to verify a proof inside another proof okay this sounds easy but historically it's been a really hard problem to design efficient recursive proof systems and um recently there's been some research on on that um in particular on accommodation schemes with halo but those have many drawbacks as we saw in terms of performance okay and so our solution to this problem is to use planck with a fryer-based polynomial commitment uh but now if you try to write naively the verifier in a circuit you'll end up with a quite a large circuit with 2 to the 16 gates so around 60 000 gates which is okay but the prover is a bit slow so around 10 seconds and so we really focused on on bringing this number down so first a quick recap on planck so in planck you have a table of field elements and on each row of the table you have a parallel number constraint that evaluates to zero and each row can have different constraints and so if you go and look at the original plonk paper you'll see that they use a table of which three so only three columns and and most planck implementations use between 10 and 20 columns but we decided to to use a much wider table uh with a width of 135 okay and this may sound like a lot at first but the design of our system is sort of it's it's really okay for example we can commit to all the columns with just one more root with other printable commitments you would have to commit to each column individually which would be terrible for performance but we can just do it with one local route um but so why do we need such a wide table is uh to design complex gates okay so for example in the fry verifier there is this complex operation which is interpolation so we have eight pairs of points x0 y0 to x7 y7 and another point z and we have to interpolate a polynomial of degree 7 on these pairs and evaluate it at z okay so that's quite a complex operation that would take a lot of gates um if we did it with a classic planck but with our white table we can actually do it in one gate okay so how do we do so um we start by finding the interpolation polynomial f outside the circuit and then we evaluate it at z to get the value of v and then we put all these variables in a single row as you can see on the bottom right and then we have a bunch of constraints on this row uh the first set of constraints uh checks that f is indeed the interpolation polynomial of the pairs and then we add a constraint to check that f of z equals v okay another thing we can do with our white table is hashing in one row so the fry verifier uses a lot of hashes like currently they make up 75 of the circuit so it's crucial to us um to have uh hashing as efficient as possible inside the circuit and we decided to use poseidon which is an algebraic hash so a hash designed for arithmetic circuits and it's quite a popular hash and it's used by other teams in the space but what's special about 22 is that we can actually do one possible permutation in just one row okay so in in some sense 2 is as optimal as possible in terms of hashing and so this is like a the most common optimization we used in planky 2 is exactly this so we have a complex operation in the fry verifier and then we use our white table to write complex gates to do uh these operations one one other nice optimization we use is uh what we call miracle caps so the fry verifier does a lot of miracle proof verifications so on the left here you have a classic miracle proof verification so you start at the leaf and then you hash your node with its siblings um until you go up to the merkle root and you compare the hash you get with the merkle root and except if they are very cool but what you can do is you can save hash hashes by stopping at the second layer so on the right we stop at the second layer and we compare the hash with the second element of the second layer and so this layer we call it the merkle cap so that's a cap of height one and instead of committing to the root we commit to this merkle cap you can also have a miracle cap of it two or three and those save two or three hashes and so this sounds like a small optimization but since we do so many miracle verification in the circuit they add up to to a nice improvement and and what's cool is that uh the this cap height is a parameter in the code that we can tune to optimize either for proof speed or proof size and and we have quite a few of these parameters um daniel mentioned the fry rate okay and so we can tune these parameters um to optimize for speed or proof size so this makes plunking into a quite a versatile printing system and all these optimizations add up to a nice result that we're really proud of is that our recursive circuit is like really small at two to the twelve gates so that's around four thousand gates um and this is one of the main reasons why we can have such uh fast proving times um yeah and now i'll let jacob talk about low level optimizations alrighty um so let's talk about speed we've talked about um high level optimizations we talked about optimizations to the circuits now let's talk about how we implement the very lowest level things i'm talking like multiplication really really fast how do you make math fast to make math fast you need both mathematical insight and technical expertise an engineer can take a spec and write code that runs really really fast on whatever hardware you're using um an excellent mathematician can take whatever work needs to be done and minimize the number of some fundamental operations and like multiplication but you know finite field multiplication is fundamental mathematically not physically there is no logic gate for finite field multiplication so the mathematician only really sees a proxy um on the other hand for the engineer the spec is immutable there's only so far that software engineering tricks can take you and a spec can only run so fast to make math fast you need to flip the causation right the work that you you that you're doing needs to be determined by um the physical reality of the hardware that you are doing it on plunky 2 is built for performance right when we were designing the protocol we were already thinking about how fast it would be running on commodity hardware the earliest of decisions in planky 2 were already guided by performance let me give you an example when you're doing mathematics in these kinds of groups you're working with a prime field fp where p is some sort of a prime number what's a good choice for p that's a very fundamental question well firstly it needs to fit within 64 bits like daniel's already mentioned and the reason why is that computers modern computers are 64-bit machines and once you've crossed the 64-bit boundary everything suddenly gets so much more expensive but it also can't be much smaller than 64 bits for security reasons so you want it to be almost 64 bits but but not quite and a different choices of p will also have different algorithms for multiplication and addition with different performance characteristics so we want to choose p such that um multiplication and addition those fundamental operations are really fast so we went through a lot of candidates uh here's a github thread where we documented our research we considered many fields each one with their own algorithms for arithmetic operations we derived those algorithms we examined the compiled assembly code we counted cycles and micro ups and what we ended up settling on is this really neat prime number um that has a frankly beautiful mathematical structure that just happens to play really well with how modern cpus and isas work let me give you another example as williams already mentioned we spend a lot of time doing hashing and the hashing algorithm we use is called poseidon within poseidon we use an mds matrix for a particular step and the mds matrix is up to us to choose as long as it meets particular criteria and we spend a lot of time doing matrix vector multiplication with this matrix so so it's important to get the matrix right to make that particular matrix multiplication fast there's a few tricks that we use in order to choose a good value so our the choice for the mds matrix is firstly circulant what that means is that every row is just above it but shifted by one position it means that there's fewer numbers for us to have to remember and this we're spending a lot less time just loading constants from memory which makes our code faster secondly this matrix is composed of powers of two and multiplication by powers of two is way faster on a computer because you're just shifting the bits by by some amount as opposed to actually having to go through all the steps of multiplication um these powers are small which makes the finalization at the end like way cheaper and a lot of these elements are one which is like multiplication by one is free like you just don't do anything um we make extensive use of vector operations so most of the code your computer runs is scalar the computer kind of takes let's say addition you have two numbers and um there's an additional instruction that takes two numbers and returns the sum as one number but your computer also has what we call vector instructions so these do more with one instruction as long as you're doing the same thing for multiple values so you might have a vector of four numbers and you there's an instruction to add that to another vector for numbers um getting four sums as the result in one vector compilers are generally bad at generating these kinds of instructions so if you really want to make use of this that's a lot of work for the program in plunkett ii we make extensive use of vector instructions um this was a lot of work for us uh but it's really worth it so the vector instructions give us a two times improvement on poseido in addition to that there's a bunch of asms scattered in the code base so for example the top one made finite field multiplication 10 faster the bottom screenshots are from parts of our positive implementations on arm and on x86 look the result of all this work is lightning fast zero knowledge proofs right and you've seen the impressive results from brandon but we're only getting faster there's still no gpus involved for example um we didn't think that you that we wanted our users to need gpus to run uh plonky too and i i'm looking forward to how fast we can get in the future the reason that we were able to do this at polygon zero is that we are both excellent mathematicians and also hardcore computer scientists you know it's not often that you get to work on something truly groundbreaking and i feel extremely lucky to have had the chance to work with such a bloody fantastic team right um i really can't believe that i get to work with my co-workers because they're just amazing and i am really excited for what we will be able to achieve in the future and we'll have to take questions [Music] jacob and team thank you so much for that amazing uh presentation and just kind of walking through all these improvements uh we do kind of have uh a small caviar which is we are running a little bit behind so we'll have to cut the q a short but there's one question in the audience and that question was just overall kind of how do you think about now new application to sort of enable uh more things with just the polygon ecosystem and also the ethereum side just like with these new improvements with these new performance metrics how do you think about just the evm world and what would not be possible here um just yeah giving that taste really good so probably you know i should take this disclosure question and uh you know uh you just also wanted to you know put a closing note also that you know how this adds immense amount of value to the overall ethereum ecosystem and polygon ecosystem uh you know is that uh this this prover system like uh you know many who understands the zika technology they know that this is the most efficient and the fastest uh zk approver system and this will enable us to create a very high uh performance highly decentralized uh zika zika roll up uh or evm compatible zika rollup which will you can already imagine that if these uh you know this will provide full security by ethereum and then you know imagine uh you have a zika roll-up where it provides ethereum you know full security by ethereum and the apps can use that roll up at a much uh you know low cost transactions by using validium kind of systems or you know later on with e 2.0 data shards it it has a potential to provide a massive scalability to uh you know to the dap ecosystem and polygon already has a huge dab ecosystem uh which which is you know we we keep getting these demands that you know we need to have more uh you know ethereum secured uh you know and decentralized layer two and i think this will this this takes polygon community very close to that that dream and then what what uh brandon and team and daniel team are building is you know would also provide like this is like a bit technical uh getting into it but you know might also provide an a virtual machine which has more op codes uh to use to the developers uh you know relevant to the question uh you know that uh you can like the apps on this uh you know layer two can actually run more uh opcodes like that means more functionality more features uh on this one while still be fully approvable uh you know on on ethereum so this is really really massive for polygon and you know with that closing note i you know want to congratulate the entire polygon community that uh you know uh this this uh establishes polygon at uh one of the top most player in the zika space and and you know let's let's build a highly adopted uh ethereum layer too and bring the you know mass adoption to ethereum thanks a lot karthik over to you thanks everybody and uh congrats again to the polygons of your team i think uh with that we are ready to move on to our next talk and now for this one i'd like to invite paul and to tanya for talking to talk about polygon nightfall so without further ado let's welcome paul and team hello hi everybody paul brody here uh so i'm the global blockchain leader at ey and wow so just i was watching the last presentation that is a very tough act to follow but we are gonna do our best i'm only going to take like two minutes before i introduce my colleague tanya who is is incredible but i want to talk just a little bit about the history of nightfall and how it became polygon nightfall at ey there's a couple of a couple of things that we've spent a lot of time thinking about one is what is it that enterprises need right and there are a lot of brilliant people like sandeep and others who have been thinking very heavily about scalability right but one of the big challenges for enterprises is we can't enterprises can't just use a mixer right because enterprise tokens and assets are often they're unique they're often attached to business contracts that represent a kind of complex business relationships and so we have spent a great deal of our time over the last five years really pretty much non-stop over the last five years from a research perspective thinking about how to enable privacy at scale now the first version of nightfall i think you know back when gas back when ether was a hundred dollars a piece we spent three thousand dollars worth of ether just to build our prototype zkp based supply chain and move some tokens around it and that was an amazing piece of work and it proved the case out for us but we kept iterating and where we ended up earlier this year was with nightfall 3 which is a zk optimistic roll-up technology and uh we built that out and in cooperation with polygon we're really turning this into a layer two system where we can everyone can access private transfers and payments at scale with very efficient gas costs directly on the ethereum mainnet so that is kind of critical to our goal and when we started talking to the different layer 2 providers and there's a lot of them out there and they're they're all populated by incredibly brilliant teams but the relationship that we struck up with polygon with sandeep and the team was incredible because at heart they understood a couple of really critical things that we shared with them a huge commitment to public blockchains an unshakable commitment to the ethereum ecosystem right eway ey we are without a doubt the biggest sort of large enterprise investor in the ethereum ecosystem we pour our energy into this ecosystem and then thirdly we're both very committed to zkp technology both for scalability and for privacy and so that match the relationship that we built over that has been the foundation of the work that we've done we are so excited about kind of where this is going it's going to be i believe we are all underinvested in privacy that enterprises need it for sure it's an absolute requirement but i think far too many consumers and individuals are underselling their own value by not investing more in their own privacy so i very much look forward to that and i am incredibly delighted to hand over to my brilliant and talented colleague chitanya conda to actually explain what uh polygon nightfall does and give us a quick demo chatanya over to you don't forget thank you paul um yes i'm titania and i work with eui's blockchain r d team today i'm going to talk show you a live demonstration of what polygamy nightfall does um and hopefully a very wise man once told that live demonstrations especially of tech nature are a courier seaside and one that i repeat time and again because i always end up with a short end of the straw i hope today is not one of those days for me let me share my screen as i walk you through okay now i hope you all can see a powerpoint presentation so i'll give you a bit of context about what polygamy nightfall does before i give you the nightfall demonstration itself what is um what is polygon nightfall um you all must be pretty much aware the very first iteration of nightfall was built as a privacy protocol or it still is a privacy protocol what it enables us to do is to do private transfers of erc20s 721s or 1155s between two parties such that the contents of that uh transfer is private so nobody can see the value or the token id at the same time in the not the recipient is also anonymous um so it's a it's essentially a privacy protocol now just to give you a high level summary of how it works because uh this is not the point of our talk here uh you have three kinds of transactions you have deposits transfers and redress in with a deposit what you do is you take a token out of l1 and you put it into lsu when i say alto here it's really a shield contract that sits on l1 and you take any tokens of your rc20 of a certain value when you submit this proof caller deposit proof which then provides the equivalent commitment that holds the value you've just deposited a commitment is is just one way to obfuscate the contents of a token in terms of a deposit that's really not private because when you move tokens from layer one to layer two or back to layer one that's not where we get privacy we get privacy when we're within layer two itself so now moving on to the transfers where we actually get a private transverse itself what we're going to do is in order to spend a token a user will first have to create something called a nullifier a nullifier essentially spends the commitment they own and there's no way to connect a nullifier to the commitment uh it belongs to so all someone who's watching the blockchain can see is that something has been spent but they can't see what has been spent and how much is within it at the same time when you're transferring something to some other person what you're going to do is create another commitment of equivalent value that you've just spent um and obviously because it's con obfuscated inside a commitment the contents of the commitment itself are not visible so the value is hidden once again and also the other piece of data that would be as part of this commitment is the public keys that the new recipient owns so this new commitment now belongs to somebody else uh anybody who's watching the blockchain can't really see who that recipient is or who received this new commitment and now withdrawals are pretty much the opposite of deposits pretty similar to transfer you're going to spend the commitment you can't uh you when you spend when you create a nullifier for a commitment there's no way to associate what commitment which is nullified so no one can see what commitment was spent and you have an equivalent value being released from layer two to layer one um the layer one part again is visible for everyone to see so you would really want to do transfers with privacy inside layer 2 that's that's the whole point of how you'd like you'd want to use polygon knight for now um this bit of work that i've just described about has existed for some time now it was first released back in 2018 so what's different between the russian then and now is that it does optimistic roll-ups um the very beginning the cost of the transactions that we've done were quite high they ranged about in the hundred dollar nature uh back in the 2018 the reason why it's quite high is there's generally a lot of uh data that's been stored on the smart contract in certain in terms of um what's held in a merkle tree all the commitments and nullifiers and you also have you're just sending one transaction at a time you're not really batching up now both of these changes have already been modified where the smart contracts are optimized such that we really don't store any data that's not really required to be stored on a smart contract and two we also batch but what optimistic roll up does it takes it to a whole different level where you're really not doing a computation itself on china anymore you're you're really providing with the end state of a computation should be in in case that is wrong you're expecting a challenger to challenge that the end state that you just provided was wrong by doing this you're saving all of the gas costs of computation itself and at the same time you're really being able you're able to scale a lot more in terms of having higher transactions throughput because you can fit more data into a block now so that's how polygon knight 4 uses optimistic rollups polygon knight nightfall uses zero knowledge proofs for privacy so that's the zk part and it uses optimism for uh scalability both in terms of higher throughput but most importantly lower transaction fees now just to give you where this sits in the layer or in in the current ecosystem of layer solutions um later solutions exist in the form of both off-chain solutions and on-chain solutions with on-chain solutions there's usually two approaches one tends to take lately you either use zero knowledge proofs snarks or stocks of some kind or you use optimistic roll-ups using both of these um you can either get the benefit of privacy with some very good scalability or you can directly write it as just purely a scalability solution where polygon nightfall is is it's an optimistic solution that uses scalability but also provides privacy you would know of other optimistic solutions such as optimism and arbitrary today uh which are built to be scalability solutions um from scratch and so moving on now just to give you a comparison of private transactions if you were to do an optimistic version of a private transaction as opposed to a z case version of a private transaction what i mean is using optimism for scalability versus using zika for privacy zika for scalability there's pros and cons to both of these a very good pro for an optimistic optimistic solution is that the gas costs are way way lesser so the gas cost of polygon nightfall is about 9 000 gas per transaction um that comes down to about two dollars at 2.5 dollars today um based on various um gas prices of course that number is calculated based off of 75 um as opposed to zika solution zika zika solutions which tends to be more expensive in terms of gas now on the other hand with aztec like solutions which is zika zika solutions they have finality much faster than optimistic solutions they have finality pretty similar to what a layer one would have optimistic solutions tend to have a one week wait periods predominantly because you would want to give enough time for challengers to challenge bad blocks if if there exists any one way that polygon nightfall circumvents this is it uses the concept of instant withdrawals a feature that is provided by using liquidity providers for fungible tokens such as the rc20s and so that using liquidity providers with instant withdrawals which is a built-in mechanism you wouldn't have to wait one week it could just be the layer one finality as well um the other big differences between the two approaches is that polygon nightfall being an optimistic solution uses something like a fraud proof fraud proofs tend to be ones that are only submitted in case and invalid transactions upon the blockchain zika solutions on the other hand work on valid proofs which means you can't submit an invalid transaction to the blockchain to begin with you've got to have a valid transaction that will verify the proof um generally or with zika solutions as well you have computational resources that both of these use and with a polygon knife flex solution it's generally it's relatively lighter in terms of the kind of uh xenon zero knowledge proofs that you would do as opposed to an entire zkzk solution that is on a very high level what polygon nightfall does but before i move on to showing you a demonstration i just want to give you a quick overview of what are the various players involved and what do they do in inside a polyline knife system so here you can see three players you see a transactor a block proposer and a challenger transactors are users who generally create transactions such as deposits transfers and withdraws and then they submit them either directly to the blockchain or they can also submit these transactions to the block proposes um the transactions that you submit to the block proposes usually are transfer and withdraw transactions not deposit transactions those will have to be submitted directly and always to the blockchain um block proposes then take all of these transactions that they receive from various users and they put them into different l2 blocks and they submit the zl2 blocks into the layer one which is ethereum in this case um each layer one block an ethereum can hold more than one layer to block depending upon how big these layer two blocks are um and that would exist on so the state of l2 blocks uh the final state they hold will exist in layer one inside uh layer one blocks now you have a third player called challengers who are continuously monitoring the system what they do is that they will take these blocks that are put up these l2 blocks that are put inside the main net they will check the validity of both the blocks as well as the transactions within the blocks and if everything's fine with those blocks they do nothing but in case they do find a wrong transaction or a block that was uh created wrong they will then create something called a challenge this challenge will basically prove how the block that is on the main net is actually wrong uh when i say maintenance sorry ethereum is actually wrong and if that proof verifies uh then challengers will succeed and two things will happen one the block proposer who just proposed the wrong block will be slashed and two uh the block proposed the block proposal would have submitted something called a stake with every l2 block that they create and that stake would be released to challenges as payment for the work they've done so that's the incentive mechanism that the challenges would have and block proposes uh their incentive mechanism is that transactors will pay them fee with the transactions they submit to to to be put up into a block and so those are the different participants and finally i will not go into further details about off-chain transactions how block proposals are registered how challenges of challenges are submitted uh just to give a bit of a road map before i move on to the demonstration currently uh this protocol that i was just discussing about uh has been deployed into the test net uh and we're testing it rigorously to catch any obvious or not obvious and tick all sorts of bugs that we could think of once that's done uh the next step would be is to have a proper security audit of the protocol and then it would be there would be a restricted value maintenance deployment um when i say restricted value it would be the amount of transfers would be pretty small originally and there might probably also be a bounty program front friends and tenancy after the main deployment and finally they will be after the main deployment has been successfully run for quite some time of course uh the restriction will just be lifted and for there to be no restriction in terms of the transfer at all and those are that's everything in terms of context now i will move on to the fun part for you but the scary part to do a tech demo of me this this here which you see is a polygon nightfall wallet and it's very preliminary looking just because we're much more focused on having this test of itself rather than having to having it part of uh polygon's website already but you can imagine it to be as sophisticated and as beautiful as polyvin has their wallet today and it will be part of the website too um so this polygon nightfall wallet is run by any user who wants to use this layer to solution to do transfers of private nature um all of the keys both that pertain to the transaction the keys that would be used to send transactions to the ethereum blockchain or the keys that correspond to the layer 2 itself which is the secret keys that will give you ownership of commitments all of those are stored within user's browser none of this will be inside a sub or anywhere so th this will have proper privacy for for a user and everything in terms of uh privacy sits at the users and in their browser now another thing is you notice that here we have erc20s any rc721s and obviously deposit transfers and withdrawals for all of these um something else to note is eos 11th but twice is also possible it's just not part of this demonstration right now and uh yes that's pretty much everything uh before i go on to give you a demo um so firstly i will start off by doing a deposit of erc20 and to myself which means when i when i create this transaction and i sign this transaction what's happening is that the very first transaction was to approve the shield contract it takes some erc20 balance uh from my erc20 contract and send it to itself the second transaction is to actually send it to the blockchain so what you see here is i have an element balance of a million i had and now i moved ten of that to layer two but it says pending deposit so that's interesting right why is it not part of l2 already we've got to remember that it's it's a roll-up solution where lots of transactions are required to be put into a block at the moment all i have is just one transaction before i go any further i just want you to show what other players exist here so the one who is running the browser this this wallet here i'm the user i'm one of the participants in this in this network the other users are sitting here so you have our block proposer and you can see that a block proposal was started and they've registered themselves onto the layer 1 contract and they're listening for any sort of incoming events and then you would have a second player called the liquidity provider who provides the services if an instant withdrawals required and then you have a challenger who is continuously monitoring the system to see if there's any bad block proposed um other than that uh this is just the transactions being uh run up and a bunch of vlogging that's not very interesting um so today right as we speak there's four players all up and running the system so one one transaction was created it's still pending so now let's create a second transaction this time let's do an erc721 transaction um i'm picking it token id1 and submitting this from [Music] i'm approving this first in the erc721 contract and then i'm actually submitting the transaction so what happens now is that this ersc 721 token has to move from l1 um to l2 since everything is pretty quick what you have not noticed is that initially it was a pending deposit but as soon as a block was created what what that means is that it is now in l2 so the l2 balances have been updated so we've created one deposit of usc20 that was 10 and one one other deposit of year 721 so that's where you see these balances in lt the current block size is just you for the sake of demonstration i did not want to create 20 transactions as you were all looking at it quite boringly but obviously this block says this can be much bigger than that and very quickly let's see what our other players are doing so we have a blog proposer blocker brothers just said oh i created a good block and this is the hash of that block that i've just created and this is the current balances for the user myself so both of us are doing our jobs quite well and we're both pretty happy now what i want to do is i want to do a transfer i want to do a transfer to often erc721 let's say uh to somebody else the default address is to myself but that's not fun is it so i'm going to do a transfer to somebody else and let's pick a token there's just one token in but in layer two for the moment in the exit 721 i picked that and we'll submit this transaction um i'm signing this transaction to be sent to the blockchain and pretty soon you will see that uh this l2 transaction that i have goes into pending upflow once again it's spending because it's not been proposed as an l2 block onto l1 yet um we need a second transaction to create some blocks so i'm just going to create um another transaction in this case let's say a deposit of 10. um of erc20 and i'm going to sign that as well okay now you will see that oh well let's update again now you will see two things have happened um a block must have been created yes indeed it was a git block was created and so i now have a balance of 20 because i i had 10 originally and i just added 10 more and i'm left by one erc 721 token so just to quickly give you an overview there is a second account here obviously that i used to send it to and that second account has a balance of vrt 721.1 just to give you a quick understanding of the second account for the moment i'm just running the second account in the browser myself the keys the level layer two keys that we're using are very different here but layer one they're both the same people because i'm the same person so my layer one balance according to that activator my dress is still the same but under layer two i have two different accounts and these are the balances of those two different accounts but you can extend this logic to anybody out out there in the world this it doesn't necessarily have to be me it would it would run the same way now just going back to my original account and i'm just going to quickly log out and log in again okay now let me retrieve my balances now i'm going to do something else here which is to do a withdrawal of uh of an erc20 i will click on withdraw and i will submit 10. but you've got to remember if i just click submit right now there's the problem of finality being one week and i really don't want to wait one week i just want my transaction straight away because i need to use it for something badly so i have the option here called instant withdrawal so i'm going to click on instant withdraw and i will click on submit and then i will confirm that transaction and so now you see i have a pending outflow of 10 which means i should pretty soon withdraw 10 out of layer 2 and that should end up in my layer one balance at the same time i need a second transaction for this to be blocked so i'm just going to create an erc721 transaction uh of token id2 i will sign that too and then i will let's look for the balance in a minute okay okay so you'd notice that uh well another step actually and you notice two things here one i've just given myself an rc721 token again into l2 so there's that but there's also an update to my l1 balance um i moved 10 out of my pending outflow into elven balance which meant i was able to instantly withdraw um the the the ten dollars instead of waiting for a finality of one week now let's just see what the liquid liquidity provider has saw so the liquidity provider receives an instant withdrawal request of some for this transaction hash and he provided or she provided me with with the money straight away uh the the mechanism behind this is this um when i create an instant withdrawal what i'm doing is submitting a request that i'd like to instantly withdraw this token and i'm this is the fee i'm willing to pay uh liquidated providers are listening to these bans that those are the events that they're listening to and as soon as they see an event what they're going to then do is to basically create um accept this this request i just made and transfer the subsequent amount for me while uh taking on the wait time for the wait time i would have gone through over a week period onto themselves so liquidity providers like challengers are continuously monitoring the network so they would know if they give me the money straight away today is that safe or not they would know that because they're continuously checking all the blocks and all the transactions and all the blocks by doing so they know that eventually after a week this transaction will be finalized for sure because there's no bad block before this this transaction and there's no way for this transaction to be removed from the network considering that they will do this verification themselves provide me the liquidity that's required straight away and they will receive the money that i would have received a week later so that's what's happening with the liquidity provider so just one final interesting case i'd like to show you i'm going to do just two deposits for this of erc20 okay that's pending i need a second uh in a minute or two you will see there's a pending deposit of 20 you will see that i now have an l to balance of 20 but let's just give it a few minutes and we'll see something pretty interesting okay now you saw i had a balance of 20 or 30 earlier but now i only have 10 and 20 were moved back to pending deposit so what just happened with something naughty let's look at it you see there's a block proposal here and clearly so far block proposes has submitted three good blocks but the last block that he just created with my two deposit he purposefully created a bad block with of type incorrect root and he submitted onto the blockchain so what happened was as soon as uh this block was submitted because that was part of layered layer uh that was that was a block that's part of le ethereum now in layer one my balance got updated saying well your balance is now 30 but clearly somebody was wanting the network which was a challenger a challenger was monitoring monitoring my network uh the network of ethereum and then they realized oh this is actually a bad block let's take a look at that they noticed that i just received a block here q new block proposed event i've just received a blog proposed i'm going to verify the validity of the block request that's what they would have done so as soon as they verified there was a warning they said block checker block invalid with so-and-so issue and as soon as that was that error was was picked up it then tries to create a commitment for the challenge which is challenging incorrect route and eventually it would have submitted the challenge to layer one our challenge commitment has been mined and sending reveal which is yes we will use to reveal the challenge itself and the challenge would have been suggest provided to layer one layer one will then do the computational steps of the challenge itself and on successful challenge it will do a rollback of the entire state until the first bad block was until where the bad block was when that happens a rollback of all of the transactions that is done and the rollback was received by everyone including the challenger so that's what happened our state was rolled back where we still have 20 waiting to be deposited and i don't have that balance yet now what happened was i only have one proposal as you can see and as soon as this bad block was submitted this proposal was basically slashed so they're no more the proposer now on a good day if i were to have another blog post right now these transactions would have ended up back in the mempool for the second block proposal to have picked up and they would have submitted for me so these transactions would have eventually made it back to l2 without me doing anything and that's just the way the entire network works and the different players are interacting with to give a proper valid transaction state to the system uh that's pretty much everything well in terms of demonstration um i'm happy to take any questions now tony i gotta say you keep saying that like live demos are career suicide but the reason you keep getting picked for them is that you always execute them and then look like a rock star so thank you thank you so much i'm sure carthage didn't tell us that because of the time constraints there are no questions but really uh truly brilliant i'm so proud of the work that you have done r d team that has done and the work that we are doing in cooperation with polygon to make this real apology that's that's mostly correct uh there is one question that i do want to ask a quick one that came in but uh before i asked that question i want to just say i've just been looking at hundreds of comments and everybody's just been extremely happy with how easy you made that to uh to understand and follow and of course the demo works so i just want to make sure that i point that out it was a super um conveniently well made easy to follow presentation that covered really technical topics so uh so thanks for that the question is is a simple clarification that came in from one of the listeners uh and the question is would challengers be able to reverse transactions uh made a mistake uh such as kind of getting an invalid wallet address or any other keys and kind of just how does the uh if if it's possible to reverse a transaction upon um finality i guess uh so yes there's a one week finality period um you know you really have to do any sort of challenge within that period um so there's various ways a transaction or a block could be wrong either the transaction that was submitted to begin with was invalid the block browser still went ahead and submitted it as part of a blog or the block itself that was created with these various transactions was malformed um and that was submitted onto the network what then happens is that block now is part of layer one let's say other blocks were continuously proposed on top of these blocks as well um at this point a challenger was looking monitoring the system they would pick up this very first block proposed that was wrong block they will then submit a challenge that challenges that very block and when that happens all of the blocks along with this block and the ones that come after will be rolled back so that entire state is basically rolled back and when that rollback happens that rollback is not just happening in layer layer one of the blockchain everybody who's keeping track of their own state in terms of commitments or transfers or transactions they would also have to update their own state so block proposes challengers and users they will all update their own state according to the rollback that happened i i'm not sure if that answered your question to carthage i hope so too uh we'll uh we'll see if there's a follow-up question that comes in and if if there's any uh still any confusion or follow-up questions i'll just ask them directly over email and connect you uh both so thank you so much with that um with that amazing presentation and we are ready for our next talk thank you so next up i want to invite bobbin to talk about polygon mining so um robin whenever you're ready feel free to uh get started welcome thank you kartik and uh hello everyone very excited to be presenting amongst such brilliant people and talk about polygon maiden let me just share my screen all right i hope everybody can see the presentation everything is good all right so before i explain what polygon midan is let me give you guys a bit of history so i've actually started working with zero knowledge tech and start specifically in spring of 2019 and the first thing i did was build this uh john stark library which was a stark grouper for uh you know aiming to be be able to generate proofs for kind of any kind of computations and once i build it i realize that it's extremely difficult for regular developers to just pick up and start using something like this and i can spend a few months trying to figure out how to make it easier and try to develop it first kind of that means specific languages that would make it very simple to create start proofs but even that turned out to be quite complicated so the next thing i try to do is i start to think about start-based virtual machines and i actually made a this youth research post early in february 2020 describing kind of the my thoughts on it and at the time i actually didn't think it was going to be very powerful machine i didn't think it was going to be turing complete but as i started working on it i realized that uh you know there are a lot of things that we can unlock and there is actually a way to build a practical start-based virtual machine and a few months later i built the first version of this staff vm and you know improved it over the following months uh and this this fbm is actually one of the foundations for uh polygon maidan uh the other foundation for polygon mind and is winterfell which you know after i work from this fdm i joined novi and as part of being there built this winterfell star prover it's uh in a way very similar to jen stark but it is much much more performant modular and you know up-to-date implementation of star protocol and now polygon midan vm is a kind of a combination of dystaffium and winterfell and in a way it is a culmination of what i've been working on for the last two years so to say so i'm very excited to kind of talk about it so uh what exactly is polygon midan polygon midan is general purpose stark based uh ck roll up and uh i know a lot of people here are really familiar with all of these terms but for those who are not let's uh go through these terms one by one and start with ck roll up so what is the ck roll up uh in a zq roll up we have uh users operators and then we have an on chain contract which is you know on the studio main chain and the idea is that that users send transactions to operators and you know there are many users and operators aggregate those transactions into batches and then they submit kind of the state change or whatever has changed in the internal state of the ledger that the operator is uh managing together with the zk proved that they verified all of those transactions uh to the on-chain contract which verifies that so why do we care about this type of uh setup and one of the answers is that it gives us a lower fees because you know we can compress a lot of things using zk proofs and there are other reasons why you know for example we don't have to include every single transaction if some transactions happen between the times that uh we verify zck proofs and those transactions can actually become you know completely absent from the kind of the state change that gets pushed into the main so we can get up to actually over 100x reduction in fees as compared to ethereum and the other thing that is that is important is we can get this without sacrificing security so uh in the ideal scenario the the zeke roll-up is just as secure as ethereum itself because it inherits security from ethereum there is no way for operators to submit invalid transactions or you know submit a state transition and sdk proof for something that is not really valid so if they didn't have a transaction to verify they couldn't make it up and um the other exciting part is that the roll-ups can be very high throughput and the reason for this is that um unlike um let's say in the decentralized laser of kind of the way ethereum is the operators can be fairly heavy and they can have very you know sophisticated hardware to process a lot of transactions and um you know we only need to really to have one operator that is honest uh to for the roll up to work and even if the single operator for whatever reason decides not to be honest um we still the users can still go directly to the on-chain contract to reclaim their funds so we of course want to have the roll-up to be decentralized but this kind of setup allows us to have have much more flexibility and experiment with different consensus models execution models and you know structures of the ledger and achieve this high throughput lower fees while still maintaining the same level of security and that's flexibility portion is something that is really really exciting to me personally uh okay so this is a very high level overview i'm sure most most of people here really knew what the secret roll up is but just to kind of set context okay so let's talk about the next term start based so uh before we talk about what the stark based means we need to talk about proofs of computational integrity and um you know we we talked about this as zk tag but actually the thing that we really care about is this proving that the computation was done correctly zk in many ways is incidental to a rollup although you know it is a useful property if we want to achieve privacy as well so in terms of proofs of computational integrity usually there are two parties there is the approver and the verifier and approver was to say that they ran some kind of computation and they have gotten some kind of result in the context of a roll-up the computation is verifying all the transactions that they want to put into a batch and they want the proven to accept that they've computed uh the result correctly without prover without the verifier having to re-run those computations themselves and obviously we want to be in a setting where we don't want to trust the prover on their word so the verifier cannot just accept uh the progress result but what we do is the approver can send this zero knowledge proof to the verifier and the verifier by examining the proof can be convinced that the approver has run the computation and the gut can receive the claim result uh without having to do the computation themselves and uh it's very important for uh for in our context that the proofs are small and fast today if it's hard to verify so overall in terms of proof of computational integrity there are two large families uh starks and snarks and you know stark stands for scalable transparent arguments of knowledge and they were developed by brilliant people at starkware and there are a lot of different stocks developed by many different teams and they stand for succinct non-interactive arguments of knowledge but rather than saying like there are two different families there's actually a lot of overlap between them so and non-interactive stark is actually also a snark and the scalable transparent snark is also a stark so in our context we're actually working with non-interactive stars which means also that our system is snark as well all right uh there are a few advantages uh and uh reasons why i think starks are a very good choice for um a roll-up type system and i'm gonna go through them uh in the next slide so start advantages as i mentioned um or is it comes from the name starks are transparent and scalable so there is means there is no trusted setup we don't need to worry about toxic waste or you know trusted setup being compromised and you know potentially somebody one of the operators or proverbs uh generating fake proofs so that's a very good fundamental property to have the other advantage of starts is that they use very lean cryptography and what i mean by this is that they rely only on collision resistant hash functions and that makes them post quantum secure and in many ways it's a future proof technology they're very flexible and here i'm getting a little bit uh too technical but uh you know we had the talk from uh polygon zero where they talked about selection of a field and you know the same applies to starts as well you can choose a specific field which is very performant and allows you to generate proofs faster you can also dynamically kind of trade off between prover time and proof size so you can increase proper time by reducing proof size and you know uh play with different security levels without having to modify anything uh kind of in the proving system uh you can do it uh you know you don't need to choose a different elliptic curve you don't need to uh redo your circuits you can kind of dynamically trade off these properties and tailor it to a specific use case and then in terms of performance this is i think one of the big advantages of starks as well first the verification is very light in terms of like real world performance most of the proofs that for any practical computation are just single digit milliseconds to verify and the description of the computation is distinct as well because there is no process pre-processing that needs to happen and then on a verifier on the approver side the proof generation is also very fast so a few benchmarks of on the current implementation of might and vm is that right now on a single cpu port you can verify um you know you can execute about 10k instructions per second which is you know 10 hertz uh if you think back to uh daniel's uh talk a little bit earlier where he mentioned that it took like multiple seconds for a single uh instruct instruction to be executed on the tiny rndm this is already many orders of magnetic improvement uh but it gets better because we can parallelize very easily and even with the current implementation we can get up to you know 400 hertz uh if we spread the generation across multiple cpus and we believe that with some additional work we can improve this significantly uh in the future um and then um you know if we go to gpus and fpgas in the future where there is hardware acceleration this is going to be in the megahertz range and probably very very fast for all practical purposes um starks are not perfect there are some disadvantages to them and the biggest one is the proof size um usually proofs are in dozens of kilobytes so uh just a few benchmarks uh for mid and vm uh in its current form uh if you if you execute 1000 cycles the proof size is about 35 kilobytes and for a million cycles it's around 80 kilobytes um and you know if we need to go more it grows but it grows logarithmically so it's uh it's not gonna get too big it's probably not gonna exceed you know 150 or 200 kilobytes at the at most for uh pretty much any practical computation um this kind of like the proof size is lead to relatively large gas cost when we try to verify proofs on the helium so between three and five million gas for verifying the start proof probably closer to five million uh on ethereum um but um once eip4488 comes through we should see that uh gas costs dropped to hopefully under a million but uh this will need to be verified but uh again because the the actual proof is very fast to verify it's just the proofs are large once the whole data cost gets reduced uh the cost of verifying start proof will get reduced significantly as well uh the other thing is recursion and it is possible to do recursive starts they haven't been demonstrated yet but especially in a context vm this becomes like having a virtual machine this becomes a real possibility to build recursive stark's proof as well and this is something that we will investigate uh on our end too now let's talk about general purpose so if we think about zk roll up specifically there are two ways to think about them so there is a specialized type of zika roll-up which handles a specific use case like payments and you know exchanges nfts and so forth and then um there are general purpose zika roll-ups which allow you to write arbitrary smart contracts and this is um the more exciting type of a roll-up uh which you know developers can build their own logics and you know smart contracts that we can think about and like have the full power of uh you know ethereum type environment within the roll-up and having building a general purpose dk roll-up requires to have a or at least you need to have a zero-knowledge virtual machine so um in a context of uh uh polygon might and the uh well let me talk about first about what the zero knowledge virtual machine is so zero large virtual machines you can think about as a virtual machine that takes some initial state and sets a set of programs executes them and you know gives you some final state but it also uh gives you a proof that it has executed everything correctly and this is the crucial part where you can execute many different programs on this vm they don't have to be all the same program they could be different programs and you'll get a single proof that says all the programs have been executed correctly and you one other distinction is that you can provide this witness data for example uh in the blockchain context this could be signatures that once the vm has verified them you don't really need to include them into your proof that goes um onto your theme so you can basically discard the sweetness data or it gets discarded and that allows you to significantly compress things that need to go onto the main chain in the context of polygon midan we have might and dm which as i mentioned is based off of this vm and let me give you a few highlights about the vm itself and where we go going with it so one of the main focuses that we want to have is that to have the vmb developer friendly and uh by that i mean we want it to be as close as a typical uh virtual machine that you know you might have encountered outside of uh zero knowledge or a cryptographic context so that you don't really need to learn anything about cryptography so for example it's going to be a simple stat based machine uh this is what polygon midan is right now it is going to support natively 32-bit integer arithmetic so no need to think about field elements or understand finder fields or anything uh of that sort you can just work with 32-bit integers um it will have read-write memory so very easy to kind of like uh you know learn how to use it it will have native exception handling which is very important in the context of smart contracts where one contract can call another contract and the other contract can fail so we want to make sure that uh even if there's there's a failure somewhere we can still prove that uh that uh you know the original smart contract that called the field uh contract executed correctly uh and uh we are planning to build a very extensive standard library which will support all kinds of uh goodies that people and developers and uh you know for smart contracts and defy might be interested in um the other important thing that i want to mention is that we want to make a mighty uh multi-language solidity will be the first class citizen in this vm but we also want to make sure we can support other languages such as uh for example move uh or other languages that people would want to compile into my vm assembly and um one of the other design goals uh and um it feels kind of related to multi-language support is that we want to make the vm uh safe and actually safer than evm itself so we will not allow dynamic code targets uh and uh you know you still can compile solidity into uh this and still will work fine but underneath the vm will be safer than the evm itself uh which will also make uh making bugs potentially much much more difficult uh putting bugs into your smart contracts and then we're taking some hints from like brass programming language all operations in the vmware safe by default um and there is also privacy focus kind of like you want to build in some privacy features into the vm but this is not the focus right now this this is but we do want to make sure that the groundwork is there where we want to support uh you know both data and functional privacy in the future so that the vm can execute kind of privacy preserving smart contracts and just to kind of like show this diagram of how we think about multi-language so solidity uh we will uh can be compiled into you uh intermediate representation right now and then we'll have this ultimate uh transpiler which will transpile you into midan assembly language and then the modern assembly language is the native language of might mdm and then other languages will be able to compile into might and assembly as well so i talked a lot about where we want to go now let me give you a little bit of a roadmap of how we will get there and uh right now basically last month we released the version 0.1 of my mdm it is it already has kind of stock manipulation arithmetic operations and basic control flow implemented in a few months from now we will be releasing the version 0.2 which will have memory native support for 32-bit integers and we'll have procedures and we'll keep building on top of it over the next year to add such things as storage and support for things that are important uh for edm compatibility and uh you know our plan is within a little over a year to get to a point where you can compile solidity into uh might and assembly and then in parallel uh toward the kind of uh you know late q2 of next year we will start building the actual roll up around the vm as well starting with you know initial couple alphas and then hopefully by q1 of 2023 we'll have data in production uh to experiment this is a fairly aggressive timeline but we'll do our best to make it happen and then to wrap up just wanted to talk a little bit about uh kind of some of the values that we have with polygon might and the the first one is we do want this to be a community driven and fully open source project so uh the source code for even the early version of light and vm is fully available right now we do want to collaborate with anybody who is interested in building on polygon maidan or even just using vm for whatever purposes they want to have so please uh you know check out the repo and uh feel free to uh start using it and let us know if there are any features that are missing so we really want to make it so that uh we built it with the inputs from the community we will be releasing the initial draft of might and assembly and probably within a week or so and the feedback on that will be very helpful and um the other set of kind of values is we do want to build a fully decentralized and censorship resistant roll-up so uh not a single up not to have a single operator but to have uh kind of a network that is fully in uh alignment with kind of web3 ethos and here is uh the link to the repo so check it out and let us know if you would like to use it in some way even before it's ready that would be very helpful what's that i'll open up two questions there any thank you so much bob and this was an incredible um overview on all these improvements that you you've kind of made with the help of the team uh we don't have uh any questions specifically to uh what you've talked about it's just been comments from like this is uh incredible and uh some of these things are just amazing that people are enjoying um if something comes up i will uh happily share them on the chat here uh to you in the zoom but uh you know some specific questions about yeah definitely definitely if somebody thinks of something else later on feel free to drop me a line on twitter or something or anywhere else perfect well thank you so much and uh with that we are ready for our next talk and uh for this i'd like to invite jordi on stage um and jordan is going to be talking about polygon hermes and especially introduction to the polynomial identity language so without further ado let's welcome jordy hello hello everybody thank you very much uh let me share my presentation right now well first first of all before starting i just uh want to well welcome to the to the polygon zero people uh it's a luxury to have them on board and very excited to work with them uh we're already as as as daniel and brandon already said we have been already working with them in the last in the last weeks and has been very productive and i also want to thank you to the to the polygon funders for setting up this collaborative environment in the technical things i think that we can go much further uh together helping uh one each other as a team so it's gonna be very uh incredible and i'm sure that we are going to scale ethereum that then is the goal that we have all together so with that said i'm gonna present today the the the pill uh just to understand where the pill fits in the in the in the project uh well um let me just here let's see half okay so uh the the idea of so the most critical thing for uh roll up is generating this proof uh generating this proof is is is quite hard but the idea of this proof is we have an auto state we have many transactions in the case of rms we have uh normal ethereum transactions we want to one we want to be ebm compatible by code but by code we don't want to compile or just using the compiler we just want to implement the actual by code so we have normal ethereum transactions and we want to create a new state route okay so building this uh circuit is this deterministic program or this circuit is the the challenge so to build this circuit what we what we do is we have uh mainly well we are the approach that we are built that we are following right now is we are building a stark and then in order to reduce the gas cost to uh 300k or even lower it's just to uh verify these stark with a plonk or growth 16 proof okay so this is the the main approach and um the fact of using starks this this is a little bit uh a game changer between traditional r1cs systems like plonk or uh glow 16 and uh more polynomial polynomial base this is in in comparing with electronics this is uh just working with normal ants and or normal gates and you just put them a lot of them or when you introduce the clock when you introduce a clock in the circuit you can reduce the same electronics and run many cycles so here is where the polynomials takes uh very importance you when you read the word polynomial just understand it as an array of values a piano polynomial is a set of values a line you can represent a line by two numbers well you can represent an uh a big polynomial with many numbers so you understand the values and this works well in this concept of uh clock of a step so you have a state machine and every time in the state machine you have a new value and this is represented by a polynomial with in each step one one value and so on so and uh and and the idea of defining the of defining this prover is that we need to define we need to commit to a set of polynomials and then defining a set of relationships identities uh between these polynomials in order in order to build these state machines that work together and here is where the polynomial identity language is the language that we created for creating these special these relationships between polynomials and building these state machines okay uh with this relationship with this uh set of relationship with this compiler this will be used for generating the starks uh for degenerating the the the prover of the stark and then the very fire of the star okay so to start i think the best way to understand how this uh porno identity language is just work just do an example of how this works okay let's start with what would be the hello world of this language in this case is we are defining a very simple uh state machine that actually computes 32 bit numbers through uh from uh two numbers of 16-bit numbers so it's a state machine that has two steps okay so just even steps and odd steps in even a step we just get a number and put it in a register and in the second step we just shift the register and add the second number and then we and then we start over again so we can generate as many uh 32-bit numbers as we would so we have one register in this case is one state variable this case is out and we have one input that it's a value that we can put at any state we can put the value that you want we can put new each value and every two steps we are generating one new 32 bit numbers so hope we would be write that in pill okay i was this would be as simple as that we have a constant polynomial it's a constant polynomial that's so the first is the first number it's one second zero one zero one zero one zero this is it's the instruction what it's doing okay we have the frame the input of the system the number that we freely can set to any number that we want and we have the register itself the register out okay so in this and and then we start defining the constraints to these polynomials the first constraint is that we are forcing that frame is a third it's it's a 16 bits numbers okay in this case we are doing it's included in another constant number that we have defined here in the on top that includes all the numbers between zero and ffff so all done all the 16 bit numbers okay so with this we guarantee that the freeing can be any number but just between zero and uh zero x ffff okay and then we define the the the constraint uh for this sustained machine actually we are saying that the next step must be if it's if set is one if it's so it's the first step then this is going to be one the second part this is going to be zero so the first if if it's one we are just putting the input to the uh to the to the register okay if a set is zero so it's the second step then this is going to be zero but this second one is going to be one and then what we are doing is we are shifting the out and adding the freeing so every two steps we are generating a number that we know for sure that's going to be maximum 32 bits number okay and we define it this this this way okay let's move to a more complex uh state machine this case we are going to define state machines that generates groups of five numbers of 32 by its numbers that fulfills this relationship a times b plus c must be d to 2 to the 32 plus e okay so we are dividing this um more significant bit and less significant but we are doing the arithmetic operation this case is a multiplication and a division okay so how we do that okay we have a state machine with five registers a b c d and e and uh the idea is that uh in in the in the in the five first um steps we are just latching we are setting uh we are setting we are putting the the input we are setting these registers we are setting a we are setting b we are setting c we are settings d and when setting e and once we have other registers set then we force the condition okay when we have to force the condition where we have this second this this this next polynomial we call it latch that's actually when you want to force that this condition fulfills so we have a state machine where you just uh load uh five numbers and then you you you are forcing that these phase numbers are fulfills this condition so how would we write that well we will write this we have this constant polynomial set a set b set c and c b and c d we have also the the large polynomial okay and we have also the frame it's the input and then we have the five registers okay and how and here we start adding the the constraints the first constraints is we say frame must be a 32 bits number here we here is the first thing that it's important we are using the results of the other state machine the bytes4. so this this frame must be a number that must be included in the bytes out for in dot o that means that this number must be a 32-bit numbers because we have the conditions that we generated in the last state machine okay then we do the next uh then we calculate the next stage machine in case it's the a prime b prime c prime and d prime and this is if set a is one then we just uh set the frame to a and if it's zero we just keep the last value okay so this is the conditions for the next a b c and d okay and finally when latch is one we must fulfill that move zoom is zero and mulsum is like an intermediate polynomial that we say that a times b plus c minus d to the 32 c is is and this must be zero in order to fulfill this this operation when latch is one so here we just define it a same machine where we can do arithmetic operations in this case it's this addition plus so here you can do a an addition just for example setting b to zero or you can do a subtraction by setting a and b we'll see in the next in the next example so we hope we use this arithmetic uh circuit so let's now build something more complex okay let's build a computer let's build a state machine that actually execute instructions okay so here we have uh five uh state uh have five registers okay we also have a six uh register we call it the program counter actually is the line that i'm executing this is another register that this is in this state machine we have a bunch of polynomials that are the that in somehow they are the instruction that i want to execute okay we'll see these polynomials how this works but this is the instruction okay we have also a frame okay and with this we have a state machine to go from um a to a prime is the next state of a and we are evolving we are executing we can put any instruction that's defined by all these bits that that compose the instruction and we are executing this okay so let's see different instructions how this would work okay well first the definition would be just we define all these uh all these uh polynomials okay the same way that we saw before okay we defined here a set of um so for this this instructions must fulfill some initial conditions for example we want that in a binary so we say that in a times in a minus one must be zero here the only condition for this to happen is that in a is either zero or one okay we want freeing to be also 32 bits numbers we have in uh we want also the constant to be also 32 bit numbers but in this case we are at um an offset just so that costs can be also positive and negative 32-bit numbers but uh uh signature positive big numbers okay and then we have the address that's just uh 16-bit numbers it's the 2 the one the first one that we we had okay so let's do let's start with uh instructions that move from a to b or from b to a or something like that okay for this there is part of the instruction that's it's in a in b in c in d and in a here we have this is our selectors of which uh register we want to we have this intermediary register we call it top okay so and the data is we will load uh from a b c d and e and we load two up okay this is the first line of the polynomial in the language so if in a is one then we load a if the name is if if in b is one then we load b okay we add them together we can also set the cones we can also put a constant value so the instruction can have a constant value for example if you want to put an uh seven in register a then cons would be good would be value seven okay and we also can select the the the the the friend okay and we set we have this set values to a b c and d so for example if i want to move from b to c then in b would be one the other scenes would be zero the bonds would be zero okay and the set c would be one and they said i said b said d and c d would be zero okay so in this case with this will fulfill this relationship if i want to put a seven in d then cons would be seven all in a c d e and in three would be zero and the uh set d would be one and the other sets would be zero okay and this is how we define until these instructions so the instructions for moving okay we can also do conditional jumps jumps and conditional jumps in this case is we have a circuit we are i'm not going to enter in detail how this works but we have a circuit that determines if the operator is zero or not okay and if it's zero and the instruction is a jump okay then instead of loading the program counter to the program counter plus one we just load a new address so we do a conditional jump in here just by defining how so far seeing how by changing how by defining how we set uh which condition must uh the program then the next problem counter mass mass must fit okay so we have also so we have conditional jumps and we want to do also arithmetic operations we can do we want to do multiplications additions and so on and and how we do that okay and this is probably the most important um slide of my presentation okay because this is the the trick and the main thing that we are doing here okay so we here is we are connecting the last state machine the state machine uh uh that do operations and with the main state machine when the main state machine we have the uh the the instruction arithmetic when we have when we force we have this arithmetic instruction then we are saying that the uh the values in a b c d and of the op operator must be included in the last state machine arithmetic a arithmetically animated cn so somehow what we are doing is we are connecting we are saying in in the main in the main state machine we are assuming that uh summary mating function uh is okay okay so we can set up freely we can put the values in the registers and we are saying okay this condition must be filled and we assume that this is okay but this is because this is delegated the verification of this is delegated to the other state machine okay and this concept of connecting uh different state machines with the block up again thank you to the aztec people with the block up idea this is a great thing this allows us to connect these state machines and this is what allows us to do engineering we can generate like many state machines that are doing different things and connect them all together okay so um yeah this would be of course no we have a processor so we have a processor then we need to define an assembly this is a little bit how we would work the the assembly okay we defined this from going from one register to other and we have the arithmetic or the jumps operation so this is an assembly but there is something that's missing here and it's who we warranty because who guarantee that we are executing the right program we we want to have like a rom we have one to have a program and we want to force that we're executing a program because if we see here this is this these are free inputs you can put any instruction that you want this is okay it will fulfill these uh relationships but how do can i create a rom and execute this program okay but let's move forward and we create this uh from this assembly we create this uh we create this uh uh table so we we encode these instructions with with bits we have in a b and c and b and we create what we call it a rom a rom at the end is another polynomial where the first instruction the second instruction the third instruction and we encode the all these polynomials somehow this is a linear denominator we have the constant the constant value which is wider the address and we also embed the line with the line of the the line of the program this is the first line or the line zero line one line two like three and four and five okay so we have a set of values and with this we have a polynomial okay we have a polynomial it's the wrong okay and we want to guarantee that we are executing this rom and how we do that with again with a single block up we're taking all the polynomials that compose the instruction the address the set a set these jumps arithmetic all the the instructions all together we pack the same way that we did it in the rom instead of having the line in this case we have the program counter it's the problem contour indicates which line should be attributed and then we just force in the last is that in stress is included in the rom with this condition we guarantee that we are executing actually uh uh a rom okay so with this we can have a rom that executed that a rom so we have a processor with a rom that's executing and all that define it with a polynomial identities with with this language okay of course um the evm for building the avm the state machine is similar to this but with much more complex we have of course instead of working with 32 bits operations we're having 256 operations we need to deal with the gas we need to deal with the maximum we need to deal with the stacks we need to deal with the calls so there are some extras because we are tailor-mading this uh uh build so this this processor in order to execute of course ethereum of code so this is why we are adding these these um you know fine-tuning things that will help us to implement the job codes but they are the same of course would be many other state machines like you know cacao signature verifications uh comparators there is a lot of it's a lot of a lot of steam machines but with this we will create the the full uh ethereum virtual machine so to summarize is we have this spin language this is like the hardware layer you know we are defining these state machines that work together we have uh uh with this we define the the the state machine we have an assembly okay and with the assembly we create a rom which actually has a specific program and it's a program that actually implements ethereum it's actually it's a program that process as an input process many transactions and uh actually and calculates the new state actually it's a a single process that executes all these okay we have an executor that actually is the the runtime the thing that takes up the transactions and actually creates these uh polynomials and then uh with this we generate the star this is better represented in this next slide okay so we have the executor executor generates a proof of course here we have an uh an assembly we have a rom we compile that to an assembly to uh you know and this we we we plug it to the executor the chiquita generator start okay with the pin language we generate the we compose the stark end up with the pin language through silicon and all the all the uh the start would generate a circuit to validate the stark and then this is uh then at the end is verified in solidity so this is the the process is very much uh is very much this of course the prover is not the only the proverb is not the only thing that we have to implement the proverb is just a piece for running a fully kvn there is also a note with all the you know transaction pools and so on and of course all the smart contracts layer one layer two here uh if you want to see how we are going to do all the transfers layer one two year two we have a uh i did a presentation in liscon so you can find it in youtube you can find there and of course we also have uh this um coordinator selector we have a new protocol we have we will now improve efficiency probably have some opportunity to explain how it works but this is also what gives the centralization but all this piece is what we are building somehow in inner mess and the planning for hermes of course this is not uh full commitment this is the project we are doing is quite challenging but this is the the this is the the schedule that we are managing internally we are very excited right now we are running very fast we have an incredible team right now that's very motivating for doing this i think we have at least in my eyes we have the best team in the world for creating that uh we are very i'm sure that we are going to get it we are very really very very very excited and yeah looking forward and hope to skeletonian very soon i don't know if there is any questions but that's all on my side thanks a majority um there's a couple of common recurring themes in the questions which is uh people want to know how they can learn this on their own pace uh because there's a lot that happened in this talk and uh people are curious if they can find out whether it's the copy of your presentation to other links to understanding where you've talked about pio i will make it copy and everything we are doing is open source so uh sometimes we are not publishing right now because we are changing very much and it's things like that but uh we will open very soon as as the pieces get more consolidated and yeah you can check out our polygonerms repo and there is a lot of information there awesome um i think with that we are ready to move on to our our panel which is also the last discussion for for today i'll just quickly introduce all of our panelists uh this is going to be a discussion around everything that's happened today we're just going to talk about all the new themes and and all the new products and for this panel actually invite vitalik buterin mihello paul bobbin brandon jordy uh and a lot of you are already here and then this this whole panel will be moderated by anna rose from the zero knowledge podcast so um without further ado let's uh welcome anna and team uh to uh join us on this panel and i'll let and take it from here hello hello hello hi ask everybody turn on video and uh here we go hello everyone again hi so is everyone up let's just give them a second to put their videos on hello metallic and i think paul's here too paul hi and i think we're just waiting for brandon brendan let's see if he can make it as well we'll just give it one more minute uh there we go do we have everyone i think we're here okay very cool so i guess we should kick off um i feel like you've already all been introduced and actually a little bit i want to share a little bit what the plan is for this panel so we have a pretty big panel um and we have myself and vitalik who just joined so you have heard from a lot of the other panelists before you've heard kind of deep dives into what they're working on but the way i want to structure this is i kind of want to start with talking a little bit about a recent blog post that vitalik wrote called end game then i want to bring in all the other speakers to talk about the different zk solutions they're building to potentially compare and contrast them or see how they're collaborating i want to talk a bit about zk community the tools that are missing and if we have time i want to talk about like the zk future so let's kick off with this uh i want to say like high level deep dive into end game the blog post that you recently wrote vitalik so in it you you outlined multiple paths that would give us a trustless censorship resistant ethereum but it would be scaled and the first sort of block was very much a road map and to me that was like a road map without roll-ups there's four design possibilities for design changes um that would help so do you want to just explain to us a little bit what that road map looks like sure so that um a part of the post was basically actually trying to answer the question of like if you take a block like a big block watch it or something even much bigger than ethereum right like one of these chains that tries to you know like have very high tbs have not like and uh completely sacrifice decentralization and have very few notes right so going much further on that dimension than ethereum ones like how if you start from there how could you turn that into something that like say like i would feel comfortable building an ecosystem around um and basically the chat like the challenge with those blockchains as they are today right i think that i get outlined in a little bit when i wrote the post on that uh i think uh the women stopped watching scalability i'm about half a year ago right yeah i kind of wrote that little short story of like you know what happens if like percent of the validators actually do communicatively try to force through some change that everyone else said and hits and you know if you don't have a fully validated nodes then like they're going to be able to actually force that change through and users are not going to be able to do anything about it until it's too late um so the idea that i have is basically that you just add in the like elements of a decentralized validation right and decentralized validation is something that we do know how to do um so we like for example we know how to um you know mark put the state into a merkle tree we know how to like put state routes after like every small bundle of transactions we know how to make a fraud proof we know how to make it so that if the blog has even one mistake in it then like you can make a fraud proof that covers just that one mistake um and then broadcast that broad proof and then the rest of the network even if they don't have anywhere close to the computational capacity you need to actually run a full node they would still be able to verify this broad proof and they would know that age you know this one this blockchain is trying to push through something that hurts the rules right so that was one change um and then the alternative to fraud first was to instead use a zero knowledge first sorry again ck starts and just like verify the correctness of the block correctly and then um also if you want an extra layer of protection you can add committees um and if to verify the data is available then you know we can add in a data availability sampling which is in itself a big technological rabbit hole but like the goal of it is to basically like have this kind of collaborative decentralized way of checking that the data of the block actually is published right and so you know if after publishing that block the notes disappear then like there's still the data floating around somewhere so that the rest of the network can keep building on top of it and if it does have fraud so the rest of the network cannot fraud through it right so my argument is that if you start with the centralized like chains of today and you just kind of add this extra armor on top then you know you could end up with something that's pretty decent right like you know you you solve the fraud you solve the data availability issues you add like some extra channels for transactions to get in that have to be included so they can't censor and like the result is something where blood production is centralized but like all of this kind of extra protocol armor that you're adding prevents them from actually like doing anything really terrible and they're in that first model that roadmap you also said oh i'm getting an unstable message just let me know if you can hear me okay okay okay you actually made a distinction between the block validation and the block production is this like can you kind of go a little deeper like without like law production as a as a separate thing sure um so i think uh original bitcoin philosophy right basically like said block production equals block validation right because you know to produce a block you take a bunch of transactions you run the process of like validating or executing or whatever those transactions um and then you make a block um and you publish the blog and then in order for someone else to verify the blog like they have to run like basically the same code right they have to check the signatures in the same way that you checks the signatures they have to update the state in the same way that you updated updated the state um they have to um you know check the hashes in the same way that you check for hashes like they basically repeat what you did right and so there's this kind of one-to-one correspondence it's like every unit of effort that you use in producing a block is also a unit of effort that every other node on the network has to individually take in order to validate it and this is one of these uh like big bottlenecks to scalability right um so the yeah now with some of these newer technologies we can actually start to decouple uh production from validation actually even like if you don't take into account like fraud groups and cps dark said any of the fancy stuff one other technology that lets you do this is a stateless clients right so this is the idea that whoever builds the block they have to hold the stage so they have to hold everyone's accounts what everyone's balances hold everyone's public keys and they um then like have admirable proofs right so they had workable proof so i can use miracle branches proving all of the individual accounts that were actually touched by the transaction and then they just include those merkle groups as part of the transaction and someone verifying a block does not have to have the state right like you do not need to have like more than you know like a few megabytes of ram basically in order to verify one of these stateless blocks because uh everything you need to verify the block is part of the block right and like the block just tell like the block just has this extra information telling you like hey you know bob did have 75 coins and joe did have 115 points and now joe is going to have 90 coins and uh you know bob's going to have 100 coins and here's the merkle proofs that prove um that like the the numbers are all correct that they like match up with the word right and so if you do that then you're already like you're making this separation right you're basically saying well in fear of producing a block requires having a big hard drive verifying a block does not require having a big hard drive because you just verify these vertical purses instead right and then ziggy snarks they go way further right just with a zika snark you're making production even more expensive to some extent because you're saying if you produce a block then you also have to you know do all this extra polynomial magic with all of its overhead on top um but then when once you do that and once you create and you add the proof verifying the proof is really easy right so the amount of work needed to create the block goes up but the amount of work needed to verify the block goes way way down um and so this already starts to like bring us closer to this world right where um you know you have these uh more centralized block producers but with these like more decentralized forms of verification you can prevent them from doing out like as much um actually mean things because like if they try pushing an invalid block through then like guess what they're not going to be able to make a power proof i see yeah you know i've disappeared but i guess um but just to kind of continue through my job my post uh on my own a bit um so like that that's kind of the the principle that enables all of this right i think i might even say like that's the principle that enables kind of blockchain scalability theory in general like i to me blockchain scalability theory is all about like removing this one-to-one correspondence right and making uh verification easier than block production um so then i started talking about roll-ups right and roll-ups are this really fascinating layer two scaling uh strategy um where basically you like you do even more separation of uh like basically block reduction and block verification right um because uh what a rollups it does is it says well so you have the ethereum network and the ethereum network has its layer one state like you know like like alice can have 75 keys bob can have 100 and all of these like you know the summer smart contract you know a piece of code all of these things are just like kept track of by all the ethereum nodes and then with a roll-up we say well we have this one smart contract and we can call it like this gateway contract and that smart contract stores in its storage and workload and that merkle route is from the point of view of ethereum just a 32 byte hatch right like whatever it's a 32 byte hash if you're i'll give you any handles 32 byte hashes all the time um but from the point of view of its own protocol it's the merkle root of this whole other universe that ethereum is not even aware of and inside of this universe you know like you can have lots of people doing their own things like people can have different coins you can have even more smart contracts and the id theory is basically that to update this workload you don't actually have to like directly tell ethereum what every single one of the transactions are you just provide a proof okay because if you provide a proof um then like you are doing the proving off chain and you're providing a little bit of data and uh like that little bit of data goes um goes on chain and that's just like a highly compressed record that just says you know ls25 everything looks like you just uh muted you got me you accidentally accidentally muted yourself so like i got a question for you if you want to uh since we're waiting for ana to come back i have a question um i saw a couple of excellent tweets the other day talking about this kind of end state and one of them that i thought was particularly thoughtful was uh in this future uh is ethereum really for blockchains and end users are going to migrate to the layer twos and and the future of a theory is blockchain the blockchain connectivity and our multi-chain future is actually um you know it's all the ethereum ecosystem wondering your sort of thoughts about sort of what is the future of the end user in that ecosystem that's exactly where i'm what i'm uh what i'm getting at right um because uh what what you have with these roll-ups is right you have these like extra universes that get created where like in the ethereum protocol by itself is not aware of them all it does is just verifies a few proofs um but then the roll-ups like they do have all these universes and doing things inside of these uh roll-up universes is much much cheaper than doing them on ethereum because of how it doesn't like the entire ethereum what um ecosystem doesn't have to verify all these transactions just just the people in those roll-ups have to verify those transactions and so what that basically means is that you you have much lower scalability or so you have much higher scalability um and you also have like much more creativity for people to do different things right like you can have a roll-up that runs blossom you can have a roll-up that runs a bitcoin like utxo model you can have a roll-up that runs like a version of the evm but paralyzed um and you know you can get a lot of these really cool benefits of having a multi-chain ecosystem except with uh ethereum acting as this common base layer that basically provides what we call shared security right so like instead of you know having like 100 blockchains those hundred blockchains all talk to each other and then if you have lots of these very interconnected applications if even one of those blockchains get 51 attacks then that might like risk the entire network you just say well no all 100 uh all of these hundred things are roll ups they all like live on ethereum and uh you know when they um and they all derive security from you know the ethereum base layer the ethereum data availability layer and all of these things and so in order to make any single one of them break you basically have to make ethereum break right and like so this kind of like shared root of trust and shared root of bridging like it allows for cross roll of communication to be really efficient into the last for sure security you have all these um all these nice properties but at the same time right what this means is that like the long-term future of ethereum you know either one roll-up wins um in which case that one roll-up has to learn how to be really scalable and we actually end up in a world that looks really similar to a world where you start with a non-scalable base layer so you start with a scalable but centralized base layer and then you add this kind of thrust armor we are instead here like you start off with ethereum which is just the trust armor and then you like add this extra roll-up thing that provides a scalability right so if one roll-up wins then we're in that world and then if many roll-ups win then you know we're in the kind of interchange world but with shared security so i thought that's like really interesting right like there's a lot of different paths to doing things in the blockchain lanes but a lot of them actually end up leading to very similar places [Music] very cool um i'm back i apologize i dropped off at some point but thank you so much for continuing there and actually this is a great point to open the conversation up a little bit um on this topic of multi-chain i mean here we have with us a number of teams that are going to be building very different types of roll-ups um but first before we do that i wanted to actually ask mihailo polygon has invested in zk and i i'm very curious like a lot of the teams have presented very roll up focused concepts do you feel like it's primarily driven by the scaling need right now or do you think that there could actually be new possibilities that are opened up by this you mean by the by the zika rollups in general like these are basically yeah it's like investing into zk for scaling do you think that's purely for scaling or do you think it opens up new possibilities definitely yeah that's a fantastic question yeah thanks so much anna we believe uh zk zq based solutions have have much wider basically area of application than scaling specifically it's only that scaling is now still a pressing need in the ethereum ecosystem so we have started like basically vitalik started something amazing and like millions of people want to join this this new paradigm this new idea of global internet of value and they're free and we really have this pressing need to to um welcome these people and allow them to experiment with the uh with ethereum basically so once ideally i foresee maybe in one to two years we will we will have some uh uh some solutions in place that will be scalable much more scalable than those that we have today and that will have security properties of ethereum and that pressing need to scale will be kind of relieved kind of relieved then i believe we will see other interesting and amazing applications of zika technology primarily on the side of privacy uh related to identity for example and many many really other other interesting applications that is at least my opinion or my bet for the future very cool um i want to ask a question to jordy and brendan actually so the two of you have proposed uniquely the zk evms and i've been very curious about kind of how those are similar or different like i know you're working together in a lot of ways but could we explore that a little bit what a z like a zk evm maybe maybe also just define what that actually means and how those two solutions are potentially a little different go ahead jordy yeah i can start it's yeah there are i think there are two projects that they are started in different point and probably with different goals and with different environments and we were independent none of us were at quite for polygon when we started the project and we have we come from different places and the well i think we have a good things and things that maybe other teams are better and they have good things and maybe things that we can go better and the the the good thing of of being in this case together in the same umbrella of polygon is that uh we have this collaboration relationship and right now i understand much better what they are doing uh and they probably understand much better what we are doing and uh we can collaborate one and help on each other actually i would like this to be even opener not just to the polygon projects just to wider projects and and because we are solving it's a humanity challenge you know it's not uh it's something that we need to solve if we want to block change to to scale we need to somehow work together and and for us you know these are uh there is no body that holds like the other knowledge for building this and and as much knowledge we can join together in order to scale this we will do it faster and better and and this is a little bit the spirit of course collaborating teams is is not easy but uh you know it's just we are just starting you know it's just uh it's just the beginning but the the the feeling is that we are sharing the same goals we come from different places we have different knowledge but we are sharing the same goals and uh i'm sure that we will uh work every time closer and and better nice brandon do you have any thoughts on that i mean i think jordan you just shared sort of the similarities i mean the the use of fry or the use of fry-like techniques in both are a similarity there that i noticed from the presentation that brandon did earlier um but brandon yeah maybe do you want to share a little bit about the way you're thinking about it uh yeah sure i i think just to echo what jordy said um it's been really great working with hermes and with maiden and i feel like we've sort of created the foundation for something really special moving forward um i think that the way that we see it um i think i i don't want to speak for for jordy but i think what they're working on is is super cool like um sort of simulating uh the evm kind of op code for op code or close to that um in a snark is like an amazing goal um for us like our ambitions might be a little bit more modest um our goal is we we'd like to be able to take existing solidity code and transpile it um into what we call zk bytecode maybe swapping out some things that are really expensive uh in a snark like sha 3 for more arithmetic friendly um hash functions um and yeah just just trying to to to build something that's that's very performant and um and can scale um so that's that's how i'd sort of characterize um both like it's just great to be able to to work within polygon and to kind of explore uh the whole design space and collaborate i actually want to i want to bring in bobbin a little bit on the conversation now because i mean that use you talk about the collaboration and then you're using fry which te like originally came from starks bobbin you're building a stark-based roll-up do you want to tell us a little bit about i don't know maybe we can talk a little bit about the connection or what you're getting from some of this dialogue oh you're muted by the way about that yeah so i think in terms of collaboration there is a lot of like i think there are some similarities with both projects that uh kind of we think about uh in many different technical areas one example that i kind of talked about in the uh in my presentation is that because for example uh um starts are very flexible and you know fry allows you to be flexible in terms of choosing the finest field you're in uh maiden is actually using the same finite field as polygon zero uh is using so we have abilities to uh kind of like jointly work on different things that might be useful in that specific field and uh use you know it will be used in both projects and that's in terms of optimizations and in terms of like developing arithmetizations for specific problems that you know either they come up with and i can use or i come up with and they can use and you know uh that works uh very well and uh you know their other decision points like in terms of like how do you like overall i think there are three big buckets of choices when you kind of think about these solutions and the first bucket is uh you know what proving system you use and what are your utilization you use and then the other bucket is kind of what is the design of your virtual machine for example are you going to you know which field are you going to choose what is going to be your instruction set is it going to be very close to the edm or as close as possible or is it going to be slightly different and then you've got transpile and the third design choice that they need to make is what your network architecture is going to look like and you know there are different uh like you know with hermes we might look more into like their network at the architecture of how they're thinking about and try to uh have like some some of the learnings from there to apply to maidan but uh in the proving system uh aspect we might be more close to what polygon zero is doing and so we might take some learnings from there so there's a lot of different areas to kind of collaborate and uh cross-pollinate knowledge across i feel like your conversations are probably incredibly interesting i hope you document some of this for others because like just thinking and actually yeah what are the plans for how to share information within the org and also outside of it i think one of the things i want to say the conversations that we have is like probably a highlight of my week like i have to wake up a little bit earlier than i usually would like to but uh it is a highlight of my week when we just basically chat for a few hours about like the things that we've done over the last week and what we're planning to do and like what problems everybody is facing and like what solutions uh we have found so it's it's really cool yeah i can only second but what uh what bobby said and it's like really really one of the things of course we are really proud that we have uh all these amazing people and these amazing projects now under the polygon umbrella but what we are really really proud of and what is really exciting is to see actually these teams working together it's like very clear spirit of collaboration not even a mentioning or even thought about you know one team being competitive to each other or something like that and we really are aware that this is like early stage of innovation and it's like really amazing and inspiring to to watch these teams basically uh on these calls uh discussing exchanging knowledge updating each other it's like really amazing and exactly as jordy said we would really like to be we don't want to be a closed source you know zk powerhouse like or something that is not shared with the public we want to include all other relevant projects individual individuals we are still discussing and thinking about ways how we can make this more open to the general uh community and to everyone basically that are interested in scaling ethereum in this way cool well i like to hear that paul i want to ask you about your solution um so when i heard about nightfall it like this idea of mixing uh optimistic roll-ups with zk roll ups yeah tell me a little bit about when that kind of came to fruition what was the like why did you decide to actually add the optimistic to this so i mean we've been working on privacy for what five six years now we actually showed the first version of of nightfall in 2018 at devcon in prague and we've been we've been cranking out we cut the gas fees we implemented you know simplified all of the data sources we we ended up with uh batching and you're just in an endless race with how you uh kind of drive down the gas fees and the cost structure to make it affordable so you know there's if you talk to enterprises there's a couple of things that they really care about number one right they really want to have privacy like you know enterprises will not do anything if i'm going to move my inventory or buy stuff from another company that's sensitive business information how many widgets i'm buying who i'm selling them to how much i'm paying for them that is incredibly sensitive they will not do anything without privacy that's like job one and then the other issue is and and this comes up all the time with enterprises and you know i um is they're like i'm worried about the gas peaks are the gas fees going to be unreliable is it going to be too high right they they understand that they want some predictability in their cost structure so back at the beginning of this year we were looking at sort of where to go next and we realized like optimistic roll ups give us this excellent combination of um kind of speed uh cost and privacy and so nightfall three became an optimistic roll up and then we're working with polygon we're able to turn it into polygon nightfall and add things like instant withdrawal that we showed earlier today very cool and that focus on privacy i actually kind of like we mentioned this sort of like maybe with the scaling comes privacy but to the other three teams here bobbin jordy and brandon is that very like how close on the road map is privacy for each of you like is that built in or is that kind of coming later i can try this i'll go ahead okay i'm just going to say very quickly like we're you know the the level of cooperation that you were talking about inside of polygon across those teams that has also been applied to the relationship with ey and the level of cooperation is astonishing with so many brilliant people like brendan and bobbin and and there's a you know mahalo is coordinating a lot of that so i will say we are we're we're not going to silo the privacy knowledge in one place i wouldn't imagine that it sounds like that would be very off-brand cool okay bobbin yeah you're about to say something about that i do want to say that like even though the scaling is the primary goal and uh but i do think like for me personally privacy is also uh almost as important as scaling although we need to prioritize uh scaling right now but i do think like when i think about like the design of the virtual machine is like like how can we support privacy design of the network itself that we roll up how can we support privacy in the future and i do sometimes very conscious design choices which may not be necessarily ideal for uh supporting scaling although it's not going to hurt it too much but they will make supporting privacy or privacy preserving smart contracts much easier in the future so it is very much front and center for me cool jordy brandon yeah it's part of course it's part of the roadmap of course it's not like the first priority that we have right now as as as boeing says i think we need to solve right now the first is to scale but uh immediately next is uh immediately next is privacy and this is of course uh important and yeah maybe brandon can add some value here but i think that the recursive recursive snarks is a key piece for for privacy uh at some point so maybe random you can add some some points there yeah i think just like you said i i think for us one of the obviously we've been focused on scaling but one of the benefits of having um really fast proverbs is and sort of focusing on uh commodity hardware for approvers is that for applications that require proofs to be generated on the client side we can do that like very very quickly and so our proven performance sort of makes transaction creation feel like instantaneous for a private transaction whereas before if you're creating a proof for a more complicated or like general smart contract it could take like 30 or 40 seconds with previous primitives and so that that's something that that we're excited about but like bobbin and jordy said we're sort of focused on scaling for short term cool and paul i have one last question on your project which is like i i know the project zk opero which is also a zk roll up optimistic roll up combined how do you see yourselves as different from that or similar so we looked across when we started out some of our work and thinking about optimistic roles we looked across all the layer twos right there of which we sort of counted up 12 right four of them are focused on privacy um zakapuru is next to ours in terms of snark usage one of the most efficient i think where we see kind of a really important difference beyond the the integration that work with polygon is our license is a creative commons public domain license and there's a very conscious choice behind that there's no gpl there's no anything right so there's no complications there's no friction you don't have to sit there and go through the license terms i think is there a gotcha or a surprise here it is a pure unrestricted contribution to the ethereum ecosystem and and there are there's no surprises there's no monetization model in there um uh i realize i'm not gonna make myself ceo of the year with this speech but uh it really you know we wanted to drive adoption and make a contribution to the community and and this is where we kind of made our stand and in in working with polygon we found people who were like yeah that's a good idea and and that made a big difference that actually like the next topic i want to touch on are the tools that are missing and i feel like this kind of open source ethos like let's think of it in that context so very very usable very open source what what are the tools that maybe the polygon teams or polygon affiliated teams or maybe outside teams should be thinking about building in the zk space i know that's pretty general but i'd love to hear from all of you what you're thinking about this sure yeah thanks anna for the question and i think of course the question is very important and there's like several uh ways or several things that we're doing in in that regard so first of all with all the teams are pursuing this ethereum compatibility in one way or another either implementing the evm itself or introducing compilers that were support uh solidity etc and that is already building a huge service to ourselves first of all like we're benefiting from everything that has been created in the ethereum ecosystem so all the toolings from wallets to block explorers to all all those very important toolings and we are very much aware building polygon and polygon psja an example and plasma before that we are very aware of this big difference once you build a solution that is not ethereum compatible or even compatible like we did with plasma initially and once you have an evm compatible solution it's like a huge difference so you instantly are in a much much but better position because you just simply can inherit i mean simply uh relatively easily you can inherit all the existing toolings of ethereum that's number one number two we are fortunate that we have now multiple zk based effort or zika roll-ups specifically within the polygon ecosystem and we are already starting an internal effort to to kind of uh introduce some standards because there are a lot of components and tools that will be shared between these solutions so the core i guess engines and or virtual machines so these solutions are quite different i would say but the other components are are relatively similar and there's a lot of overlap there and moreover there's overlap with the existing pos chain there's a overlap with the the stack of our polygon sdk a solution another solution that we have and we are currently establishing these standards and we believe this will significantly uh speed up the development and make the solution at the same all the solutions at the same time uh uh more bulletproof in a sense that all of them will use the components so there will be the same component so they will be uh much more battle tested and and uh i guess secure uh the third thing that we intend to do is that we as as i said uh earlier on the panel we have this one billion which is pretty significant commitment to develop these types of solutions we will be offering extensively grants and publishing constantly missing components or tools that uh are required for the ecosystem in general not only for political of course and these are at least three things that immediately come to mind when it comes to uh building or introducing all the the missing components yes so yeah i'll leave it to others yeah that question of which tools this has actually come up recently we're doing a bitcoin side round just fyi all focused on zk tech and one of the thoughts that we had around that was like how how like what what does the community actually need in terms of zk and how is that communicated outward like i'd love to find ways maybe together with the polygon team to like gather some of those tools and share them to the larger community so that like we can find those teams to potentially build them i kind of want to throw to vitalik again about this like from your perspective maybe maybe it's not exactly tools but what pieces do you feel need to be implemented for some of those futures you outlined earlier in this what kinds of tools needs to be implemented let's see i think um i get like first of all league yeah the ethereum layer one um needs to be yet improved obviously like you know we need to actually finish up with the proof of stake um we need to execute on the shorting roadmap and you know like actually add more data space um so that rollups have uh some place where they can put data on number like about a week ago i got released to this post that was the the data shorting road map where ikea talked about how to like split it up into different stages where like just adding more like reducing the gas cost of call data would be the first step and then you would add like a shard or like a a couple of shards to just had a bit of data space and everyone would still download it but like it would at least get the uh shorting machinery um in place and kind of test it a bit and then that just gets expanded over time and then we add data availability sampling over time so the whole thing stays so likely friendly and eventually we would just have this like you know several megabytes a second worth of that data space that rollups could just freely use and that that by itself is already enough to just give a you know a huge amount of steel ability for rollups but in order to get there that stuff has to actually be built right yeah so that's one piece um and i think uh you know zk evm implementations they've even talked about them a lot but also just like very important in the very general purpose uh infrastructure um i think one other thing also that i think doesn't get talked about with the zkdm implementations is that they have two purposes one of them is that they enable fully edm compatible ck roll-ups and the other is that they enable better light clients of the ethereum chain right because um for the ethereum chain itself like you know we would love to start it we will have to make it so that you don't need like as much of a heavy note in order to be able to process and like verify things that are underneath your engine um so if uh you know that itself gets us then then that's uh also something that's going to be amazing um sure no no go ahead sorry um number three um cross roll-up uh bridging infrastructure i'm just uh you know if we're going to have a goal year two world in general like that said you know can't be avoided um like i've written some posts on like how i think some crossbow bridging can be done in a fully decentralized way i know there's a whole bunch of projects that are working on like a bunch of different uh versions of ways to do that um so really excited to see that space mature more um pretty cool and then yeah that's spirit stuff yeah okay that oh no no keep going if you have another one well you just mentioned actually bridging and i want to understand like as as we imagine more and more roll-ups coming out all having these different forms mihaela you talked about standards but what is like or and maybe it's too early at this point but like are you all thinking about the bridges between your different potential roll-ups and and is this something that you already see on the horizon is this a problem you're gonna get to later i'm just curious what the thinking is around that yeah great question and i just might use the word standard the word standards is we're being very cautious with that word it's polygon at this point like we don't want to impose any sort of constraints we believe this is a phase really of high intensity innovation and experimentation within the ethereum community in general including polygon of course and hence why we do not want to impose any final you know global architecture or ways or standards how these applications or projects will communicate among themselves we really want to we want things to happen organically we see a lot of collaboration between these teams and let's see what happens organically so might we might see some of these solutions converge into one maybe they will not continue to exist separately again what is really important for us is that they're all working together towards the same basically goal so we are being really careful not to impose any standard standards when i said standards i meant like help for basically uh reuse using components in a helpful way um but that being said moving forward in the future like vitalik as he mentioned he had a very interesting idea uh how to to bridge these rollups in a trustless manner and we are willing to explore we just want to explore and facilitate as much innovation as possible we believe polygon is a very good position now in terms of resources rich network effects and everything and all of that was or pretty much all was enabled by by ethereum basically sovio ethereum community and ethereum uh uh layer one a lot so i think this is now the perfect time for us to to start giving back actually cool i want to kind of imagine like i think i think we are at the stage where we can start sort of also like predicting or not predicting imagining the future of zk what what could be coming down the line so you know we've talked about this pri the private roll-ups often in the case often it's like private transaction within the roll-up but when you start offering something like private computation i'm kind of curious like what does a theory what did what are things living in a non-private and a private actually like how would they is there any envisionment on how these would interact and i don't know vitalik if you're thinking about like what a privacy it's it's a roll-up in cosmos you call it a privacy zone but like what kind of thing what what does the fully private roll-up with potentially private computation actually mean yeah i think like privacy preserving smart contracts the infrastructure is uh i mean it's a really building it's it's a really big and exciting field all on its own um i think like one thing is that there is no magic bullet right i think uh and like there is no magic like and that's more true for privacy than it is for scaling right because like for scaling you can't say that there is a magic bullet you just like try really hard and have a bunch of amazing and really smart people make ckbm implementations and then you just copy what you have right but with privacy the problem like the is that um you have to start thinking like much more explicitly about well like who actually gets to have each piece of data right because in order to make an update with a piece of data you need to have a piece of data and so you can't have uh you know literal black boxes where nobody sees anything and you know you have to have data that data has encryption keys and there's some people see each of these at each of these encryption keys and so like you have to think explicitly about like who are the owners of things in a privacy preserving system that you don't have to think about in just like an evm system and so like you you are going to have to like work hard on programming languages in a way that you don't if all you're doing is just copying solidity um you have to like actually think hard about like what applications are possible do applications even need to be redesigned like is privacy preserving human swap something that's even possible right like who would even have the portuguese to the am app or you know do you have to design it in a totally different way um so you know it's this big field and i think uh it is going to take quite a bit of warning um yeah yeah i expect um you know we're going to start with the simple stuff like you know just being able to move coins around and then people are going to build more and more and we'll see what happens that idea of the private amms and this is a topic i've covered a couple times on the show i wonder if the roll-up builders are the roll-up builders working closely with the amm builders on that front like is that is there a strong connection between those two communities paul i don't know if you have any thoughts on that so i don't know if there's a strong connection but i i do want to there's there is something very there's a couple of very specific things that we are working on that i think are are relevant and you could make a private amm one is uh we're building something called starlight and the idea behind starlight is you know what you can do is you can mark up the logic in a solidity smart contract and we're doing this now in cooperation with polygamy mark up the logic in the solidity smart contract and it will compile it into a zero knowledge circuit now right now we can take a standard erc20 contract we put it into starlight and what we get out is the original version of nightfall basically right so we can do it for a simple erc20 what we want to be able to do is to basically throw any arbitrary business logic on it and then generate a zero knowledge circuit coming out of that so you can have logic that runs on chain and the reason you want logic to run on chain with privacy and not just do it off chain and post to proof is a lot of business processes between companies there are multiple participants right if you have a network level business problem you can't have an individual participant successfully solve that for all the other network participants behind their firewall in their erp system right now they can see too much confidential information from all the others so uh this is something that's super important and then the other thing is um we're working on something called midnight the goal is to make all the transactions look the same right now you can still look at oh he made a deposit she made a withdrawal we want to uh basically really make things are even darker and make it harder to do network traffic analysis on who's doing business with him got it cool jordan b i have a quick question for you i don't know if like are you have you spoken with a any of the amm builders any of the d5 folks i'm just so curious if there's like if there isn't yet like a crew doing that we should start bringing you together i've been talking with some of them but not uh in the roll-up context i've been talking just for uh you know editing smart contracts and you know formal verifications and other stuff that are very interesting and they are managing but not in the uh not in the not not yet no it's not but at the end they are a smart contract so of course if you go to privacy i mean this is like another story but at the end uh actually it's like an example of a smart contract that can be run in uh the kvm and and and from this point of view is not the only smart contract because we are planning to run any smart contract on there but of course is uh if it's not the first it's the second smart contract kind that you are looking at and yeah and yeah that's that's that's important a lot of the traffic right now ethereum is happening in in those smart contracts so far i know we don't have too much time left but i want to ask a few more things so like so far we've talked really about zero knowledge for scaling zero knowledge for privacy but i know that there's some like moonshot use cases for zk and this is maybe not on exactly what you're working on but i want it since i have like six zk experts here why don't we if you're up for it just sort of share some ideas or some things you've heard in the zk space that could be really really exciting coming down the line that maybe even go past those two use cases this is to anyone and i hope there are some ideas i think um like oh well like they do end up having to do with uh scalability and privacy but there are applications that take advantage of the privacy in very different ways um so voting might be one example right like a lot of the voting in these existing taos just ends up happening completely in the clear um and i've written many times about like why yeah that style against that leading to bribes it ends up leading to just all kinds of collusion and there's some kind of like fairly nasty things and you know having more secret ballot elections is something that could really improve uh governance um so that like that and you know the work that's happening with macy and like all those uh projects i think is something really interesting um another one is um zero knowledge proofs for like anti-dos like anti-spam purposes um so the idea that of that like you can prove that you have some token um and uh without actually proving like which particular uh one of those people you are um like i think that's something that can be used to like protect decentralized messengers um status like for example against the dust attacks um it's uh if the token that your zero knowledge proving is uh a proof of humanity token then um you know you can have like things like ubis and like things like that you know like civil resistance in a way that's friendly to people who don't have lots of money um without like having to actually yeah you know create launching happenings because people like transactions of people's literal faces um so i think that's uh you know something that could be huge um yeah i know what you just made me think of as an example that comes more from the financial which is like white listing using zkp's but actually that idea could be used also just for like accessing groups or like knowing that certain tokens are in somewhere without knowing anything more about that that account um or that you you user like it's very i mean there's some really cool id like identity groups group management like yeah i don't know this is an exciting space so what about things yeah as vitalik said i think more or less all of these use cases go back to either scaling or privacy in one way or another basically but there's like really so many uh interesting or really necessary applications like one recent example it was this constitution dell where people amazingly you know pulled together a capital and almost bought the the us constitution and that's just one example where you really need privacy because if that fooling of capital was private then the opposing side or other leaders wouldn't be able to know how much constitution now is actually willing to bid and they won't be able they wouldn't be able to kind of somewhat trivially outbid them because you just simply know the the amount of capital that the bidder has a disposal so it just won one out of hundreds of very interesting applications of uh zkp's when it comes to scaling and privacy right private voting is really important and then the other one i i think is going to be super important is proof of regulatory compliance without having to disclose all of your personal private information right and i i think this is going to be a super interesting one for regulators what's going to happen the first time a company submits a mathematical proof instead of all the documentation i i want to be in the room for that yeah there's going to be a little bit of educating to get there on more on the regulatory side but um what about games i mean zk games there's one obviously great example with dark forest but i've also recently seen a lot more folks kind of come up with game ideas and yeah i'm wondering if you've seen anything in that direction as well that might be exciting anyone anyone we can just leave that there games and zk it's a thing check it out i don't know um i haven't seen anything but i think once uh like building ckp um focused applications and like uh you know if it's easier to do with virtual machines so which compilers or whatever once it becomes very accessible there is going to be a lot of use cases within gaming where you want to you know prove something to the other person you know whether it is for exchange purposes as it is already happening using blockchains but maybe even outside of blockchains there could be very interesting use cases uh for using the kps but i think we do need to get to a point where a regular developer can just pick up and start programming something that has a zkp functionality built in very cool in general actually do you see like the polygon zk fund this pool like this is meant i'm assuming for a lot of tools like scaling solutions libraries but do you think that those like applications that live on top but like really really deeply use zk would also potentially fall under that category absolutely absolutely also funding uh important research activities and everything that is directly or indirectly related to to building or adoption of zk uh based uh scaling solutions and private privacy focus solutions everything falls under the the scope i guess of of this fund definitely that's why like for us it was really a huge commitment like as i said maybe as i said in the beginning of the the event we are fortunate enough that our treasury is very strong now but even like for us this was a huge huge commitment from our side and it just uh basically maybe manifests our our belief how important this technology really is for for the whole ecosystem for the whole industry and that's why we decided to commit this significant amount and really help again everything that is indirectly or directly related to either research development or adoption of uh this technology that's awesome and i want to mention like just uh to piggyback when mikhail was said is that they're actually we're trying to do a few a couple of like fundamental research type of uh things where we try to uh you know figure out and you know engage researchers even outside of the polygon to figure out how we can for example speed up our station from the hash functions even more once we have like specific constraints of you know we're gonna use this field can we do something much more uh uh kind of performant in that specific field rather than trying to build a high arithmetic friendly hash function in general so uh there are a couple of projects that we're trying to do just even in the foundational research type of area fantastic i think we may be at time um kartik yeah are we if there's any uh closing questions we can we can go with that but uh we're at time okay well yeah if there's any like anything that anyone here wants to mention before we sign off anyone nothing ck is exciting join us maybe one on i previously said that i'm really personally as a co-founder of polygon i'm incredibly really humbled and proud and grateful that we have all these great people now under the the polygon umbrella and such a huge support from the ethereum community including vitalik this is like very huge honor and again a responsibility but we are like fully committed and again grateful for all the support and we are really just starting expect a lot of great things from ourselves very nice cool all right well i guess that wraps us up then good luck everyone and we can't wait to see what happens in the polygam in the polygon zk world thanks a lot everyone thanks anna thanks everyone thank you everyone and uh thank you so much anna for moderating and uh i think we are ready for a close and one thing i just want to say is i want to just thank everybody for tuning in and watching this whole thing because what we're talking about what we're doing now is what's going to be the standard a few years from now and that to me is the most exciting piece like you're you're seeing a sneak peek at what the future looks like uh in different pieces and just kind of it's just the same quote around the fuse is already here just not evenly distributed and uh we're just seeing a glimpse of what what's to come so thanks everybody for sharing everything that's that's happening in this entire ecosystem it's not even just polygon but all together ethereum and zero knowledge and snarks and starks and i can't wait to see what comes out of all this that benefits everyone sorry and huge thanks to you kartik and and uh your whole team for facilitating this thank you so much we'll be incredibly helpful we're we're honored to to fight the opportunity so thank you so much and with that congrats on everybody who stayed till the end and uh tuned in to see what's happening in this entire ecosystem this wraps up zika day for uh for us and uh for uh those of you who are wondering and i know that i'm going to get this question the most in the next minute we will be sending the poep tokens to everybody via email directly so don't worry you'll get that email directly into your inbox if you sign in and uh stay tuned for more things to come from the global site we'll be very soon sharing what 2022 looks like for all of us and uh [Music] hope all of you enjoy the weekend and see you all in the next year take care everybody [Music] you 