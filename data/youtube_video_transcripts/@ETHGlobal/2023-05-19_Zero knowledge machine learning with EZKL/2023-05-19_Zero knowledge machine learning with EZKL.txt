foreign hello everyone and welcome to the easy KO Workshop zero knowledge machine learning with ezko joining us today is Jason Morton and Dante Camuto who will be taking us through this session and with that I'll pass it over to Jason and Dante to get the session started yeah thank you very much um nice to meet you all um we'll Jump Right In um with a screen um okay um yeah so we're uh Jason Dante we've been working on uh building this uh software for machine learning um uh on chain or for doing zero knowledge proofs for machine learning and today I want to talk about how to use that for autonomous worlds which is something that people become increasingly interested in um in recent days so okay so what's the picture so one way to think about it is that we're providing kind of a physics engine for autonomous worlds um so we can do kind of arbitrary Matrix computations uh arbitrary kind of machine learning computations convolutions all kinds of crazy stuff um things that you wouldn't or be able to write in solidity because of their you know gas costs you can sort of offload those heavy computations to this engine um so the client uh the game client or server can prove sort of a state transition of a game so you can imagine you know the there's a diffusion happening in the game or there's uh an evolution in the world State that's more complex than what you want to implement with like a ECS and and so you can use this as a way to kind of short shortcut that um you can also use it to build NPCs that are powered by AI um and a lot of the examples people have done as well are things like kind of a Fickle God model where um you're trying to please some you know you're trying to pre please someone like a cooking mama or you're trying to please some some of my creating some kind of pattern in the game um and then a class what's happening behind the scenes is there's a classifier model that's being run on your input um of course sometimes those models don't work as well as you would like them to but the fun part about a game is that even those failures can become sort of fun part of the game like beating an NPC um so let me back up for a second and say what does it mean to run a model on chain an AI model on chain so you're all familiar with an ecdsa signature which is a kind of zero knowledge proof takes public inputs and the private key combines them in a well-known signature function produces a result which is the and the signature and then you pass that to the chain to verify it um or you know someone else can verify that signature and what a zero knowledge proof is is basically just a programmable signature so we can replace that signature function with any function we like the functions we like are kind of ml models and linear algebra and what we're doing with Ezekiel is to make it easy to create a model in Python describe the pipeline that you want in terms of who is responsible for approving who's responsible for verifying and what's the setup we bake that into a prover verifier pair um and then someone produces a proof either on the client or the server and then the chain verifies the proof and it's as though the model has run on chain even though it ran off chain um and the big picture of course is that AI is something that doesn't run well on chain now so AI is and machine learning and linear algebra are are great but require trust either in the general context you know trust from someone like open AI or trust from the person who's writing the game server or the game client um we can work at python or other numerical languages there's a big library of existing models and Transformations you could use it in a game for on-chain contracts um they obviously have the property of being decentralized being trustless being composable um and having this property of autonomy that with it many other people talk about um but they're less scalable because they rely on a consensus mechanism the same computation has to be repeated many times and they're kind of too slow and too expensive to do complex Ai and machine learning or linear transformations um Decay brings uh to the trustlessness decentralization and opposability some scalability because now instead of having everyone have all you know all the consensus nodes have to produce do the same computation um the single a single model can be a single machine can do the single client or a single server and then everyone else can trust it but of course um would pay a cost in terms of math and security properties in a weird programming model um and possibly weird languages um but you know what we've done with Ezekiel is to make it easy to use Python uh models or python description of the linear algebra or the machine learning model that you want to run um so that you can access the library of existing models and or train your own models uh and then make it easy to deploy it so that we can use it in an on-chain game so um and I want to mention an example by you know the rich Patricia and Paul Henry even though it was on the call as well I think um and what they did was to produce just so you get a sense of what good kind of thing that happens is they made a world the players could um take actions and then you evolve the state of the world in response to those actions like you know trees spread and grow or fire spreads or whatever with uh with a zkml model and there's another llm that's not right now on ckl but will be eventually that tells stories about the things that happened um just to give you a sense of the kind of evolution that might be possible um the other piece I want to bring up here is how this might interact with mud so there seems to be a lot of connections with lattice's mud and I think we're going to see a lot more integration happening um as people experiment with it right now if you look at the bottom part of this um picture is kind of that without a mud we're going to be responsible for writing the code to ingest state from the chain um or to take an action the player action prove an update of the state how the player's actions or interactions result in a change in the state of the game and then that that proof goes to a verifier that lives on chain and updates the state and now it's back into State on chain so there's kind of a game that's happening between the client and the state and the state that's sort of a shame where the more complex updates involving diffusion and whatever else happened uh client-side or server side and not not in a smart contract um that would require a fair amount of development like an example I showed you before but with mud you can use it the ECS framework to sync the state um uh with the chain with the client State and the chain State and uh it's sort of for sort of the diagnostic about where we're going to do the proof see that on the client side or the server side um but a lot less work in terms of producing that of course like what folks talk about that so the basic idea of how um a zkml proof works is very simple um so we're taking floating Point numbers uh and representing them as six point numbers so we're picking a denominator like 256 and we're quantizing it um and then we're representing a small floating Point number seven to six as literally seven we're just storing the numerator and then we keep track of the denominator 256 in the type system um and to prove uh dot product um you could write a custom gate in other words make the constraint uh into the constraint system of the zero knowledge proof that says that y the output of the first of the two vectors dotted together is equal to whatever they are that's not really what we do we do something more complicated there's a lot of different arguments that can be that can be made to accomplish that but um sort of not critical for you to worry about exactly how that argument works because we've abstracted that away and then to prove that Y is equal to ax plus b then our matrix multiplication and a shift has been applied you just basically repeat that argument or arguments like that and then to prove that uh a non-linearity like Realo has been applied um you leave what we do is we pre-fill a table with all the possible inputs and outputs that might occur given our assumptions about quantization and the maximum size of the numbers that might appear and we prove that the input output pair lies in that table that's called a lookup argument there are other alternative ways to do that and there's lots of new augments of pipeline to make it more efficient but it's already pretty fast um so so um I'm going to pass off in a minute to Dante um but I want to say depending about the big picture about how to use Ezekiel basically what you do is you find a model that works for your use case you either train it yourself you design it yourself or you download it um for example you're using it to compute a state transition an evolution of the world or the natural world you might be using it for this judge you know is there's a player recipe uh make me happy or did the picture that the player Drew or the song that they sound uh satisfied they should be basically on chain AI or to run an NPC AI or some other idea that hasn't occurred to anyone yet that you'll come up with um then you take this model and you compile it you bring Ezekiel to a sort of a triple of circuit um which is expressed in the setup uh approver artifact which is either a was improver or a binary approver and a verifier artifact which in our case will be an on-chain uh EDM verifier and you have to deploy that verifier and sort of Route it into the uh route the the update into it and integrate the prover either on the server or the client um and then you launch your game and these are just our telegram group and our GitHub you check out um and I think with that I will pass it to Dante uh to show you how it works well all right um I see there's a few questions in the chat I don't know if we want to do those now or later on um there's one question which is is the output a regression result or is it limited to classification results you can do both I'm going to sort of demonstrate the computational graphs that you can build but you're not really limited um to either regression or classification um there's another one which is can we use Ezekiel for approvable training as well as inference uh we currently only support inference meaning um yeah training is a lot more computationally expensive um and hopefully at some point we'll support it but currently we're just um working on uh yeah with inference uh someone asked you to go back to the slide with the GitHub and Discord I just put it into the chat if you just search for easy KL um I think we'll still find it all right awesome right well I am going to Showcase uh what all of this looks like in practice basically so how do you there's actually quite a few we've developed a lot of tooling to make it easy for you to use those um so the main library is written in rough I know it's a challenging language um so we've sort of wrapped it in a python we you can compile it to log in if you're running a browser application so you can run proofs and verification straight in the browser but today um kind of as a as a neat sandbox I'm going to be running you through like how you might use Ezekiel to like uh generate proofs inside of a Jupiter notebook or something just so you have like a nightstand box to uh yeah it's just kind of a nice sandbox to try out different models and see how they perform um yeah let me share my screen how all right so um yeah so this is kind of a pretty typical setup for starting off an Ezekiel project um so what's kind of cool is that you can actually just Define your circuit or you can think about your circuit just as a pie torch sort of computational graph that you're going to start typically by defining like a wrapper around and then module which is really just like uh you know how you might Define a neural network in Python more broadly so how are we going to do this all right let me create some sort of model what's interesting is that I have copilot on so sometimes let's adjust layers um as I'm typing so oh yeah there we go let's see what it comes up with uh kind of interesting to see what the AI might be interested in new building all right so we're gonna Define okay let's start simple all right let's start simple and let's just have a single column layer and then a value non-linearity um as part of our model um and our model is just going to be applying oh look at that oh man this stuff is amazing so our model is just going to be applying a comp layer and then subsequently applying a really non-linearity uh onto some data let's say that data is like feminist shaped or something um which is uh like images with a single Channel wow and yeah 28 by 28 images um and we need to instantiate the model once we're not going to train it um because we're sort of limited by time and I also don't want to download a data set um but all you need to do now is you can just export this and we'll get out locally um so let's run that so make sure everything works yep all right so we've exported the model um and the format that we use is something called Onyx um the details of this don't really matter but we've just saved this model locally um and what we need to do as Jason described like we use a slightly different representation for numbers so we're not really working in floating Point space we're working with field arithmetic so what we need to do now is quantize the model so basically fit everything to into that field and make sure everything runs correctly um and this is done with the forward method on the easy scale Library there you go so we've quantized the model and now for a ZK proof you know we need an SRS string which this corresponds to like the parameters that we're going to be using uh to generate like the verifier key The Proven key and subsequently it proves we also provide a method to do this um to make it really easy for you to use this takes a little bit longer to run but hopefully not too long and yeah there we go um and then we have What's called the setup phase which is where we generate the verifier key and the proven key so the proving key in particular is important for you know it's in the name it's important for generating proofs and the verifier key is important for the uh verifier to then subsequently verify the proofs so this is all done in the same step and then it can be distributed to the two different parties that need to you know either prove or verify a proof generated from the specific circuit can we run that all right and nice all right so that ran and then generate a proof using a simple method by providing like the paths of all the things that you need and then you can verify it and voila that's kind of it we also provide a bunch of methods for you to I don't have my ledger with me right now but if you plug your Ledger in uh you can basically submit proofs on chain you can deploy uh verifier contracts if you need so we provide tooling to do all of that internally to the library we've made we've tried to make it as easy as possible to like generate circuits uh you know prove verify generate on-chain verifiers and then submit proofs to those verifiers but let's go back up and start generating something slightly crazier here um okay what do we want to add let's say we want to double X and then subtract it just for you know just for fun and then we want to take it to the power squared all right let's see if this works all right let's run through those cells again but basically the idea is that you're not really constrained by anything when you're generating these circuits we support a whole bunch of methods so if you want to generate something really crazy you can Chuck it into a pytorch model um and then start running proofs on it um so you haven't generated the setup phase again and they're proven and there you have it verify it again all right let's see if we can chuck a second conf in and make this a little bit spicier but uh um all right let's add that into here oh wow um and then maybe yeah all right that sounds good but yeah in terms of circuit size um if you're wanting to prove something you know in less than 10 seconds um we've gotten like six five or six later comps uh yeah we've gotten five or six layer comps to prove in less than 10 seconds so you can go pretty crazy in terms of parameter size um yeah all right let's see if this one verifies I kind of want to see if I can break it at this point like see how much we can throw it in all right sweet all right that's still verifying oh my God I don't know if we're gonna be able to get this to stop working today um yeah I see there's a few questions in the chat and we tried using Mojo with vkml I'm actually not too familiar with Mojo um but yeah are there any questions on like the specific demo as to like how to use this I'm a bit confused what the demo was trying to prove well basically yeah okay so what is the demo trying to prove could you explain all right yeah so if you if you were to try and build ZK circuits on your own um to generate proofs of like better zero knowledge machine learning model was run correctly you would basically either have to go into circom or Halo 2 and start coding up you know like a gate for matrix multiplication or a value for a convolutional layer all those sorts of things um and what we've done is we've started to abstract away a lot of that complexity um and what does that mean so it means that like instead of like having to go into those languages like circom and those sorts of things to generate like you know proofs or a circuit that represents a specific machine learning model you can just use the Frameworks that you are probably familiar with if you're a machine learning practitioner meaning that you can just use is uh meaning that you can just use things like Pi torch or those sorts of things to generate a model we take that model and then compile it into a circuit for you so you don't even have to think about it oh you meant what is the ZK proof proving all right yeah good question so currently we're just proving that the model executed correctly so given that you know a specific given that there's a public model and there might be some public data you're proving that you executed it correctly and then you can submit that proof as call data to the chain but you can generate a whole host of other proofs like for example you could keep the parameters of the model private and run it on a set of public inputs you could keep a set of private inputs but run a publicly known model on those and then generate a proof that you ran it correctly and then submit that proof on chain basically so we allow all those sorts of different scenarios to to be realized what model am I using currently um well I just ran through it um I'm sort of generating these models on the fly so you can really generate kind of anything um sort of anywhere can the code be shared uh yeah we actually have a Jupiter notebook demo straight into the um yeah straight in our repo so if you want to play around with this this is all available inside the GitHub repo um but yeah definitely definitely already available someone's asking someone's saying they're not familiar with python is it possible to have a similar tutorial with JavaScript um eventually So currently we are so on the JavaScript side but we're really focusing on at the moment is getting uh approvers and verifiers in wasm so that you can run these things with relatively good performance um and yeah create in the browser so you can generate proofs for a specific model or verify proofs for a specific model straight into a browser if you have interest in getting this stuff into JavaScript um definitely yeah open up an issue and we'll take a look is there some mud integration allowing game clients accepting events from the indexer So currently we are not fully integrated with mud but that's definitely something we're moving towards so we're aiming to like integrate as much as possible with them to make like using our tooling uh within autonomous worlds as quickly as possible and ml a lot of people talk about keeping input data private I'd imagine it could be interesting to also get the output data private in that scenario also could you go through how a proof like that would be set up um yeah so what's pretty interesting is that the so the proof command uh down here you can actually pass you can actually say what parts of the model you want to keep private and public um and all you so you can actually just pass a flag that says I want my public or I want my parameters for example of my model to be private I want the output of the model to be private or I want the input of the model to be private so we allow all of that to be configured from start to finish so if that's something you're interested in we can already do that we should also probably show something from uh evm verify since that's all into this yeah for sure but I actually don't know if we have that in uh I I don't know if we have that into the python bindings yet um I can do the docs or something yes I yeah let me show that I don't know if I'm successfully sharing are you sharing um but if you go to the doc site there's a section on verifying on chain um which shows you have to generate a proof with the with the transcript set to evm um and then you can generate a verifier which emits code um deployment code uh specific to visit the command line specific to your model and solidity code and then you can test it it verifies um by using a verify evm and that just is running the code that you generated just before it and then you can deploy that code to [Music] um to a chain um and then you know using using you would need to assign it and you can also send purse or you can send those posts manually or deploy.com manually um I just wanted highlight that it is possible to create a verifier so a very common use case would be something like uh you have a state update you want to check that the state update is valid before you apply it and so you would uh create that verifier and use use a verifier called from your main smart contract in order to check that the update was correct foreign I think that answers another question in there which is are we making function calls to the verified contract uh yes so there's a method called verify EDM where you can just pass a verifier contract address and then submit your proof there um but yeah we also have a bunch of methods for deploying the verifier contracts so you don't have to think about that either which is what Jason was just showcasing should work yeah I would love to see that if that's something that you uh you end up doing yeah our hot dog detector or whatever this kind of stuff um some people have done uh using this this Doodles thing that quick dude the Google Quick Draw program for example um where you have to draw a picture of the snowflake or horse or whatever um that should also work foreign any more questions yeah if anybody has any other questions feel free to type in the chat or take yourself off mute very curious to know whether you have a complex use case that you've imagined that you can share with us it's another one I guess there so there's two ways to answer that I guess there's complexity in terms of the models that we're trying to get in um so most definitely we're we're aiming to get a Transformer based models and at the moment so in that sense for sure um and I guess we're also trying to like at the verifier contract since a larger gaps and stuff um so start composing complexity within those sorts of applications yeah there's a lot of interesting I guess autonomous worlds yeah yeah in autonomous world people thinking about you know for example having a world physics or Evolution function being something that is could be player owned or you know could be like the player commits to an evolution function and then is responsible for maintaining their planet or you know NPCs with with serious AI um you know powered by Transformers and whatever else um that can even you know move between games stuff like that so that that kind of thing should should eventually unlock it so um certainly the state upstates and really really basic AIS but more sophisticated AIS would be really fun yeah and I guess for autonomous worlds people specifically have been using it as like not as ml but as like a sort of physics engine which is what um yeah with Jason's literature showcased earlier which is like all the state transitions between things um yeah all right this can be huge for verifying physical objects and useful and not a lot of things as well yes definitely not just strawberries and hot dogs are a nice places they're relatively light that's true I mean I'd be curious to see the strawberry verifier so if you want to whip that up um definitely message us and we'll help you get the strawberry verification of the wine um but yes I guess one more thing I wanted to address was the JavaScript question just to go back to that you can actually so there's a bunch of ml libraries in JavaScript which will export to Onyx which is the neural network representation we use it's a sort of universal format so I think tensorflow JavaScript can save to Onyx and then you can point our CLI so we also have CLI chlorine to that on X-File to then replicate the flow that we just showcased so you're not limited to python basically any any framework that can generate onyx um yeah as as useful basically awesome well cheers thank you all very much for uh this presentation and thank you all everybody else for attending um this will be um the recording will be shared to you all and so yeah if you have any questions please feel free to reach out uh to Dante and Jason on on the Discord channels and uh yeah so thank you all for the for this we have one more session later and then the team formation will be at the last session of the day so cheers thank y'all thank you 