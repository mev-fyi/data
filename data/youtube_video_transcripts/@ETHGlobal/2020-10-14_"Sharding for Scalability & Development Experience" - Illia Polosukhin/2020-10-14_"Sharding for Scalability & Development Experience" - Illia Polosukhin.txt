on stage ilia from near protocol he has just um finished the the neo protocol guys have just finished their hack the rainbow hackathon that was a huge success um but elia is going to chat to us about sharding for scalability and development experience ilio can you hear us yeah thanks simona for introduction uh well it's great to be here and welcome everyone i hope your hacking has been starting well um yeah let me let me kind of talk through some of the sharding and uh i'm sure with all the news around these two there's a lot of questions uh so i'll try to kind of cover some of the topics and uh surface some interesting questions but if you have anything any other questions that i didn't cover please uh bring them in the chat and we'll respond at the end so first i wanted to just mention like uh near itself so nir is a pro stake layer one that's specifically built with experience in mind so which covers user experience and developer experience and scaling if you think of it as part of this kind of experience because as your users as your uh like user base grows as your application gets more usage you should not be dealing with you know scaling your application and also your users should not be dealing with you know high fees and figuring out you know what to do with all that right like this stuff should be completely transparent and just work and so kind of the general idea right is to build the experience that matches kind of the web 2 right the existing internet and in such a way that you know you can bring in non-crypto people and they can just use applications and it's been pretty important to obviously interoperate with ethereum because this is where kind of most of the existing developers and users are but we also want of it to onboard lots of developers who are coming new so let's talk about scalability so uh let's start with how you cannot scale right and so there's been a lot of kind of uh new consensus algorithms where people would be you know providing an idea how to scale the uh how to scale the blockchains with you know some amazing consensus algorithm and you know being able to do stuff stuff that is uh faster there's been a lot of also like you know changing from blocks to trees from to dags to some other data structures which are you know presumably allowed to uh to do something faster or switching to different proof off right um and to be clear obviously proof of work it has inherent like limitation and that limitation actually is in the fact that you first need to organize a block and then you need to mine it and that mining time is actually adds on top of your kind of execution time and then actually that is possible to pipeline so in theory it it's possible to like optimize that and actually uh have throughput faster uh with work but it's still not the bottleneck the actual bottleneck is not data structures it's not consensus it's not a proof of it's actually just executing and the reality is we we write we want to run a smart contract platform right we want to run generic code code and this code needs time to execute and so this is actually just syncing the uh helium blockchain with fully downloaded right snapshot of all the blocks and so there's no network there's no consensus there's no you know data structures it's just purely evm execution and as you see there's a limit how much gas you can push through in in a second and so evm itself uh was unlike a kind of um somewhat default hardware right uh maxes out at like 200 300 transactions per second of like normal smart contracts this is like real real smart contracts on mainnet and uh this is the bottleneck right like it doesn't matter if you switch to you know proof of something right like proof of authority it will be the same way uh it doesn't matter if you have like uh you know the magical pipelining and stuff like this so the only other option like the way to to scale right is either significantly optimize a transaction processing so so this way is includes you know changing from evm to another uh kind of execution environment right so web assembly is faster but has its own challenges there is a kind of berkeley network the berkeley system that uh solana for example uses which is faster you can put execution on for example gpus and stuff like this the problem is all that really works if you have a homogeneous load because uh if you only need to do payments for example you can scale you know pretty dramatically because you know you can parallelize that you can have like lots of uh processing power like on one machine going going really quickly and doing things in parallel but at the end actually one of the core limitations is not an interpreter it's actually uh hard drives right just being able to randomly access hard drives uh is hard so and again like you can put more hard drives but like there's limit and so the other option is just split the work right i mean that's an option that the whole internet is using right google is not running on one machine facebook is not running on one machine right there's lots of machines running in parallel processing things in parallel and so this is what what's called sharding right uh i mean this is not a new concept right it's been around for you know probably two decades if not more and the idea like we need we need to scale up like we cannot have that for pretty much everybody in the network doing exactly the same work they need to paralyze the problem is this is very hard and this is hard because there's a lot of kind of technical challenges on how to make sure that everybody's doing the right thing how to make sure that the chain is correct when you don't actually run all of those things so i'll just cover kind of like a little bit of of a difference but uh between kind of default sharing approach and also what we've been working on but so the general idea right is you paralyze the work which means you kind of need to do a bunch of stuff in parallel which the kind of naive approach or like the approach everybody starts to take including us is to paralyze the chains itself rather multiple chains in parallel and then in some way to kind of link them together and and do this um like use this as a way to communicate between them and so this approach that you know originally uh like heath 2 took its approach originally we took its approach that polka dot implemented this approach that kind of uh if you think of cosmos as a sharded chain which it's not and it's a little bit different but like in general the concept is the same uh it's by scaling by a lot of chains and but like one of the core issues with this uh and we'll cover like a lot of other uh interesting interesting issues it's just like it's inherently complex and has a lot of kind of uh repercussions and so the approach we're taking is really paralyzing the blocks and i also draw some parallels with with other approaches as well so what are the kind of technical challenges with sharding well one of the big challenges is if you think of any proof of stake system usually if you have a one-third malicious and network split you can actually take over its network right so either stall it if you have one third plus one or of stake or if you have a uh like network control you can actually do some uh like fork of the network and one sort of stake you know usually like you can you know you can incentivize growing the stake uh it's still sadly in current proof stake systems we still have a pretty big concentration but uh hopefully over time this will grow um and you also need to kind of track like yes you're pretty much saying that because we assume that there is less than one-third of uh malicious actors been on the network then uh pretty much the whole like by induction right the whole chain is valid because like every other block is is every next set of validators is selected by the others that were correct um and you need to actually like have some some intersection between when you when the validators are rotating because they need to actually download the state so that's how the this works uh the sorry the problem is that if you if you think when when we're doing sharding right what happens is that in each chart you have uh you need that assumption of functioning malicious and so this leads to a problem that if you sample uh kind of for a long term right we just sampled it and just run with that uh this validators can get corrupted right all right and and sampling you know like let's say you have you know 10 shards 100 shards sampling into that will uh will pretty much lead to like the probability if only if less than one third like if one third or like even 20 is malicious like sampling into one of the shards it can be higher but also what's more important is that uh people can get adaptively corrupted right over time so you need to actually rotate validators across across the stack and make sure that they are pretty much cannot um like cannot know which chart they're validating too much in advance so that's where like this thing come in is like you pretty much wrote any validators uh but you need to give them some heads up to to download the state of the next shot because compared to a normal like single chain right where all the nodes have the full state of the whole network now you only have a partial state and if you are responsible for validating next chart you need to go and download the state of that chart before you are able to validate otherwise you'll be lagging and uh and i'll mention data availability in a second so the other problem is what happens if somebody actually did attack right the let's say there is a more than one third like two thirds of uh malicious validators in one of the shards because like in general the numbers are smaller of stake and uh the problem is that the other shard cannot detect this uh very easily right it's not validating the chart it's not actually validating state transition it only kind of keeps track of that chart so uh and like the kind of attack would be to send let's say invalid state like for example say well all the money are mine on this chart and then send them to another shark to exchange and exit right and so maybe like network like everybody will detect this but it will be too late and people already stole the money so the kind of the general approach to this is called fisherman and this is where uh somebody can pretty much tell the other shards that hey something is wrong in this chart and they provide a fraud proof right and so this is similar to like what we've just heard in uh kind of plasma's approach as well where if something is wrong on a plasma right then somebody can provide a uh fraud proof to the helium and and uh exit pretty much from this and so the the problem here is and similar to plasma is data availability right so you need to somebody needs to actually have a uh fraud proof generated in the first place to be able to uh to prove that something went wrong right and so this malicious actors did not produce a block right if they did not post the block to the network right then this uh let's say honest actor in that chart has no way to really uh create a fraud proof and prove it to the other chain the other side is obviously finality right so like if you send a kershaw transaction and there's a fisherman right the fisherman needs some time right so in case of plasma it's seven days big part of it obviously is because of gas issues and you know transaction sensoring and bunch of other stuff but this change period needs to exist i mean it can be shorter but like this is the finality of this crochet transaction right we cannot execute it until we make sure that there's no proof coming in and there's no like no no like there's no way to not prove there's not going to be a fraud proof right so there's no way to like speed it up and so this creates a lot of issues on like just kind of creating a fast cross-share transactions between chains uh the other thing is like if if you for example do speed up then you need to actually like roll back a bunch of stuff right you need to fork off like if you allow transaction to to go in and you just kind of revert it you need to revert like a kind of a cascading effect of all the shards that get affected and this also can be very complicated and then the data availability part is what i mentioned right if validators did not post invalid block right there's no way to prove that's that uh that is an invalid state transition and like next validators cannot do anything about it um so so important is obviously to have a mechanism to provide it available to the network and so one of the main solutions that pretty much everybody uses uh so it's us polkadot and other folks is to pretty much use the ratio codes and so this idea that when when somebody produces a block or in our case a part of the blog they need to send out parts right so they need to send out the razor coded parts of this block to other participants and only if those other participants saw those parts they will include this block as a valid like they don't need to validate it they don't need to uh make sure the state transition is correct they just need to know that there's data availability so so what you do you weave in into consensus right you actually include this into consensus that you can only accept uh those things that you saw parts of and if you have enough of these parts so enough people are tested to having these parts then it means that actually from these people and that you will be able to reconstruct this uh this block it's the that that was transmitted right so even the original val original block producers or in our case chunk producers disappear uh you'll be able to reconstruct just from these parts the full information and so just just for context razer coded means that it's a special encoding that even if you only have um like in this case you have six parts right to split the data into six parts and any two parts is enough to reconstruct the original data right so it's kind of a 3x duplication of information that's like any of the two parts is sufficient and so this mechanism is kind of at the core of like our approach and polka dot as well uses this uh to pretty much uh make sure the data is available across across the network and so i mentioned before right so we transitioned from this idea of having multiple chains running in parallel to having actually one chain and sharding blocks right so and so this actually allowed us to do a lot of things including because we have now one blockchain uh this allows to really uh simplify the whole challenging things right because if there is a issue that have happened at some block right we can just roll back the whole chain uh with it like that would uh that contains all the malicious situation right so we don't need to like find you know a bunch of chains and figure out where where they went wrong and roll roll them back you just have this kind of uh one finality for all of them and so this provides kind of a better experience it's also just easier to implement like that that'd been one of the core uh core things because obviously like this things this stuff is very complicated and it's very hard to test so uh the other thing is obviously it provides a better crosstalk transaction uh functionality and so we can move into this i think like a few things important to mention here which um like if you think of it what draw-ups are and what plasmas are in a way is kind of a specific versions of like of this as well right it's it's all a way to run some kind of computation outside and then settle into the main chain where you main chain is providing data availability and it's providing the kind of finality security right the challenging system right so in in in this case like in our case we're using kind of the our like food chain to do this but you can imagine this like food chain would be fidium and then all the shards are just running as a parallel chains and that's kind of what like plasmas and optimistic roll-ups are and then like they have all the same problems that i mentioned right they have a seven-day challenging periods to to do this if it's optimistic roll-ups for zk roll-ups right they can actually prove the full stage transition so they don't have this problem and uh this is kind of how it all links right in in reality all these methods really just uh like different ways of handling this problem so that as i mentioned now the important piece is how to communicate between right so if you have a multi-chain environment or multi-shard environment or like multi-role up environment right uh it actually becomes really complicated how to how to uh connect them together right and specifically there's a lot of kind of issues around drawbacks right so if the other chain rolled back what do you do right uh like you know if you feed him roll back if you are optimistic roll up you actually need to roll back as well right because you're tracking hedium uh if you're a separate chain and there's a rollback uh like what do you do right and there's gas cost issues right it's because like if you're trying to send a message from one chain to another chain if the gas is different there it's in a different token right uh like who's gonna pay for it how are you gonna calculate when you sending transactions that you pre-paid the right amounts and stuff like this right you know i need to attach new like another token or something else um the critical state recovery is is is around roll-ups is around uh rollbacks right if something happened right if if a roll-up producer stopped right uh what do you do like like it's a mass exit it's a kind of you know like you roll back some stuff you prove things you make sure that you have all the data availability but like what happens with all the transactions that are in flight right what happens with all the cross chart like you were calling from here to here like what happened with them and it's important to figure out like will they like what's the order of execution that will happen on the other side if that happens right it's like optimistic roll up but producer stopped everybody's mass exiting but you actually had a proper exits as well and they'll not get executed at the proper time because they're all like waiting for seven day it's like what what's going to be happening in in all those cases um and then the important part is composibility right if you have like two rollups or two shards or two chains right if uh the communication between them is complicated it has gas it has all those like seven day waiting periods uh or even like a minute waiting periods like composing contracts composing applications becomes super complicated and especially if you want to also receive a call back right and and hear back about what happened in zelda sharp so with near like was near approach right we have pretty much been focusing on fixing all those issues and making the experience really really uh like really smooth on how this works and uh think of it right because we have one blockchain like from a developer perspective you actually don't need to think about uh kind of shards and sharding in general like if you go to our explorer right now you actually will not see we're running one chart we have another test network with four shards but you'll not see any difference because the sharding is completely hidden from the developer and the kind of main thing that we propagating is that all the contracts working asynchronously so the communication between them will not happen within one atomic transaction it will happen with the next block right so you're pretty much creating a message and you're sending it to another contract and another contract will execute it in the next block and this is similar to like message passing in erlang or any other like similar language and first of all it removes any kind of reentrancy problems uh second if you know if there's any rollbacks this can be handled cleanly because all those receipts are kind of staying in the block and then like you're all back you restart it and you have all these receipts just coming in and you need to process them uh they are mandatory to process before any other transactions coming in that are like all the receipts uh like these messages passed from one contract to another uh kind of coming in in the beginning of the of the next block um it solves those gas costs because we kind of uh pretty much like one of the core ideas is uh dynamically balancing contracts between shards and so this would allow to kind of even out the gas costs over time so it will not be like immediate uh like you know within it was in a like few hours you may have spikes but like over long term right if there's some contracts that are more used than others it would pretty much even them out and split them between different charts evenly maintaining kind of as a balance um but apparently it kind of has this compassibility property right because now you can realize that the contract will be delivered right and you'll get a call back and so uh and like the only thing that you need to handle is the fact that it's not atomic right so so you do need to handle kind of state reversals somewhat manually uh there's like few tooling tools to do that on developer side but you do need to kind of handle like a failure of the execution of the sales contract the benefit is now you can handle the failure of execution of such contracts which you actually couldn't on ethereum so you can actually call contracts see if it fails or not and execute some logic based on that uh so think of it try catch and yeah so this callbacks just kind of allows you to like attach obviously near as a resource it allows you to attach information and receive it so here's an example how this works uh so you pretty much this is in rust for web assembly contracts you can create you can call is white listed based on some interface uh you provided the kind of contract that you want to call some arguments no deposit in this case and somewhat of gas and then you have a way to attach a callback to this execution on on one of the methods on the contract itself so pretty much we'll call it it'll execute something and then return and actually that contract can call something else and then process something get call back and get callback right so you can actually have like a pretty long chain of of of calls uh that processing uh now so this is actually like this is very useful you can actually have like a very interesting like multi-shard contracts now built on across this you can also shard your existing contract so for the users like for example for a token i actually have some proposals for sharded tokens because tokens actually don't need to have all the accounts in one chart right you can actually share them as well and there's like few other applications that kind of need the scale of multiple shards so going beyond even one uh one chart but uh there's also a lot of current like the the like the d5 specifically right has some issues with with async calls and like this this will be very clear because one of the kind of benefits of of d5 and like i usually call d5 is just a like a programmable exchange right so if you think of it you deposit money into exchange right and it gives you a bunch of tools i'm talking about like nasdaq or you know one of those exchanges they have a bunch of tools already pre-built on that exchange you can trade you can do stuff and within within that exchange right everything is atomic everything very fast nobody can frontrun you hopefully and and then you can go to another exchange right so so and so what the helium was d5 provides in a way is ability to deploy new contracts new code into this one kind of uh one exchange one place and so this is provided by like ability to execute kind of atomic transactions atomic across multiple calls so it it does become complicated to do that for async calls and one of the main issues is that um because if you think of it like if you have two contracts that have a sync and you have some kind of logic flying between them people can start try to front run you to go after this contract earlier on right and so there's ways to mitigate that in this kind of multi async environment but obviously just like as as the system grows right as you have more and more pieces working together this becomes more complicated so there is a value in sync calls and so we are actually uh just launching the edm uh what we call the volts so in a way it's like a one shard of a zvm and i mean it can obviously scale with you know but goes up to one chart and within that evm uh we have sync synchronous calls right so within that vm it behaves like ethereum so you can think of it as just launching helium shards inside mirror actually originally it was done as a just a smart contract so we literally compiled the vm code and just launched it as a smart contract on near and had like original initial testing but now we pre-compiled it and kind of speed it up so that gives us like you know whatever the raw performance you can get from the from the avm you get that the interesting thing is you can have many of them so you can have multiple evms in parallel running and have async calls between them so you can think of it like can have multiple exchanges or some other contracts maybe that are not exchanges that are dealing with like you know in-game items or whatever they don't care about like specific defy stuff uh and so you can still async communicate with them within one block so you have like all the same functionality and you can communicate with all other contracts and in a way this actually works as like you can call it realistic roll-ups right because what really what optimistic roll-ups is is you execute something off-chain and then you post it on chain as a data and then you don't execute it but then you wait for anybody like up to like pretty much optimistically it's there but then somebody wants to challenge you need to wait for period here you actually execute the state transition vpn right away and it's like cloud right away right so like i call it realistic roll-ups i mean even even better was if there's evm that's running like off-chain and they just post them all post all the data and gets valued on chain but because we have one second blocks you don't even need to do that it pretty much gets the same performance so like each vm vault will be like as fast as like optimism or any other uh kind of evm engine running as a separate thing but it will work within this whole environment and this like kind of going forward there's some ideas how to scale them in this up but this is kind of future work uh so before kind of ending i have one one last thing i want to mention so simone mentioned about our rainbow hackathon and so this idea is like in a way um like from perspective of ephedium near is you know is kind of a small charter environment in which you can run contracts and with evms it's like many vms and there's no kind of like with this bridge we're actually connecting those two environments and from the ethereum side evm is uh from ethereum side near as a side chain right it was made or like many side chains connected together with zvm and this web assembly from near side evm is ethereum is just another shard right because you can actually call into it and get responses as callbacks and kind of have all the same uh functionality just with a little bit of lag on finality and also timing and so this is pretty much facilitated by like a light client smart contracts on both sides uh that are providing you pretty much this trustless decentralized way of connecting to chains and uh and it's done very in very generic way where you have like multiple uh like levels of smart contracts that pretty much provide connectivity and as a developer you can link in any way and prove any fact about zaza chain uh just to finish up uh we like one of the main things we've been do we've been doing is really focusing on education as a space so we actually have two series one on youtube and one as a podcast uh one on youtube is called whiteboard series and it's about uh kind of talking about protocols and talking about uh kind of how they work on whiteboard in in depth it has you know ethereum2 cosmos polkadot optimismatic and and i think there's like over 40 protocols now and then the uh the open web collective is buildings open web podcast is a podcast about entrepreneurs and builders and and investors actually coming together and sharing sharing information and sharing kind of insights on how to build businesses and how to build in this open web so keep building join us at near.chat i mean if if you are interested in kind of expanding your business we have a protocol agnostic open web collective which is really a incubator and accelerator for uh for companies in open web and uh if you just want to learn more about nir there's open chat and i'm open for questions awesome thank you so much ilya i think my favorite piece was realistic roll ups versus optimistic 