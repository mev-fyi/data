good morning good afternoon and good evening everybody welcome to the optimism future of uh future of ethereum summit so we're really excited uh to be going live here with so many awesome talks about the optimism ecosystem optimism is a huge partner of youth globals over the last couple of years and we're really excited to be working so closely with them to celebrate uh an important part of the optimism journey um as well as chat about uh the newest things are coming down the pipeline for uh the scaling solution known as optimism so thank you everybody for being here my name is jacob i'm part of the youth global team i'm also an optimism delegate um and i'm also going to be joined today um as i'm seeing by katie um he'll be joining in just a second um thank you everybody for coming online on our youth.org global.tv platform um this is a really nice way to interact with everybody um feel free to log in and join the conversation ask any questions as people are going through their talks and we will relay those to the different speakers um and of course feel free to enjoy some low-fi beats uh during any of the breaks uh throughout the week or throughout the session so as i mentioned katie's joining me as uh as one of the emcees today um katie works for the udhc team and also is an optimism delegate so we're excited to have uh representation on the optimism dow side uh here as well and together we'll be popping in today uh to go through the different sessions so i just wanted to give a quick little overview of today's schedule um so we have carl joining us in just a minute uh to talk about the modular optimism stack openstack uh we have joshua to do an introduction to architecture optimism as well as chat a little bit about bedrock the next upgrade we have norswap joining us at one o'clock p.m eastern to talk about modular sequencing followed by proto-lambda joining us for pluggable data availability at 1 30. um after that we have a really cool panel at 2 pm eastern on the state of uh eip 4844 joined by tim biko terenchao mofi and roberto ballardo of the coinbase team and finally a cool uh overview of one of the major products that is completely built on optimism uh which is quicks their nft marketplace so mark dawson the founder of quix is going to come and chat at 2 45 about what they're working on there so without further ado uh would love to welcome carl onto the stage so carl whenever you're ready feel free to unmute to share your screen carl is uh listed on this chat as carl with many opt op with many exclamation points so you know he's excited there you go welcome you already know how could i not be oh my goodness let me make sure that i'm okay i got all the things you're all set i'm all set we did it read it there we go everything looks good all right well i'll get out of the way and excited to hear more about the opie stack okey dokey so welcome everybody going to talk about the op stack bedrock and the rise of the super chain and we actually got a solid amount of time i didn't realize we had so much time but i gotta fill up all the time so without further ado i'm carl of course i work at op labs um doing this optimism stuff and so now whenever i give this or whenever i talk about these things right these words pop up on the screen there's one word that everyone sees they don't see the rest of the words they see super chain that is what they see realistically so i'm not going to start off by talking about the super chain i'm not going to talk about how it is you know all these chains kind of combined into one super duper fantastic thing i'm not going to talk about that to begin with instead what i'm going to talk about is the problem ethereum congestion back in 2016 2015 this has been a problem for way too long oh my god it's been a it's been a long it's been a long many years um but we've had too much demand for ethereum ethereum is too good and so what happened how did we solve ethereum congestion realistically speaking well we copy and pasted it into more chains all these various chains and generated by dolly too that is what we did that is what we did and then what did we do well we connected these chains with largely insecure bridges these largely insecure chains realistically speaking and we gotta attack some together to address this horrible problem of congestion and in some sense optimism is kind of one of these things right it's it is a layer two but from a user experience perspective it still has a bridge it still you know feels a little bit distinct but it does have the nice quality of being rooted in ethereum and in sharing some of that security and so right this is kind of the state of affairs in some sense and you know if i'm a user this like cute little little gerbil also generated all these most of these photos are generated actually um then you're you know going from one chain to another trying to like deposit withdrawal it's like just it's just kind of it's just kind of chaos to be honest with you and you can kind of visualize this as a crazy like market right it's like a marketplace and you're going from these different sellers and you know maybe you go to this person you're transacting on this chain using this vendor and and behind you there's a bridge that just exploded and another one that just exploded and then you went to this other chain and and now you know oh they actually it's actually a rug pull like it is true and utter chaos right now this is not good this is not a user experience worth bragging about right this is just honestly very sad um but you know it's better than no one being able to transact which is the alternative because ethereum was just way too congested but we can do better we can do better and what if i told you what if i told you that there was a solution to this problem there was a solution where instead of going from one place to another everything is all in one place and you don't have to transact with all these different you know vendors you can instead trust you know transact with one trusted seller one trusted seller and you've got seamless atomic checkout in this market well it's the supermarket whoa oh my gosh okay this is a joke right we're not here for supermarket technology um but it's just you know super market a super chain um anyway really though really though i you know i can't help myself the j our chain should have these properties right we demand these properties that you know everything in one place is single trusted sellers you're not getting hacked all the time right we shouldn't have to bridge from one chain to another we shouldn't have to get hacked i mean my goodness that's a low bar um and also transaction fees should be cheap and sometimes pretty much free like that is legitimately it is not there is no technical blocker and so there is no reason why we should not achieve this don't fear though don't fear the op stack is here all right we finally got to the actual meet of the talk to an extent well state of affairs kind of crazy absolute madness but we can fix this how do we fix this well we can build standardized open source libraries that provide shared security so things are actually secured rooted in ethereum there are fallback mechanisms there are things that prevent terrible hacks also just the fact that it's standardized makes it way more secure because now there are way more eyes on it it's not just a one-off solution because that's how you get hacks additionally you know composable right we need these systems to all work together you can swap out one module they're compatible with one another and additionally finally coordinated sequencing i won't talk about how incredibly that important that is but it is actually incredibly important for creating an experience now what is this experience that the user you know like at the end of the day right all of this technology is in service of building something that we can all use right i want to use a better product than we have today and so what is the user experience of this system well you actually with the compatibility that this you know the shared technology enables we can actually create a virtualized single super chain right we can actually interact with the system in a way where you're not having a bunch of user-facing bridging all that kind of stuff can hand be handled in the background right we can have robust modules that's insecure that ensure security and additionally it should be hella scalable with horizontally scalable some call it quote infinitely scalable but i feel like that's just like a savage marketing term horizontally scalable so we can have near free transactions transactions that essentially cost the amount of money that it costs to run that transaction on a computer so it's like renting from aws as opposed to paying surge pricing on uber so we can realize this super chain we can realize it with open source technology shared standards and working together and we can make that happy experience for our happy gerbil now if we want to talk about how exactly the kind of like combination the virtualization of the super chain works you can kind of take a look at end game that has a bit of a hint at it and then there's even more talks that we can go into but i won't do it right now because we don't have enough time or at least so i thought but you can ask questions at the end um of the presentation so now we are past the intro maybe i needed all the time that we need that we have let's talk about the core of the op stack so this is not necessarily everything but this is the kind of bread and butter okay what is the ob stack again standardized open source modules for layer 2 chains we are using it for of course optimism mainnet but it is open source therefore anyone can mess around now there are three key layers to this right any stack has some layers the consensus layer the execution layer and the settlement layer consensus that's how the chain gets the transaction the chain gets constructed the execution that's the virtual machine that is actually being you know executed over this chain of inputs and finally the settlement layer that's how withdrawals occur so if you look into layer one if you look closely with a magnifying glass you will see that layer one is actually made up of its own consensus layer and execution layer so this is a recent change that we've modularized layer one in the very same way that we're trying to modularize the layer two technology and the way that that works is that we have prism these consensus layer clients not just prism don't just use prism y'all we need diversity additionally we have execution layer clients like geth and we have that same structure playing out in layer two as well now let's get into the nitty-gritty of the consensus layer execution layer and settlement layer what are these different things so first for the consensus layer we have two key modules two options that we can choose from we can either choose roll up which basically means that all the transaction data is fully available on chain or plasma now plasma got a bad rap back in the days just too early and now there's a million different terms for plasma like valladium valydium all of these things that various things basically at the end of the day we should just call them data availability challenges um we have a consensus layer client that and has off chain data with data availability challenges on chain so those are our two kind of like broad categories of consensus layer type deals additionally we've got execution layers so that is the evm the classic one right we've also got x extra fancy ones like this weird one in the middle that i'm kind of just like hinting at but not saying because i don't know if i should talk about it yet but anyway it's we've got other execution clients that are special purpose maybe for some games maybe not you know whatever um and then finally we have the settlement layer this is how things actually get withdrawn from the system this is how you kind of prove the withdrawal now notably i have here multisig fault and zk proof i am writing this as though the settlement layer is if you have a fault proof or a zk proof that is you know kind of working but you still have a fallback to a multi-sig then because of lowest you know the the the weakest link security at the end of the day you're still multi-sig and so that's why multisig is here obviously no one wants multi-sig to persist into the future but the reality of layer two right now is that things are being secured withdrawals are being secured by the multi-sigs which is you know it is it's better than it being secured by an insecure proof system so let's let's be thankful um anyway consensus layer execution layer and settlement layer right you can choose each one of these and you know if you choose this configuration then you'll end up with an evm equivalent optimistic roll-up right this is the perfect flagship for for an l2 but it's not necessarily the only configuration you might ever want and it does have different security trade-offs or withdrawal trade-offs right all of these things are trade-offs now additionally right eventually you'd want to plug in potentially a zk roll up a zk proof system a zk settlement layer and so what that means is that we can have an evm equivalent zk roll up and additionally because it is modular we can swap it in and swap it out so the same chain that was running a fault proof can without changing anything about a developer experience without changing anything about you know the nodes that people are uh running you can actually swap in a different proof system for withdrawals and so that means that we can get these like seamless upgrades and write compatible software actually in fact the key right the key to scaling up the production of software is defining really good abstraction layers and so you know this is actually more important than you might imagine because it allows people to work on different things in silos and still be compatible once they have done the incredible amount of work and zkp's are an incr great example of something that's going to take a lot of work to make extremely inexpensive and let's continue so we've got the consensus layer right we can do things like layer 3 by layering in plasma with the evm and fall proofs right all of these things are options and even swap out the execution layer all of these you know configurations are valid but the key is that we should make sure that the way that they communicate the way that these layers communicate with one another is standardized so that we can all benefit from each other's innovations and do build out the you know blockchain stack candidly in an open source and permissionless way so that is the core of the oh pizzle stack izzle so the bedrock release we are finally at the section that section okay what is bedrock right bedrock is a major step forward across two primary dimensions so first off obviously it is the next upgrade to optimism mainnet that is we are upgrading our main net if you deposit into optimism then that thing is going to be using bedrock very very soon and that makes that's going to make it the most literally like the most secure the most scalable roll-up that is out there let's go so bedrock is actually legitimately extremely extremely dope um the second thing is that it is a foundational it it supplies the foundational op stack modules right these we built it out in this modular way already and in fact in the later talks you know with josh with et cetera we we're gonna we'll go into the details of like how these things actually fit together and what components we we built um so right we have a configuration of the op stack and we have all of these different packages which it is using right we have this op node which is the consensus layer kind of equivalent we have op geth we even have op arragon right this is literally two ethereum clients that are compatible additionally we've got all of these other you know batcher proposer contracts we have canon for fault proofs all of these things are you know built in this modular way and so here i'll go over into like more specific details of like where the major steps forward and modularity are so first off modular execution with the engine api so first i told i started this talk by saying how the consensus layer and the execution layer are actually being you know built out and were created for layer one development well what did we do well we swapped out the consensus layer and replaced it with the op node so now we're coming to consensus by using l1 consensus that's literally like the diff definition of layer two additionally we have the execution layer so we have def but we can also swap in things like aragon and it takes very little code to modify these execution layer clients to support layer twos and that is critical right code reuse is a key factor of security and these systems need to be more secure than any other system you've used let's let me tell you is it is the security requirements are insane and so this also enables us to have a multi-client ecosystem we would never have made the merge upgrade we would never have done the merge upgrade without multiple clients because multiple clients catch divergences between them making the entire system more secure that's why the merge went through without any critical bug reports right that is literally the technology that the security technology they used was a multi-client ecosystem and so we need that very same thing for layer two and that is absolutely critical if we want to enable fault proofs for instance and i'll talk about that a little bit more later so yes additionally we can even have more fun and you know maybe it's not gonna have a multi-client ecosystem but we can input this like weird you know weird uh uh mystery uh vm and boom now we've got even more execution engines that we can use so so that is the first step right it's the engine api the second thing is modular derivation so first execution the second one is derivation or the consensus layer so in you know normal in back back back in the good old days a year and something ago when we were writing our rollups we thought that we needed a bunch of smart contracts on layer one to make to actually derive the the roll up so essentially you take in l1 this function right is like the rollup function it produces the layer two blocks and what we were doing is we were saying okay you need layer one and you need some smart contracts on layer one to do some like management for you and you know whatever whatever but this meant that the system was actually really brittle it was really hard to upgrade because you always needed to maintain these layer 1 contracts but now what we're doing is we're consuming raw layer 1 blocks raw layer 1 block data to produce the layer 2 chain so what does that actually mean it means that we can swap in because it's just raw str like a raw stream of data we can swap in new data sources like eip4844 or even plasma right things like off-chain sources of data so this allows us to you know both a scale up you know be ready for when we're improving layer one because we're pushing forward eip-484 which is a major improvement to layer one but we're also able to support things like layer three and finally modular proofs with canon right canon is it generates the execution trace over the uh you know over the client and it allows us to prove that execution trace but we can prove an execution trace with either fall proofs or zk proofs right these two things totally work the ck proofs might be too expensive right now but that's something that we can work on over time and importantly right importantly bedrock again supports multiple clients and multiple clients is incredibly important to support production high security chains and specifically to support fall proofs and zk proofs without big bad upgrade keys right we need our upgrade keys to be you know upgrade and then you wait like two months because you can actually withdraw and we need them to make sure that the mechanism that allows people to withdraw from layer 2 in case there's a malicious upgrade we need to make sure that that mechanism will never break and the only way to make sure that mechanism will never break is with multi-client support so there we go you know more information on that in our pragmatic path to decentralization because we need to get rid of those upgrade keys we need to not be multi-sig roll-ups um anyway great this you know this evolution right the state of affairs is madness with these bridges going crazy and these different chains and this terrible user experience we're already making progress there right bedrock release is fast approaching we're about to take a massive step forward in the op stack additionally we already have multiple op based chains so it's like not even it's this is not even like a oh you know you know we need an x we need more op chains like literally we accidentally had people deploying opi chase just because we were writing our software out in the open um and additionally we've got teams experimenting with bedrock pushing the boundaries of what was possible on chain right we're gonna break through even the scale like the scalability that we have today is not even close to what we're going to have tomorrow it's like it's like you don't think about the resource constraints on aws unless you're a massive crazy you know team but if you don't really think about the scalability constraints on aws as an average individual you should not be thinking about the scalability constraints as you know you're transacting or deploying dapps to a blockchain this is just the beginning right we are about to take over with the zombie horde of gerbils generated uh by artificial intelligence um we will take over the game and soon we will realize the final evolution of blockchain walmart no i'm just kidding um we will finally realize the super chain [Music] thanks to the op stack because we're working together to solve the problems in a secure and scalable way hallelujah we're going to provide that good user experience to us because we're the users too you know and the rise of the super chain that's it and by the way um this is also a super chain uh image but including a gerbil um and i just love the way that that these ais or like dolly specifically cannot spell it's so good anyway thanks everybody i guess we don't have that much time for questions but happy to answer them um oh pizzle stack is let's go all right that's it i mean i don't know um i'm now in my room i still feel like i'm in my room um is there a place i should be looking for for like questions do we have like a couple a couple um like you know i don't know i don't even know i don't know where i am well i feel so alone wait am i oh it's because i turned off my mic because i can't hear maybe hello okay fine i'll just talk then hey carl sorry no i think hi it's me um no i think uh i don't see any questions i think i think we're good let me just all right sounds good well thank you i guess we can open it up really quick does anyone have any questions they want to ask carl how did you find such a cute dog photo well all right let's move to josh okay awesome carl thank you so much i love your presentations you're so enthusiastic you're awesome okay cool um hi everyone i'm katie um so up next we have um joshua gutow who is going to be giving us an introduction to architecture oh hold on here we go here we go okay so we have uh joshua gutow giving us introduction to architecture josh welcome thank you so much for being here i'll let you take it away awesome see i get my slides up okay slides up cool awesome hello everyone my name is joshua and i'm going to be talking about uh introduction to architecture this is actually a little less architecture in terms of buildings and uh cool shapes in the sky but this is going to be about software architecture and then more bedrock's architecture um and as carl is talking about the various components of the op stack this is going to really highlight uh how bedrock the system fits together and all the different pieces there cool so the introduction ah so today i'm going to be talking about what even is optimism better carl's talked on that but i'll expand on that a little bit more um i'm going to give a overview of the system both of kind of like the components and also the actors so we have a bunch of different pieces but some of them do different things uh then we'll go through the rollup node uh which is one of the components the sequencer and the voucher and then the proposer fault proof challenge agent so that's all the stuff that enables withdrawals uh we'll go through a couple data flows um and then go through some kind of final implementation notes at the end so what even is optimism bedrock uh bedrock is a network upgrade to reduce fees and increase implementation simplicity uh so kind of go through a little bit word by word because it's a big complex sentence with a lot uh underneath of it uh so network upgrade um so bedrock is going to be kind of a point in time release uh the thing we'll have like we have our current system i will have the bedrock release and then we'll have more releases in the future uh so it's not it's the network is still optimism and this is just the network upgrade and as part of a network upgrade we'll be keeping all of the history uh with optimism network this is just one more step and improving the user experience and specifically the biggest change to the user exp experience is going to be reduced fees and then the non changes to the user experience are going to be increased implementation simplicity this is something that most users won't see but really improves the performance proves the reliability uh enables a lot of other things down the line so how do when we get there bedrock achieves this by redesigning the roll-up architecture so a lot of the things from the previous system uh kind of are getting modified and then we're drawing a lot of like new uh lines between different components inside the system and really making sure that everything fits together on a way we're happy with and also in a way that kind of enables different components to be swapped in swapped out uh how components can be tested uh by themselves and making sure that everything fits together very neatly so now we get into the fun part which is the architecture so we're going to go through the system overview so here's kind of the discrete components you have a up node uh this is op node and then there's the execution engine opg we put op in front of everything because why not uh the batcher is the op batcher and the proposer's op proposer and then we don't have a challenge egypt and the fault proof is canon so if you know to no big guev um are kind of the things that produce blocks in uh that's like your full node on ethereum op batcher submits l2 transactions dell one um then the proposer submits output routes um and this is what enables patrols and then we talked a little bit about canon and we'll and have talked more about canon in the past uh but that is the fault-proof system and the challenge is just a simple uh agent that interacts with the fault-proof system cool so those are different code bases and components how do they translate to the actors so the actor is someone running so you can run a multiple of this lots of times so verifiers that's a full node um so there's going to be a lot of those that's just like running your full node on ethereum and that's you to do that you need your op node which is like your consensus client as well as your ob gif which is your execution client then you have the sequencer which is a full node that is mining and so it produces blocks and then it also gossips them out and does a couple small things but under the hood it's very similar on to the verifiers and then you have your batcher which is an actor that just maps straight back to the code base same with your proposer and then your challenge agents as well the sequencer and verifiers actually have to map to a couple code bases because they're more complex things so giant diagram type um so this is going through how all of these uh actors interact um so if you want to kind of deposit into the system we'll just follow that execution flow so uh the user sends a deposit transaction to layer one um and then the sequencer reads the deposit sequencer also then gets transactions kind of separately then it creates the new blocks sequences them the batcher then picks and that's kind of how you get deposits now once you have your l2 transactions included a user sends them to the verifier they get to the sequencer you create your blocks the battery submits the batches now then the verifier reads both the deposit and the batches and then it derives a chain that matches the sequencer and so that's kind of the transaction flow there's a lot of moving parts here but that's kind of what it is and we'll go through some of these flows on a little more depth later on and then when you're doing output proposals uh the output proposer reads the state route from the verifier and then just submits it to l1 and that's the op proposer and then the challenge agent would interact uh would kind of use the verifier again looking at the state route and comparing it against what got submitted on l1 uh and so it's reading between the verifier and those are again all the like system actors and how they interact cool so time to dig deep into the roll-up node so it does several things reset from l1 has both deposits and batches uh as carl mentioned it uses the l2 the engine api uh to talk uh to kind of the execution client op gif uh there's a p2p network i'll talk a bit more about that later um and then just as an implementation it's way simpler than the current l2 geth implementation it's a lot closer to upstream and it's a lot easier to work with so the diagram so this is basically all of the components that you need to run a replica or a sequencer just in a node on opt-in a full node on optimism network uh so on the ethereum side you have your consensus client uh and your execution client they talk to each other using engine api um and then your op node which mirrors the consensus client uh uses just your standard json rpc uh so it does stuff like gets blocks gets transactions gets transaction receipts i just kind of essentially reading in on the chain data from l1 and then it talks to the l2 execution api uh l2 execution engine through a very slightly modified engine api it's very similar but i'll talk through a couple of modifications there and then the op geth or l2 gef is a small diff on top of gif it's actually a little larger than 500 lines right now but not really that much more um and so that's a lot easier to work with and keep the op up to date with upstream so we can bring in a lot of those improvements a lot faster than if we had done a lot more modifications to that and so this is a single node so now what happens when you have multiple of these nodes uh so this is where every node is talking to almost every other node so the vertical lines there basically maps back to the previous diagram and the or sorry horizontal lines map back to the previous die again and so the top line and the bottom line are two different sets of nodes so on the left here uh you have different consensus uh clients for ethereum talking to each other through the beacon chain uh then l1 clients on ethereum have a sync protocol they have a transaction pool and then on the l2 side uh we gossip out l2 blocks for the opi node and then we actually use a lot of the same sync mechanisms transaction pools all that when uh l2 execution engines are talking to each other yeah so getting into the details of this engine api which is what connects the op node to op gif um it has two extensions and these extensions are to some of the uh payload attributes are like fields so we have two more fields when we're calling fork choice update but the rest of it's basically the same an engine api is actually really simple there's four calls one of which we don't use at all because we don't have a transition so it's just not relevant onto the new payload uh call is just insert a new block into the execution layer simple uh four choice updates a little more complex because it does two things at once uh it sets a head block including kind of safe and finalized blocks um and it also if you give it an optional parameter it starts the block building process and it returns back kind of reference to that in-flight block and if you want to get that uh in-flight block that it's building you just get call get payload um so and that's basically it and this is pretty standard api it's specified by ethereum um and it makes it really easy to include new execution clients opi aragon that's not that much work to pour it over a thousand line diff and then eventually we only have one roll up node right now but it wouldn't be that hard to create a new rollup note that spoke this api and and worked with any execution engine so deposits and batches um so we've briefly spoken about deposits as kind of a way to get money into l2 these are going to be a lot faster now because we're reading them a lot closer to the chain tip but they also serve another purpose and the sequencer basically cannot delay deposits outside of a certain range and has to eventually include them if you're interested in more details about the specifics of how this works i gave a talk at hcc that does a really deep dive into bedrock it goes through all this and why this is true um but the net result of this is that the chain advances even if the sequencer doesn't want it to um and then this enables sensor step resistance and then if you never had a sequencer again you could still unwind the chain through deposits so deposits also do a lot more than just uh mint money on l2 they're fully featured transactions so anything you can do with a normal transaction you can deal with the deposit um and so typically you actually wouldn't use deposits that much to do this because deposits just get included on l1 kind of one transaction at a time and actually have some state execution on our one versus a kind of direct l2 transaction when we batch those we take a bunch of those we compress them and then we just shove it in call data of l1 so direct l2 transactions tend to be cheaper and that's kind of the typical flow but if you need to deposits are always there okay p2p um so here we kind of described how sequencers gossips out blocks and so the feature that's enabled here is that if you're just a replica on the network and you're subscribing uh to these unsafe blocks or blocks that have the sequencer has just created your view of the network is incredibly up-to-date and so you can often choose to insert these blocks um now as part of kind of inserting these blocks we don't actually mark these as fully safe because there's no guarantee that the block that you received will kind of get submitted to l1 um so we prefer in the roll-up node whatever gets submitted to l1 over what the sequencer says and kind of as a result of this uh it's a very minimal trust assumption to insert these blocks and stay up to sync with the tip of the chain um they have a really good user experience without actually sacrificing trust and this is kind of a similar thing to the liveness thing aliveness in ethereum where you have a head block uh which could reorg uh but then you have your safe block which is assumed not to reorg and then you have your finalized block which will never reorg um and so we have uh we've brought over the same kind of terminology and uh same usage of the engine api to mark blocks as safe which are very likely to not reorg finalized which will never reorg and then your head block which you've got from the sequencer which is likely to be included but it could change and that's kind of the whole pdp subsystem and then as a part of a result of getting these blocks as they're distributed it's really easy to hook into a snapseed uh it's not yet implemented because you can just re-execute all the blocks but that's something where this architecture of splitting out the execution engine from the consensus layer that's something that's really easy to do and something we will be doing in the future cool so we've covered the roll-up node on talked about opi geth and opi node how they interact and how different nodes interact with each other on the network and now we're going to talk about the sequencer and the batcher sequencer in the batch so what's here is basically talking about the sql search a normal node essentially it runs a mining process uh and then it's like just like ethereum how every full node has mining code in it it's all like every rollup node will also have mining code in it um and there's just one specific one that can run uh and then the batcher is a pretty simple code base just takes all two blocks transforms them to the data that you expect uh does compression say you get low fees and then just submits them on dell one uh easy pc ah and then having that be so easy is basically like small modular architecture that's a theme here so now the proposer fault proofs and challenge agents so this is a group of uh kind of three which will be at least three different code bases and a couple different agents but uh what's here is this is everything that's required to enable withdrawals um so the proposer is kind of the entry point into withdrawals so it reads state from l2 and then puts it on to l1 and then after if you put it onto l1 and no one says this is invalid it's that assumed to be valid that's the optimistic part of our roll-up um and that kind of validity then enables withdrawals um and if it's oh sorry uh and if it's not valid then you go through the challenge game and the fault proof uh so it's not yet live uh see some cannon talks that nor swap's given uh for more information about this um but the fault proof is what secures the bridge this is the how you say someone submits something that's invalid as a proposer the fault proof is what marks it as invalid and it we time bound how long you have to say hey this is invalid so you can actually eventually withdrawal actually eventually withdraw um and but it still needs some amount of time for the finalization period uh to basically uh have enough time that no one can censor the chain for long enough with this withdrawal period so if it's like a 10 second finalization period you can censor the chain and then get any fake output through with a week you can't censor the chain for a week and then if there's ever an invalid output proposal it will be caught and then so the actual fault proof game is super fun and exciting and i'll nerd out a little bit over it so the entire thing is called canon and it has its whole own sub architecture and it's like a lot of interesting things in there um but it's an interactive proof game over our mips execution chase so like everything here it's a lot summarized into a short sentence um so the interactive proof game is kind of this binary search like thing so you start out with some code you execute the code and you can imagine the code is like executing an instruction like an ad a subtraction a load from memory um and so what we do is we have kind of an on-chain version of this and then your two off-chain components the two off-chain people both run their execution chase execution your challenger and your defender so the defender is the person that submitted that proposal and your challenger is the person that says no this is invalid um and at each you run the execution trace and you agree on the start and disagree on the end um and then you can kind of mercalize the state uh do some funky things in there and then basically run a binary search over like okay we agree at this step we disagree with this step and then you can finally narrow it down to a single instruction execution uh and then execute that single instruction unchained uh it's super fun super interesting uh and then actually enables you to kind of fault proof really complex things really easily so anything that's deterministic and compiles to mips can be fault proven so the whole derivation process is going to get imported to canon uh all of the evm is going to get imported to canon and we'll kind of go through the fault proof any compression we do uh again just straight through uh so we don't have to deal with like loading up data we can load in the raw data that the roll-up node uses which is again just transactions and blocks and so those are really easy to kind of load up on chain and prove that hey this is actually the block hash on chain and so that's basically uh all of that super fun super interesting uh not yet live but we're working a bunch on that okay getting back here okay so that's how we secure the system so i'm going to go through some data flows in the system you'll see how kind of different things interact and maybe it'll make it a little more concrete so we'll talk through the deposit uh l2 transactions and we'll also go through withdrawals in a little more detail so the deposit you send a transaction to a contract and the key thing is that the contract does some computation make sure you pay your fees make sure you lock up your wreath and then it admits the event and that event is what we're looking for on the ltnode that's the authorization that yes you locked up you eat yes all of this is valid uh then the world node reads uh the event um creates a special transaction type on l2 and then we insert these transactions at the start of the block and then eventually that deposit gets executed on l2 there's a couple more details in there around like gas accounting but not quite relevant for this talk so then if you're submitting uh direct to l2 transaction uh you send out your transaction it gets to the sequencer and then the sequencer has its a set of transactions and kind of local mempool and then it creates a block based on that then once the sequencer has created its block um the batcher sees hey there's a new block uh we need to go submit it to l1 um so then it takes it combines it with a bunch of other blocks does some data transformation compresses it all does a little more like data slicing and then puts it on l1 and then then the rollup notes read all the call data look for transactions to specific address check the authorization on those transactions and then reassemble the blocks from that submitted data and then you have voila your transaction on l2 cool so withdrawals so withdrawals are a little more complex because you have to manually kind of do the communication across the layers so to initiate your withdrawal i send a transaction to l2 um to the withdrawal contract and so the withdrawal contract does several things um basically creates a commitment to how much you're withdrawing um then uh burns how much if you said you withdraw and that's event to make it easy to index and track it and then it also touches some l2 state based on kind of the commitment to the withdrawal and this touching is really important because we'll use that later to verify that you actually burn your funds on l2 and can kind of withdraw them from the optimism part on l1 so after you do this then you have to wait around for the proposer uh to say okay i've got this new output route go have the proposer submitted on l1 you have to go through the challenge period uh to say that no one's uh making a fake uh withdrawal and then finally you can execute your withdrawal on out one and so what you're doing there uh is verifying the withdrawal contract or the optimism on l1 when you're doing withdrawal is verifying that you have a valid proof of your kind of commitment to your withdrawal so like you have a withdrawals that says i want 10 heath back and says hey you actually did have an event or a state on l2 that shows that the only way the state could have been true is that if you had actually burnt uh 108 on l2 so that's like the proof system there um basically the overview there's a couple more details of about assembling all of the proofs uh but that's kind of highly mid-level overview not fully high level so we have a couple data flows let's talk about how it's all assembled so we've been talking about a bunch of different components and these are a lot of small modular code bases so the op node is about 15 000 lines of code the opg it's about an extra thousand lines of code on top of normal gif um and we find that's kind of what it takes to get another execution engine on to optimism uh they'll be bachelor and the proposer around 2000 lines of code uh as we kind of get those more production ready i'll grow a little bit but it's likely to remain under five thousand uh the contracts around the portal l2 contracts all that's around 10 000 lines of code but that's with all of our tests and supporting code so none of these are giant uh hairy code bases that are all intertangled they're all kind of off in their own little world they have clean api boundaries between them and they're a lot easy to work with easy to upgrade and so that's one of been one of the big pushes with this new architecture so if you're interested in kind of looking at the code learning more about it uh all of our code is in the optimism monorepo we have the opi node code base the op bachelor code base the op proposer there's just top level folders uh if you want to look at the bedrock contracts it's in packages contracts bedrock self-explanatory um and then we have specifications for how the entire system should work inside the specs folder cool so in summary edward is a network upgrade to reduce fees and increase implementation simplicity and we basically talked through all of the architecture that enables this ah so this was introduction to bedrock's architecture again i'm joshua gupto find me on twitter trianglesphere and if you're interested on working with optimism or all sorts of positions there but in particular we have positions to work on bedrock so this is exciting to you uh we're hiring at jobs.optimism.io cool that's it for me have a good day everyone thanks joshua great presentation really appreciate it um okay so up next we have a norswop who's going to be talking about modular sequencing noir swap welcome happy to have you thanks for being here hey everyone oh there you go bear all right all right all right [Music] we [Music] are live okay i'm going to be talking to you about modeler sequencing so i want you to know that i did not come up with the title of the talk uh otherwise would have been transaction ordering is a social construct who watches the sequencer muv is everywhere wow that's a lot let's unpack that first of all none of this is future plans we haven't decided what we're going to do about this stuff these are some thoughts and ideas to explore design space okay so uh you know don't make investment decision or whatever based on this uh just don't do anything stupid really um so modular sequencing core talked about how the op stack is modular and just like everything else sequencing is something you can change in the op stack okay so what's sequencing sequencing is picking which transaction to include in blocks and how they are ordered okay and also who sequences these transactions right um and so who gets to pick and what do they pick how do they choose so first we're going to be talking about who and today it's us it's oop labs that runs the soul sequencer so we get to pick your transactions in practice we just pick them in the order uri first in first out it's very simple we haven't done another work on this yet uh what could go wrong no comments but you get some idea censorships all that bad stuff so what you want to do really is decentralize the sequencer which means having multiple sequencers that cooperate to come up with uh blocks okay and so what are our goals when we say sequencer decentralization there are two goals really one is liveness such that if a sequencer were to go down uh the network the l2 chain keeps progressing anyway very important the other one is censorship resistance and that's like making sure that we op labs cannot make your transaction be censored forever and that nobody can force us to do that either right the non-goal of this is safety uh optimism is safe but the mechanism that we use to ensure safety is not this it's the fault proofs okay so even if there's a single sequencer then the the chain is safe and we can store your money but secret decentralization is important nonetheless uh for questions of censorship resistance and liveness so how do we decentralize um like everything there's a short-term view and a long-term view their things are easy to do and things are harder but maybe more interesting and so at first the idea that we're playing with is to have a low number of permission sequencers and two of them run roundverben round robin so what that means is that there will be one sequencer it would take care of a certain number of l2 blocks like 10 or 100 uh we need to figure that out and see what's feasible and then it would be the turn of another sequencer would do the same okay and so if one sequencer is misbehaving or censoring or things like that he would only be able to do that for a certain number of blocks before another sequencer takes over and if we diversify the set of sequencer well enough we put them in different countries and we make sure that they have some vested interest not to misbehave so they should be like known entities they have some reputation or maybe they put a bond that the governance could take from them if they misbehave then we ensure that at least one of the sequencers gonna behave correctly and the chain will make progress and your transaction will be included so there's another model for this which is uh interesting it's called meva and meva stands for mg auctions we'll talk about image a whole lot in in the rest of this talk but mav if you don't know what it is you can think of it as the arbitrage profit right if some token is cheap on uni swap and it's expensive on telegram while you buy it you need upsell on the drum you make money who gets to make that money or the first person that notices this and does it and who gets to decide who's first while the sequencer okay so sequencer is able to capture this profit if he's aware of them um and so yeah and so since there's a lot of money in this basically one way that you could say that the sequencer could that the sequencer could be decided among a set of sequencers the sequencer that bids the most uh gets to sequence the blocks okay there are questions here right like we you don't want to make to make it so that the same sequencer wins every single auction so you still need some kind of mechanism to ensure that this does not degenerate in a really bad scenario but otherwise it's a pretty solid one to uh decentralize and also uh make sure we capture some some value through and i'm going to come back to mg later all right so another model that's interesting is that we could just go and say well optimism will behave like a small blockchain like a blockchain that's not very decentralized like some of the blockchains in the cosmos ecosystem or even solana and then we'll run some consensus on all these sequencers and they will come up with a block and then that's the block that will propose to the l1 chain um yes let me see if i need to add something on that i think i've covered everything uh yeah and the thing that this small blockchain is not very decentralized doesn't matter because like i said before the safety comes from the fault proof okay so even if you have a 51 attack it doesn't matter it's just a way to ensure that um again the network is live so even if notes go down it still works and nobody can censor because this mechanism is gonna pick different uh people to propose the block each time and there are many existing consensus contents algorithm we could use for this we could even reuse the uh proof-of-stake algorithm from ethereum or we could just use something simpler like temperament the reason we don't need to reuse the algorithm from ethereum is that the ethereum stake according is made for a huge number of validators right and we're only dealing with a small number at least at first um so i've been saying yeah sequencer can misbehave they can they can do things they shouldn't do and so i want to like list the the ways in which sequencer could misbehave so a sequencer can lie and just include transactions that don't work um if they do that if they submit that transaction this just they will just be ignored basically uh other sequencer will just read them and say this is it doesn't work and so they'll be ignored the other role of a sequencer so one order of sequencers to pick the transaction and propose them the other role is to submit output routes okay output routes is a commitment to the result of executing your transactions and if a sequencer submits bad output roots uh then the full proof can prove that these are bad and the sequencer will be slashed and so it will lose the bond by deposited so it's economically unsound to do this the other things that can go wrong uh the sequencer can go can go down meaning that he will not do his work what it's supposed to do he can censor and then he can um misbehave in terms of mev and we'll see later what i means but say we decide that optimism we don't want uh sandwiching okay if you don't know what it is i will explain it later but or think front running if we decide that we don't want that but the sequencer does it anyway because technically he's able to then what we can do in those cases where a sequencer is done all the time he sandwiches when we said that we don't want that or he sensors then governance can take some action and just you know ban the sequencer basically and that works if the set of sequencer is permissioned right if we if you have to apply to become a sequencer and you have to be accepted then we can ban you and then you cannot reapply if it's permissionless we can't really ban you right you'll just go to another server get a new ip and then join again in that case it's necessary to have a bond that governance can slash and basically take your money okay like you saw there's a lot of ideas on how we can do this but the point i want to make is that all these models are compatible with our current architecture we can just slot them in and so that that's very exciting uh we have like a free reign to to come and freeze the best solutions in the short term and then in the long term all right second topic on modular sequencing what can we do sequencing well how do we pick the transactions and surprise this is a talk about mg okay uh you might have heard about nav you probably have but just in case what is emiv mv is value that can in theory be extracted by the actor that's the power of ordering transaction so in a certain proof of work there was the miners improv state it's divided there and on uh layer two and optimism it's the sequencer uh some people also call that maximal extractable values but whatever so always mg extracted and this is a copy okay there we go um so how do you extract the mvv so there's a different kind of images and we're gonna review uh the different major cans in a bit that will make it like much clearer but let's take the example that was before of an arbitrage if nobody knows that the arbitrage exists you can just send that transaction do the arbitrage boom you won except people might be watching the mempool if it's public they can see your trade they can see um this is a profitable trade what if i do it before this guy or uh i put higher guess fees so i will be included first and so people can actually steal your transaction this is called front running so in general you will need to outbid other people so in ancient times of the actual blockchains people did exactly this they did something called price gas auction which means they put a really high gas price to be the first in the block and the miners would include them based on the gas price then came along uh a little company called flashbots and they released a software called mg gef and they offered energy get to all the big mining pools and basically how it works is uh if you are someone that wants to extract mvv an arbitrager and now we call them searchers mg searchers you will send your transaction to flashbots flashbots will relay them to the miners and miners will include them in their blocks based on how much you pay the miners a very important thing the flashbacks does com compared to president's auction is that it guarantees that if your transaction is included it will not revert which means that uh basically you can send a lot of money you'll basically check if you get the arbitrage and if you get arbitrage you send a bunch of money to this to the sequencer or the miner etc another thing that flashback enables is that it enables you to send not only transactions but also transaction bundles and so you can see a transaction in the mempool and you can put trades around so before and after that and that enables you to something called sandwiching and we'll see that in a second and uh people don't like that usually this system has a flaw that is not often discussed is that all the the mining pools that run mvvgf they can see all the trades sent by the searchers okay and so if they wanted to they could front run all the searchers they could just steal their trades uh they don't do this because they know that if they do this they have a lot of reputation and then if they are found out they may they might be kicked out of flashbacks which will end up costing them much more money than they can hope to make by uh by basically stealing a transaction but they could and this will be important later then we switch to proof of stake and in proof of stake uh flashbuster prop was a new system called mg boost mov boost is something called a a proposal builder separation system or pbs and basically you get a a new entity which is block builder that builds the block and then you got the block proposer and that's like a validator so if you're picked as a validator in ethereum proof of stake you can ask block builders to to bid for the rights to pick the transactions in the block and then the block builders will now act like the demanding pools in the previous model so they will probably run mgdf or something like that to get transactions from the searchers so why did we change this model the reason is that after proof of stake no matter what people say um validation on ethereum is much more decentralized there are a lot of small um staking pools they're out of small stickers and we want them to be able to make money if they get to pick a block they should also get the emergency money unfortunately you cannot trust everybody with the transactions right because anybody that can see the transaction from flashbacks from mvgf they could steal these transactions and this is okay if you're relying on like six mining pools right it's a very limited set of people but if you have thousands or even tens of thousands of validity it becomes almost impossible to say who stole the transactions and so instead you will know centralizing the the trust into these block builders and all the valuers can contract block builders okay so now let's like what what's a movie let's see a few flavors arbitrage if there's a decentralized exchange where price is low you can buy there and you can sell higher another exchange very simple uh back training simply means capturing some financial opportunity right after the transaction that creates it right that's you want to be first basically and how to be uh the first is to be right after and so the biggest example of this is on landing markets there might be liquidations because the price has dropped the price of the collateral dropped and so these landing platforms want you to take over the collateral for a price in stable coins or something like that and you will sell it but they will leave you a margin which will be your profit and so people will compete to be the person that liquidates uh these loans so that's like a case of backgrounding uh simply for nfts uh get get if it's uh you know completely open mint the first people to mint get the nft and so that's a race you want to be first to mince the nft generalized front training we said example when we say that pools can steal traits from other people so basically you look at the mempool you look at all the trades right and you'll you'll try to see well if i were to do the same transactions could i make money and if the answer is yes then you just do that you just do exactly the same transaction you will just bid uh more than the other guy and then you will take the profit so that's called generalized front running generalized because you're trying this strategy over every single transaction in mempool um then we got sandwiching okay essentially scan a complex but it goes like this a user makes a big trade say on uni swap or something like that so say uh he wants to buy eth with usdc the mvp searcher says this assault sees this sorry and so what it's going to do is gonna do the same trade but before him so it's going to buy if for your dc which pushes the price up then the user is going to make the trade he's going to have a worse price and so that also pushes the price up right so if the initial price was x after the sandwichers buys it's express a and after the user buys it's express a plus b and then the sandwicher turns around and sells what he bought in the first step okay and basically he's able to sell that at a higher price that he bought because the user pushed up the price and so basically is is making the price worse for the user and is pocketing the difference and so that's that's just makes life shitty for you if you're making a big trade and people don't like this a lot there's also something called just in time liquidity um i i'm not sure i'm gonna have time to talk about that it's it's a little bit complex to to get into all the nuances uh but this slide is there and i will link the slides later and maybe we have time and we'll come back to it i think i'll leave it if there's some questions about it basically bill movie is big business so this is a screenshot from the flashbots explorer which i took in august i think so this is not entirely new but it gives you a you know an order of magnitude on these things right like on a month 8.5 million on 20 in 24 hours about 6k and in total and i'm not sure how long this tracking has been going on but like 600 million so this is a massive money that's being made here it's also highly asymmetric right like some blocks will have like massive uh mvv and some blocks will have very little mvv right like if you multiply 24 hours the 24 hours number by 30 you don't get the 8.5 million so it's very lagged asymmetrical um about 33 percent of this energy is paid to miners so this is not a lot right you would expect people to know the opportunities and basically bid a lot to the miners like up to 90 percent in practice we don't see this yet uh maybe we'll see that in due time uh they've been 18 transactions extracting more than one million it's interesting and the biggest one is actually occurred on l2 uh there there was uh some five million plus arbitrage on arbitrary a while ago so uh you know mvv and l2 already exists even though there's no flashbacks even though there's no auction there are there are arbitrages to be made and people will make them people will make the money so it may be good this may be bad people often say energy is bad that's a knife take um someone will profit from arbitrage like i said if there's a discrepancy in price someone will pocket that profit and i think personally it might as well be the network that enables those things and it might go to public goods and to a whole lot of things are that benefit everybody basically instead of just benefiting uh some guy that runs a algorithmic trading or or some kind of energy software at home he's not really doing like arbitraging is useful but there's no reason you should be paid a huge amount money for it right so sandwiches are almost purely zero sum someone that does a sandwich is not providing any value on the network it is possible there's been a paper published that says in some cases it could be better with sandwiching than not i i really doubt it is relevant in practice but you could in theory you could construct these scenarios just i mentioned this just to be complete uh but you know my conviction 100 the work would be better off without sandwiches nobody likes front running just you know people if you you know smell an opportunity make a good trade someone steals from you it's kind of shitty uh and just in time liquidity um i didn't get into details so basically this one is good for users but it's bad for liquidity provider and in turn that makes it bad for users in the long term so preventing mvv is dumb but people say we should just prevent mvv you can prevent arbitrages basically uh one way that some people try to prevent mvv is to this thing called fair ordering which is first in first out um and this is not fair at all because it privileges uh organization that can build our infrastructure to reduce latency of energy extraction uh and so you know what's fair in that right now before you had at least a competition between guys uh it could be a home and you came up with the cleverest solution and now you have the same thing that's as flawed but also only rich organizations can do it this isn't great and if you do this you still have a problem that people will try to start spamming to capture opportunities is this is not uh hypothetical it's happened on interim pre-flashback it's it's happening on solana it's a play now on binance smart chain uh and i'm probably on l2 to some extent though not quite as bad so how knowing all this like now we have a big picture of emig you know like it's it's it's not good it's not bad uh we need a solution for it how do we make sure that we capture all this money for the protocol for the public goods for the ecosystem and there are a few uh a few solutions for that so yeah the goals uh capture value for the protocol as i just said make the user experience good for users and so for me that means no front running and no sandwiching uh this is sometimes something people argue with they're wrong you know like fight me maybe we'll get some some interesting questions around that but basically this this works pretty well and it is actually feasible to achieve that and i'll talk briefly about how we can do that and then decentralize effectively so all the mv solutions that we came up with must be compatible with sequence recentralization as we talked before it's extremely important to decentralize so how do we do it basically we want to run some auctions just like flashbots one thing we could do is just run energy gas on the sequencers uh the thing is that with bedrock we'll have a two second block time okay and so two seconds is not a lot of time and if you have to you know people need to send their transactions to f you need to simulate the blocks you send that to the sequencer maybe the latency starts to add up and two seconds is a little bit too tight what you could do instead is do immediate so that's mvv auctions uh if you have something like mg booster energy boost basically does a movie auction actually uh and then you would um you would offer you would sell basically the rights to sequence more than one block you could sell right to sequence 10 blocks or 100 blocks or something like that so that's uh for mechanism so making auctions make sure the protocol captures value and then we could also impose some rules on how mut can be captured and so the simple effective rule is no front running if you come from trend you can also count sandwich and you can sort of enforce that on the short term you can have uh governance oversight so basically we say to the sequencers guys it's not allowed to front run and if they uh violate this rule then we can we can slash them or we can ban them just like we discussed before this is not great right because we have to monitor uh what the sequencers are doing and uh make sure they don't do this and then we have to do a governance vote it's a lot of like drama and uh people need to vote et cetera um like i'm i'm of the opinion that generally governance should be minimized whenever possible in this case it is indeed possible via technical solutions okay and these technical solutions basically uh work in two steps the first step is to remove the power of the sequencer to order transactions all together and there are basically two solutions are being talked about in the space right now the first is a first universal oracle so chill link has a social identity called first sequencing services again this has nothing to do with furnace boo and the other one's called threshold encryption and that's a solution that a company called shutter network is working on and there may be other people working on that yes there is another company whose name i forgot entropy maybe entropy but i'm not sure but in both cases basically you have a network node that provides an ordering for the sequencer to respect right in the first in first out oracle how that works is that all the nodes in that network in the first sequencing network they will reside or receive a bunch of transactions and they will say they will give their ordering of transactions and if a majority of them are honest then it's impossible to front run basically in a threshold and christian case you will send your transaction encrypted to nodes in the in the network and they will make an ordering of the encrypted transactions nobody knows what's doing what the sequencer will sign on the ordering and then the network the shutter network for instance uh will come together create a key to decrypt all this transaction then you you're getting decrypted after the sequential set yes for sure i will use this ordering so that's how you take the power away from the sequencer now you can't order anything and so as a user if you use that's the solution you know that you won't be frontrunner sandwiches or anything like that the next step is to reintroduce some form of actions because we still want to capture energy and we still want that value to go through protocol and to public goods how you do that is that you conduct two kinds of auctions ideally a top of block auctions for things that should do at the top of the block so see say last block after the transaction were revealed you're like oh there's a massive arbitrage now where you can bet to be on top of the next block and then some uh transaction also creates a massive mvv just like uh let's say um oracle updates and so this could be in a public pool and people could bet to background them the reason why you could say well why don't we just put the um oracle updates encrypt it and then we reveal them and then people will bet to be on top of the next block that doesn't quite work the reason why it doesn't quite work is that people will know the oracle is coming and they will start spamming to be by luck after it right they'll just like spam a whole lot and then they will get they'll get to be in the same block uh after the encrypted oracle update so you have to you have to do something special for them you have to allow background options basically and i think this is the end of okay in the dna representation so some conclusion there um so the european stack in bedrock is the first incarnation of the op stack they are flexible and you can easily change sequencing and we will indeed work on that in the future and the goals with uh changing sequencing would be to decentralize for liveness for censorship resistance and to extract mvv so that we accrue value to a protocol and ecosystem and public goods what are our best solutions well that's still we're still thinking about that we're still talking to people in the industry if you are smart and you have some ideas there feel free to message me i love to talk about that stuff and they are short-term and long-term solutions uh on short term we can use the governance as a crutch to say hey guys behave or else but in the long run there are technical solutions that we can implement that make sure that no matter what you can't misbehave all right i've been north swap you can find me uh basically everywhere my handle was in workshop including on twitter and uh yeah if if people have questions i think we have a tiny bit of time hey never swap good to see you man hey there how's it going good yeah thanks for your talk um there were a couple questions so i'll just kind of ask them to hear directly yeah so yeah so first question here is just about kind of the fraud proofing and how that works is it you know somebody individually is submitting um that there's kind of a fraudulent thing that's happened and if that's the case then you know how would you go about preventing people from front running the the fraud proof basically and getting the reward for doing so oh that is that's such a good question i love it um yeah that is a difficult question i think like the the disappointing answer is that you would go through something like flashbacks that has like sort of an uh a trust agreement not to frontrun uh because otherwise it's really difficult um i i i i've thought about this a little bit like we're not quite there yet we still need to build actually like the the the technology of the fall proof fully and this economic part comes even after that um but i've started a little bit with like trying to compensate people that like contribute but it's really difficult because then you can just like sort of copy what other people are doing without doing your own work and you don't want that either so it's a real problem gotcha and then another question that came up uh was and i know we have one minute left which might not be enough time but uh you know you mentioned as one of the uh the ways to deal with that cat is here uh mev is sort of you know threshold encryption is one of the kind of the ideas that uh that's been put out for for managing this can you maybe chat a little bit more about like the interaction of threshold encryption and mev and kind of how that might be a way to mitigate it yeah so threshold encryption basically um alteration encryption i should first explain is you have like this private key and you split it between all the a bunch of people and in this case the nodes of a network but the public key is known to everybody and so as a user you encrypt your transaction with the public key and only when the node come together to create the private key can you decrypt the transactions and so it's a way to basically uh remove the the power of the sequencer to order a transaction because now you are ordering uh encrypted transactions you have no idea what's going on there and then the sequencer will commit itself to them and then later you can reveal and so it's a way to make sure basically you can't front run uh or or sealed opportunities or sandwich and things like that and if you want to extract image then you need to introduce action but you can do that in a way that you can't do these bad things basically gotcha awesome okay cool well thank you so much for being here and running through your talk with us modular sequencing which was actually just a secret mev talk in disguise um almost all of them are uh but yeah thanks for being here northwell we appreciate you taking the time and uh if anybody else has any questions uh feel free as north south mentioned to grab them at any platform at northwell cheers thanks a lot loved it see you around you too awesome up next uh we're going to have proto come on and chat about uh pluggable data availability we're really excited to have uh the king of the pineapples here so proto when you're ready feel free to you know turn on your video unmute yourself and share your screen hey protor either looks like proto just dropped off so we're just going to go to a quick break while we get them back in here and yeah we'll be right back thanks everybody um awesome i think we're back i think protozoom crashed are you there now free to unmute oh there we go but we can't hear you unfortunately maybe it's the wrong audio source still can't hear i'll let you know if i can there we go that's this test yeah you're coming in clear awesome feel free to uh screen when you're ready as well yes i hope the zoom does not crash again you're coming there you go that's great hello alone all right let's get started sorry for the delay so um my presentation is about applicable data availability um with optimism where both stewarding layer-1 development of increases in depth of variability as well as building the technology that uses data availability called rollups and so i'm going to expand on uh fight for fire also called prototype sharding that's me and with op labs we're building this roll-up technology and building towards the bedrock release which uses this data availability so what does this talk about data variability layer 2 usage word for fire and then some layer 3 design at the end you can plug and play and build something new so what is data availability really that availability is this primary scaling bottleneck of ethereum over time the sharding roadmap of ethereum has changed a lot early on sharding looked like this thing where we had many many different charts and they would all have data as well as execution over time we realized that having so many shards would create communication problems if you also try to solve the execution problem and so instead of trying to solve everything at once we have this realization where we on layer one we can just focus on data securing data and enabling layer twos to create this like competitive environment where the execution layer essentially is out of protocol but secured by layer 1. so how does this work the inputs to the functions are secured by layer 1 the outputs are there to concern and so what we get from securing the inputs is this permissionless ability to reconstruct states and this allows any honest actor on the layer 2 to compute otherwise states and then contest what the layer 2 output is on layer 1 and then secure the layer 2. so a rollup is really just this combination of data availability and this execution check the thing that secures the withdrawals going back from layer 2 to layer 1. we have various different types of execution checks they're also called validity proofs or fault proofs or previously fraud proofs so if the sequencer makes a mistake or if there's a malicious sequencer they might post wrong output they might post a state that's not actually valid and then you have to prove them wrong on layer one to secure the layer two outputs and secure withdrawals secret roll-ups of validity proof where you prove the secret technology that the robot performed a certain state transition given their own inputs optimistic pull-ups they defer the the proof and basically only when the there's contention in the pessimistic case then you you play this interactive game on layer one to prove or disprove the output so with layer 2 we only really need this data availability for other honest actors on the network to reconstruct the state reconstructing the state is very easy if the state was only ever changing like once per month and very very slowly and everybody agreed on this debt but the challenges is that player 2 is quickly changing new transactions are being confirmed every second and so all this data that's changing needs to be made available in a permissionless way where we do not rely on the sequencer and this is what layer one offers this is data availability hosting data that's changing and that is uh cannot be relied on from just one actor from the sequencer it should be available to everybody so if you think about modular book science there's this narrative coming along you're taking apart the stack and you see this on there one with the separation of consensus and execution and other blockchains this enables us to encapsulate complexity and to enable scaling so abstractly this looks like we have a data provider and this layer 2 for derivation execution this is the simplified version a little bit more complicated is the reality so we have deposits going or like layer one messages that change layer two states from later on to there too and under is this refers we have proposal transactions which you can think of as an oracle to state what the layer 2 state is and to enable withdrawals so we have both data communication as well as this communication for liveness and for this messaging across layers and then the layer two is designed the same way as layer one difficult layer and an executioner and so if you think about their two state transition function it's really just a process of incorporating more layer one data to extend to layer two and then we can prove those there are two outputs on layer one and so often i see confusion about the proof of stake vanitization and the rollout finalization before we add proof of stake we used to use finalized for layer 2 data that is proof that is proven on layer 1 but now that we have proof of stake we also have this notion of finalized data on layer 1. so if you think about a rollup as a function that consumes inputs well then the function is not going to refer if the inputs are not going to be verbs and so when the inputs are confirmed in layer 1 then the layer 2 can also be finalized but it will take some more time for the layer 2 to also be secured on layer 1 for withdrawals this is the slide client type of thing that the oracle i was talking about to enable returns there are separate things and we're focusing here on just feminization of data because we're talking about data availability uh securing that was that is confirmed i'll give you a little bit more technical feel so we've been running test nets of the next petrol upgrade here's an example of a test net with two replicas and one sequencer and then what we're tracking here over time over the span of three hours is the traversal of the layer one chain these are these colored hashes and every colored hairs is a different layer one block and then a little bit down the graph you also see the layer two chain being traversed and the layer one references of the layer to gene being reversed and so over time we consume new data and i'll talk more about this data availability as we're consuming more data we basically need more capacity to host more users and so how do we get more capacity we need to scale the layer on data availability the starting roadmap has been split up into incremental phases this has been done before and now for the latest charting model map we have this idea of dunk sharding based on dunkrod from the firm foundation basically championing this idea of data availability sampling very conditionable data more nicely between many many different network nodes for the short term we do want data in data increase but we are not there yet with the network layer on their one to host a thousand times increase in data like that or not ready for data availability sampling yet but we are ready for a smaller increase and so this is what fire4 is about is to make this future compatible increase in data availability going from the approximate 50 to 100 gigabytes data consumed per per block now today to somewhere closer to a megabyte to two megabytes per block of data for rollups to consume and so this is what it will look like with right for far with the contents there exclusion there then we have this new data layer essentially attached to the layer one where we're hosting these blobs and it blocks the functions inputs to host the layer 2 epm activity lifecycle of a blob or this piece of data that layer two transactions are confirmed are in is like this where we have the user creating a transaction the roll-up operator building the transaction delay run transaction pool basically propagating the transaction until it can get confirmed and then the layer 1 confirming the bundle of layer 2 transactions as this is this blob of layer 2 content we think of this as a side car it's something outside of the regular beacon block it's synced separately but it's this condition where we ensure that the data is available along with the beacon block itself and then the exclusion payload of the beacon chain stays in there one blobs they are pruned after have been available for a sufficient amount of time for layer 2 to secure their network and then it's up to the layer 2 to persist the the historical state which like basically only changes once per month um for other layer two users to sync from and so how do we adopt this kind of new more scalable type of data we have this uh this derivation pipeline which transforms the later one inputs into layer two outputs into the layer two blocks as they are processed by the exclusion engine there are different stages that we designed as part of the bedrock upgrades to modularize this this transformation and so traversal retrieval and deposit processing they're all separate processes and we can swap out the data retrieval process for one that supports the new erp with uh if berlin we've been doing exactly this this hackathon of just like a week ago or so we built a prototype where we took optimism data or the optimism roll up and implemented the changes necessary to use the 484 data and so prism be extended with a retrieval api to fetch the blobs gaf now combines four four and layer two divs so we can use all the new changes in the same codebase and on the op node we derive data from the blocks and submit new blocks that are being built by the sequencer then we have a full layer 2 deployment that looks like this we have a layer on beacon node there one execution engine and there are two rollout nodes and there are two execution engine you can see the similarities here where they both have an engine api and a separation of consensus and execution and under these helper modules that make the data the inputs and of the outputs available to the higher layer and we can take this and you can stack it so you could think of the layer 3 that is basically just the same software as we run for layer two except stacked on top of the layer two instead of layer one and so this is like a thought experiment where what if we just reuse the same code to build the layer 3 and to get to make things even more scalable now what you see here is that this only really works if the blobs the things that host data in layer 3 are not bubbled up all the way to layer 1 but rather hosted by this layer 2 with a more even more scalable data layer and so you could adopt earp484 on layer 2 to host their tree but it doesn't mean that you have to build this alternative even more scalable data there and then well how does this compare to plasma or how does this compare to these skating solutions that host their own data well you could separate it from the layer 2 more where now you have the separate data module to fetch the layer 3 data from it's less equivalent to what you would like to see from layer one technology but it also does the same job and you can still stack the rod node the proposer and the execution engine so all this software is the same you then host the data ulcer and so i've been experimenting with these different types of vase like to further scale what we now know as a roll up but then in the future that's like this this modular system that you can reconfigure to become a layer 3 or plasma or so on and this is what applicable that ability is about once you have this stack of modules you can configure the way you like and then build cheaper better solutions based on the type of application you want lost if you want to learn more about bedrock the modular stacker building and the like the upgrades really of optimism you can find this on bedrock.optimism.io we also have a live testnet you could use and then there is erp for it for fern this layer on scaling efforts which you can contribute to which you can read more about on earp484.com and then contribute to github or in discord any questions thank you proto uh just check in the chat to see if there's anything that's come through uh doesn't look like it's so far uh might be worth uh just you know as we're gonna jump into a uh a panel about 4844 following this directly um i don't know maybe if it makes sense to maybe chat a little bit more just as sort of a preamble to that panel um about you know 4844 what it's gonna maybe enable and also just sort of like any other thing you think that's relevant uh to know ahead of that panel uh with tim and the rest of the team right um [Music] with fight for far what we're introducing is this this different type of data i think the context that many people may be lacking is how rollups today are hosted and how this compares to rolex in the future will be hosted today we're using call data which is this type of data that passes through the ethm and was never designed to be used in this way it was designed to be used as an input for a smart contract and then it happened to be hijacked in some way for rollups to host and make their data available in the end this just means that this type of data does a lot more and it's a lot harder to maintain than the type of data that we really need and so what we're trying to design is the type of data that can be hosted and available without being unsustainable without crawling in the bounded the current evm on layer one and all the inputs is this inbounded type of growth where the state the history and so on is not sustainable to maintain long term only when you can prune or reduce the strength then this this thing is sustainable to host on regular hardware and so with layer 2 we're trying to find these balance where we're securing the layer 2 but we're not creating this unsustainable resource users on layer one and so by redesigning this type of data availability we can make layer one a lot more sustainable we can secure there too at lower costs and so we can increase capacity for users as well gotcha awesome uh yeah i mean that's good that's good context especially to know that like right now i think the most important takeaway there is that you know the chain isn't really set up to do a good job of hosting that data right now and it's being you know almost kind of hacked in um and and really kind of the principle behind this change is that you know actually doing it the right way versus a kind of like other way um maybe a quick question while i have you from the chat that came through a bit of a roll up 101 question from abhishek um just a question around data availability he says i understand that rollups make a state route commitment to the main net then where is the actual data of the smart contract stored if it's a roll-up chain is it completely down well if it's on roll-up basically is it completely on the raw obtained and if so how do i access that data all right so i think i should introduce this notion of blockchain state versus blockchain data history but you could think of as a blockchain is just a sequence of transactions that has to be available or accessible to people to reproduce a certain state and so there too makes this sequence of transactions available by bundling them compressing them and submitting them to this layer one which can host them with higher security guarantees and then it's up to the users to read all of layer one and then reconstruct the state and then as you reconstruct the states you can find your layer two balances your layer two interactions everything you might need to layer two and then when you want to redraw from there too all you really have to do from here is to make the layer two output the state root visible on layer one and layer one unlike a regular note does not not have its own view of these things happening outside of the evm and so rather than what happens in a full node where you can run this layer 2 function to compute the layer 2 state you need to prove the state with a light client this light client is essentially a smart contract on layer 1 that can tell you the correct state of layer 2 to be used by other layer one contracts that can then process things like withdrawals the layer two states can be proven in several different ways this is the differences i talked about at the start of my presentation with uh zika roll-ups optimistic roll-ups and the case of optimistic roll-ups what we do is we always post the layer 2 output but only when somebody disagrees with the output then we built this game where a temperature and the original poster of the output can battled out and like the posting the right output and who's posting the room claim or output and so you can secure the the claims of layer 2 output to always be correct and to just secure the oracle and then secure the withdrawals gotcha okay awesome well thank you so much proto i think we're going to go to a brief break while we set up the panel um for the state of 48.44 but uh yeah thank you for your time frodo um and uh it's always great to have you on on eatbobble.tv and we will uh transition over to a quick break um so everybody will be back at 2 p.m which is only about a couple minutes from now so see you all there soon all right hey everyone welcome back um so up next we have a panel on the sata 4884 and we have a really awesome uh team of experts here to talk to you guys about it so we've got the legendary timbeco mophie and robert bellardo welcome guys thanks so much for being here thanks for having us yeah well i guess um yeah to kick this off uh maybe we could just take a couple minutes uh doing quick intros especially because uh 444 has been interesting in that it's mostly been speaker have been not my client team so far um obviously you have terence here uh who who's on a client team but yeah um do y'all want to take just a minute or two and kind of share who you are and how you got working on 484 um absolutely i'm happy to go first so my name is roberto ballardo and i work at coinbase i've been there for about a year now um and i'm not on a client team therefore i was not as overwhelmed by the merge i think as everyone else and we had deep interest in scaling ethereum and i was given the opportunity to participate in this for a bit um and so jumped at the opportunity um and you know hope to keep getting more involved as things go on mophie you want to go yeah we go next hey um i'm mophie tywell um i work for op labs which is building the um optimism roll up and um i started working on eib 444 all sort of like out of necessity um way back like early summer um because it was critical for optimism to have something like this to keep uh roll up costs uh pretty low and uh yeah pretty exciting the opportunity to um get this out the door and uh get it done parents hello hello yeah so hi i'm terence from prismatic labs i am mainly a core dev from the consensus layer so i've been working around this space since 2018 i started working on proof of stake now it's known as the beacon chain and last year i've been working on the merge and also a little bit of mvb stuff here and there and i've been interested in 484 actually since if denver this year february and i hacked a little something with proto-lambda so that was fun but yeah i've been mostly watching from the side for the last few months because of like the merge was slightly more important but yeah i'm actually really excited to start picking this up again and uh hopefully we can get this into shanghai nice yeah i think i think it's worth adding that i think when we say we're interested in the ip484 i mean we're interested in eve scaling right and this is kind of the natural first step to getting where we want to go yeah yeah absolutely and um yeah terrance you mentioned you know this first got prototyped around denver and um maybe do you want to actually take a minute or two and walk through like how this relates to bank sharding for people who have like no context um because you know we have this this plan to scale ethereum massively like roberto was saying and and and 4 is kind of one of the first steps towards there um so do you maybe want to just take a minute or two and tell us like what 4.44 is and how it relates to like full full scaling on ethereum sure yeah happy too so um so how do we scale right so right now what we're doing with bitcoin and ethereum is just every node download every single data and to verify every data and that's obviously not going to scale right and um with without this raw central roadmap essentially with the our future roadmap it sounds more like that we will essentially put the data on chain and then run the execution of chin basically but in the event of fraud the institution needs some sort of data to basically prove that where the fraud exists right so um so um so the nobles so usually how it works is that we will have sharding basically it means that not every node has to download every single data but with sharding it comes data value sampling which may take a few years to resolve and 444 is essentially the stepping stone basically say hey we will essentially black box the the data available sampling aspect and just put all the just make every note download the data as well but it's a nice future stepping stone for the future yeah right so it's almost like today all the nodes download and execute everything with rollups you can have the nodes only download the data but not execute every single transaction that happens on the rollups eventually we'd like to get to a spot where like not the nodes don't even have to download all of the data that's gossiped on the network they can only kind of sample part of it and know that uh that that it was correct um and 444 is saying like we're gonna we're gonna add more data capacity for these rollups but still get everyone to download everything roughly right um and uh and like you're saying you know this kind of came about earlier this year and and got like hacked on at denver um and after that uh mophie you were like one of the people to actually like start working on okay what does a proper prototype look for this um what do we need to do to get this in and get in prison first and get all the kinks ironed out um so can you kind of walk through like you know what you've been up to over the past few months uh what the implementations were like uh and yeah like where things are at now yeah yeah certainly um yeah it's been a while right um past couple months uh getting into like the ip444 it was like something i wasn't even aware of it until around like april even though like it was worked on the 8th denver um but my first thinking was okay we really need this for optimism like gas costs are out of control right and um and the rate of user growth that we're going um we really need this soon as well so i go basically roll up my sleeves and uh start working on an implementation at first it was uh basically looked at already existing work like what terence and uh mentioned he and proto already had like um a proof of i guess like a proto proof of concept of eip4844 um there are some things that are missing uh first step is basically to take a look at that existing um code and extend it basically um so what i did first was uh complete the implementation in the execution client um the one that um already existed was geth so all i had to do was uh continue working on gef and have that done and the next thing was uh to have a consensus client that also incorporates eib444 um even though it's not quite evident if you look at the spec there are a lot of like consensus uh level changes to ap 444 and so um i basically looked at prism because it's written and go and um i'm pretty familiar with girl lately so um that was like my client of choice for this but i really could have been any client um so i looked at a prism basically started implementing um what's needed to um get eip-444 working and um yeah that's pretty much mostly what i've been doing for the past couple months and then there was a point where um once both the death and prison implementations were good enough and implemented most of the spec it made sense to like start integrating things and um that's what gave birth to um the first devnet um so the devnet is basically a small like network that's running um the eip 444 hard fork um it it contains implements both the consensus and execution level changes to the spec and uh it's kind of like our like playground we're using to like test out you know how would the spec behave in certain conditions and what sort of transactions can we like sent to it and uh testing like the threshold of like the spec itself sweet yeah that that was a great overview uh roberto terrence anything you guys want to add on that i just want to give a shout out to like mophie and roberto and team it's like having a deathbed now it's a pretty a mess it's a pretty amazing like accomplishment right just because like we even had the devnet before the merge and we had this death net before withdraw and everything and it's just like remarkable and i always find it like impressive and possible that people go into other people's code base and then able to make like advancements such that so so all the props and all the all the all the props go to them yeah i was going to give props to mophie specifically as well i mean he's really pushed this along consistently been an amazing help in providing guidance to to those of us jumping in new completely cold and and being able to get reasonably productive anyway i also wanted to shout out to michael de hoog who um did some work on this from the coinbase side before i did um so i'm a relative newcomer here um i did some work uh predominantly in the consensus layer side um implemented the um the the new style uh blob verification for so on um for example um and continuing building from there hope hopefully we're gonna get the the next version of the devnet up soon where we have the latest spec changes um for the most part implemented um at least you know still remains a little bit of a moving target but at target but it's starting to settle down nice and yeah it is i believe the first time that like coinbase like gets involved in working on a protocol change um i think that i recall um i'm curious you know why does coinbase want to do this like what is the rationale to put engineers on this uh instead of like the thousand other things i suspect you all have to do um yeah it's that's that's a it's a great question and as you know tim i've been talking with tim since i've started at coinbase trying to figure out how we can contribute and it's um there are a lot of competing um um efforts for for time for a coinbase employee um but ultimately i mean i think we realize that um you know we want to we want to raise raise all boats we want to build out the ecosystem and and you know that's not just purely purely charitable um we we view coinbase as not you know just trying to be kind of the most easy to use and secure exchange and custodian for crypto out there but we view it as a you know a broader um uh onboarding mechanism to to crypto as well as all of web3 um so if you think about it that way um and look at the kind of number of users we're we're dealing with um if we took all you know roughly you know 100 million-ish users of coinbase and try to dump them on chain um you know it wouldn't work out so well right now um so and we want to do that right we we want to start blending coinbase's centralized experiences with more decentralized experiences again provide this you know very straightforward easily usable onboarding mechanisms and so on um and so in order to get there we need to scale the ecosystem right and the best scaling roadmap out there you know we believe is the ethereum scaling roadmap so that's the natural place to contribute and um yeah and and so you know mike got involved at first um i've joined in we got a couple other people we're trying to ramp up um and yeah just really delighted to be uh be a part of this yes i can't like emphasize enough like how much effort like coinbase is um putting into this um we have like a very good relationship with coinbase because we're all both like in the same boat here we really want ethereum to scale and we also anticipate like um the huge influx of users as you come in the next couple years so it just makes sense for the collaboration effort that we have going on yeah and maybe uh you know that they're very impactful like how we actually get this done um terence you you know you're you're on the client team obviously um and all the time teams have been deep down and emerged for the past year um but some people have started to like look over the edge and think about what comes next what's your general read of like how client teams perceive 4.44 now like what did they see in terms of like the complexity here the risks um you know what yeah if if people have had time to even look into it but yeah what's your general read on that front right so i can give my perspective from the consensus clan so my first impression is i really like it i think it's elegant and i think it's a good stepping stone and it's funny because if you look at ethereum history right we don't usually just dive into something we make incremental progress i think a good example of that is just that we should prove a statement handshake but we didn't just merge right we ship this proof of state beating chair we let it run in the wild for like one year and then we hop forward to a tear but we didn't merge there right because we want to try to understand how to hard fork in this type of environment so we take our time right and another example is just hybrid pbs right we didn't fully insurance pbs first we have this hybrid pbs we use like mad boost just because we don't want to commit into something and i think this is a very similar concept basically we are exploring ish right i think i think my perception is that i think some of the teams also think that i think it's important to get it done but we want to get it done as safely as possible we don't want to trade off between quality and speed for example i think majority of my time will be spent on testing and then in terms of just implementation complexity right i do think it's a little comp i do think it's a little complicated just because there is this new cryptography primitive such as kcg and most of the client teams unfortunately don't have like expert cryptography we have to work with someone else and unfortunately when you work with other dependencies it takes longer it introduces more complexity but the good thing is that we are using ibos like in the current uh previous day so we do have some experience working with like the teams as well and i think the consensus and the network changes are fairly easy just because we've done that before many times for the hard fork and then i think majority of the fun part is just like client implementation detail right for example like we have this concept of optimistic thinking now what can a client do when the data is not available right i think like using the merge as an example right there's probably like in prism there's like 20 000 lines of div but there's many there's only a 2 000 lines of beat diff that that's actually important right and then we spend the longest time just looking at those 2009 div to make sure hey this is good this is good this is good this is good so i will say definitely um lots of time on testing and stuff and i also know like mario's from geth has hacked like an eip-444 branch for lighthouse at berlin is super awesome and enrique from tech who has also been uh asking questions on that front so definitely seeing a lot more progress right now but overall i am very bullish and do you think introducing because we're basically introducing a new layer into ethereum right with this eip we have like now we like to call it the consensus layer execution layer you can almost think of these lots of like a data layer that we're introducing um how does that like change you know the client architecture or the testing infrastructure like is this as big as the merge where you know the merge we had no way to test the combination of execution and consensus player clients and we have to spend you know months and months building all that infrastructure is adding a data layer similar or is it actually a bit simpler because you know it's still handled by validators on the beacon chain and we have a bunch of infrastructure there so yeah that's a good question if you asked me that like a year ago i probably will be more like pessimistic but if you ask me now i think like we are like used to this concept of layering right for example we have translucent layer we have an execution layer and with like all the mdb stuff we have this builder layer so like everything is become more layer-ish and then so like every sulfur implementation is very complicated but they are as complicated as basically the api between them right so we have this concept of engine api where consensus layer and as an execution layer basically uh it exposes certain things to each other and that's all they have to understand and similar with this like data layer whatever we're calling right we will essentially expose this api such as like getting blob such as and stuff like that so yeah i would say the complexity is pretty like encapsulated so that's nice and we have also done this for the merge so we're quite experienced at this already and then it's just testing and which we have been testing for the last eight months right there's like great people like pairing and everything we know we know how to test this with different client combos yeah i i just like to jump jump in and i think this is way simpler than the merge um the proto-dang charting in particular it's it's pretty strongly coupled with a consensus layer at this point it's not like it's adding a completely new independent layer so i think it's i think it really hits a really great balance of you know getting us taking a step towards that independent data layer without having to go all the way there so i i actually view this as a relatively low risk change again being somewhat young new to this project than probably naive but um compared to what i saw going on with the merge this to me seems like almost a piece of cake nice um and you know like we keep saying how this is uh what we'll use to scale ethereum and how rollups we use it uh mophie do you want to take maybe like a couple minutes to walk through why is this actually better for rollups like how does a roll-up use proto-bank charting to provide lower fees to their users yeah certainly um so right now um roll-ups basically have to after like doing sending transactions and doing everything in l2 we need to post that data back to l1 um it makes it easy for um a new node that's joining the network to derive the chain right once that data is available but posting that data is actually expensive right now today ethereum um we the main way we do it well the only way i think most surveillance that i know does it is by using call data um when we create a transaction on l1 all the transactions and state routes and commitments that we roll that up hence roll up um into a transaction on l1 and that's posted via the call data but the call data is quite expensive um over the like the past couple of months a couple roll-ups including optimism have like implemented like a couple techniques to cut reduce the size of that call data like um compression but overall it's it's still not sufficient especially the rate of growth we've seen at like l2 zone it will like eventually be more and more expensive so what blobs transactions do what which is what eap 44 foot brings is provide like an alternative source of that um roll up data that we need um and also provide like a an alternative but also cheap way to post that data and the way we're doing it with eap 484 is rather than just post it back to l1 as call data um we would use like a special transaction type uh called the blob transaction and this just kind of goes back to like the data layer that tim mentioned so the the data that we're posting back to l1 um kind of like gets posted to the data layer not l1 in particular directly and the economics of this makes it much more cheaper for real ups to do because um blob space data space is cheaper than just using the l1 block space um so that's kind of like in a nut that's part okay that's like one part of like how l2 should be using rollups the second part is also like um in false proofs right as i roll up well for optimistic roll-ups in particular um we need to be able to um easily um dispute any invalid like uh state group that's been posted on l1 and with eip-444 this also makes it easy all we need to do is just change where we're um getting the data used for that fraud dispute to use blobs instead um what i like about this design is like it's very easy for optimistic roll-ups to just plug in what that data source you're using for this it's either from the data layer or from l1 call data doesn't matter so yeah that's kind of how it works in a nutshell nice and i guess oh sorry good oh no please all right though i was just gonna add in and and right so then when you can start um extending the data later to implementing the sophisticated um you know erasure encoding um rather than keeping it with every single consensus layer but i think one of the things to add um if for the for the viewers that aren't intimately familiar with the spec is um the blob data doesn't need to be kept around indefinitely by the by by the consensus layer it can be it can be thrown out and i think you know the discussions i've heard so far is you know probably after a month or so right because i think the main need for that data is for these fraud proofs or availability proofs and so you know after the dispute period has elapsed um there's no reason for a standard node keep that data which is one of the reasons it can be priced cheaper for example exactly nice and um you know so okay so we have all this l2s are going to use it it's going to be great it's going to lower their fees um what's left with you to ship this like uh you know first of all we this hasn't been accepted by client teams as part of the next uh the next network upgrade yet uh because we frankly haven't have time to to discuss it in much detail because of the merge um but from like a purely technical perspective right to bring this to a spot where like it would be safe to deploy on maintenance what are like the big things that are missing um and that you're all kind of looking into uh i i can go first then maybe like people can fill in from my perspective i do want to like um start merging the changes into prism which means that currently the code is more for like death net but it's not so much for like a production readiness quality so um it will be hard to merge it so i would definitely want to clean up whatever we have today merger with upstream and at the same time i do want to like sync with the devnet a little bit play with the devnet try to break it just to make sure it's robust it's it is resilient and i do think it's very important to write more right because right now i look up i type eip 444 on google and nothing really show up so i do think we need more like nice resource material such so that to bring more attention to the community and i think that's like a great way to like push forward yeah i think the other thing is just more client implementations right so we've we've um you know we've got gaff we've got prism and they if they're preliminary and um you know terence is absolutely right there's a lot more that needs to be done to productionize that code to test that code to um you know prove to everyone out there that if we launch this on the mainland um things won't explode right so the other thing i think we really need to do is we need to set up an official test net that i think has the ipv4 for active um where you know the developers of l2s can actually start creating their next version that takes advantage of it and seeing what what benefits it provides yeah and and to add to that um it would also be i think we also need like a proof of concept use case of uh the ib444 for like a rollup um because okay maybe i'm a little bit salty about access lists but we don't want to end up in a case where we deploy aip and people start using it and realize that oh the gains are not that much that we expected you know we want to like show that this thing actually works it's a valid use case we we would like use it once it's available and it'll be a huge spoon for the roll-up um teams so i think demonstrating this um by like i in particular rather bedrock we're actually working with coinbase to um integrate bedrock with the red rock is like the optimism upgrade that's currently important development and we want to like integrate it with eib444 and then tell like the client teams that hey this thing actually works um it's you know you can see how it works it's not just vaporware or something that'll be really i think a good way to like convince client teams to like accept it absolutely yeah i mean the the back of the envelope math looks great we expect it to scale help scaling dramatically but uh you know proof is in the pudding once we have code and a usable um usable system i think it'll be a lot more convincing right right yeah yeah and uh i'll oh plus one mophie it's your pointer it's like we've we've had features in the past that we deployed on ethereum where like the um yeah the usage didn't end up being what we expected so access this were once there was once another pre-compile for interrupt with zcash and that uh i think zcash changed the curve a few months after so it didn't end up working great um so seeing something like a you know cutting edge roll-up implementation that's that's working on a prototype of 4844 that would be that'd be really valuable um and one thing uh that 444 also introduces uh that's kind of new to ethereum is uh basically this idea of kzg commitments which require a trusted setup um so it's like a new cryptography primitive that we're adding to the network um movie do you want to kind of walk us through what that is at a high level like why do we need to add new cryptography to make this work um yeah and just your general view there yeah um so the data that we're posting um to the is where we've already dubbed the data layer that we're actually reusing we we need a way to be able to like um prove that you know a particular piece of that data we need to prove what that particular piece of the data means and this is like really useful for example in frog proofs where rather than sending the entire data set to your fraud proof you would just send little bits of it right and you can like convince the fraud prover that hey this data like matches the little piece that you've posted so what kcg lets us do is um it's basically it's like a commitment similar to um marker proofs but it's different from like um marker proofs in the sense that um it's easier for you to like um prove a particular point um with the same like proof size and also it's very easy for you to like extend it um this goes back to what roberto was saying about like um full data availability and erasure codes so with kcg it's very like um you can like extend like a kcg um your data with the kcg and then easily like make it manipulate availability um so that's kind of like gets over like the overview of like why we need that k2g about 444 but it's like new cryptography like what team says um they were introduced into the consensus uh spec client and um it's uh it's there's like some unknown unknowns a little bit that we're still trying to figure out in particular like what's the most efficient way of like um verifying these kcgs and um it's still like an open research question there but um i think as more and more people are like getting into like um the yeah the spec and taking a look at it we'll be able to like get develop better solutions there yeah i think it's also worth adding that while it is cutting edge crypto it's not quite as cutting edge as some of the stuff that's going on in the like zk evm world right um it's a fairly contained um i dare i i wouldn't dare say well understood but it's i i don't think it's like we're you know really inventing brand new stuff here um so and some i'm not a crypto expert uh by the way um but i was able to follow a spec fairly um easily and go and implement it with with the benefit of you know the proto-lambda work and go kcg libraries and so on um so yeah i don't i i just i guess i don't want people to be too frightened by that work that's going in there because i haven't found it terribly intimidating again as a non-crypto expert yeah true like that the math like kcg is based on is pretty old math like 80s math it's just like it's just recently we're starting to like actually implement it in a production-ish system and that's where like we're trying to like figure things out yeah and mophie in particular has been doing lots of work and optimizations and benchmarking and things like that so that that's another part of the project and work remains to be done there making it go faster i guess the fun question is like um do you foresee that we just have one crypto library for kcg that all the clients can use or do you foresee their that they're like they're like there will be multiple efforts that every client team will build their own kcg library i think there will be one or two um in particular with go we're we're thinking of using like one particular library that has like written in c but there are some like particular things with go and interrupt with like c code that might prevent us from eventually using that but i could definitely see like other client teams that perhaps using rust would probably use that as well and yeah it's pretty much mostly two implementations um at the most i think q feels like it would be a sweet spot um one has been a contentious topic in the past for implementing stuff on ethereum because we do have this multi-client architecture and the whole purpose is that if there's a bug then um not all the clients are affected but then if you have the central point where everybody's using the same library in the background um you know for the purposes of those operations you're back to like a single implementation so um hopefully we get to see two high quality ones emerge over time um yeah yeah it's been interesting we've been trying to sort of figure out where to set that bar where the appropriate abstraction layer is where we start sharing code right you want to set that as low as possible but without requiring people to like implement their own very low level bit twiddling to get the crypto right um so yeah correct yeah yeah yeah um and i guess the kind of go back up a little bit um you know we we talked a lot about 484 itself and and where we're at there um terence do you want to like walk us through what you know if you look at like the whole road map towards full sharding what like percentage of it is done with 484 like what are like the the boxes that we check off and then what's the stuff that we would need to do over the coming years to get kind of better scale on a theorem right i i can miss a few points so what 444 for includes right well 484 for includes essentially this new transaction type it's going to be exactly the same format that we will be using in full sharding and the most of the execution layer logic required for food sharding is also going to be there and most of the execution and consensus there cross verification checking logic will also be there right so those three are essentially getting piggybacked by 4844 and like you guys said like the layer of separation between like this data layer and the consensus layer is also there as well the concept of sampling blob right now essentially you have one beacon node that samples everything but with with fushardi you have this committee of of of validators that we can note simple portions of it and that's the power of sharding right and uh i think the guest pricing is going to be highly similar as well so that's very nice right so maybe we can talk about what food sharding um has that 44 doesn't have right full charging please have this low degree extension for the blob basically allow 2d sampling and then it has the actual implementation of the data value sampling it has a builder and proposal separation because i don't want proposer to essentially sample 32 megabytes of blob that's just a lot right and hopefully some proof of custody stuff as well to ensure there's no laser no lazy validator that just they just pretend to vote on everything so yeah feel free to add on guys if i miss anything yeah i mean i think to dumb it down a little bit because um it's pretty complicated stuff for someone who's just coming to this um but basically 4844 every every um node is still downloading every blob still storing every blob um with the exception of that what i mentioned earlier where they can delete it um the full sharding roadmap you know once we start doing this erasure coding each you know you have these committees that can be consulted to reconstruct the data and so no longer does every node have to store every single thing so so that's why it's called full sharding rather than proto prototank sharding i guess right then i guess if you think of it in terms of cost right right now when you use call data you're storing data that is stored forever by every node and so that's really expensive 4.44 gives us data that's stored temporarily by every node so it's like cheaper and then full bank charging gives us data that's stored temporarily by only a subset of nodes and they can be even cheaper does that roughly make sense also like a little thing that's like a bit different with the um i guess the security of uh assumptions of uh um the data layer with uh prototype sharding is like you only need like one node um that's available serving having the data available to be able to like sync up um whereas with um with like call data right you need like all your peers to have that data available so you can be able to sync up the tip right yeah yeah so you only need to see the data one yeah i think like that additional like that relaxed assumption makes things is also somewhat contributes to why like lobster cheaper in some sense uh like indirectly yeah right that makes sense um and i guess one one final thing i wanted to touch on is um there's been tons of like community enthusiasm for 4844 there's been like random people popping out wanting to help and i'll give a quick shout out to kane here from synthetics who's funded a lot of those that just come and help out with various things um what if if someone's like listening to this and they want to get involved in 484 what are like the things that are most needed right now and what are the the places that they should go or follow to to kind of be informed of the latest developments you want to take that you've got the best landing page i think well yeah there's of course eip 444. um it contains like links to several other resources that i think should help anyone like get wrapped up to speed with the spec um we also have a definite and yeah but that's also another good way to contribute um basically just running a node in the devnet um building running a node you know participating creating contracts sending transactions there's a faucet available in the devnet that makes it easy for you to you know fund yourself and do things with it um so that's where i think i would start it makes you like get familiar with the network before taking a deep dive into the spec yeah i was referring to um mophie's hackmd page where as instructions for getting up and running with the devnet so is this a great resource yeah shout out shout out to gabby and the ether dude discord for um setting up the faucet oh yeah absolutely and and a bunch of us hang out in the the charted data channel and the discord that's another good place to to interact with us i guess another one is just to write like learn the material guide and stuff just a lot of people like don't know what 444 is and they're not probably not going to read eip or consensus layers back a bunch of python code right they probably want to read something that's more like humanly readable i guess so yeah i guess like more educational material more resources those will be lovely yes and if people do those please tweet at me or mophie or terence and we'll add them we'll link them on the 484 website for sure yeah and if any testing experts out there we have we have a lot of work to do there um for example there's a i think that's one area where there's some uh it's an easy way to easier way to get started right you have to understand the entire spec you can pick a little piece of it and you know take a look at the code we've written and how poorly tested it is so far and then dive in from there yeah there's a lot more fruit there yeah so testing is a big one um cool um and so if you have testing experience uh please give us a shout uh on twitter emails on the website and we'll we'll uh we'll find something for you to do yeah and optimism optimization and benchmarking critically important here i mean this is this uh you know we've mentioned it a few times already um this this new crypto is pretty expensive um you know if you don't do it right you introduce denial of service factors um we want to make sure that doesn't happen so that's why that area of work is also important and plenty more to do there sweet um i think this is a good place to wrap up so we've covered what the eip is why it's valuable where we're at what you can do if you want to get involved um do any of you have any kind of closing thoughts you wanted to share with folks um i guess to close we really need our loves to be cheap if you want here to send your nfts and mince them really cheaply uh this is the way to go i think my only closing thoughts is that this is something we really love to target for the shanghai release um i personally believe it's um not too ambitious of a change to to to you know slip beyond that um but we'll we'll do our best to to make our case through you know stuff that works um hopefully we'll get there my thoughts is um i think besides withdraw scaling is probably the most important thing to work on post merge because like danny said we have sustained we have now we're sustainable and we have security so scalability is next and it's funny when i first started working on this space back in 2018 i wanted to work on sharding for the longest time and then finally now this is the time to work on sharding so yeah okay so yeah i think that's a great place to wrap up uh thanks a lot guys for coming on i appreciate you all taking the time and thanks again to eat global for for hosting us thanks for having us thank you thank thank you guys so much yeah thanks for all of the information and it was a really awesome panel appreciate you guys all being here thanks bye hi okay up next we have uh mark dawson from quicks um who is going to talk to us next hey mark thanks for joining us hey katie great to be here yeah thanks for thanks for being here for sure um all right i will hand it over to you you can go ahead and get started okay great i will go ahead and do a screen share um [Music] okay great can you see that katie sounds good okay um awesome yes we're good to go thanks okay cool hey everybody my name is mark dawson and i'm the co-founder of quix um and i'm going to talk about the optimism and ft ecosystem today so that'll be a little bit of a change of pace from some of the more technical talks um this one will be a little more fun and a little bit less technical so uh let me tell you a little bit about quicks um we're the largest nft marketplace on optimism we're a top five delegate in the optimism nft governance system so you might have heard about the token house and the citizens house were a top five delegate in the token house so we'll be voting on proposals for the governance fund if you're trading on quix the average gas fee is 14 cents um compared to open c or like eth main net which will be today usually about two or three dollars um a few months ago you might remember it was close to like fifty dollars so we're probably 10x or 20x30x cheaper than uh ethereum mainnet and we've traded about 2500 ethan volume so far we launched about um uh oh i think i lost my uh my screen let me make sure it comes back um sorry everyone um we we launched quicks in december 2021 right around the time that optimism removed its white list for apps and we are coming back um uh and basically i want to spend time today talking about like if you're a creator um uh like how you can get involved in the nft ecosystem [Music] uh or uh kitty can you see the screen okay yeah i'll assume yes okay and it's full screen yeah it sure is okay okay great thanks so much um so if you're a nft creator you probably have used l1 ethereum maybe solana maybe polygon um the optimism in its ecosystem is much smaller today than the other ecosystems but it's very exciting for a few reasons so why would you be excited to use layer 2 nfts [Music] layer 2's are of course like faster and cheaper and that sounds great but it's actually even more exciting because it's like 10x faster uh maybe 100x cheaper so it means that you're going to be doing totally different things the way i like to say it is that like where one ethereum is like you're driving a car in layer two like optimism means that you're going to be flying in a plane so you're going to be taking trips that you didn't take before um so what kind of trips might those be um so the breakout use cases that i'll highlight so far creator and smart contracts um mirror.xyz is one of the biggest web3 blogging platforms out there and historically they had one big smart contract on ethereum a net so that people um didn't have to deploy a smart contract for every new blog post that they made um that saves on gas but the problem is that your writing nfts can be mixed with like other people's blog posts which creates kind of a confusing experience on openc or marketplace and it can just like make your work harder to find so earlier this year mir moved over to optimism and now when you're creating a blog post on mirror you can deploy your blog post as its own contract on optimism it's going to cost only maybe like five dollars compared to maybe like 50 to 100 on mainnet and um on our marketplace you can trade the writing nfts and they're all their own distinct collections and like on ether scan they're all going to be their own contracts so that's pretty exciting and pretty creator friendly another cool use case of where to is is reputation in fts so just about a month ago rabbit hole which is a big um like web 3 education platform launched some badges on our platform so you can collect these nfts to prove that maybe you learned a new skill set or that you completed some tasks so maybe you learned about some d5 protocol and now you want a reputation in ft that doesn't really make that much sense to do on ethereum mainnet because you're probably not going to make a lot of money off of the like badge nft and so you don't want to be paying like five dollars to mint it um but on optimism you can mint it for like 10 cents and what's really exciting for me is these reputation nfts today they're mostly about collecting but eventually they could be about unlocking um maybe like some you know special skill set um i think all these like if we put more things on chain like if we give you more badges for all the stuff you're doing that gives developers a richer set of data to um build on top of so my hope is that like if i've done a lot of default activities maybe when i visit a web free site it'll kind of recommend to me defy activities or something like that so reputation nfgs are a great use case of layer twos um composable nftp's are another one that means nfts that combine with each other so dope wars is an nft project that was released actually like mid to late 2021 so they're really early on it they began on ethereum and they wanted to have these characters that they could equip items to so you know they have these characters called hustlers and you want to give them some like sneakers and you want and the sneakers are nfts and the house there's an entity and you want to combine them um the problem on ethereum once again is that you're going to be looking at like a five dollar gas fee for doing that and really as l1 ethereum gets more expensive the only things that you're going to want to be doing on one ethereum are things that have direct financial benefits so maybe you'll trade on one ethereum but you're probably not going to want to like equip or interact with your nfts um i know an ethereum so they built their own bridge actually which is pretty cool and bridge their collection to optimism and that's just the beginning there's a lot more i'll say about composable nfts later um another great use case is dragonia they're on chain gaming platform i think one of the first on-chain games on optimism so with these you buy these dragon trainer nfts and they can battle each other and it's like a little bit like pokemon or but much more simple what's cool is that the battle interactions happen on chain um once again as you're putting more stuff on chain and doing slightly less financial activity on chain you can't be doing that on ethereum because it'll be too expensive so you're gonna want uh a layer two like optimism um these are all live projects today um and there's a bunch that i didn't get to talk about um so let me go ahead and focus on the ecosystem we have um about 40 000 people trading today on optimism um 40 000 unique wallets i should say and a lot of those are in thailand and east asia which is pretty cool um many are pre-existing in atik communities so crypto testers uh which is this guy with the opi hat and he's green um the crypto testers is a community that's existed for a couple years and their goal is to test out new crypto protocols um they issued their community membership nfts on optimism which we're really excited about and those are trading on quits today um the other nfts um i'll point out here a lot of them are pure art nfts so this top one is uh called optimism and that's from our community in thailand they have a very active community and some really strong developers over there a lot of them will like review smart contracts that we put out and they've created their own launch pad uh this middle one that looks like you can't quite tell what it is that's called ganon land and that's ai generated art um by a project called fractal visions and that's based in colorado i believe in the united states um we have board town in the top right this is like the animal looking ghost and that is a collection again from creators in thailand um that one's really fun the bortone animal ghost project has the honor i think of being the first collection that was copied by like a scammer and put on open c so like a scammer um you know copied all the all the art and launched their collection on ethereum mainnet um and i always take that as a as a sign that you're doing something right when um [Music] when people want to copy copy your work and take it to other places because early when you're establishing an ecosystem of course the first thing that's going to happen is people are going to sort of copy what happens on layer one if you haven't tried an optimism but now people are copying what's happening on optimism and trying to put it on layer one um so that's pretty cool this one of the person in the helmet is called motorheads and that is a collection by a designer actually at op labs named jvmi and this collection is kind of about video games and early 2000s nostalgia so like there was this really cool mid page that had a game boy on it and that one's a pure art project that's quickly developing its own community and it's one that's kind of cut off a little bit this is a duck called usagi duck and that's from an artist in japan and she made this collection of 2000 ducks that all look very funny and whimsical um so this is a taste of some of the communities and artwork on quix today that i wanted to give a shout out to um and another cool thing is that many of these communities launched using our launch pad on quicks so i'll take a minute to talk about that our launchpad is a creator owned smart contract um tool that you can use to deploy your collection and will also create a mint page for you and you can have your own allow list and um you can also reserve some for your core team and do the drop like directly on our website if you're a little more sophisticated you can of course make your own smart contracts and make your own mid page and will also index and pull those collections into the marketplace so we're just a big secondary marketplace and we're going to pull all the nfts to pull out an optimism into our marketplace um so out of these ones ganland motorheads board town and usagi duck all used our launch pad and optimism created their own website and i think crypto testers did as well and hmism also created their own launch pad i think um which is pretty cool there's also a bunch of other launchpad tools um and creator tools on optimism so you can find them at nifty kit and mint flex and there might be another one um but yeah more and more creator tools coming to optimism um i'll say it's still a very tightly knit and community so a lot of these collections will work with each other um so like the motorheads will often like cross promote with the board tone nfts for example so you might be asking what's next for optimism nfts because so far the ecosystem is collections that you might see on mainnet um so like the art ones the tradable community pieces we expect that to continue to grow but we have these really exciting things in the pipeline so we have an nft bridge coming up that allows you to take an nft on layer one ethereum and bridge it over to optimism i'm gonna discuss that more at length in a couple minutes we're excited about these reputation nfts so that's something similar to what rabbit hole is doing these um sometimes we call them badges the difference here is that these are often earned rather than purchased and they're often not really good assets for trading because um they're they mean that you did something right so like if you completed a quest on rabbit hole there's no reason for me to purchase that um but it's still really interesting and i'll talk a bit more about reputation entities and then compose one of t's so that's what i was talking about with dope wars there's a developer named austin griffith he has a pretty big following on twitter so you might have heard of him and he has proposed this erc called erc 4883 which is a specification for composable nfts that can like fit together he created a collection on optimism called optimistic loogies where you can have these little characters and you can equip items to them like bow ties um or lipstick and there's a specification for doing that so we're excited to help people make composable nfts and we think that's really something special that you're going to want to do on a layer 2 rather than on mainnet ethereum so let me talk a bit about the nft bridge that is slated for launch in october 2022 so that's very soon um we have development was led by a software engineer named sam goldman on the quix team and he has uh contributed that to the optimism code base so that will be maintained by op labs when we were designing it our goals were first and foremost to be credibly neutral so like no fees or anything like that and then fast easy and permissionless we want the structure and semantics of the bridge and especially of the corresponding smart contracts to be very close to layer one and i think you'll find that that's a theme of a lot of projects on optimism because of the edm equivalents it makes it easy to be very similar to layer one smart contracts and projects and and we want all the tooling around nfts that you have on layer one to also work great on layer two so that means for example that every layer one collection that is a smart contract is gonna have a mirrored smart contract on optimism we're not going to try and make for example one giant contract um and deploy everything on that one contract and the reason for that is that a contract for nfts is often synonymous with a collection and we want to keep that boundary there we're using the official optimism cross domain messenger and what that means is that we're using the official optimism protocol to handle cross-layer communication this is different than using an oracle um or a service like layer zero layer zero labs created these fun omnichain nfts earlier this year starting with the ghostly goods collection and that was really fun but they did use a web 2 style oracle to handle the message passing which is cool um because you can send nfts um uh you know in minutes um but you you might have security concerns so this is a highly secure bridge um the trade-off there is that it might be a little bit slower than using an oracle so if you're going from layer 1 to layer 2 it's going to be about 15 minutes i think after the bedrock upgrade for optimism it's going to be more like five minutes to go from a little layer to layer one it's going to take a week because it has to go through the challenge period for the fall proofs so um for this bridge it's like highly secure and therefore a little bit slower um in the future there might be a fast bridge that uses uh like an oracle like larry zero but for this bridge for the very first bridge in the optimism entity ecosystem and one of the first official bridges in the um for one of the first official bridges in the layer 2 nft world generally we wanted it to be as secure as possible and then i'll just briefly discuss how it works a little bit um so it's an escrow system so it just locks your layer 1 nft on in a layer 1 smart contract and then sends the message to layer 2 and then mints a corresponding nft on layer two and then that nft can be passed around it can be traded around and then whenever you wanna leave optimism you can burn that nft and it will unlock the layer one asset and give it back to you so um this is how erc20 bridges work often and it's how the optimism your erc20 bridge works and this means that the asset is just as valuable on optimism as it is on layer one because owning the nft on optimism means that you own the asset on layer one so that is a deep dive on the nft bridge um i wanted to talk about how we plan to use the bridge in our marketplace so bridging there's a technical aspect to bridging which is what i talked about with the message passing and with uh the smart contracts but what might be even more interesting is the social aspects to bridging so nfts are a little bit different from erc20 tokens because they often have artwork and a community associated with it and a community it's kind of all you have in a lot of cases and so it's going to be the nft owners at the end of the day that have to decide to bridge to optimism to take advantage of the cheaper fees and new design possibilities with the lower fees and faster transaction times it's unlikely for example that the core developers of a project can just tell everyone to bridge and so for that reason we want to make it really easy for owners to bridge to optimism and also we want to make it easy for them to list across layers so this is like the borton project and this is how a collection is laid out on quicks today um you're going to see a bunch of the assets like in a grid format and i just put this here to show you what like a unified collection looks like and for layer one collections we want them to still be unified even if they're partly bridged so um someone might point out for example that there is a polygon in nft bridge and that open c currently supports ethereum and polygon and polygons much cheaper so why isn't everyone just bridging their ethereum entities to polygon and trading them on open c um since there's like technically support for that and the answer is that the platform is not designed to encourage bridging because once you bridge let's say you have a board ape if you bridge the board up to polygon that's going to show up as a completely different collection on openc and people might think it's a scam it's going to be weird all the activity is going to be segregated between the two collections um the you're probably not going to be able to get as high of a price for that reason because people might think it's less credible um and so the way we're designing quicks is we want it to be designed so that you bridge nfts so we're soon going to begin indexing and showing layer one nfts in your profile and quicks and we're doing that not because we're about to um also have trading happen on layer one because we were an optimism in ft marketplace but it's so that you can have the opportunity to bridge your l1 assets and begin trading them on optimism so if you have a board a for example on quicks what you'll be able to do is uh let's say you list your board ape on quicks then you'll be able to see the board ape and its attributes and its rarities and its activity in the same place as all the layer one nfts so there's still going to be 10 000 items in the collection um if they're if that's how many are on layer one and it'll show maybe like 100 of them have been bridged and are for sale um [Music] but we'll still do our best to pull in all the activity and all the context from layer one that you want to see and this is really important because when you're trading nfts getting context and um feeling like you're a part of the bigger community is really important so that is kind of how at the social layer we're encouraging people to list their nfts um on optimism to bridge to optimism and to use them on optimism and if we do this correctly then this is just going to be a pure win for everyone because the lower fuses on optimism mean that you can have lower price collections so on ethereum today if your floor price for your nft collection falls below a certain threshold the gas fees make it not worth trading anymore and that can be really sad for the collection so you might be able to breathe life into that collection by bridging it to optimism um because it allows uh trading to kind of continue even if it's only worth a few dollars and in fact a lot of the collections on optimism are only worth a few dollars um and we also allow creators to um create a custom contract for the optimism collection so um what i mean by that is when you bridge an asset from a theorem to optimism we deploy a standard plain vanilla open zeppelin nft contract for you that's great for trading and transferring and collecting um but let's say that you now want to add the ability to i don't know have your i don't know two nfts have a baby for example i don't know let's say these two um poor dapes can have a little baby baby board ape um you're gonna need to add some functionality in your smart contract to do that so if you're looking to extend your layer one nft collection um yeah cryptokitties throwback thanks jacob um if you're looking to extend your nft collection uh you might want to combine that with bridging so um because the way the bridge works remember is you escrow the layer one asset and then you mint a new asset on optimism but you can be minting that in a standard erc721 contract or you can be minting it in a custom one that adds some functionality so um the goal for bridging for us is that you're opening up a new design space you're doing things that you haven't been doing before um and that we're helping your community scale and the other piece that i'll add is that um we're looking to integrate our bridge into our launch pad so um we're kind of in a weird purgatory period today with layer two adoption because layer one definitely works and layer twos they work but there hasn't been a social consensus about which layer twos to pick or um you know their security hasn't been as battle tested as layer 1 ethereum obviously and so let's say you want to launch a companion collection for your 01 collection or you want to launch a brand new collection and you're excited about optimism but you want to give the community the ability to bridge back to layer one we're gonna add into our launchpad the ability for you to launch a collection on optimism or on ethereum with like bridging kind of pre-built in as part of the plan um now you don't need to have that launch but we want to like once again give the like design affordance to everyone to say like hey like think about scaling right now because when the next bull market kicks off gas use my crusher community so like let's pick a place at launch for us all to move to or maybe even you meant the token originally on optimism but you know if if you don't want to be on optimism in the future you can like bridge back to whether or not ethereum um you can kind of plan that out so we're still in the design phases of like what a what a cross-layer launch pad looks like where you get the benefits of launching an optimism while still having the ability to exit to layer one if you want so that's like a really active area of design and like product research for us um so that concludes the overview i have of the optimism nft ecosystem i have a couple minutes left so what i might do is just show you our platform um so this is the quicks website um and you can see we're featuring our mirror writing nfts we've recently been doing a o-p-o-g nft drop so we gave this early optimist we named him tim or timmy for optimism and we gave this to like people who were trading on quicks um you know in the early days uh and so it's really easy to get started um we're at qx.app and we encourage anyone who's interested to join especially one of the like lower cost nft communities and hop on the discord and if you're a developer you know consider checking out what you can vote on layer two and that kind of wraps up my presentation um do i stop some time katie or should i like should i keep going or turn it back over to you you all hey mark i'm gonna jump in for katie good to see you here um uh yeah uh i think we'll just double check if there's any questions or anything but uh you're free to use a couple more minutes if you want some more stuff to chat about um otherwise i'm just gonna check if there's any questions also give me one second here cool yeah i mean i can um i can of course do ecosystem spotlights all day long because we have so many um cool i'll just keep spotlighting some groups and ecosystem so this is uh called draconic egg and it was fun because these were originally egg nfts and then they hatched at a specific day which is kind of like a reveal these are actually generated with um i forget which model but it was like an ai that made all these um which is cool cool to me anyway um uh and we've actually seen a lot of dragon themed collections the other one all spotlight is dragonia and this is like the player in game um so these dragon trainers can like fight each other um and that's a great layer to use case because you don't want to be paying 01 gas fees when you're interacting with the game on chain um [Music] and let me go to this explore page um we're in the middle of the optimism quest if you haven't heard of that uh definitely go check it out this is um a way to explore the ecosystem and earn these fun nfts and you can look at your nfts on quicks so if you've traded on uniswap you get one um if you've used synthetics you get one there's you know all sorts of velodrome is another big decentralized exchange type protocol if you're writing any article on mirror that's going to deploy an optimism and show up on quix and it'll trade on quicks so a lot of fun announcements um like this group called sync swap um announced their mission statement and did it on mirror and you know they traded several thousand dollars worth of that blog post on quicks and on optimism just pretty fun and very cool how am i doing on time jacob i mean we're right on time i mean we can wrap up here really really cool to have you here mark i mean kwix was uh i guess at the time quixotic was the probably the first like meaningful transaction i did on optimism otherwise instead of just like sending money to somebody so it's really cool to have you be a part of these yeah i'm so excited and yeah i guess the easter egg is we used to be called quixotic but um we got the feedback that that was like really hard to say and hard to spell so we shortened it to quix um i like the name it's nice very thanks yeah it's uh yeah it's quick uh quick quick to transaction optimism love it awesome well thank you so much and i love the work that you're doing especially around education and you know tying nfts to to getting people to to learn more about uh and just incentivize exploring what's out there on the optimism network so um it's been awesome to uh to have you here thanks mark appreciate you being here thanks yeah it's been great to be here awesome okay cool well that about wraps it up on our end so thank you mark again and then yeah on behalf of katie and i thank you so much for being a part of this event and for joining us to her we hear more about the optimism ecosystem both from the people working on it and from some of the people that are deploying on top of it um see i'm really excited to to be finishing this conversation up um and thanks so much if you're going to want to watch this later it's available on our youtube page at youth global on youtube um but otherwise uh thank you so much for joining us and we hope to see you at our next summit tomorrow cheers you 