[Music] cool um i don't think that this these hello i'm angela i am going to be monitoring this panel um but i don't think any of our panelists need it needs an introduction so i will just go forward with the central question that i've opposed to our panelists and the question is how could mev specific to your layer of the stack positively or negatively impact mev on other layers and or the entire system in general so um i would like to hear first the panelists we do have 40 minutes so um i would like to hear each of the panelists first take some time to express their thoughts on this question and um then we can go from there so maybe we should start at the end with john sure i guess at the bottom of the stack yeah since this is at the bottom of the stack it kind of makes sense and then we'll work our way up right okay so uh is this my gun okay uh yeah sorry so uh hi everyone uh for those of you who happen to miss my talk earlier on modular blockchains and exploring mbb in a modular blockchain world i guess i'll give a quick recap which is that uh in the modular blockchain world you have multiple layers uh multiple layers that kind of have a dependence from top to bottom in a vertical way so at the base you would have a consensus and data availability layer on top of that you would have optionally a settlement layer and then on top of that you would have an execution layer and the execution layer can either settle to the settlement layer in other words verify its fraud or validity proofs there or it could actually essentially self-settle similar to a layer one by posting this data directly to the data there but regardless of which choice you make there's this kind of notion of verticality and this is slightly different than what exists today where you either have one single domain that's monolithic that does execution and data availability and consensus all at once or you have multiple domains that are essentially siblings to each other and this is what you know james and arjun were talking about a lot i think with their across domain and bridging talks which is that you know traditionally we've had either one domain or multiple domains that are siblings to each other in the modular world we're doing something slightly different which is that we have our domains that don't do the same things but they're vertically stacked and in this architecture because it's not just one stack of things or you have one execution layer on top of one settlement layer on top of one data layer but you might have like a tree of things where you have one data layer and five settlement layers and 100 execution layers connected to each of these settlement layers it's very dangerous if the base data layer has something that will bleed upwards or if one of the upper layers has something that will bleed downwards and then affect everything else so in that context uh mev is something that we have to be careful of because if someone constructs a execution or settlement layer that will bleed mev down to the base layer then this can actually interfere and disrupt how other settlement layers and how other execution layers on top of the data layer actually operate so just to summarize john it seems like john as the consensus and data availability layer you wish that because you do see multiple execution layers happening you do hope that they contain mev at the top layers of the stack of the execution layer yes the ideal situation is something similar to what the e2 researchers are doing with proposer builder separation where you essentially contain and you know compartmentalize mev into a particular layer and you don't allow it to bleed down or up so to the execution layers maybe you can how do you feel about john pushing mev towards your your lair maybe ed first um i guess i don't conceptualize what we do as an execution layer to start with um so um but here's the way we think about mev in our system first of all um we try to prevent in the design of our system extraction of mev from our users against their will that is a cost that our users would experience if we don't try to stop it raises the cost of using the system and it creates inefficient incentives for them to optimize for resisting mev extraction instead of optimizing for efficient resource use on our chain which is what we want to incent them to do for everyone's good so we think a lot about the incentives and what incentives it creates our system like most rollups uses a sequencer the main purpose of the sequencer is to give people certainty very quickly order of one second on what the ordering of their transactions will be and that also affects the opportunity for mev extraction that the sequencer might have because whatever it does it has to do it very quickly rather than accumulating a big mempool and then trying to optimize over it currently we run a centralized sequencer and i think there are basically three ways you can do this you can be centralized which is what we currently are and we ask our users to trust us and try to be transparent that's obviously not the long-term place we want to end up the second thing we could do that we don't especially like is serial centralization where parties take turns they're elected to be the centralized party and and they just take turns and you hope that things kind of average out over time we don't like that approach because we think it leads to a lot of extraction of mev instead the approach that we'll be moving to over time is a distributed is a distributed sequencing model where um we have a committee of sequencers and provided that k out of n of them are honest then you're guaranteed a sort of formally specified um distributed first come first served policy so the goal of this again is to minimize the amount of mev that gets extracted against the will of our users we think that's a really important goal for systems we don't think maximizing mev revenue extraction is is the goal that we should be aiming for rather we'd like that funds those funds to remain in the pockets of our users oh i'm the other okay well let's tie it back to the execution layer question that we kind of started with i i agree and ed and i are building pretty similar protocols and a lot of that applies to optimism too i think i will say though that i do agree with you john to go back to the original point that yes it is the case like when we when we posted about sequencing on each research the thing that we were doing was saying we should move the sequencing out of l1 and you know we have different you know terminologies now and data layers and execution layers and it's funny i i think i would consider optimism and arbitrary execution layer or maybe there's a different term there that we're that that you want to see yeah exactly yeah exactly um so it is the case that we move mev uh into the other layer and also i think i go backwards in the up and down of my layer two as everyone else that's another thing that always confuses me i don't know you guys do you guys go up for layer two or down for layer two oh we got some downs back there it's very contentious um i will say that we're introducing mi mev by creating a new uh you know asynchronous domain though i will say that so the existence of this other domain does introduce cross-domain med which i do think is interesting and whether it's you know the the kind of panel topic is whether that's introducing md at other places it's hard to say if cross domain mvv is a part of the chain that it you know is being going across or not i don't know but it definitely does introduce some of you there cool so it sounds like um the execution layers are aware that um mev will kind of concentrate in your layers i'm curious like um in in your previous presentation you said that arbitrom's approach to fair sequencing kind of moves the in in a centralized sequencer it's very obvious that the mev is in the centralized sequencer where does it where does it like bleed into in a more decentralized sequencer world for the execution layers is my question yeah i mean i think basically what i said is that it will go to the edges so if you have a model like like many of these fair sequencing protocols where there's a distributed set of parties and it's basically the sequence is a function of the what they all submit then the mev will move to manipulation around those layers be the networking stack and trying to get your packets in first or doing other things but that's definitely where it will live i don't know that it is a different layer because it kind of is the layer of sequencing that you've built for the l2 so by edges do you mean it would move to for example the 20 it would just be split between the 20 sequencers right exactly and then the mev you know searchers that are trying to do this would then be going you know going to that part of the network right okay i think we need to be careful in thinking about this not to pre-assume that the amount of mev that gets extracted from users is independent of what kind of protocol you use right one of the reasons why our protocol is designed the way it is is to try to reduce or minimize the amount of mev that's extracted from users right and so you can ask where the opportunities for mev might live but you should equally ask how much opportunity is there for mev extraction some designs tend to maximize the centralization maximize the efficiency of mev extraction from users ours is designed to try to resist extraction of mev from users now that is can't be done perfectly you can debate how best to do that but i think that's the right goal in how to think about designing one of these protocols right right total of course and it is difficult to do that especially when you're trying to build something like google which is a completely application generic you know platform so there's definitely challenges there too yeah yeah and i guess i would say that um the one thing we would not want to do is come up with a protocol whose effect would be to put mev extraction power into the hand of the of the party who's most efficient at extracting it in a centralized way i think that's the worst thing that we could do in terms of trying to reduce mev extraction i think it's important to distribute it i think it's important to distribute it administratively but also geographically because the network distance among parties also tends to act as a break on mev extraction um and i'm happy to dive deep on that with people afterward if uh if you're wondering why that's the case cool so before we introduce bridges to the mix um john as a as the consensus datability do you have any thoughts you want to share no i mean both of these uh execution layer teams or roll up teams if if they don't want to be labeled as such brought up some good points right which is that just being a roll-up doesn't inherently reduce the amount of mev but by making systematic changes to how the leader selection process works and also within the leader selection process uh how blocks are built then you can have a reduction potentially in mev uh or maybe extracted from users which is definitely one of the benefits of this modularization approach because it allows us to experiment very quickly and iterate very quickly on these experiments in ways that just would not be possible in a single monolithic system okay so introducing a little bit the bridges so um james here is working on the messaging part of the bridge and on top of that is connects which is the liquidity layer um and let's start let's start with jane yeah so i think the original question uh back at the beginning of the panel was how does mev on our layer affect the other layers and you know the existence of bridges actually creates a kind of crazy weird amount of mev on the base chains they're involved with one of the reasons we don't see safety or liveness failures on base chains very often is because they're awfully hard to monetize um when you cause a reorg a safety failure or a chain halt a liveness failure it's difficult to like extract value out of that chain you've just busted bridges change this equation uh they make it so that just breaking the security model of some chain they're connected to is potentially very profitable on another chain uh for these like safety and liveness failures the difficulty is getting your money out of a busted chain but the bridge solves that for you you can't use coinbase to get your money out you can't use like usdc to get your money out they'll prevent it you can't use an external short to get your money out because crypto prices are completely irrational when iota had a liveness failure for example the price went up that day um so bridges give you an opportunity to bust a chain extract your money out to another chain and go about your day with a lot more money so we're creating like sometimes egregious amounts of mev on these base chains in ways that potentially interfere with their operation one of the ones i think is going to be productionized in the next year or two is a forced synchrony between proof-of-stake chains anytime you have validator sets staker sets overlapping between two proof-of-stake chains there is an opportunity for them to take actions for the shared validators or stakers to take actions on both chains simultaneously and enforce that those things happen atomically and uh they are the only people capable of doing that so for example for any two cosmos zones if there is a staker with 10 on each zone every 10 minutes that staker gets a chance to set exactly what happens in both zones at the same time extract all of the mev for both zones in a cross-chain way that nobody else can replicate so in this way like the existence of multiple domains and bridges between them creates an incentive to mess with the liveness of tendermint chains in order to extract value to delay blocks so that your shared window comes around more often or to cartelize the validators to create you know more of these mev multi-chain mev windows so we're going to see like bridges start to interfere with the safety and liveness guarantees that the base chains provide and it's going to be productionized like within a couple years easily yeah um i guess following on that i think one of the things that might have started to become clear to people over the course of that is that like uh the the higher you go in the stack the more of it that's kind of where the source of mmv comes and like when you get to the liquidity layer of bridges like we are the problem any any mev that ends up bleeding like out of our part of the stack and is is what everybody else on this panel has to deal with um and and that's that's obviously quite difficult um and i think i think a a way to think about it is that there's um each each layer has a potential for how uh uh the extent to which transaction ordering could affect um uh the the like like basically mev um but uh it's that potential isn't fully met unless you actually have mev bleeding out from the higher layers and i think this is why john was saying that it's it's better to try to constrain mev to the higher layers in the stack because as it gets further and further down in the stack it becomes more and more devastating and then also the the possible ways that you have to try to mitigate it uh get constrained further and further um now the the flip side of this of course is that with bridges uh and especially the liquidity layer of bridges which is really where like uh you you are extracting value um this is not always possible uh it's just fundamental kind of distributed systems problems that you start to run into and i kind of touched on this in my talk earlier where i talked about what it looks like to build a cross chain decks and why that really really sucks um that like you end up running into all these weird problems with synchrony uh assumptions and things like that where where it is just effectively impossible to stop from mev being extracted unless you find better ways to develop the applications in the first place um so i think one one of the things that that maybe like i hope we end up talking about more on this panel is just like what are the ways to kind of build safeguards into each layer because really it's the connections between the layers where the the the problem of mev ends up compounding go ahead john so a lot of this talk around you know multiple layers multiple demands multiple things it sounds a lot like the best approach to building a blockchain is in fact something like solana or you just have a single layer that does everything and you don't have to worry about cross-domain anything do we think that the solana approach is better than this multi-layered and multi-domain approach criminally underrated imo i think in the ethereum community we ignore a lot of the very legitimate stuff that solana is doing and john deere credit fuel does some of this too to do make better execution or we can have larger synchronous environments i don't think it i don't think it's like better to make this centralized thing but i think we ignored the interesting thing salon is doing because of their failure to meet the like values that i feel you know believe in in ethereum you know so basically ben confirmed to have solana bags never never i technically like some of the things that they do though it's true and it's easy for us to ignore that as a community i think when we see you know five validators or whatever the hell it is because rightfully so that's a reason not to get involved right but there's interesting things going on there uh when are we gonna see bpf canon oh you know it's coming soon um actually i i really agree with that notion because i think there is there tends to be a lot of really interesting experiments that are happening around these kinds of things and other ecosystems that that we as a community tend to kind of ignore because we have you know a lot of bag holder rise um uh getting getting back to your original question i think i think like it's not to say that there's like necessarily one better approach or worse there's there are trade-offs and i think in this case the trade-off may be that you're swapping out one demon for another uh you know we're we're saying you know the the kind of like downside of this this module approach is like more vectors for med but in the monolithic approach your your downside is like this these fundamental problems of decentralization and at a certain point we're going to have to figure out the trade-offs between those two and figure out how to balance them because this is like effectively this is one way to think about this on a macro scale is that this is the cost that users are paying or that the system is paying as a tax for decentralization as well yeah i think it's important when we're talking about monolithic versus layered to sort of separate two notions one is is the system structured in layers that interact so when i see john's slides and he uh he talks about the layers that you know in the modular approach and i look at the design of arbitram i recognize that we have within our system things that are reasonably well modularized from each other that play those roles although we used very different terminology so it's not that the design is monolithic the challenge really is to have layers and to think in a modular way but at the same time to have a design that where you are thinking and optimizing across layers for things like mev control and you need to get there somehow and i think you'll see a bunch of layers sometimes you'll see multiple layers inside the same product sometimes you'll see them in different products but i think from an engineering sort of how we think about design standpoint you'll see similarities i happen to think that the layers that we provide together and co-design really do work well together and but from an engineering standpoint i think there's a lot of similarity what john's talking about and and it's a really valuable way to think uh i agree um the whole like modular narrative is like we've had this single blockchain ecosystem for so long what if we just unbundle it a little bit uh and you know to john's point the the response to that is well what if we just rebundle it a little bit in a few more years uh in like normal tech development bundling and unbundling is a cycle that people do every time they get frustrated with the current paradigm we unbundle things so we can introduce specific optimizations without worrying about other parts of the stack and then we go hey wait a second why are we spending so much overhead on layers when we could just bundle everything and we'll be a lot more efficient to doing what i want so you know maybe we'll see an unbundling period and then a rebundling period uh in a few years and solana might be a good choice for rebundling i think it's partly about standardization as well if we're in a phase where there's still a lot of uncertainty about what is the best interface between layer a and layer b then it makes sense for different people to experiment try different things and and it will necessarily be the case that if i have a different idea of what the interface between two layers is than ben does that my layer a won't interoperate with his layer b and vice versa just because we have different ideas about how the layers should interact i kind of feel like that's where we are now that there is not i think a clear idea at least for some of the interfaces as to what the division of responsibilities or contract should be between those layers and so we're in this period of innovation yeah we had a good conversation about this at modular summit yesterday um essentially five years ago when oh it's more than five years ago six or seven years ago when ethereum started uh we could not build a modular blockchain because we had no idea how to build a blockchain in the first place just making it up as we went along there was no prior art we didn't know what layers were where they were what they should look like and so really the project of the modular blockchain stack is you know like figuring out those layer boundaries what's appropriate what's useful and trying to come to shared standards about them so maybe you and ben will always disagree but i think we can get there on a lot of these layer boundaries especially like cross chain things i think we can get there on like just the cross chain communication channel versus cross chain app layer boundary that one's easy to see these things always clarify over time yeah and you know when we started building ethereum they were completely unclear and we were all kind of dumb we still are but we used to be too yeah cool so just to get a a little pull here um so who on this panel thinks it's better to in this current phase let's say in the next two to three years to unbundle further or who who thinks to so the two choices are to unbundle further or to bundle back into one thing so who who here thinks that we should keep on bundling are we allowed to just hedge our bets and buy some soul and some eath and then we're good regardless of what the outcome is all right so it sounds like most of this is yeah so i guess i would come down in the middle i think there are some places there are some interfaces where it makes sense to bundle and others where it doesn't i think for example with respect to bridging i would not want to try to bundle all of the bridging into what we're doing because i think innovation in bridges and having multiple bridges there is what serves our community the best because because multiple bridges can co-exist more naturally in designs like this are we allowed to have nuanced opinions on stage no sorry um uh yeah i mean i actually think that this maps really will too like how we think about bundling and bundling in web 2 as well i mean like with traditional tech startups i mean this is something that people have been talking about for a very long time and that's just like a the way that the market evolves um and and usually like the idea behind the whole unbundling and rebundling is like in the process of doing that you actually optimize and this is this is like the meta process that we are following intrinsically the epistemic process that we're following intrinsically to understand like how do you actually optimize these systems and to do that you have to break them into small pieces that kind of don't really work um figure out why they don't work uh get rid of the stuff that doesn't work and then like put them back together again and then break them again and so i think you know it's like i agree that we are in the part of the cycle that where we need to be unbundling for the next couple of years because we've spent the last you know almost over a decade now um like with a single bundled monolithic blockchain and uh and now and now we need to figure out why we made those decisions in the first place i guess why satoshi made those decisions in the first place cool yeah so i do so i think that it's inevitable that um there will be mev opportunities across the stack in between um so kind of to for actors to be able to extract the mev opportunities that exist between you guys there is a strong sense like push towards centralization and like bundling to be able to extract such opportunities how can the five of you work together to keep to sort of steward towards these this unbundling future that you guys all want i mentioned this in my talk but i think this is actually the first time that there's ever been any like formal conversation around cross-chain mev and like mev throughout the stack and how it actually fits together um i think a big part of of like fixing this problem is to like keep having these conversations because i think over the course of this panel like i am already starting to build a better taxonomy internally about like how we should be thinking about this and i'm sure everybody else here is as well so i think i think it's just like a it's a topic that just really needs a lot more discussion and it just hasn't existed yet so thank you for putting it together yeah i think we also need to figure out better what our goals are quite frankly it's not immediately obvious to me that all the types of crushing maybe are bad like is arbitrage is like making the price more efficient and the same across a bunch of different chains a bad thing maybe not but it is cross-domain mev i think one question is maybe that we want to do it in a way that doesn't centralize right so like actually one thing that i didn't hear um the bridge people talk about on the top of mev is that you guys are at least responsible for funding some of the cross chain mav right like the ability to move your funds back and forth between chains faster does allow you to have funds on two chains and execute a cross-domain arbitrage on them right and i think that i don't necessarily think it's even a bad thing that people are going to make the prices you know more efficient i do think that if we do it in such a way that only the richest people will be able to do that or they're at some kind of advantage that would be a problem so maybe your goal as a bridge is making that really democratized and not requiring a high budget um and so yeah i agree that we haven't had a lot of conversations and we don't even know what's right and wrong uh i reject the idea that you can make it not require a high budget uh the nature of transactions is that the transaction fee will rise to the level of value that you can extract from transacting you can't sell the five dollar arbitrage for a one dollar transaction fee can't be done no scaling system will enable you to do this no multi-chain system will enable you to do it you can't sell five dollars for one dollar uh i do think that you know minimizing the impact of mev on users trade prices across many domains is very important we think about it a lot and it's one of the reasons why arjun and i both gave talks on why cross-chain dexes are such a bad idea yeah it's very very unfortunate that we didn't coordinate beforehand too good perspective i feel like my talk was a good introduction for yours i think it's important for each for each of the players each of the components to be really clear with the others about what it is that we provide and don't provide and what are the mev extraction opportunities that would exist in our systems and who might be in a position to exploit them you know in general it's good to try to minimize or damp mev extraction opportunities and we want to do that as much as we can we certainly don't want to be in an architecture where we're maximizing mab extraction but if but we should be really clear about where we are so that we don't get composability effects or composition effects across these systems that are really unexpected or unfortunate yeah so i think going forward as as we said before one of the primary things that will probably help towards mitigating this mev is interpersonal collaboration and team collaboration into project collaboration because we're moving towards an era of modularism and not maximalism right you see on the bitcoin conferences nowadays right they're very much maximalists they refuse to collaborate or even talk to other projects that are also doing research in the blockchain space you know for whatever you know pure periodical reasons or whatever right so i think you know getting different teams different projects that are working across the stack together on stage like this and also in the audience like this is critical to actually moving forward and mitigating these issues [Music] awesome so we have five minutes left um [Music] thanks dad i was afraid you didn't bring it i always got my melodic on me yet come on now cool um we have five minutes i have i asked phil diane if there was a question i should ask that i could not ask each of them individually um should do we okay does do you guys want to hear the question all right the question is um what is your least favorite trust assumption that some other protocol on this stage makes do we have to not make it too we have five minutes to answer quickly let's let's start with you then ben i mean it's a love-hate relationship my least favorite is the fact that we have upgrade keys we've done a lot of writing about how to get rid of them it's not an easy path ahead so we make this assumption too but it's the one that i hate and that scares me a lot and you know um that's what i would say i'm actually less concerned about the the the me assumptions than that one yeah i guess i would give the same answer yeah others do make this assumption we make this assumption too for now right and we're all trying to we're all working to figure out how we can safely maintain the ability to respond to security incidents um quickly and safe and uh and securely um while still not holding too much centralized power ourselves unlike roll-ups um bridges have it even slightly worse is bridges require a trusted setup um and in the case of any uh events like any issues with the bridge they require the trusted setup to be redone which means that the human like held keys for upgrading or affecting the system are you can never get rid of them there is no way to remove the admin or upgrade keys from any bridge the only thing we can do is mitigate their impact on the running system and mitigate the uh like potential damage they could do um outside of my own protocol because i think that was the question uh my least favorite trust assumption is the relationship between roll-ups and data availability um that there will be roll-ups uh for users uh require the presence of you know some inferior like uh trusted data availability provider if that person doesn't exist you're not able to use the roll-up without running an ethereum node plus a roll-up full node it just like feels messy to me it's not a bad trust assumption it's just one that i you know tastes bad it's it's not very harmful it just tastes bad yeah so i guess i'll i know i i generally agree with the the upgrade keys assumption that means a pretty standard one this is where hopefully the notion of sovereign rollups on top of the data layer directly will help because they retain the ability to harden soft fork through off-chain governance while sharing security but more specifically you know while other engineers repeat repeating whatever else agrees with clearly upgrade keys uh it's uh the notion of synchronous communication which i think every protocol here probably makes it's not that it's inherently bad assumption but it makes analyzing the protocols very complicated if we have to assume there's synchronous communication between honest parties especially when one of those honest parties must be some layer one blockchain that has very weird guarantees around the availability of its block space so which is really hard to design around um um yeah i mean yeah i think i agree uh synchrony is definitely extremely uncomfortable especially as you start to deal with different domains um or different execution layers um yeah i think that i have two that i really make me super still make me uncomfortable and i know like i have rational justification for both of them why i shouldn't be uncomfortable but they still make me a little uncomfortable sometimes um the first is the way that we think about fraud proof windows over different domains and different execution layers i think um you know it's like most of the fraud proof thinking has been done on ethereum and i still haven't gotten to the point where like um i fully am comfortable with it on other domains um i know james and i have had a ton of discussions about this and he keeps being like no it's it's fine and i'm like ah is it um uh and then i think the other one that that uh you know this one might just be because i don't understand it well enough but i also have some concerns around like um the security of the data availability there just as like what additional trust assumptions are being introduced by the fact that you have an entirely separate potentially an entirely separate set of actors that are hosting data right we're just on time ed did you want to give a okay oh i'm good i already i agreed with ben i think that's the right answer um upgrade piece yeah all right awesome thank you everyone for coming and thank you all to panelists [Music] 