okay numerical analysis of non-atomic trade execution protocol uh excellent here we go so importance of doing a numerical analysis for a t-wan uh otherwise known as a time-weighted average Market maker is actually considerably more important than your constant product Market maker that you're all familiar with like uniswap and the reason for this is the qm trades are much much larger or expected to be larger where typically in the past we've seen orders as large as maybe one to five million on what I call a c-bama constant product automated Market maker in tum because the orders are spread out over time and we'll go into this um and the orders are larger if you don't get your numerical analysis right you're going to be losing a lot more capital for people and you don't want to do that um so before we dive into this numerical analysis I'm just going to give a brief overview of cpam and dpam just so that everyone's kind of on the same page so so a constant product automated Market maker that I referred to as cpam has this interface that you see and the main way people interact with it would be this mint burn and swap functions um inside of the pool itself you have two assets token X and token y here and then you have pool tokens for people who are providing liquidity that people are trading with when people perform a swap the swap is bound by the constant product formula um so token x times token y the reserves each is a constant and that maps to this bonding curve that you see here uh on the right which determines price action um another part of it to know is there's also an instantaneous price that's determined by the reserves divided by the reserves of the other token however in practice if your trade is much larger than the actual reserves in there you're going to move the price and get it very different effect price I'm not going to go into more detail on this the unit slot papers are really an excellent source for this um now let's just consider the price action of a simple trade so we have a pool and the reserves are 100 of each token and at this point we expect the price of token X or sorry the token y to be one token x per token so now we're going to swap 500 and notice that I said 500 token because that's much bigger than the 100 token that are in there in reserves for the opposing token so what happens when you do this is the 500 token get added to the pool and I'm ignoring fees and a bunch of other complexities and we move along this bonding to this disadvantageous point here where now the silicon X reserves are 600 and we've significantly reduced the amount of token y because we've put 500 token X in and we've removed 84 to get to the users as proceeds so now there's only 16 token y remaining the thing that's that catches the person trading may be expected to pay one token per token they ended up paying six tokens so that's six times higher and you'll hear this referred to as slippage um so now I'm going to show you the time limited average Market maker and how it differs and what you see here is you have the same three functions that we had before mint burn and swap but we also have a partner Swap and the partner swap kind of ties into the last presentation because it's basically like a paid for overflow mechanism and we're partnering with rook and like actually to allow people to give options to bidders and we reduce the fee that they pay to incentivize swaps and the reason we do this is those swaps form Arbitrage that actually move the trade back on the bonding curve to a position that's advantageous where people were expecting to pay the prices they have so rather than suffer massive slippage by incentivizing order flow from these arbitragers we give long-term Traders a better fill and the other benefit is when he was talking about tips we take a portion of those and we feed those back into the pool liquidity so that the liquidity providers are now getting a benefit and what this net effect does is takes the Meb and it gives it to the pool instead of to the microwave language and so that's how it ties into the the previous discussion so the other major thing that you'll notice is on this interface is the long-term swap cancel and withdraw and the pool is also significantly different in addition to the X and Y reserves and the pool tokens you have the order and proceeds tools so when you put an order in it doesn't execute immediately it's not Atomic it executes over time and your orders capital is stored in this order pool and then it slowly moves across into the proceeds pool where you're able to withdraw so just some major features the Highlight it's not Atomic so it's occurring over time not just in one transaction in one block you can withdraw your order one or more times and this is actually really interesting because in future we plant that kind of adapt that to streaming applications where you can now have a Time weighted order that goes across multiple pools um you can cancel the order in working with books what we discovered is you want to be able to delegate the management of an order because a dow has difficulty getting attention of books to do things so you might want to say we want to put in a limit order let's have this person be able to withdraw cancel it but the capital is always going back to the originating address so this way you can kind of delegate the work of managing the order to somebody else or a service um there's some significant features that Paradigm built into this uh when they wrote the paper to reduce gas use that's concurrent opposing swaps so a conventional pool can't do this today where you swap in two directions from X to Y and Y to X in a single transaction cm is able to do that in long-term swaps your orders are pulled so if you and I are making the same trade in the same direction our orders are pulled together and we only pay gas to enter the system we don't pay additional gas to keep having that trade happen and the person who pays that gas is actually the person's X on the pool next and that's the lazy evaluation part so if nobody participates in the pool or sends a transaction to the pool nothing happens the state is basically where the last transaction left it even though as Time Marches On the reserves and trade is actually changing it's virtual so we have a bunch of looked up functions for that and then the last thing I'll say is that there's an index expiring update and what I mean by that is trades can only end on certain blocks if they ended on every block the amount of gas to do this would be too hot so let's take a look at the same price action on the t-wan pool but now in a vacuum and the reason I'm doing this in a bathroom is I want you all to see with the math and the bonding curve is effectively the same when nobody else is present so we have the exact same trade we're doing 500 token but now we're doing it over five different roles so what that means is we have five discrete points where we're injecting 100 tokens into the pool and you can see on the bonding curve here that we have five movements across the Curve the math is actually exactly the same in a vacuum because nobody's interacting with the pool so you get the same terrible proceeds of 84 instead of the 500 you expected from the instantaneous price now this vacuum scenario is not real and the reason it's not real is because this is distributed over time you're going to have arbitragers like the folks from the last presentation come in and sell bids and what will happen in this hypothetical situation that I've shown here we move two intervals worth of price and one of these arbitragers somehow wins their auction and says hey this is great I'm going to move the price back and that's the green line where we've now moved back on the bonding curve and for this five interval example if you crunch the math what you end up discovering is that the proceeds are now 184. so that's much much greater than the original 84 and your effective price is about 2.7 tokens which is actually less than half that you paid in the previous uh interchanges so there's a lot more detail I can go into in TM but I'm not going to here because I don't have a lot of time but I think one thing that's important to understand is just how uh the process goes so any of these transactions that get sent to the pool in Burn swap partner long-term spot cancel Etc results in the execution of virtual orders and in order to do that we convert we compute preserves at the last time something was happening we execute all the virtual orders that are present and that gives you the new reserves to have the new transaction and act upon and in this execute virtual order is that lazy evaluation the pooling of orders and the index expiry that I was talking about another important thing to note that I'm not going to go into too much detail the longer the amount of time since that last order the more gas you're paying and the reason is you have to run the loop iteration and the loop iteration has a fixed gas cost so it's like n times that gas cost foreign full on into the numerical analysis I need to show some optimizations that took place in this um and this one has to do with virtual order calculations so you're all familiar with no active orders that's basically no math you don't have to do anything a unidirectional active order so that's where you're going from X to y or Y to X but not both that's exactly the same as the cpam math and we kind of just show that as one trade and then the more significant case that the Paradigm paper addresses is the twamp valuation and that's where you have X and Y and Y to X concurrently the arithmetic for that is significantly different and I'm not going to dive into this heavily but this is the math and the Paradigm paper and you can see that it's fairly complex there's exponentials and square root functions all over the place it's about 15 operations and the team from frax came up with a fantastic approximation here and that reduces it to a very very simple I think it's like three or four operations and in Desmos there was a great comparison done and you can see that it really tracks very closely to the original function we've kind of gone beyond this with our editors but or sorry Auditors but um I just I can't address that today in terms of time um but suffice it to say you can see it tracks very closely and the trade-off is definitely worth the gas savings and the complexity in the compute analysis another um thing to show with that is you can see that the math when we do an estimate of the gas used we cut that by literally a 10 uh 10x so we go from about 17 000 gas to 1800 gas from that exact same arithmetic the next optimization that I think is significant for us to point out is scale proceed storage so if you were to profile this and take a look at your gas use that algorithm that I just showed was about a third of the gas this is two-thirds so what's happening here is every time you execute virtual orders at one of those index points that I was talking about you're storing two slots of data two slots of data is going to run you 40 Grand in gas in the worst case so what we did is we took that and we said what if we store that in one slot and that the reason I'm pointing this out is this is what made our numerical analysis much more tricky when you have 256 bits to play with you can get the scaling factors wildly wrong and you've got the resolution and Headroom and dynamic range that you can get away with murder when you cut it down to 128 bits you can have a disaster ensue and you're about to see the disaster but just to show you how much this is worth this is a different uh gas use comparison and what's Happening Here is we run our benchmark test and we Compare the numbers before and after you can see that we're getting about 20 Improvement in gal production so that's why that's Incorporated so now we can kind of dive into numerical analysis so our numerical analysis actually started into code uh neither of us are solidity deaths here let's come from the space both of us come from this little indomie so what we started doing was we actually um had a variable naming convention and the reason we did that is solidity variables are best when they're native 256 bit but a lot of the values that we contain in them never exceeded another note I'll give you an example units called 112 bits so we would have been a suffix u112 the values and so forth and then we added tags to explain sections where we had uncheck math now our protocol is implemented in balancer which is solidity 2.7 and what that means is all the math is unchecked unless you check it yourself so this was really really helpful for us to actually see and you can see I don't think you can see this text is probably too small from here but by looking at the suffixes you could actually tell if you were going to overflow based on the expectation and you knew where to put safe operations and unsafe operations and by having these explanations in the code it made the Auditors a lot happier because they could see that thought kind of went into this um we also use tags to explain where we expected over and under flow so in the oracles you would expect to overflow because it's actually the difference between the values it's not the actual net value I'm just going to kind of skip over that for now but one thing I'll add is where we had fractional bits we actually added that to the suffix as well so another thing that we did is because both of us come from the Silicon domain we actually started doing system diagramming as well and you can see the code that I showed you previously here is actually diagrammed here in a schematic and the schematic is annotated with the container type so the 256 bits as well as the expected amount that it's representing and by looking at that in all of these ways you can actually start to get a picture of where things might go wrong and where they're right so here you can see we have unchecked math and the value coming in is colored in blue and the reason is we're like we think it's 112 bits but we don't know for sure and then we perform a check subtraction and that exclamation mark there tells you this thing froze if things go wrong in the pool and this was actually quite useful in the broader context because that block that you just saw now is put into the larger subsystem and we were able to examine it in the context of other scenarios and understand what the side effects were and stuff and getting this alternate perspective kind of gave us an ability to carefully review all of our existing code and it revealed a lot of gas optimization opportunities um and as I mentioned before we had color-coded areas that were kind of like to Do's in the code as well so like where signals were red we knew that we had to really find out if that works so we would actually write test cases for that to make sure that that was checked elsewhere and because we're using balancer some of their code did a lift for us and did that check as well um so from there we kind of went into Focus area abstractions because the schematic level is just too low level to look at this so This diagram here kind of shows the main area that we were concerned with and we were able to kind of break it into two parts the update algorithm which is like the cpam and tem math and then how we store the scale proceeds and this whole thing shows basically two orders coming in and the sales rate gets calculated per block and then merged this is the pooling and then you can see the reserves come into here the iterations of the loop and then the storing of the scaled proceeds and we had static scale factors of 64 bits initially that was my first yes because I'm not really from the space I hadn't really thought about parameterizing this yet and I was really really concerned about this this was like kind of keeping me up at night it's kind of a weird thing to keep you up at night but I think it kept me up at night for about a month or so and I was wondering like would the gain or the amount of tokens coming through this overload this and cause these scaling factors to overflow and uh I was walking around and I was thinking about this a lot and I started to realize things about the cpan model that I hadn't really thought and it's lost conservation of energy is really applicable here and what that means in this case is tokens can be created or destroyed if I'm creating tokens or I'm destroying token then I'm replacing the functionality of the mint and the burn in the erc20 contract for this so what that led me to do is avoid any operation that rounded up because what happens if you're rounding up is you're creating tokens and we have another method that I'm not talking about in here but it gives us a degree of Freedom where the reserves are not actually stored all of the accounting around the reserves are stored and those are calculated from the balancer notion of the Vault and what that does is it allows the rounding errors to disappear and we no longer create or destroy tokens with our intermediate math um and then the last realization about this is that the algorithm is basically bounded input bounded output so if I throw 112-bit value in I can't get a 256 bit value oh unless my math is wild wrong and the most I should be able to get out is the amount of Reserve that's in the pool and that's actually the limit that you can't reach because it never quite hits that point so the real problem that we were trying to figure out is how many trades can a person perform in this system before a user loses proceeds and a user loses proceeds if these scaled proceeds overflow and the reason is it's the difference between those proceeds that determine how much the user gets their share so if they overflow is twice in that period the user misses an entire segment of proceeds so you can't have this thing overflow and our maximum trade length is five years and the reason it's five years is we wanted to kind of support the VCA example where you could dollar cost average in and have longer term positions in play so more than one overflow huge problem um so what I did was I thought okay I'll do a concrete example I'll use rap e to wrap Bitcoin and that has 18 to 8 decimals um and let's see how my 64-bit scaling example works so this is obviously way too complex to get into here but basically we did the calculation job for that trade and we had a pool with about 60 000 rapid 4 000 we're at Bitcoin and we traded 40 rep Bitcoin over two weeks that's 1344 nickels and you can see the worst case scale proceeds is like two times ten to the 32 which is huge but it turns out when you crunch the math that actually gives you 29 years before overflow so that's actually pretty good but 42 rep Bitcoin not a particularly big trade so if you have a bunch of Trades like that maybe you have six of those all of a sudden I'm you're down to five years so I still didn't know if it was really good and that's where our Auditors came in because I told the Otters hey I'm freaking out about this I'd really like you to look at this they had more experience in the domain than me so they picked the worst case they picked uh TUSD and die and that's 18 to two decimals and the decimal part isn't really important the important part is the numbers are so much larger in this case so when we work through that same trade map what you find is we're now at about one times ten to the uh 37. much much larger number and now we're overflowing every four hours way way too many overflows that's 84 overflows in two weeks so that's not gonna work and what we realized is we had a couple of solutions we could do to this we hadn't scaled the internal math and the problem is if we would have done it the way that the answer does it would have added a lot of gas a lot of developer time and a lot more to the testing of audit and contract size so we came up with an Innovative approach that allowed us to actually just tweak our scaling factors asymmetrically so the two tokens in the pool at construction and let's just see what the effect of that is so for that same example you would now use instead of a 64-bit scaling you would use the number 100. so with a hundred we find that we now get only 7 times 10 to the nineteen much much less than 10 to the 37. and what that means is we're actually now overflowing every like I think it's 8 trillion years so that's like going to support a much larger number it's a much safer way to do this and basically now we're at the end of this numerical analysis segment and just some basic conclusions for you guys automated tools would have made our process a lot easier we took more approaches than this we even did finite element analysis on it um so we're actually really looking forward to trying hard sell the next-gen solidity security tool that Brock was talking about yesterday um our development process was really really helpful we had a safety test it had about 300 different tests and a lightweight typescript model as well as hand capulated analytical inspection points so whenever we change things we could run it against that and see if we were breaking assumptions in how this thing would operate and we also had a benchmark test that allowed us to see if we were affecting gas and that was actually super super useful so in summary combining Miracle analysis methods and extensive preparation gave us a really excellent leveraging of our auditor skills they didn't have to look at the basics because we didn't have a lot of small order basic errors we had more complex area where we could really leverage better work I just want to say thank you to Grants from balancer and rook and uh if you want to follow us there's information there thanks any questions thank you I'll be honest I did not follow them I would be curious if it would be helplessly if you could just go back and summarize the relationship between the auctions yeah what assumptions so this part is a larger no in the system we know today on the four uh in in the General chain Arbitrage happens you know what happens for a small amounts I've seen values as well as ten dollars for arbitraging the idea here is our pool is entirely considerable the fees and the reason we have that for each individual pool is this is a competitive marketplace with a race to the bottom situation going on so different pools are going to have different fees now we have a general purpose swap that any person can use like unit swap today and then we have a discounted spot we took an initial guess we took unit swaps of value for the B we divided that in half and said okay let's throw this to folks who are doing Arbitrage the folks who are doing Arbitrage have different mechanisms for selling that Arbitrage is right but what we've given them in the pool is an exclusive right to whitelist a bunch of addresses that are able to Arbitrage and with them we have to work out and this is a more complex system that is yet to be determined to be honest we need to work out a way of figuring out a percentage of the auctions b or the tip to rebate to the pool liquidity providers so we have a mechanism to donate liquidity to the pool with no LP token expectations and that allows these people to pay the pool at various points but as for the fees and the assumptions it was too difficult to model because there really isn't data for this so we've opted in set to make it configurable and to just kind of play it by ear with a couple of school so typically the auction is uh like the right to orbitals after a train has happened what is like first first one access to this it's first right access so let me let me give you an example if there's an Arbitrage or opportunity for twenty dollars but you're an Arbitrage partner you get a better margin on that because the fee is lower so the idea is to incentivize the Arbitrage before the general purpose population would actually see it take advantage so if it's not worth it to you at ten dollars but this person's making 20 because they get a better margin then the idea is that they'll bid for it and take it in advance and the reason we're doing that is because it gives a better feel for our long-term trade customers because it Returns the price on the bonding curve and it gives a better deal to our LP providers because instead of having that competition happen on the chain with the miners it's happening in another environment that our partners are managing like Brooke for example yeah okay um like is that mechanism specific to qm or could you just have that with a bank you could you could have that with a regular A M I believe uniswap actually discussed it um PB actually knows more about that than I do uh when we went into audit we actually had a more complex mechanism because we were worried about people gaming this system and gaining knowledge of when those funds would be given so you could do a liquidity attack put money in and then pull out an undue amount of the reward and exit and you could be especially advantageous if you uh knew about that early but the problem with that system was it just added too much gas too much complexity and we didn't have data and without having data it just it wasn't worth it to expose that surface area to risk actually the all the Challenge questions this idea of giving a better future um so if I was like two Publishers trying to overdose the same opportunity um and it's like worth ten bucks today uh and you both we give them both some like fixed discount um from their perspective there's a constantly different because the competition is like everyone is completely same the same way I guess the the one effect that I that I'm also curious about is the conscious reducing competition in these kind of auction they don't know how big how many parties so that aspect is our partner okay yeah not not so much that so we have the ability to add infinite partners um but what they do behind the scenes is more opaque and what we've seen is it's kind of been a to be determined thing they've started with one guess and things have kind of evolved their interfaces have default so I can't really speak to that you would think that open market competition would be the best way to go and I hit PB with that question one night and you're like oh my gosh what are we doing here maybe this is a ridiculous idea and then we worked it through and realized the thing that we were circumventing was giving money to the miners if you have that competition in a place where people are dedicated to carving and they're competing with one another instead of the capital going to the miners the capital goes to them in terms of margin and goes into our pool to the liquidity providers so that's that's the hope and that's the thinking as opposed to going open market sorry um so the fee tier for those folks doing that Arbitrage is half so right now but it's visible you can change it based on what you see in the data and by having a lower fee tier they Arbitrage more frequently they get a better deal than the general purpose person and that better deal results in them giving a portion back to the pool that makes sense good question so is there kind of fundamental idea behind this to solve the profitability issue um so I guess the question is like on why pursue the IMO like is that even that in the long term uh why not like assume that you know that LP profitability is solved by other types of decentralizing changes I'm not the best person to talk on this because I'm not a crypto Twitter person I'm more of it but this is this is what I'll say that I understand from what I've seen complexity and I think concentrated but the complexity of managing those positions has been really cumbersome for unsophisticated investors I don't know that the tools when it came out were able to help them bridge that Gap so that created this vacuum where the sophisticated investors with resources came in and did it for us this was kind of a simple foray that met our uh technical skills and also kind of gave the advantage potentially back to the more passive investor and instead of giving money to the money giving money to the lp and still having mining in place seemed to be a great way to start we don't know for sure though like I thought about modeling this and the problem was like I know how to do the big data with ethereum but there's no data that was realistic pulling to look at this everything was like pseudo data I could make up a value but it's like what's the value of that it's like yes I'm not throwing Millions into these things so I don't have that intuition myself I don't think I'm going to throw one there is that in terms of who might use this over concentration would be the example was bells that have a financial incentive to Market make say so again that they produce and they're willing to you know accept like the Divergence loss of an amm with Simplicity of not having to you know like have some centralized tea like constantly Market making and they're okay with the trick off because having teleported to be there it's not talking for them and then they get like three like the ability to buy and sell tokens easily as well like in a safer manner unless we're worried about slippage yeah do you think that uh with this mechanism you can actually get better execution than say an OTC desk would give that is a an interesting question and I can give you some thought around that with an OTC test there's an immediate disadvantage in terms of trust you're delegating trust to humans so you don't know what they're going to do to you or not the other thing with an OTC desk they have avenues that we don't but we're transparent in non-chain and if they were to try to do this on layer one without some special contract they would have to pay gas fees for each of those discrete trades the Paradigm breakthrough was brilliant and it rolled that into a socialized paying of the gas to do that in an aggregation so we suspect in an environment where people are not willing to delegate trust want greater transparency um in what their trade is doing that this will probably yield a better execution but the corollary to that is that in an environment where there's black holes that we don't have access to it might be possible to get better execution but at that possible that risk 