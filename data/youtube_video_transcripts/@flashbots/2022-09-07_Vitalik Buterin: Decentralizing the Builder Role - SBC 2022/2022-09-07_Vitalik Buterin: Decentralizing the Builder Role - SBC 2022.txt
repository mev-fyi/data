today i will talk about uh decentralized builders um so who here is familiar with uh proposal builder separation okay well i'll have a brief introduction to pbs anyway um but uh so there it is um so the general idea of proposer builder separation and this is this uh concept uh that uh we're looking at getting into the ethereum protocol a extra protocol sort of somewhat trust-based version of this is exists and is going to be activated post-emerge in the form of mev boost but the basic idea here is that we split up the role of creating blocks into two parts right one part basically involves what we call builders these highly specialized actors that construct candidate blocks so they just can they do all this work of gathering transactions figuring out like what kind of block optimizes the mev at a particular time and construct these blocks and uh send off bids to get those blocks included and then proposers just listen for bids and they just accept whatever the highest bid is right and so the goal is uh basically that proposers can continue to be these very simple actors that just run a very dumb algorithm so it's this completely automated role and builders are absorb the responsibility of like being this very specialized actor that runs custom algorithms and constantly keeps very custom algorithms updated right so builders absorb the economies of scale and proposers stay decentralized so why decentralized builders right and so the the philosophy behind proposer builder separation right is to basically kind of you know to take all of this sort of like junk this uh centralization pressure from economies of scale and kind of sweep it all onto one side of the room so the other side of the room can be completely clean right but why maybe it's still a little bit bad for that for that side of the room to be dirty right like even though proposer builder separation can greatly improve validator decentralization it does have this extra consequence that it leaves builders very centralized and there are downsides to that and the question is to what extent those these downsides can be mitigated right so the downsides they could be pretty serious right like we could be entering a world where potentially one builder produces more than 50 or even more than 90 percent of all blocks if there's one builder that has a reliably better algorithm for mev optimization than other builders then it's possible that that one builder will just reliably almost always win and it could even be a self-reinforcing effect where the more that one builder wins the more other builders like don't have revenue and so they can't keep updating their algorithms and so they're forced to shut down and you know you could risk creating a situation that's pretty centralized where basically you have one builder and the only time um that uh other builders come in is when there's like exceptional uh transactions that those other builders capture that one that the original builder does not or if that the first builder uh like feels the need to censor some subset of transactions then the second builder eventually comes along and makes a block just like full of uh basically just those sensor transactions and they win but you know you do you risk having a market that's pretty lopsided now we can mitigate the a lot of the consequences of this what's transaction inclusion list right so the proposer of a block would be able to specify a list of transactions and that list of transactions would be required to be included um by the builder and the so this basically prevents the builder from censoring right and so instead of it being a block auction it like becomes a partial block auction where the proposer provides a bunch of transactions that they believe should unconditionally be included and then the builder comes along and they choose the order and they have the right to add a few transactions of their own and this is good but like it's still not very nice if one builder produces more than 90 of all the blocks so the question that i want to ask here is could we make the winning builder itself be a decentralized protocol so what makes sense to decentralize in a builder right so one thing that makes sense to decentralize in a builder is algorithms for choosing transactions right a builder is ultimately an algorithm that collects inputs in the form of transactions and puts those transactions in some order and possibly adds transactions itself this algorithm could possibly be decentralized the inputs to the algorithm could be decentralized um so that's the first thing that we could decentralize the second thing that you could decentralize is the resources for block construction so this matters particularly in the context of full dank sharding in the context of full dance sharding i mean you have a really big block and you want to potentially not require one node to be responsible for the task of creating and distributing the entire block you might want to just like split up the resource requirements of that task and finally third category this is a more exotic thing which was actually originally the in intended to be the topic of my entire talk um but you know builder decentralization started to be more important and so i'll sort of squeeze that talk into this one but uh the concept of like extra builder services and particularly pre-confirmations so another question what do we mean by could right so one type of could is technical feasibility and i think we'll argue that you know technical feasibility here is going to be achieved the other kind of could is you know would a decentralized builder actually be a market winning actor right there are ways in which decentralized things are like less inherently less efficient than centralized things do they have advantages that are large enough to be able to you know reliably or at least even potentially like beat out the advantages that centralized builders would have so part one algorithms for choosing transactions um so this is the builder searcher architecture right so this is uh the kind of roughly the way that like flashbacks mev geth um works uh today except uh you know i don't think flashbacks maybe geth ever really got to kind of add like adding any layers of encryption it was all trust based um and this sort of architecture is the sort of thing that you need to have in order to have a decentralized builder at least in terms of decentralizing block construction so the idea here is that we split the builder role into two types of actors there are searchers and there's the aggregator the job of searchers is to create bundles so a bundle is a collection of one or more transactions that are intended to be included in a particular order bundles may also have a flag that says you have to add this bundle in the at the beginning or bundles could be just like edible whenever um they i mean they could even have some like prefix requirement that says like here are things i wants to be true about the state before you include them or they might not but you have so you have these kind of partial blocks that we call bundles and the idea is that you have searchers that might be specialized toward finding specific types of mev um so you know you might have one searcher for example that is uh really good at uh doing arbitrage between unit swap and sushi swap and all that they do is like they just become really good at that and they publish bundles whose sole purpose is um arbitraging unit swap and sushi swap and so you have these bundles and then they come to and from searchers they are come together into an aggregator and the job of the aggregator is to construct the entire block and then the aggregator and the proposer use some aggregator proposed or interaction protocol could be mbb boost could be um some ethereum protocol pbs in order to actually get that block out there and uh published so challenges one is how to protect searchers from movie stealing so this is a kind of equivalent problem to the problem that has to be solved within pbs itself right in pbs there's this problem that if the builder provides a block to the proposer how do we prevent the proposer from examining the block and instead of paying the builder uh just uh creating a block that just replaces those transactions with transactions that use the same strategy but that paid the yeah proposal directly and so that's the same problem except here we have to protect the searchers from the aggregator another question is how do we allow the aggregator mechanism to even combine surgery inputs right if he wants to protect searchers from movie stealing then the bundles cannot be in the clear if the bundles are not in the clear then how do you aggregate them um how do we ensure that the aggregator mechanism can actually publish the block right so if the bundles are not in the clear well the contents of the block are going to be have to be in the clear eventually like what's the process from a sort of just cipher text being out there going to to uh plain text also being out there and whatever that process is um how do we make sure it works without like requiring searchers to be honest and how do we protect searchers not just against aggregators but even against aggregator proposed or collusion so those are some of the challenges so one idea is trusted hardware right so the idea here is that uh searchers send bundles where those bundles are encrypted to some key that's uh in where the corresponding private key exists in a trusted hardware module the aggregator runs some merging algorithm inside of the trusted hardware module and the trusted hardware module unlocks decryption only when it sees the signature from the proposer plus some proof of uh the uh availability of the proposed signature right so the reason why we need this proof of availability is to basically protect against uh proposer um aggregator collusion right so just in case the proposer is the aggregator the whole the reason why we want to see the proposed signature is because when you have a proposed signature you have this like thing that stops the proposer from later making another block that proposes something else because if the proposer proposes two different things they can get slashed right but this argument does not apply if the first proposed signature can just be hidden forever and so you need some argument like reason for the by which the tpm can be convinced that the first proposed signature actually did get published right because only if the proposed signature actually did get published did the proposer actually commit to publishing and supporting only that particular block so one option is a testers right so if the ethereum protocol is designed in a particular way that like let's say you have one round of attestation happen after the proposer then the tpm could check for the proposer and enough to test their signatures as well um another option is you could have some m event assumption within the aggregator right so if the aggregator itself is like some distributed mfn system then you could have a threshold thing and you can trust that the threshold thing is honest um also you could have a sort of some kind of like low security real-time data availability oracle so you know if you want you can use chain link or whatever um and uh basically you know just uh like attest to the fact that this signature is out there on the on the internet and it is going to be rebroadcasted right so basically the kind of idea recapping step one searchers send encrypted bundles and these bundles are encrypted to this key um and the only place where the corresponding private key exists is inside of a trusted hardware module that's inside the aggregator so only the trusted hardware module cannot decrypt and nothing can come out um except the uh encrypt basically the the header of the block um that that gets created header of the block goes to the proposer proposer signs it and then when the tpm can is convinced that the proposer signs it and the signature is published then the aggregator can now go and release it uh release the red or the tpm inside the aggregator can release the rest of the block so this is option one um option two is uh if we want to be a bit more clever um so this requires an m event aggregator right and it assumes the mfn aggregator is honest but we can get rid of the tpm searchers send bundles that are encrypted to the sum threshold key and these bundles also contain an access list so a list of like what's what store what accounts and storage slots they access and they also come with a ziki snark of correctness the aggregator chooses the global kind of total bid maximizing disjoint set of bundles right so basically we're only aggregating disjoint bids there are ways to potentially improve on this a bit further but like just to keep it simple we'll only be aggregating disjoint bids right so and then finally the it's the aggregator's job to compute the state route right so the i mean computing the state route is a challenge right because to compute the state route you do have to um actually yeah like see the transactions in the query so that you can process them or at the very least you have to see the state updates in the clear but the problem is that even the state updates themselves might be enough information to do mv stealing um so one option is to have like one aggregator node decrypt and compute but then that they can collude with the proposer another option is to compute only after the proposer makes some commitments that like whatever this block is and whatever state route it has the proposer will only support that block right so this requires kind of the eigen layer technique right where the proposer makes like some kind like on uh unchained or well not on chain off chain message uh committing uh that the only block that they will produce in this turn in during this turn is a block that contains like this set of bundles whatever they are and only after the proposer makes that commitment uh do the uh bundles get decrypted and the estate route gets calculated right and uh this uh if the proposer violates this commitment then the proposal gets slashed right this is done with the you know the eigen layer technique where basically the proposer sets their withdrawal address to a smart contract that also enforces these extra slashing conditions before giving the giving them their money back so a couple of different ideas this is something that can be improved on right but like if he wants to try to decentralize block creation you know you want to go in roughly this sort of direction um block instruction post dank sharding so this is uh this gets into a kind of this more complicated territory right so post full dank sharding a full block is 16 megabytes and it's possible that it will increase even uh more than uh to 16 megabytes publishing the block um requires publishing across a huge number of subnets right the point of dank sharding is to avoid requiring any individual node uh to download the entire block and so instead there's like some kind of architecture could be subnets could be some new form of dht and this is also an active research problem uh joaquim recently he made a great post kind of detailing some options i think it was on the paradigm blog so basically the problem is that you don't just need one node to do a bunch of like complicated work uh to create a full block um that contains like all of these different data sets and that has all these different polynomial commitments and relations between polynomial commitments and erasure coding but we but that node would also need to connect to all of these subnets and like have super high bandwidth we want to avoid requiring a single node to kind of have this really complicated function so distributed erasure coding right so this is uh something that can be done and it's actually not very hard so step one whoever includes each data transaction um is responsible for encoding that data transaction and propagating the blobs or this should say propagating the chunks um the chunks of the blobs uh to the uh like to the the subnets and the data availability network right so if you wants to push through a data transaction you are responsible for encoding it and you're responsible for sort of making it available in the way that the data availability protocol requires the aggregator when they choose which data transactions to include they can use some real-time data availability oracle that does the sampling for them right well i mean actually yeah it's this is a bit trickier i think you need the oracle to actually try to download the whole thing because uh data availability sampling is not secure when there's only one person doing sampling um right and so you can't just have the aggregator sample like you actually need to have some oracle that's some distributed process that's trusted that like actually tries to download the whole thing and then the network can fill in the columns right so here like basically we have this sort of two-dimensional um aggregation pro erasure coding process where the data gets sort of extended horizontally so you have you know the each blob is 512 chunks but it gets a ratio encoded 2024 but then you also have this vertical process here where you have like let's say if you have 32 32 blobs then they get extended to 64 blocks right and so you have polynomials going horizontally and polynomials going vertically and filling in the columns can be done by the network um now this gives the reason why you can fill in the columns it depends on kcg commitment math right pcg commitments have this lovely property that they are linear right so if you have a kzg commitments to a and the kcg commitments to b then a plus b commits to uh the or the the commitment of a plus the commitment of b is a commitment to a plus b um and furthermore you have linearity of proofs right so this is something that ipas um do not have right so merkle trees and like even snarks start to merkle trees they do not even have commitment linearity ipas have commitment linearity they do not have proof of linearity right so basically if qa is a proof that like a equals some value at some coordinate and qb is a proof that b equals some value at the same coordinate then you can make a linear combination of uh qa and qb and that itself is a proof that the the same linear combination of a and b is has the right value at the same coordinate right and this linearity property is actually what is needed to make it possible to kind of fill in the rows right basically if you have proofs for like let's say the third column going all the way from 0 to 31 you can use that to generate proofs for the third column all the way from 32 to 63. and this is like a really mathematically amazing thing but it only works with kcg if we don't have kcg then the best technique that we know about is uh it's the builder's job to create row commitments um so commitments to all the basically commitments to all the roads actually the builder doesn't provide these these are just uh the first half of the row commitments is just the blobs so that's fine the builder would have to provide the rest of them and then the builder would also um have to provide some call and commitments by the way thanks to dan bonaver and of helping us to think through some of this um but and then the idea is that these commitments have to match right so it's like basically the i throw commitment at the jth coordinate equals the jth column commitment at the um at the ice coordinate right so they they're just you know one of them is sort of saying we're going to go through rows first and then columns the other says we're going to go through columns first and then rows it's uh actually kind of similar to what the original um 2d data availability paper did um except instead of using fraud proofs to check equivalence you can you can do a like a z kp to check equivalence so this can be done distributed but it's harder to do distributed it requires a at least two round protocol um like whereas the kcg thing is just one round right it's like the builder um first uh you know just uh checks all of the blobs and then the builder just publishes and then there's just like what totally separate process the builder isn't even involved in that fills in the rows here like no no the builder has to be involved um so possible but harder extra builder services so in the last four minutes and uh 20 seconds uh this is my entire original speech for the for this so builders can provide pre-confirmations this is this fascinating concept where you can basically like try to get like so ethereum in general right i think it has this weakness um that it has these fairly long block times and you know the fancy vc chains they tried they often argue like oh you know we have these amazing 500 millisecond block times and i think like my opinion is that a major credible neutral public blockchain trying to provide 500 millisecond block times directly is a big mistake uh because well you know you need you need to have large safety buffers for decentralization and there are even more subtle reasons why you need to have large safety buffers like for example if censorship is happening then you know you need large large safety buffers to be able to have good network-wide consensus about like who is responsible for censorship and whether or not a transaction is being censored or whether you just have a transaction sender that keeps publishing the transaction like half a second late um so long block times and also longer block times enable like much larger validator sets um but the question is can we have the best of both worlds can we have ethereum with a finality time of like let's say 30 seconds and at the same time have like some high degree of confirmations that come quite more quickly so let's have the builder do this right let's say the builder publicly agrees that if a user sends a transaction with a priority fee of at least five the builder will immediately send an enforceable signed message agreeing to include it in that slot if the builder does not do this so if the builder does not make a block and that's what um and that's what that gets included the builder gets slashed if the priority fee is greater than eight then the user even gets a post state route right so the builder basically an even higher priority fee forces the transaction to get included in order and so the user knows what the consequence of it that transaction is immediately this could be done with um with uh like the fact that this server exists and service exists and the fact that like you get this fast confirmation if you at all if you provide a fee that can be done either with builder reputation or you could have a third-party data availability oracle so can a distributed builder do this right this is the question so you can run the yes i mean you can just have uh a system that runs tendermint internally and internally has a 500 millisecond block time or one second block time or whatever that just that does this exact thing um and uh the final signing can be done threshold style right and you would have to penalize participants in the distributed builder either for breaking the tender mints right so for doing double finality inside the tendermint or for contributing to a final signature of the block that is incompatible with what the tendermint agreed to now this duh if you want maximum security this does require some kind of like account abstraction for final builder signing because threshold bls is um unattributable right so if builders just have to like be a less sign the final block then you know you don't know who did it and so you have this kind of problem of uh well you know who do we slash if there is a miscon if there's a misalignment but you know if you have some kind of more abstracted form of final signing then you can do it the more participants the more security right so this is where we get to kind of the holy grail right which is uh if you have a distributed builder that has like many thousands of participants and you even maybe do like fancy eigen layer stuff and you get like 10 of the ethereum validator pool to participate in the builder then you can basically have a pre-confirmation service that has like huge security deposits behind it so within one second you can get a guarantee that either your transaction gets included or this uh pre-confirmation service like has to burn a thousand ether or whatever um a distributed builder what other advantages does it have it could be more easily trusted by searchers right and distributed builders like is more well protected against cheating searchers and and even if like people collude to cheat the searchers that collusion is easily detectable distributed builders because they're protocols and not operators they're more censorship resistant so there are these different advantages that exist but at the same time distributed builders have these weaknesses which are basically one is just inherent latency of distributed system and the other is that like any decentralized thing is uh slower to adapt so the question is like well you know can distribu the competitive advantages of distributed builders overcome the disadvantages and you know can we find other competitive advantages like what other things can distributed builders do and how do we make these protocols be as efficient as possible and as trustworthy as possible and try to like really minimize what all of the disadvantages are [Applause] is this open okay yes um so i have a question on the post state route uh you said you can provide the post rate route could be possible that you might have conflicting state route because of distributed builders or is that not a problem oh it's the question like what it will like is there a possibility that you can't compute the state route because you have the distributed builder mechanism yeah yeah that's a good question it is a challenge and it kind of like intersects with the other challenges right like what you could do is you could have a mechanism where the proposer for example keeps on making updated proofs right so it's like you know we have round one which like within us a lot which would basically where the builder agrees on some transactions and then the bill and the builder says hey propose or can you pre-agree to sign a block with this prefix and the proposer assigns and i get only your message that says that yes i pre-agree and then after that the builder reveals and then you have round two and then the builder kind of accepts the second batch of transactions they send to the proposal the proposal pre-agrees so like you could do like kind of fancy multi-round stuff like that and uh i guess you probably you will probably have to um and in the trusted hardware route like it just gets easier because you i like it it is pretty safe to give state routes but well i guess giving state routes is like not very pointful if unless you like actually give people receipts as well so that could probably be abused for like uh in um strategy stealing so but well i guess though even even in that case like it's in the trusted hardware so like he would not be like it would not actually output the full header for like anything other than other than the correct block so i guess like yes in both modes there is a pretty natural solution okay thanks hmm [Applause] 