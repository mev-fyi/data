let's get started uh I brought you five out of ideas one of the ideas is applied that which took gorgeous so look forward to that it's going to be number two um this is basically motivated by me reading stuff I normally work in uh methods and design microeconomic Theory yeah let's just can we switch this off it's like let's distracting uh just switch off the TV oh uh okay uh I normally work in mechanism design or microeconomic Theory and then I'm like much more of an economist than and I've read the game theorist and I recently also work in reinforcement learning and they're actually coding realities so some transition into more practice but uh you can query The Economist test uh with a classical social scientist and you can also query the reinforcement learning head who is uh had to is going then to talk more about practical coding realities or how you build systems the first idea is just just an overview of how you improved systems in the corporation sense that we've already seen and one structure that I want to get into your head is the first idea is that newly um three different approach that you can go for one I would call the media debates having a couple of agents that interact in some way and you're introducing mediator into the system there is the chart of work under the name of stackable games that work in this environment you're essentially introducing a new agent that proposes gives mediators no no I'm for sure having a point we're not in a rush so we have international new agents this comes out for example when I read that the protocol becomes aware in at the ram that wasn't one of the checks or the protocol tries to do something there's an agentic nature through the protocol and you're introducing a new age the second approach I would call be a declared leader this could be like August sand the sign this is endowing particular agents with specific actions that um constitute commitments we have this in the real world a lot where real hierarchical structures that allow some agents to make commitments for others and the paper that I'm going to present you is this style and we can we cannot guess that the third one is equilibrium so here it is one agent thinking or the threat which is learning something of the of the form I am going to not be too naughty here on this because otherwise the police come and puts me into jail like probably I would need to really break a lot of norms so that that happens but this thread is there and I'm cooperating for that reason this is uh what I would call Subway perfection so we actually have these these three ones and um for me as an outsider to crypto kind of like I read a bit before this talk now I I would say that um the first one which has a bunch of work now but uh in academic venues but also with uh just stackable technology uh doesn't sound very Crypt as in in some way because actually it introduces a centralized agent that has again a lot of power so here the question becomes okay there are two ways one running punishments I actually disagree with the same the punishments are bad like we have a lot of very good emergent Behavior because we actually don't punish if the punishment are in place people just choose to not do it and there are other other things that are essentially technology and I will interchangeably also um talk about agents versus the technology sorry this technology should go here noticeable so maybe uh as the second idea to give you an example of concrete reinforcement learning domains that we're working in this is the main thing that I think or news is interesting here uh it is motivating social dilemmas which I think you as an audience don't need but you will see what the game looks like as a simulator I'm going to finally talk about two very concrete things that are apparent if you're training reinforcement learning systems that I haven't heard or read the main question of Corporation and also this paper is how do you get selfishly uh essentially motivated agents to cooperate we already heard of a lot of a lot of them there many that are actually still while far away the bottom of the vehicle things were probably 10 plus years away to see that we have however if things were control systems are already out there smart grids are control systems and having smart meters and Google nests have things that have a gigantic nature already right now so that's quite close this tape is very motivated by uh warehousing Solutions where you have robot fleets that are in Amazon warehouse doing packaging fully automatically here you have some very clear Cooperative framing is Amazon controlling all the robots but the robots are independent for computational research there are also other esoteric ones in in manufacturing that I'm not going to go into the domain that we are working is the grid World domain which features a river a tree um and a robot the robot gets reward by picking apples when the robot um goes and grabs apples over time there's a build up oh what is this yeah I I don't I can't seem like it's weird I cannot see that cursor but now I can admit um there's a building up in the river um and the robot needs to go and take another action which is cleaning pollution pollution cleaning does not yield reward so they clean the reward there is again an Apple so in a single agent setting you have the cycle from polluted the robot goes to clean apples grow the robot picks apples it builds a pollution again and it is in in the cycle this is something that a reinforcement learning agent can learn reasonably well you need a couple ten thousand things I'm just going through the this bit in like two minutes whatever is it we have more agents you might hope that they grow they clean they grow at some point they realize that they could divide labor um and one cleans one eats apples uh pollution builds up they both Harvest does this happen yeah not really because um Port agent so what you see in both agents end up waiting for apples that don't grow because the river is there because the agent who uh would be in this model the harvesting agent gets apples the other one gets dirt but they the second agent doesn't really get anything we have a social dilemma um in in this one which has also an inter-temporal component to it so I think now is a good time if you have a question I was just wondering if the pollution is in some way relate to you all dependent on the apple picking just like that just over time the logic the London is if there's a certain dirt level no apples grow but pollution uh increases over time at a constant rate um now um how do you train how do you train the this thing um in environment are already set for a single agent you can just view this game as a very big game there's some State there's some actions uh there's this uh now pretty why uh algorithm this pretty widely used it's called proximal policy optimization um which optimizes whole policies so whole functions from state to actions um and yeah here Apple report rewards as a number of apples that are picked per an episode you could do separate training and you will converge to some form of Nash equilibrium where agents might randomly grow uh go out and clean and go out and garden to pick apples but this is some approximation to a complicated looking uh mixed equilibrium which is uh not very I'm not very high performance you can have one agent that controls all um all robots in this environment and you see some performance interestingly uh you can also look at benchmarks that I'm not going to talk about and you can allow for a commitment and you see here that this example the commitment decentralized solution that we're proposing this paper outperforms even joined um which looks for now so surprising but um I will tell you why that's so let's see let's see what commitments can do here I think it's clear the whole session the whole uh meeting here is means I will commitment share my apples with you you clean the rip uh the other robot says yes what happens in this case a certain contract is in place which Alters uh in this case how many apples you effectively get some apples are automatically transferred um robots clean uh apples grow they go into some division of labor um and they said there's no path that brings them into a data message so what happens here well it normally as normal it happens but there's a transfer of apples which is huge normal Employment contract you you can you could think of that they're learning from some set of possible commitments this doesn't require any Ultras this is that at every point this is the right thing like as soon as the uh the one robot committed to give the other one apples for cleaning is in the best interest for the cleaning agent screen and the other one yeah sure if they don't have anything to do they will eat up so at this point this is clear this is clear is also the choice to propose a commitment device inside of compatible yeah like you're comparing an outcome which is Paradise essentially there's a working agent who gets something and there's an uh that eats apples this is a good outcome um two The Dilemma stage which is bad for both so to every stage you're purely into self-interest and and I think there that commitment devices are actually but the use of commitment devices is incentive compatible it is something that's well known how do you make this formula uh this is uh a Markov game we have a state space we have some initial State this is like the grid World these things are all a bit more High dimensional um you have vectors of actions which could here be move up move uh move down move left move right pick Apple clean uh uh clean garbage from the river you have a transition function that map State actions to new States uh here the state contains apples location of garbage um and the locations of all the robots um and you have a reward function which in this case gives you number of apps so we want to bring something new in this so we bring commitment devices into this Markov game environment environment and this we do buy a a set of potential commitment devices there's quite similar to The Proposal I forgot the name again of the specification with commitment devices it wasn't in one of the things one abbreviation and a function mapping the set of commitment devices to essentially semantics of the commitment device the commitment device here is is zero sum transfer depending on state so here it is if you're in a state where there is the river is clean there is a transfer from one of the agents to the other agent and this is the the form of commitment that that we're looking at in the exam um we also need to blow up slightly this state Space by tracking which contracts are currently enforced the rest of this is going to be why the of this second small idea I promise the other three will be shorter uh is why this works in theory and while this works in practice yes can you go back one second uh devices as part of the state space where you need the extra what is it in the University that you kind of like calling the problem is so it is it is like you can totally view this as a this will compile down to another Markov game okay and when I tell you how to solve it uh you will see um person Theory we have a theorem for an arbitrary full information markup game what information is important I'm going to talk in number three about that then Friday's officially Rich Contracting space all equilibrium of the augmented game are equal to a jointly optimal policy profile of the original game under the contract proposed in the equivalent if that doesn't make any sense to you right now that is fine because it it says only the allowing agent to contract with sufficiently Rich commitment devices mitigate and mitigate social blood under complete information um yes well that's the efficient language this is High possible High possible trans I need to have that otherwise I don't really I don't really care it's going to be degenerate but there for this theorem I essentially want you to want to force you if if you're if you're not doing the best thing I'm just voicing does it also depend on the dimensionality is teacher if it's a parameters based and so on course it's not what a rich means here no no it's really it's really about the positive the possibility because it is about so here it is really the property that you need is that your uh contract space is Rich enough to detect the VHS well in some sense okay there yeah have a look at the paper the yeah I think the theory here is less interesting than actually an implementation and my my goal for you is to get to the point of how would you actually train this um so we can take this game and this is uh as you you asked this is just a normal game you could totally encode that and uh you can train it with um with the generic algorithm you're just saying there's a first puzzle stage there's some acceptance stage then you go uh Fair performs fine um how can it outperform joint well the thing is that if you're doing joints you really need to look what and what caused what because then like essentially you're having two agents and you do something to two agents so you have a two-dimensional action space and you need to learn okay so if I'm doing this one agent then this affects welfare in the following Global Way you have a single agent environment that is working under some contract then they will very easily figure out what their part of the reward namely their apples all that depends on their action this is called an uh right trusted learning credit assignment which part of your actions do what the much easier problem uh to decompose and one of the reasons why you actually would look at independently trained agents because their problem is much more local it respects locality that actions that that are taken by a particular agent actually affect their reward quite directly not so much the global way the problem is that this doesn't really scale uh to more agents and one problem is that the Contracting Faith to value for the agents so to utility for the agent's mapping it's more and more discontinuous over time like in some sense you have I think optimal reactions in the sense of best responses and and it might not be that there is no exploration so what you uh what do you do to solve this is to say well let's decompose the problem the first solve for different possible worlds under different enacted commitments and just learn policies and hence learn values and after that just optimize over that you then have just a function that Maps your contract parameters to how much utility is there and you can you can optimize as um as a proposing agent what you do um Secrets I see that you're you're not no no oh yeah yeah yeah so like how does the agent so so is here like your the agent is learning what contract should propose in the second yes yeah and the first is they're only learning to play under an arbitrary account I say yes and decomposing these essentially gives you an expiration bonus on this if you want to view it but it also essentially brings down the reinforcement learning only to this paradise problem this one's then classical search which is computationally much easier the key idea hands is to learn uh how to play given a contract before learning to contract yeah I was gonna this is I think probably answers my question but I was gonna say does this basically end up then being kind of like a stackable game in which you first have to select the contract and then well there's some element of kind of subject Perfection here where you like what you're doing is like for any contract how am I gonna best up best act optimally and then yeah yeah here the the here the the thing is the there is interpolation in in in that and that you're really relying somehow on the geometry of your contract space to give you okay I only like tried a thousand different ones let me use my neural net magic to actually interpolate okay fine play yeah otherwise but yes so here in in stackable sense one thing going back to this it's not your new agent we're just declaring one of the agents to have the possibility how how do you assign that it's nothing you're not changing the agendic environment they're still NH which maybe you're going to answer anyway but um on what kind of time frequency does the contracts like change or get updated I will come to this uh let's look at more um more experiments uh princess dilemma and public goods is two classical static gains Harvest is another grid worldly environment which is mostly an inter-temporal social dilemma where if you over Harvest now things won't regrow again uh cleanup is the domain that we have been talking about uh in on this lines merge is a self-driving car domain which is essentially dots on the line chasing each other so relatively far from realism um and we're comparing the four baselines that I told to you about uh or told you about already and this two-step procedure that I proposed in the early earlier slide which is multi uh objective Contracting augmentation learning uh and uh what we see is that more custom scales more nicely this is kind of what we set out to do and the experiments confirmed future work scaling to more agents here one one big question is how can you transfer between agents the knowledge of what it means to convert if you can do that then you can just clone things and you can do it like the DOTA 5 um where you have five times the same agent so DOTA 5 was this opening I bought that played depends on the engines um the second one is uh learned contract functions so far I said that there is some given logic to contracts but you might actually want to have a lower parenthetized uh set of contracts that are more reasonable and for this you would need to learn the lot um and there is renegotiation which is uh after which time so you allow for some time after some time for contracts to relax there's always a trade-off in that contract run out the long-term incentives are broken because you know that the commitment is not going to last so in summary of this paper social members arise in multi-agent systems when managing shared resources Contracting or commitment devices in this environment create incentives for post-ocial Behavior without assuming any altruism and letting agents trade voluntary voluntarily mitigate social elements and both Theory and so we saw now uh an example or what's the time oh Philadelphia excited I started a point not anyways so um you now saw the talk at Ms that might collaboration gave and two comments on here on your stated goal of today with the conflict domains um and I will give them names the first is centriole here we are uh working in simulators and all of rain for reinforcement learning right now happens in higher or lower Fidelity stimulates one of the questions that I would give to you crypto community that I see and see in many of you is how are you thinking about a simulator versus reality in a crypto environment because we have so these are simulations of the environment on 100 CPUs in parallel of these worlds are you going to simulate a ledger in training are you going to simulate and blockchain or are you going to simulate a smart contracts while you're training or is this purely a deployment deployment thing um I think there are several questions wrong around that um and here how would even reinforcement learning training look with cryptographic techniques while also realizing that you need to get on onto the order of hundreds of Millions of environment steps uh that this is really open question the second one or an environment like here the very particular type of wire heading and I also would like to pose you as an open question if you're saying that commitment devices are going to intervene directly into Rewards that agents get and I think there are a lot of arguments for doing that as opposed to intervening interactions or delegation uh which I see very often um say later why you will run into a situation where you might as a robot be able to evade receiving the reward signal from The Ledger if you can do that it's a perfectly fine way to wirehead yourself because there needs to be some reward signal that you get instead is the centralized blockchain tells you oh this was actually really not nice of you that you didn't clean but you for some reason can move to a part of the Earth where you are actually not able to access The Ledger and there is some decentralized computation which you will need in robotics what do you do here yeah they don't know why heading into it okay so here is your message you're messing around in your own hand here at me here it means you can influence the reward signal that you yourself are getting and here I'm thinking about the application of if the reward signal depends on some public Ledger and you can evade receding that thing then um that's a very clear reward hack right that I see in such an application I think that's something to really think about in a real deployment there was a question yeah it's I guess uh yeah so are the contracts um proposed or like enacted unilaterally I'm going to come to that in in the fifth part um okay these are all like two concrete things that I would love for someone yeah in in in the crypto space to think more about um the next part is that I would like to be a bit more clear about the thing that I already earlier was said where the delegation versus transfers and and about intervention into gangs so if you're augmenting a game to get Cooperative outcomes you can do something later essentially hey I'm not going to do that because I'm actually uh delegating to another eight the second thing is oh yeah I'm going to just input in certain world States why don't you transfers yes you're delegating that what this means is you need to have a fully flesh policy that is run somewhere else otherwise it could influence it that actually acts for you if you think about a real robot picking up picking up stuff you're not going to be able to do that that any computation on a blockchain will be too slow to get you need some local computation you can send some certificates but I don't see that I don't see that happening under uh reasonable like itself so the the whole idea of and here like a bit more of the the blind person here the whole idea of oh we expect each other's source code I don't I don't really believe that that movie at all working uh one thing that can work is to say you have a reference World State you can call this reference World State he has an mp500 in in nowadays it can be the fetch rating or uh the for Google or for Italy um and based on this you have certain automatic rules flash crash a lot a lot of things based on these automatic World States you're saying I'm how to allow reward transfers based on that you don't need to know anything about the local computations of a rocks you don't need to know their action spaces you don't need to know something about their internal representations the only thing you need is to know that this thing is optimizing it's maximizing a reward and you can give it reward you need some way to make reward comparable across agents but there that's straightforward in many environments if you're going for an Amazon warehouse you could say it should be the same reward for every agent to pick up one box is your welfare question but you can do it so here delegation Solutions I don't see as not reinforcement learning head I don't see a schedule it's for Solutions I think they're there is this problem with wire heading I think it's much more attractive um to the question of yeah contractual outcome number four it was actually super crucial for the paper that we have we can we don't have um equipment information we can contract on the true state of the world this is going to not be true and in one of the papers from yesterday or it's abstract was exactly that question like you don't have that the chain knows everything um and in thinking about um in thinking and thinking about how contact input will work in the real world you really need some sort of the chain meets the real world so you need some sensors that you can actually rely on this I would say is not necessarily saying that there is a centralization going on you just need to have some Trust if you're leaving the chain you will need to leave the chain that's done so one question here is what is uh contractable outcome that you would like to go to to go about um to incorporate into into your system these types of measurements if you're going Beyond finance application or transaction applications are actually not as straightforward you're thinking about uh the upper right one which was the the electrical grid this is a lot of things you might want to contract on that are actually very hard to really control as stable and not hackable this would be the New York Times headlines this would be um certain things happening even in Mexico South of the Border if you were talking about Texas this could be rainfall uh rainfall numbers in Northern America a lot of things you need to um inform these contacts is the contractable outcomes are are there you can uh write good contracts if they're somewhat limited you're running into um a problem called moral hazard that I'm not going to explain um to not get over my mileage attack number five is uh a little bit of oxide um I think I finished slightly earlier um um you want conversation then why don't you call it I think you just call this distribution is um is about the concept of the proposer here one of the main things why uh commitment devices are amazing or or welfare is that the proposing entity can actually uh the proposing entity is aligned with social welfare because they can extract all Surplus from from doing so and one thing I want to put into your head and this last thing is in many applications it's not a bad thing this is the the beauty of working with robots if you're in an Amazon warehouse and you're making one robot extremely poor you don't care it's it's not it's not really a problem because robots are on Star there's a question is like there's ownership of robots but you don't necessarily really care in these environments having one agent who is and give it a lot of name a lot of human society names uh that sound bad but you don't need to I'm gonna have one robot who extracts all the Surplus from you know from uh the system and the system behaves amazing all the problems of commitment races or other miscoordination problems arise from this assumption that you need to allow every agent in your system to get the fruits from that and I want to give it to you if you're really going in engineering a multi-agent system just don't allow that just declare One agent was going to be really rich and rhombus in many environments there's actually going to work the only problem is the miscoordination from this desire of humans to create something Fair which are enrolled in robot situations is not clear that not that the robot is entitled to getting a certain amount of reward foreign you also get into more into more uh yeah no that's that's base coordination also leads to then arms races and ineffective investments in that just saying absolutely if you just drop your fairness consideration between between robots uh which I think it's fun if you're thinking about so going through the five ones again my first invent three exiles uh of media Jersey clear videos and some imperfection that's raw solutions to the coordination problem then it gives you one complete situation or their leader and talked about how you would do that in in Great Walls I also gave you two um two corporate problems uh that I think are important to work on I argue that transfers a transfer based commitment devices are uh the way forwards as opposed to the navigation based ones we will need some contractable outcomes where um the chain meets the real world and thinking about which these should be uh is an important question and I argue that caring about distribution is often a problem that could be a problem that makes it much much harder to actually make progress which you might not care in Earth yeah looking forward to all of your thoughts well uh yeah um great job by the way um so I kind of uh one question on I would have seen my agency I guess what we just said yeah which is so something that um you know uh some people are thinking about is proposer commitments uh in the in adoption context right and I think that that um you know highlighted some of the ways but basically highlighted uh kind of uh because of the contracts uh that validators into and something interesting about that is that the contracts that are actually portable are you know the contracts that you know reported by the chain are the contracts at the checkers information uh about but um you know something that we saw uh was like uh two ways in which you know the blockchain can learn which is the first one is obviously through consensus so you know learning whether for example um say a contract was you know the commitment was properly executed in the real world or something in the real world happened and the other one the other way of learning about the real world from inside the chain is through cartography so for example posting a cryptographic group submit and then we're buying the proof as part of uh you know the the protocol so to speak yeah um and yeah another thing I wanted to ask you about is so what I would quickly comment on that because one thing that here also just as like outsiders look or now Economist has look on on things that I read in preparation or this seems to be an interesting bias in crypto talking about talking about what the chain does that's very Financial economics I would I would say it's all about information aggregation it's like if all information is aggregated on on the Chain that's correct and here not necessarily that you really need for a good for a well-performed system to know all these things the thing that's really important is okay what in the what in the real world happens are there flash crashes and and so on I totally understand it if it if there is this more narrow view of the chain as being in transaction medium then it's all about about that but you can you can have contracts with severely limited information there is going to be strategizing with that but that's not bad and that's not bad like many people many people are saying oh we need to make it strategy proof to essentially make it robust against strategizing that's not my view it's like it's gorgeous a lot of good outcomes are coming out of Auto shred so not that I said it every by the way and in fact people don't hear that um those systems have a bit of an optimistic flavor where for example you know there's uh some you know but there's asymmetry uh or you know information that will arise in the future then you know some commitment or some uh trust is made to is made to is put on today and you know when that information becomes available then the agent that you know uh made this uh assertion so to speak about the real world can be uh punished as a way of enforcing the fact that um listen yeah and here also like these are all relational contents like one one very very standard way to do these are the punishments we call relational contracts which is just like I'm talking to work with you like if you lie after me so just here but the totally like natural way of humans to enforce things or you're ending friendships yeah okay yeah and then what about about contracts where uh where the enforcement uh so you were saying you know there are contracts that uh you know due to incentive compatibility you know they have to be enforced by um say uh some external thing to the party's uh involved in the contract uh but what about uh contracts that are that lie in between where they can be for example might be influence you know they sorry the importance of the contract can be mildly influenced or someone influenced by the parties so in other words you know the they they lie somewhere in between the absolute um enforcement depending on the external world uh but they can be influenced by the agents um you know if you think there are some interesting ways in which that could be uh Quantified say for example you know these contracts are uh uh robust say uh here reads this paper foreign [Music] thing that companies that are cooperating with each other are writing down values probably they're written down things but they're clearly not not enforceable um this paper makes the claim that there is a coordination aspect to partly formalized but actually punished in the real world by actual humans things and one thing I'm kind of kind of thinking here is that he is okay I'm also implying I think here that the you can influence how suddenly that's semi-automatically enforced that that won't be that trackable I do I the the thing that I'm saying though is that having written things that govern or that that outline out punishment is going to happen in a more subbing perfect world that we have these the humans and these are potentially totally feasible for uh for a ledger that you're essentially saying your recommendations to punish uh for people we're not forcing you to do it we do it in in the uh in the real world but let's write this down and this I have promises but they are thinking people probably have much better it's just read this very general General audience you're request then you're there um yeah so just one uh like yes uh in the paper that he talks about uh your paper uh is it just the one distinguished I begin we give General enough like you could you could have um an arbitrary agent and you can transition back into Contracting nodes as soon as you have several uh Contracting agents you lose the theoretical guarantee because I'm going to try to influence the world state to make you propose in the future State accordingly that's better um and you totally see that like if you have for example a re-election in associations that people will will try to influence future elections already arguably the United States is one one such Association then you um question about the kind of partial observability bill um so obviously that breaks a bunch of things yeah I wonder if there's a way do you have any thoughts on the principality of uh kind of like failing gracefully so the more uncertainty you have than the kindness like uh like the more kind of better so to speak uh or is there just like pretty kind of discontinuous things being like both it's not it's not a solution observable then you can just get kind of like Abacus yeah there is um this might be cool this is like very econ match but foreign that's uh so I'm taking your questions is this continuous so this is discontinence if you're getting we're getting imperfect I'm claiming that this is continuous uh continuously continues in the distribution so continuous in in the distribution yeah you have two agents the buyer and a seller um they have this this good let's make it an iPhone uh bullets have um science there's some uncertainty over that um any mechanism tries to listen to the elicit this information um the claim is that there is no restrictions that means Whenever there would be against from trade they are the trade actually happens mechanism or this the reason is that the fact that you would be willing to trade at a certain price tells me something which I want to extract something from this is like the whole thing about in uh information rents here the thing here is that if you're moving to the vicinity in some Metric on probability distributions of um a concentrated thing so we don't have limited information that that actually um they probably wouldn't need to be something like Earthmover distance it's actually going to be continuous because you can only lie so much it's like very it's naturally small game that you're making which is going to reject only arbitrarily through trades which makes it spontaneous so here I I would I I would take this one as it's not that problem the main thing with like the really bad outcomes with unraveling in the market for lemons after loss the thing is this is very very strong in non-observable Like You observe you don't observe a ton of things about cars in that so here yeah almost mean there's no reading list uh yeah lemons mini cars not not meeting fruits you awesome yeah uh okay thank you very much implementation about theory that uh so when you describe the main pyramid there is enough [Music] um there yep you tend to you need them some kind of punishment that's something to be able to import the outcome and I think around the books in spatial because I think that building and there is a question probably say again what is the command I think what is implementing what on this the change yeah it's lasting so if some people do something wrong they they lose a lot of money they lose all their money okay and so that's uh that's called stressing yeah and there's a question for me can we show um that sounds things and impossible if you don't have this very realistic punishment any single lingo or that if we don't have this really space so we don't have business expungements we can also something do you think that are there interesting results without expungements or we can yeah everything is possible without punishment I think you think everything is Autobots models but why do you exactly needs is depending on your game it's just like it's a very simple thing to just say I'm going to burn down the Earth if you're disagreeing easy but really what you need is what is your best deviation let me just charge you to make into this incentivize you to take that best deviation all right if this is this is this is exactly what you need and often the less deviation is not that profitable right so you need something very small and often even the value of deviations will some smooth function if the function is a smooth function also underlying parameter so you don't you don't need crazy things um and there's no possibilities in in that um once in here that I didn't bring up um this is now Bank trans this is a moral hazard and observability here it is there's some hidden action that is taken there's some outcome that's observed you don't know exactly how bad it is you have some smooth actually um incentive contracts that uh is enforcing and then it's mostly depending on risk preferences which I think then as the governor's governance group foreign blockchain I think actually it's thinking about risk references of people on the Chain would be relevant to see like how much you need in our work everyone is mutual so you need extremely strong punishments um the question about this uh delegation response system yes which actually might be somewhat related to task restart um yeah I think you make some good points in favor of transfers uh but I wonder about the limits like essentially if you can delegate to an arbitrary agent like let's say you could like delegate rather than delegating their like giving a specific algorithm you can just delegate a reward function and happen what's this right historic option then you have a lot more freedom than if you can only commit to transfers right because if you can only come into transfers like utility has to be concerned or maybe you're allowed to burn money but like that's obviously so unfavorable whereas if you can delegate arbitrary utility and then you can yeah I mean you guys have a lot more Credo which will be useful um I'll put push back by saying all right bye I think it's kind of a science fiction term for critique like oh there's a more direct World here so who is delegating Lily we are already delegating to machine we're delegation to machines and Amazon warehouses what you you seem to be saying with delegation here is that we have robots delegating throughout the world one of the questions with uh or one of the duties of uh very personal learning policies is that they're digital Goods we could just copy them so if you would like something that optimizes in an amazing way just imply directly no delegation is needed to do that if you are um saying that the delegation actually gives me more flexibility to implement stuff well the theorem that I showed you is telling us that the decent information is no it doesn't give you more because in some way you can view transfers as LaGrange relaxation of limits on what you can do like I'm just disincentivizing you in some smooth way from doing something but there's like I iclaim is actually not more powerful even if it were more powerful in a world where we have algorithms delegated to other algorithms I think we first have the step that actually algorithms just use certain Technologies independently like one of the Beauties I think of of this paper is it essentially is a black box intervention you're just switching this on it's only going to group the performance you don't need to tune anything about that if they can contract they will get better social outcomes with this distributional concept but it didn't really get better Circle okay that's it I think the main the main point here is and that's maybe something for for offline is that I am claiming that you are not going to be more performance or the lab pair with that so the flexibility actually doesn't buy it yeah yeah let's let's do that outside um yeah just um coming back to I thought it was really interesting that your times it's uh kind of was that part of the Moga where we have like an object yeah basically more explanation of expression you know but in you know yeah I was wondering if that two-step um kind of structure of kind of there are people making sense is needed to achieve this better outcome or if you just kind of add a little an absolutely parameter of the [Music] this is a question for it is it are you interested in that as a pure reinforcement learning question are you interested in that in a system engineer I'm I'm interested in both I think but like I'm kind of interested in um what the role what's the role of the higher Bhopal and she said okay or the model in achieving those Builder results or is it just like kind of a way to augment the exploration parameters there is uh the the short the short answer is uh you could build uh you could build systems where you have separate exploration rates right uh I mean this is going or less performance right because that in early phases where everyone is kind of stupid you're we've been trying to learn to contract doesn't make sense you need to have a head start on how you have it the detection really performing better to say let's first offline learn to behave reasonably as agents and then do it it will work like we didn't run this but but I expect I expect that it will it will work reasonably well it will be drawn but it won't be lists maybe that word uh yeah I might only have to put that in the um in the abstract so when I'm saying when I was talking about this wiring heading example there's all this underlying uh um question around if you are deploying this there needs to be some Ledger for some uh public computation engine that gives these rewards to you and the problem with that is that leads to happen very very frequently like if you are in a let's say warehousing as an example like there is all if you want to get a good reward signal for uh for an agent which is well shapes you need to at least when every package when any box the whole Warehouse is picked up you need to send something to all robots and they really need to receive it so this puts you into an environment that released the computation that you're having with sending is on the order of half a second second which you can totally do um there depending on which which environments and how how distributed we are this is going to be a concern and for me the more crucial one which I then also wrote down is this wire heading that the agents have an incentive to make it harder to receive the the rewards but they're like honestly I don't even know how fast computation on at the ram is right now uh I I was yeah in this release thing yeah okay so it's so it's half a second good so then then we are exactly in this use case realistic that this could happen on the ledge but there essentially the questions like add in what time frame would you like to give agents something about their uh some information about the externality for them to take good actions and if it's not too fast it is I think we have five minutes until yeah uh my yeah yeah wrote me uh an email there's more things that you would would not do this and thanks for having me yeah thanks for coming 