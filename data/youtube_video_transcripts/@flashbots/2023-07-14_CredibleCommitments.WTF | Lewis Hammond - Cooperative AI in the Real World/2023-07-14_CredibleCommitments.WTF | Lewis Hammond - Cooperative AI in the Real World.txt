yeah nice okay cool um so um I think I've kind of changed tag slightly uh from what was on the kind of like uh abstract thing what I said I was going to present um partly because I ran out of time to prepare a bunch of new slides uh and partly also I'm just kind of overhearing kind of conversations yesterday where I think I had multiple times like I don't know like what even is this like with AI thing uh it like kind of makes sense and maybe there's some like relevant bits that like she didn't have others have already been gesturing at but maybe it's actually not really clear I think like exactly what I mean by that and exactly like what sorts of problems uh I personally in that research Community are interested in solving and so I thought I'd kind of gives just like a little and a Whistle Whistle Stop kind of tour uh and quick kind of overview and intro and just to kind of set the scene um and then I'm going to hand over to other people so Andreas uh and um Casper and Anthony and various people who are going to talk about into MacBook and it's slightly more kind of concrete bits of work in the space relevant uh to this kind of uh question of how we might be able to use credible commitments and commitment devices uh in order to reach more Cooperative Solutions again on some games okay um so I'm going to move through the first part like relatively quickly because this is just like a relatively kind of intro uh presentation named like people who maybe especially slightly less Technical and so I'm going to kind of gesture it things at quite a high level and really just focus on your scene setting stuff and then I'm going to raise a few questions right towards the end about this idea of uh real world kind of Testament or deploying property AI algorithms in uh which I'm hoping that you'll be able to kind of like mull over a little bit as we're seeing some of the other presentations and try to potentially kind of Link this and think about how this links to uh work in the kind of real world settings that uh you as researchers and practitioners are involved in okay um so this is the outlined quick bit of motivation talk about what Cooperative AI is talk a little bit about the real world though we're trying to think about that too much sometimes uh and then I mentioned I put discussion here I don't know whether we'll have discussion after this or whether it's uh better more to move on to kind of other things um but we'll see how that goes maybe some questions will come up during the tour as well okay um so this like field in general is motivated uh by kind of two things and the first thing is like operation is really important and the second thing is AI is going to be increasingly important for cooperation right and therefore we were to think about AI in the context of cooperation so the importance of cooperation is probably something that I don't necessarily need to tell you all about um but just that uh many of the kind of biggest and most important problems that we're facing are about problems of cooperation so this might range from kind of climate change and war or pandemics and all sorts of things like this all of the kind of global uh kind of coordination problems and challenges that we face um and yet also many of our great successes have been gutile ability to cooperate so we have trains treaties collaborations Etc um and I think one of the takeaways here is that um as societies and economies and industries Etc become kind of more powerful more interconnected than the needs for cooperation grows uh so when we're all kind of little tribes living on little isolated Islands it's more scales you know you didn't really need to I think this is the kind of cooperation tools that we could get away with using were kind of relatively primitive and this was kind of all fine and now that's kind of changing um um and so this is the kind of second part of the story which is that um I think and I think it's reasonable to think that advanced AI is gonna both increase the power and interconnectedness of those actors and therefore the need for but also hopefully the feasibility of cooperation so on the one hand you might think that uh AI systems might exacerbate our worst cooperation problems so probably everyone uh has like some idea of what this represents but this is a little graph of the flash crash uh which I don't need to talk about in this room because I'm sure you're all more than familiar with what that was um there we go uh and then on the other side of the thing you might realize instead uh you might ask instead if we can actually kind of reverse the question and maybe instead of AI being making things much worse we can in fact kind of Leverage AI to make things a little bit better and so this is what this incredibly trite and cliche uh graphic at the bottom represents um yes it is okay so one thing I want to pick up on which just really quickly before I move on which is also something that surfaced and Jin's talk the other day when he was talking about the space of commitment devices that you might be able to design and how that might kind of uh be useful at some levels but actually then lead to kind of worse outcomes from a kind of welfare perspective depending on kind of how you design that space um and this is basically the idea of this sliding so in some sense Cooperative capabilities can be dual use so forming Federal commitments uh can be used to kind of make offers and so on and do all the things that we want to it can also be used to make credible threats right this seems bad um reaching we truly beneficial bargaining Solutions um would also uh lead to collusion when we don't want collusion so I think it's interesting that I'll touch on this and set in a bit but one of the areas that we already see sophisticated multi-agent systems uh already existing in the real world our Market is an automated trading agents and so on um and uh yeah but actually we usually think the competition there is a good thing uh so we actually would be maybe less happy if a bunch of our automated trading agents suddenly decided to like start colluding with each other and this might be bad from the Market's perspective so to speak because the negative externality is induced um and then final example is um this idea that if you can form alliances and factions that actually could lead to Greater conflicts right so here is a picture kind of illustrating then it's the Cold War blocks in some ways and actually like it was you know partly the fact that these two superpowers emerged then could kind of gather so many extra capabilities and resources that precipitated some of the kind of worst crises uh event and if you couldn't in fact join with your neighbors to try and like take down other people uh then uh maybe this would be something will be more avoidable okay so ideally what we want is this notion of what I'll call differential progress on cooperation so I don't know if this is this might already be something that's familiar to some of you but here's a graph uh it's toyr it's not real bro um and the basic idea behind differential progress is that um you might want to give AI systems capabilities which can uh improve their kind of ability to cooperate more than they it can improve their ability to do other things um and so you know capability here could be some kind of cocktail capabilities and capability here it could be something else I don't know uh it's got a kind of guess but messy here but um the idea is that kind of yeah what here like sort of for like most things I don't know obviously we have a bunch of stuff which is actually like superhuman at this point um and you know maybe like image recognitions like air or something on this axis and like I don't know uh writing novels is like here uh and there's like a bunch of like you know planning physics experiments and maybe like even less good or something um so there's like we're like somewhere in this extremely high dimensional space right and and then there's this idea that well what we really want to do when we're kind of navigating this space is to kind of steer development of AI systems uh in a way that kind of can give them better Cooperative capabilities uh more than it kind of allows them to do other things and the first thing you might think is like okay is this even possible is this even a coherent concept isn't just everything like arbitrarily dual use which I think is like an interesting question maybe like one like extremely minimal existence proof is that you can think of him kind of just in kind of uh purely Cooperative settings where you but where you still face kind of equilibrium selection problems so where quarter ordination is kind of non-trivial then you can introduce things like a cheap talk channel right so in a cheap talk Channel we can use that to kind of coordinate our actions together and yet worse comes to worse you can just ignore what I'm going to say right and so it doesn't actually it doesn't it's not going to make anyone worse off necessarily um and so it might be interesting to think about whether there are certain kinds of capabilities or certain sorts of things that we can view our artificial agents with that have those sorts of properties or that even if they don't uh kind of have no kind of downsides to them might kind of steer us roughly in the right direction um so quick thing about what is cooperative AI is and what I actually mean by that so I'm first going to talk about where Cooperative AI is uh so General product project of making AI systems go well uh and then you can kind of carve this up very crudely at least they're not perfectly into technical work and non-technical work and then you can maybe carve things up in technical work into stuff like alignment and interpretability and robustness and Cooperative AI uh and other things and in non-technical work you know you maybe have policy you have Norms you have institutions you have governance mechanisms you have all sorts of kind of socioeconomic kind of standards and kind of practices Etc um and okay well this is a little bit delay um so yeah here's where property bear is so it's kind of interesting in that it's like technical so we work specifically on kind of uh building better AI systems uh but we solve problems that we try to solve problems that look a lot like non-technical and governance problems um which is maybe something that uh you are all also familiar with right because this is kind of similarly uh sort of the kind of sort of value proposition in some ways as well um and then kind of like I said what is well today I um so it's probably a kind of subfield that they are that uh aims to improve the ability that AI systems to engender cooperation between humans machines and organizations by which I just broadly mean uh groups of humans machines um and the basic aim is to help produce risks uh I thought I took this out this isn't X risk which means existential risks which is something I care about but come and talk to me about that afterwards if you'd like uh from cooperation and failures resulting from from AI um and it forms therefore the natural kind of complement some detectable work and Alignment as well as some of the non-technical work on air governance um and so one other way to kind of cash this out is to talk about the task of improving the Cooperative intelligence of AI systems uh where one kind of like hand wavy working definition is this which is based on uh the um rather well-known within the field of AI at least think will be leg definition of universal intelligence uh which is a kind of informal of what it means for any given agent to be intelligent but it nonetheless has some very interesting kind of theoretical basis to it and so the idea is that the Cooperative intelligence of an agent is its ability to achieve High joint welfare in a variety of environments with a variety of other agents um and one thing I want to point out is that this definition focuses on capabilities rather than dispositions right so this is like I I in fact may be capable of kind of helping everyone to reach a very very Cooperative solution I may just not want to and likewise I might like really like desperately want everyone to get on and so like the world to reach this huge kind of fantastic Cooperative uh kind of outcome and yet I might be unable to and so there's a there's a kind of Distinction here capabilities in dispositions which is related to this issue of alignment which is like closely related to cooperation but very much not the same thing and that's something I'll touch on in just a second uh in fact right now um so what is alignment you've probably heard about this it's in the news a lot right now uh um but better or worse um so Enlightenment is basically the problem of you have a human or you have some principle that you care about and you have an agent uh and here's some kind of uh and basically what you want color color denotes kind of preferences or objectives or something and you want the other agents the robots to care or act according to the human salaries great that's what alignment is in a cartoon here's what cooperation is in a cartoon at least as construed by me which is you start with a bunch of agents and they all have um objectives already right they're they're all got nice little colors indicating they all have nice little objectives um and what you instead want to do is you want to take them with their kind of preferences fixed as input to the problem uh and you want to make everyone get on and to reach like good outcomes where like good might be something like I don't know getting closer to the perito in Frontier is like a first approximation of this round um but then there are all sorts of other ways you might want to try and cash that out um and then like one kind of again extremely like cartoonish way that you can think about this um is that sometimes people are like oh kind of let's just focus on alignment Etc let's not worry about these like operation problems um but basically like alignments doesn't like get you all the way right it gets you like part of the way but then here we have just like a bunch of access uh who all maybe have you know I have my AF system you have your area system great they're aligned and now like you know we're gonna be uh potentially in some kind of General some scenario and this might end badly for example knockout so we still also want to be working on the start of the two part of the problem too so you can kind of compose these two things there's like more or less captures um the kind of like broad problems from a technical sense of like making AI assistance go well and one kind of alternative view on that is that alignment can be seen as addressing this kind of vertical problem and cooperation that can be seen the progress in a horizontal problem so what do I mean by this uh here I'm indicating that by by this kind of like vertical axis I'm indicating some kind of like enormous precedence right so we just in fact care about this agent and their preferences more on this agents this is why they're kind of like delegates into that agent right where this is like a little robot and in kind of cooperation problems we usually don't think that like any one particular agent is like privileged up Beyond any others right um and so it's much more kind of at the same level which we're trying to get them to coordinate together um and then obviously the world is really much more complicated than this and in fact the world is much more complicated than this picture too here I'm sure you've all noticed that um and this brings me on to the real world which is a segue that I didn't have in my head that actually works really nicely uh yeah Okay cool so so just quickly then just talk about kind of problems and progressing up to there in terms of what people are working on at the moment so you might first ask like why are Corporation problems in the context of AI different why is this not just like regular AI stuff slash why is it not just regular like study of Corporation stuff which has been studied in a bunch of different subjects for a very long time um so the first point is that cooperation can be harder because AI agents don't possess simple features that allow biological creatures to cooperate and equally cooperation can be easier in some cases because air H Tower at least can be constructed to have non-biological features that might also allow for better cooperation um and so these features introduce a range of kind of interesting open problems some of them are more or less open so the first is kind of uh things about understanding so the world the behavior of preferences of other agents and dealing with recursive beliefs so I believe that you believe that I believe that you believe something comes up and getting through all the time we can also think relevance this ability to kind of form commitments by devices such as delegation contracts including hardware and so on we can also think about communication so dealing with issues around kind of Common Ground important problems of bandwidth latency teaching learning Etc and then finally at a kind of metal level we can also think about institutions more broadly and this might be informal institutions such as Norms or reputation systems but they might also be kind of much more formal as well um and so then the thing that I want to kind of leave you with and I'm going to kind of use this as an excuse to pretty much wrap up and and hand over um is that I think one thing that the AI field likes is uh to drive progress is like test domains and challenges and like things like this right so I don't know if you heard about the imagenet competition uh back in around 2012 which really ended up driving much of the progress on deep learning and kind of highlighted to everyone how powerful these methods really were and kind of kick-started a lot of work in that area a lot more work in that area obviously it's going on for a long time um and uh yeah in general uh this is like a feature of kind of lots of AI research uh is that people get hooked on these like specific problems and challenges and what sorts of tools and capabilities they can uh use to to solve them um you might actually argue that this is now shifting away uh from this Paradigm uh with the use of kind of these called like so-called Foundation models and so on where we actually just train some like massive uh system based on a lot of kind of like relatively unstructured data and information and then fine-tune it to help us solve Downstream tasks um but nonetheless I think this is an important Paradigm so like with my field building hat on which is like however work I do uh then I want to claim that we actually need some kind of real world experimental test beds in which to empirically test corporate agents um and this needs to have ideally kind of certain features right so the first is that they want to be kind of large and complex and actually relatively similar to real world problems that we are likely to face but still kind of scoped enough to be studiable um second you need something that can be done kind of safely and in a at least relatively like low to moderate Stakes way um uh yeah you don't want to accidentally like mess anything up too badly if you're deploying systems in the real world I also need to be informative so you need to be able to kind of like monitor these experiments and systems and actually gain relative information about them and ideally although this is always challenging in the real world you want them to be to some extent at least kind of reproducible this is all stuff about just like good science right basically um and ideally as well you also as far as possible want them to be kind of important and relevant to real world problems that people actually care about so there might be something that's happening in the real world such as I don't know I'm not gonna be able to think of a good example uh like the way in which like pigeons like congregate in certain parts of London or something uh like people don't no one really cares about that oh like it's not that important it's an interesting real world it's like a real world problem maybe that pigeons end up in certain parts of London you don't want them to be maybe you can like Shepherd them into like various others I don't know where I'm going with these examples uh but but like ideally if you want to hook people in like image recognition image recognition comes up all the time it's like a useful thing it's partly why people are like oh wow image recognition um and so ideally you want to try and hook people in with something important but also relevant um but in reality there are actually like pretty few General sounds that just created multi-agent systems out there right now um this probably looks like it will change in future so I think this produces like then like a really open question that's like how can we start working on some of these things like at scale in the real world trying out some of these kind of slightly more weird and wonderful ideas from the kind of computational Sciences uh and so on so basically it leads people to work on things like this um which is I don't know if you've seen this is AI Economist um and uh if you actually look at the paper this is just like a graphic that they made on top of the paper the real world doesn't even look this it doesn't even have 3D Graphics or anything it's just like it's like there's a reason that sounds like it's really bright here it's actually just an insanely basic grid World um but nonetheless they put this on the website because it looks cooler um anyway uh but essentially what you'd end up testing in these things is you have like very primitive kind of grid world-esque models uh and you do multi-agent rle type stuff and and so on and it's like this isn't good and important for like testing Integrations but as we all know in real world there's a bunch of kind of complexities which can't necessarily easily be captured by these things so I don't necessarily want to be disparaging towards good worlds because they are nice and they are very useful um and this is more like yeah this is my impression of like this is what the real world looks like of course either uh but uh this is maybe the in contrast to this um okay and the final thing I'm Really Gonna quickly gonna do so okay this is like think about this okay your job for the rest of the afternoon it's like pitch pitch interesting things about like where some of these like things that I've very Loosely touched upon and things that you'll see much more detail on in the following talks and just like try to think about like some of these ideas because I think like I do think there are interesting synergies here I think that maybe there's stuff that we can contribute and maybe there's also stuff that uh you can help us like kind of like you know you can help us get a nice uh shiny stick and point to your thing and be like look a real world problem and we're gonna solve it um and then yeah as a quick kind of plug uh to some of this so I work at a place called The Cooperative area foundation uh these are the trustees so this is on the phone these are deep lines or corbetts Microsoft Julian Hadfield she's at uh open air Toronto this is Dario CEO of anthropic and this is Rue uh who's at Polaris Ventures who's our Thunder um and we're a kind of charitable entity we're a Research Foundation uh we're field building organization uh this is kind of our mission and if you this actually kind of makes if you're interested this is my like regular Spiel slide so we have a bunch of things uh that you can find out about us online um and if you I think you've already already mostly done on all of the further reading because Jin was very insistent three of these things the slightly incredibly short notice uh so you've already read these things I guess but like if you want to read more things come and talk to me or read the papers that are about to be presented on which maybe you will have also read by this point but anyway that's it um yeah I don't know what time it is now slash when there's time for discussions wait 2 30. fantastic thanks we've actually got loads of time um so yeah I don't really know I could open it up I guess first of all just in case there are any kind of questions or any things that uh anyone wants to kind of like raise or flag or chat about or if anyone has any solution like uh interesting examples of these kind of rule problems and so on um that could be an interesting thing to do kind of like now ish and then uh I kind of like slightly through uh Andreas under the bus uh but because I came to his uh cook yesterday morning when I arrived in London or he's a poster and uh it was really cool uh and it's relevant to commitments so I've now invited and give a talk in this slot uh instead or as well as me um so he's very kindly agreed to do that um but I think I still think we have capacity um so I'm gonna stop talking now um and if anyone wants to question someone can do that when you talk about real world examples um you look at Online Auctions um so eBay is when they don't go where people are like um can see other examples um yeah lots of people do work on this and I do not personally work on this and I guess this is like one of like the main applications of like serious algorithmic gang Theory and so on um which is like a very good example um yeah no I've not really thought about that personally um but yeah this is great more like this please uh yeah so I'm very useful the it does seem to me that uh what we call AI in a two basis might be different or different levels of capabilities in the nine year program you need the AIS to develop their own goals with an AI complex enough to have their own goals so that I like the peace Accord pressing Cooperative alignment impression is that with much less powerful AIS you already are like an interesting thing to talk about right there in a sense that if you're like in your area in Australia government say we want to play some game with humans and with like use of programs don't like make the outcome better you already have problem there and the AIS there don't need to have like uh have big capacity to get rid of their own roles does this make sense yeah I think it does and I do agree that uh like a like alignment when you have extremely basic agents is like not that much of an interesting or like not as much of a problem um like the basic thing you can you know you can just like train some like incredibly basic agent perform some incredibly basic tasks and like I don't know it probably just does the first time around if not if it's incredibly simple you can literally just like look at the algorithm and just like change it this is just like the basic idea of like debugging code or even if it's doing something like slightly like not what you want ah it's just kind of like operating on some like small system and you can just like fiddle around with it and hack around with it until it basically works and you can be kind of satisfied that it's going to do what you want it to and then if you have extremely like powerful and capable agents that are doing much more complicated tasks uh potentially even tasks that humans can't really understand right so this is the ideal aim uh with with AI research is that there'll be machines will be able to do things that humans can't even do um and then as soon as you have that sort of thing then it becomes uh yeah much more tricky with the alignment saying uh and maybe a Cooperative problems like do emerge like maybe like earlier in some sense because it's like yeah more feasible that things could be uh yeah kind of misaligned with each other so to speak um at the same time however like we also when games are small and when agents are simple we can like just look at them using standard kind of game theoretic methods and kind of like analyze them and like solve problems and proof theorems about them in a kind of like quite nice way in a way which we aren't usually do with like big sophisticated AI systems so I think that yeah both problems definitely like scale and get much more difficult with uh with kind of complex with kind of the complexity of the agents um yeah yeah I was um interested in what you're saying about the biological teachers um that should be either like avoided or maybe purpose to facilitate uh Cooperative Ai and I what kind of I didn't agree with the the premise but Leia was kind of wondering whether you have it like more specific ideas or things that might be like night to replicate um in terms of biological features and things that shouldn't be avoided like what are um maybe like one idea for a biological feature which would be nice to replace machines all those potentially risky is a theory of mind so humans have theory of mind right I I but I which player which I mean kind of I kind of like look at you and what you're doing and I can make some pretty good inferences about uh what you kind of want or are thinking about based on your actions uh because I happen to at least believe uh that your mind is sufficiently similar to mine where I can do this and machines don't have really this notion um so kind of inference of intentions and so on an intent is something that humans actually find pretty mostly quite easy uh at least like we sometimes think of it as hard but mostly in complex settings like most of the time it's actually pretty simple um and machines don't really have this yet so this could be a good thing um and then yeah with the kind of like non-biological things I'm thinking more about things like uh yeah they're like speed uh at which machines can operate the fact that they can operate in highly distributed settings uh the fact that uh they are written in code and we can actually like look like in theory at least we can look at code and like figure out what it's doing and why now this is pretty hard when you have big inscrutable neural networks but uh this is why a bunch of people work on the typology and so um yeah there are kind of things like that um like help you a bit more but maybe like um the memory part of pledge what uh what I do because it's been studied in the AI right quite particularly like oh all models can forget and but you deal with that and I was wondering whether are you you can like take inspiration on some biological features of actual human regarding memory and kind of um you know yeah I was just wondering specify something I know yeah so there's there's definitely work about this in like so like the idea of like computational boundedness obviously or boundedly rational agents like does come up all the time in Game Theory and so on and in this study too in kind of like AI stuff but my impression is actually like not as much as you would expect given that like uh like AI people like underly CS Theory people love to talk about like computational bounds on things and like you know uh like complexity stuff uh And yet when it comes to like applying those sorts of ideas to kind of like games and and kind of in multi-aging kind of interactions my impression is that this has been done less uh Nathaniel actually might be like a person who's able to like comment on this more easily than I because this is like slightly more of his wheelhouse but um yeah I don't know if you're having is that a thing am I like mischaracterizing people who actually work on this loads I would agree the dominantly rational stock is less popular like it's pretty un poor I shouldn't say on but it's yeah oh yeah perfect snow as well yeah so yeah the problem is that like why the groups made up for which violence so this work in the last few years on mechanisms but specific performance nationality which is absence of contingency that works for humans but doesn't work for machines uh there is work done by ultimate plus yeah so that's why it drives to model computational complexing decision making you know part of my work it's trying to actually the techniques have developed for it's called obvious strategic to see whether I can apply them to this um open and pass computational complexity stuff but it is company and I agree it's very much understudied but it's because of the that's not something many people want to do that's unfortunately ideology uh and not that many people went to it yeah that is the one paper that I always think of it's like yeah how can I pass it's like I think it's called like Game Theory with like computational costs or something uh yeah uh is cleaner I think as in what might be a model uh but yeah but it again it works for humans you can set out and I you mentioned the concept uh you know the reasoning uh contingently so this yeah so it's basically people and I'm not able to believe the Nets from their head when they make a decision just I believe you so this is experimental finding if you run seven plus options it's exactly the same option so people should not play differently but in practice they misunderstand misunderstand the former maximum so this honestly concept exactly captions and there's also a dimension yesterday when in the Q a we do there's also one Dynamic speed amongst movements which again tries to uh get to the bottom of why people understand certain options of making some steps but I think in the area or in this context if the programmer of the assistant is on the same incentives you're still at single yeah well because they cannot call it up properly I had a different question but I don't know I think yeah I call a very quick question which is so I was thinking about ways of enforcing these conditions right um a very interesting uh question is you know how can you uh ensure that if there's a greater Collision it's not split up and you know something that I was wondering uh and you know has been obviously explored in the real world is critical threats and you know something an interesting thought could be whether enforcement would be from One agent on to the other so for example on you know uh if say some agent breaks from the Collision then the other artificial agents could uh leverage yeah I'm giving up very uh you know um stable example right um you know could try to find some kind of vulnerability right or a way to uh you know attack and so forth and the threat uh of this even that these agents say are very capable or whatever uh is you know uh has the effect right the uh of course into some degree Collision so my question is you know uh has there been uh any you know do you know any resources or work exploring uh threats among agents as made of enforcing these things um uh I guess there's like loads of stuff on this um like this is basically for example like what the Grim trigger strategy does uh which is the kind of source of the folk theorems in Game Theory which is like some of the most kind of fundamental results which is basically like playing a iterated version of like personally or something and you're like cool the way I'm going to get used to copyright is if you like ever defect I'm just gonna like punish you for eternity or something um and uh at least in like infinitely repeat again and and uh so so yeah and in terms of like coalitions I think you can do this as well I actually haven't worked that much on this kind of Cooperative Game Theory type stuff where you think about coalitions but um yeah often there's uh positive to be some like mechanism via which you can in fact kind of uh like punish uh deviators essentially and this is what allows you to form the coalitions to begin with um so I kind of mentioned threats as like an obviously bad thing but it's actually far from true that they're an obviously bad thing right we have like socially kind of desirable kind of threats made all the time right like the fact that the state has a monopoly on violence and if you kind of go and do something horrible you're going to get present is like you know that's a threat if you do this you're gonna go to prison but it actually just turns out that we want uh those sorts of things mostly uh and so yeah I think it gets back to kind of Shin's point before about like okay like what commitments do you want agents to be able to make what commitments do you not want agents certain agents to be able to make and so on um but yeah I mean yeah like PVP are between uh artificial agents uh so to speak right um yeah excited commented maybe one of my questions so no your framework you went human regions alignment yeah agents regions cooperation right yeah although you can also think of cooperation is like happening at like a level between kind of humans and AIS or like humans and humans all like any sort of agent where you don't immediately think there's like some kind of like normative precedence I guess um so it's that's it's that's what it's that what that's like and usually we think of humans as having normal precedence over saying machines uh and who knows if that will be the case well the alignment is perfect so as even as a supplier I want to maximize my reward yeah so the agent does exactly that and when they cope when they go under the award there's an emergent cartel even though they were not programmed to the captain then they do the cardboard so which means the problem is in the societal alignment yeah right that's what I'm trying to say and this is much harder maybe yeah definitely um but you also might think and I think uh Casper might talk about this a bit later potentially is uh that if we can in fact delegate to AI systems to make some of these decisions on our behalf they could actually end up being like better at cooperating than we are um and so they might take our interest into account but they might help us reach uh kind of more kind of Cooperative outcomes um uh but yeah fundamentally there is definitely like a bunch of these problems are going to actually surface at the level of like societal kind of like Corporation programs uh instead of like what's happening at the Asian agent that also to speak yeah efficiencies were brought up about the social Corporation in New York's modern Norms or how AIS May learn some social laws of the car as humans invoke very better yeah definitely there's a bunch of work in that area it's not something I personally have worked on myself but there are lots uh lots of work on kind of like yeah norms and AI systems and so on Norms and multi-aged systems and various different ways of formalizing them as well exactly yeah so some some people do it as kind of like these like logical constraining something people do as like kind of soft kind of like losses and yeah some people do them as kind of like system imposed some kind of try to model the emergence of norms um and yeah there's very many different ways of approaching it um I don't necessarily think it's a solid problem but it's definitely something that like people also celebrates yeah in general I'm like kind of fits this audience I guess I'm like kind of like anti approaches where you have to rely on something like massive system designer who can just like impose a norm on like everyone else uh ideally you'd want something that's kind of like self-sustaining and somewhat decentralized uh because that is actually what's going to function at scale in the real world um uh yeah and I'm also usually a bunch of this like Norm's work was done in the context of like various kinds of like Logics in multi-aging systems and so on which is like very fashionable in the 80s and 90s and so on and uh has not really stood the test of time if you asked me personally uh I can say that because my supervisor isn't here it was basically like the person yeah yeah and I don't know if anyone knows them but uh yeah and uh yeah all of them okay thank you I don't know very cool there's also work on like learning to uh yeah training agents to like be able to learn what Norm exists seems like thought right with that yeah including scr or the center alone which is why the family used to work I also temporarily um in fact yeah I don't know is Anthony here Anthony's worked on this is Anthony going to present later I've forgotten schedule yeah yeah great you can ask him about this he's worked on this uh yeah and then like one one kind of obviously interesting question is like okay so say we get to the perito frontier great that's good now we just have to like pick a place on the burrito Frontier hmm all of a sudden I guess like pretty challenging um because you can think about kind of like what trade-offs to make between different agents and so on and like what is right there um and it may in fact be the case that agents just have kind of different intrinsic Norms about this so maybe I adopt some kind of like total utilitarian style so maybe you're kind of like more of an egalitarian and you want to kind of like minimize maximize the welfare as the kind of worst person and things like this and these things are then just like in some ways just can be kind of incompatible often and so then there's a question about like what do you do when you get to games with ink battle norms and this is some of the stuff that Anthony was working on so you can ask them about this thank you or [Music] I guess it depends what kind of trying what kind of field you're trying to build uh this is like an academic field so field building in an academic field mostly looks like um trying to build up a community of researchers and practitioners who are solving who are like dedicating their like working efforts to try and solve these problems um and in practice what that looks like is doing things like helping like co-run events like this or workshops or uh yeah putting on we're doing a kind of contest in Europe's which is the big AI conference that's happening in December so there'll be a popular video contest there um we like publish materials online we're hosting a summer school uh we're going to like do kind of Grant making yeah sort of like what you'd mostly what you'd expect uh but yeah you have any ideas of how to do field building better but you know also come talk to me and tell me how to do it and that's 