[Music] if uh if you're feeling happy um xor your private key is an odd number clap your hands um if you're happy xor the second bit of your private key is odd clap your hands okay baby there's not enough time to extract private keys um okay so great uh so what i'll talk uh going to talk about today is basically an overview of how upcoming changes to the ethereum ecosystem so both ethereum lawyer one um and ethereum lawyer 2 are going to end up affecting the immediate landscape what differences they'll create between mev as it exists in ethereum today and like the way that the mov ecosystem will just have to be structured um in the years to come um and basically like what can we do to make the best time out of uh out of the world that we're going that we're going into um so start with uh the first thing on the timeline right the merge um the merge is happening i mean i hear the court devs are saying like 2027 or so uh but um you know it's happening it'll it'll be here eventually um uh and um riverstage is going to significantly affect a medium right um so one of the kind of interesting and very subtle ways in which a proof of stake affects anybody is kind of this fact that you know basically proof-of-work style sgx protection is kind of like can't be done in the same way right so just to get into the weeds here um this is uh a technique that's theoretically possible i think there's like a version of this that's kind of being uh being developed or has been developed for proof of work but figuring out how to translate this to a proof of stake context is not very trivial but the basic idea is like how do we ensure transaction privacy up until the point where those transactions actually do get included into your blog right and so the basic technique is um that build um miners could have an sgx module and they would receive uh bundles uh that get um encrypted uh with a uh a key um that only the uh exit where the private key only exists i mean inside of the sgx module so anyone can encrypt you with a public key but only computation inside of the sgx can decrypt uh and basically they will get these encrypted bundles then the miner inside of the sgx would be able to build a block i mean construct the block body construct the block header if you assume sgx is safe i mean a big if then the miner has no way of kind of extracting the contents of the bundles out so they can't do mv stealing um and then the sgx enclave would only release the header um and then the header would go to the miner the miner would a mine and once and only if the miner actually creates a valid block uh would the enclave actually release the body right so the core idea here is that like the body in cleartext only becomes visible at the moment when a valid nonce is already created um and so if the minor does anything other than just immediately publish the block exactly as it is then the minder pays a really huge opportunity cost right because they basically lose that block um so this does not translate that easily to a proof-of-stake context right the reason basically is that in proof-of-work the main form of authentication on the block is the uh nonce that gets checked with a proof of work function in proof of stake the main uh thing that you check is the signature with the proposer's private key but the problem is that you can't just make uh releasing the body conditional on the signature with the proposed private key because while the proposer can make that signature then they can get the block and then they could sign use that steal the mev generate a different block they would double sign they would create like actually have to sign two different blocks but because they never actually published the first block they would never get slashed for it right so this is kind of the key reason why the proof-of-work approach doesn't translate into a proof-of-stake directly um now there are some kind of some solutions to this um so one of them is but basically what abv boost is doing which is uh if you rely on kind of doubly trusted intermediaries so intermediaries that are trusted both by the validators and by builders so you know we call these relays um then they can basically be trusted as oracles to kind of store the block um and not actually release the block until they just see that there's a signature from the proposer um and then at that point they're the ones in charge of releasing and so the proposer can't really trick them and then the second approach is if you have some kind of in protocol pbs then you could design the pbs in such a way that like basically after the proposer signs there's a round of signatures and then after that round of signatures the the the body gets published and then this kind of release body check uh basically instead of uh depending on just the proof of work or on just one signature it would depend on the entire committee signature right so basically the body only becomes visible once the lots and lots of attesters have actually agreed on the header um so this is like one possible way to to make it work if you don't want to have like any trust assumptions except the majority of validators um so that's kind of one way in which proof of work and proof of stake are just different right and like you have to design differently around them they have these uh different properties where they're not exactly equivalent to each other um another really important one here is like the next proposer is known right so in proof of work the next proposal is not known um and this has interesting consequences so like for example if you want to do cross domain mev between multiple proof of work chains then it's harder because there's no possibility of like an actor knowing that they're going to be the proposer uh the next proposer on both chains but in a proof of state context it actually is possible right because you know ahead of time that you're going to be the proposer and so let's say you have 20 of all the stake on chain a and you have twenty percent of all the stake on chain b four percent of the time you're going to be the next one on both um and so four percent of the time you're going to be able to extract the cross domain any view right so this is like both potentially useful but it's also a centralization vector because well you know if you have ten percent of the stake you could do it one percent of the time if you have 20 mistake you can do it 40 of the time if you have 50 mistake on both sides you can do it a quarter of the time um so like it is a centralization factor and like this is one of the reasons why um i think uh kind of thinking about the cross domain um mev problem and kind of creating ways of uh getting those gains without actually needing to be the proposer it was uh so important um another interesting issue is that the proposer has this kind of game where they can like either publish at the normal time and then be guaranteed that you know yeah if the network is not totally broken the attackers will see everything in time and they'll and they'll publish or they can wait a little longer they can wait and try to get some more mvv you know instead of publishing at t equals zero they would publish it like t equals 3.7 um and uh you know basically yeah kind of you know live on the edge uh then try and like get to the point where like if on uh you know if more than 51 percent of attesters um uh see or block on time and sign off on it then in some ways you didn't publish late enough right so you can kind of play the edge award game if you really want to um and you know this is something that doesn't really exist in proof of work but it's uh you know existed proof of stake which is also um interesting now maybe if we somehow change how proof of stake works you know you could use vdfs to kind of simulate more properties of proof of work um but um you know this is a an issue um also the possibility of making side deals with the proposal right if you know ahead of time that you're going to be the proposer you could send transactions directly to them like i even know there were chains way back like i think nxt back in 2013 like they tried [Music] i forget maybe this was what they call transparent proof of stake or something but where you send transactions straight to the next proposer and it's like supposed to be more efficient but of course it also has dos risks um and then you know if you use single secret leader election then while the proposer doesn't actually reveal who they are until they make a block um and so but then they could still reveal who they are to a few people if they want to make some side deals so you know there's some issues that you have to think about there too right so the question of like how do we minimize trust in the builder market is um interesting right and one of them uh so this is starting to get a little further beyond proof of stake right proof of stake does kind of change the game in a lot of ways it basically means that like these designs for markets have to be designed differently in like a whole bunch of subtle ways that are not very easy to like summarize in one sentence um but you know it's just a different system it has different properties um so mev boost is this kind of shorter term technique um right so uh i know unfortunately the images are a bit small but um if you just uh you know if you search for both of these on each research you can find big versions of the images so like uh you know feel free to basically mev boost uh says um you know you have builders they send their blocks to the real layers relayers send to the headers to the proposers and then the relayer publishes the block after they see a a valid signature and then in protocol pbs is this more complicated thing where it tries to remove all of the trust assumptions involved in uh relays and this particular thing is the two swot version of uh p of proposer builder separation where basically what what we do here is uh instead of requiring the proposer to directly um or instead of expecting the proposer to directly create the block contents you have an explicit in protocol auction right so it's like even slots are basically auctions for like either who's going to create the next block or like what the what the hash of the next block is going to be and then odd swats are just there for the winner of the auction to be able to publish right um so it's a different architecture it's an interesting architecture and it does have the benefit that it lets you basically separate out the proposer from the builder and keep proposers very decentralized well-behaved internally right so can you make builders more decentralized internally so a lot of the thinking in tank sharding for example is about like can we make builders more decentralized than internally um actually yeah one of the benefits of using kcg is that you can do that much more easily than in a lot of other systems um and you know there's the question of like well can you use like sgx type stuff so that none of these decentralized builders can give people more kind of pre-transaction privacy right so users can't get exploited uh so in protocol uh pbs markets like basically you know designed the ethereum protocol around the assumption that just random validators and builders are going to be these kind of separate classes of actors where builders are just likely to be much more sophisticated and i guess the hope of many people in this room is that builders don't even have to be like monolithic kind of individuals organizations builders themselves can be you know fairly decentralized ecosystems right uh and this uh so there are going to be kind of more assumptions you know possibly governance layers i mean trusted hardware whatever whatever on the builder side but then the proposer side is kind of even further insulated from all of that right um so basically the kind of the goal here is to keep on reducing the trust assumptions and the reason why you reduce the trust assumptions is because that increases the possibility of decentralization right so today builders have to trust miners which is horrible because only the big miners can be trusted um tomorrow builders and stakers so and validators need to trust relayers so nobody needs to trust the validators though you don't need to trust the builders and so both of the both of those can be fairly decentralized and then relayers are kind of this relatively dumb functionary you know in principle you can have a whole bunch of them in principle some builders might merge with three layers but they don't have to and there probably will be at least some real layers that you know are willing to work with anyone so still some trust but it's kind of better it at least creates the possibility of more open markets and then in protocol pbs is like no trust and it allows uh you know validators to come much closer to this role of like basically like just being dumb functionaries who execute code and who don't have to like make decisions of you know hey which real layers do i trust um so tank sharding so this is uh happening in parallel with pbs but is a one of these other big things on the horizon um basically the idea behind dank sharding is this uh new form of uh sharding that tries to be like really ultra minimal on like how much it actually shards right it doesn't chart execution it only shards data it doesn't even start proposing it basically says there's still one proposal like there aren't actual shards it's all kind of sharded in the severi sort of continuous way where you know it's still technically sharded because every node only needs to download and check a little bit of the data um so you know it fits the definition it fits the goal of giving much more scalability but otherwise it like tries to pretends to be among a sim a simple monolithic chain like as much as possible right and that like reduces the conceptual load of having to deal with the system and has a bunch of benefits um so make some what are the consequences of gank sharding right so one of them is that it it is probably going to make some kind of uh proposal builder separation mandatory the reason why is because you have these uh 32 megabyte blocks and the proposer of a uh uh of a block like we're not we can't realistically expect them to actually be able to download and check like 32 megabytes and potentially more like potentially going up to 128 megabytes or more in the future right it's like you know ethereum satoshi's vision here except it actually works um uh but uh you know basically yeah you can't expect individual validators to be able to have that right but and so you you are going to need to have some separation between the the builders who actually will needs to kind of handle bigger amounts of data and validators which only needs to actually verify a tiny portion of the data individually so there's actually a few different options for how to handle this so one of them is to go the route of pbs um basically you have proposer builder separation you have auctions and then okay fine you have one builder and the builder has to like handle you know how the heck am i going to make a make 128 megabyte block and if they have amazing data connections they'll be able to do it just fine um and everyone else only needs to like download and check a little bit of the data um option two status quo plus availability oracle so this is like an interesting intermediate option right this is like if we're allergic to pbs then like how could we make it work and the idea basically here is that with the with these blob transactions right where in dink sharding basically blob transactions they have a header and the header is very small a couple hundred bytes and then they also have a body and the body is currently 128k it might go up a bit in the future um and uh the bot so but what you can do is you can basically have the headers and the bodies be broadcasted on different subnets right and so then as a builder what you can do is that you can listen to the mempool that contains the headers you can grab transactions that have the headers and then for every header like simple and dumb solution you ask a channeling oracle like hey is the body available and you know if it tells you the body is available you accept it and you don't even try to like figure out whether or not the body is actually available yourself and then you publish the blog um and then you basically just rely on like the distributed self-healing and like you know two-dimensional data availability of sampling blah blah blah to make sure the network kind of gets all of the con uh all of the data and the contents that it expects um so also an option uh but you know this would require proposers to know which oracles to trust um and like it requires proposers to make these more active decisions i mean which is not very nice option three is like builders plus availability oracles right so this is like what i mean by the district uh decentralized builders right basically you have like a builder or that just looks at these uh headers i mean that tries to just make an optimal block based on the headers but then you also like they listen to various availability oracles and they just kind of outsource the job of figuring out whether or not the data is actually there to other people um so within the context of dank charting if we wants to have pbs um if we wants to have kind of mev optimization be compatible with uh decentralization like you know you have to start looking into some of these issues um layer two protocols so we were talking about layer one for now layer two was like a whole other thing right and uh layer two is interesting because it does look like very significantly impact how the emmy the landscape works uh basically um and the kind of impact that it has really depends on how the layer 2 is structured right so basically how is roll-up sequencing controlled what is the mechanism that decides who actually has the right to publish the next optimism block or the next arbitrary block or the next you know zika sync block um or you know the next stark net or what a block or whatever else um so the key question is like yeah how is rope sequencing controlled one very simple and naive way to do it is like anyone can submit right anyone can submit the next block historically i think anyone can submit has been rejected because anyone can submit would lead to like a lot of overhead from basically multiple people trying to submit the block at the same time and uh most of them like not getting in um but then still having to pay because like their transaction still you know pays gas um and so you just have like a lot of junk on the ethereum chain and that's like not very good um but with a much stronger and health theory vehicle system that problem doesn't exist anymore so you could just like do anyone can submit i think the reason why i don't expect anyone can submit to be popular is because ultimately like roll up teams they're going to want to capture the mvp about value for themselves right and so roll up teams are going to want to kind of front run the implied ethereum auction and um create their own auction uh to try to auction off uh sequencing rights um so if the roll-up control submission then the situation is kind of similar to cross-chain mev right though it's also different in some ways too right it actually depends on the details of like how the roll-up controls submission like do you have auctions for every swat do you have auctions for like periods of 30 minutes is it something else i don't know um so i think like there's two major categories of possible worlds here and the question is like to what do to what extent do people seriously try to create or capture cross domain mev uh between multiple rollups or between uh rollups and the base chain so one of the worlds is kind of no merging right basically roll up sequencers so whoever like wins the auction to be a sequencer and publish blocks in the roll-up they just submit their blob transactions and that's the only thing that the uh proposers can or the builders can include and so they just get included in the merging world the question is like well so you have this kind of roll-up auction and so the auctions do kind of get separated but then you have you have to sort of bring the auction back together if you want one person to be able to optimize on over both at the same time right and so basically one possibility is like if the auctions happen you know out of uh out of step with each other so the the roll-up auction happens first whoever wins the roll-up sequencing they have an incentive to try much harder to win the block uh the auction on for creating a block on the ethereum side um or the other option is they could kind of sell their sequencing right to the high to the highest bidder um or just like basically sell their sequencing right you know lots of ways to kind of pre-commit to this you could even you know do it off-chain um and try to just make side deals to make sure that um like they actually you know whoever has the ability to make the optimal block actually kind of gets the power to choose there right so how this would work i mean i think it's still one of the topics that we need to do a lot more thinking about but you know it is uh an important issue um so if these are things that like issues that are going to pop up over the next few years and some of the ways in which the ecosystem might respond um the question that i want to end with is like what do we want right like what are even the goals of an immediate landscape and what are even the goals of like like what are we trying to achieve when we design the mev ecosystem in response to these kinds of pressures and in response to the desires to achieve more scalability and other properties of the protocol so the way that i think about my goal is basically insulating the ethereum base layer from centralizing tendencies of complicated stuff happening on top of or around ethereum right so like one way uh that i think about this is like one of the kind of reasons why a lot of people in bitcoin land are upset or do not want to see bitcoin have you know what i consider real smart contracts is basically because like it's not because they're afraid of technical complexity of the bitcoin base layer because the amount of technical complex that you need to like enable arbitrary applications is actually not that high the thing that they're more concerned about is basically applications existing around the uh bitcoin and like interfering with the base layer in all sorts of ways right you know mev is actually one good example right like basically mev just like if mev on a chain is possible if a chain is powerful enough to support applications that have mev then mev itself kind of feeds back and creates a centralizing risk to the base chain right there's also political risks like you know if you support applications then does are those applications going to start like pushing for changes to the protocol to support them so like all of these kind of unavoidable you know systemic interactions between the layer one and layer two right and i think the best that we can do in ethereum you know if we do both value the goal of you know having this very performance way or two that actually supports people's desire to do all of these amazing awesome watching things on it but at the same time still have this layer one that you know is robust um is ultrasound money has justin still here yep yay um you know his ultrasound money is robust it's kind of like does this job of like being this really stable and sturdy thing then like the goal is like you want the way you're to you want the interaction between the two to be structured so that the layer one gets insulated from this the crazy stuff happening around it right you want some layer of uh separation um so you know in a meme format this is basically it right so you know you have your economies of scale um and you know we want like layer separation propor so between layer two and layer one um where um you know you have layer two side auctions that try to kind of absorb all of these mav issues proposed to builder separation so builders absorb the economies of scale and proposers don't aren't as affected by economies of scale like all of these things kind of make sure that you know the ethereum based lawyer can kind of sweep soundly and have as few economies of scale causing centralization risk um as possible um so layer twos might help propose a builder separation might help but then there's also this open question of like well how can we make this kind of these kind of separations even more robust right if if in the case that this stuff is not enough then you know what other kinds of protocol level reform what kinds of ecosystem level reform uh could actually get us all the way there and i'm gonna create like both med utopia and uh baselayer utopia thank you [Applause] questions hi thanks for the talk that was really interesting um the um the threat to the base layer that i'm not sure is addressed by any of those things is the um the variability in proposal rewards because most proposals will only you know if you're a solo staker you'll propose a few times a year at most so there's a massive variability there so that's going to be a centralization pressure because you can if you can socialize those rewards such as lido is doing then you get a big advantage there come is there anything we can do to share that out i agree i mean definitely a good point i mean justin over there has been a fan of mbv smoothing for a long time which like basically forces like the large most of the bid to be split among all of the validators instead of uh going to the the specific one that happens to be the proposer um so that's one option um i mean aside from that there's obviously the strategy of like mev minimization which is like an ecosystem layer thing and like you know we should probably like do uh some of that too so combination of both of those oh one third technique i should mention is that the like more crazier idea is uh sell block building slots further ahead of time so that you have less knowledge of whether or not there's going to be crazy ndv then hey vitalik uh over here quick question just to follow up on your comment about kind of layer twos uh likely looking to take some of that mev sequencing value what are could you just double click on that and talk about ways that we could design the layer one going forward to be more resistant to that or is that something that's kind of inevitable and you're relying on player two competition among each other to maybe yeah i mean i think the question one question is like do we want to take away layer 2's ability to uh to extract immediate right because uh like layer twos do have more political room to do certain things that layer ones can't right so like if for example layer one started i mean no printing eth to pay core developers then like that could easily turn into a political show uh but if you know optimism and arbitrarily started to doing that doing that which i mean like optimism has you know signaled their and already started doing all these retro efforts to do that themselves then people tend to be more comfortable with that and you know the ecosystem can benefit a lot from having these diverse funding sources so that would be a reason not to care um i mean if we do care then like obviously you know you don't want to layer two is to just be to completely yeah i'm extractive and make users that like actually have to pay more than they need to um and in that i mean a competition i think pretty much uh has to be the answer at that point you can take the last questions hi thank you um at this point do you think it's inevitable that it's going to become an mbv dystopia or are you hopeful for the utopia side of the argument i'm definitely hopeful i mean i do think that like realistically you know we don't want to try to kind of overextend ourselves and it's important to like design in such a way that like if central some centralization is inevitable then like don't just think about minimizing but also think about like directing it and containing it and so kind of pushing it into places where it's less harmful i think at this point like with all of this stuff like it definitely gets to the point where mev is is like far from the largest factor um in staking centralization for example right like if we already get to the point where the bigger concern in staking centralization is like say liquidity of staking tokens for example um then like in some ways we're kind of one right because like the bigger problem is something else and we have to focus on it um and you know achieve it doesn't um what we could um but uh yeah i mean in general like yes i i am hopeful that we will be able to achieve many other goals and second question what are your private keys um i think it contains a four [Applause] [Music] you 