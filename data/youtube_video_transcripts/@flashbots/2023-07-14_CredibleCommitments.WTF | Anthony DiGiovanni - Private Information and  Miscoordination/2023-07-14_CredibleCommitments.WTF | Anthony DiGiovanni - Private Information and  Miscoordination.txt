oh um yeah thank you and uh this is not here we also invited me for this um and yeah and thanks Jasper for uh sort of setting things up here with the SPI intro um because yeah some of this work is gonna supplement that discussion um yeah maybe I should just say like a bit about generally what central amount to risk those um for context so yeah I do not know crypto um so uh largely yeah my sort of uh well I'm hoping uh can be like Mutual benefit from this is uh we so CLR does like some largely work that's in this operative AI rain um we're concerned about scenarios where there's like lots of Delegation to uh Power equally AI systems and big dates like these theoretic uh traps that Human Society is based for a while now and right so what could be the mutual benefit here um we can get like a more grounded understanding um how the practicalities of like cryptography and interact with initial improvements that's uh yeah accessories make and um yeah in turn we can uh share with you guys some updates on the theoretical aspects uh that's what I'll focus on largely here of um basically what uh what is our ideal for uh rational agents of operating what conditions would be sufficient uh for cooperation to be rational um it has this like interplay with uh conditional commitments yeah so yeah as instead there's going to be both about the private information problem and conditional Insurance solution to that and the miscoordination problem as well and yeah this is during work uh with my supervisor Jesse clipton also at CLR um and indebted to cost per person discussions yeah so I want to say a bit about the the scope of all this um my work is largely motivated by thinking about um scenarios that we currently don't face with uh yeah current AI is as impressive as they are we as we start seeing and every week uh of news about gbts um they're still not like fully autonomous agents uh that humans have like delegated um ice takes decisions to but when we think about scenarios where uh we do have really Advanced historical autonomous agents making these decisions where cooperation failure is large uh that's the kind of uh scope that this work is intended to apply by two um and consequently that's why this is going to be at a pretty like high uh theoretical level um uh my sort of research philosophy is like I don't want it to be too anchored to um particularities about what current models look like um because progress and yeah in in AI can often be like quite and made in directions you wouldn't expect but we'd like to get at these kind of fundamental aspects of uh what are the incentives of rational agents but at the same time we want to think about like somewhat AI specific uh capabilities that might be important like uh greater commitment ability conditional equipment ability than humans now um through uh prospects of like Mutual transparency machine accesses whereas like humans um at least I made it by uh certain external commitment Technologies we can't read each other's minds for example yeah so to get as like as concrete as one can with this sort of thing you may want to imagine something like Auto gbd is like people attempting to leverage these uh state of the Arts large language models into agents but imagine that for like an n greater than or um large enough that's yeah these are like advanced oh by the way yeah any at any time I'm like saying something that's pretty confusing to you feel pretty brought obviously pretty sure [Music] um and hopefully the talking thing would be too long well by any time for discussion um yeah sorry I mean move the new thing nice so yeah as I said we want to consider like uh an ideal in the sense of uh what are conditions where if these are all satisfied you you make it so that any rational agent is incentive advised to cooperate in some sense um and if you negate that question uh this tells you you want to say like what are conditions that are necessary for there to be cooperative failure and uh so this is kind of working paper uh establishes its results but it's largely uh kind of a corollary in the conditional disclosure paper um I'll get into a second combining that with talking about equilibrium selection and you get this breakdown of um the sort of three major reasons that rational agent employer cooperate are one that they have inability to uh commit to Cooperative outcomes uh I'll talk to the right here um second is like private information problems that's the first thing I'll go over uh and lastly miscoordination where even if we have Cooperative equilibrium existing um agents like make conflicting demands for a different equilibrium with the sorts of examples that Casper talked about and this is emitting some like relatively minor exceptions you'd also need like randomization ability but that seems like relatively straightforward uh yeah and uh I'm largely gonna bracket the Clinton ability uh problem at least as far as like that um these are the kind of Maintenance necessary for um Cooperative agreements uh likely because this is like the kind of thing that um yeah crypto offers like promise for making incredible commitments but the tricky parts are like even when agents are able to make credible commitments uh what problems Do they base that like philosophical sort of cooperation so focus on these others I'm gonna not spend too much time on private info largely because uh I was like pleased to hear there's like so much familiarity with the idea of conditional disclosure already um that ends it's uh it seems like overall a slightly less neglected Problem by uh ongoing research sentiment coordination so um we'll spend a little bit of time on this and then move on to this coordination so the problem is like kind of a combination of incentives to blah uh as to like not accurately represent their private information and also presence of exploitability of information that you might want to otherwise want to disclose start over both of those pieces in this example let's imagine we have like two AIS uh gbd Allison gbd Bob I'll just tell them Allison Bob the rest of all this who are tasked with uh negotiating over some good and because like the we'll assume that by default the evaluations um that Alice and Bob had on this good are like not common knowledge that that hence this is private in uh Alice of course knows their own valuation of the good and Bob knows his and um what I'm going to say here applies to both sides but let's just consider one side for Simplicity um Alice has this like uh on a space incentive to uh pretend to have a lower evaluation of G than she actually does um I see there's a discussion of this market for women's problem earlier uh it's a very similar idea um if Alice could convince uh Bob that uh she doesn't value the good that much she can get like a much better price uh for it is the idea and um a straightforward like solution for this if you had uh AI system you have ability to share something like their word function they're trained on would be the following so I'll just slide but yeah if you to send that you're familiar with alignment problems uh of course like reward functions themselves this is like the incentives you get when in training uh say a reinforcement learning agents um those don't directly translate to what the agent is actually optimizing for uh what it's like preferences are um but if you let's say like you get alignment going pretty well uh in the single agent case such that if Bob didn't see this reward function published Bob would know uh you could be able to like check what's Alice's valuation of the good actually it's so verifying the private information could in theory get rid of the uh potential inefficiency here but uh if like it's if it's not possible for Alice just to like publish a just like the kind of subset of her summary of her reward function that proves here's my evaluation let's go ahead and nothing else maybe yeah they just don't have that much find varying control over what they disclose to each other and instead this needs to be this like pretty course training like I publish my whole world function this it we can debate how realistic this particular example is but in general you might have inability to uh not selectively disclose exactly the kind of input he wants you might want to tap to disclose a bigger sets of info containing that so the word is that yes like publishing this work function tells Bob okay this is what Allison documentation is she's not bluffing but um other rewards on word function info that Bob Lawrence might make Alice exploit um you could tell stories uh [Music] just let's stop figure out like here's some weird adversarial example that uh Alice has um that would like make her behave in like pretty stupid exploitable way on some inputs for example um in general you can imagine yeah it's just like risky to generate too much info about yourself so because Alice had faces this risk of being manipulated by disclosing too much uh she refuses to disclose and unfortunately this results in inefficiency in the sense that the trade fails to happen um this is a pretty low stakes you can imagine that like eventually they they can like keep paddling and eventually trade so sometimes wasted but this is just like a toy example of inefficiency you get from private info they're all clear so far yeah nice um yeah and this is all again thankfully this is already discussed um this like basic Insight was indented into the Myerson salary theorem so what's like a nice capability that could resolve this problem uh conditional disclosure because yeah even if you have you have to like disclose more info than you'd like um what else can do is say I'm going to commit only to disclose this reward function that proved my valuation and some other exploitable things if and only if Bob is committed to exploit me and yeah so similarly Bob might want to do this symmetrically if this kind of initial disclosure is available then LCL RS is incentive to reduce the disclosure and the private problem goes away in the sense that equilibrium now exists just like that's not particular but I'm wearing private boats involved basically yeah that's that's all the theory of um the sort of Ideal solution to private problems if you had uh pretty sophisticated conditional commitment ability um yeah in the discussion we could talk about some of the practica but for now I'll move on to this coordination so yeah I want to um start by grounding something in a case where this possible miscoordination can show up when you have like commitments being implemented uh with the blockchain to like somebody understands the practicalities um so if I say something that's wrong here please get ready to correct me um so one takeaway we got from zeven's talk yesterday is that you have like some uh blockchain mechanisms or protocols where uh we don't have just like arbitrarily fine-grained continuous time to publish and broadcast Institution if that were the case then you could just say like whoever and it's first um they're probably their commitment sentence instantaneously broadcast to everyone else and everyone else knows okay I just have to follow this commitment um I kind of lose the race but if there is this kind of minimal time between when we can have commitments that are broadcast um between the agents then there's a potential for multiple commitments to be submitted onto the protocol uh within that time frame and so the second mover hasn't learned that the first Uber has already committed uh before uh they commit themselves so this leads to like practically simultaneous amendments in the Strategic sense so again let's we have Alice and Bob who are instead of um haggling over like some some discrete good now let's say they just want to like decide some allocation of this fit and it's like divisible um again something like the demand game that uh Casper's talk talked about uh could also be an example of this so yeah between some time T and like t plus this like minimal block time both Alice and Bobby submitted some commitments and troubles if their demands of each other are incompatible for example Alice demon 70 of G and bomb demands fifty percent so when they're incompatible the trade doesn't happen again we have some innovation so yeah I want to make uh the connection SPI here and and talk about basically some breaking uh sort of the the two nice features of spi parts and talking about like under what conditions do we need um one of these parts to make sure the first part works that's the high level story here so what this SBI proposal gives us is uh yeah a pair of conditional commitments that could resolve miscoordination here um so putting things in the language of uh other this is described in the SPI paper um we'll say that it's like Pi of G is the way that Allison and Bob would play this original game so um how to deciding how to allocate is good um or which which demands they make in the allocation of the grid and this is like the way they play this game is kind of induced by this pair of communities so you can think of like the the pie here is like this function mapping from the game to the way they play and the function here is the pair of delicates that the Casper's SBI talk was talking about um implicitly um even if there aren't literal delegates that Alice and Bob have when there's in this protocol um the the sort of virtual players that end up being created by the error commitments they submit is the pair of bell gets involved foreign natural way you can resolve miscoordination with the SPI here is this conditional equipment to both apply that same pair of delegates they they would have applied to the original game they're going to apply that to the SPI G of s um trans just to recap that's like transforming the uh payoffs that the delegates are playing according to um instead of the original gaming chaos and you could also restrict the action space so this is uh well the reason I'm going through this is like just the highlights that we can um have an implementation of the idea I'm going to talk about through SPI um even if there isn't like directly um a transformation of payoffs being done by these commitments the agents submit um SBI like one way you can get the commitments to the agents submit transformed into something that creating groups um yeah so what's spr oh sorry uh let's say cradle improvements last time yeah and so and what is the definition of coordination like well for maximizing or hmm um no so uh coordination in this context would be like they did and so like end up like both aiming for the same um efficient equilibrium or efficient outcome so like they make compatible demands and they end up in some position and this coordination is like not that they're aiming produced um yeah so maybe I just like also want to highlight uh yeah just to be clear about that uh usually we think of coordination in terms of uh agents kind of with the same goal but who are like making decentralized decisions and so they end up like in inefficiencies because they weren't able to like literally talk to each other the kind of miscoordination I'm talking about here in these bargaining problems broadly construed like the man game example or that threat game example um so in fact their goals can be like quite different and the bark printing problem comes through them thank you and yeah so I'll ask you so this is like as Casper said this needs to be a conditional commitment you played the SPI uh if and only if the other is also going to do that and if the only if if and only if the other is going to apply that same pair of delegates by um to the SPF so I'll talk more about the synonyms like in general what what's the what's the thing that we care about um whether it's like mediated through SPI or something else um a Fail-Safe is like paternals uh where but I thought we have some uh pair or like more than two agents still having this problem set of commitments that uh make incompatible demands or are incompatible in the sense that when they're put together with each other the commitments are run on each other then the outcome is cradled that we what the failsafe does is like Maps those to different commitments that uh pray to improve on original thing thank you and yeah the goal of this is to reduce the downsides um so for example like an implementation of the scale-safe thing could be committing to let let your delegates say STI so the question I want to talk about um is when exactly are agents incentivized to use scale safes uh like spis for example uh given least they have evaluation that probabilistic in the sense of yeah I think you're um 80 likely to uh make this really high demand against me in 20 to make like a fair Etc and I'm emphasizing this point um because so the motivating question um as framed in customer stock was even if agents don't want to like say what's the like expected utility of this strategy um but rather I want to have some with probability one guarantee uh that we pray to improved is that is it possible to do that [Music] um so we saw that that's true uh but in in cases where like just in general we want to be able to talk about like when when agents realistically do have some beliefs about each other um one needs to be true about their beliefs such that they'll uh either want to use skill safes or may have some incentives not and is this like the the adding the beliefs part is uh what what makes style saves different on the like like the previous outcome yeah okay um right so similar to the uh we're just I guess like generalizing the problem that was also discussed in the say prayer improvements talk about um how you need to commit to use do not like modify your delegates to uh like trick to give yourself a better deal say in general the problem the prima facie problem like that that's gesturing out is or that that's illustrating is that having this risk of miscoordinating of like having some inefficiency one where our demands are incompatible uh translates to bargaining power or the other player meaning Bob might think okay if if we do use a Fail-Safe Alice is likely to demand more because uh she no longer has anything to lose uh when like she protected from the downside of this variation and yeah so the example in the story of Gold's taste there was um if the human doesn't care about uh like some page uh yeah a page of like a binary digits being uh written out I'm probably like balche or mixing up uh examples from the talk but if yeah if like humans we don't care about bats and we uh make an AI a pair of Rob apps that thing such that whatever it happens when it has bargaining failure with another other agents is this thing we don't care about we would have this incentive on Space to like train this AI to be more uh like aggressive so I really want to like break a sound into two questions um when exactly and why is eating a problem uh this cheating fitting being that's uh yeah Bob does this worry that Alice is gonna can add more because the risky miscoordination are gone uh or at least heavily uh mitigated into what is a problem what are the like qualitative like kinds of capabilities for our conditional commitments which of those do we need uh to avoid cheating problems um so similarly to how like with the private info problem we said you can you need in general you need conditional able to solve apps um what kind of conditional commitments in that sense is uh in general necessary and in some cases sufficient to uh avoid this cheating problem the motivation for both of these questions yeah cool um so one of my highlight is that uh if Bob's worry is that um Alice is going to um demand more when fail safes are available if Bob is like rational and not just kind of um stubbornly like refusing to use this thing that gives us parade of improvement why would Bob want to refuse to use the failsafe or refuse to commit to the STi for example um for that to be the case uh my claim is that Bob needs to have some chance before this interaction happens to incentivize uh Alice not to cheat um in particular Bob needs to have like some uh Power to incredibly commit to ignore or fail safes that are potentially cheating why because like if this weren't the case if they're totally acting simultaneously um just like separate stepping into this human game then Bob's choice to commit to ignore appeal state that cheats is not going to influence Alice's decision whatsoever um because yeah it's done simultaneously Alice hasn't seen this commitment to ignore so in some sense we need we need like the game to have this extra sequential step um for for cheating to be a concern um it does seem realistically that this uh this would often be the case though uh they are just like popped into existence uh and like hooked up to a blockchain and they said some commitments uh beforehand they may um yeah they may be thinking about like okay I'm gonna get into potential bargaining like interactions with other AIS we may have this little exploration thing oh here's this SPI thing um it sure would be unfortunate if um I lost bargaining Tower uh from someone cheating at this and so yeah realistically we will have this sequential structure happening um however but you both need the sequential structure that I'm depicting here where there's a potential to commit to ignore the pension shooting hail saves but you also need uh that's Bob's like commitment to ignore them can be made credible so really sufficiently credible um if Bob just has some cheap pack of uh I'll ignore this tailsafe that um that makes me worse off um but Bob Allison buy this um yeah she thinks he's Bob's bluffing that was Alice may still insist okay um let's use the skill save that that cheats once Bob has gone into that decision nobody and hasn't incredibly committed to ignore um well sorry I should step back if Bob actually hasn't committed to ignore then he's gone into a situation of yes I've been cheated but uh I might as well go along with this because at least it makes us better off when we miss work um and so we anticipate that being a problem he's only going to want to convince you ignore he still says that like they both sides better off if he expects this to actually influence Alice's decision now the last thing I want to add to this um is that's like in these like games with conditional commitments um in principle you don't uh you don't need a sequential structure for there to be like influence on the other agent's decision uh which are committed um it's possible yeah that's Alice's uh excuse me Bob is committed to um ignore potentially shouldn't kill safes because he expects uh Alice to be emitted into not use sales fails conditional on seeing that bond has this in advance to ignore them that could that in itself Could Happen um even when there's like no sequential structure to the interaction um so that Dynamic I think is important to even buyers um they get a sense of like even when there's no opportunity for this like preemptive uh credible commitment to ignore um there is still a incentive to ignore non-cheating excuse me to ignore achieving fail safes however what Alice can do the slight pads is commits to have a commitment that reacts exactly the same in the sense of like making the same demands against um commitments that either go along with it but they'll say or ignore if that's the case then again there's there's a sense like no incentive or um maybe I should use the Whiteboard just to like really make this here I already to take the picture so it's I already took the picture yeah yeah so let's say um the Canadians let's say that the uh couldn't bottom makes um kind of by default to see um and um let's say see that is uh C but um follow us that fail safe so illicitly um the original LLC is like ignore everything then as long as Alice's commitment um it's called d has the exact same uh okay this is a bit fuzzy um you can you can like actually see the improved coordination be a field system deep conditioning position programs uh for the technical details about now this but as long as um I'll use the state quality to mean like the names in some sense are equal between Alice's program d uh which does we want to use the PLC if DF on C makes the same demands as DF on the C of f then like yeah it's sort of by construction uh Alice has made it so that there's no incentive for Bob uh to use this C that ignores this when we use the conditional because this is an all commitments by um Bob might have been his like route for uh preempting cheating trickery um the person sending the commitment you know make the same thing man no matter it's the other person does it that's it but not um because she's anticipating estimating that like um people like Bob might be um worried about like losing bargaining power from okay yeah so like in particular Bob does need to kind of think before he's committing this commitment okay someone like Alex is gonna be pretty smart um and realize that I would want to um admit not to go along with cheating fail-safes uh all else equal but if but I'm not gonna refuse to go along with them and it's this property Awards so balance is smart shall make her commitment of this property and the key thing is like this is totally consistent with cheating meaning that this is a statement about the equality of the responses of Alice Stevens to get those times um in this like program equilibrium kind of setting it but it doesn't tell you anything about uh how aggressive are Alice's demands relative to what she would have done if the fail safes were those are two different things and so we can even we can have that was being um cautious in this sense of she's not going to demand any more against the Fail-Safe following one then they feel sit ignore me so she can't fully cheat this sucks but she is still uh free to like make this like this demand not the one she would have made without they'll say so is this distinction clear or uh we want to discuss this right always the same yeah I should I should reiterate that in practice we may I have like this sequential structure in real world so um as far as we expect possibility of committing to ignore uh cheating fail sticks to to be present um I think that's like sufficiently likely though um it since you do want to like avoid miscoordination uh would not want to like in general you don't want to take the risk of of cheating so a reasonably cautious yet uh non-explodable thing to do uh is to use the non-cheating email safe uh like how sdis are constructed um when you're like we think it's like reasonably plausible that this incentives but this is mainly to highlight like when this is intended to exist and why is it the case because this will help us say uh what capabilities do you need like the initial agreements uh to avoid machine um so what are those capabilities that you need um this like as again as like a generalization of what uh when SPI is doing what we need is this it's conditional commitment to only use the Fail-Safe uh if the other side applies it to their people uh not cheating um now so in this diagram like the just looking at the bottom uh two thirds with a rectangle and circle this is the classic program game story I have a program that's um makes me uh output an action conditional on your program devices but what we've added is also programs that can condition on each other's release and the reason that's useful is that and like I claim in general necessary there's like an example a proof of this kind of result for some classic games in their paper um yeah the necessity of belief conditioning and what helped makes it also sufficient is that player 2's beliefs give P1 all the information P1 means to say like is this the program that P2 would have wanted to submit um if I did commit to ignorance because at least tell you okay what's what would Alice expect have expected me to play how do we not use stale safes therefore what would her best response with this like program game and structure what order is subjectively optimal thing to do I've been under those Suites so belief conditioning helps us do this whole conditioning on you not cheating thing um nice yeah sorry this is a bit taking a bit longer than I expected but um I really just want to like make this film sufficiently clear um yeah this what this is doing in this guy framing is uh that this part where Alice and Bob commit to use the same delegates as if sbis weren't available in uh Barry um in general like uh when they're allowed to pick more hawkish delegates uh anticipating this for like getting extra instructions than it really gets that's that kind of introduces cheating as Casper described before uh the terminal I'll use for like committing to not cheat in this sense make the same depends as we would on Fireball we'll call that default and yeah believing conditioning lets you verify um rather than just taking it on the against Wardens um yeah okay last lunch line mayor is uh if you can incredibly commit to ignoring bailsafe but we do have this like sequential game structure I described a few slides ago um you can prove uh this itself is not um in the fail stage paper currently it's ongoing research formalizing this um but uh yeah you can prove that this developed preserving property plus some conditions on the player's beliefs about each other uh basically the kind of beliefs that uh I was talking about here like I expect Alice to be reasonable in the sense that he's not going to uh make unequal demands here as a function of my scale State using like they'll say to ignore commitment if they're really satisfy that property um and we have default the default preserving uh initial commitment this makes the pale safes individually um yeah I'm happy to like explain like formally what that means in the discussion but yeah so some ongoing research is like getting this General sense of uh if we have different ways that layers beliefs might be updated um by learning something about each other's commitments for example are they committed to use sales uh when are pills stays still rational cool uh that's all for like the main substance of the talk uh I'll I'm going to like put up some possible discussion pumps um but if people have like questions on the stuff I talked about directly and welcome to thanks like in the paper did you mention anything about like verifying the beliefs are realistic do you expect that to be yeah I mean I think this is like um probably a pretty hard problem um so largely uh the motivation like with the paper was to emphasize like what's what is the property that you need um the show like this is this is an open problem it's not clear that we can do this as a current attack um yeah because like as far as I underst understand like that we don't have human bits that can like human signs unfortunately um if we have uh islamismical learning this kind of thing and we have some um kind of sock commitment power used by like humans to tolerate each of those AIS um without like influencing how they bargained too strongly then the hope is that um if these ARS are like sufficiently autonomous and like yeah it began by the humans to design them then the AIS who are like implementing peel States excuse me um could condition on each other's beliefs by like yeah looking at parameters and all that works uh I do respect it like to be challenging to verify this I don't I don't know how horrible verification works but um in principle it's like bi-level stories like AI is uh and had like more transparent to each other Minds than the giving sound at least so yeah yesterday we were talking about [Music] um uh simulation as a means to verify so if if we're treating people considering the release of helicopter Network um could pretty much you know stimulate that up right so we would stimulate a good uh even if I like it then what would the status which is kind of a way to build to build a model or beliefs of the element yeah um that sounds right to me I think I paid the um maybe that's challenging part is like you you don't want you can't just like uh like when you're simulating you need to be able to say like what do I intervene on in uh simulating these beliefs so like how can I um what input do I get into like this this network so that this corresponds to believing that fail Stakes will be ignored um so you can like check that's the case um yeah uh maybe this is like more chapter than I really expect which would be great I have a I haven't found those events um that practitioner will be out yeah I have a question uh Turtle maybe some of you answered uh so it is for our beliefs right so there were Alban summons uh some probably institutional person space right did we use the way understanding is awesome probably distribution right over demonstration but then this is somehow like a continuous object right and you assume that this is being given as an input so how important is it in August framework like I I know this with like perfect accuracy uh yeah I a little bit or like the formal result you need like the condition on the exact beliefs but I think in practice um there's there's some slack here because like for one thing because the fail safe rides a Pareto Improvement unless it's like a really trivial one uh the agents will think um like there's some margin of error here as long as uh I still expect to gain a lot from with sweating a story mate then um I can tolerate breaks down the chances of uh yeah they're being like locality in or lower than perfect Fidelity and like weight conditioning um but yeah more to the police um yeah I suppose you need like enough resolution to be able to uh make it so that um I don't know you absolutely some guarantee that whichever beliefs you kind of feed into uh it's like mapping that tells me like what the the default uh program would have been um but this is like close enough to the real world um and so maybe like there's some interesting like work to be done I mean yeah getting 30 on how we're continuous assumption is I guess like if you break things slightly through the eight ball programs like wildly changed um yeah foreign outcome and such things can be combined and with the faces and SP I'd say oh if you know in in our you know program Evolution it great hooks we failed and we will renegotiate whatever yeah I think that's a cool idea um I haven't thought of that uh but I think it's like it feels like less Central than like agents have many incompatible demands and that's why you need to just think about it or why there's a miscoordination but yeah if like they're both aiming for the same equilibrium but just the implementation is too brittle uh yeah man you feel safe is gonna help with that too um you might still have like this like regressive uh are the fail safes also uh yeah so in particularly like what how realistics would have to work or like now SBI would work and as like a big example of this is you need to be able to you're reading this code to say like are you committed to play this SDI um as well and it's like the way that that's written is also really brittle then it's also not going to work yeah yeah but I mean like English yeah I can kind of some cases like kind of levels like social consensus right maybe like everybody kind of agrees dancing it all happens say like oh we both want to solve this program and they you know eat breakfast then weak and kind of all agree that's at least you need yourself to something that does not lead to us or be fatigue then we are the same negotiation and that's like the you know the same negotiations yeah um that sounds actually uh yeah I think that's like a nice nice aspect of this instruction is like I'm I'm like boxed in my theory random here's the central motivating problem but uh leading the same uh the other backup locations here this is important um speaking of yeah getting more concrete I am uh uh I'm curious yeah to dig into some of the stuff on here uh I guess we we talked about this issue of um you get miscoordination from sort of a lag time between when women into broadcast so um if anyone is like learning to share or like some other example you've seen in the crypto space service would be relevant and need to hear that I mean I kind of wanted to eat more into the uh programming field of resolution can we use spis to mitigate that that problem yeah so um so over there like like you know if realistically we want to implement some kind of uh you know programming equilibrium if systems let's say you know actually you know on blockchain through users commit to you know some transact either a smartphone a smart contract or some transactions uh uh that depends on other users Community transactions rather simultaneously do the commitment as they are coming in kind of like interdependence and so like they want to play some specific game let's say I want to you know settle some trade against you like I want to trade SMA to sib if you communicate as a view to answer a with and then you know um then we said harmonic trades um then like we really don't want the brick on this to happen because um you know maybe the big one is is that we'll go some trade but at least you want to like at least you want to trade just maybe you may want an English sheet whatever what is the price Insurance um yeah so you so in that example I would imagine maybe um we have some like already established so we'll dance on that oh if this first level programming could be a resolution fails then we all use this real client negotiation on Broad shapes uh to further negotiate the price but then like to compute or to simulate those negotiation process it caused a lot of computational resources uh computation resources as we know is not that but like it's kind of scars are blockchain do you see any way that we can like you know further layer this process maybe like we compute the first level programming program resolution um you know it sounds like Broad blockchain and so like the negotiation process is taking care a small smaller set of nodes and like for example one it's just so many times they're all there welcome suggestion foreign [Music] one of smart transactions usage is real-time user negotiation and the real-time user negotiation sounds a lot like you know Celsius and spis where the commitment of the protocol is a negotiation for the part between users I don't think lab has a specification or has thought about this teacher because you know I I I trust there are some interesting ideas there you go though you mentioned in one slide what she was talking about this today where you have two in combating I don't really understand how that ties into what they matter and I feel like this has to do with this real-time negotiation problem can you maybe just draw that link again oh yeah um I guess I could literally Drive [Music] and I remember when I get rid of this long it's uh and don't let the guns down there where we go here um yeah so the I mean the argument is that um if like a time tea M's this time G plus that plus looks like a t as well um so like tea does our little T is our block time um so in like in this region uh there's no like broadcasting that can happen between those two times as I understand it um so because there's no broadcasting you might have like uh like Bob Simpson here um Alice does here and so Bob's might be uh yeah to demand 70. analysis uh kind of unconditionally so basically because there's no broadcasting um Alice doesn't see that Bob has like committed here and so like makes sense um whereas like if there had been like a continuous time for broadcasting and Bob skill and then we're broadcast Alice could just say oh um I'll go along with that because I mean the demand has been made it's incredible so so the argument was that if I broadcast my commitment very long we will pay save case the commitment zones makes it that um we don't find the efficient also so if I make the demand but also I have this they say say that's ended the denied unable to Edge against situations where I might get into outcomes where the great installation and then we will pay safe I can achieve the STI outcome uh so how does that let's say look back inside yeah so there is this like uh uh leadership is used exactly exactly number one started so uh this is something that Casper's character gets into that when you kind of have a um SPI selection problem uh in the sense of like yeah how do they agents agree on the failsafe um well suppose that um there's yeah it's just like to say in some scenarios uh there is kind of like a either like a unilateral um SBI you can make uh or um it is bilateral but um sort of neither side can can game it and so they're all incentivized suit at least go along with that DLC uh even if they don't get all the weaker in the frontier because they still ought to like solve SPI selection um the yeah the federal space paper gives an example of this uh but yeah I think in general this like second order coordination problem uh props up but there was around it so let's like say that that's been solved and you could have like um the yeah how would this work um yeah in principle so like if this is like uh on like here 100 guns did Alice um you're 100 goes to Pop um yeah and the parade around here is there um if only stimulate happens when they don't trade it's just like go down here then the failsip we just map this outcome to anything whatsoever uh but they kind of in the focal point two coordinate on would be like something that's fair in some sense um it could also um oh yeah maybe I should just like illustrate the uh the thing I was talking about that the example given in the paper because I just like make this more concrete um sorry um I'm just asking those signed version like what's something about I get I got the first you get the birth and let's try to start um and why why do you burn it seriously put the equivalent of just saying we never do anything then developed that person right so we kind of want to have like most every from part of here you learn new business um sure maybe what I want to say is that the uh like Bernie might make sense uh the reason you end up burning is yeah they can't agree like getting all the way to Frontier but like why do they already and like they stop R is my question um and so this this thing I'm gonna illustrate if I can remember this properly um and and give you a principal way of doing that so let's say um uh 0.7 Bob and either Dallas that's up here um Alex is the man would like slap them barely so it's up here and what they do is um go to the intersection uh like these these lines that are like perpendicular to these um why is this nice the reason is that like you can't game this by demanding Warfare yourself if uh if Bob Wick instead of having 0.7 went to like 0.9 and so this is where this this is other demand made by Bob but also say it's the same then the point goes down here um and about as an app so the nice property here is that like each player's allocation depends on the other player's demand um not on their own um but it's still a critical agreement on this one I didn't go up with this by the way it's like um it's a blog post called uh cooperating with agents with different Notions of bareness exploitability something like that um yeah I really would have liked it with excite in an academic Source on this uh indicator yeah it's when you have no there's not like one attempting we won't have the speaker oh wait is that thing yeah okay and we do have this this one as well difference like this this thought but an academic paper then yeah I guess we're probably at times okay take it 