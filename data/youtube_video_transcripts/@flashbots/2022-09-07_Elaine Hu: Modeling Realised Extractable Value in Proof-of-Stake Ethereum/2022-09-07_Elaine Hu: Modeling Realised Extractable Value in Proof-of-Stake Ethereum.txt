good afternoon everyone has anyone um heard of genie index not so many not as many as a pbs right so this is a way of measuring wealth inequality is often used by the economist to compare the wealth inequalities between different countries so i've collected some data here but before that i want to show you that a genie equals 1 means perfect inequality a genie equals zero means perfect equality so looking at some of the data collected from the world bank this is all the countries in the world with their genie index so i plot them on this chart you can see they range from 23 to 63 percent so the question is now if we treat the mav from each block as an individual's income where do you think that will lie is it going to be on the high end with south africa or on the low end or somewhere in between the u.s and the uk any guess okay so it turns out that it's 77 so it's even higher than the most unequal country so now if we aggregate the mav to a minor address so some all the mevs belong to the same miners address and recalculate this genie index where do you think that will be higher lower yeah so it turns out it's 93 so it's almost close to a perfect inequality so we know that the mev world is very unequal but that's not what i'm going to talk about today i'm not trying to solve the problem of inequality either in the real world or in the mdv world but i think the problem of inequality in the mev world will be easier to solve because i've heard many people talking about ideas of how redistributing back the mev to the user who solved this problem but that's not what i'm going to talk about today what i'm going to talk about is how to estimate a reasonable average mev so why the quotation mark around average well because surprisingly if you take the average mav it doesn't actually give you the average an ordinary or normal miner is expecting to get because there are so many outliers there's so many extremely large values that will skill your average result so there's a story about the statistician who has his head in the oven and his feet in the freezer on average he feels fine so as you see there's a problem with averages and we can't really uh just take the simple average here so here is a chart showing the total to return given different numbers of validators on the network and we're using two methods one is the mean the other one is the median so as you can see at the current number of validators we're seeing using the mean we can reach 9.7 percent but using the medium only 6.2 so this is already a significant difference of 3.5 so last year flashball has published a paper a piece of analysis estimating the expected validator return and in that analysis we use the medium so we get to the 6.2 by using a constant medium mev and we simulate the block reward along that so that's how we get to the 6.2 percent but you can see medium is already more reasonable and a more robust way better than the averages but that's not enough the problem is with a lot of the analysis out there you see that the validator returns always predicted as this perfectly smooth line right but in reality this shouldn't be smooth there should be some fluctuations some variations um the expected return should be affected by some other factors right for example the gas price the base fee the ethereum price it should respond to the network conditions it shouldn't be a fixed value so what i'm proposing today with the new model is try to estimate the mev with a more dynamic and responsive way by including these possible features or exogenous variables so how do we do that well before you start modeling anything the first thing is to collect the historical data and we know that mev as a concept is only theoretical so it's the maximum extractable value so in reality some of the observable values will extract extractable values will not be observable because for example if a searcher wants to pay the miner directly uh he could be paying the miners through an l2 solution or even through a bank transfer right so that kind of extractable value would not be on chain and we will not be able to observe that value so we have to accept the fact that there is a gap between what's theoretical and what can be collected and we have to do our best to collect the data which is the realized extractable value so in the following analysis i'm referring to mev as the rev which is the actual value extracted from the blockchain so how do we get the data from the for the ruv first we take the minus balance difference we look at what the minus balance is before a block is mined and after that we also take away the two is stack static block reward in today's proof of work and we also take away the burn fees because the data is collected from post london fork lastly we take away the transactions originated from the miners address so this could be mining pool payout or any kind of internal transfers that's not part of the mev so now we have the rev data on the right hand side we can try to model or fit some variables to estimate the rev in the middle we have the block reward the block reward previously we take out the two is static reward now we have to replace that with the proof of stake block reward so that's depending on the number of validators on the penalties and different factors so this can be simulated so with the rev which can be estimated and the middle part the block reward which will be simulated we can get to a more dynamic validator return so before we go into the model let's have a look at what the rav we collected from the post london fork data look like so from here we see that actually more than 90 percent of the rev of the blocks have an iev of less than or equal to 0.5 eth and if you look at the lower end it's around 56 less than or equal to 0.18 so it seems like most of the ruv are concentrated in the lower bucket so now looking at um rev across different time intervals so we want to know whether there's any seasonality or there's any any trend in the ruv um so in here we do the box plot and we plot every minute within an hour every hour within a day every day within a month and every month within the year so here we're looking at the five uh values from box plots so briefly very quickly the top and the bottom line are the maximum and the minimum the top of the box and the bottom of the box are the top 25 and bottom 25 percentile so the line in the middle will be the medium that's the one we're looking at so on the left hand side chart these two you see the medium are both very stable you don't observe any trend or seasonality for the bottom right hand side chart which is the monthly frequency we see the median value is higher in the month from august to november but bear in mind this is only one year's data so it's not that representative um so now looking at the hourly we also see a slight elevation in the m the rev medium value so from this is a utc hour 1300 to 200 we see slight increasingly medium value so this period of time also coincides with the u.s stock market opening time interestingly so we know that there are some seasonalities in the hours so we want to aggregate the block level data to an hourly data so why do we want to do that well the benefit of doing that is that it naturally removes the outliers so if we take the medium value of all the blocks at ruv we naturally remove that little spike at the end on the left hand side so this is the probability density chart and you can see there's a spike on the left hand side chart at the end and once it's aggregated to the hourly uh it's removed the other benefit is if we want to do some kind of a time series model um the time intervals needs to be equal right so in the current block time it's not equal it's not yet 12 seconds fixed so by doing this hourly aggregation we're able to create this equally timed step so that's the other benefit of course the last benefit is that you reduce the number of data points from like 2 million over 2 million to only 8 000 so that makes the training of the model much quicker faster so now looking at the models i've tried a few um the first one i tried is the decision tree so here i'm trying to um estimate rav as a class a bucket so the decision tree will predict what's the probability of ruv falling into a specific bucket let's say 0.1 east to 0.2 east so as you see the accuracy score is very low so accuracy score is just the um the percentage of correctly predicting the class so we're only able to predict 39 correctly not very good so then i started to try random forest which is a very similar method to decision tree except that it builds many multiple trees and aggregate them to an average so this time i model the rev as a continuous variable and the measurement used here is r square so as you can see it slightly improves performance so r square is basically the percentage of variation of the actual points that can be explained by the model so we can roughly explain 44 of the actual rav here still not so good right now what about the time dependencies what if there is some kind of correlation what if the past rev can predict the future so here with the arima model you're able to throw in the different lags of the ruv so we're using the past one hour iev or the hour before that to predict the current rev and in this case you can also throw in the other exogenous variables like gas price base fee gas used so with a few iterations we can see the final model is has these variables so we found the past hour rev has a positive correlation with the current hour the hour before the past hour has a negative correlation we also see that gas price gas units used have a positive correlation and the base fee has a negative correlation so this is just the optimal model after a few iterations so now let's look at the uh prediction so on the left hand side we see a log transformed prediction the green line is the predicted rev the blue dots are the actuals and the gray in shaded areas the confidence interval so as you see that on a training set most of the gray shaded area kind of covers the blue dots so even in some cases the extreme uh extremely high actual ravs are kind of covered similarly on the right hand side we have the testing set as the green line so here uh interestingly interestingly the testing set is during the recent period so we know there's a downturn in the recent period and the model is also able to capture that downward trend so that's a prob a promising result now what if we plot the prediction uh along with the block reward so we assume that every validator gets a full block reward and add that upon the estimated rev we just did so even though the left-hand side random forest was able to capture some of the extreme values on the top here but the reason period the the last thousand data points you see the prediction is very flat and also the confidence interval is also very narrow so that's not so good that's probably also why the performance is not so good but on the contrary the arima model similarly the left hand side you see the predictions on the green line this time the last 500 points on the right hand side you can see the prediction has some kind of fluctuation right so the time dependency creates this momentum and mean reversion little cyclical prediction which covers the recent period pretty well so the confidence interval is also able to capture some of the extreme points so finally i want to show this graph because the model predicts at an hourly level so given one number of validators you have multiple predictions of validatory term but in this chart i take the medium of all the predictions and plotted this chart and this way this way we can compare um what we had at the beginning of the presentation which is just the smooth line with uh what we have now is the new model which has these fluctuating predictions with confidence interval even though the random forest is not a good model it still manages to create some variations in the prediction so the time dependency arima model we see it it captures the volatility during the time when we had 200 000 validators so this is around the time i think summer last year where the mev is very volatile and you see the confidence interval are also wider whereas now the volatility has died down the confidence interval is much narrower just to wrap up we had the old model which we used the medium mev as a fixed value simulated the block reward estimated a fixed return which is 6.2 but given the same number of validators we have today let's say 410 000. we're able to uh with the new model we're able to predict a range of different validator returns so ranging from 4.8 to 7.8 and these differences are dependent on the different conditions of the network which are indicated by the different variables we use so in the new model we're able to predict uh with all the exogenous variable and predict better what the validatory term will be so now we've moved away from the average mev to dynamic mev we also moved away from the old model with a smooth perfectly smooth curve to a more dynamic volatile responsive curve so hopefully the validators can look at this and think more realistically about the return they're expecting okay quickly on the improvements um there are a lot of things we can play around with this model for example the assumptions i made in auto charts are the participation rate is a hundred percent uh everyone gets the full base reward these obviously can be changed or reduced and you can also include other exogenous variables for example the trading volume of ethereum or the volatility or even some other token price or volume you can of course change the frequency of the prediction so in the model i showed this hourly you can change that to 12 seconds post merge 24 36 whichever frequency that you think might improve the performance you can also of course try the other models like the neural network autoregressive model or also the markov chain state transition model so these are a few options to explore that sums up everything i've included some of the results in the code i did and also the previous papers we published thank you 