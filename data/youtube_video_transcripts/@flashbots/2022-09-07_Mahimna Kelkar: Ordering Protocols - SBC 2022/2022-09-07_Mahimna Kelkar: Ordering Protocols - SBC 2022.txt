okay all right so today i'm going to talk about decentralized ordering protocols pbs and more uh i'm mahima kalkar i'm a phd student at cornell university so before i begin the talk i just want to put out a little disclaimer there i am an academic first and this talk is much less academic than the ones i typically give so i like to give this disclaimer and whenever i give this type of talk so this is going to be based more on ideas and conjectures than rigorous results it also includes a lot of scattered thoughts and hopefully a lot of thought-provoking questions that you can take away from this all right so let's start so what are sort of the goals of pbs so the core idea is to sort of logically separate roles for block proposers and block builders so the block proposers are part of the consensus layer of the protocol and the builders are part of the ordering layer of the protocol so the goal is to logically separate these layers so the main philosophy is that if mev becomes a centralizing force then we don't want the centralization to affect the consensus part right so it's better to have a building part that's centralized than if then having the proposers or validators or the consensus layer be centralized so i just wanted to kind of talk you through a couple equilibriums that can happen with pbs and kind of try to understand them um better so here's sort of a first equilibrium that can happen right so if most of the mev strategies by value are known sort of publicly known and they're also easy to deploy so this is one possible equilibrium of course they can be shaken up once in a while if there's protocol upgrades or new juicy smart contracts but eventually it will get back to equilibrium so what happens in this kind of scenario is that mev can be primarily extracted by validators and likely the building role is no longer required to exist um there's some problems that arise in this kind of equilibrium is like some kind of consensus destabilizing or dos attacks as if if i am the proposer of the current of the next block then i can try to dos or reorg the block from the previous slot in order to extract the mev because it's cheap to extract and i'm not giving up any resources to do that there's also a lot of incentive for validators or proposers to collude across laws so if you control two consecutive proposer slots and if you um have instances of mev where the mev across two slots so maybe reordering across two slots is larger than the mav you get from each slot individually added up then you can collude with other proposers to get more benefit yourselves so this is kind of the first equilibrium that can happen with proposal builder separation here's another equilibrium so what what happens when most mev strategies by value again become easy but you need some kind of infrastructure that requires significant capital in order to extract so you can say that something like hft is in a similar stage where most of the strategies for high frequency trading are well known but you need significant infrastructure and latency for example so in this kind of equilibrium you can assume you can basically say that you know large validating pools can run their own mev algorithms small validators can still exist and they'll use a whole building market to propose their blocks so in if something like this happens then you can see that it's easy for the biggest validators to become the biggest proposer uh biggest builders and vice versa because again you need significant capital to um extract these mvv opportunities but they're not necessarily hard to do and it can also possibly lead to validator centralization for executing mev strategies because validators can pull together their resources um to build this like infrastructure that they need for extraction so here's sort of the most equal most interesting equilibrium i would say so this is the third one so what happens when complex and highly profitable mev strategies are regularly found indefinitely right so this seems similar to kind of non-hft traditional finance but not exactly the same because we have some kind of transparency for blockchains this seems to be the path that mev is going so far but it's unclear if this will be sustainable long term as well so in a situation like this you can imagine building become hyper-specialized and this was alluded to in a few previous talks as well and validators can be just thought of as simple proposers and this was kind of the original design that pbs was meant for but the point here is that even when new strategies are found it's likely that the fraction of new mev that can be created by these strategies will be small compared to the overall mev pool so if you have one builder that controls most of the block building and is like significantly more powerful than others then you can consider consider a scenario where the builder is kind of forcing proposers to exclusively get blocks from this builder and i'll talk a little bit about this in um so-called consecutive or two block mev in the next slide as well so my argument is that builder centralization has significant negative consequences so the first sort of consequences is it's very likely that large builders will become large validators as well and partly because becoming a validator has low additional overhead over already becoming a builder the second and more interesting thing is if you have two or more consecutive proposal slots they can lead to better mev strategies if two block mav is greater than the sum of one block mavs and so it's sort of profitable for a large centralized builder to also become a large validator to extract this muv the other thing that can happen is builder centralization can create collusion incentives for validators or even validator centralization so again if you have one builder that's significantly more powerful and that it can find consistently more mev than others then it can incentivize or force uh proposers to completely ignore other block builders and so that they it's rational for the proposers to do so because they get more mev from this particular block builder right so it can form these exclusive partnerships with validators that pbs is trying to solve the most another thing that it can do is it can get exclusive order flow from wallets so all of the transactions kind of gravitate towards this builder the builder builds all the ordering um responsible for them and then it also colludes with the proposers to exclude from any other builder so that's sort of the nightmare scenario i would say for pbs in the case that mev opportunities still continue to be found and the builder becomes centralized so the problem here is like actually if you look at it from a formal perspective we still need a lot of rigorous analysis on pbs we currently only think that separating these two roles of builders and proposers is possible or it can actually happen and we don't actually have a good understanding yet of the centralization points and this is again me speaking as an academic so we don't have an understanding of under what situations a single entity can play both the role of a proposer and a builder and what happens when it does um but more importantly we also don't have a good understanding of how this affects users and i'll nicely segue into this in a few slides later so the likely solution is to decentralize block building or ordering um and this has also been alluded to in a couple talks uh before and one one kind of possible solution um i would put out there is these so-called decentralized ordering protocols that i've been working on um so the idea is to decentralize the transaction ordering to many nodes instead of a single node and you can get some kind of strong time-based ordering properties if a majority of the nodes turn out to be honest so these notions of receiver batch order fairness which roughly say that if a transaction tx1 was received before tx2 by many nodes in the network or a majority of the network then the first transaction should be ordered earlier and the crucial point here is that even if you don't make any honesty assumption the ordering is still decentralized and it does not actually break safety even if you don't have any honesty assumptions and all of the fair ordering protocols are all of these decentralized ordering proto protocols like iquitos or themis they all work this way so just a quick two slide kind of summary of how icotos operates but please see the full talk for more details is the idea is quite intuitive you essentially build a transaction graph where vertices in this graph are transactions and the edges denote ordering preferences and then you can compute the condensation graph of these to collapse the strongly connected components because it turns out that you can get some cyclic orderings due to a connection with social choice theory and then finally you can output the topological sorting of the graph in order to get the final total ordering and in the new protocol themis what we do is the leader when it's building this graph it can categorize transactions into three types the first is solid transactions that are received many times which will be included in the block proposal blank transactions which are not received enough times and they will be excluded and other cases are shaded transactions and they'll only be included if they have some path in the graph to a solid transaction and importantly the missing edge between these shaded transactions can be added by subsequent honest leaders and this only depends on network delay um so yeah again this was you should see my more technical talk on this at spc the main conference for more details here's also a couple paper links for these ones another kind of uh sort of problem that's being been stated about these kind of fair ordering protocols is so-called latency wars so i like to claim here that despite popular belief there's actually easy ways to prevent latency wars on more time-based ordering protocols so how do we do this so you can actually change the granularity of fairness that you want to consider in the protocol so you can bucketize the time or by either the time or the number of transactions and still get time-based fairness across buckets so we can call this some kind of fuzzy or bucketized notion of first in first.ordering and you can randomize ordering within each small bucket but you still get significantly better than like significantly better even spam protection let's say than randomizing full block and basically another advantage in latency terms is you don't have any more advantage of having a faster network than the granularity version so this is actually in the themis paper as well another sort of thing that i would point out for in comparison to latency wars is it's very easy to think about like latency wars and directly point to something like hft where the nc wars are bad but it turns out that in a lot of markets the way hft works is very different so this kind of reductive analogy doesn't also doesn't necessarily work um for example in some markets instead of the network switching doing sort of a first and first come [Music] for ordering transactions in the centralized marketplace uh you can look like basically there's some markets that locally randomize transactions that are sent in some duration so hft looks a lot different in these kind of markets because having a significant latency advantage no longer helps so the point i'd like to make is that you can't just think of latency wars as like being reduced to hft equals bad there's actually ways that you can fix this and we do have the chance to think about this more deeply when we integrate this into protocol design so i want to talk about a couple sort of more dispersed thoughts and segue into them a little bit so the first thing i want to talk about is actually long-range mev extraction and why it's terrible for the ecosystem and should actually be completely abolished so if you think about sort of full block mev auctions especially happening under proof-of-stake then you can think that they lead to significantly larger mev or significantly more long-range mev than is actually required for example if you control consecutive proposers you can order manipulate across you know two or three block time periods um which is like 20 or 30 seconds even which gives significant um bad ux to the users who can have their transactions be reordered across like 10 or 20 seconds and this is very easy to detect and it's clear to everyone that a user transaction was received before if it was reordered like 10 seconds into the future basically um so you know it's hard to agree on the true ordering quote unquote for transactions that are close to each other which could be sort of shuffled just due to network delay but it's very easy to detect transaction reordering for transactions that are clearly far apart and we should definitely definitely not glorify order manipulation across such long time periods and even today it should be easy to detect when something like this happens in employee slashing by something like eigen layer for example the other kind of scattered thought i wanted to talk a little bit about is incentive compatibility this has been a pretty hot topic in this conference as well so my main point here is actually mechanism design needs to take into account actual users and not just protocol participants so incentive compatibility for builders or proposers or validators doesn't make sense if there's no users right negative externalities that you're thinking about should be considered for regular users as well for example are some users being harmed more than others so if you think about for example sandwich attacks um reducing like gas cost for everyone across the board but they harm like specific users and make the use make the cost like better for everyone so is this um sort of an infrastructure that we want to create where one user gets harmed significantly more than others right so that's sort of one thing to consider the other thing i'll say is actually maximally extraction or mev auctions they're not incentive compatible for users so you can argue that they're incentive compatible for protocol participants right i can maybe even formally study this also although i'm not sure of any like formal study on this but you can potentially say that they're not incentive compatible for users on the other hand you can say uh that something like first come first serve is likely incentive compatible for users because it can give users best price execution right so you at the end of the day you don't want your users to leave and sub-optimal profit will always be preferred by protocol participants if the alternative is your user's moving somewhere else and of course the thing is incentive compatibility even on its own is a difficult problem and if you bring users into the mix it probably becomes even more significantly difficult an additional point is like even sometimes incentive compatibility may not be enough you can think of like killing the competition by temporarily taking losses something like amazon lowering their prices in order to kill the competition and become more centralized so these are some things that we have to sort of think about even when we think about incentive compatibility so this was sort of alluded to by first talk as well but we need to balance incentives for protocol participants and incentives for users so my controversial take here potentially controversial hopefully not is that incentives for users should actually be significantly more valuable than incentives for protocol participants because bad incentives for users ends up with just users leaving the system regardless of how good the rest of the system is right and understanding the real long-term cost of some of the solutions is very key especially of this transaction order manipulation for example an easy example to say is what do you prefer right no fees but maximally extractive versus no order manipulation but higher fees and a classic example of this is robin hood versus any other real brokerage where robin hood was selling order flow but giving free transaction fees to its users and actually the sec found recently i think a year and a half ago that if the users were getting significantly lower price by using robinhood with zero transaction fees than if they just pay the transaction fees at a different exchange in the first place so this is something that we want to think about from the perspective of users as well and that sort of you know comes under this umbrella incentive compatibility um there's a couple ways that i thought about combining sort of decentralized or fair ordering protocols with pbs the first thing that should be done like almost immediately is some kind of time-based ordering for transactions that are clearly far apart so we don't want the terrible ux of reordering transactions that are 10 20 seconds apart right so we can still let short-term mev exist via auctions but this would be significantly better for you a significantly better ux for the users while keeping everything else the same another sort of potential long-term idea that still needs a lot of formal analysis is to split blocks into buckets and so you can think about each bucket as having a top part and a bottom part where the top part um has some kind of mev auctions so these are transactions that are within network delays of each other and have mev and the bottom part of each bucket does random ordering so these are transactions that are within network delay and don't expose muv so we can just order them randomly and then you can think about more first come first serve style ordering across transactions in different buckets so you cannot reorder transactions across different buckets and basically provide better ux for the users of not reordering across long time intervals and you can also think about dynamically changing the size of each bucket and the parts within um so let me end with a couple takeaways that if you listen to nothing else in this talk i would like you to get out so the first thing is you really have to think about incentive compatibility not just from the protocol entities but also from the users if you think about mbv auctions you can think about incentive compatibility from protocol participants but not users if you think about something like first come for serve it's incentive compatible from users but not protocol participants so are there ways to sort of solve in either direction or some ways to combine both of them right the other very key very crucial point is that we need to think about long-term optimization of our goals and not just in the short term so we need to understand long-term effects of the decisions that we're taking right now otherwise what ends up happening is you get stuck in a local extrema rather than a global extrema and you cannot escape this local extrema very easily so for example when a short-term optimal design gets entrenched into the system it's extremely difficult for people to switch to a better more sustainable design that's more long-term focused in the future and this is something that we need to think about when adding things to protocols in a permissionless network especially the other kind of interesting takeaway that i want you to go away with is reasoning by analogy or bias sometimes versus reasoning by first principles it's often easy to make reductive analogies to something to argue a point and in fact they're highly effective sound bites right you can get people's attention very easily by making reductive analogies they're useful for propagating information but the real takeaway here is when we want to actually deploy some of these systems we need to take a step back and more precisely more formally understand what's going on and in ways that do not reduce to just reductive analogies so it's kind of another key takeaway that i want and i'll sort of end there if you want to reach out here's email and twitter i hope this talk has left you with some questions probably more questions than answers um hopefully some thought-provoking insights as well and happy to take any questions i'm not i can't promise that i'll be able to answer all of them but i can try and here's so while i think about questions um here's sort of a bonus slide which says you know without context of sometimes the path that looks worse at the start can actually lead to the paradise with the unicorn um one quick question on the uh low courser resolution fair ordering yeah how do you order transactions inside each bucket we still nee we still have a single threaded execution engine um yeah so basically they're bucket ties so um you still have a total ordering within them so the way i was thinking about it was you can like randomize the ordering or you can even do something like mav auctions within each bucket but you want to have the ux of not reordering across like long time intervals and especially with something like proof of stake where you know if you're a proposer in advance you don't want the kind of bad ux of someone waiting two blocks um and using like mev from two blocks right so reordering across two or more blocks hey thank you for the presentation uh two questions the first one um do you have any clues for researchers in the room on how like pbs should be formalized or like the types of tools that can be used uh and they're like formal weapons uh the second question is you mentioned there were markets that uh have like slightly different market structures in traditional finance where that lead to like different outcomes for how like higher frequency trading players interact with these markets could you give examples of these markets um yeah so the so sort of the first question um it's sort of a challenging problem right formalizing pbs and understanding all the nuances of like rational or in general it's kind of hard to formalize um rational behavior and incentive compatibility especially in this kind of permissionless setting but i can imagine sort of starting small with some basic model and then kind of building up of what insights you get from this and then kind of throwing more things into the model for your second question i have been told that something like the indian market the way like the network switches work is somewhat in the sense of like randomizing the ordering across some time intervals so um you can't do like very high sorry like very low latency hft there thank you for the talk um if i understood correctly you said that long range reordering can be detected and slashed i wonder if you can expand on that if and if there is any relationship to detecting censorship here um yeah so there's actually a number of ways you can do that for example you don't even have to because the things the way things propagate on the mempool you can actually take advantage of even like light clients that are just observing the mempo and not really processing transactions and it's significantly easier for example to have an honest majority assumption among like all the clients rather than uh just like a portion of the validators or something you can also have more fancy techniques of like audit-based mechanisms to make sure that um make it like incentive compatible for builders to not reorder across blocks so you can think of like one transaction piggybacks on top of another so that you can come up with an actual proof that there was an ordering violation can you describe the like randomization inside each bucket that you proposed in the middle for i could basically micro scale like a shuffling so you prevent hft player from predicting how things goes um yeah so basically the idea was you know in each bucket you can think of the top of the bucket as some kind of mev extraction but within the only within transactions that arise in this sort of small network delay right so they could in general be randomized as well and then the later part because it doesn't have mvv you can just order it however you want um so that that's what i was thinking for the random part don't need like true randomness um typically so i mean that's sort of an orthogonal question typically you can use something like vdfs even to provide the randomness um you really only need like one honest participant to provide like true randomness right thank you okay thank you 