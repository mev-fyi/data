and uh it's quite i probably will rejoin and the network scene is a little bit lag give me a minute it seems perfect to me maybe just start with a brief introduction of of yourself and your team of researchers he i believe he left and he's rejoining gotcha but i'm sharing my screen with the paper i don't know if that's useful but you know yeah i guess uh while we wait uh if everyone anyone hasn't read this paper uh including myself now take some time to read it in more depth right i share the link my name is airport and uh it is being people in very political london i'm working on life and broadcaster so yeah i'm happy to share that we recently we released the paper um quantifying the production expectable value so basically we measured in the past two years um much value can be expected for has been extracted from the israeli blockchain uh from liquidation um arbitrage and generation and we we also provide a general general record uh replay transaction you play everything that allows an adversary that to replay the connection from the victim and the front run this victim transaction to extract the value and also we captured the transactions that are provided privately mined by the miners which that which means that these transactions are mined into a block but never appears in the memory proof and i think this is the three main part that we are present in this paper so have some questions if yeah so i read the paper quite interesting so a few questions i had in your measurements of the past two years you basically just look at the just blockchain data just data that was you could already read or it was some some of the measurements done on the mempool itself in terms of what transactions that you are looking for or is it just what was actually happening um we only like uh in in in the like in the value measurement uh we only look at the transactions that are like finally submit into the main uh into the mention we haven't captured the the thing the strength is that appear in the memphis but never like uh mined in into the eastern blockchain i wonder if there is more interesting information that can be extracted like besides looking at the final transactions i mean i i already have some heuristics of the what's a destructive front running and others if anyone has any ideas on that then please please mention them but for example transactions mean probably there are transactions that are for example were cancelled and they were trying to take the same thing but are not were not actually successful because they failed or because someone else says they grabbed the dmv already before them or one one did the gas war etc so how how do you categorize them or did you even look at these uh is it interesting to look at them yeah i mean it's definitely interesting to look at these transactions that are not like executed successfully or even like not mined into the main chain but it's more challenging than just to look at these changes executed successfully because like if we um so for those are not executed the directions it's hard to like dive into what happened or what's the underlying logic so i mean it's essentially like it's i mean those like reverted connections are not executed on on the expected block state right so it might be challenging to to figure out what is going on there and whether it's extra value extracting transaction or not but it's definitely interesting to to like look deeper into into them okay yeah so uh so that's what was the first question we had a second regarding the private private transactions that we had in the last section so a as i understand that the measurements were made on a single node over several weeks and basically you compare what what was seen by the node and what was not uh but was later on the blockchain itself besides besides this have you er you take a look deeper look at what these transactions are for example easter mine i know they are not trying to hide that they are distributing mining rewards in the top of the block without the gas payments and i guess these would be would appear there as well i mean they're not broadcasting in the transaction they used to distribute the funds uh so i mean i guess we can distinguish these kind of payment rewards and transactions that are actually a private dark pool transactions or however you want to classify them so i wonder if you looked into that um yeah so so yeah thanks to this these comments like from like tuna and the other guys like um yeah are some transactions that are actually payment transactions so so we already like operated this table which hasn't been reflected on the outcome but yeah we already updated the table so we distinguish the transaction that only cost uh 21 000 guests which means that this is the single payment transaction and also it is that that cost more than 21 000 deaths which means that these transactions are like invoking contracts and we find that um around the eight percent uh private demand checking that are like invoke smart contracts so these are more interesting transactions that we should look into eight percent of the ones you have here or percent out of the list transactions are already listed yeah okay yeah make sense thanks i also have two questions i mean happy to also see uh any other results you find interesting i don't want to derail with all of our questions but uh i like it when people ask random questions too um so i guess one of them is is can you talk more about the front-running taxonomy and like how you guys arrived at that and uh like another one is have you have you thought about maybe checking if there's any overlap between the transactions that you've seen that are replayable in the past and kind of uh insertable with the minor address so the kind of last uh last attack in the paper um if there's any overlap between that and like privately mined transactions um so that would indicate that those are open front runnable transactions that are being taken by the miner i guess yeah that's that's a quite good uh good question so um uh regarding the textile mini like we need basically extend the texture from the previous slk fc paper and we change the name a bit which we think is more indicated um so like in the previous slk paper uh basically it covers the the front running and backgrounding so we basically extend it we like distinguish two kinds of different front-running techniques like one is destructive which means that you don't want the victim transaction victim transaction is actually can be executed successfully and one is the cooperative front running which means that you want the the transactions that you are front running can be executed successfully then you can kind of for example in the standard attack you can first front wrong a victim transitions then you perfect from another background transaction and it will also add in a clogging transaction which means that you are like dumped a lot of generation into block to delay the execution of your target transaction so basically uh we kind of want to um sorely like summarize all the ordering transaction ordering technicals and regarding your second questions um you are asking if i understand correctly asking whether this um replay about transactions are from miners right yeah like if any of those uh the the replay transactions that you identified as being vulnerable were also not broadcast on the mempool if they were in that time period that you were um so basically i mean we we measured the replay transactions in the past two years but we haven't recorded manpoo in this period so unfortunately we haven't captured this but in during the week that we measured the privately mined transactions we uh we evaluated our replay about algorithm on this privately-minded transitions and we find only only i remember is 13 transactions that can be replayable i mean certain privately mined transactions can be replayed yeah that's super interesting i'm sure those transactions will be will be interesting to dive deep into yeah we we we check a bit of all of them uh like one inch transactions so they are just yeah one inch because some the user ones can like submit private transactions so yeah all in our initiation that makes sense i guess were there any results that surprised you or that are especially uh impactful yeah one thing interesting is like we case studied the top high profit replayable transactions so which is present in table 9 we find that the the the bjx attackers and the other defined decks that these attacking transactions that can be replayed so which means that if i was running this replay equation at the time of attack i actually can steal the profit from these d5 attacks why do you why do you think it wasn't done um i'm not really sure maybe i mean i don't know maybe the attacks are not aware of such vulnerabilities probably at least with what was it what are they called so there definitely were some transactions that were actually being frontline so uh maybe there is some double counting here as well like uh it is possible that some of these are actually bots but i mean it's it's hard to tell but it is possible that some of these are actually what's work front running someone else who is doing the same it feels like there might be an interesting uh opportunity to have like a um like a transaction replayer savior in a way or like for all these hacks you know they get front run for the one that can be replayable and then like the money is given back like a white hat generalized front runner does that make sense um one question that i wanted wanted to ask kai kaiwa i'm sorry if i'm not pronouncing your name right when you when you count liquidations i understand that currently uh for fixed liquidation you you basically uh you know um compute what you know that fixed spread is depending on the amount of collateral and stop there in terms of estimating what the profit is but should it make more sense to include the because you know when someone executes the liquidation and you know has a bunch of collateral on their hand they usually want to get rid of it as fast as possible um and so it wouldn't make more sense to calculate the profit of the liquidation from after they've gotten rid of the collateral that they just got from the liquidation or if they don't get rid of the collateral scott was mentioning this to me yesterday to do something where you look at the you know uh the deepest liquidity pool at the time and you calculate some kind of implied price slippage because because in general a lot of liquidators while they do for example make five percent uh from the fixed spread they might pay you know eight percent in slippage where the actual profit from the liquidation or this whole endeavor uh is is negative for them um instead of instead of positive so how how do you would you draw the line to estimate that and and i guess the follow-up question to that is how do you um so how do you see the number that you currently have i think it's around 20 20 million for the last two years yeah so this is these are my questions sorry um yeah i think um so currently we just simply calculated the the difference between the the i mean we we calculated the price from the price oracle of the liquidation um uh platform and we just like uh count the difference between the the debt you repay at the liquidator repay and the glacier he buys he buys and we calculate the difference as the profit um so it's quite simple uh but i think you you your your metric is quite interesting and uh yeah we we we we would like properly consider it because like it's it it's it makes sense to consider the the the other stuff but not just like calculate the difference as the perfect cool and um in that same kind of vein what um what do you expect are improvements to the paper that you're you're uh kind of looking forward to do so as an improvement in some of you know the way you collect metrics or some or maybe in the heuristics you currently look at uh sorry i i can hear you clearly oh sorry um are there any obvious improvements you're like you're going to work on for you know new iterations of this paper by improvements i mean in the way you quantify some of these metrics uh or some of these numbers um or maybe adding more nuance to some of the data if you think some of it would benefit from it so is there anything there that you're thinking of yeah i mean the the thing you just mentioned is quite helpful like to to improve the people and also probably uh we received some feedback regarding the the private mind transactions like we probably may set up more notes to to listen uh from the um to peer network to make the the result more accurate and we probably would like dive deeper into these private money transactions to to figure out what is going on there where the miners are actually involved in the mdv or pv um something like that yeah cool one tiny comment and it's it's not strongly held opinion but in your front running taxonomy i would potentially consider changing the cooperative term because it sounds like um it's like relatively positive or something like this especially when it's uh you know opposed to destructive one i mean in fact the cooperative can be you know increasing someone's i mean the the cooperative type of front running can potentially cause more user harm than the quote-unquote destructive front-running right in the sense where one is trying to snatch an arb and the other one is increasing a user's slippage for example yes i mean this is it's a tiny detail to be honest uh but um that's kind of what came to mind when i was reading it hi alex yeah sorry for joining with late thanks for those very invaluable comments yeah yeah we had a hard time to finding the right words it's like so tricky right for these these terms no but it makes sense in in the in the way we're like you know the the the front runner needs the transaction that it's trying to front run to also execute right so there's like an element of like dependency or like conditional uh maybe conditional from running it could be parasitic front running i don't know if you want to get more moral about it constructive parasitic um so anyway arthur we were talking about your your paper and kaiwa was giving us a rundown of some of your results and answering questions very nice yeah yeah we have good comments from from tina already on on twitter regarding the uh the private transactions one thing i'm not sure you discussed already but on ether scan you can actually see whether it's a private transaction or not according to each of the etherscan because etherscan shows you uh time uh mind for a transaction time it took to minor transaction if it doesn't show a time then a court like i suppose according to the scan it's a private transaction because at least for those that we tested manually we found that to be the case so sometimes people could go and scrape the etherscan data because they probably have bigger data that's right or ask them to provide um i had another question regarding yeah you had some metrics on the gas usage of the the the front running types so how how did you measure or how did you quantify the gas used for for back running okay most of these transactions won't be i mean it would take some some parsing to figure out where they were participating in the background so how did you quantify that i would relate to okay i'm actually not sure right now i'm i'm not very sure so which table of figure are you referring to um you have a figure of a gas prices [Music] yeah yeah that's the one uh i mean for the liquidations right in this case yes for the liquidations is so where i guess it's a for for front running it makes sense to take the price of the transactions that transaction that they won the ndv race but in case of back running there is one transaction who is winning and maybe hundreds of others that don't do anything which basically clog up the network a lot more so i wonder if you took these in consideration and if you did how did you find that they are indeed part of the background um so the way we we like we we test the weather liquidation transaction is um frontline or background we just like executed on the at the top of the block so if it's still executable then which means that it is based on the on the like i mean it's based on the like state of the previous block right so but if it's not then basically it rely on on the like on the transactions that are executed before it was executed right so then we based on this methodology we did we tested whether liquidation transactions is a background of returning i guess like uh the the if you're just looking at gray and not total gas usage it might be it's probably a reasonable proxy to use the like successful back running for all back running because you don't know which one's gonna succeed so you'll get like some random sample of uh of like viable back running transactions as the one that actually succeeds i think it would also be cool to see the total gas used for for back running but i think that's maybe also one thing that uh that that we want for uh mev explorer because it's a very important kind of community relevant metric of how bad the blockchain is getting congested and i also think uh yeah the direction you mentioned about studying the private minor transactions more is obviously super interesting and uh i'm sure there will be a lot more of those soon yep um so something else is interesting is that i believe compound uses a an oracle mechanism where you bring a signed transaction to the chain and so i think that that leads to a lot less background possibilities since you your transaction effectively is the is the oracle transaction so you can kind of see it in that chart that back running is more significant in ave and dydx where that is not the case where there's some sort of like a chain link or other oracle update yeah we we mentioned in the paper that the compound is different that the the like the average gas price of compound background transactions are actually higher than the front-running challenge that makes a lot of sense yeah that's cool it's uh it's relatively it's a relatively new change right they changed it around a moderate thing or something like that so uh i mean not necessarily in this but would be you can share the data it would be interesting to see how it changed before and after they applied this change yep oh nice i didn't know this recent change thanks regarding the private transactions i think tina mentioned there's this tai chi network um so we we couldn't really figure out what the contract does that we referenced for spark pool but it seemed like they're doing an arbitrage um this zero zero two contract um and it doesn't seem to be uh related to the tai chi network so i think i would test tried that out quickly yesterday or today um so we are still not we're not sure whether it's related to the tai chi network i mean i also have to say we only collect i mean the data here is only for a week of network data it's not that much right so um there might be many more insights if you look on a longer time frame yeah i think there's tai chi and also archer is on main net presumably mining things so there should be some maybe replayable transactions there eventually in addition to the one-inch ones i would expect at least uh if there's not there may maybe they have protection against that or something um yeah that might be the other thing that could be interesting looking at like replay protection because a lot of these uh these bots have been replaying each other's transactions for a while so they have like super sophisticated mechanisms to to make like the simpler replay algorithms fail uh so i wonder if you guys ran into those or or if that's like uh future work because i know there's definitely some out there yeah the abstract another the appendix i think you wrote some honeypot um example in the in the appendix um for simple replay algorithms okay you can you can maybe present this quickly or discuss um yeah so so i mean so um the basic idea is like when you replay a transaction you just execute the transaction on the on the state of the previous block so actually an adversary can actually like issue two transactions that the first one like change the state of the the contract or whatever and then when the replay attacker try to replace the second profitable transaction it actually will be exceed upon after the first like we call it first transaction then it will actually lose money like which is the different result from the local emulation when the transaction is actually on the previous block yeah that makes sense i think it's been super interesting seeing the games people have played with uh that kind of cat and mouse game i know that those bots also griefed each other for a while to try to like make each other waste gas on uh on those kinds of honey pots and things like that by playing with monsters and gas prices right exactly yeah all right um if you don't have any more questions i mean i i i expect uh for those of us that haven't necessarily go very deep in the paper yeah that you know more questions will arise as we uh you know examine it very very closely um but unless there are any other questions or arthur and kyle unless there's anything else you kind of wanted to talk about or bring up on this call in particular uh to like discuss with the people here um we could potentially move on to the next part of the call sounds great i mean i would i'm just like really happy that that you guys find that interesting and and and and give these great comments right and i think it's it's a it's a great community effort in the end um yeah and we're looking forward to working on the frps as well yeah super interesting work thanks for presenting it and definitely uh i think yeah the more measurement we have on this the better uh because oftentimes i think otherwise this would outpace our ability to understand it very quickly so it's a it's a very uh interesting fast-paced i think uh topic of uh analysis totally i also look forward to like non-intuitive wizard results uh that are like um backed by data i think i'm like excited about any any of those that are found um sorry if you let me know if you hear noise i don't know if that's a little bit but it's not bad okay um yeah i can't do much i'm sorry um all right tina should we move on to the next part of the agenda sounds good um yeah i think it's uh highly related that's why we group these topics together uh this time for so that we could dive deeper into the empirical aspects of quantifying the um mbb realized on chain so go ahead alex now yours all right all right yes this is related i guess um as some of you may know already part of uh the initiative at flashbacks is to eliminate the dark force similar to um arthur's and kaios and lee's paper uh and to you know provide this information to the community at large and so um one effort to do this is to give the community a general dashboard linked to our data collection and inspection efforts so i want to kind of show this dashboard really quickly or at least the latest version of this dashboard which is still being refined in its design and also in uh the the the the kind of data that's collected in it uh i just should be able to see unless you can't see i think you think you can but yeah um all right so we've called it mv explorer um it's to to preface all of this the data is there that we've collected ourselves with a 3m full archival node and it is data from february 11th to today there are gaps in the data because there's still blocks that we need to run our uh quote-unquote protocol inspectors on but um it's it's you know good enough right now that it's it's it's worth presenting um so kind of diving deep into the the numbers uh on the left here you have oh and another thing to mention so this data here quantifies the value that's extracted from uh the ordering of of transactions so in essence most of this value today on on ethereum is uh through you know snatching an arbitrage transaction snatching a liquidation where there's a sense of ordering where you you want to be first to to snatch that opportunity um it discounts um hacks and it discounts the quote unquote basic mev which would be uh the value miners get from transaction fees and and block rewards right which is also maybe in the sense where it is a value that uh the miner is getting for transaction ordering it's just quote unquote naive value um which we think are less representative i mean we could add these numbers here but currently they're not added um and yeah one last disclaimer this is this is not trying to quantify the the theoretical maximum value that that could have been extracted this is going back historically and quantifying the value that has been extracted so the realized med revenue um yeah so first on the left here is a cumulative realize there may be revenue um and you can see kind of how it how it uh has grown very quickly uh somehow with how d5 has been growing um the actual full number up there at least the number from you know around today is 142 million um dollars um on on the right here is the same but not cumulative so you can kind of see the distribution the y-axis here is a is a power uh connection it's the best way access to be honest but when you when you keep it linear uh it's not necessarily uh you can't really see anything and um so i mean the the whole point of having these two and having this one on the right in particular is because we kind of want to show that mvp opportunities are not distributed evenly within blocks uh and the the distribution is actually uh highly irregular um and that's i think important to point out for for several reasons both for you know traders trying to extract this value and to understand you know the consistency of their profits similarly for you know from our consensus consensus security perspective uh the the less this value is distributed evenly the more just is kind of like instability around minor incentives right um which is important to cornell um i when i dive deeper in arthur's uh paper um i definitely want to compare some of our figures to to yours uh definitely um this number here 13 million is total fees traders have paid for uh unsuccessful mev transactions so reverts and the equivalent wasted blocks so if you take the reverted transaction and their their gas spent and then you you take the aggregate gas span you divide that by the gas limit uh of ethereum blocks which is 12.5 million uh for that for that period of time you get about 32 000 uh ethereum blocks that could have been filled with these uh these reverted transactions so quote-unquote wasted um blocks from the perspective of you know the transactions weren't uh useful uh and you know they need to be recorded on the ethereum blockchain um and then we kind of get into you know naive splits of some of the values above um so the revenue quote-unquote revenue split between miners and traders so separating from the 142 million figure uh how much of that was transaction fees so about 30 13.2 million and how much of that was lose to the rest uh similarly separating by strategies so this is currently again [Music] uh it's mostly charged so about like and about only five percent are our missions um one thing i should have mentioned is the the coverage of what we're doing here so let me actually pull up uh uh [Music] we we cover a lot of but not all of them so for example dydx liquidations are not counted in right now so i'll i'll give you more of that in a second and the protocols interacted with um so for the arbs that you know go across several protocols uh we kind of assign the value of the arb equally between each protocol and we we weigh the the protocol interaction that way to get to get to these percentages right so really quickly i should have mentioned these th this is what's currently covered minus um dydx and esd dsd is still to be covered so i have a compound curve balancer 0x um and what what we do essentially is with our ethereum full archival node we kind of go back in ethereum transactions and we we inspect its trace and we kind of build um first we we inspected ourselves and then we build an understanding of specific types of trading strategies and we build that mechanically with an inspector that we then run against uh the rest of our data and that way we um have a database of quote-unquote mev transactions with their related revenue because part of the inspection has to do with uh understanding the value flow which a lot of the time can be relatively complex but can be simplified a little bit similar to if you've seen um tx.info similar similar to their little like table of uh you know evening out where where where transactions uh i mean where the value is flowing basically um going back to movie explorer we didn't have a little bit more granularity by showing like the latest image transactions that um have gone through our through our system um that's that's it for explore we do have um several more kind of like transaction views that would be interesting um to people that want to drill a little deeper but in general the idea from ev explorer is to cater to the a very general audience that is is just starting to understand what mev is and starting to and nuance um but we do want to be precise of course uh we want to be rigorous right there's there's uh about months to to strike here um so before i kind of move on to something related to that i'd love if you guys have any questions on the metrics itself or um anything else there i have one which is uh so i guess these these numbers are just starting it looks like uh in 2020 so what was the i guess there's obviously like some sort of recency bias is that just because like the old mev types aren't supported or is that just because there's a much more mev that started at that point i think there's much more mov started at that point yeah um and it's also because the the way our infrastructure currently works running these inspectors is quite time consuming and um having all that that block data actually accounted for can be a long process and so for the for the for the benefit of you know wanting to push something out uh that is meaningful but at the same time that won't take several extra months to to do uh we're kind of going with uh 2020 onwards i also think most of the value has been extra i mean if you discount hacks most of the value uh has happened as the fi has grown tremendously and as you know volumes has grown and um as more liquidity pools have been created and you know a lot of these arbs are you know across sushi swap and unit swap across balancer and unit swap and um all of these have seen the light of day relatively recently right balancer was in december sushi swap was in december 2019 sorry sushi stuff was in 2020 of course um so yeah this is the current current idea do you do you think there's a significant amount of data um that was before 2020 or like a value sorry um i think that's a that's a reasonable approach um yeah it's hard to say um but yeah no i think that's reasonable uh i guess another question uh is like have you thought about how to let other people contribute to this uh easily and is there a way i guess especially anyone on the call who's interested can either refine these or like introduce new metrics absolutely that's another great question um so once we as we release this there will be this kind of document that i i kind of um went on afterwards where um all the metrics and the queries would be explained we'll have a a public github um and we'll also have like documentation for people to contribute because we definitely want this to be a community effort going forward so while we're kind of seeing this as like a bootstrapping way of putting it out there and getting people's attention i think uh we want for example arthur's team and like their numbers to be potentially included in or any any kind of granularity there that we can add would be amazing for example i also think a lot of protocols themselves will want um you know they have intimate knowledge of their smart contracts and how value flows within it and they have an incentive to also help us uh you know have the right numbers on there so i definitely want to involve the protocols themselves so i'm thinking you know uniswap a balancer i also don't think it's a massive engineering project on their side it's like really a side project and it's part of the ethos of this ecosystem of contributing to public public goods um so there are definitely plans to involve the community and ideally i think uh you know maybe two too idealistic but i really would definitely want this to be just general community maintained effort where like flashbacks of course uh commits to maintaining it and then commits you know a human capital to maintaining it but that generally it's kind of cruising thanks to a community effort and as a new protocol arises uh and creates new interesting opportunities that they're they're added in um that is definitely the vision for this awesome um i was curious about like what are the specific metrics or heuristics you use for for each of the sources because like for instance um from from the work we've done i can see that i mean i have less confidence in the heuristics for sandwich attacks and arbitrage then for example for the data from the liquidation events um well it depends i mean the backgrounding like identifying what is background and what not it's also kind of a heuristic but i think it's always good if we can i i quantifying the error is probably difficult but if we can give an indication of the level of confidence we have on the results um did you have you written down the heuristics you used or the metrics you used like a bit more formally maybe or or just like sentences yeah so we have uh i i wouldn't say formally especially given the academics on this call uh i wouldn't say formula but we have written the the caveats to to the metrics uh i mean to to our approach i think one thing that's important to to say is this doesn't account for sandwiching attacks so this is this is single single transaction um mev anything that's beyond silver transaction it requires more um work on you know basically the heuristics that you guys apply right which is like looking at several transactions and uh deciding at a general level what what is a sandwich attack or not depending on on the behavior of those transactions together so currently it's limited to single single transaction behavior so anything that's over multiple transactions is not accounted for in this in in these in these metrics um and so that that kind of applies to to all forms of back running so i think it's important to quantify back running but you know for back running for liquidation for example you can still quantify the liquidation profit i guess the back running element is more to discount part of the profit uh based on you know the the gas that it took and the effort that it took to spam the network with background transactions that's that's pretty cool yeah um i wonder why you get so much more revenue than we do um do you consider more exchanges or what do you think is mean no no not really i think the our approach differs in the way where we kind of look at the the traces of each transactions um and there there might be something there where the inspectors were running there is extra um there's there's you know if you if the i mean depending on the rigor with which you code the inspector [Music] i mean you can have heuristics still valid uh and that can capture a lot of value related to transaction ordering in a way that uh might be harder to do if you don't have access to the trace uh in in in your guy's case so i think that that might be an answer there another thing that i definitely want to reiterate is these are um estimates that we're we're kind of refining um and so there's there's there's definitely stuff there uh that might not be um as accurate yeah yeah i'm just looking at like i mean uh would be awesome to find what we missed right because um it's um i think it's great that we have two different separate approaches of doing things and ideally we try to merge towards the same numbers right at some point um like an obvious next step to like make sure that all of the numbers that you guys released are kind of also included here and maybe part of that is generalizing to multiple transactions maybe that's like a clear action item on the explorer side um and i think also the reverse of like cross-validating i think is uh why it's important to have these independent efforts um because because these the data is just like they're so so large of a volume of data having worked with it for years now and like the heuristics tend to be like pretty complicated and like you're looking at like so much data at once that like i think it's just good to have multiple people trying it independently and then like converge i think is a good approach so that's interesting so um okay do you have any um can you share maybe later right offline like maybe any transaction hashes where you think that having access to the trace actually uh allows you to find a revenue that you wouldn't capture through through for example like simple heuristics that we use that would be quite quite cool totally um i mean i i wonder if scott has any any like uh first first ideas on that no but i i do think that as everybody's saying here the comparison is by far the most important part um i mean if if you're in the dashboard there i mean you can just show like ev every transaction hash is is easily identifiable as the the way that it was categorized and um yeah like you know what we thought how much it made and what protocols we attributed that to and i think that checking against another source that has a different methodology it would be really important these these numbers are definitely not there's no like perfect here because it's it's i think impossible to perfectly analyze every single ethereum transaction without fault um so i think that yeah we just need to continue to drive towards reducing the missed categorizations and i think yeah the only way we can do that is through a couple different approaches and ways of thinking about these things we can try to be less wrong yeah exactly exactly yeah it's it's a really it's a really hard problem but i think just even getting the numbers that are you know in in the right direction or like relative like months or you know what happens during price spikes or gas spikes i i think it's just really eye-opening to just see how much the mvp is spread out and and if it is you know as a seasonal is it's i think there's even if the numbers aren't absolutely correct their relative directions i think are some of the most fascinating bits of data we get out of this yeah i think uh one of the interesting things is so there's a ph i guess not a research scientist in our lab who's developing a paper that requires an archive node right now and our archive node broke like a few months ago and i've been too lazy to fix it so uh i couldn't really provide him one so he's had to go hunting because he really needs this like per opcode data and what he ended up having to do is get access to a service that's run by jpmorgan so i was like it's pretty hilarious that like in the open financial system the only way to like get the data to answer the questions that you need on like how this protocol is executing is to like beg jpmorgan for an api key um so yeah hopefully we don't end up in that future but it's definitely some parts of the system are trending there already i think isn't isn't alchemy uh providing free archive notes she said something about alchemy i don't remember i think so i think a lot of the ones that provide free archive nodes don't let you do per transaction tracing or he had some other or like don't let you do full op code level tracing um like that's what infuria says even though they have archive nodes they claim not to store that data they just prune it for some reason which in my opinion is probably because they're gonna have a paid service to charge money for it uh that's the only reason i can see but anyway i don't know uh i will i will see i will go back through our logs and i'm pretty sure he said he tried out for me but if you didn't i'll suggest it he said he got it from archive node.io eventually i know that second time i heard an archive note crashed or like broke and it takes like months right i mean maybe weeks to to sync it um so he claimed that he's been talking to people on twitter and it might be possible that turbo geth has fixed all these issues but we're testing that now kind of on a separate uh separate note but yeah we'll see um but either way i think no matter what like this data's gonna outpace uh our ability to be perfect about it so like the more heuristics and the more accurate heuristics we can get to the community i think the better cool so one one thing we tried actually for i think it was the garbage not the center sandwich at the heuristic yeah so sandwich attack section 4.1 heuristic five um so we work we basically took like uh um like a magic number of um that the second sandwich transaction the um which is the backgrounding one that it must be within 90 to 110 of the amount bought in the first one and these are just random numbers right i mean we can take out any number out of our hands um so we we also played around with this a bit but we don't really know what what is a good number and what is correct and what is wrong or what is less wrong in a sense so yeah i would always call these results estimates because of that especially cool awesome all right well um kind of moving on slash related from this and general quantification and the the flashbots research process um is a lot of i mean some of these metrics and some of the metrics in arthur's paper are um related to our first research proposal fr one out of metrics are floating um and i i i would given that this this call is kind of collaborative and we're brainstorming together there there are definitely a few metrics there that i i would love to get your your your thoughts on and metrics that are mentioned in uh frp1 this being frp1 let me send the link it needs to be cleaned up a little bit but um purpose of this call yeah so i guess but let's let's go through some of these metrics as they are and i would love for general general comments uh um if you have an idea some of them they're already done some of them would be great to to to do um and you know we have uh people on the skull here that are participating in our research efforts and i think my use some of these metrics it would be great to to to to get more more information from them as well um all right so gas metrics i guess one of the metric we want to have is uh measure the gas saved with perfectly efficient art in the sense where uh the gas that would be spent by the winning transaction when it lands on chain you wouldn't have the same kind of bidding involved in in gas price auction uh and so you the the the gas needed for uh these trades will will generally be be less right or like the gas pen will be less it'll be enough to you know for the compute needed for the logic of of the transaction itself but it won't be iteratively bidding the gas price up so it'd be interesting to to understand that um i think we can calculate that pretty easily so i don't have any kind of strong strong uh question there um the other gas metric is is the gas currently wasted by trade zone chain so in my in my eyes that's the the gas that traders use on reverted any v transactions which we've quantified um in in in here um through to a few servers and this this is going from from from the gas pen um some i wonder actually is aside from reverted mvv transactions are there overturn wasted by uh users on chain can you repeat the question please so oh sorry is that any better now yeah all right so as as part of our effort to understand the negative externalities of current mev extraction techniques and just the activity of mev extraction we want to measure the gas that's corona could waste it on chain by inefficient mev extraction right and so currently the way uh i understand this metric is you know the gas for reverted mev transactions that was spent but it was quote-unquote wasted because these transactions didn't have to live on chain um and so i wonder if if that's all the wasted gas or if there's other types of transactions that could be added to to this metric i think that's definitely yeah go ahead sorry there are transact there are the cancellation transactions but i guess they are tougher too to detect they are not necessarily reverted they are just they pay to sell for something like that but they wouldn't be sent unless they were trying to cancel out uh some mvp transactions that they figured won't be profitable or was already the media was already extracted or uh you know it's too risky for them to try so they cancel it um there's also the the fact that enemy a flashback transaction will be um will save gas in the actual execution of the winning transaction since it no longer needs to check chain state since it executes to the top of the block it doesn't need to you know if you're arbiting let's say uniswap against uniswap if you submitted it as a standard broadcast transaction you would need to like go check the reserves and go do a bunch of math on it but because you have a perfect knowledge of what that chain state is going to look like when you run you don't need to perform those read calls to protect against landing improperly yeah and i think that's also the case if the miner takes the arb or in any situation where they're partnering uh in like a trusted context with the bot exactly yeah you know you don't need to say like hey is this you know is this position still available for liquidation you just liquidate it because the fact that you're running means that it's there versus so if you're bidding you know 100 000 way you probably want to check first before you start doing a bunch of work it's going to eventually revert totally i think that's that's a great point for and it's it's it's more for that first bullet point right uh but generally it'd be in in terms of like general market um efficiency if you know there's a significant gain in in efficiency by having arbs running through such a system uh that'd be super interesting to to to point out and if there's a lot of uh savings from from the user's perspective in terms of gas they spend um and you know the the related um network savings i think that's that's also uh super interesting alex just tell you i muted yourself all right [Laughter] all right moving on um the last kind of gas metric that i think would be really interesting to look at again to further our goal of quantifying the negative externalities of current mv extraction techniques and understanding if something like flashbacks i mean some of the numbers behind flashbacks um i think it would be trying to quantify the the upwards pressure on gas prices from bot operator activity i think that's uh maybe a little harder than the two ones above but generally understanding how have gas prices been historically influenced by a trading activity that could have been completely uh running in a separate channel and not not in the kind of quote-unquote public um ethereum uh and potentially graph the gas price over time versus what the gas price could have been in it with a system like like like flashbots in place um but generally understand i i guess that's maybe even qualifying either harm to some extent when in periods of high volatility of ethereum price um you you do have you know extremely high gas prices that make the network unusable for for for a lot of users um so i'm not entirely sure how to quantify that because you know gas prices uh i mean you would have to look at the mempool as well to some extent and understanding the impact of different transactions on gas prices but also kind of make the assumption of if these transactions didn't exist what would gas price look at so maybe just taking the rest of the mempool and averaging it that way but it does sound like uh mempodena maple data would be relevant here again i think another interesting metric might be like the throughput loss uh of each protocol so like what percent of each protocol's gas is arb mev related activity and what percent is uh users that are using it over the longer term um what did you sorry what do you mean by throughput laws where i guess through grouping penalty right like uh i don't know like uh let's say unislop uses like 40 percent of its uh you know gas usage on on just bots that are extracting mev then i guess that kind of implies that like if uniswap as a whole system were to pay the same gas prices for operation on ethereum um and it had a more it had a different structure that uh had less of a percentage of that going to bots um then would the users get more throughput that way um in like this kind of competition for resources scenario i see there's also some like deeper research questions here about like so like if you look at market fairness and like traditional uh like continuous markets uh people look at like the the negative externalities imposed on liquidity providers by like snipers and hft traders um but in that setting there's not really this like gas market or like there's not as limited of a resource people are competing for so i wonder how these systems are different from a fairness point of view sorry i'm writing it down that's a great question um but like a deeper one in the sense of of metrics and um quantification um right cool um maybe moving on to the network metrics and that uh people people have comments on on all those um i definitely would like to be able to measure the the ptp network overhead from from back running uh um and the overhead you know for full no the overhead for a minor and similarly wanting to measure the pcb nano cover head from front running as well i think it's part of quantifying the negative externalities uh on ethereum of this type of activity um one concern that i have with it is uh you know the the gossip topology of the ethereum network is not a uniform and uh it might be hard i mean for each transaction depending on how you know the the the person sitting in the transaction how well appeared they are the network overhead might be different in some way so it feels like either you have to you know apply a lot of simplification to it um or maybe there's maybe there's a way to be rigorous about it given given this like extra layer of complexity i i i don't have a clear um vision on on that i would love to to get comments on that actually yeah i mean i think one one possible way is to like just run a node and kind of classify things into mev explorer in real time and measure like what percentage of messages on the node relate to our versus non-arb transactions um we would also probably have to classify things that don't go into the chain in that case because probably a lot of the overhead on nodes is like transactions that never get mined so that could be another right like how much does each node waste on transactions that never get mined and how many of those transactions are like also from addresses that uh do arbs on chain yeah and i mean to to get to your point there's also just in the front running case i think for me the network overhead you want to measure is you know how much of an overhead are these uh gas replacement transactions uh given that you know sometimes you have a lot for a specific opportunity um yeah interesting and but how would you measure the the quote-unquote waste from the from the node as in you would would you just measure its its capacity in some way and see how much of that is dedicated to i'm not sure actually fairway yeah what percent of the capacity that's being used is dedicated to this maybe uh i mean i think that the challenge is detecting them in real time or maybe recording them somehow for a later inspection but maybe a just using gas as a proxy like how many taking their gas cost as if they were computed and yeah something like that [Music] yeah i haven't looked at the maybe we should maybe someone else on this code knows uh or someone else on this call knows i haven't looked at the data for like uh spam back running personally on like the node level so i wonder if there's like an obvious heuristic there but the pga one is is pretty easy which is that like if you see a replacement transaction and the gas price is much higher than the average gas price in the block um that's almost definitely a pga so um yeah we can we can probably come up with other broad strokes heuristics that are pretty good for back running and spam also that sounds good that sounds good that sounds exciting to to measure i haven't i haven't seen a lot of uh measures of that kind of network overhead uh maybe because it's really hard to to have like good good measures for that type of stuff but generally quantifying the impact on the pvp network itself really really actually as we expect the fight to grow uh its activity to proliferate even further if you know if you don't have uh solutions that that come online like like flashbacks or or arch without um we might get to scenarios in the future where uh it's it's very clogged in in some way or the the impact is actually quite important so at least yeah quantifying it and getting more understanding around that um feels important uh and i think is that gonna lead to the next question right sorry phil i was just gonna say i think it's like super important for users and i just think the only reason it hasn't happened yet is because no one's done it and we we should definitely do it um maybe this could even be part of paper to the ethical side of things um quantifying users or harms of just like systematically enumerating all the resources that uh an arbitrage bot uses that uh it's not paying for directly and then coming up with metrics for each of these so pretty much what we're doing here it's not that hard either and i mean for the private transactions we we recorded the network layer and um [Music] i mean okay there are ways of optimizing also the data structure and how you store it but this was quite a substantial data we collected and we also um we collected not only the first peer that sent us the data but we recorded the first six ip addresses that send us the data um because i mean this this could also be interesting for for later usage um for example for privacy gamingization and identification and so on but um this is i mean okay it depends also a bit on how many connections you build up right how many network connections you build on the pdp network so i i could imagine that those that do adversarial front running back running its own they will have more connections and therefore they will also negatively affect the network um meaning they will make other people work more because they kind of request more right so there's also the impact of of kind of i mean we call them spy notes but or like spider nose because they have like like a web of connections right um but this might also be something to consider here yeah we we ran like uh six or seven nodes like that for the flashbots original kind of paper and i think we ended up spending like tens of like tens of thousands a month in aws bandwidth uh which means that since we were probably mostly peered with aws nodes that on the other side of it like 255 people per node spent like 10 000 over 255 uh of bandwidth sending stuff to us on average so uh yeah i think you can probably even quantify the the cost at least for cloud nodes um the problem is because everybody's on aws you have to be on aws it's always you too slow right that's the irony of it yeah exactly um since we have uh 10 minutes i'm gonna jump to the kind of some of the other metrics um but thank you arthur that's that's super helpful i wrote down uh what did you mention um yeah so in the in terms of bot metrics uh definitely want to measure bot activity in a more uh granular way like has been done in arthur's paper uh so we want to break down uh the realized mvp by protocol and by strategy that's kind of something we we were doing with explore right very nicely but but still uh still uh uh doing so um and the two other bot metrics that i think are interesting is one is now looking at it not from a general ethereum user perspective but from a bar operator perspective uh it it'd be interesting to look at the the efficiency of bots so uh how often do they fail um or like burn gas versus succeeding um this is this is and i guess it's another perspective on on looking at wasted gas and that type of stuff and understanding if there's a way to make the market structure more more efficient right so it goes back to that first metric of a measure gas save with perfectly efficient um art think looking at that would be super uh interesting and i can think of you know we we have known bought addresses and we can look at their uh have failed and i think we uh have some heuristics there about uh how we can use those to arrive at some measure of efficiency um so naively i think this is potentially something you could do there um and then related to biometrics it's also just understanding the breadth of the of of our analysis um that's particularly hard in the sense where like it's it's it's not static right as more protocols come on and sometimes as they tweak a parameter that opens up a strategy and there's a new bar spun up to to do that um but generally it'd be i you know it's related to what i said before about having some extended error error metric as well uh uh to to to understand what kind of boundaries um we were looking at when we're measuring these things and the percentage of coverage um so we we kind of did something that phil mentioned uh there currently where we we we cross reference some of the data we're running and the addresses we have database with um you know the top 100 addresses that are using gas tokens and we try to see if you know with the assumption that most of these addresses must be uh bot operators and then we try to see if those have been quantified if they if they're not in our database we kind of look at them manually and see if there's anything wrong with them um or not if there's anything wrong with them but if if they should be added to our database in some way so if we should code up an inspector to include the that type of strategy that's the way we're kind of currently naively approaching this um and i would love to think of more advanced ways of looking at um the the basically coverage metrics in general types of coverage um yeah and lastly seven minutes left um kind of minor minor metrics i think is is particularly interesting i think these these two first the the minor latency um is is is a little bit hard to i both know flashbots i mean there's no minor latency i guess in that case given the given the system and with with saturation i think miners can get um you know overwhelmed by by transactions uh and in that case i'm not sure how the infrastructure handles that it might be a case-by-case basis so that this is maybe less relevant as of today um this is also maybe less of a metric question i think one thing that i'm particularly interested in and and is more particular to flashbacks is trying to understand um in a world where flashbacks is adopted what do minor revenues look like um and while it is you know thinking about flashbacks and maybe less of a wider metric i think it's important in the sense where you're trying to understand trying to understand minor revenue it's a proxy to understand also network security to some extent right like miners need to be incentivized to to chrono code secure the ethereum network and contribute to consensus and so there are like several competing forces within them so if you know in a world of flashbacks is widely adopted gas prices are less high and so miners make less money from that but at the same time the blocks and block spaces is freed up because you don't have these reverted transactions as much anymore so there's new activity from that at the same time the ethereum network is more usable because you don't have these like uh periods of high uh gas price that make it unusable for smaller users um and at the same time you have some some type of increased mvp activity through a system like like like flashbots because of the the no loss situation you're in and also because of the seal data auction where people will be incentivized to bid as as as much as they can while still being profitable um so there's these kind of different different things to think about there would be interesting to have a rigorous analysis of this uh and arrive at uh maybe some it sounds like it might be a scenario analysis to some extent uh given different levels of adoption um and also given different level of what the if your network is used for and that type of these type of things um but i thought that was a cool question to also look at for frp1 and yeah so if you guys have comments on that one and if not that that's kind of uh going over some of the metrics and things that i uh look at and have been listed on uh flashbacks first research proposal and you know as as you can see in the side question here arthur uh our data collection notes well maybe not combined without you anything running as we were talking about before but definitely integrate the data in some way um or or uh generally contribute to to each other's efforts um i think that would be super cool so both for this for this research paper but then more broadly i definitely want to make sure uh the flashbacks organization anyone from the organization on on on the research front et cetera can contribute from the data we're collecting so make sure uh there's a way for for people to interact with it contribute to it and use it for their own research i'm good thanks alex i think the one thing i would mention from my end that brainstorming might be interesting is like also per protocol metrics of uh externalities and congestions it might be a way to like force to adapt developers to adopt good design so showing like when you move your oracle to this new design it decreases the amount of failed transactions and you're you know uh the amount your your griefing factor on the network or whatever uh goes down in terms of mmev so just like creating a metric for like how bad of a citizen each protocol is uh in terms of the mev it exposes and how problematic it is in that in like the different types of exploitations um i think would be cool i love that thank you that'd be great for paper too as well right well i think we're near the end of the call but uh definitely anyone who wants to stay um feel free to stay and uh chat more um i think one one thing is um i want to ask uh two questions i want to ask the general uh well folks here who are interested in research um first is there any rust developers here um we would love to add to the mvb inspect program as uh much as possible as soon as possible um and so uh if anyone here um uh you know uh you're a rust fan or you um well it's never uh you know too late to get your hands on it right as some folks who uh wrote this um you know in stack and rust that believe is the superior language uh in all way shape or form there was a tweet about it earlier today so um yeah so um i think any um meaningful contribution or attempts will be super interesting um and also i think it would be a really cool crash course kind of on for folks who's uh relatively new to uh deep arbitrage and in this world to be able to get your hands on it so yeah and if you know any great uh uh rust developer in d5 uh feel free to uh bring them to our discord the second question i have here is um i think for our next rows and as well as research workshop now that we alternate on thursdays we have a couple of major topics we want to kind of go deep dive into similar to today um will be um actually one is the auction mechanism uh where um i think we have so far uh flashbacks alpha being live um the somewhat still bit nature of the auction already starts to invite questions about uh from the uh mdv searchers these days um that there are a lot of challenges on the merits of seal bit auctions uh phil you may have missed some of the fun interesting feedback we've gotten from the uh you know from the mev searchers uh that are live uh at the moment and bot runners a lot of questions on the merits of a seal bid auction and their tendency to want to get an edge and to get as much information as possible so that's uh that's um actually one of the things i pinged um soraya about um that was super interesting from an empirical standpoint um some of the more nuanced questions alex was um communicating a lot more interviewing and serving some of the bot runners so yeah exactly they want more information so there's always a tendency to want to reject a system where they can't have an edge and go to places where they can have a have an edge and also that in in the world where adoption is a gradual facing and there are multiple options there so they essentially place bits all over the place on pga uh in other network and trying to collude with mining uh miners and mining pools um that is not yet uh uh using flashbacks and also uh placing bids on flashbacks while also trying to uh see if they could uh collude with um um mining pools in getting more information um about um what are the like how like uh what are the price uh like the pricing of the bits uh are on flashbots um because at the current moment there is a um there is a essentially a more loose trust assumption there um and uh my miners among pools at the moment can uh still see these um information so it's a very various kind of interesting information or like active behaviors that we're observing um already uh within the couple weeks of flash fast alpha being uh live so those are things that are interesting to kind of um think a bit uh more and that's also related to kind of the last frp one uh sets of metrics um in terms of like what would be the world like with uh without flashbacks and with flashbacks and the likes of the system so um i think in general like um auction mechanism literature survey auction mechanism design um and uh examine some of the behaviors we're observing and feedback from the uh the the players in the ecosystem i think could constitute a entire session that we would love to dive deeper into um and another session that we would like to host is uh on uh privacy um essentially um i think there are multiple uh there are various um approaches but on ethereum potentially the the options are limited in terms of achieving full privacy and namely the original flashback spec was um uh specifically uh suggesting uh sgx research and phil can share a bit more details into it so i think um uh we would also love to host another session that's in depth about um essentially transaction privacy as well as um the various available solutions out there out of the box and kind of theoretically what would be the ideal approach um given the uh different architecture so yeah so those are the sessions that i could think of and i would love to kind of hear more from you guys in terms of um which directions um or in the next uh you know weeks uh beyond we would love to um you would love to kind of attend and and be part of and also any other kind of chunk of questions that you would love to kind of dive deeper uh within uh with a more in-depth session like this yeah so those are um essentially open questions for us all here and um um uh i wonder whether um essentially um uh saraya you have anything you would like to share and post as questions for our future sessions for the folks here since you are leading the church on um the survey of the auction mechanism yeah so it seems to me that at least um in serving auction literature that that the any that any idea of sort of collusion or cooperative you know strategy is kind of ignored altogether in almost every um in almost every sort of auction marketplace and the reason is that they really only care about strategy-proofness and then they rely on i think both the the legal aspect of it as well as um as well as sort of the competitive nature of it always incentivizing someone to you know break from any cooperative strategy and so i think that you know the the ad auction literature kind of uh completely falls short of any consideration of a conclusion or you know like even small conclusive sets i think they just completely ignore it all together and so we may have to sort of engineer that theory on our own which i think though is is probably going to be along the lines of uh along the lines of uh pre-existing sort of theoretical actions like this hector marsh auction i just brought up and sort of just playing around with revealing enough information to make like you're saying mv searchers happy get still getting some you know at least close to truthful bidding i think that's super interesting it might help the transition for searchers because definitely a lot of them especially the very successful one a lot of their edge is in being really good at bidding and like that that uh gas auction that is on ethereum and so asking them hey just forget about all this this edge that you have and like come to our system where like you know you're you're running like you know in a sealed auction and like you don't really know how much you should bet um yeah you're right you're right um all right thank you guys um great thank you all for for joining me and for listening thank you arthur kaiwa for presenting your paper i'm definitely going to be reading even more uh carefully i'm still going through it i'll be sending you more more comments separately awesome yeah thank you so much guys for presenting and thanks everyone for coming and thanks alex also for all the great discussion on uh metrics and explore all right um and also uh ping me on discord or like uh leave the note on github i think a lot of you guys have written great posts on uh on on uh on this uh subject matter and have been especially alex um alex um the other alex um that we would love to see how we could uh dig further as well as some of the other researchers here just you know get up or discord whatever just uh leave open an issue or leave a note be cool thanks guys see you next time all right bye bye thank you have a good day you too bye guys you 