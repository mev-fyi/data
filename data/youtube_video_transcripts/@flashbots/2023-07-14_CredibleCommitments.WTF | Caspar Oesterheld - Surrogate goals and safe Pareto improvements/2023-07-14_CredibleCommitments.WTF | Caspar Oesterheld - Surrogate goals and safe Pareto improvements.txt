great um yeah uh hi everyone and uh thanks for letting me present remotely here so I'll present on um surrogate codes and save Creator improvements uh just quickly can can you see like the whole screen is there some issue if I move around the because because I have like I have like small Zoom windows in front of stuff but I guess you can see the whole slide and everything right yeah yeah good okay just uh making sure uh okay yeah my name is uh Casper osterheld um and the the work I'm presenting is kind of joint work with uh with some people people like the papers with Vince Conner also worked um on this stuff with with uh some others and I want to in particular highlight uh Nathaniel uh and Anthony because they're actually in in London so uh if later over dinner or the like you want to discuss this with um these kind of topics uh I think they'd be uh great to talk to I think Anthony will get the talk right after this one on on some similar idiots maybe Nathaniel maybe uh I guess maybe you're I mean it's a small group probably all no uh each of those things that was plunged but uh very nothing you can raise your hand or something um okay so okay so here's the plan I think we have an hour uh in the schedule and I have uh slides for uh 20 to 30 minutes um and uh yeah the idea is for you to ask questions uh during the during the presentations if you haven't uh if you have any um and then we can use the remaining time for for discussion and kind of brainstorming so the the presentation will mostly not be talking about any kind of cryptography ideas uh even though yeah I actually like currently with Nathaniel working on a project that does use kind of cryptography to like Implement all of these ideas but I'll mostly keep it out of the presentation part and maybe you can discuss it uh during the brainstorming okay uh so the general the general motivation for this work is that um uh is that sometimes in the real world uh there's kind of some kind of bargaining going on and people make uh kind of yeah conflicting demands for some um some Goods like I don't know countries making uh uh conflicting demands about uh pieces of land or something and uh and then if uh if the demands are inconsistent then normally what happens is uh some kind of conflict or at least that's one of the things that can happen it's like some some kind of conflict um in which utility is basically burned so um like players kind of actively pay to hurt the other side like uh like you think can think of War as uh as being being like this um at least some kind of walls I guess okay um okay so here here's an example of a normal form gain uh where something like this could happen so for now uh ignore this beasts kind of eliminated parts of the game so those are actually that can be eliminated by strict dominance so we would not expect them to be paid played um yeah just ignore them for now they'll they'll become relevant later uh so we basically have this uh two by two game uh in the top uh and it's it's kind of like a game of chicken uh the two players can make uh they can make demands which this is like this DM which is kind of like I demand this item or something like that and the problem is it's uh post amount item then well that's that's conflict so so the story in the paper is that it is exactly this kind of um territory that's that two two countries both uh have demands to NX okay and there are multiple Nash equilibria and uh yeah so that's this Nash equilibrium selection problem uh gets similar to a game of chicken and we might imagine that in the real world sometimes they won't be able to kind of yeah you might say coordinate on the same natural equilibrium or they might disagree on what the appropriate National equilibrium uh is one of them might think that they like they might both think that they kind of rightfully that the thing that they're um bargaining over kind of should belong to them and so they might sometimes it might happen that they both play uh Tien and uh that's conflict occurs okay now let's imagine that this game is not played um kind of directly by the players but it's played via delegation so the um let's let's say like both kind of original players or principles are humans and they kind of recruit they each recruit a robot or can more more generally some kind of Representative to play the game on their behalf uh and yeah we we might imagine that um one way they can do this is like to write some algorithm or like I mean if it's just a simple game like this they could just explicitly specify what they want to happen uh what what they want their representative to play in the game so they might they could just explicitly say Okay played DM or play RM okay if they do that then of course doesn't really change that much about the game but now um the first we're going to make kind of two two kind of additional assumptions the first is that they can make they can make some kind of joint uh credible commitments in how they instruct their representatives uh so so one version of this is this kind of program equilibrium idea but you can also imagine that it's kind of more like mediators or there's like some joint contract that they both sign which says they're going to tell their representative certain things okay so this alone still doesn't really solve this game um because it's still there's still many equilibria they might still disagree about uh what should happen uh in the uh what should happen in the game they might still like each of them might still say okay well I want I want the Dr dmrn outcome and then this this bilateral like both being able to make these Global commitments doesn't really really help um okay but now an extra assumption that we're going to make is that the representatives are actually kind of they have some intrinsic uh intelligence or abilities uh and in particular they're kind of able to they're able to play games really well so instead of telling them what action to take or giving them some algorithms we assume that the humans can just tell them to play the game uh in whatever way they think is reasonable and in fact we are going to assume that they are kind of so good or like the humans trust them so much to do this to do this well that kind of by default if there's no kind of if they aren't able to make if they weren't able to make credible commitments or if they weren't able to uh if they're kind of unable to agree on some uh some some joints uh bilateral commitment then the default is for them to just do that so just say okay just play the game as like with these utilities uh and as best as you can okay even so like the equilibrium selection problem remains uh if they if they if they do this Alliance delegation uh if they tell the tell the robots you know here's my agility function do what do your best the robots might still uh they might still end up in in the dmdm outcome okay now now comes the the idea for how to uh decrease uh decrease the or like basically eliminate the loss of potentially uh playing this top left um this top left dmdm outcome uh just quick question can I see okay see my uh my cursor probably probably not right you can okay can you see it reasonably well so like if my point of things yeah yeah inside okay uh so so this is kind of the the bad outcome right okay and now uh now these kind of other parts of the game they become relevant so again like like normally by default we expect that only this top left two by two quadrant would be played okay but now uh the area is calling the two original players uh give the following instruction so first they tell them to not play DM and RM they both they both they both tell us to their representative and all of this like conditional on the other one until like giving the analogous instruction so they tell them to only play DL and RL which ones uh originally uh relevance uh to how the game is played um and uh now these TL and RL basically seen as kind of kind of harmless versions of DM and RM uh so so in particular DL is kind of like the m is kind of like demanding the thing but then in case of dldl kind of this there's no conflict there's just kind of nothing happens or something like that um and the idea is that the original players uh tell the representatives to Value this dldl outcome uh in the same way as they would value the dmdm outcome so minus five minus five so these blue numbers are the utilities that the original Tales tell the representatives they should assign to outcomes in the game okay so this is now minus five minus five uh and and and these are they kept us in the original game which which happens to be the same as as this part of the game okay now why might one do this so by default so the left is now the game that effectively would have been played by default and the right is the game that is now going to be played with the blue utilities being the utilities that the representative are supposed to assign to the outcomes and the the black utilities are still the the ones that the original players the humans um assigned to the outcomes uh and now the the point is that the the blue utilities game on the right hand side is isomorphic to the uh to the to this original game that would have been played by default so we might imagine that like plausibly the uh this right hand game is going to be played kind of in an analogous way uh to the left hand gain so this this is this game is going to be played in the same way as kind of the game would have been played if it wasn't for this possibility of making commitments um and uh the important difference between the two games is that one of the outcomes is better for both players than in the original game so uh if the MDM had been played in the original game then in this new way of playing the game we would expect the LDL to be played which of course like to the representatives that's kind of like they don't so then that's the same but to the uh principles to the original players this one one outcome is much better than the minus five minus five outcome so it indicates that this conflict outcome occurs in the original game it is better for the representative for for both of them if this uh new game is played where this harmless outcome occurs okay and the remaining outcomes uh like if we expect that like if in the original game dmrm had been played then we might actually expect TL RL to to be to be played in the new game um uh and and that has the same utility uh for for both players and that's the case for the other ones as well uh and that is basically uh an example of uh of a safe Pareto improvements which is a um uh which which is kind of like a an instruction to the representatives to play a game in a new way such that it's kind of guaranteed to be Pareto better for the original players even in the face of equilibrium selection and so on so the the important thing is that the the the original players like to improve on the default the original players don't have to resolve in any way the equilibrium selection problem uh in this uh in this original game okay I'm now going to move to move to a second example maybe maybe people have have questions before I get to the second example so I still don't get why why you the players wouldn't would do that because they you know want um I mean those actions are dominated right so why why would they give these instructions to to uh nominated actions or dominant strategies uh so they only yeah they that's a good question um they only do this kind of conditional on the other player giving DNA giving like the analogous instructions like one player wouldn't ever unilaterally say don't play DM RM play DLR that would definitely be be bad um I guess one like one like one I guess one kind of very simple case where this where you would tell your representative um to play a dominated action conditional on other people making analogous commitments will be the prisoner's dilemma right and the previous dilemma cooperate is strictly dominated but if you uh if you tell your representative to um or if you kind of write I don't know write a com make a commitment to like play corporate if the other player makes an analogous commitment that's uh that's reasonable right and in some sense what's going on here at least with respect to the um to this kind of commitment to only play these dominated actions in some sense that is um analogous to this commitment in the prisoner is that does that make sense okay um I'll continue them okay so here's a second example which is um uh which is which is actually older than the the first example um and yeah people sometimes call this a surrogate goal like the the example that I'm not going to give or it's written and neutral surrogate goal um and yeah I view it as kind of uh um a special case of a unilateral safe Pareto Improvement um where unilateral just means that only one player has to make commitments okay um okay so here we have a blackmail game so we have two we have two agents uh the write agent has this item the star and the Leicester agents once the star uh and the only thing that the left agent can do is uh is that she has this kind of lightning thing which represents um something bad that you can do for player two so she can hurt player 2 in some way um and so if she wants to get the item the natural thing for her to try is to uh to Blackmail player two uh into handing over the items so she could say um I'm going to Blackmail you if you don't give me the star I'll do this lightning thing uh against you okay and of course this has to be credible it should just like says this then uh there's no let's just cheat talk right so that's somehow this has to be his credible commitment or at least uh that has to be some kind of credibility um and then also uh we assume that this lightning like uh using this lightning against player 2 is not something that she intrinsically wants to do it's something that she uh otherwise wouldn't want to do okay uh and now if player one decides to make this threat player two can decide whether to hand over the uh the star um in which case um well he's unhappy because he lost the star but I mean at least no no threat was carried out and player two may also decide to not get into the uh to this uh blackmail attempt and then uh then kind of everyone is unhappy because player one doesn't get the items and she has to execute the threat uh which she didn't want to do and player two okay uh kept the item but the threat was carried out which is was like have to be like I mean for the threat to make sense has to be worse than uh than losing the item so this is kind of like again this kind of utility being burned uh outcome as a result of people two people making conflicting demands for the star so here like player one once this kind of insists on getting that Stark player two also insists on getting the star and the the result is that utility is being uh being burned okay uh and here this is us uh given us a normal form game so this game is it's a bit weird because it's because giving in conditional on being threatened is uh weekly dominates in this game um the strategy of um of not giving in uh someone has to when one has to tell some kind of story for why uh why player 2 would ever not give in to threats uh or to blackmailing scenario um and I yeah I mean there are lots of there are lots of stories that one can imagine here like reputation or um or maybe uh player 2 can kind of commit beforehand um and there's some chance that player one will see whether player 2 has committed beforehand um or the like um in any case it seems like kind of just giving in to threats whenever they're made against you is kind of like eight agents that behave this way um probably end up not um having all that many resources um okay but I'm I'm not going to kind of model this I'm just going to assume that sometimes this uh this kind of weak equilibrium of uh in which player two doesn't give in that this sometimes can happen okay now here's the proposed the proposed solution for this so again we have this gain but now we imagine that player two and and only player two this time plays this game through a representative so this time uh they're all smiley faces so I guess they're all humans um and what basically what player 2 does is player two kind of hands over the the star to um to his representatives and representative and kind of says like yeah you have to manage this now and again player two can give some kind of Destruction instructions for uh for what to do and uh one might imagine that kind of by default like if if there was like no credible Commitment if yeah credible commitment wasn't possible then we might imagine that uh we would have this default of a line delegation which is just like player two telling his Representatives like here's how much I like the star uh here's how much I dislike um getting hit by uh lightning uh and it's like do do the best that you can uh given this information for me okay now of course this now if if this uh if player 2 can credibly give instructions to player to to his representative then there's another thing that she that is very natural for her for him to do which is to say just never give in the threats um and that's and then it kind of becomes like a stackable game uh where yeah but player two kind of uh wins by being the first to to choose and it's kind of it's kind of the the in some ways kind of the opposite of the or like the the other direction of the um or like the other ordering I mean just just uh two minutes ago I talked about how um um maybe for player two it makes sense to always give into threats like similarly maybe for player one it then always it makes sense then always um not make threats if player two commits first um but I think again no one can make yeah one can make all of these or like come up with these kind of different uh real world stories called why uh why this doesn't work and it seems like in the real world um just like someone committing committing first doesn't uh doesn't always resolve [Music] um these kind of disputes okay um okay but now so it what what if player 2 doesn't want to just give this like never get in uh instructions what what if player 2 wants to do something kind of uh soft that maybe player one wouldn't mind that much okay so here's the idea the idea is that player one can't doesn't only have the scraps that she can make against player one uh she also has some other thing that she can do that player one doesn't actually care about um but that is equally costly to her as the original threat so like if I don't know if the original practice um throwing paint at player one's house then maybe the new threat could be uh throwing paint That's player one's neighbor's house or something like that so player two's neighbors house um which I guess is tough for player choose neighbors neighbor but maybe player two didn't care about okay and now the idea is that player 2 could tell his representative to uh to not so so there are different versions of this but one version is like uh player two tells his representative to not care about the original threats um but care about this new threats the player one doesn't actually care about um and again this we assume that this is credible and then given that this is named that that this instruction has given uh player one will oh like player one can again decide whether to Blackmail or not but player one will now for player one it now make makes only really makes sense to threaten this new type with this new thread Target but player one doesn't actually care about so uh so what might happen uh so player one could yeah could could could blackmail and then well it could be that player 2's representative decides to give in to the threat because player 2's representative actually is told to care about this new kind of threat uh so then the uh the Stars transferred and then basically everything is asking for but it could also be that player 2's Representatives uh designed to resist then player 2's representative is kind of hit with this uh uh with this new uh type of lightning this new threat uh and now for for player one this is just as in the original game for player two this is also just that oh sorry for player choose representative this is also just like in the original game but for player two the original player 2 this is this is now fine because uh the original player 2 doesn't actually care about this lightning uh okay and yeah we can again um yeah we can look at this in the payoff Matrix um we can look at how the game is played by default then we can look at how this game is played with this instruction so the yeah the utilities need to kind of be exactly isomorphic to the utilities in the original game uh and then uh we again get this yeah the safe pressure improvements that um if we assume that the games are played isomorphically it's guaranteed like this new game is guaranteed to be at least as good for both players as the uh original game uh and there's kind of this this one outcome see if we can quickly find it I guess this outcome yeah the outcome where the thread is made and player 2 doesn't dip in uh player two's representative doesn't give in in that outcome um player two the original player 2 is now much better off than in the original game okay and then so I mean basically I've just given examples right so um yeah there's this this paper which gives um this kind of defines what a what a safe pressure improvements is um and um yeah shows how how to like like find these in general and gives like complexity results and these kind of things um okay so now kind of moving a bit towards the like more discussiony part um in my mind the the biggest obstacle to implementing safe pressure improvements or surrogate goals is that uh one has to make one has to make these these instructions credible um and there are in fact two things that one has to make credible so the first thing is that um sorry like in the surrogate goal language like one has the the player 2 has to make it credible that player choose representative actually cares about the new kind of threat um so yeah that has to be like player one has to kind of really believe that player 2's representative actually cares about the new this new threat about as much as say two's representative cares about the um sorry about as much as player two cares about the original threat and um also you have to make it credible that you didn't then kind of uh mess with the um with their behavior their representatives behavior in in other ways so if you so so one thing that you might try to do is you you tell your representative okay you like care about this um this the surrogate goal you make this credible but then also kind of sneakily you kind of tell them like oh by the way like don't give in as much um and then it's not um I mean maybe that's like a sensible strategy but then uh if that's a possibility then it's not it's not a safe parietal Improvement anymore because now it's it is kind of a strategy for like for example in the in this partner again it is a strategy for player two to kind of screw over player one uh to make player one worse off than in the in the game that would have been played by by default uh and this seems relatively difficult I think like I think it's a relatively difficult difficult kind of credible commitment because it's not just about what action you take in the end but it's kind of it's also like about the process um at which you uh arrive at these uh actions or about maybe it's about counter factual it's like what would have happened in the original game uh so it's so like in the end if if we observe that for example in the blackmail game player one makes a threat and then player two's representative doesn't give in we can't tell whether that's whether everything was done reasonably or whether there was some some funny thing going on where player two told her uh his representative to navigate him okay um okay I think I want to just uh very briefly give one of these kind of stories for how this could work in the real world um so I think if uh and and it's like not not using uh cryptography not leaving that to the to the discussion um so let's say that let's say that I get fired um and there's this kind of typical kind of dispute between uh my um now former employer and me about I don't know what my severance package has to look like uh and things like that so I hire a lawyer my employer hires a lawyer and now these lawyers kind of um argue with each other about about what should happen um I I don't know anything about law by the way so so it's like a uh it's very naive uh view of what happens probably but um but that's kind of how I imagine this happened leads even have these lawyers and they they uh have these debates and now uh let's say that my employer could use various kinds of threats to get my lawyer to accept a lower severance package so for example my employer could say um well if you don't settle for this low severance package we're going to put on our website uh a page that says that that Casper is very unreasonable it's awesome thing like that um and they have no interest in doing this the only reason why they might do this is to force me into accepting a lower or false my law or get my lawyer to accept a lower uh severance package uh and if that in the end actually happens um then kind of that would be like burning burning utility uh again um and now one thing that I might try to do is put in my contract with my lawyer uh um incentives for my lawyer for like I I mean I'm pretty sure that this is not not exactly how it works in the real world but in principle I could the way I could uh set up a contract with my with my lawyers that I say uh you get paid in precaution to the to how valuable the outcome of this negotiation is for me uh and now if I make this contract public or if I if I show it to the university if I make it transparent to the university what my lawyers incentives are then I could use a surrogate goal for my lawyer so for example I could I could say that like I could put in the contract that my lawyer gets a low pay uh not if the university puts well my my former employer which I guess in my case my employer as a university uh if if the employer puts puts up a page that says that I'm unreasonable um If instead I I could tell my uh instead I could tell tell my lawyer to that that I'm not going to pay her if or pay her as much if the the university puts up some random page that just I don't know contains like some random ones and zeros because then essentially the the university could uh use that threat that is completely unimportant to me in the negotiation with my uh with my lawyer okay so that's uh that's one um that's one story I think I'll to keep it short I'll I'll skip the other ones uh and uh yeah thanks for for listening and I hope you have some interesting discussion and brainstorming foreign yeah that would be that would be good okay and you you can indeed see the audience right okay um oh yeah yeah yeah nice okay yeah I want to start off with a question so you mentioned like you we need to make sure that there are no further secret contracts and so is it possible for or have you thought about uh the uh does there exist some ways for us to make a agent to commit to lose the ability of further uh commitments so if the agent is able to make such a commitment then you know it does it cannot enter a further secret contract yeah uh that's a good question um yeah I would have I would imagine that this is very dependent on the the exact application so for example in this lawyer story well I mean I don't I don't really know that much about law but I would have I would like I don't know kind of I would assume that it's possible for a contract to specify that no further payments are going to be made between the parties or something like that um in yeah I think in the case of AI it's very tricky because so yeah so another like I guess the application that I usually think about is we built uh we built AI and uh we install some surrogate goal in it so that if people uh want to Blackmail the AI or like have some conflict with the AI the surrogate goal is targeted rather than our real goals and there yeah one has to like they're all of these different ways in which we can kind of sneakily tell the AI to do some other things and one has to make it credible that one doesn't do that one doesn't use any of these ways uh to yeah to tell the AI to foreign what are some examples of telling AIS to sneakily you know yeah so I think the I mean the simplest is just to uh train the AI a bit differently um like if you have some if you change some ml like if you use some some ml techniques um to to train your system on like to yeah behave reasonably in uh bargaining situations or something like that then you could if you know that a Thorogood goal is going to be used right you will want it you'll you'll want to train the AI in a way that it that is less likely to be a bin and there might be there might be relatively kind of yeah there might be various ways in which you might uh be able to to train the AI to to be less likely to give in like you just you just train uh change the way that the that the system is trained or something like that in such a way that it is more likely to learn to uh not given um and yeah I don't I would imagine that many of these waves are difficult to detect like even if um even if the even if the opponent could look at the source code of my training scheme it might be hard for them to tell whether this training scheme is set up specifically to make it less likely that the resulting model is going to give in let's see any any other work like the Assumption was said see uh the person delegating the play to the Asian playing the SPI like trust the agent to you know play it similar to the what's a human or like you know the delegator would have played the game uh in the you know in the isomorphic whatever like case yeah oh that's yeah all that um like even like I I guess I think the the the way the paper is written is that the human kind of has no idea how to play the game like the human sees this this uh equilibrium selection like the original player the human uh in the in the slides see if this equilibrium selection problem and says like okay I have no idea what what is supposed to be done here uh and then just says okay I I'll just want to do whatever this system decides is best for me other like this representative yeah might be an AI might be might be uh an experienced uh negotiator or something like that so I have a question um does this like so if you're imagining a Oster I Casper uh if you're imagining I'm just kind of off camera but if you're imagining a situation in which you have like pretty sophisticated AI systems uh that are kind of acting on behalf of humans to do maybe tasks that we can't do ourselves um then that's so that situation sort of makes sense um but I wonder in that case whether like any sufficiently capable AI system that's can like do that will also be uh like able to just infer the preferences the true preferences of the agent of the human or person that we care about the principal who is delegating to the agent and in which case they can just they can in fact just uh maybe threaten that human I suppose in that case uh if it's the AIS system is like the only person like taking actions then maybe that doesn't buy you anything but I wonder like is is this a concern or does this just get kind of washed away by the fact that like the humans totally in some sense like out of the picture after after the agent is is kind of delegated to um I'm not sure I'm not sure I fully understand the question so I mean one thing is that um normally the idea is that everyone knows what's going on uh or at least potentially everyone knows exactly what's going on so like if if I tell my lawyer to disvalue like if I say okay lawyer I'm not going to pay you all that much if the university after firing me puts up a page that says zero zero one one one one zero zero one then everyone will know that what I'm doing there's no no one will think maybe maybe Casper actually cares about this page everyone like my lawyer will know the the university will know everyone will know that what I'm doing is uh deploying a surrogate goal but they just kind of don't they don't really care right like the lawyer will want to maximize that that payment uh so they're going to um act on whatever payments I uh I set for them so they are going to try to avoid this from happening and the university just sees this thing okay I guess now we instead of threatening to uh put up a page about how Casper is unreasonable we're going to threaten to put up this other page well to us it's like this also is kind of the same as the it's kind of the same as playing the original game so we don't really care um and that that's kind of uh that's kind of the idea so like even if like even at the yeah even if everyone knows what my original preferences are um that they're still like the idea is that this they still go along with this because um well the representative goes along with this because that's what they're told to do like the rep sorry my representative uh and the other the other side goes along with it because it's it doesn't hurt then they have no reason to say um like actually we like we're going to punish you for for using the scheme because for for them they're going to be equally well off as in the original game okay cool yeah sorry some of the stories in my head were about kind of things where the pref the principles preferences might have been kind of hidden or kind of you know whatever but I guess that's not actually necessarily what needs to be going on here it seems fine if it's not that yeah girls um yeah and I think since anybody has other questions otherwise I think we can ask away instead yeah okay I have one did you share the end of this this end of the discussion that was from the last time uh I heard a bit of it I um yeah I I'm I think the yeah I'm not sure I heard the path that you're going to reference now it was if it was the discussion between uh yeah yeah okay so there's this uh distinction between uh like delegating to a representative and just like telling them exactly what to do you like specifying a strategy for them which is like a particular form of commitment which is sometimes useful like essentially equivalent to something or to behave in a particular way or to just like as in your paper uh delegate to a representative and change their incentives like by changing their utility function um and yeah we heard some reasons in the last talk why uh the former is prefer or sorry um yeah there's also this idea of like committing yourself to transfer utility um which the yeah the previous talk was kind of bad getting poor and like saying that it works in their setting like kind of it's basically like as powerful as anything else you would want to do and it's like kind of like good for other reasons like it's more credible um and yeah more like realistic or something uh sorry I forgot some of the reasons but they were good reasons um yeah and I just do you have any do you have respective on this like do you kind of agree with the idea that just commending yourself to transfer a utility would be like as powerful as for instance like what you want to do with say great imperf guns um I mean I guess okay I mean I guess in a sense you can like I guess one so one thing if you look at the first example you showed uh you can't like just by committing to transfer utility you can to like transform the upper left of the quadrant into the lower right ah yeah you can do it different like if you have the ability to burden utility then you can like kind of reverse like there's like this before like quadrants of the game are kind of like a person you're still alive and you can like reverse that furnace or so maybe if you like commit to burn utility but um yeah so that's this that's this thing right um so what so I guess you can are you saying like you could use burning utility to commit against DM and RM basically yeah so yeah so I guess the point of this is that like if you do if you pursue this strategy like you also have to burn six utility in DLR ultimately and then there's no no point because now you're just playing the original game yeah that you can delegate to somebody who like where your representative like imagine that they're burning utility but really not then it is again um yeah I think the yeah I think the the answer does kind of depend on whether these kind of schemes count um yeah I mean yeah like depending on what things you allow it might it might enable base to basically do exactly the safe Pareto improvements um idea which is yeah as I say to transform this game into the the top as the the bottom right game um but yeah I mean I I guess normally normally I wouldn't think of these ideas as um us kind of being allowed when when you say like yeah you can you can commit uh transfers um and yeah I would yeah I would think that that normally like if you can just um if you can just commit to transfers um you do you that yeah I mean there are problems that [Music] um says pressure improvements can help with that yeah committing the transverse doesn't help with um but I mean that's like I mean all of this is partly because safe improvements takes this like somewhat unusual perspective of yeah assuming that there are these equilibrium selection problems and assuming that you don't just want to resolve them right like if you if you just analyze this as like a normal game like if you if you analyze this this top left in the normal way of saying well there's you say equilibria right then um in some sense there's not really uh there's not really a problem even in this game right so you could just well like for example like dmrm it's it's like a perfectly fine outcome right it's it's it's Pareto optimal um uh and and so on um yeah I yeah I would imagine that's like the the normal way of thinking about committing to these transfers doesn't always resolve equilibrium selection or like enables dealing with it in the ways that say parietal improvements uh can deal with of course there are there are lots of other cases where it can resolve the problem in a way that um that's maybe better than safe pressure improvements um yeah but I I haven't thought about this that much okay yeah and I I think it's a time for um the next session but before that we're gonna have uh let's say 15 20 minute break your session can eat into the you know the the end of the brainstorming session so that's good and cast for sex for theory in the dialogue okay thanks yes yeah the authority to talk to you more thank you 