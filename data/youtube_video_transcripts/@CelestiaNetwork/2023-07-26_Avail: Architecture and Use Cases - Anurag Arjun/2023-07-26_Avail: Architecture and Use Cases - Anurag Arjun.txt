hello yeah uh good morning uh very pleased to be here very well organized Summit from the Philadelphia folks my name is anurag and I'm going to talk uh about a whale architecture today uh can I have the next slide please yeah so just to provide some context on what a whale is right like I mean uh uh I so Avail was started within polygon in November 2020 and we recently spun a whale out in March 2023 to become a completely separate independent entity uh what Avail is is a data availability layer which uses a combination of eraser coding kzg polynomial commitments and datability sampling and essentially you know like uh and I'll provide some context on where we are in that some of my background days I previously founded polygon in 2017 and because we and I started the project with my co-founder Rubble uh in 2020 and uh and and we just spun it out just very recently so the entire team of polygon uh came over uh to China's a whale so that's some of the background and you can see some of the history here so today you know like a lot of people you know asked me on Twitter regarding uh the architecture and so that's why I focus wanted to focus on what Avail is what are the use cases and so I'll try to get a bit into the technical details um as much as possible within the short time window uh so before I get into the meat of the presentation right like uh some context I know this is a modular Summit uh uh audience and so in general no major introduction required but essentially what I want to talk is that Roll-Ups are now acknowledged to be the main way to do option execution um and if you can see in the rise of ethereum Roll-Ups right like all the big activities happening on layers like polygon GK VM or arbitrum and optimism and so the Roll Up is now considered as the best way to do option execution ah and but if we consider that the roll up uh is the way to go uh and blockchain constructions are becoming more and more modular uh with the rise of roll-ups now it is important to see what these Roll-Ups uh really want and what are they hungry for and the answer is that uh you know they really want lots of Da or data availability uh and that's kind of the primary reason why we are working on a whale because and I want to make a bold statement here in the sense that every base layer blockchain in the future is going to be a DA layer even ethereum is uh has already pivoted to a role of centric roadmap and it is you know like prioritizing pivoting to a DL air if you've heard of Proto dank sharding dank sharding all of this point to the fact that the base layer is going to be a DL layer and all the execution is going to move to the roll ups on top and that is the context in which you should view Avail that Avail is a base layer that provides scalable data availability for rollups now what is a whale you must I mean this is a Celestia uh I mean is the modular Summit and you know like Celestia is one of the organizers and so you'll ask what is the what are the differences between a whale and Celestia and I'll get into that uh but essentially Avail is a modular layer uh that focuses on datability it does not do any execution it accepts transactions from Roll-Ups and makes them available uh via a combination of eraser coding and kzg polynomial commitments and in a sense what it does is um it kind of orders the transactions that come to it and uh provides it to the light Land network in general the mental model for looking at a whale is very similar to what ethereum like layer provides to the Roll-Ups on top so in in that case the Roll-Ups do the execution on layer two and then there's the base layer that does the data availability and so you can have a variety of roll up exhibition environments uh so this is this includes something like the evm but also more complex environments like svm but also app Train app specific chains and how we do it is a combination of Erasure coding kzg polynomial commitments and uh the USP of datability sampling uh which allows downloading of block data within a few random samples and we'll get into how uh that gets done but in general a variety of rollups can leverage this uh capability um this is the um base layer architecture and in general if you look at it right like I'll just go through it in little bit of detail so what is happening is the primary consumers of Avail are roll ups roll ups accept transactions and basically submit transactions directly to Avail we have this concept of application ID where each roll up corresponds to a particular application ID and then they can submit that onto the same base layer so that we can have multiple role of submitting data to Avail demarcated by application ID um what what we then do is kind of extend the data or Erasure code the data so if you see this diagram so the original data is then extended uh ah in general via it is a coding and then what we do is primarily create commitments uh of the data so I mean if you see this uh slide so this is the rough structure of the blocks right like so if you see the original data um uh the data from the Roll-Ups is packaged into the block and we create polynomial kzg polynomial commitments for the data and its erosion coded in such a when the homomorphic property of kzg allows us to mirror the Erasure coding in the com of the encoded data on the commitments as well so if you see in the right hand side the C1 to CN are the commitments of the original data and because of the homomorphic property of the case energy commitments you know like we are able to kind of extend that to the Eraser coded data as well ah and so once that happens so okay how do I go back this one yeah okay [Music] um in general uh we are able to you know adjust the Matrix size so what we really create as I told you is like an M cross n Matrix it is Erasure coded to create uh in general uh you know we double the data in in general and what we then do is take the commitment um and put into the header so the header has all the commitments to the data it also has the app index and certain other meta information now this ah block data is then propagated to all the other validators and basically each variator at the moment in the current implementation regenerates those commitments and you know it comes to consensus on the Block so that's how the base layer works and we've already gone through the block production stack um in general we've used ah substrate to build the relative node and the consensus that we use on the network is Grandpa and babe so babe is the block production mechanism uh and we have grandpa is the finality Gadget so it's a hybrid Ledger in that sense and you know like protects against large number of Roads crashing Etc the incentive mechanism is nominated proof of stick why we chose that is basically because it allows for wide stake distribution and so what happens in nominated proof of stake is that you do not delegate to a single validator you delegate or nominate to a pool which then is fairly distributed to a large number of varieties so you can do a rank choice of validators and so essentially why is this important is it allows us to have a pretty decentralized set of validators and we can have up to a thousand validators in the validator set that's that's um that's what we get from you know like using substrate we use substrate but you know like we of course it's it's a data availability layer so there is no execution so we've disabled all the runtimes and such and so it's a very light run time on the base layer itself now ah the beauty of this whole construction is we are able to do a Pity neat Lifeline Network and in general once the blocks are finalized and the headers are propagated to the light clients even if a validator withholds the data the lifeline networks you know like basically because they can sample the block pretty efficiently they can come to know if there's some withholding of data possible and in general uh we want to Target like a large number of light lines but in in a sense a few hundred or a thousand nodes are pretty much enough to kind of sample the block data pretty quickly um to get into the light lined uh uh architecture a little bit right like uh so it's so we started with a different implementation initially but then we had to kind of build the light line node from scratch we use Academia DHT implementation uh distributed has stable implementation and so the LC is basically form an overlay uh p2b Network on top of the base layer ah initially they use the full node for the bootstrapping but over time when you have a number of light lines on network a new light line that enters the network uh essentially can start sampling from the light client So within the light line Network there's a P2P Network as well as a DHT uh and so I mean you can think of this from a mental model uh the lightline network almost is like a torrent lag Network it's not the same but you can think of it like that because it stores some of the sample data locally as well for a period of time [Music] um how we do the proof verification is that you know these we we generate cell level proofs so as I mentioned we have this m cross n Matrix and so depending upon the size we'll have these proofs generated on the cell level and so that's why uh there will be a random sampling from these light lines and they they are able to verify uh these cell level proofs and as I said within within a few samples or if we have like a few hundreds or even thousand light lines the enter block gets verified and of course there's this property that the larger and the number of light lines in the network we can increase the size of the blocks uh as well uh you know like in that sense uh and I will say from effort engineering effort perspective um bulk of the work that has gone into building a whale and so we've been building this with uh for more than two and a half years now first within polygon but now as a separate identity uh and most of the work is on you know like the bulk of the effort is on the lifeline P2P because there are a number of issues uh that you know performance Etc that have to be looked into uh there uh this is sort of a visual representation um of of the data Matrix that I talked about uh we can play around with the number of rows the number of columns and you can see a reference Benchmark um on our performance in the sense that if we have like if you see the table on the right right like um like there can be uh two if a for a 2 MB block size right like I mean the rows and columns are such and the times to generate the commitments the polynomial commitments uh are pretty pretty neat and even if we increase the block size to a 32 MB or a 128 MB you can see that these are well within our Target block time which we have at the moment kept as 20 seconds for now and this allows for you know like propagation across the network as well as well as verification of the commitments as well um okay so um I will I will have time for Q a after uh the session but so in general if you have any questions on the architecture happy to answer post the talk um but having said that I mean once we've arrived at this whole construction um uh what is the ecosystem that we are envisioning that will be built on a whale so as I said this is a role of centric uh uh blockchain right like we provide we our primary customers are roll-up developers infrastructure developers and so these are the different kinds of ah solutions that we are kind of looking at right like so Sovereign roll ups validiums optimistic chains uh and you know like app specific change right like and think Cosmo Style app change but more in the validity proof or optimistic construction uh Manner and of course uh General opens rollups as well and recently I mean um so the way we are thinking about this uh go to market and I'll get into that a little bit is that there's been a lot of activity on the um L3 uh uh area so a lot of the l2s all the major ethereum l2s are now looking at their own L3 initiatives so if you look at something like an arbitrum orbit or a z k saying hyper chain or a polygon 2.0 or Star Quest fractal scaling strategy or optimism super change strategy and so they are basically optimizing for a lot of l3s uh in general because they what they want is um to optimize the L2 as as a liquidity hub for all the l3s on top so that's why you'll see in the coming days a lot of one click L3 deployment Stacks now why am I talking about this is basically because when we talk about l3s on ethereum right like the first thing that they need is a DL air to do um you know like dump their uh for the datability needs and they cannot use ethereum for that perspective and so uh this is sort of the graphical representation of uh you know like how a whale will be used in conjunction with these l2s and we're working with starting to work with a number of these uh to come up with these constructions we also release the Avail attestation Bridge recently it's a pretty interesting Construction in the sense that there is a data attestation bridge between the Avail base layer to ethereum right now of course it's on test net for now we are doing an optimistic style construction of the bridge but we've been also been working on a ZK snack based data attention Bridge with our partners at succinct in fact we just shot a whiteboard session on on the construction it's pretty neat and so we are going to be working with Susan in general to create this bi-directional bridge between Avail and ethereum because sustained if you know already has a telepathy Bridge which proves ethereum's proof of consensus and now we have with them um you know like uh zika snark based construction which proves avails proof of consensus which is Grandpa babe um so that's that's one Focus area um uh that we are going to work with and of course we are going to work with Sovereign labs in that Sovereign Roll-Ups in that sense in the sense that current rollups are primarily implemented as um you know like um to be verified on a smart contract on the ethereum base layer but with our data really sampling light lines what is also possible and especially with ZK constructions and recursive proof mechanisms we are also able to kind of propagate these proofs to the light line layer and in fact we are also talking with a bunch of wallet teams uh in in general to kind of embed the light client uh into the wallet itself right like I mean right now uh light lines um are run via desktop apps or CLI or something like that but what we envision is eventually they will make their way to wallets and so very similar to you know like let's say Bitcoin light lines or such right like where uh where the user doesn't even know that the light client is working in the background and the and why we are able to make this possible is basically because of the lightweight Construction in which these light lines can actually even work on mobile devices and hopefully in the browser and such uh at some point in time and so we we envision that there will be a lot of light lines and essentially I mean this is an underrated uh development but today you know like to verify for a user to verify the state of the blockchain it's it's not that straightforward on today's blockchains and what we will enable with this combination of Da light lines plus recursive ZK proofs is that any user will be able to verify the state of the blockchain uh pretty easily um and as I said modular baselines are perfect for Sovereign roll ups and along with us of course Celestia is also taking up the mantle and so we are very happy to grow the ecosystem together um I'll quickly end uh with um you know like the development stage and timelines uh as I said we have been in development since uh two two and a half years uh currently on our second long-running test net which is we we call it the carte test net it's named after uh aniket Kate who was one of the researchers behind the kzg polynomial commitment uh we already have a robust set of external validators already on the test net and we are targeting 200 um next month I guess and we want to do a incentivide test net which we want to scale to a 5000 light client uh a number pretty quickly we'll have a incentivized testnet also this quarter and the main net Target is end of Q4 or early q1 that's the main net Target for now uh we are pretty comfortable way in terms of where we are in terms of development uh quickly uh getting to the optimizations uh So currently in our base layer what happens is when the block producer creates the uh proposes the block uh creates the commitments we propagate this to other violators and they regenerate the commitments at their end what we want to be want to move to a construction in the future is a construction where other validators can just verify the commitments and not regenerate them which will make it much faster for um you know like to arrive at finality and such we are also working on a very uh neat construction called kzg multi-proof so if you remember the Matrix that we create we create these proofs at the cell level and so what we want to uh do is do SUB Matrix level openings and so you know like we kind of reduce the complexity of verification pretty significantly and this will you know like create huge improvements in terms of you know like the opening generation the DST population and you know like overall keeping the network uh streams you know like manageable um and of course uh while ensuring backward compatibility and as I said a little bit earlier we are working on the ZK snark base data attestation bridge and this is a pretty neat construction where uh you know like uh we are able to prove Avail proof of consensus which is Grandpa consensus uh within the snack circuit and I think this will be pretty useful for deploying or connecting our chain to a variety of other ecosystems because the nature of the uh Bridge itself yeah I think I'm um wanted to I've wanted to cover whatever I wanted these are some of the links important links you know like please feel free to scan the QR code we will have this presentation uh available uh online as well um and general yes we are hiring across the board we when we started like three months ago we were at 18 now we are at 27 and so we're looking for uh quality folks to attend uh join the team and so if you're looking uh for a you know like to join a growing team please feel free to Ping us and this is also our Twitter handle um so you know like looking forward to talk to some of you and of course I'm available and a lot of team is also available uh in the event um and you know like happy to talk to you as well yeah thank you 