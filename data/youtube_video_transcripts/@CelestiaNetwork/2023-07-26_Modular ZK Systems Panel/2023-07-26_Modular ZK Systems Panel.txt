cool well what a what a panel I have here this is so awesome um this is this panel was inspired by an episode Benedict and I did recently um in that we talked about the different parts of a ZK system so this is going to be a pretty technical panel I think we're going to go pretty deep in but I think let's start with introductions so I introduced myself earlier but I'm the host of a show called um zero knowledge yeah and I also am curating this this afternoon of CK through the ZK validator hi I'm Nico and I think says up there yeah I'm a researcher in cryptography mostly apply cryptography at geometry hey my name is Benedict I'm the co-founder NG scientist of espresso systems work on decentralized sequencing it's a whole other track for that the other one actually but I also dabble in zero knowledge proofs hey there I'm Zach I'm the CEO of Aztec um we're a privacy infrastructure provider for web3 and yeah I I also dabbled them Xerox briefs I'm Christopher I'm one of the co-founders of the anoma project I am not a classically trained cryptographer so I'm probably horribly out of my depth but perhaps I can provide a perspective on how these zero Knowles proof systems look like from the outside we'll see very cool oh yeah so I wanted um in this intro so before we did we dive in I do wonder if you could choose sort of one work paper project that you think most defines you that people here may be familiar with sure um I think I'll go after Zach actually it makes more sense in that the work I'll talk about probably builds upon the one he's going to mention now okay why don't we actually start from that side then right um I have not written any zero knowledge proof systems sadly I hope to someday but a few years ago I was starting to go into the space and there just seemed to be a lot of brilliant people writing zero knowledge proof systems and I figured like we need people doing something else so maybe in 10 years when I'm like off in a cabin I'll do that but probably the most popular paper was IBC unfortunately I think the paper is terrible oh no I want to rewrite it to be clearer but maybe I'm just biased after the fact yeah I guess um uh yeah plunk I guess is that's the thing that's what I'm known for sir yeah and does everyone know what Planck means at this point who here knows what Planck means actually like who who here knows the Planck proving system yeah yeah okay so people know the proving system but nobody knows where that word comes from amazing stands for permutations over the LaGrange base for ecumenical it's better to know um non-interactive arguments of knowledge and the a is silent hahaha some creative Liberty and the word plonk also has a double entender there there is a definition yeah it's it's British slang for cheap liquidity wine um because like like a bottle of bad booze getting to the bottom of a plunk it's going to give you a hell of a headache is it like a bottle or are we talking like bag like is it oh it's a box box okay okay yeah I guess probably still I'm most known for bulletproofs um which is a zero knowledge proof that is used in uh Monero and many other different proof systems I think Z cash now actually as well are parts of it yeah not as known as the these two cryptographers but I I did put out uh let's say an observation Building open plonk uh that's some techniques that we saw elsewhere in the in the ZK stack were applicable to Planck so that thing is called sangria fittingly right nice um okay so I wanted to start this with a little bit of a very kind of high level history of snark systems some I think these names may be familiar for anyone who's been following the space and what I'd like to do is kind of go through them and and any of you can actually weigh in here basically let us know what was the big change or what part of the stack was identified and optimized as we went through each one of these things um so yeah the fir I mean I'm going to start and Benedict you can tell me if I'm wrong here but I'd start with like Roth 16. sure I mean we can also start this is like starting in 2016 we can also start in like uh 2012 or well I was going to say like 1980s yeah but yes let's start with like practical approved systems okay okay so we're starting with growth 16 still I mean it's still a system that's used a lot right these libraries were really well developed very much and it's still the system with the shortest proofs that we know of so that's that's sort of why it still has um a place in people's heart because if you're trying to post a proof on chain that will be the cheapest way to do it yeah and just a note here there's a ton of proving systems that came out in 2019 we're going to kind of what I'm trying to identify are the proving systems that we just got like a lot of Mind share around they kind of won that round in a way and then you kind of see those be used for a few years and then a new one a bunch of new ones come out one of those kind of becomes the standard in a way this is like subjective but uh the next one I have is planck actually so Zach maybe you can help what what was the big innovation or what was the part of the ZK stack that it yeah what was he the key Univision of plunk was it was what it enabled was practical Universal ZK socks so um one of the big downsides of course 16 is you need to do is trust is set up ceremony for every single second you make um and so we wanted to preserve the like the nice succinct poly logarithmic properties of elliptical based snarks but one where you didn't need per sector set up so it was integration of Sonic which is suppose the integration on bullet person and a lot of other things um uh but the The Innovation was really it was a way of efficiently validating copy constraints so that was that was kind of the big bottleneck with universal snarks it was how do you um verify that all your gates are wired up correctly um so we we had an efficient way of doing that um and that then the effect of that also meant that it'll like a rather nice arithmet away a nice way of arithmetizes arithmetizing it's not circuits kind of fell out a bit um which was then turned into kind of the plonkish arithmetization and this is where people were optimizing on what you had done but the thing they were focused on was this era arithmetization part like they were changing it um yeah well yeah basically like so the idea is with it's with Planck the nice thing one of the nice things about Planck is that the the um the algebraic expressions that you're checking on every single gate um nicely mapped to the overall um like proven verify albums that are being run which means that you can then construct like relatively complex custom um algebraic statements specifically for a system that you're building your stock for um and so yeah that that kind of took off in a big way I think one of I mean there's many different ways to look at the Innovations of this in the systems but I think one of the realizations sort of that came out in that time and and where planic in Marlin and Derek and others were sort of a big part of is this modularity right like the separation so the separation in this particular part into kind of I come I have a computation and then there's something called the polynomial IOP which is basically reducing it to some polynomial checks so I just checked that some polynomial is at some point is evaluated to I don't know zero um and this basically allows you to sort of pull apart you know these monolithic proof systems into two components one of them is this arithmetization component something like plong something like Marlin or you know there's others Stark like a air I guess would be the other one and then the polynomial commitment constraint and now you can plug in different pieces for these different things so you can plug in for plunk you could plug in kzg but you know you could also plug in like some hash based thing like fry and like the nice thing is and and sort of there were also form ethereums about this that basically if you take a secure thing from one side and nice to take a secure thing on one side I get like basically the plugging together works and this gives you a a new proof system so for example now we have like Halo 2 which is uh plunk plus like a bulletproof style polynomial commitment or we have um I think plonky 2 which is uh Planck plus uh like uh the fry and I'm sure we have like air plus kcg and you know there's basically a bunch of these things and I think this separation is what I would view as the sort of realization of that time that then enabled Innovations like Punk yeah so you mentioned sort of the next stage that I would say had a lot of Mind share although I do feel it's just part of the ecosystem but it was like the Halo work and then the Halo 2 work especially got a lot of Mind share a lot of people building on it you had just said that it was using something like Planck had its two defined parts and then you could like take out one put in another did Halo 2 also introduce any other places to all of a sudden break off into another module okay to be honest I don't really know what Halo 2 is anymore because it's like Halo 2 to me is one of the like no offense there's anything it's very confusing because it's a library there's a paper Halo then there's like Halo 2 there's also like what people think it does and what it really does um like so it's an arithmetization language so it's many many different things so it's like also names just like terribly confusing we're really good at that I think what like I actually view Halo and Halo too quite separately and and what Halo 2 does it's really an a nicer a slide A variation on the plunk arithmetization like a really nice way to sort of encode the the the plank arithmetization in this this column for format with coming with a library a very very good seemingly good and widely used Library uh like cryptographic library with it is that do you agree or like yeah broadly I think maybe I'd add one other thing which is that like I think one of the things that Halo 2 really played was the basically the use of cycles of non-parent friendly curves to do to enable like uh recursive pre-core position um but that's just not implemented in Halo 2 right now like this is like a really confusing thing is everybody thinks this is implemented in the library it is not right now so I think the confusion comes from the paper that came out from the oh interesting uh Z cash people so I think it's Tara shonbo and I forget there's a third author on on the paper and that paper sort of describes this yeah thanks um that paper describes this idea of how can we get cheap recursion using cycles of Curves um so maybe that is what people refer to as Halo or yeah that is what that is there's the Halo paper which is about recursion which is another interesting area but it's not implemented in Halo 2 maybe right now maybe in the future it will be but like this is why where I'm like uh it's like this is why this grandmas because it's just like so confusing like uh you know these things because cycles of Curves which is what people think Halo is which which is what Halo is is not implemented in Halo 2. one other thing which sorry I've been one thing which is it complicates matters which I'm certain that that I've I've um uh accomplished in this as well is that all these proving systems are also kind of attached to Brands um or their brands they become brands for the companies that use the where the people inside this company inventing them um and so I think that that also has a big effect on on the on the confusion aspect as well and yeah yeah yeah I mean I was just going to ask because I was curious in sort of sounds like you've looked at the Halo 2 library and there are 15 different Forks every time I go to a crypto conference I learn about two new Halo two forks at least one of them implements something I want and I didn't know about it it's a big counterparty Discovery problem but I'm curious you know in the modular stack we see like clearly separate roles like data availability solving execution it sounds like they're kind of two now like the the first or the arithmetization part and the polynomial equipment part do you think they'll be two forever like is this the final decomposition a dwager already at three where you have um your arithmetization then the darker parts is something that your polynomial IOP can deal with and then you can use whatever Plano commitment scheme you want so it's already I think like a three layered thing and then you add folding for fourth layer and look up tables for optimization so those would fall yeah is that more of a technique I have all of this this is actually my next question maybe let's let's finish our history but I'm getting into it well I guess it is the next type of the history after a sort of yeah well you're right you're right because I my net obviously the next one that a lot of people have been talking about for the last year is the Nova work which also leads to the hypernova and protostar uh work which introduces this technique of folding or accumulation schemes which I've always understood as being like deeply built in but it sounds like like does it change one of those three that you just described is it in polynomial iops is it in the arithmetization is it in the polynomial commitments is it a sub so similarly to how Groth 16 were like was like this monolithic thing and we started to take it apart yeah yeah I think this technique started from a very specific application so in the Halo paper they were like we are using this bulletproof style polynomial equipment scheme and we can actually defer some of this to later and then slowly but surely we're like picking threads out of this and called this to be very very generic with protostar super recently I will also say I know Benedict you had done work that sort of did this before Nova but described slightly differently am I correct yeah we just didn't give it a fancy name so it didn't get any attention you know but you did do bulletproof so you knew no that's like the less you knew that's left yeah no you got to give it fancy names no being facetious um I think it's so the most important separation for me is like this sometimes it's called front end and back end where front end is what the developer interfaces this is how you code up your computation how you express your computation right this could be like we and we have this separation in normal Computing as well we don't need to think about uh like uh you know sort of snarks we can think about like your programming language right that that is the front end right this is what you write and CEC plus buzz but whatever and first this is you know like something like the Halo 2 front and like sometimes it's r1s yeah sometimes it's like you know a higher level thing right like there could be also multiple levels in the front end like uh you know circum or whatever that then gets compiled down to something below and on the back end um there is the proving system and the proving system can also have you know multiple layers right it can have like as you were saying you know the the compilation down to a polynomial commitment and a polynomial commitment I would say that you know these these folding schemes give you uh a proving system with like very particular properties they especially work for these or they designed for these iterative computations so a computation where you have like a step that you do over and over again for example a blockchain it's just like one block and then another block and another block and another block that's the computation that you do over and over again it's a so-called iterative computation and for those we can use kind of these folding schemes or these IVC things so but this is all in the back end this is the sort of the technical infrastructure the like sort of the assembly or like no it's really not even the assembly it's it's the CPU right that does the that does the execution and again like CPU we can have you know multiple layers in there we can have the instruction set uh we can have you know all of these things and then some things fit together some things don't fit together and some things you know you can build a compiler to make them fit together this is where this picture looks sort of complicated but like really the thing that I like the image that I think you know helps sort of people think about it is just like front end like how do you express your computation hopefully in the future this will get easier and easier and you can literally just write it in you know I don't know like rest or like python um and then back end hopefully those things get more efficient and Powerful as we go along and which sort of like executes the the the computation and both of these within each other within itself can have modularity which is the cool thing and the more modularity like I think historically what we've seen is exactly this trend that you just said usually like sort of the Innovation comes like someone looks at something like monolithically comes up with some you know uh like a genius new ideas something like Halo which is by the way a beautiful genius idea like and Halo is more in the Nova like folding kind of line of work that's where I see it um and then people started like picking it apart and making it more modular and this enables new innovation and this enables like new sort of techniques um so we should you know like sort of always uh cherish and you know like I think the name here with the conference and the panel is very good because like the modularity seems to be extremely helpful for both new innovation and for understanding these things yeah I was just going to ask me one thing modularity helps me understand in the distributed systems context is what the hard axes of trade-offs are right like there's a trade-off between privacy and efficiency and a sort of counterparty Discovery matching sense there's a trade-off between trust and efficiency and kind of how much you need to replicate your verification sense and these trade-offs don't change like you can make the parameters faster but the trade-offs will always be there they're just properties of how the components fit together and what kinds of properties you want out of them holistically and I'm curious does the modular decomposition of like zero knowledge proof systems is it yet at a point to provide Clarity on these kinds of axes and you know what are they this is like benchmarking a lot of this stuff I guess I think not necessarily I'll say sometimes in our case modularity comes at some cost where because we're not looking at things monolithically we can't open these black boxes anymore and there are some small tricks and optimizations or things that we could have done otherwise that we don't do anymore not say that we don't do them at all like we do have systems that go in and break the black boxes um but I don't think the modularity itself I okay I guess yeah you can draw some lines of you know the trade-offs but it also draws them slightly out of like somewhat artificially but I guess like maybe your question is in distributed systems we have like these pretty strong lower bounds right like we have some sort of like impossibility results that that are pretty strong uh what is interesting is that in zero knowledge proofs like uh there's some like lower bounds but like not really that many and that meaningful ones it's like it's it's not even clear right that proving is necessarily more expensive than Computing something right like if I want to compute a square root then I need to compute the square root but if I want to show to you that the square root is uh like correct I can do this with just a squaring right I can go in the inverse Direction which may be cheaper so it's not even clear or like maybe like proving it like executing this computation is sequential but the proving can be done in comparable so like this like as far as I'm aware of this like some you know lower bounds but these lower bounds are usually in in like uh stylized models and if I go out of these models I can oftentimes even circumvent these lower bounds so we don't like one of the Beauties is we don't have like strong lower bounds but this also means that like modularity there's uh like um yeah it's it doesn't really like uh we don't have to like it doesn't like show oh you know here's like clear trade-offs yeah just to feather that yeah it's It's tricky because it would be nice to be able to clearly Define a treadle space but I think if you take a snapshot of the ZK landscape at any one time then yeah you can probably you can probably Define some kind of trade-offs space between all the various Moving Systems and all the major components but because the low balance of nowhere near being reached yet yeah you flash for six months and everything you've done your analysis on is now Obsolete and it's been replaced by new stuff that's better just in uh I still don't think we've properly placed folding schemes and lookup tables into the like the where are they in this stack I still don't I've been sort of referring to them as techniques because I'm like just like something you use on top of but that it's not its own system in its own right I don't know I can take a step so I think like where I would place look up tables is they're just they're part of the second arithmetization uh so it's because there's a subset then it's a it's a way of converting your lookup table into algebraic statements just like the like like the rest of the Edition like our magic Gates of your circuit so I think you can place it at that layer more or less when it comes to folding schemes I would say it's it's at a it's at a higher level than the than the underlying proving system because they're like there's been some work that like the protostar and pretty galaxy are basically the the actual proving system is left as a black box more or less like you need you need to you're the only requirement is that you are using um you have an additive homomorphism um between the I mean they're not I mean protesters and even to define the use of polynomials but assuming you're using polynomials then you then you have some kind of additively homomorphic commitment scheme for your polynomials so I would I would place it a layer above the underlying printing system and it's if it's it's a layer which if you system if you need to compose if you have some higher level architecture that just that composes multiple proofs together then you apply the holding scheme on top of your printing system to get that capability um so does it have could it have its own category would you yeah yeah okay and I guess the reason like those two that I highlighted the reason I wanted like so we've mentioned a few of them each one of these sort of has a line of work and there's researchers who are focused just on those things um something we didn't actually send and now I'm realizing I'm not sure I know this at what stage like lookups which I just brought up there was pluck up there had been work before as I learned on a panel yesterday there had been some work done before Mary Muller was part of some work that was doing that but then there was the planckish uh pluck up was created but was that like what system does it does it go with Planck does Planck then come with lookups or like a a second generation yes is that the introduction that's that's kind of what Ultra plunk was okay look up so it's yeah so pickups was it was it certainly wasn't the first lookup table scheme um I think it was the first well at least at the time I thought I don't think it was I think it was the first that had a like the access cost of your look of tables was constants um most of the ones before were quite you like the number of constraints in your system that you needed um to do a lookup was logarithmic in the size of your table and look up is this one gate I think there were systems before that did that but look it was the kind of the if I might if I make the claim that it's the first it was the first constant time practical lookup scheme is that controversial I'm not sure but yeah I mean I think also that you know this is another great case where sort of modularity was sort of observed later on where you know sort of a lot of these lookup schemes were invented and they were invented sort of in the plunkish land I'll say that like but you know most recently you know and and I think you know sort of for example this was observed in in a paper about ccs and and other things is that it's actually you can pull this out like you can you can pull it out and reuse this component in in other systems and it is not like tightly coupled with the Planck proving system um and and yeah so uh that's sort of a very nice observation where again like we've observed some modularity and and this this helped sort of improve the the space I want to just kind of go over what we've already said just again just in case people have like aren't fully following so so far we have polynomial IOP polynomial commitment scheme arithmetization subset lookup tables and somewhere else in somewhere is folding schemes beforehand is recur does recursion and folding schemes get to live together yeah yeah okay same same thing ready is there okay so that that sort of maps for roughly what I had written down here but it's nice to see it map though but is there anything else what else is there so like going back to your point like as we Define these components researchers can focus in and optimize them and find new combinations find interesting properties of other parts that we just described that interact really interest like nicely with yeah I don't know if I describe that so well but we're looking for new things we're on it we're on a search now I think I think I think you they also I think I think you can add other layers of abstraction but I think that's where you would like you'd stop writing soundness proofs and papers as in like if you like there are plenty of high level constructs but maybe you could well yeah you're saying we're at the end of all all modularization this is it no no no we're done maybe maybe at the end today but I think Benedict has some words to say on that I don't know I mean like there's even you know like you can go even lower level right so one of the things that okay this is like uh another slight rant like when people it's not around like when people start learning about you know like I've sometimes if people approach me like you know oh I want to learn about cryptography and it's like oh but I've been looking at these elliptic curves and like they look so complicated and what I didn't you know I got stuck and I like probably like after I wrote like I don't really understand elliptic curves and like for at least like the three first three years of my PhD and I'd written Bulletproof by that I really did not understand elliptic curves and that's totally okay because again we can use abstraction right these elliptic curves are just you know one tool like they're they're mathematically they're then uh an algebraic group um with a group operation where the discrete logarithm is hard or something like this right and we can beautifully use abstraction and you do not need to understand how these other components work and there's like sort of this whole like cryptographic layer where for example a lot of these proof systems use a hash function and we have like you know there's a new hash function coming out every every week and especially there's been a lot of focus on on these uh snark friendly hash functions so you know that's another component or there's different elliptic curves uh with different like security and efficiency properties uh that's uh you know sort of another component at the you know cryptography layer that's a really good point let me just to add to that to maybe to try and systemize that like a bit more the um like all of these stock systems they all of their security proofs boil down to um a relatively common set of computational hardness assumptions but that I mean like basically you're saying basically you basically prove that the only the only way for a adversary to break a printing system is that they can solve a particular problem that we assume is basically impossible to solve um and so for example like one of the common ones is the elliptic curves the logarithm problem basically saying you can't find the discrete logarithm and and so yeah there's a whole there's a whole low very very low level abstraction rate of The cryptographic Primitives themselves where you where you connect where you at like the the actual like constructions that you um pull this hardness assumptions out of um and yeah that's that's a whole other level field of work and this is again beauty of modularization you don't need to understand it it's totally fine just sort of deal like be happy with abstraction when you're trying to look at things like you know really be happy and and sort of embrace the abstraction embrace the modularity I would say maybe that's one of the points of friction that I want to see sort of sold now it's this friction between the back end and the front end because all this ZK stack we've been talking about so that's mostly in the back end these things change very quickly and we get better and better very quickly and they also like you're essentially changing the interface right that you have with the front end so there are new things that are available for the front ends and it's really hard to know like okay can I start thinking about a good front end for this yet can I start thinking about a good language or a good representation of computation to throw to these back ends or are they just gonna you know is the ground gonna move under my feet yeah this is actually to you Chris a little yeah I mean looking at things a little bit more from that perspective as we do it seems like the really hard problem is dealing with differently sized finite Fields that's like the essence of the problem that we see from the higher level language perspective is that we want to you know the choice of the field to me is like an implementation detail that should live in the back end and the front-end program should be portable across this choice so that like as there are different interactions between different different systems and the underlying systems change and certain things become cheaper with small fields or whatever or you need certain cycles for recursion these are like details in the back to me that we want to abstract but it's difficult because at the same time in order to get like efficient execution you kind of need to know about this detail when you're writing your programs so there's this like bleed through of something that is really at the very bottom like this is the thing that you're abstracting over right there's this bleed through all the way to the fronted language and I have not seen a convincing like General approach to translating between just modular arithmetic over different finite field results of the mathematicians I meet tell me it's really hard it's like discrete algebraic geometry like hard stuff but I don't know I'm curious I mean yeah I think you hit that end on the head when you said that is there's an efficiency problem there where like every abstraction it has it has an implicit cost associated with it and right now like over the last 10 years I'd say we've been building up a lot of abstraction layers from what used to be just a core monolithic grouping system you know like the glow 16 or BC GTB 14 I can't remember their names um but yeah the one the one that's really missing is is is creating something which is field agnostic and I don't think it's it's probably not going to sum up for a while because it's a hard problem like the the one way to solve it is you basically you create a a virtual machine so you instead of instead of writing turning your program into constraints for a specific brimming system you turn them into operations over some imagined virtual machine that you then prove in your underlying Brewing System um and then ideally and then that assumably one would then not work in finite Fields when we're working just regular you know base two fields um well everything's just um yeah but like we are we are not well depend it's far to use case specific because the the the slowdowns never are gargantuan I feel like if you often I think that the history of ZK snarks and ZK proofs kind of tracks similar to the history of computing in that if you think about where we were in the in like 1990 it's similar to where we were in 1936 when like Alan Turing was writing paper saying in theory we could do this wonderful stuff but God knows how um and then you know the like the very early ZK snarks they were basically the the very first digital computer series vacuum tubes where you like you know forget about cost like custom programs like you like your programs you had to like hard rewire your computer like in a plug board to get different programs because it was that low level and then as as this as the performance improves you start to layer on more Obsession levels so you start to get primed to programming languages in in the in the computer space and that so and that that could be translated to creating some of these very basic abstraction layers that we have today but we're still we're still early I you know I say if you can if you can draw analogs between the path of a ZK prison path of cryptography we're in like the 1960s a best yeah but you know in the 1960s we went to the moon so maybe we can do that again when Moon if you draw that analogy out I think the conclusions are pessimistic not optimistic yeah we haven't been back to the Moon that's true well it was damn hard and expensive so right right true true so are we saying that we're doing something damn hard and expensive that won't be repeated in the future but we did it on like the computational power of like I don't know probably like my watch that I'm not even wearing but like yeah um all right well I think we've covered most of the questions I mean I had I had one last one but that's kind of in the weeds you'd sort of mentioned hash functions and I was thinking like hash funds with pairing based where do we place this is this under the polynomial commitment scheme so like the you go into the polynomial commitment fry based will be using hash function kzg is using pairing based yeah it's a technique no exactly I think it's a technique that is used it's very tightly coupled usually to the polynomial commitment scheme okay um like each cup polynomial commitment scheme requires is built on a different cryptographic Primitives um which then defines a lot of the efficiency that's the problem that we were talking about that it bleeds up right because it does Define a lot of the the efficiency properties but it even defines you know to some way like unfortunately it defends in some way like how you can express your computation um and uh but it also yeah defines your security assumptions and and your like trusted setup and all of these things and um then the the other thing that I was saying the modularization there is just that there's many different elliptic curves and you know like kzg can be implemented on many different curves um and which one you choose again different trade-offs like all the way up yeah like like the blend depends like oh what does ethereum have pre-compiled for right because uh yeah and also the the landscape of Primitives is is much more slower moving than the high level construction is because these computational hardness assumptions uh you can only really get consensus around them over time because it's not like you have it's not that you can formally prove that a particular computation cannot be done um in in polynomial time that's kind of the P equals MP problem so for example pairings were around since the 1980s elliptical pairings they weren't used in commercial cryptography software until the mid-2000s I think I believe just because um people just didn't trust them didn't trust them uh it takes about 20 years but well I'm sure we'll see new premises we're going to start to see I suspect polynomials based around lattices turn it cropping up in in ZK you say pairings 1980s I think so what you said they were invented in 1980s oh no no no no no like from like 19 yeah yeah yeah yeah but um as like the cons the concept of using a billionaire pairing in a cryptographic protocol was was 1980s wait you're about to say something else what did you just say um what was I going to say that oh just I was just saying we're going to see some New Primitives eventually this is a cool I'm kind of curious about like a higher level like elliptic curves are like one of the most primitive algebraic structures that you can use like there's there's a whole field of like things like tour space cryptography um or you know you can add dimensions on and you can get possibly some interesting properties and anyway but so we'll we'll see what happens yeah maybe that's sort of the last so the area of potential improvements or Direction you're thinking like lattice I guess that would open up all sorts of cool new techniques is there any other like specifics what trilenium Maps yeah well elliptic curved pairings are a bilinear map um which basically allows you to do quasi multiplications but if you had a trilinear map then that would open up um it's okay so I'm this is this is this has been a long time since I've looked into this and I'm a bit of a dilettante in this field anyway so um take this as like just like some guy on the internet but um I think it would open up the ability to create fhe type constructions with an elliptic well whatever whatever is you have the training map with the same with that elliptic curves are added to be homomorphic I think you could think you can get something which is multiplicatively homomorphic with the trilinear map maybe completely or which is partially so it was under the impression that bilinear Maps allow you like one multiplication and if you go train linear you get one extra one so you get two multiplications we need probably more if you want like fully homomorphic so the thing that trilinear Maps like one of the key this is you know outside the ZK space but really exciting is like the thing that trilinear maps and more generally these multilinear Maps uh give you is obfuscation program application which is the sort of the Uber crypto primitive because what it means is that basically I can have a smart contract um which can have an embedded secret key and nobody can read the secret key so for example like the the smart contract could literally store some some balance in it and and nobody could you know see sort of the balance and it can update it locally or it could have like yeah some sort of I think we had some talks on like private State and public State uh I guess you talked about this and and like with obfuscation yeah these things would become uh significantly more powerful and and yeah there's really cool things that could happen there um that's sort of the part of the crypto future out there like yeah yeah exactly like in the private transaction World somebody has to own encrypted State and control it and you need their and they're there The Entity that needs to be able to construct the proofs of computation over the private state which is kind of a pain if you know you need to for example liquidate that person they're not going to make you a liquidation proof um and also yeah if you but yeah anyway I could walk on for ages I think we're getting the sign to leave the stage which is a bummer because I didn't leave any time for questions I think we I had too many um but you've met these wonderful panelists and I I guess if you can if you have questions please come join us after thank you so much thank you to all of you thank you [Applause] 