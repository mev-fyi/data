hello uh I'm Uma one of the co-founders of succinct and I'm going to be talking about aggregation is all you need okay so I'm going to start with some background on succinct so in particular if you look at the ZK landscape today there's been a lot of focus on two types of applications so we're interested so there's zke VMS and then there's also privacy preserving protocols like tornado cash and it's distinct we're pretty interested in exploring the rich application design space beyond just these two types of protocols that are really well explored today so we thought a lot about how else can ZK help scale and make blockchains better and we started by working on ZK like clients which is basically verifying consensus protocols inside a ZK circuit to allow for efficient verification of consensus protocols in evm so in particular to get into a little more technical detail on how this works you would have a source chain that has some consensus mechanism you verified the consensus in a ZK snark and then you verify that proof in an execution layer very cheaply and then you can run a succinct on-chain like client so that one chain the target chain can talk to the source chain natively and this solves a lot of existing problems with interoperability protocols today where generally if you want to transmit information or data between one chain and the other you have to rely on a trusted multi-sig or some trusted group of entities and with succinct on-chain ZK like clients you can basically do interoperability without these trust assumptions and you can get much more secure interoperability and the first DK light client we built was a ZK like client for ethereum so our first protocol telepathy which has been live on mainnet since March uses our ethereum ZK like client and with that you can send arbitrary messages from ethereum to any other chain and you can also read ethereum State on all these destination chains because you have the ethereum state route on these chains and then it's also useful for bridging information from ethereum's consensus layer to its execution layer and so we have a few people using this one great example is gnosis chain is using our telepathy protocol to secure their native bridge and then eigenlayer is using us to operate the restaking protocol by getting ethereum consensus information in the execution layer so we've built this ethereum ZK like client but we want to expand ZK and operability by supporting more consensus protocols so there's only really a few consensus protocols that matter tendermint is one that's commonly used across a bunch of ecosystems because it's the native consensus protocol of the cosmos SDK another consensus protocol that's used by the substrate SDK which is used by the polka dot ecosystem is Grandpa and babe consensus and so there's a few consensus protocols that we at succinct care about proving in a ZK circuit to kind of expand the domains that can talk to each other through these ZK like clients and these EK light clients are in general useful for two different things one is L1 to L1 bridging so for example having an ethereum chain talk to a cosmos chin that's using tender mint and then also another like subcategory of this that will be the main focus of my talk is data availability layer bridging so if you have a DA layer you can Bridge the state of the da layer to ethereum and so I'm really excited to announce that we're working with Celestia to build a ZK bridge to bring Celestia state to ethereum and this is kind of like the big scope of my talk and one question you might ask is okay why are we interested in bringing saucity estate to ethereum and so here's a really helpful diagram to show why that's useful basically in the modular stack a chain can decide to have their data available on Celestia but then they might decide to settle on ethereum so this diagram shows how an L2 operator might send proofs whether they're ZK proofs or they might sound like uh you don't send an optimistic fraud proof but you have that settled on ethereum so you send it to your L2 ethereum contract and then you send your transaction data or your da to Celestia and Celestia has this super scalable really high throughput da and so for the roll-up it's much cheaper for them to operate their role up in this way and then you would use our ZK like client to basically attest on ethereum that the data is actually available on Celestia and so Roll-Ups kind of get the best of all worlds in this way and it's much cheaper for them to operate so there is an existing protocol to bring Celestia state to ethereum It's called The qgb which stands for the quantum gravity bridge and we are basically turning that into the zkqgb uh and one of the a lot the zkqgb has a lot of benefits so right now the quantum gravity bridge is this kind of sidecar on the Celestia protocol and one of their core values is you know really having this minimal protocol that's minimally simple and only does one thing which is data availability and so by having the zkqgb we can take out the existing qgb and really simplify the core Celestia protocol and then just take the existing Celestia validator signatures verify them in a ZK snark and then move that out of this core protocol uh and this is really nice because also Celestia can scale their validator count without having to worry about an on-chain like client gas costs and we in general can reduce their gas costs a lot for the existing qgb by bundling all these DK qgb verifications together so now I'm going to dive into like some of the technical challenges that we faced while making the zkqgb for Celestia and in particular ZK tenement and one interesting thing to note is as I mentioned a lot of chains use tendermint so this is like actually reusable work throughout the cosmos ecosystem so in general when you verify a consensus protocol in ZK you have to do a few different things you have to verify signatures you have to provide hash functions and you have to do some decoding and in general the pseudocode looks generally always very similar you verify signatures you make sure at least two-thirds of the validators have signed then you have to prove that the existing set of validators is like the correct validators and then you have to Merkle prove any inform important information against the header such as a message was sent or some amount of money was deposited in a contract or burned to be minted on the other side and then in particular tendermint has some particular technical challenges associated with their consensus algorithm so they use the signature scheme ed25519 and you have to verify these n ed25519 validator signatures unfortunately these the signature scheme has no aggregation in it like the ethereum BLS signature scheme so that is a technical difficulty and then tenorman also has no Epoch time so in the worst case you might have to verify many headers in a row and then finally when tenorment was designed they didn't really design it to be maximally snark friendly so they have these snark unfriendly serializations used throughout such as protobuf that have various challenges with being implemented in a circuit so I'm going to Now cover some of like the more specifics of what we implemented at succinct to basically have our ZK circuits be able to handle uh tendermen and have verified tendermen consensus in a snark so here's an outline of the techniques we used and then combined together so one interesting thing to note is that verifying validator signatures and hashes is actually embarrassingly repetitive and a very parallel task if I verify one signature it has nothing to do with the validity of another signature so you can really trivially parallelize the verification and if you just do this naively in a circle in a normal circuit and you just verify 100 signatures serially you're not taking advantage of the innate structure of the problem and so we think we can do a lot better and so we call this idea ZK simdi simdi stands for single instruction multiple data which is like a concept that's prevalent and you know a lot of other contacts like gpus and AVX instructions but basically it provides this form of data level parallelism which lets you compute the same function f on a bunch of different inputs X1 through xn in parallel and so we realized that Starks are actually really convenient for these sorts of parallelizable computations so typically people use Starks for VMS like a lot of the ZK VMS or even zkevms are written in stark-based languages and so Starks have this like single state transition function that repeats across all the rows of the circuit and as a result they're much more lightweight to prove and often they have much faster proving times than other arithmetizations like Planck and so we figured out a way to arithmetize constraints within a stark to implement an abstraction very similar to CMD so as I mentioned before we have this like kind of abstraction that lets us in a very general way specify an f a function f that we want to compute independently over a set of inputs and compute a bunch of outputs and in the particular case of signature verification f is just the function of verifying a signature and X is just the actual signature that we want to verify as valid and this is like a very simplified diagram of what's going on but basically in our Stark that's verifying in parallel a lot of these signatures we have 2 to the 16 rows and then we have 256 signatures that are getting verified throughout the course of the circuit so they're not getting verified serially what happens is that we have a um is there something uh so anyways is there something that happens where basically we are able to verify all the signatures in parallel and then we have an accumulator column that accumulates and verifies the results of all the computations uh together and at the end is a random linear combination check to make sure all the verifications actually went through okay so at a very high level okay we talked a little bit about how the subtraction works and we built this like nice framework to let us do these computations in parallel and then we compared it to like some existing implementations and so in short these benchmarks were taken on an M2 Mac but end to end our Stark framework uh proof generation for 250 verifying 256 ed25519 signatures took 80 Seconds and so the proving time per signature is around 320 milliseconds in contrast if you were just to verify one signature in the ponkey2 proving framework using planckish arithmetization that would take around 17 seconds and then if you verified it in good narc which is a grot16 based proving framework it would take around 14 seconds and so you can see our like abstraction of basically paralyzing uh verifying all these signatures as a batch results in a much faster per signature verification time which is what we would need for some verifying something like tendermint which you have to verify a lot of validator signatures because they're not aggregatable so I've talked a little bit about how to do this like parallel computation of the signature verification another interesting thing is that you can further use recursion to further reduce the unten latency of these parallelizable computations so in particular at the root of this tree each leaf or sorry the leaves of the tree at each Leaf we verify a batch of signatures using our ZK some D abstraction and then we actually verify each of these batches in parallel if we want to verify something like a thousand signatures so each Leaf is a stark and then we recursively combine the verification of the Starks together and then we're able to do this in a tree-like structure so the end-to-end latency of our whole computation is simply the depth of the tree which is log 2 of the number of signatures that we actually want to verify and this means that even though we're able to throw more compute at something the n10 latency of verifying a lot of signatures is greatly reduced so one problem is that when we're doing all this stuff with zk70 with the Stark based framework or the recursion we use this proving system called plonky2 and in general recursion friendly proof systems are typically not compatible with the evm in the evm if you want to verify proof really cheaply it's best to do it in grot16 or something pairing based because ethereum has pairing pre-compiles so our solution is that we have to wrap a recursion friendly proof system with an evm compatible snark and so what in particular what we do is we take ganark which is Groth 16 or punkish kcg based and then we take that and we wrap the ponky tube proof and then we verify it in evm so our proof system composition where we're combining three different proof systems a stark-based one a punkish fry based one and then a punkish kzg based one unlocks the best of all worlds you have really fast proving for batches of signatures then you have really fast recursion for reducing n10 latency and then finally you wrap it all in the final layer that gives you really cheap evm verification and you kind of need all three of these components to prove a consensus algorithm like tendermen that has all these like extra challenges associated with it and in practice I kind of touched upon this we have this particular proof pipeline where we're doing this proof composition and aggregation so the first step of our proof pipeline is we have an application specific circuit that can recursively verify batches of things and so in this application specific circuit we kind of have the business logic of the consensus protocol where we're verifying validator signatures maybe we're verifying headers and hashes and other things we need to do then we have a recursive circuit that verifies the proofs from step one and it normalizes everything and makes the proof size and the custom Gates a constant and then finally we have this Groth 16 recursive circuit that verifies the proofs from step two and basically what that does is the cheap evm verification so we have this like three-step proof pipeline uh that composes to get us all the properties that we want which is it's really fast to generate a proof and then also really cheap to verify and just to throw out some benchmarks about the proof recursion so the recursive circuit for ponkey2 which is step two in the three-step process I described is actually really fast they heavily optimize their framework for recursion and so in net it takes less than two seconds to do the witness generation in the proof time and then for the final wrapper circuit for cheap evm it takes around like 16 seconds to generate the proof so it's actually still very feasible and manageable to generate this proof and then as you can see the on-chain verification cost is around 400k gas and probably could be optimized further so it's very feasible to run something like this on the evm today and then finally proof aggregation we think is super important so so far we've done proof composition of a bunch of different proving systems but eventually you can imagine that we would have a bunch of different consensus protocols that we're verifying in ZK circuits so we'd have this Grandpa proof of consensus we'd have tenement proof of consensus we'd ever ethereum like client and maybe we have a bunch of other proofs as well and one nice thing you can do is you can take all these different consensus proofs that are coming into ethereum which is a very constrained computational environment and you can aggregate all of them which can save even further on the cost of verifying all these proofs and so when you aggregate all these proofs you dramatically reduce the cost of verifying them on chain and you can also verify proofs that aren't like clients as well and in the end we think this will be like a huge unlock for making gas costs much cheaper and then also you can have the state of all these different chains on ethereum more frequently because the gas costs of verifying an individual one will be much less so yeah this is a meme about kind of all the ways we stocked these different techniques to finally get to something where it's actually fast enough and feasible to verify in evm so our ZK simdi which is a Starkey based framework will be open source soon it's written pretty generally so that basically if you have a function that you want to prove over a set of inputs in parallel you can use that and write a circuit and we'll open source it soon and we want people to contribute and collaborate on it and use it for whatever like parallel functions they want to prove and then organic based wonky 2 verifier is actually already released under an MIT license it's open source it's available at that link and we would love for other people who are using ponkey2 we know it's like a proof system that has a bunch of users uh because it's very fast to use it to verify their proofs on ebm and also collaborate and contribute so if you're interested in using any of these things you can check out the GitHub repo and then also talk with me after and ask if you if you have any questions about using it thank you very much Uma [Applause] 