hello everyone I'm Alex Evans I work at band Capital crypto we talk a lot internally about applications and infrastructure and sort of the ways in which they interact and we're particularly interested in ways in which those interactions might change uh particularly with modularity being a key driver those who want to share some of those ideas the hairbrained or otherwise with you all today so and I'd say for most of my time in the space the nature of the interactions between applications and infrastructure have been mostly harmonious um with some key exceptions and some of them are sort of listed in this slide and coincide with the end of the last two uh respective bull markets and remarkably sort of parallel stories we're right at the end of a cycle an application draws in a lot of excitement and fervor and interest crashes all the infrastructure a bunch of people get frustrated we get some cool scalability ideas out of it uh the application developers go off or want to launch a new chain and um I'd say that this sort of four-year four or five year transition here like occurred while ethereum got meaningfully better as infrastructure in the meantime right the block limits unlike some other blockchains increase that roughly the rate of Moore's Law actually a little bit higher with burst limits given EIP 50 59 which was a major Improvement roughly Forex reduction in call data costs on a relative basis as well and that's even before things like the merge The Surge the Splurge you know stuff that sort of started happening later in in 2022. right but I'd say qualitatively the nature of interactions between applications and infrastructure during this period didn't change right so Applications had a wholesale choice to make when choosing what infrastructure to deploy on and what I mean by that is you embrace all the constraints as well as all the positive aspects of infrastructure um by deploying on there or you choose not to right and you can choose to deploy on ethereum or Solana or both or some combination or launch your own chain but that's sort of the type of interaction that you have as an application developer with the underlying infrastructure that you Deploy on uh and we think that's about to change and we use the term better here and not necessarily better just there's qualitatively different types of interactions that we think will be available to both infrastructure and applications that are enabled by succinct proofs and in particular the horizontal scalability that's a sync proofs in different forms enable so that's I include underneath that things like datability sampling optimistic systems snark based systems and so forth but just to make this idea very very concrete I'm going to go through two general examples that also a little bit of audience pandering and that I know a lot of people in the audience work on one or the other of these two things or maybe some combination so I'll use snarks as an example in which this horizontal scalability of infrastructure enables new types of applications and then I'll use data availability sampling as an example of a changing the interaction between apps and infrastructure in a qualitative way you could swap these two but I just want to make it concrete so I'll just go through each and each one in turn and just demonstrate the principle so starting with modular and easy K and by the way I made these slides on the plane over before I spent the last two days at Zeke with ZK content there's more ZK content today some tomorrow so I'll go through these relatively quickly um but I think most people at this point even just in the last two days have seen sort of this diagram right um and realistically when I was first looking at the space that you would see these papers there'd be just like these entirely monolithic constructions for the most part at least from where I was sitting right was like hey here's my snark or Stark and it's like fully featured and like it's better than this other thing in the literature or asymptotically or concretely or uses fewer assumptions and please accept my paper into your conference um and I'd say like over time and in particular more recently you know you'll see things that come out that focus on just one of these components what are these components so you start with like a front a program written in some high-level language compile it through a front-end to a set of constraints using interactive Oracle proof to reduce that checking some evaluations of different functions and commitments against some commitments that approvers made and using sort of a functional commitment scheme or polynomial commitment scheme and maybe fiatrimir you produce this short proof that you can circulate around a P2P Network or post on chain or whatever right so the point is researchers developers can just focus on one of these components right and achieve material advancements to the state of the art even in some of it by advancing one of these Sub sub components of these right and then they get reassembled back into uh more General Frameworks and I'd argue something like that happened roughly in the 2019 to 2021 era where things like plonk came out and high degree Gates became a thing and lookups and you know interesting advancements in polynomial commitment schemes they sort of got reassembled again accumulation in the sense of Halo and ultimately swapping out clunk and creating the framework Halo 2 which a lot of people use these are general purpose Frameworks that combine the modular component improvements into General Frameworks right that people can use and it sort of marks a transition between at least when I was looking at the space you know 2018 most people using some sort of growth 16 variant but circuit the sort of Marvel computer science you know 10 years leading up to it or maybe even more right um you know but circuit-specific trust it's set up to what people call more Universal architectures in the case of Halo or plonky 2 resero things like that and we think this sort of architectural transition is enabled to fundamentally different things uh the first one is a transition from roughly more specialized architectures to more Universal architectures I don't just mean this in the sense of like you don't need to do circuit specific trusted setup I mean this in the sense of very concretely people are building ZK VMS out of it implementing an instruction set of the evm inside of you know that's provable um or um uh I think I lost the slides um okay thank you uh well they while they also boot up the light node in the meantime yeah thank you um so anyway the oops let's go back oh and I think we're missing okay never mind all right so yeah the key ideas is transition to you know people building risk five provable risk five chips and so forth like it's a very very conqueror ZK wasm or whatever like we sort of are going from to sort of more microprocessor bases I know these things did exist in the growth period but like fundamentally the the way they say if you look at ZK VMS they utilize these modular components like like not just recursion but lookups very extensively under the hood and so forth so that's kind of something that's been driving um new types of VMS and so forth we think the the more interesting thing that's been enabled in succinct proofs has been recursion it has enormous economic implications for the type of infrastructure and the types of applications that exist thereafter and again if you look at something like Halo 2 or you look at plonky2 you look at a lot of these sort of more second generation universal proof systems um they they roughly have the performance on a single machine of like something like a 1970s computer right like but recursion fundamentally enables you because it's proving it's very parallel to add lots of machines and as a consequence be able to amortize the cost of compute over for over a larger number of users right and so the analogy that I'm roughly drawing here is in the 1970s like the types of applications and services that made sense in Computing were Mainframe applications hence the system 370 analogy here that I'm Loosely trying to draw but you can this it's really expensive but you can amortize it over a large number of customers in the Enterprise right and if you look at most of what's happening in ZK from in terms of what's getting funded and what people are excited about it's mostly selling succinctness in some form in the case of Roll-Ups or things like a lot of you a lot of these by the way are things that have been founded and most of these have been found in the last you know two years or something something a little bit older especially on the roll-up side right so the ability to add more machines and horizontally scale without increasing trust assumption has enabled sort of this Renaissance of more applications that sell succinctness and this has kind of been um I'd say the area of the largest growth in ZK in the last few years so this includes things like Bridges and coprocessors integrated what I call Integrated Roll-Ups that build both quote unquote the processor and the roll-up but then also things that take modular components and assemble them kind of in a way that an upstart PC manufacturer maybe in the 70s would have done right using op stack or taking a chip from risk zero or something we happen to work with as a portfolio company for disclosure and then using it to build some ZK role or before I used to take 10 million dollars to build one of these Roll-Ups or maybe more it's now these Frameworks have made it a lot easier to just assemble them as a service so I like to contrast this with what's happening on sort of the client side of the market where you can't really take advantage as much of horizontal scalability add more machines because you're fun because fundamentally what you're selling to the client is privacy usually and so you're much more limited in terms of how much how much you can take advantage of recursion capabilities right and so as a consequence the types of applications you want to run are fundamentally things that you'd be comfortable running on a 1970s computer right roughly again the the 70s Hardware analogies may be a little bit drawn out too far here but the the idea is that application specific architectures were kind of still more useful at the time that said there are really interesting examples and uh of of interesting applications that people have been trying to take advantage of these new capabilities like the ZK the attested mic uh that Anna and Kobe did like these things take advantage under the hood very often of things like lookups and so forth that are relatively novel capabilities in these systems people are doing experiments in identity and you know shared shared Global state with private client-side information and a whole bunch of interesting experiments are happening but we think fundamentally we need more vertical scale to enable more interesting and expressive applications on sort of the quote-unquote PC non-main frame side of the market here's just a couple of examples of strands of literature that is producing crazy advancements continually these are not all compatible with each other yet and I won't go into each of them in depth there's cool things on error correcting codes there's really cool things on um you know fft free iops that work with Planck and customizable constraint systems is obviously people have heard a lot presumably about all the advancements in folding and a whole sort of strand of literature spending the last year and big table lookups that allow you to do a bunch of all these things are the only thing I want you to take away from this slide is like people are sharing like are shape turning like square root things into log things like they're shaving log file like in a lot of more mature Fields these would be breakthroughs now I agree these are like asymptotic and algorithmic things like concretely we'll have to see and they're not fully compatible with each other but very often the combination of these things once you are able to first of all they are becoming more compatible once you combine them they often become greater than some of their parts as we saw in the case of Halo 2 plonky 2 has been the history of the ZK space Also more generally the history of computing so we think what's likely to have where like this is aspirational at this point this is not real um there's a transition up kind of if 2019 to 2021 is a guide right that these components then get reassembled modularly get reassembled into general purpose architectures again um and that these Universal architectures um are then more performant than what we've seen and aspirationally that have sort of the Macintosh taking us into the 1980s um which you know I've talked to some people are working around plonky III and they can run you know roughly at this level of performance maybe a little bit slower than that something that is a chip that's like fully compatible for instance with a high-level language like rust so maybe that takes client-side DK into the 1980s which is where PCS and so forth and more client-side things start to really take off maybe but hopefully it'll be fun if it did happen um okay let's switch gears and talk about data availability sampling and one of the ways that we talk about this internally is as abundant highest Insurance underpinionated bandwidth of the word abundance should be clear they're just more of it uh Assurance is Nick just gave a great talk on how we gain assurances um by sampling um you know fully downloading data as a way of getting insurance as well right but maybe the point to motivate a little bit more here is this notion of unopinionated bandwidth it's available to roll-ups and again the Paradigm that we've been in in terms of how applications scale and how infrastructure scales infrastructure updates you know there's Solana today's but in the Solana last year hopefully next year it's even better um and then sort of applications get this choice of like when infrastructure would I deploy on and are they famously you know have the strategy deploy in a bunch of different EVMS have a bunch of horizontal deployments in a bunch of different places so you don't miss out on users and usage and so forth of course if we take web 2 as any guy these are just stereotypically you know customer focused companies from web 2 that I pulled up and like of course all of them take some some advantage of integration in the in the vertical stack right so Netflix with open connect Amazon doing fulfillment Apple integrating into Apple silicon these companies as they scale they have very strong opinions about how the customer interaction should work they don't like to import other people's or third parties opinions about how that interaction should work they have their own and so it's quite likely at least that some applications maybe not all want to take advantage of more vertical flexibility and importing the opinions of the infrastructure doesn't necessarily accomplish that for you so specifically what I mean is right now Solana has a lot of bandwidth available the way you take advantage of that is you launch an application on there now this could change very soon and I think a lot of people are pushing to change this it's fairly easy to do ethereum is becoming more unopinionated in the way that you use call data instead of use blobs and so we're becoming less opinionated in the use of bandwidth overall and then the other thing it's happening is the and I promise Nick this slide um uh what's happening is the supply of unapinated bandwidth is increasing uh pretty rapidly in the next few months and maybe a few years where again like if you look at ethereums at like 100 kilobyte blocks and roughly the block time you're roughly at like a 56k bit modem from 1996 and you're about to go to you know fairly more modern broadband connection right and like of course like interesting applications come out of transitions like this like web 2 coming out of you know similar transition to broadband um like within a few years of that um but we think what's fundamentally more interesting and by the way there's there's more vertical scale that can come from lots of improvements that are planned along these things like proto-dank sharding the Dank sharding those really cool things and how you do client-side decoding really cool peer-to-peer and networking problems and how you discover people that have samples in the network uh cool things and how you prove that you've done an encoding all these things like improve more vertically but what's more much more interesting to us and Nick sort of put this much more eloquently I think than I did in talking about user-facing wallets and applications running like clients is there's a different qualitative interaction between apps and infrastructure in this Paradigm which is an application could Garner a lot of usage and excitement and that could lead to more security for the underlying infrastructure by drawing in more people running light nodes and this is different than something like cryptokitties making the infrastructure worse for other applications at least temporarily right you could draw in a lot of usage and actually increase the assurances that everybody else in the network is getting and maybe even allow you to increase block size and performance over time so that's a much more we think virtuous interaction between applications and infrastructure or the infrastructure can scale horizontally as the application chooses how much vertical integration to do over time something similar I guess we noted as well in the case of recursion and snarks you can add more machines but the level of assurance doesn't decrease and that enables more virtuous interactions between applications and infrastructure and so aspirationally we think this is the Paradigm that allows infrastructure to scale a lot in terms of capacity but applications to have a lot more control over the stack that they ultimately deliver to the end user and we're very excited to see what types of applications and what types of interactions between applications and infrastructure sure emerge from this paradigm are we doing questions or I did save some time for anyway like a minute no okay thank you [Applause] 