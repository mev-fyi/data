okay so we are going to be talking about some exciting ZK applications as you may know dkps have only really become feasible in the last few years and there's been tons of experimentation going around trying to use them to extend and improve on existing technology so specifically we're going to talk about machine learning and a little bit on identity given the expertise of our panelists so we'll start off with introductions I'm ashita and I do research in the space hey everybody I'm Daniel and we started a company called modulus we work on zero knowledge machine learning I'm I'm lakshman I work on Persona where we think about what identity looks like on ethereum in a hundred years which turns out to use zero knowledge can each of us or can each of you walk us through the state of ZK Tech right now and specifically what's been built within and outside of crypto and what's still a work in progress sure yeah I think actually launchman and I uh agree on maybe uh a lot of this so I'll make sure to leave some protein to be shared but um kind of from our perspective and modulus the big scary thing about all this investment into ZK and its application to uh blockchain networks is that it's all focused on proving uh VM operations or EVMS right which kind of makes sense right we want to improve the entire execution of a blockchain network compress that effectively and then bring that on to the network that's doing the verification awesome but it leaves a lot of space for other kinds of uh kind of specialization right so in the case of modulus we're really excited about machine learning and if you know anything about AI operations you know that it's super repetitive uh it's highly structured and it has these architectural features that represent a down selection of all possible VM operations right and so I guess our Insight was that potentially if we build a ZK system that's you know purpose oriented for machine learning then we can get much better proving overhead and actually this this is a little bit of what Harry works on as well um but uh yeah so that's kind of our orientation we think that might be true but yeah yeah I mean we were we were just talking about this backstage but a very similar observation that um you know what what a zero knowledge proof isn't the ideal there's there's many different parameters you can improve on based on what specifically you're trying to do um and so a lot obviously most attention is on scaling VMS which I think is is a good thing um we we think mostly about the zero knowledge side and specifically uh getting things to work in a very resource constrained environment like it's super important for a true zero knowledge application for like the users of the application to not be delegating the proving anywhere else or sending the private information anywhere else so we spent a lot of time thinking about this very specific problem about making these very specific ZK circuits I.E like proving a signature for giving crypto system work on a mobile device and that's let us down like a pretty unique path in terms of in terms of proofing system and stuff like that and um but at the same time it's it's it's super inspiring seeing we we take a lot of inspiration from um all of the stuff happening in the more General General systems because um you know the thing I'm learning I I'm honestly um not a cryptographer by training I've been learning a lot as we go but the the uh the moving pieces of a lot of these systems are kind of very common um and so we're learning a lot just learning like seeing what's being tried and understanding like the different trade-offs between using the different components of these systems which is cool hey harry you want to do a little intro and we're just talking about the state of ZK Tech um so talking about what's been built within and outside of crypto and what's still a work in progress awesome um so yeah I'm Harry I'm one of the co-founders of Jensen we're a machine learning compute protocol which uses proof systems in order to lower the costs of verifying the trading has happened um concretely we use polynomial interactive article proofs in our verification system which were quite recently popularized by Justin Fowler I think broadly speaking in the kind of wider applied zero knowledge space there's like free Trends which which come to mind um the first would be around the overalls kind of trend away from using elliptic curve cryptography into using more of kind of hash function security so you see that of examples like start net you also look at things like the kind of emergence of material knowledge um if you're in Virtual machines uh which assist with the The Wider kind of confirmation of a lot of the blocks in the chain itself so you see that with scroll but then most relevantly for us it's about very kind of operation specific forms of zero knowledge um so for example you see with z conduits a ziko library for doing um serial knowledge image classification like zkc and N they've uh they typically use kind of off the shelf um improved systems for that but we think that that can become you know much much faster like maybe free artitudes of magnitude faster uh if you if you specialize the the proof systems so that's a bit about me and then the kind of free areas that I'm excited about thank you so first let's touch on identity I'm excited to see what you say about this so in the Advent of AI do you think it's important to distinguish Bots from humans and how how far can biometric verification take us yeah so I I've I you know it's funny I've been having this conversation with a lot of people recently and I I'm I'm in a like strongly opinions weekly held per place of Mind where uh I want want people to tell me I'm wrong but also I feel it's something feels quite true about what I think right now which is uh I think there's like a conflation between like Ai and Bots which is humans controlling AI um in the sense that you know distinguishing between it's it's if if we're distinguishing between like a human having made the signature on some piece of data um as whether or not it's like an AI a human could could stamp a like could stamp a thousand pieces of data and send them to like a bunch of bots and distribute to a bunch of people and I think that's that's the thing I think a lot of people are scared about when they look at like Twitter Bots and like sort of like connect that that problem with sort of like the value of distinguishing Ai and humans and and I think that that's really distinguishing humans from humans with Bots which is which is a which is slightly different um as for like literally whether or not it's good to distinguish humans Nai uh yeah I I I think um you know in in the stuff we're building we think a lot about the importance of exit in our systems we want humans or AI to sort of like be able to shed their old identity and form a new identity like very easily this feels like fundamental to you know some of my personal values and you know some of the old crypto libertarian crypto Anarchist values and um and feels like a good property of a system to have and it's hard to have such a system if every human is only one Identity or is identifiable as a human so I think that's quite challenging I also worry that if the probability of like AI Doom is high then any system that is good at identifying humans is really like a human directory for Bots to kind of or for AI you know like it becomes a tool for control by not necessarily something that humans are using anymore so it's a very it's a very nuanced question I don't know I don't know what the answers are here but I also think that the there narratives are probably a little simplified yeah you guys have anything to add Harry I've got a lot to add I think I am I think it's important for two reasons um the first one is there will be I reckon in the next nine months a major public outcry about the use of um or I guess the acquisition of other people's identities and it's used to defraud people you know with the state-of-the-art kind of generative voice and vision models it will become essentially impossible to delineate real from fake and a really kind of clean way of you know solving that problem is by somehow proving your humanity and then using that proof to say chunk and sign any calls or or um or kind of you know video that you you create on chain so you can imagine that if you were in like a in a zoom call at this simultaneous type of Zoom call you could be kind of independently confirming that yeah you're on that call it's you it's your private key Etc uh that's the kind of short run benefit of it I think it's just more of a kind of uh security and fraud piece but the longer term Advantage I think is you know we see a Jensen within the next five years the majority of compute being requisitioned will be by machines not people we think that the kind of substrate for artificial general intelligence will be on chain and when that happens you get these kind of very like philosophical almost like Blade Runner style questions about you know who's real and you know who has rights and whatever rights of the kind of autonomous beings out there I think having you know a way to confirm yourself is essentially the same as having a passport or an identity in general and you need that to function in a world where we're kind of coexisting with another artificial species how do you think we can be collaborative in the future foreign yeah I think it starts with the ability to I think kind of what's already been mentioned but I just stamp one as one and one the other as the other right so in some sense what we work at UH on at modulus is the ability for AI processes or AI models or AGI one day potentially uh to stamp their work right um we do the is model kind of process right um I mean the way we do it is using ZK circuits and generating a proof that's verified somewhere but you can imagine uh the analog being like an artist signing their uh their their their Masterpiece and then giving it to somebody and in that signature being what testifies to the authenticity of that work right in fact we're in the process of taking a generative model in this case it outputs pixel art but and putting it on chain via this like ZK process and embedding it in kind of the nft container right so the the actual machine artist is itself on chain and that's really interesting right because that suddenly this this like AI process is distinguishable entity and obviously like you can imagine lots of these different agents interacting with each other as a you know a kind of collaborative process on chain because there is that agreed upon immutable uh basis for reality there so that's like One Vision there's lots of other ones um but I try to get very very specific because AI is so expressive and it's so easy to get into the you know AGI conversation I'm still trying to you know get my toast to come out right every time so there's always more Nuance right and more kind of process to get there I'm actually kind of curious to ask ask both of you um and just I mean just to push back because I I think I represent the other perspective like what why fundamentally it is is it good for uh Ai and humans to sort of be distinguished online and and I think one one sort of you know this is more of a feeling than a like a rational like thought but you know uh it seems conceivable that such a delineation creates less empathy between the two agents in the very long term um like I think uh I I'm sort of maybe naively envisioning a world you know what where there are these more powerful beings out there but you know we're all sort of you know doing the thing and it doesn't matter who's who you know um yeah I'm just I just would like to I'm curious to dig into that a bit more yeah um so for me it fundamentally comes down to liability a good example this would be autonomous driving if you have a car which is self-driving and the car crashes into someone do you think you're as responsible as if you were driving it if the answer is no then at fall which is for me then it follows that when you have a brain machine interface and you know attached to your skull in approximately 10 years and and that interface is using a neural network to you know expand the you know the scope and memory that you have in your mind and also making some decisions for you like maybe it's filtering your emails or something or maybe it's doing even more things are the actions of that model your actions I think the answer is also no so that requires there to be an individual identity for your gun the organic part of your brain and the artificial part of your brain yeah I think the liability point is very concrete but uh maybe the the other orientation to tackle this this precise kind of premise around empathy uh and to be clear I don't think it will be binary right there will be some operations and some ambiguity as a default and then there will be certain kind of properties or processes that are sampled signatures in the future that's kind of my impression of things uh but I actually think the the the better path to having empathy for one another or making the collaboration more productive is the capacity to delineate bot from human right uh you know you just look at kind of stability Ai and Sable diffusion and all this uproar against like I'm an artist I feel like my artwork is being attacked because um you know it's being used to train uh models are much more performant than I can be at a really high scale we'll look at the Hollywood kind of protests right now between the writers and the actors saying I just want you to put in the contract that you're not going to replace me with an llm that's very performant right um I don't think it's at all practical to put the brakes on the you know life-changing technologies that can be very productive for society like llms or generative models but I also think that if we want to build like some uh like this is what I guess blockchain systems are really good at right which is like ground truth right like this is the the reality that in the digital world anyways that we can all we all have to agree to so if we can take advantage of that property and put the things that we want to differentiate or delineate on that ledger then that for me anyways is a more egalitarian way of uh approaching empathy right between humans and agents yeah that makes sense I mean I I I suppose there are also humans whose factors on chain make them see more AI than some AIS right like yeah yeah like I that that's that's a fair point yeah it don't enable gays in this too much and and take the panel away from actual applications of CK but we can talk I'm excited to jam more on this later yeah that's interesting um so we can switch gears and talk about zcam a little bit um so Daniel and Harry what's sort of the state of machine learning at the moment and the role that trust plays um sure yeah I guess um you know from my angle um your modulus we spend a lot of time thinking about costs I'm sure Jensen's actually quite similar in this regard uh in part because to me as a you know we all kind of we're doing AI research before this right um the story of AI is a little bit the story of cost right like what happened in 2009-ish that suddenly deep learning as a method for training models and then generating these like emergent amazing magical outputs uh why is it that 2009 was this pivotal year where Decades of theory suddenly got put into practice uh for me it's gpus and the fact that this this new like Computing form factor became widely accessible and like look you can build a GPU server Farm out of CPUs it would just be significantly more expensive right so we brought the cost of compute down significantly made it much more accessible and in selling these these methods of training models um deep learning right often time zone um gave us the models that we are all like really excited about today right and uh you know for for obviously we can draw parallels to the ZK world and circuits I think that's later in the conversation but maybe I'll just end by saying um it's this it's like super non-obvious right like it's not like if I get more parameters my performance on my model is like linearly better it's this weird step function looking thing where emergent properties just show up as models get bigger and more sophisticated um that doesn't seem to be changing anytime soon so we're just all marching towards more parameters and more compute and more electricity and so forth um I'm sure that like blows up Jensen's Tam into something amazing which is awesome I'm very excited for Harry and Ben but um I think from my angle we're just at the beginning right uh because although that is the dominant Trend there's also the trend of more sophisticated architectures Better Learning models or Better Learning methodologies right uh quantizing our models to save on electricity and make them more ZK friendly there's all these subfields of research which are emerging as well underneath the current of more compute more compute more performance more attention more money um so yeah AI is uh most likely or almost certainly going to be the most expressive Transformer transformative technology of our time besides maybe crypto but I'm biased yeah I would just favor on all of that um for anyone who doesn't know the kind of tour for through the force of uh of deep learning came out in the kind of late 1940s the neural network architectures then during the kind of 50s had it's first applied use case predicting Weber for the U.S Navy big like kind of winter in between late 2000s stacking different layers of a neural network and combining it with something uh called stochastic gradient descent so basically the idea that you are comfortable with the model learning are slightly randomized but more efficient way combined with the compute power that Daniel mentioned caused an explosion in image recognition models so like the classic was like predicting is it a cat or like you know is it like a Muffet or is a dog or is it a muffin and all this kind of like these kind of funny toy examples and then throughout the kind of 2010s we got to the point where around 2018 2019 Transformer models came out Transformer models were basically uh used a concept called attention where you have different heads on the model you could think of like a kind of hydro of lots of different heads and they're each paying attention to different things in a sentence and then more recently kind of the fusion models came out I think core to to our thinking around the kind of reasoning for using zero knowledge machine learning in this space uh is really what Daniel said it's around cost so just kind of two important things here generally speaking if you throw more gpus at something it gets better the kind of scaling laws do hold we haven't broken them yet um and number two there's enormous kind of margins charged by the cloud oligopolists like AWS and Azure around 70 to 80 percent if you can find a way in a decentralized setting peer-to-peer with no middleman to train these models you essentially increase every dollar spent on training by four to five x the best way to do that in our opinion is to use very lightweight proof systems and then immediately you find yourself in this kind of conversation around showing almost machine learning because you don't want to set you don't have lots of people trading the model it gets super expensive a good example of like an attempt and that would have been true bit by a touch and others in kind of 2016-17 uh which was a really good kind of first step in the direction but it didn't go far enough to getting the overhead down I think we calculated it to be like 6X which get basically adds the margin back on um however with the recent advancements and perfs you get that cost way down we anticipate that the cost right now is about 25 uh in Jensen to verifying the the models being trained which makes it you need economics substantially better versus the cloud Giants So Daniel could you discuss some of the trade-offs between using um ckp's amongst other Fair like like fhes or something else with varying to ml sure yeah I I guess um um you know ZK has become kind of a dominant narrative within the crypto Zeitgeist because it has this like kind of bizarre property that we call succinctness that makes it a great fit for blockchains um but it's always important to remind ourselves especially here in the ZK world that there are in fact other cryptographic techniques um and they do different things and they're all super exciting right fhe kind of I guess uh fully homework encryption uh is most um I guess by default associated with privacy and the ability to operate over data without understanding or having the capacity to understand that data the actual like content of that data um so you know there's a lot of privacy applications on that and of course there's NPC where you want to split up some key or any number of other techniques as well within the the Canon of cryptography right a lot of which is often used in the crypto industry I guess what I'll say on that is um and not to hammer a point constantly but is nonetheless cost right there is always a cost to doing these sophisticated cryptographic operations than to not do them right in fact like when we talk to our early customers the first thing we always ask them is can you get away with not using this right just like don't do any crypto just do it the normal way right um do it centrally are things broken okay then we can talk uh the cryptography right because it's always a premium and so the that that's what I try to focus on just to make things really specific and practical again but certainly um when it comes to the day-to-day work of what it looks like to marry machine learning and ZK we actually still have a lot to learn from these other avenues including fhe right which have been dealing with the intersection for a lot longer just as an academic discipline um so a non-trivial portion of my time is spent talking to academics in those uh kind of disciplines as well so how what has been achieved so far as representing ml models as a circuit like are we at the point where we can express really complex models with a lot of parameters or has its just been pretty much r d early stage um yeah training maybe yeah um with training uh it isn't at a level yet so if we think about the circuit generation or even as Daniel said you know looking at from a fully homomorphic perspective it's just it's just too slow for our use case which is generic training without the kind of privacy component it is just too slow but we do use it for though is inference within training to monitor model loss and that works well um typically because you don't have to you you don't have to a put all the data through you can just put like a batch for um and also because you can typically you know check point loss in a kind of in kind of intervals as opposed to having to do it all the time um and the benefit of course of being able to monitor the model's training uh like learning I guess over time is that things start to look unusual you can then pinpoint kind of audits around the area which looks unusual which makes this the whole system much more secure I think broadly speaking there's a kind of there's always going to be a lag particularly with the fully homomorphic stuff um behind the the kind of state of the art just using a GPU kind of locally with in kind of clear text um but there are definitely use cases for as Daniel mentioned I guess laksham what does that look like in like the world of like signatures right which to me right in the machine learning world and probably Harry as well it's like oh signatures should be relatively straightforward um but maybe does your focus on like client and devices change that yeah so there's there's one there's one like um so so one of the really um challenging things that I think is true of a lot of like ZK applications is we need to sort of like adapt sort of the length or the the the numerical Primitives of sort of like whatever whatever we're trying to do to some field some finite field um I think a lot of our challenges have come from doing um trying to do arithmetic in a field that's smaller than the field that of the cryptography we're actually trying to do wrong field arithmetic and this is like really untenable on a on a like a mobile device because the the witness of the ZK circuit becomes really really large um so we've spent a lot of time so so literally like the memory requirements become very very big that's the thing that'll probably heat up your device if you're using like some other other zero knowledge proof stuff and I think um so we spent a lot of time kind of trying to see I think I think broadly have the premise that there are certain crypto systems and signature schemes that'll probably be 80 of signatures that matter matter in the world but then kind of want to be open to like making statements about many different systems so uh We've we're choosing proving systems based on their ability to be agnostic to the underlying field so that we can kind of like change that up as we go um but yeah it's a very it's a very different much tighter and scoped problem but also one with the constraint of you know like memory on a on a mobile device yeah okay very interesting and part of why I ask is because we also struggle with memory now the scale might be a little different right it's like vgpus and AWS and Azure sorry Harry for now um and uh you know it's it gets very expensive very quickly because um you know oftentimes it's like the peak memory consumption in the proving process right and so we need to rent a massive machine even if we don't use all the capabilities of that machine most of the time right and that's just a product of what proving systems often are but that doesn't make any sense when you think about the structure of machine learning compute right which is like generally pretty predictable very consistent um and so the I guess our Insight when we bumped into that initially in our early days of of just like looking at this almost as a scientific curiosity is this proving schema is not appropriately acknowledging the fact that we're down selecting for a more specific kind of problem right um and maybe the two Harry's earlier point about building custom provers which are actually tailor-made for that per like that that specific function I think one of the narratives which will pick up some more gas again in the crypto Zeitgeist if I'm allowed to Hazard a guess here is this idea around specialized proving CMOS or proving systems um and I'm very excited for modulus to put in our kind of spin on that as well as Persona or Jensen but um you know in some sense a proof is in the pudding performance one of those um and I think when y'all see these numbers it's like man for these really intensive applications whether it's training inference for machine learning or for proving anything on a mobile phone on an edge device it makes sense to go down the specialized route and once you do the water is warmer the sky's clearer your spouse is more attractive your kids are more disciplined and you know life's just better so um this panel I think actually represents like one side of this debate um unfortunately we all agree with each other maybe that's not very exciting well there are some there are some other like for I don't know if this is true if if lookups are something you guys think much about but I think in my perception there's like you know this one day magical technology and improving systems like a lookup table basically and at some point it'll be economical to just do it do whatever we want with the lookup table and we're sort of like following the space and we expect at some point that curve will cross the way we're doing things in a very specialized way but also maybe not um I'm also actually on this topic and just because I have like two experts here to ask about it what's it like doing like floating Point arithmetic in uh in a field I mean that's a brilliant question we spend a lot of time thinking about this where basically we all right for our system more broadly we need reproducibility in in the kind of model training process there's kind of like there's four places it gets ruined the first is machine learning Frameworks typically just aren't reproducible they have Randomness in them obviously like random seeds Etc a bit lower down you then have you know if you're training in a decentralized system you have all the various devices because we don't use a standardized like piece of Hardware it's lots of different kind of gpus from different manufacturers mainly the video um and then they're all different then thirdly the way the gpus kind of compile the the kind of the code the way they execute them in the kernels are all different so we're actually having to rewrite lots of the kernels ourselves the carrying GPU Engineers to do that so that our runtime does it and then you get even more abstract that you know you can get like a bit Flip or something from like a cosmic ray yeah add all that in and it's a really tough challenge uh we looked a lot of kind of like model quantization of course the issue around the quantization is if you quantize it too much and you just you lose the kind of the signal so to speak you know then if you're plotting kind of lost but it's quantized to like three or like four bits or something like that you're not really it's just like a straight line so uh so yeah it's it's a really really good question the trade-off would be you know if you create hard you know custom Hardware which works really well with it then you come you kind of fall into like a centralized trap which is something that we think about there's a good example of um like internet uh computer protocol they went down this route of like we're gonna make this like one type of Hardware it's going to work really well they call them like canisters whatever to us that's just way too much centralization it also makes you into a hardware company which is you know makes a makes a hard problem even harder I mean I would just Echo everything that Harry just said uh I think the problem is it's acute but it's not as acute on the inference side um you know uh we we get away with a little more thankfully um but maybe just to attack the same question from a different angle you know when we talk to our partners and early customers never do they go how what is your quantization schema is it 8-bit is it like what happens when you like lose a couple digits when you blow up you know your your your floating Point into a finite field um they just go hey does it still work question mark right like how much performance do I lose when I need to prove to the world that I didn't manipulate my algorithm right um so if you want to take it from like the customer Centric approach the the pain point lies in that more Precision less loss usually means more expensive on the proving side right so um we're helped by the fact that again machine learning models are generally a bit squishier so they're a little more tolerant to to these kinds of accuracy losses um but it is very much an ongoing kind of research question the last thing I'll say is we're not the only ones thinking about it right because obviously when you quantize a model um first of all it's a lot more memory efficient and so all these folks working on llms that are eating into like massive like Hardware uh you know there's only three of these machines in the world and so they're super expensive they're really incentive to find ways to bring that memory footprint down and so we're helped by that research interest we're super aligned on that the other thing is when you quantize a model there are a lot more energy efficient as in literally electricity sipped from the wall right and so if you want to run some complex model on embedded Hardware on your smart glasses or on your phone or on your smart watch you know you want to quantize it and do all that so there are a lot of compliments and in the same way that earlier I mentioned you know ZK ml anyways we have a lot to learn from the fhe world because they've already been working with machine learning operations for a while it applies to quantization as well yeah I've had one final point to that again I agree with all of it it's just common thread on this panel but we uh we very much think of like the kind of I guess extensive quantization as being very connected kind of computational Liberty a good example this was around the Llama model which someone kind of quite quickly quantized when it was leaked and then you got kind of running on a Raspberry Pi and then that's you know there's a decentralization kind of meta point there about you know if you can actually make these things work on consumer Hardware devices in a relatively kind of you know constrained way that does a lot for people's ability to you know number one compute but also then to just build a different kind of models as well so you touched a bit on how expensive it is to build out a decentralized GPU Network specifically for machine learning um could you talk about why it was important to stay narrow and just focus on training machine learning models um and why would it would a developer or a team go for a decentralized network that's it potentially expensive potentially um yeah like potentially expensive potentially take a long time to put together yeah sure I think just kind of three points here um so our our kind of our founding my co-founder and IB kind of came from the machine learning world we didn't come from the connect cryptography or or wider crypto world and we Face these problems ourselves so he was doing a PhD my co-founder Ben he had like four gpus under his desk but he was competing with you know papers from Google which were using a thousand gpus and it was just completely kind of you know impossible and then I was an industry where we were trading um fire detection models at large scale and that you know burned a lot of a lot of cash uh so our our uh our thinking was number one it's important to get these costs down and that's only going to get worse with time which is proven true with the recent advances in the llm training on the numbers we're seeing the second point is that if people are going to use this number one they're not going to be crypto people they're going to be machine learning people so immediately they're kind of at odds with you know holding cryptocurrencies and everything that comes with it you have to drop into your workflow usually people will have kind of free clusters they'll have their kind of data cluster or these server training data they'll have their parameter cluster or the slowly updated parameters and then they'll have their compute cluster which is where the kind of the cache gets burned we we basically drop in where that compute cluster is I should say that for us the it's very expensive to build what we're building but it's expensive to build it and it's because it makes it super cheap to use it and then the benefit will be we anticipate a roughly 75 cost reduction to 80 cost reduction versus using AWS and a really interesting way of thinking about this is if you look at some of the rounds which are happening just now on the market you see you know inflection Ai and stuff like that they raise like over a billion dollars you could easily kind of bet the building that roughly 800 to 900 million of that's going on compute training costs and if you come to someone and say hey we can turn that kind of 900 million into 4.5 billion dollars for you that's Ultra compelling so when we saw that you know a couple of years ago we were like this is the most important thing and finally to your point about staying narrow we think that the problems in training are quite distinct from the problems of inference and we're very big Believers in the kind of Finn versus fat protocol dichotomy so lots of thin protocols makes more sense than one kind of big generic fat protocol for the same way that if you're actually building proof systems it makes sense to have super kind of custom improvers Etc it's just more efficient and effective so for us we thought we're just going to do training it's just going to be the kind of you know neural network training it's not going to be like statistical machine learning models and uh we're going to ignore all calls to sort of do inference and lots of people try to sway US during over the years during the the art boom like the stability AI like generative art and people are like oh you know you should do like art for nfts and stuff like this and we were like oh thank you it's interesting because we've also gotten the other direction we focus on inference and we're like we're going to stay narrow I see Jensen out over there and I'm terrified of them so I never want to compete with them and throughout our um you know past year or so that we've existed there's been a lot of like why don't you tackle the training problem right why don't you uh look at the opportunity space there um and yes plus one to everything here you mentioned um and then one last thing which is in operation these two kind of steps look quite different I mean that's the truth of it right so even if kind of from the outside it's like ah it's a uniform surface area right just machine learning what does that mean when you actually dive into the pond so to speak there are radically different animals right and it makes sense to have specialized approaches to tackle each problem statement so how big of a problem is it that models like open AIS is behind an API and these developers are just trusting the output that comes out of them right would opening I haven't want to in the future like build in a verification system they're just right now trying to keep this model alive yeah I mean anyone who's a student of History knows that when you trust huge centralized entities with run by small a small number of people with something as important as kind of as what Daniel said you know like well it's truth what's ground truth what's real what's not real it ends up getting pretty pretty dark like pretty very pretty pretty quickly so there's that kind of philosophically it's it's not a good idea I'd say number one but number two there's just also a kind of I I kind of I think a lot about the idea that AI is really just like the kind of artificial extension of your own brain and it comes to this idea that if you were if if you have to trust something to read your faults you know you read like it gets to a point where you just have to know that it is what it says it is you can't let open AI read your thoughts on like a high bandwidth BMI and just be like cool with yeah sure it's it's probably fine you know it's like Google was the the last like point where you could do that I think because most people what they think versus what they put into Google is probably like 99.9 percent true the result of that hold out there's always that stuff that you might not want to type into Google for various reasons particularly if you're not living in the Western World so you're living in a more oppressive country and you don't want to kind of you know expose yourself to have like different political views Etc if you've got something drilled into your skull and you think through the wrong fault and then that can result in you being detained or something bad happening as a horror story so it's a very good kind of antidote to the kind of tyranny which will be ushered in if it's kept centralized foreign honestly I'm just learning from you guys right now which is really sweet um I I'm curious um yeah I think I think it's like very clear to me why um from like from the perspective of like a company doing training why training verification is very valuable and it's obvious why from like a a human interacting with open AI interest verification is valuable are do either of these do you perceive that either of these sets of proofs are valuable to have on chain for some future notion of a chain or is it sufficient for them to just be kind of like move you know just being distributed on the internet I mean my thinking I've got lots of thoughts on that I think like a kind of slightly less like glamorous one is actually just for like tax purposes and so if you if you um if you sell your your likeness which everybody here will do you know in um in uh I don't know if anyone's watched the movie Her of Jacqueline Phoenix there's a scene in it when he's talking to um the the machine which is playing by Scarlett Johansson and she's having kind of like 5 000 simultaneous conversations that you know the fact that we are constrained is people to one Zoom meeting I think is like pretty bad number one is like dislike Zoom meetings intensely but number two because you know you could do a week sort of Zoom meetings if you just license out your likeness and it did all these things however you you you um you and then you yourself could be in you know the real you could be inserted in zoo meetings but when you you think about you know how if you would say an actor or something and you make your likeness available to a studio how can you prove that you were in the model you know how can you prove that it's in your training date how can you prove that actually they produced it with you and having the receipts for that on chain even if they're shielded in some ways very useful that's like a boring example tax but it has way more kind of like the for being more philosophical reasons I think it's a very very general or cool generalization of that which is that chains are really auditable so if you want it seen and understood by everyone then that's a it's a good place to put it yeah yeah I mean I I spend a lot of time thinking about um like why are we in crypto uh right like earnestly right like like there's a lot about crypto that is not great um a lot of grifters a lot of fraudsters a lot of ups and downs emotionally it's it's just like taxing my parents are not proud of me because they don't know what I'm doing um you know like I think a lot of us in this room can just like walk away and go into AI there's like more money there right now right more excitement more intention uh and for me it comes down to something very simple which is I like it here like I like these crypto values around like decentralization being building really robust secure systems and networks uh Distributing ownership in a way that you know my backyard Silicon Valley has been so bad at right despite all the prosperity that has been built over the past however many years um I like those values and uh you know it's built uh what I think is is this like very exciting Dynamic industry that we get to play in and then when it comes to AI right like here is a technology which is diametrically opposite in personality right well while cryptography which underpins the crypto industry is generally speaking very humble very discrete very specific this is a statement that I'm making and no further right AI is so expressive right it's infinitely expressive and it's so centralizing it's taking all these resources and Gathering it and so when it comes to marrying the two in a way that I think acknowledges what both are good at and also for me has like this like mission-driven Dimension to it where it's like maybe these values can travel Beyond crypto I hope crypto is huge uh in the future but maybe we can bring these values around like trust or don't trust verify uh out to the rest of the world right um it's like a really exciting intersection right to marry these really appropriately but then also leverage as a vehicle to bring these values elsewhere right we often talk about you get a credit score you get an anti-process whatever and you see like a Twitter verified check mark next to that score at least back when Twitter verified check marks meant anything but um and like you can click on that and like interface with the on-chain verifier contracts see the model card understand what that actually means for you as the end consumer and then potentially bringing that to the rest of the world right uh I mean maybe the rest of the world doesn't care that's fine but uh I'm very excited to try nonetheless yeah thank you this is really insightful we are at time thanks everyone [Applause] 