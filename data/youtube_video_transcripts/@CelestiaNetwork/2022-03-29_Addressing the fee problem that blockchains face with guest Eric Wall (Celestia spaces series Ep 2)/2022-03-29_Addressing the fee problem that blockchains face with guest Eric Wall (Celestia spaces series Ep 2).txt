[Music] and eric thank you for joining us you know you and i have chatted you know in the past before and i know you have some really strong convictions about the space let's go into some introductions uh i'll introduce the celestia leadership team first uh mustafa you know so each of you guys just give like a quick of like who you are what you do at celestia mustafa we'll go with you and then we'll go to the others hello everyone i'm the ceo co-founder of celestia labs sweet john hello everyone i'm john adler the chief research officer and also co-founder of celestial labs great and then nick i'm uh ceo of celestine labs and then we were supposed to have ismail who's the cto um but i think they'd be joining shortly cool and then let's kick the ball over to eric eric uh not only introduce yourself but tell us a little bit more about your core focus now a lot of folks may not know who you are would love a quick eric intro okay sure uh so i'm eric wahl i'm the chief investment officer of arcane assets which is which is a scandinavian cryptocurrency investment firm uh i i actually my my intro into like knowing about celestia starts with knowing john so i think i think john sort of popped up on my radar in was it 2018 or 2019 and i think i remember very clearly that you know john sort of popped up as this character that uh also was very interested in heavily criticizing um various cryptocurrency projects so my mission at that time like the thing that i was focused on the most was trying to identify the absolute weakest link in particular blockchain stacks so uh we'd look at things like trying to identify like where is the point of centralization in the system and where's the scalability bottlenecks in that system and john sort of popped up in this context and also shared that interest and i think that was around the time where i was known as the old coin slayer was back in 2018 2019 perhaps a bit into the into 2020 um and i i remember clearly that uh vitalik he he tweeted out like john's name and said like if you wanna if you want to look at what a good ethereum critic looks like you should look at this guy and he linked to john's account and after that me and john we've sort of been sharing notes on uh identifying uh like we looked at for example like header hashgraph uh their claims about their scalability and then comparing that to what the actual reality of the of the situation is and i suppose the big difference here between me and john is that john then went out to say okay i'm gonna go solve those problems so john took the path of actually like not only identifying the the problems but also trying to come up with what is the most logical uh design wise solution to those issues as well whereas i've just you know remained as this uh unnecessary non like uh non-productive complainer that just criticizes things uh but i suppose i mean now that we're here i suppose that john and the rest of the celestial team recognizes that even though you know that i sympathize with a lot of the the science the decisions that celestia has made that doesn't preclude me from you know i'm gonna i'm gonna do the exact same thing that i do against any cryptocurrency protocol which is i'm gonna try to identify the worst thing about celestia and then just stick on that thing and try to and try to squeeze out as much information about how the celestia thing thinks about that so uh i'm going in here with you know the gloves off and i mean that comes from i i think i think you know people sort of had to understand that um you know now that i've i've explicitly said that i like the celestial project it just means that i have to criticize it twice as hard in order to like keep the credibility uh in this situation thanks eric and a lot of folks don't actually know what celestia is and and all that mustafa can you give a quick introduction of celestia in the project sure so uh like dldr is celestia is a very basic layer one that does the core components that layer one should do and does it very well and scalably and nothing else and that's basically um consensus and data availability so you can think of celestia as a blockchain where you can basically dump data on it and it orders that data and makes it available and it's and it does it very scalably using a primitive called data availability something and this is um effectively within the modular blockchain philosophy where celestia sits at the bottom of that stack and then you have execution layers that are that can be based on roll-ups on top of celestia thank you mustafa and it looks like ismail just joined ismail what happened give us an introduction as to who you are i am the cto of celestial labs and i'm apparently not able to join the the first space as a speaker because i joined on the desktop that happened i was in the i was in the twitter spaces since like the moment you started it apparently john just wrote me that it uh gives you a pop-up that it doesn't work on web but i didn't see that oh good man glad you made it yeah and uh let's get let's get into it yes so i'm gonna frame the conversation and then we're straight up going into a debate and our strongest opinions so when i look at the l1 landscape i see a lot of noise and a lot of that noise conflicts i've gotten to the point where i don't know what to believe and not to believe so i wanted to host this conversation to understand some of the myths and the truths going around and from what i can tell there's a lot more myths and misconceptions in the l1 space than there are truths so in the next 45-50 minutes we will cover the major trends and forces at play that are shaping the l1 space and then we're going to talk about where everything is going so we'll constantly be going back and forth between misconceptions and truth and then ultimately the endgame we'll talk congestion fees induced demand bottlenecks scalability all sorts of things now speaking of fruits uh i think it's famously said and written that the only truth and constant in life is change and we'll begin there and the question that i'll open up for the group is what is the single biggest change that has happened in the last 12 months in the l1 space and then what's going to be the single biggest change that will happen in the next 12 months eric why don't we begin with you what's your point of view well i think it's quite obvious and this is not a change this is just like um an outcome that was already bound to happen from the start so i think that a lot of people in the cryptocurrency space they noticed of course you know fees going rampant on ethereum and then uh that has led to you know general mistrust towards the ethereum project whether or not you know it is the ideal solution to facilitate defy and then we've had all these other little ones pop up like avalanche and solana and many of them are based on the same premise that uh if you just increase the throughput capabilities of these systems then fees will go down uh so the what we can already see is that you know fees are going up on on avalanche and i think that the general principle here is that the more that log space becomes valuable the more demand will be created for that blog space so it's not necessarily so it's not necessarily the case that the reason that the fees are high in ethereum is because the the the block space is limited it's more it has more to do with every every single block in there there's a valuable transaction that you can make you can claim uh an nft air drop you can liquidate someone's position and there's a there's a a dollar value in into making those transactions that is worth spending in form of gas fees and if you increase the throughput uh so that you have just bigger blocks in ethereum then you're going to you're going to well it might decrease fees in the short term but it's only it's also opening up the door for more protocols to launch on ethereum and also and create more air drops and more uh liquidations uh that you can trigger so the the conflict that is here is is you know whether just increasing the throughput actually decreases fees and i think that the the solana system they have one a different way of managing uh congestion where they don't actually have a mempool so you won't see the same type of of gas bidding in the solana mempo because they just drop the transactions if they don't get in in a timely manner but what happens instead then is that you can't actually use the solano system because they're dropping the transactions and then they have outages instead of fee spikes so there's they have liveness issues in two different ways either the fees spike on like platforms like ethereum or or avalanche or you have outright outages so you don't necessarily see those fee spikes uh but what we can sort of understand from this is that you know no layer one that just focuses on uh scalability or just throughput at the base layer is actually you know meaningfully addressing a long-term sustainable solution to the fee problem and i think that this year like very recently i think that the tone the tone in the space and the interest is because because we have seen that these later ones don't necessarily address the problem in a sustainable way their needs like are there other ways to address this this problem and i think that celestia is sitting at the core of that so even though you know celestia is not live yet it doesn't exist yet there's so much of the intellectual uh of the intellectual debate that is happening around the protocol because i think that everyone that has been in the cryptocurrency cryptocurrency space for a long time is sort of understanding that at the base at the root at the basis of all of these scalability issues there is one thing that sets at the focus and that is data availability so if you want to scale some parameter of these systems then building a scalable data availability layer is where you should start now what i'm not so sure about you know is that you know is that the solution that does that solve all the problems or is that just creating the the optimal solution that also fails to solve the problem thank you eric mustafa we'll go with you next what's your take yeah so i mean in terms of the question like what has changed in layer one space um so i would say like if you looked you know like there was a period back in 2017 2018 when there was a lot of new layer one projects that kind of raised big realms and you know like you know avalanche lana near political and so on and so forth and a lot of them are kind of based around like a similar premise which is ethereum but more scalable or like you know a more scalable share smart contract platform and in the sense that you have like a you know you have a synchronous execution environment where all transactions kind of run within this you know world computer model that you know ethereum has proposed in 2014. like where does this there's this world internet computer where all transactions are processed um but now we've kind of seen the results of that play out like we can see you know 2022 that does not really work um like you just cannot have a single synchronous world computer uh chain like as a shared smart contract platform like you know you've seen uh you know issues avalanche and solana and bangladesh chain all had struggling to keep equipped with demand even though you know they seem to be more scalable than ethereum so i think the biggest shift we're seeing now is this idea of a multi-chain world and like it's no longer a world cup it's no longer a paradigm with a single blockchain that can synchronously process all transactions and now people are realizing that actually you have to have a portfolio involved um and that's like similar to the specifically the cosmos vision that was kind of introduced a few years ago and ethereum is also taking this approach because roll-ups themselves enable are basically a implementation of a multi-chain world because rollouts themselves are chains so effectively ethereum's having heading tables this direction as well where um old execution happens on roll-up chains that connect to you know the area of main chain as kind of hub so i would say that's kind of the biggest shift like this shifts between a single world computer model to a like a mini computer model where you have multi-training world like where the chains communicate with each other but obviously like you know there is no perfect solution um like all scalability solutions have trade-offs um and like the biggest trade-off of this like multi-chain ecosystem paradigm where everything is a roll-up and all the execution happens you know of chain and is this kind of issue of compressibility um like the great thing about this world computer model where you know just just run all your transactions on solana is that you have you know synchronous uh compressibility and yeah you can just write a smart contract that cause any other smart contract so but with this multi-chain model even though it's theoretically more scalable the the main drawback of it is that it makes composability more complicated not impossible or not too difficult but more complicated and more cooling is required and like for example like we haven't like ibc was only launched somewhat recently um and only people are realizing that uh so let's say they don't know ibc is cosmos is uh entertained protocol and we're only kind of starting to see that and flesh out and see the potential of that john let's get your point of view again the question is what is the single biggest change that's happened in the l1 space for the last 12 months and then what do you anticipate then the the next big change to be in the next 12 months go for it john sure so one big change i i saw the layer one space or just the blockchain space in general over the past year has kind of been uh an acceptance of the modular blockchain vision uh which uh for the well i mean if you're if you're listening to this you should probably at least have heard of it uh but for those of you not familiar it's essentially the separation of data availability uh on the consensus layer and then doing execution at the higher layers uh so the kind of the acceptance that this is the path forward to get the best blockchain stack you can get uh that doesn't necessarily best doesn't necessarily mean like uh you know it's uh the most scalable in all respects necessarily uh but that you know it's the best cross-section of properties uh i expect in the next year uh not 12 months starting from now but like you know next year or so uh to see a large proliferation proliferation of execution layers uh because as we get scalable data layers such as celestia launching uh we need something to execute transactions on and the something is going to look like a multichain world like what mustafa said the original cosmos vision of multiple uh not just multiple but many many execution layers that communicate with each other now that shared data availabilities layers like celestia will exist shortly this means that these execution layers can actually share security which is an important property they were missing uh before celestia came on the scene so you kind of get the best of both worlds where you get the scalability benefits of sharding without all the complexities and shortcomings and open research problems of sharding uh which is a large part of why various projects that were initially pursuing sharding eventually gave up because various open research problems so i expect to see in the subsequent uh 12 months uh wide proliferation of execution layers that do different things that try different things because now we can experiment with a variety of execution systems without having to worry about bootstrapping your own validator set without having to worry about bootstrapping your own consensus network you can simply deploy it as a rollup on top of celestia and benefit from shared security that all other rollups also benefit from additively ismail give us your take yeah i think i'm pretty much aligned with what mustafa and john said here there isn't much to add um i guess especially coming from the cosmos ecosystem for me it's like particularly powerful to see a vision coming together where you have application specific chains but instead as john said previously instead of them having to bootstrap their own proof-of-stake bft consensus networks they can run as roll-ups on top of a data availability layer on top of celestia and um yeah for me that's that's um that's gonna be huge and um um yeah i think other than that i guess like every every everything that john and mustafa said um yeah this is just what i can align with yeah okay yeah he did small does have a cosmos background and it's an interesting perspective nick on the other hand uh is coming from harmony nick same question to you uh macro changes last 12 months next 12 months what's your opinion i agree a lot with eric that um you know harmony and i think the whole sort of like cohort of scaling solutions that were built you know from like 2017 2018 until now i think we've kind of seen in the last 12 months that they do help alleviate the demand for for block space but ultimately they don't seem to be like a viable solution long term and um i think that's that's also kind of led to people realizing why cosmos for example has had the multi-chain vision that you you kind of need separate execution spaces i think to have a scalable um sort of blockchain infrastructure and so i think the next 12 months are going to be uh sort of like the maturation and development of this modular blockchain infrastructure i think it's to be fair it's still very early right so you know celestia is quite far from launching mainnet we have a bunch of roll-ups some of them are already live most of them are still sort of being built and i think in the next 12 months we're going to see this ecosystem emerge and mature and i think a lot of people are going to start to realize that i guess like this combination of a scalable data layer with uh sovereign execution spaces as roll-ups is at least so far the most compelling solution to scaling and a lot of other problems that we face you know interoperability um included in that so eric and i were chatting i think last week and this question came up in our conversation can celestia address a fee problem and it really caught my attention and ultimately helped trigger this conversation i want to go deeper on on this topic of fees and congestion eric uh go a bit deeper share some of your strongest opinions on some of the misconceptions around fees and congestion and so and the underlying truths at play that many aren't seeing possibly sure and i just want to i think that we like failed to clarify one thing when we were talking about this previously uh so we we mentioned the cosmos system and how they're doing parallel application specific blockchains and i think we should need to um clarify a little bit more how celestia ties into that and sort of what the vision is and i think one very interesting thing here was i was talking on twitter about like two three months ago broadly about uh what's going on in the blockchain space and zakimanyan commented underneath the tweet and he said didn't everyone just adopt my thesis from five years ago and i thought that was kind of a stretch so i responded to zaki and i said but cosmos felt thin until celestia came along and so zaki responded that celestia was the original idea that jay and ethan pitched to him and that they call it supertanker and then zaki said that it wouldn't work and then mustafa came along and invented data availability sampling and now you all of a sudden have exactly what cosmos was going for apparently five years ago and i think what people need to understand is that when you have uh in the cosmos ecosystem you have these zones each zone is a blockchain and each blockchain has its own security parameters and validator set the problem is that you know if all of these are communicating with each other using ibc uh if one of these chains are reorg then that causes problems for the other one basically you can rob the bridge uh in these systems if you break one blockchain but but not the other one so what you sort of need in order to build a shared security for this application for these application specific blockchains you need a shared security and that is because celestia is a data availability layer but it's also a consensus layer it's also the thing that can order the data that has been seen in the system into a chronological order it means that all these you know there can't be reworks in one of these application specific chains if they use celestia as the data availability layer because celestia is uh creating this red thread for all of the zones which means that they can get shared security so i think that that's sort of very important to to sort of understand how these things are coming together and why celestia is sitting at the focus of this conversation but now to talk about i mean the reason the reason why the cosmos project has been interesting is because the nice thing that you get from these uh asynchronous execution environments like these application specific blockchains is that the fees can be very high in one of these apps but it doesn't mean that the fees are going to be to be high in the other app so if you have one blockchain specific application specific blockchain that does you know something like uniswap and you have another blockchain that just does payments i mean people are fine with paying high fees on on a decentralized exchange because they're buying a token and they're hoping for the value to go up like thousands of dollars into the future you're fine with spending like say five or twenty dollars to to enter that position but if you're making a micro payment like a microtransaction in another chain uh or if you're doing it in another chain of course you want the fees to be low in that other system and the problem that these systems like avalanche or no avalanche has subnets but generally speaking monolithic blockchains that are have a single execution environment in these systems uh you you don't want the fees of one application to drive up the fees in other applications because that's how things become unscalable so what i what i want to discuss with celestia here when what my sort of dilemma or puzzle is if there is let's say there is an application specific like an app an app that runs on celestia through an execution environment through a smart contract it runs with it uses celeste as a date availability later if there's tons of demand for that specific application then that creates block space demand on the celestia based layer and this impacts the entirety of the celestia system because while celestia has tons of block space it is not infinite uh so if the f if if if we get one very very application a very very popular application that uses celestia what i'm what i'm trying to understand is whether that you know leaks into all other celestia using applications uh such that if you have a micro payments uh application that also uses celestia as the data availability layer is that feet pressure going to leak into the other applications i think that the obvious answer service yes right but we don't really know how bad can that get like what if there is one application that is so popular that it just you know the f it drives the fees up on celestia to five dollars per transaction like is this a is this uh a catastrophic scenario for celestia or is that still viewed as a positive outcome mustafa please yeah so yeah i'll respond to that but i also want uh john should all i'm sure john also has lots of opinions about this uh so the short answer is um yes like uh like first of all like no blockchain can guarantee cheap fees if any blockchain you know guaranteed you know cheap fees then it would be vulnerable to denial of service attacks uh because like someone could just spam it with free transactions um so like in celestia and and which includes it in celestia and every other blockchain that you know is not vulnerable to enough service attacks there is a block size limit um and that's in celestia there will be a block size limit now the question is you know what is that block size limit uh well there's several questions here so first of all like what is that block scientist and how do you how do we pick that box size limit and how does it scale and um secondly like what is the maximum potential size uh for that block size limit compared to like a monolithic you know blockchain like lana for example so first of all and so one thing i've said before is like we define scalability as equal to the number of transactions or the throughput that transact that blockchain can handle divided by the cost and for an end user or like client to validate the correctness of that chain and this is the fundamental reason why for example ethereum or bitcoin would not just you know increase the block size to something insanely high because like end users would not be able to you know run the full node to check that the chain is correct now this is this is kind of contrary to solana for example where solana have taken the approach where they don't care about end user verification and they just want to scale you know throughput um as high as possible on the monolithic chain so for the bottom part of the equation which is the cost for end user to verify the chain the the property in celestia is such that um because we use a primitive called data at the example the more users they are in the chain uh oh the more likely there are on the chain doing data availability cycling then the higher the block size because we can we can increase it to like safely without compromising the ability for end users to verify the correctness of that chain or in other words to check that the chain data is fully available and that's because of data availability sampling any any like client can verify that the chain is fully available by only downloading a very small piece of that chain and so that's one side of the equation which is that we can scale with number of users or like any blockchain including you know when ethereum 2.0 adopts it any blockchain that supports availability sampling can scale end user verification uh with number of users but then the next question is um so like scaling end user verification is all you know well and good but how about like actually increasing the throughput of the chain or the block production of the chain so uh because that's that's that's the the other end of the equation which is like how many how much like data how big block how how big of the blocks can um like the actual miners or the validators or the stickers in the chain actually scalably produce so in celestia i would argue that would be fundamentally higher than other systems like selana and the core reason for that is because the validators on the solicitor chain they don't do execution of of the user's transactions and they just do data availability and so which is a very bandwidth heavy uh resource requirement but not computation or a memory heavy requirement and the fundamental difference here is that like including like just providing data availability is a stateless operation which means that you don't have to like remember any user state so like for example in solana you have to have a huge amount of memory i think it was like 256 gigabytes of memory to run a validator because you have to remember a huge amount of state for example user balances to actually validate blocks but in celestia because we decouple data availability and execution the actual violators of the chain they don't have to maintain state they just have to put they just have to have the bandwidth to you know receive transactions and put them in blocks and that fundamentally allows you to increase block production to much higher um than if the validators all has also had to do execution um and all and computational transactions john please yeah so what mustafa said is kind of one of the key distinguishing factors that makes the modular blockchain architecture so compelling that for the first time ever uh you know we can have this decentralized network that can actually grow its capacity as the number of users grows uh the monolithic blockchains that you see in like ethereum and solana and bitcoin for example uh these all as you add more nodes as you add more users you don't actually get more capacity now one subtle point here is that the the specific capacity that's growing as you add more users is the capacity of data availability it's the throughput in bytes per second not in transactions per second and this is actually one like a litmus test if you want to see is the blockchain modular or is it monolithic if it advertises transactions per second then it is not modular it is monolithic or it's an execution layer as part of the modular blockchain stack uh the base layer of the modular blockchain stack the data layer does not support transactions per second it supports bytes per second uh now unfortunately these bytes aren't actually interpreted by the data layer so even though you get more capacity in just data throughput as you add more nodes that does not imply that you get more capacity in execution the execution is still bottlenecked by whatever parameter decentralization you set and by the particular choice and implementation of virtual machine and transaction format and so on so if you put ethereum's evm into a rollup that does not imply magically you can get more tps out of it and you if you want to get more tps out of that evm in a roll-up regardless of if you use ethereum or celestia as a data layer you need to sacrifice something fundamental you need to sacrifice some security guarantees or you sacrifice decentralization which is all not necessarily good it could be some trade-off that some users are willing to accept but ideally we would like to have the option of a highly decentralized and highly secure the same decentralization and security that you get today uh blockchain or several blockchains and have that also be scalable and you can do things like have a uh sharded or sharding without sharding you know multiple blockchains running in parallel multiple evolves running in parallel or you could have one rollup that is more efficiently engineered that can provide composable like a higher throughput of composable execution over this scalable data layer so the the kind of the tldr of this is that having a scalable data layer is a very well it's very important as the base if you don't have this then if you have a monolithic base layer you know that cannot grow as you add more users you're going to be heavily restricted in the total throughput of the system but that's only half the puzzle you also have to look at the other half you have to look at the execution layer and for that this is why i said in my answer previously i expect a proliferation of a large number of execution layers that all experiment with different execution models uh because we would like to have something that is that has the composability of things like ethereum and selena but that also uh is also you know verifiable it's an execution layer it can have broader validity proofs and act as a rollup uh so it can share security with other execution layers on a shared security base layer like celestia [Music] eric we'll let you react to what mustafa and john just said okay good because there's one thing that i really want to bring up here and it's something that's been bugging me for a while and it's good that i have have all the four of you here to sort of address this question so i've heard john just said that in celestia capacity increases with the number of users and this is something that i think i've heard nick also say in the past like he said that the more users you have the more they sample and the more the security they give to the network so i suppose that the way that select the selection team is thinking about it is that if you have more users and they're doing data availability sampling on celestia which is the thing that you you got to sample that data to know that it's there and the more users you have the more sampling you do which means that the overall data can be the overall data set can be larger because you now have more samplers so there's two things that i would really want to get help with understanding the first thing is uh the first thing is i think it's an easy one i just want to understand like okay so if i'm a user and other users are sampling like how am i how am i how do how does my node uh get the information or verification that the other nodes actually have sampled the data are there small proofs that they deliver or can you just go into some detail into how that sampling goes and then the other thing then this is the this is the the most important question i think maybe that i have in this conversation is that uh okay yes you you with more with more with more users you sample more of the data the the data can be larger but uh john i mean you have been the one the person that has always been saying that unless you are unless the security assurances are constant as you increase throughput then you're not actually scaling so in order for a system to scale you have to scale while maintaining the same level of security and now my issue now becomes is that uh okay yes uh you can by using data availability sampling you can you can prove data availability but you don't actually prove uh retrievability so and and in order in celestia in order to for the system to be secure you actually also need to retrieve the data so just just knowing that the data was published at some point doesn't actually give you the security that you can create the fraud proof or create the validity proof and get your money out of the system you need that data to be there so isn't the only real full node security is still based on that you have to have all that data yourself on your node otherwise you mean there are other nodes you can you know that they have sampled the data but what if they all turn against you at one point and you mentioned that in in in cosmos there's ah in in the in the sorry in the tendermint consensus algorithm there's a hundred nodes maximum like a 100 200 nodes are we trusting these 100 and 200 nodes to maintain that data and if they go rogue then the end user is and the more data that is getting pushed into the system then you know that still puts a bottleneck in terms of you know how many of these nodes are actually storing all the data and because i mean it's it's one thing to have the to have the proof that the data was sampled but it's another thing to actually know for sure that when you come to a point in time where you need to get that data out of the system it is actually going to happen so let's see a team who would like to respond i guess i can i can start so maybe it'll be helpful first if i kind of give a quick overview overview of the properties of data the sampling and how it kind of works in general so the general idea of uh like the problem that data availability sampling is trying to solve is as a user how can i make sure that like let's say some one megabyte block was actually published to the network by that block producer and without me having to actually manually download the entire block myself and check that it was actually published um so this is basically preventing what's called the block withholding attack where a block producer or a miner or or a validator might publish in a block header but no one but if they don't actually publish the transactions that that block header points to then people might not know what was actually inside that block and so if there's if there was an individual transaction in that block um then they would not be able to detect that and so with this availability sampling it basically allows you know a user to download a very small piece of that block and get a very high probability assurance that 100 of the block is available and the way it roughly works is i'm not going to go too much into the reads but that the way it roughly works is the block producer splits the block into you know hundreds of trunks and applies something called an erasure code on top of those trunks now the end result uh with you don't without understanding you know how erasure codes work but the end result is that um as a as a light client uh i know once you apply the erasure code the block producer has to if they want to hide even a single byte or even a single transaction in that block it's not sufficient just to withhold that specific transaction or byte they would have to hold withhold you know half of the entire block because if you know any half of that block is available then the the entire block can be reconstructed with the with the available half and so this allows light clients to kind of have this sampling primitive where if a light client or a user samples you know let's say 30 pieces or chunks from the block per block then they can be 99.99 sure that 100 of the block is available um assumed like because because if the block producer actually did this attack and they withheld half of the data and the user you know randomly sampled or randomly checked 30 different pieces in that block then there's very high probability that the user would have landed on an unavailable piece or withheld piece and therefore they would know the block is unavailable now this kind of scheme only works if there's enough users in the entire network such that they are all like they're all making enough samples collectively so that they can actually reconstruct or recover the entire block so like if you have if you have a block with 1000 chunks it's not enough just to have one user you know requesting clear examples but you know you need a bunch of users such that they all requested you know at least 500 samples and so you can kind of think of it like bittorrent if you're familiar with bittorrent you have you have peers and seeds in network and those peers you know have might have different pieces of that file and they can all share it with each other and reconstruct it and and that's kind of the fundamental reason why you want to interrupt yeah i was wondering how do how does the how do i as a user doing the sampling know that the other users are doing it and if they because i mean the other users they're not involved in the consensus they're also like sampling how do how do i know how do i get assurance from them that their sampling is also benefiting me uh how does that compound like you don't know because you assume that there are and the reason why that's fine is because usually the number of flight clients needed are like in the few hundreds um so like if you have a one megabyte block you know you only need you know if you like depending on the parameters of the construction you wouldn't need more than a few hundred light clients for the assumption to hold true and so like if you assume that the network has sufficient number of users uh or if you know that then you would assume that there's enough u that uses like enough people running nodes in the network such that this assumption holds true but that that sort of assumes that if there's one node that is sampling and doesn't get the data he's able to prove that he's not getting the data so is that something like is there a one out of n assumption here where the one person whose sampling fails he can he can flag it to the other nodes and they can try the same sample and see if they so their response so so this scheme does not involve any kind of like flagging or like uh like uh like there isn't there's no like proofs of unavailability or red alerts or flags um like the scheme works without that because if like if if the user assumes that there's enough users in the network running the same scheme as them and doing the same sampling as them then that's all they need to know um to know that data is event to know that data is actually available like because the reason for that is because like if there's enough users in the network um to request enough samples to force the block producers hands such that in order for that block producer um to like for the block producers have to meet all the sample requests the block producer would have to release just enough of a block such that the entire block can be reconstructed yeah can i also add something please please yeah uh so uh one thing you might be uh thinking of is that uh there's no deterministic completely reproducible way for you to know for certain how many honest nodes are performing this data availability sampling uh like mustafa said if this is you know a few hundred or whatever then you're safe but there's no concrete way of guaranteeing that that is true uh this is one of the reasons that uh the kind of social consensus around celestia is ultimately to use uh social coordination an off-chain uh uh off-chain governance in order to control parameters like the block size limit uh because there's no way on chain of determining on not just on chain but just programmatically in general determining how many honest nodes there are period in the network this is more something that the community can get together collectively and decide on these parameters implicitly deciding on how many honest nodes the community thinks are participating yeah i should add this is also like very like the same as any other in the blockchain ecosystem so it's like you know how is the block size how is the maximum block size limit in bitcoin um like or a 3m decided or even pocket. like effectively it's a social contract between the participants of those networks such that you know they have have a certain you know nerd resource requirements um and they control and they you know upgrade their chains and set the block size limit based on that so it's literally similar i would argue that it's still fundamentally different though because in bitcoin i don't need to rely on anybody else's account that the data is there because with the bitcoin client i download all of the data so i have it on my own computer whereas in the celestia system i don't download all of those petabytes i'm just sampling them and if the data grows and your your argument here is that even if the data grows as long as the users also grow and they're also sampling then i don't have to worry about the data growing even though i'm only sampling the the same number of chunks so what i'm really trying to get at is that if i'm always just sampling the same number of chunks and the data grows and grows and grows then i have less of the certainty that the data is there and it sounds like there's this social element to understanding that there are other samplers around me but that's not like a technical quality so what i'm trying to get at is this really or can we really say that the security scales i mean it scales maybe in a social sense but does it actually scale in a technical sense yeah so that's what can i also quickly add something i think i guess the the the sampling itself is a proof that the the data is available and you don't need to know that i mean it's a local property and you don't really need to know that um others are doing something as well it's more that if the sampling succeeds it's also proof that there is in that like there's like there's enough nodes providing you with the data right so i guess like there's some confusion also is not the sampling only but if if if you sample and also serve that data um then you provide security to the network the sampling itself is also for yourself but if you want to like provide to the network like to the security of the network i guess you also have to reshare these um uh samples right maybe that does that clarify it a little bit because yeah but i have some other questions in uh along that if if we can move on like if you don't want to spend too much time but i have a few questions that we can perhaps uh try to address so the other question that i have is that okay so when you when you sample a block do you ever sample it again or is it just when that block arrives that you sample it and then you you assume okay it has been published and then you have a look at it again so if this block was published like two years ago do you ever go back and sample previous blocks and get the certainty that those blocks are still there because your security can still depend on those blocks right yeah so um nodes that come online um will at least like light nodes will at least sample from the from their from their checkpoint or from the um trusted header they they start from but like you could also start from genesis and sample from there right like um even if it's like two or three or five years ago but i think eric's question is i think if i understand correctly eric's question is more about do we also guarantee the ongoing availability of the block is that right here yeah yeah so this is like it's a very common question um like is the question of like data retrievability uh like people ask you know we guarantee so celestia does not guarantee they have achievability what celestia guarantees is that the data was published uh at some point in time the assumption that we make for delta retrievability is like basically the same as ethereum full node which is that you know once the data has been published and once you conver if you can verify that the data was published then from from from that point on there's an honest minority assumption for the ongoing retrievability of that data um it's because effectively like if you can if you if you can check that data was actually published internet then for the ongoing retrievability you just need to assume that there's at least like one honest like no like server on the internet that's willing to um like host and serve that data or like there's at least like one available data on it on the internet somewhere kind of like a kind of like this dry sound effects in a sense um and this is why also there's a block size limit like obviously that would not work if you have like you know tens of terabytes of data because you know people would not be able to host that and but if you if you have a block size limit that scales the number of users then you're more likely to go and that assumption is more likely to hold true in particular because the way that we want to design the system is that you can actually contribute to the storage of network without storing the entire blockchain um so like we have this concept of partial storage nodes that can store parts of the blockchain and contribute you know part to the parts to to part of the storage of the blockchain kind of like a kind of like a bittorrent files with different peers that hold different parts of the file okay so my nitpicky summary of the situation here would be that you know i would say that yes if all the data in the celestia is one terabyte then i would assume that this data is still retrievable and retrievability in my opinion is the security requirement not just availability so if it's just one terabyte i would assume you know this state is probably there but if it's a a million terabytes then i would assume that the actual nodes in reality actually storing all that data is much smaller so meanwhile it is commonly said that you know as users grow you'll have more sampling and the security group scales with the number of users i think that you cannot escape this harsh reality that as data grows there will be perhaps fewer that actually stores the data and the the certainty that you can retrieve it shrinks so i think like attaching when you're saying that security scales with the number of users you're really thinking about the data availability sampling scales with the number of users but security is retrievability and retrieval retrievability is impacted by the actual size of the data so that's just like a nitpicky summary of the situation i still understand that the data availability sampling does you know give you uh [Music] favorable properties here but if you're going to the very cold harsh truth of the situation then thousands of petabytes of data on celestia you're going to have to rely on that that data still exists because you're not sampling it all the time and you're not getting proofs from other nodes that they have sampled it to your node all the time at least not in the way that celeste is designed at the moment do you think that that's a fair like did i did i say anything that was incorrect there well i mean i guess there's two things i would say to that um like the first thing is like what i've said is this is is the status quo um of like of like how blockchains work uh like you know that's that's how that's the same assumption that ethereum full node uh makes and obviously that assumption does work like no one complains about the assumption as you rightly say that's because you know we're talking about a few terabytes up there um but the question then becomes like if you have like what if you have hundreds of terabytes of there and it's like so okay there's like there's several things that will take that the first of all just because we don't we would not want to have like on chain or in protocol incentives for data retrievability does not mean that there cannot be uh incentives for data retrievability like it just means that the incentive layer for data retrievability and data availability is decoupled and you know for example like ethereum like how does like ethereum technically does have like ethereum today technically also has a data retrievability problem like that's why people pay you know infuria to get access to full nodes uh to give to to get api queries from inferior to to get data out of ethereum and you can think of inferior as a centralized data retrievability layer for ethereum but obviously you can actually have um kind of decentralized versions of that for example you know you've got pocket network which is more distributed and you have like the graph which is uh achievability for index data and then um you've also got like you know data storage protocols you know for example file coin has proof of reachability um so like the first thing i would say like there's no inherent reason to deco like to have um data retrievability incentives as core parts protocol the second part i would say to that is um like even if that's not good enough it could be the case that the like end user the the the end user applications that people belong to last year could make their own assumptions about data achievability and like for example they could create their achievability assumptions within their within their applications um like celestia guarantees the availability but if those applications want higher data which is building guarantees they could add data visibility or proof of reachability within their applications if they wanted to do so even though i don't think it's necessary but they could so i'm going to move the group uh to back to the l1 topic while i do that uh we will be going into q a so if anyone wants to ask a question uh your chance will be soon eric while we wait for q a uh is there another major misconception in the l1 space that you can point out that needs uh clarification or that folks need to talk about um really i think it would be more interesting i have more uh like architectural questions about celestia like i think one that would perhaps be more interesting to talk about if if you don't mind uh i think i think one of the very interesting things when you're thinking about the modular blockchain stack is thinking about i mean as investors and then most people in the cryptocurrency space are investors everyone is thinking about okay where does value accrual happen in the system and we didn't i don't think that we really uh in this conversation really gave it gave a good clarification of the different layers in celestia and i i think these terms are also being mixed and confused sometimes so if we like start by dissecting the stack into their appropriate layers and label them correctly and then talk about which tokens do what uh i can just start by labe and and please interrupt me here and if you think i'm taking the conversation into a direction that you don't want to go but from my perspective uh at the base there you have consensus and data availability and uh those are actually in celestia bundled together because the celestia base layer does order blocks in a in a according to the tendermint consensus that it uses and it provides state availability for that layer and that's that's the data availability layer it's but it's also the consensus layer and those two terms are being thrown around a little bit confusingly but i think that that's really one layer and then on top of that layer you have the execution layer and this is a system like fmos that provides an evm this this is an execution layer but it's also a settlement layer right so the way that you if you're defining the settlement layer in celestia is for for example roll-ups that need to if there's some inconsistencies in a roll-up at the top of the stack then they need to be disputed in an execution environment somewhere that can interpret these fraud proofs or validity proofs and sort of settle what is the actual uh what the actual state of the execution environment is so on top of the data availability the consensus layer you have the execution layer that is also a settlement layer and i suppose that the execution layer also encompasses rollups because a rollup is also an execution environment so the execution environment has really two tiers to the settlement layer but also the sort of roll up that all goes into the sort of execution layer and then on top of that you have um applications like smart smart contracts uh that run inside a rollup but the roll up itself also can has can have a token so what what i want to understand a little bit like from how the celestia theme is thinking about it which like if you go from the top of the stack to all the way to the bottom of the stack like let's say i'm a user of dy dx like the derivatives application that runs on currently runs on ethereum if we're going through like from the top of the stack is it there going to be a dydx application token that runs inside of a roll-up that has a roll-up token that runs inside of fmos that has a settlement layer execution environment token that then uses celestia as the data availability of the consensus and consensus layer that also has a token so are there really like four different tokens going through here here and which ones of these are uh like where where do you think that like do you see any value accrual at the eve most layer or is the value crew going to happen at the celestial layer and and also we need to discuss a little bit about the where does fee congestion like where does congestion actually happen does it happen in the execution environment or does it happen at the celestial layer yeah so the way i see it is that yes there could very well be a token for every single layer like you might have a data availability circuit you might have a settlement worker and you might have app token um but now that might seem like a ux nightmare but in theory but in practice it's not because the way that it would work is that like the end user only needs to be exposed to the top to the top player token which by the way it might not even meet the app token but could be like a stable coin like usdc like the app itself could accept you know payments in usdc um and like convert them to the app token uh like via a dex or something like that now so and then the way i see it is like a supply chain and so if you have like the operators of the app chain they accept you know payments in their app token or they accept you know after they are looking for some service uh but that the app chain has to pay this element there so they can like uh they can do a current they can exchange the app token for some supplement tokens um and then pay the settlement layer and then settlement player that needs to pay data player you know the data availability token uh using the same process but this is effectively like how any supply chain works like including like it except that the fact there's different tokens but like effectively that you know in in supply in a normal supply chain and you have a stack where each or the bottom supply chain you know transfers funds to the higher layers and it keeps going up the stack and like for example you know twitter pays its uh you know state internet service provider which pays hardware providers and so on and so forth we do have uh john from delphi here john go ahead ask a question yeah so thank you for giving me the chance to speak um so while we're talking about architecture here um there is the one issue that uh that i've been wondering actually so i want to get celestia team's opinion on this anyone feel free to step in so if we're talking in terms of the base layer the celestial layer we know that there's data sampling light clients and these light clients um can effectively uh know how big uh the target size of the celestial chain is um then they know this um by um looking at their um block header size um so they effectively have a discretion over how large the bloc celestia blocks sizes are but what i'm wondering is for them to um so in reality for them so if we imagine these light clients are interested in a particular roll-up on celestia for them to remain secure they would have to assume that there is a full node in in this roll-up that would hand them the the validity and fraud proofs so what's becomes a point of interest here is that or a question in here is that these light clients do they have any discretion over how large the roll-up blocks can get or can they enforce this somehow because if we imagine a roll-up a giant roll-up that does a lot of execution and that there's only a handful of uh full nodes that can actually execute them and hand over the fraud proofs then we're looking at a at a centralized execution layer on top of celestia so i was just wondering if there's any way for data availability sampling light clients to ensure the block size of the roll-ups that that sit on top thanks we'll be taking more questions in the meantime but celestia team go for it sure i guess i can go so this ties in to a bit what i said earlier which is that there's no way completely programmatically to know how many nodes are on the network therefore uh having something you know programmatically entirely unchained to decide a block size is probably not a good idea and therefore this is why we use off-chain governance similar to you know the most robust decentralized blockchains like bitcoin and ethereum uh because it allows end users to have sovereignty over the blockchain regardless of what the block proposers vote on regardless of what even token holders vote on end users running nodes are the ones that ultimately control the rules of the chain through governance so a single node on its own acting as an island doesn't really have any control over the block size but the community of node runners does regardless of what the block producers or even token holders want uh they can simply hard fork or soft fork or whatever to change the rules so they can prevent block size increases that they don't want or suggestions to increase the block size that they don't want or that they won't accept well rather in the first place that a hard fork may be necessary for block size increase yes does that answer your question i think so um anyone else i think john's question was also about can roll up blocks themselves has to have a block size limit and yes they can and like a roll-up application could also define like say like add is rules that a rollup block can no longer be over a certain size um so yeah that's that's fairly trivial to do we have another request for a question uh djonblock go for it hello guys thanks a lot i hope you can hear me well so first the discussion is super interesting but i'm not uh super technical uh so i'm not sure i understood everything the question i still had was the following uh everybody's talking about modularity now and the cosmos ecosystem is often mentioned but i feel like polka dot is less so i wanted to know guys what you thought about the design of a polka dot and could we consider the base layer of polka dot as a data availability layer uh or is it something completely different thanks thanks chance i mean every layer one is is kind of a data readability layer um but i guess um what polka dot um does differently from i mean you you as far as i understand the question you you you want to understand the differences here i guess the the main the key difference is that by default at least for parachains in polka dot the validators of the main chain of the layer one they also execute part of the state of the power chains which um come close to what you could think of a rollup in in as we have been speaking about thanks um we'll be fielding more questions in the meantime eric uh tell us more what else would you like to bring up and talk about uh sorry one second sure we'll go to a question we have brandon here brandon please ask your question earlier eric gave some really good context on what it might look like for cosmo zones to opt in here and to solve some of the data availability challenges associated with sovereign zones i think most people are pretty familiar that if you're running a rollup you have to make some decisions about where you're going to store that state whether that's on an l1 like on ethereum or like in the validium model where you're storing that with some sort of a data availability group can you speak just very briefly on what it looks like to integrate a roll-up or a zone with celestia such that it can guarantee that publishing of data has occurred thanks brandon sure uh i guess i can take it and since no one else seems to be interjecting so uh there's kind of two immediate approaches uh that one could use uh to have rollups as we know them on ethereum leverage the data availability guarantees of celestia also interject from brandon if i if if i'm not answering the question properly uh so one approach is uh what we call celestiums using a technology called the quantum gravity bridge so in uh in this design you have a roll-up it operates on top of ethereum it does adjudication of proofs so either validity proofs or fraud proofs on ethereum just as they do today uh and they can deposit and withdraw coins from a bridge contract on ethereum uh the distinction between the celestium and the roll-up on ethereum as you as we know and love is that the data availability instead of being posted to ethereum is instead posted to celestia and the immediate effect of this is that it would dramatically reduce the fees at least in the short term there's a bunch of caveats around you know long-term fee markets but that's like a multi-faceted game and stuff so we won't go there but at least in the short term the immediate effect is that it would significantly reduce free fees from you know several dollars to a fraction of a penny all while having very strong security guarantees because users can perform data availability sampling to know that uh to know whether the data on the celestial side is in fact available or not before deciding to use the celestium the other immediate approach for people who want to leverage celestia for data availability or how how does it look for rollup that wants to leverage last year for data availability is some project that we've been calling submos which is essentially an evm settlement only layer that runs directly on top of celestia and then you can build execution layer rollups on top of this so you can for instance fork any roll-up that exists on ethereum or if you want to do also a sidechain but you know for the purposes of shared security any any roll-up so anything like arbitrarium or optimism or fuel or zika sync or starkware anything you want you could just fork onto submost because it's evm compatible uh and you would essentially use sevmos as the adjudication layer so that's the thing that would verify the proofs since celestia itself cannot verify proofs it does not do execution uh and then a seventh would then use celestia for data availability so these are the kind of two immediate configurations uh that we're seeing pop up with regards to roll-ups using the celestia there is a third configuration that i think people are starting to get excited about which is a sovereign roll-up so sevmos is an example of sovereign or sovereign roll-up it's a roll-up chain that operates directly on top of celestia as opposed to requiring another blockchain to adjudicate its proofs it actually operates directly on top of celestia that is the first example but is not the only sovereign rob that can exist or that will exist and i think there are some growing excitement in the community to build out uh celestia first sovereign rollups that do not rely on another blockchain to verify proofs that are hopefully going to be built in the not too distant future does that answer your question hey can i hear one thing um i think brandon also mentioned brennan also mentioned how it would look like if a cosmos zone um would would be like turned into a roll up or uh would be built as a roll-up and i guess that ties in well into what you just said with the sovereign roll-ups because you could build a cosmos sdk chain um that uses what we have a project called optimum that essentially is compatible to tendermints like a swap in replacement and you can build like a cosmos sdk app but instead of doing bft consensus it basically dumps the blocks onto celestia so that would give you cosmos zones that are sovereign roll ups that's also just one example of these sovereign roll-ups thanks ismael we have another question from a priority hey guys thank you for taking my question a question on sevmos as a settlement layer so in theory you would be able to have both evm based roll-ups and cosmosome based roll-ups settle on sevmos and would that induce any kind of special interoperability between those two or composability that's a very good question i guess i i guess for a cousin wasn't roll up you'd rather use if you want to go with the same model that john described before where you have like a settlement layer specific dedicated changes for settlement and a roll-up on top then you'd rather um i'd say you'd rather have a cosm wasn't settlement layer which isn't like several like which isn't evm based um but that's something that's uh that's really a good interesting question maybe a mustafa i already heard him maybe he wants to say something yeah you can't set all kinds of muslim roll-ups directly on sevenmoss because seven was evm based not wasn't based however as like as ismael said like you might want to create a different like wasn't based settlement there like one thing i've always said before it might make sense for the cosmos hub itself to implement to integrate wasn't causing muslim into the hub and use it to support cosmos and roll-ups um the final thing like eric can convert this but i think like there's people in the cardano ecosystem working on wasn't roll ups um however um that might not be necessary if because it might be it might be possible to actually like all wasn't roll-ups directly on evm by compiling it to an intermediate to immediate intermediate language um i think arbitrary nitro works similarly because aperture micro uses wasn't but also like for example optimism has a fraud proving project called canon which can compile arbitrary go code or like i mean it can it can afford proof arbitrary maps code um so like if you could compile any program to mips you can prove that so in theory you could compile like wasm uh code to mips and you should be able to prove that um on the evm base roll up oh sorry i mean the evm based element layer thanks we have another question from no excuses go for it yeah um i um when i was reading vitalik's blog post and security that the future would be multi-chain but not cross-chain he specifically called out celestia and saying that consensus and data availability can't be delegated cross-chain without sacrificing consistency and security guarantees and i wasn't aware i haven't seen any written rebuttal to that claim so i'd be really interested in uh reading something about that and then my second question is what's the place of celestia or the value proposition or moat in a post shared security launch for cosmos um yes that's the first question uh i think i agree technically speaking with vitalik i mean in fact i've said the exact same thing as italic before uh in the blog post in october of last year where about the question ecosystem where we disagree is on the utility of what he calls uh chains of sovereignty or zones of sovereignty because what he's effectively saying is that his argument is effectively that um every chain should you must use the same settlement layer to have shared security and it's not really useful to have different settlement layers because you can't do a trust minimize bridge between that so you know previously i've made a distinction between um this idea of a trust minimize bridge which is what roll-ups are based in and what i call a committee-based bridge which is not trust minimized because it relies on honest majority of a committee to operate that bridge um i think like both have a place in the ecosystem um i think it's very unrealistic that we will see a single like zone like a single cell settlement zone of shared security i think in practice they will like we will see multiple settlement players that have committee-based bridges with each other and i can see and i can see the opportunity in that and because as john said there's a lot of interest like in our community of building what's called sovereign roll-ups which are roll-ups that don't sell towards any specific development layer under several reasons why why you might want to do that including like more more freedom and not having to be bounded by the social contracts of your settlement layer uh in terms of the second question which is how does like i think the question is basically how does celestia compare to the the kind of the cosmos interchange taking or shared security model so the the cosmos interchange thinking model or stratification model is very similar to polka dots where effectively you can kind of pay or like request the cosmos hub vowel layers and to also validate your chain but that's fundamentally not scalable because like all the validators have to about execute all the transactions in your chain so for example in portal dot you have a limited number of execution slots um but roll ups don't require that because robots do opt chain execution so i like and zacky also agrees with me on this but the cosmos interchange taking model is more kind of geared towards or more appropriate for the cosmos hub having chains underneath it within the government within the same kind of governance system so it's kind of like um like if like it's kind of like having subsidiary chains under cosmos hub but it's not really kind of like a long-term solution to shared security okay so we have time for one last question and then we're going to go back to eric for his closing thoughts alpha key please okay so um i i think what's kind of being recognized here is that uh the unchained data costs are extremely expensive and if you want to do data intensive things um it's really expensive to do the bond trade i'm curious of like your thoughts and realizations around splitting out the data availability later thanks gents sure i can take it uh so the beauty of the modular blockchain uh paradigm is that by separating on the base consensus there doing just data availability and doing execution at a higher layer this means that you can get a much higher throughput of data availability whether long term this leads to lower fees or potentially higher fees because of network network network effects over the over that is up for debate debate but regardless you end up with a much higher throughput of data availability immediately the immediate effect of this is you know shorter or cheaper transactions for the foreseeable future uh so so that alone just separating these things uh this mod the modular blockchain paradigm allows us to build applications that are much more data-heavy such as roll-ups because roll-ups on ethereum today you could see just the on-chain costs and then the order of a dollar or maybe two dollars just the on-chain cost and that's with ethereum's current the not too high transaction fees and it hasn't been too high for the past couple of weeks so this is just this is just the on-chain cost of data a dollar or two and this is you know comparing to a few years ago when you when people are saying things like they're in the internet of money should not cost more than five cents per transaction and now we're looking at you know a dollar or two just for the on chain data cost let alone the cost of execution and so on so the no the advent of the modular blockchain paradigm allows for data happy applications to ideally run with substantially lower fees thanks john so we're at the home stretch here and we'll go back to eric um eric you once said to me that you know celestia is one of the few projects where i feel like even if i tried my hardest to destroy it i probably couldn't can you expand on that more or just tell us where you see things going and sort of your your beliefs around celestia and you can conclude uh your point of view as it relates to this topic and we'll segue off yeah um i mean i think i think one of the main main reasons why uh celestia is very difficult to criticize is because the the it's it's extremely simple uh it is doing more by doing less so by doing as little as possible it births this whole modular blockchain stack by just focusing on one piece and doing one that one piece very very well but throughout this conversation i've been trying to okay but if i had to like what is the most annoying question that i can ask the celestia team like what is the one thing that i can latch on to uh to to with their heads a little bit and i think that the the thing that i'm landing on like the the what what it's boiling down to like the annoying question that i that i sort of could ask is that yeah we sort of admitted here during this conversation and this is not this is nothing new i think you know the slasher team has always said this which is is a very good thing that you know um we're being consistent uh i think like the annoying question that i would ask would be along the lines of okay so let's say that there are like in ethereum for instance we see that there are a couple of applications that are uh extremely uh used like the there's an extreme demand for those applications and that would be open c and uniswap and openc and uniswap impacts the fee environment for all other ethereum users so i was thinking like okay let's imagine that something similar also happens in celestia that you have two or three execution environments that end up just you know being extremely uh high demand for those and they drive up the fee levels on the celestia base layer would there like would it possible to to to say here that you know celestia is a it's it's a extremely important piece of the modular blockchain thesis stack but in itself it is a monolithic data availability layer because all the all the demand for celestia impacts all of celestia users so would it maybe and the reason that i think that this is the best most annoying question to ask perhaps john here is that i know that john is a very very loud critic of the avalanche project and avalanche their vision for scaling is basically using different subnets which is like completely separating the ecosystem into separate environments would there do john like would you would you think that it would make sense like if you have one celestial layer and the feasts go up to like it's it turns out that it's very expensive to use celestia for like micro payments use cases do you think that it makes sense to segregate even celestia into different subunits so that yes once you have one celestia system that is cannibalized of with usage then you sort of create another parallel celestia uh that would be like a subnet for celestia where you can have celestia too uh where the fee environment could then go back to being low again and then you can sort of scale the system uh by making uh more and more subnets as demand is cannibalized uh on either uh of these systems that's sort of the most annoying question that i could come up with for john sure i can i can take it since i've been called out but i'm gonna answer half of that and leave the other half to mustafa so uh the first half is uh that yes it's true that you know as demand grows the you know there's a certain capacity of block space you know maybe it becomes expensive uh but a caveat here is that uh there's a number of things that we can do to make capacity grow in ways that traditional monolithic blockchains cannot the first is the scale-out property of data availability sampling which we've talked about a few times which is that the more users there are the more you can increase the block size securely uh the second thing is because of the fact that every block uh modulo some tiny amount of uh the validator set management which is stateful but the rest of the block which is the vast majority of the work that you need to do is essentially stateless it doesn't rely on any previous block existing and not only that it's highly parallelizable because we split up each block into a square and perform operations on each row and each column of the square completely independently so this leads to the potential for doing what we call internal sharding where rather than sharding across different identities within the network and charting and splitting up work among different identities like different validators you essentially have a single validator but that can split up its workload across multiple not as powerful worker computers so this allows you to essentially i don't know if you'd call this horizontal scaling but essentially allows you to just very easily produce very large blocks because you don't need one super computer you can just spin up a cluster of very not so powerful computers and it will still work uh now with regards to even if we have these techniques even if we have these mitigations can they reach a point where there's simply too much demand for the capacity of a single celestia chain and this is where mustafa is probably better to answer because yeah she wrote a post about this uh not too long ago segway to mustafa yeah so as i said before like no blockchain can guarantee cheap fees uh the only thing like the only thing a blockchain can guarantee uh is certain scalability properties and certain throughput properties but it cannot guarantee like ultimately the fee market is a free market and it's like it is simply not possible for blockchain to guarantee cheap fees uh without conceding two things the first of which is is dos resistance and the second thing is the target requirement for um block producers the target the target hardware requirement for block producers and this is kind of like more this is kind of like an interesting debate and people have said before like solana is centralized because it has very high block production requirements and very high requirements for users to verify the chain like he did 256 gigs of ram but then the metallic recently put out a post which he called the endgame post which is this idea where he kind of had this thought experiment which said assuming that i had uh a blockchain that had a very high resource requirements to produce a block what can i do like what what what how can i adjust that blockchain so that i'm comfortable with it and the short answer to that is that to be comfortable with a blockchain like that you need to allow end users to actually verify the correctness of that chain using technologies like data availability sampling um or fraud proofs or zk proofs which is what roll-ups use because those technologies allow end users to verify the state of the chain without have with very low resource requirements even though the requirements for actual block production is very high but then the question is like how far do you want to take that end game uh like idea like like what is your limit for block production and to what extent does that affect censorship resistance and because like in theory like sure like in theory we could have like you know infinitely potentially infinitely scalable you know data layer that can you know that is google scale but then you have to have google scale blog producers and the question is like you know to what like to what extent are we comfortable with that um like because maybe you want you want to have block production requirements that are spoiling off so that the threat the the threshold for someone else to step in in case someone senses network is not that high and that's the first thing and like the second thing is what about sensitive persistence censorship resistance um so like i know there are some ways to achieve censorship resistance um even if you have a somewhat centralized block production block production uh validator said but the ques the question still remains is to what extent to what extreme you want to take that and i think that's that's kind of the final fundamental question because if you don't want to take it to that extreme even though you're comfortable with the end game where the metallic has proposed where um you have high expensive block production but cheap blood verification and the question is what what extreme you want to take that and if you don't want to take it to an extreme then you do need multiple big chains and multiple data availability layers that do have different security guarantees and fragmented security okay we're coming on final comments eric what's anything else you want to say before we go off uh i just had a tiny last question which was i heard that um evmos running on top of celestia would limit the limit its use cases to only arbitrating disputes in roll ups so the ev the evils would only look at roll ups and settle the fraud and validity proofs but the question that i had there was just it's just a small question how do you actually limit that in practice without making the system permissioned and what if someone wants to experiment with a new type of rollup and a new type of fraud proof how do you enable that so uh i'll go ahead so i mean there's kind of two ways that we're looking at restricting the sevmos evm and the first way is what you mentioned which is like you have a set of white listed smart contracts so like that's the dumb way of doing it um so like you just say your white list arbitrarily or optimism smart contracts and you can only deploy you know optimism or option rollups um like in theory that's like if you want to experiment with a new kind of roll-up you can still do that you just you just have to deploy it as a l3 um can you have to use l2 as this element layer and you have to deploy your roll up as l3 um so like it's still possible uh to experiment with different kinds of rollups or different types of types of contracts or execution layers but the second way that we're looking at it which is kind of more interesting to us is just to adapt or adjust the resource pricing of the evm to make it restricted such that it's not practical to use it for like on-chain applications but only roll-ups and the main way we would do that is by increasing the gas cost for like state access state rights and state reads such that there's a limited number you can't do more than a certain number of state reads and state rights per transaction so if you want to do more than that you're forced to use a rollup but that doesn't actually place any kind of limitations on what on the contract on the chain just on the resource requirements that each contract has such that it is only practical to deploy rollout-based contracts on top of this element there thanks mustafa well thank you hi eric we'd like to thank you for just joining uh it was awesome uh we really appreciate your time celestia is the first modular blockchain network you can learn more or learn modular section gents thank you be well 