hello everyone I'm Ismail Coffey co-founder and CTO of Celestia I'm going to talk a little bit about the architecture of Celestia how it currently is and give like a glimpse into the future um whoop they're not mine you know how do I fix this oh okay um okay um first so on the agenda I wanted to talk like two parts like the first part is I'm gonna recap the Celestia architecture and in the second part as I said I'm gonna talk a little bit about like future directions so it's not like a full-fledged roadmap but what could be on the roadmap next year or so okay so first of all what is Celestia I think most of you know but for those who don't know I um gonna explain it from a high level but also how it's implemented roughly so Celestia is like the first modular blockchain Network and what it does is it decouples consensus from execution [Music] um so what does this mean um so in In classical monolithic blockchains um usually how every chain works is [Music] um a client verifies two things right it verifies that the header has consensus and it also verifies that all the transactions are valid right like it executes the transactions makes sure the state transitions are valid um and then in Celestia though um it only verifies if the transactions are available it still validates that like the header has consensus right but it doesn't execute the transactions it doesn't validate that what is posted on chain is actually valid it just very like it just ensures that these messages that have been posted on chain um are available right they are OPAC blobs to Celestia like every application can post their blob onto Celestia their message and it's just ensured that these are ordered um there's consensus on the order and then everyone can verify themselves that these transactions are available so that's the main differentiator and um um yeah and I'm going to explain a little bit on how this works as well um you may ask like where does the execution happen and the execution in Celestia happens in so-called Roll-Ups right instead of this world computer model where like all the transactions are validated on on like one chain the state transitions are validated on a Roll-Up um and you might ask like how do uh like how how does that work in practice the um light clients for instance they to validate that like a header is um and contains only valid State transitions there's two approaches to this which is like um via fraud or validity proofs right like the in the in the ethereum world these fraud of validity proofs are posted uh on ethereum or um they're settled on ethereum but like in Celestia as there is no execution layer whatsoever on on the on the main layer these can be on like a different layer like on a settlement layer or they could be on the peer-to-peer layer um I just wanted to stretch this as many people still perceive Roll-Ups as like um enshrined with like uh with an enshrined settlement layer like on ethereum um so now that you covered like how execution Works how uh the main chain works I I wanted to like uh mention the four guiding principles with which we designed Celestia or like goals that we wanted to achieve um so it should be possible that a block is deemed valid only by checking uh block uh availability right which means you can like verify a header as a light node um without having to download the transactions uh just by verifying the availability um the second principle is application message partitioning which means every application only has to download their data right like they don't have to download all the transaction data of all applications only the transactions that are like necessary for their application and then there's um application message retrieval completeness which is the same thing but um uh which means that they can verify that all the like application messages they downloaded all of them and nothing is missing and there's also application State sovereignty which means that all the um all they need to care about is their straight Transitions and their um yeah basically their state portion uh or um and they don't have to execute other transactions of other applications unless there's a direct dependency um so next I want to talk about like how is this achieved roughly I don't want to go too much into detail but like there's mainly three three key ingredients here in play one is Erasure coding of block data another one is a namespaced Merkel tree and data availability sampling so let's dive a little bit in um eraser coding is a technique that's very well known which is uh basically used like in CDs where you can like recover data um so yeah you add parity data and you can recover data um even if parts of the data are missing right so how this works in in Celestia is the data is encoded in a certain way arranged in a certain way the original block data is um um split into equally sized chunks and then you add this eraser coating um in this Square construction that is depicted here the details do not matter what matter is that you um this enables basically that you can uh with like a portion of the data can always recover the whole data so here we also commit when like validators create a block they also commit to the Eraser coded block and not to like the the original data only as it is with like other chains um and so every every like row or column here basically forms a Mercury and these Market trees get committed to in a bigger overarching regulatory um and that Merkle tree is not like a a simple like traditional Mercury it is very similar though the main difference is um that a namespaced mercatory is used which sorts the data according to their namespace like each application can have their own namespace and the data gets posted on Celestia gets prefixed with um with a namespace right like let's get sorted and this is the data structure that ensures the property that I mentioned earlier you can like only download your portion of the data your namespace and you get like a complete test guarantee of that as well um so oops um the third and most important ingredient that makes it possible for Celestia to ensure availability is data availability sampling right so the as I mentioned before the block is radar coded here depicted slightly differently and then um um light nodes or nodes or clients can download only a small portion like one percent of the data due to that Erasure coding um to guarantee like almost 100 certainty that the data is available um yeah and so from a very high level perspective this is how we build the system um we took tenement right and we modified um oh Comet bft call today uh so Comet bft uh previously tenement we took that modified it and we added basically um the data commitment that I mentioned earlier we also used the namespace Merkel tree for that data and the third component that I mentioned the data availability sampling is done in a different layer in a different peer-to-peer Network we call the DNA networks heavily reliant on bit Swap and lip P2P a huge shout out to the node team that built this no one has built this before it's a huge effort um and yeah that's the high level that's a high level architecture overview so oops so let's go to the more interesting part which is like what are future directions for Celestia how could like a future roadmap look like or at least what could be some highlights here um so I mentioned this Erasure coding right [Music] um this is required for rollaps to ensure safety right like in in the sense that you have to be sure that the data that has published has not been tampered with like there's no validator this is like committing to garbage essentially or something else or some invalid data so how do we ensure this currently um currently um we have a a fraud proof type that is called bad encoding fraud proof that every node can generate it like downloads either the whole block or even a row or like several rows are sufficient they can like verify that the data matches the data root and can generate a fraud proof if that's not the case that has the downside that theoretically light nodes will need to have to wait for such uh batting coding fraud proofs to not happen to ensure like full finality of the block right an alternative approach is to use kcg commitments I think if you saw a Wales talk yesterday that's a technique they employ which gets rid of the bad encoding fraud proof the question is can we do better than this um I think we can so with an alternative approach here is to use um a ZK proof of the Eraser like to use a ZK proof that proves to you that the Erasure coding and the construction of the data route was done correctly so you can still compute quickly like the data root as before and send around the proof if necessary if like you require finality right um so that's that's a research Direction um if if anyone is into like ZK proofs um this is something you could look at and I will later share a Research Forum link and the GitHub link um another direction or another topic that we certainly will look at after launch is a trust minimized Bridge or like a 2A Bridge from and to Celestia So currently if you wanted to use uh the base layer Celestia token on your roll up as a guest token you have to use like other Bridges and to use like other change or third parties to to get the token out and in to your roll up and back right [Music] um ideally you would like the the the ux of this would be more seamlessly such that developers can use the the tier token directly so one naive way to do this one way to achieve this is you could enshrine uh like a general purpose execution environment onto Celestia that um like that acts as a bridge right um that has the problem that it wouldn't be really neutral in the sense that um you have to choose an execution environment and also you'd basically reintroduce yeah like you would tamper with the original Vision which is like having only data availability and consensus it turns out that um you can do this without enshrining an execution environment Mustafa came up with this idea where you basically um Leverage that um ZK snark verification is more or less like looks very similar to cryptographic uh signature verification right you also have like a a like a public key and from that like if you look look at the how ZK snacks work it's it's like from from the public key it basically looks the same so um the user flow for how this would look like roughly from a high level is that a user would enter would send a deposit of tier to a verification Key address that's like a key address like a public key or an address on by like a ZK program so you deposit here to there it gets like escrowed or burned and the transaction gets confirmed and then the tier would get credited on the on the roll up and so you moved out of of uh of you moved Tia into your roll up seamlessly so this roll up will need to track the state of that verification Key address and yeah so the more interesting part is like how do you bridge back you would send Tia a withdrawal transaction of your tier in the roll up right if the transaction gets confirmed by the state machine on the roll lab you generate a ZK proof and that ZK proved like kinda simulates or acts like a signature on the verification Key address on Celestia so that would trigger the withdrawal of of TL that's very high level um the the devil is in the detail there's a lot of like choices to make here um John would say a lot of implementation details um and indeed um yeah another Hot Topic is how to fix um Mev or rather how to apply current state of the art Mev research and techniques to like the modular stack right the the like there are two like there are several teams working on this um I think Skip and flashbots are the most uh noteworthy here um the the specialty or like the difference here is that um Mev uh can trickle from the roll up into the base layer like the the base layer validators could delay or um like even sensor batches from from the roll apps right so the question really is like how do we apply uh solutions to Mev to this like more modular stack especially given that there's like a very heterogeneous application layer um I think um Evan's talk will cover this in Greater detail don't miss it it's like this afternoon um he will speak about how to break the proposal Monopoly uh don't miss that talk um so this is like the long-term goal here that we want to achieve is um one gigabyte blocks one million Roll-Ups and one billion light notes so often when we mention this to people it's like one gigabyte blocks that sounds insane right um and it kind of does but we won't achieve this immediately anyways it will be on demand but we do believe that this is um like as it becomes necessary we will tackle this and the question really is like how um I think we're very confident that we can go to like 100 or something very very quickly but to actually get like closer to that one gigabyte blocks goal we will have to optimize commit bft's peer-to-peer layer right like the mempool is not made for super large uh blobs and um the the content the condenses reactor like the gossiping mechanism is also not made for that there's a lot of like low-hanging fruits and optimizations that we will do uh one of them's already started with like the content that has no pool um cat pool uh Callum implemented this already um similar the the peer-to-peer like the node peer-to-peer layer needs a lot of improvement to achieve this mainly uh block sync and dashing like there's a lot of optimizations to be done mostly very implementation specific it might turn out that like in the future in like several years we have to merge the da layer and the consensus layer again like architecturally from a software perspective and we will might have to employ a technique called internal node charting to achieve um like the throughput on on the consensus layer another topic that I want to stretch is we want like like one billion light nodes how do you achieve this right um so we want all light nodes to run on all devices like on all kind of devices but I think the device we should Target the next is the browser because um this is what like people are used to interact with in their wallets and um um like they send transactions through like middle mask and everything what you want to achieve is running Celestia nodes dashing light clients in the browser um so there's two approaches to this one is uh having a different compile Target for Celestia node go implementation another one is having a rust implementation um for the sake of time I'm not going into detail but if you want to hack on this please talk to me um yeah there's plenty of other open problems in research directions a few were already mentioned for instance by Yuma yesterday uh the ZK quantum gravity Bridge um yeah and I think one thing that at last word that I want to mention is that we need to focus on developer usability and if you want to do usability research for developers in the modular stack please also reach out to me um yeah you can engage here with in our Research Forum and on GitHub these are the main things thank you very much [Applause] 