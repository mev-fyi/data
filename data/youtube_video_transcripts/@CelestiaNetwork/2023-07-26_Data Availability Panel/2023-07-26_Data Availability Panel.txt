okay so I uh I guess we can start uh so let me first maybe introduce myself uh my name is bartek I'm a founder of L2 beats uh we are the community Watchdog for uh all l2s right now on ethereum and we try to inform the users what are the trust assumptions the security assumptions of all these Solutions so the users are actually aware and I've got an amazing panelist today representing I think three most known uh projects that promise to deliver da so we have Avail we have Celestia and we have a Togo from squirrel who will um I guess I have a interesting role on this panel because you will represent ethereum so let me just welcome my panelists and um let me just start by saying that I've watched this panel last year it was very docile you know everybody was very uh nice to each other and I think it was because maybe the space was nascent everybody was building but now we're like almost launching or have just launched and things are becoming a little bit more spicy so let's make this panel uh spicier and see how we go so um my first question to you guys um will be well ethereum Community uh is probably considered to be one of the largest and should ethereum Community really care about your uh Solutions or should they just simply wait for uh proto-dunk sharding and dunk sharding so maybe let's start with a veil and then you know we just go this direction yeah you know I think this is a pertinent question to ask um in general I mean if you look at the timelines right like um prototank sharding is will will come maybe end of this year early next year sometime but Bank starting is going to take a lot of time uh to come because from our perspective we've gone through the whole cycle of engineering uh the p2b on Avail as I think Celestia has been on the same journey and I think ethereum being um system that is already securing a lot of assets on the chain it's difficult to kind of uh introduce functionality you know like that can potentially you know like jeopardize the current state for example so it's going to necessarily take some time uh and you you're also seeing the rise of you know like let's say L2 systems l3s on top like I said on my talk a little bit earlier uh you know like each major L2 today is looking at this L3 strategy um you know like ZK saying hyper chains orbital morbid and so on right like I mean everyone is looking to do that and right now let's say if you look at uh arbitrum which is doing a Nova or a stack wave which is uh you know like has a DAC so all of them are operating dacs it's a pretty decentralized uh sorry centralized and uh all these l3s will require some secure da Solutions and which is less expensive than what ethereum um you know like can give at the moment of course it will the cost will go down with eip4844 so we acknowledge that but in general if you look at our architecture and you know like I'll agree with you know like with Celestia's architecture as well in the sense that we are able to provide far significantly less cost and quickly before you know taking up too much time you know like uh data availability sampling is massively underrated like it's not very well understood by a lot of people and I think unless there is a data building sampling implemented on ethereum I mean there's a lot of way to go there and we are not really using uh this construction uh you know like things like with recursive proof ZK uh proving systems already in there you are able to now give proofs propagate these proofs to the P2P for example to the users directly so something like a um uh like a starkware which puts proofs on ethereum every six to eight hours uh someone like that can actually create intermediate proofs and pass them directly to the users and of course they need to wait for the proof to come to ethereum for bridging and such but you know like it's much faster verification time right like sorry not taking up too much time but you know like I think all of these points are important yeah sure so I mean I think there's definitely place for multiple da layers and they all have different trade-offs different use cases you know like um for example for ethereum as um as Andrea mentioned you know sharding is just like a very small step to the overall world map of tank shouting uh it's a set of different trade-offs so like for example um celestion available are more overhead minimized like there's no like state state baggage already so for example like if it's more practical or overhead minimized for solving Roll-Ups for example if that's what if that's what you want to build and it's also like various design different design choices so for example you know eip4844 you can only fit eight blobs um without burst on on a block um like if you want to have if if you want to do like app chain roll ups you'll probably need some data aggregation service for that to be practical whereas like for example um available there's no specific like blob size limit uh minimum blob size limit for example and also of course there's the fact that we have data availability sampling and that's not that's still kind of further on in the ethereum roadmap uru what do you think I mean should we consider using other da so first should I explain why ethereum Community should care about data availability on ethereum or no I'm kind of assuming that we all know that so let's just move on yeah that was my guess as well um I feel like there's a place for multiple data availability Solutions and one thing to understand that we're trying to solve different problems so in ethereum the da layer serves as as a way to separate the mark to separate the markets for data and uh and execution and also increase the capacity by separating that market and basically that allows Roll-Ups that are deployed on ethereum to scale better because you can now post more data and still settle on ethereum for Celestia and Avail uh the the the target market is a bit different they're targeting silver and Roll-Ups and protocols that don't really care about execution on the base layer but more just want some ordering and data availability guarantees and I think that there's even a way where you can combine the approaches for example validiums can use Celestia or Avail for data availability but still settle on ethereum so I don't think that there's a world where only one solution is required and only one solution is needed we need multiple Solutions especially if they solve different problems so it sounds like um ethereum is targeting different users perhaps I don't know Mustafa uh would you agree with that that you know your target is slightly different than you know what this ethereum trying to to actually do well I mean I can technically deploy seven Roll-Ups using ethereum SDA but I don't think that's like the that's not something that from a social perspective that I think the ethereum community colleges coalescence is around [Music] it's more overhead it's not overhead optimal to have a software roll up on on ethereum because you have to also like run a node that gets the state of the chain but ultimately the whole point of is modularism not maximalism the whole point is like it's not a zero-sum game um there's different trade-offs like if you're like if you ethereum Roll-Ups have to have on chain the a so if you want to deploy ethereum roll up you have to use it as a d a there's no way around that if you want to do a validium or celestium or optimistic chain then yeah you can use it off chain d a But ultimately um yeah this is all security trade-offs and different design different design trade-offs yeah I would sort of agree in the sense that um there I mean it's very difficult to demarket the Target segments in general like how would you uh demarket the users of a roll up with you know like a validium with the same stack of course the security properties are very different but uh I mean that's the reality that we are seeing on production right like I mean you have an arbitrum one with an arbitrary Nova you have a stack net with a volition uh built in right like and so so we are increasingly seeing this polygon also you know like announce their validium plans they have a roll up they have a validium and so of course the target signals might be different you know like some apps need the security of ethereum for example uh but you know like the rollup developers are asking for you know solutions that help them Target uh maybe different you can say there's a different Target segment it's the same Target segment but it totally depends on you know like uh what GTM these uh roll up developers and I'm not so so but yeah I would agree that from a more Sovereign Celestia are better options at the moment okay so let's dive in a little bit into those trials so that you know it's properly understood because I think I'm actually taking it from mustafa's uh talk in the morning that you know it's not easy to talk about this trade-offs so um on ethereum especially um we all know that cost of data is actually quite high for all the roll-ups to pay and obviously that translates to the cost uh to end users and that's why we're like exploring all these strange constructions like volitions validiums that are available the Committees even such uh weird constructions like optimistic data availability schemes so um which solution do you think will be ultimately the cheapest to choose as a da uh just you can raise your hand you know I mean yeah I mean so I think um who's the cheapest that's my first question available DMS versus Roll-Ups kind of thing the one that has the most capacity well I'm I'm I'm like you know thinking about typical uh Dev team that is trying to uh deploy uh either server in roll-up or a roll-up but you know for them uh the current da uh on ethereum is just expensive right so you know they would come let's say to us 12 to beat and they would ask us what do we think you know which solution might be the cheapest and they will provide you know the cheapest uh uh essentially block space for for data so I think there's two ways to look at this the first way is you look at like closet classes of different types of the a so for example like you can say like ethereum Celestia and um Avail are like one type of its own chain d a you have like a blockchain at the a then you have like other kind of like less secure types of Da like it decentralized sorry a data availability committee you know with six with like a multi-sake of seven you know that's what stock wear and any trust arbitramp have and then you can just have like a centralized like a single server so ultimately the cheapest is just like have a centralized server as a DA but obviously that's not useful so let's let's assume that we're talking about like on chain da the cheat like so the important thing to note here is like um in blockchains you cannot guarantee um low-cost transactions and the reason for that is because if you can guarantee free or low cost transactions then you have the you have a denial of service problem so it I think it's not about framing the question in terms of cost because ultimately there'll be free markets and it'll be supply and demand it's about firming the problem in how do you have how do you get the most report the only thing that blockchains can guarantee is the report they have or the block size they have because ultimately ultimately the fee of the pricing will be determined by supplier on demand and how much demand there is for that specific box base yeah I mean again yeah I sort of agree with uh Mustafa in the sense that I mean it's not good not enough to just look at the cost sense of course there will be throughput and you know our architecture allows us to create high throughput as well but also we have to look at other factors like decentralization and stuff like that right like I mean so I mean there are a lot of factors they it will be cheaper of course and we have we can increase throughput as such ah but as you said we cannot compete with the single server doing da right like I mean and there will be Solutions like that we are already seeing Solutions like that right like uh so the the thing is it's a trade-off between uh cost decentralization um you know like throughput and lot of other factors um and again just coming back to the point right like it's certainly cost is a factor but we also have to look at things like datability sampling light lines which provide new kind of powers to these roll ups right like I mean propagating the validity proves of fraud proofs to the users directly can users verify that directly for example is like how can we come up with those kind of constructions if we should consider that as well is and the light node is also an important piece because all the Roll-Ups that I'm running on Celestia right now like the sequences running on Celestia they're not running full nodes they're actually running light nodes like the sequences themselves are running light nodes and they're not paying for RPC endpoints or running a full node and that's just significant YouTuber than having to have need access to a filtered but that's no I don't think that the for a sequencer the cost of running a full note really matters like I run a couple of full nodes at home and like even if you assume that I run relatively high-end hardware for that the the cost of one node is less than one thousand dollars and considering that like sequencers extract so much Prof potentially can extract so much so many much profits from Mev etc etc I don't know I'm not saying that's like the main thing it's just like that's a like one Nuance for example unless yeah like the main cost ultimately it's about the cost of the day gotcha so what about the um security then I mean the common uh argument that I keep hearing is that um it's always a security trade-off to cross the trust boundaries when you actually using the external da so clearly the security of validium is very different than the security overall up right and the cost of validium is also very different that's why we've got arbitrum arbitrum Nova that's why we've got starknet or Stark X rollups and Stark X validiums and I was always wondering like ants users at the end of the day they seem to have very little to say like if you are let's say the idx user what benefit do you gain from DUI dxx reposting all the transactions as opposed to them using some you know data availability committee right you just want to trade on dydx and you want to have a reasonable guarantee that if things go wrong you can always like recreate the state from the data and you can always exit and and it seems that this is all about the trade-off right so with um you know increased security comes I guess bigger cost so how do you as an app developer or as a rollup developer suppose that I'm dydx how do I make this trade of yeah I think uh ultimately I think the choice will be decided by the rollup developer and I think not by the app developer directly I mean unless we're talking about app chains for example but before that you know we are seeing like a lot of roll-up orchestration players coming to market for example and I think these are the set of developers who are making the choice in terms of what data to use and I at least from my perspective what we are seeing is it's at the moment uh cost versus security trade-off kind of a thing so of course they want cheaper costs for the users that's kind of the primary metric but in general from a roll-up operational point of view right like I mean at the moment of course no decentralized DL air is in production which is going to change pretty soon and that's why you see this datability committees in and right now that may not be a problem but I definitely see uh you know like decentralization of the days similar to the decentralization sequencer questions that will come from a regulation point of view or whatever right like I mean in general people don't I mean is for now single sequencer datability committees are fine but I think uh just to safeguard regulatory interests and stuff like that I think for sure you know like things will move to a more uh decentralized context are you saying that this is the primary reason why the idx chose to go you know its own way and like be more decentralized uh I can't talk to their intentions of course but I think in general I didn't fully comprehend the whole reason why they moved of course they have more flexibility in running their own app chain as such and so they are able to customize a lot is what I feel I think the Stark X solution uh was a bit limited is what I I may be wrong in terms of what they wanted to do and of course Stark X has now I mean there's new upgrade the stock net is much more powerful Cairo 1.0 and such so I don't I can't subscribe their intentions but um yeah yeah so I think um there's kind of like an interesting question in here which is about how much do users actually care about decentralization like pure for my ux perspective there's no ux difference probably like be interacting with the AC versus interacting with the Unchained a but consider this um like the users today can use polygon because they can Bridge tokens from each to polygon and use it like pretty much as the same as an as a L2 or a roll up even though it's not but if that's the case then why did polygon you know spend a billion dollars on ZK on on ZK Roll-Ups it doesn't make it like doesn't actually make a um direct difference from a user perspective in fact it's probably like slightly less TPS at the start until we optimize the ZK premium systems I think ultimately this is what differentiates web T from web3 from a social perspective ultimately people um do coalesce and do care about the decentralization properties of the systems they're interacting with like otherwise why has no one just created you know a set like a centralized proof of authority L1 you know just 10 committee you have 10 nodes you know billion TPS you know like that's no one would take no one would take that seriously so that's why I think like yeah people should use the ACs for certain use cases but ultimately um for an application to be kind of like credibly decentralized and credibly um like have it's like a social like social uh you know Community around it doesn't need to be like at least decentralized the a as an option uh I have a small question for you with the whole validium talk are you trying to bring back the conversation whether validiums or l2's or not because I had a feeling that that's where you're doing there so I feel like from uh it's all about the trade-offs so because when you're building a certain application on a certain protocol you need to assume what is the worst case scenario that your protocol can handle and is it is that worst case scenario worth taking certain trade-offs so for example if you're building a game that doesn't really have any monetary value then you probably shouldn't use on chain da because it doesn't matter like worst case scenario your game assets just disappear whatever but if you're building an app that has billions worth of dollars deposited on it I think then you should take more care of how you design things because billions of dollars Frozen in a contract that you can never withdraw from that's a bigger problem than your game assets being frozen um okay so let's um maybe uh uh switch gears a little bit so here's another common question that we get uh from from users uh well first of all sometimes it's hard for them to differentiate between the data availability and data storage but let's assume uh for the sake of this discussion that we all know and for the audience that don't know maybe some of you can like give a very quick introduction but the question is on ethereum uh it seems like we have a very reasonable and strong I guess guarantee of the data storage because there's this you know huge vibrant ecosystem of of different explorers and indexes and whatnot and people just simply assume that it works right now if I use availa or Celestia how can it be guaranteed that in 12 months you know I will have access to all the data and I will be able to actually recreate the state yeah so I mean that's like a general question about the difference between data availability and data storage so um you know like even with with Pro Design shotting and then shotting trading also does not plan to guarantee the data forever it's like the current plan is to prune data prune blobs after 30 days and the reason for that is because like data levels there including ethereum especially available it's not it's meant to be like a real-time bulletin board to allow Roll-Ups the opportunity to get their data to um to make sure it's published so that they themselves can store it um so what is the difference between data so I would actually propose like I actually propose renaming data availability to data publication because I feel like that's a easier to understand thing easier to understand static availability is about proof of publication like proving the data was published so that people can access it um specifically well specifically in Celestia at the moment we don't prune the data blobs like they kept around forever right now and but at some point Arthur maynet the community will need to coalesce around how after what certain time point in time data blobs will be pruned but that being said even even in ethereum even on networks where blobs are pruned I actually still expect that the data will be permanently stored somewhere and permanently accessible to the public simply due to the stressand effect like it's very like the poor data storage only requires an assumption that a single person I ideally have more but the minimum assumption is that you have a single person storing the data and that's very easy that's extremely easy assumption to achieve on the internet I think bartek's argument was more that because ethereum has a such a vast ecosystem where you have rpcs bulk explorers Etc storing the data it's highly unlikely even if all the nodes proven the data that the data will be lost and uh so for example I think it was Ripple the at some point lost like a day worth of data because there are servers somehow had a bug or something so I think bartek's question was more sorry if I'm rephrasing it incorrectly okay I understand but I argue that would also still happen on Celestia and other the Laos including a bill like it's the the costs of running the download the cost of storing data is cheap enough that multiple people will do it even even the ecosystem is smaller than ethereum and also the other very important thing to mention is that the whole point of having data availability sampling light nodes is that um your Distributing data across thousands of light nodes and that will also help somewhat with the data storage problem assuming that those flight nodes are happy to store it for for a longer period of time yeah absolutely if these light lines can make their way into wallets for example uh I think you know like we'll have the data mirrored uh on the from the D layer to the lifeline Network and that can be propagated kept for a long time second is you know like just a point that if you look at the stack that we have built on like we've built on substrate jealous is built on tenement for example and so a lot of the Tooling in these ecosystems uh is also compatible uh you know like with the stack in general so um and in fact you know like we are also starting to work with a couple of teams who want to put Avail data onto ipfs onto filecoin uh for example um and so as Mustafa said you know like ideally you just need only one copy of the data but we I mean we are expecting that you know we've uh we've been working with a few infrastructure providers in the substrate ecosystem for example um and um and there is the tooling the ecosystem is more mature than let's say two three four years ago like in all these ecosystems in the substrate ecosystem in the tendon ecosystem for example and so we don't really have to build all of this tooling from scratch and there are a lot of you know like uh providers out there but as part of the architecture we already have a Lifeline Network that mirrors the data uh you know like uh and so I mean so we don't anticipate the kind of problems that tograul is mentioning in that sense okay so um you represent I think to my knowledge the three most known uh Unchained data availability Solutions but uh uh very recently um it seems like there's new kid on the Block somehow and everyone seems to be talking about it uh and I mean eigen da any of you have a spicy Take On eigen Da and the trade-offs and its role in this da kind of landscape uh just to before I go on to express my opinion I would like to add that I don't represent EF just just in case the anchor it kills me if I say something wrong uh I just accidentally stumbled on stage um I think from the perspective of ethereum eigen the makes sense because it utilizes the existing validators or a subset of existing validators to offer extra capacity that is much cheaper than Unchained da obviously the security guarantees are much different so it's not comparable with just yeah what are these security grantees isn't it such a just a fascinating Community yeah I'll make this I'll make things spicy like like first of all there's no dogs like in the a so it's very hard to even compare in first days like people keep saying oh like you know mantle and claim to yeah they launched right they claim to launched the main on the a but I had like other people are saying no it's not actually like I have no way to verify that because there's no docs anywhere and like like it's you know people say but there's no docs so when there's locks it will be easy to compare but from what I know so far and it there's a various set of trade-offs uh like first of all I'm very skeptical of the idea that you'll actually be okay so first of all like um yeah I'm scheduled the idea that uh there's enough demand for mistake Services I think there's enough Supply there's a lot of supply of validators wanting to be staked very skeptical there's demand and because that this is that at least it's very hard to bootstrap your roll up if you're saying like the to convince validators to come and restake if the initial rewards only come from fees which might be very small to start at the beginning um and I hope and the whole point is that the fees are supposed to be cheap in the first place uh secondly um as far as I know again the a requires a dual token model anyway because you can't slash data on chain and so uh like a kind of the piece the whole point of restaking in my opinion because you have to restate not only eth plot but you have to restart the eigen token as well yeah because you because you need you can't slash data on chain so so I was about to add up on this I I'm still not sure if that's a very sound model the whole data availability based on crypto economic guarantees because essentially the assumption is that if the data is withheld then the price of the state eigen token is going to drop and it acts as a disincentive so essentially you can see it as implicit staking like the one that chain link used before and I'm not really sure that that it's a strong enough guarantee for a lot of applications obviously as I said before some applications it's fine but like for a lot I just not sure if that's a because it's also potentially vulnerable to griefing attacks the same way methods Mathis d a solution is so yeah I'm not sure I don't want to like attack against the 18 watch when they're not here to defend themselves yeah and that's true I mean I I think last year's finalist Freedom was there and I really like him uh but you know we have to get to see um um a working system like I haven't seen that so I would deserve my judgment based on that first uh but I mean yeah I mean risk taking like not about eigen DM but restaking itself is you know it's a pretty neat idea but I'm I'm still skeptical of how it will actually work in production uh settings you know in terms of you know how many were letters sign up what is this actual security you know like giving attacks such and such so I I will I would want to reserve my judgment at least you know like I want to see something working before um worrying too much about it actually uh like from my point of view what I would love to see more is like more I think thorough security analysis of such potential attacks like griefing uh we had a discussion about that about two years ago or maybe it was a year ago when Mattis launched so-called smart L2 and optimistic da which for those that you know they seem to be publishing data directly to the storage uh without actually mentioning the D.A layer and if anyone like notices that that I was not published to the storage they could in theory challenge the sequencer and like force somehow sequencer to pose the data on chain so and the cost of such solution as you can imagine is extremely uh low uh hands fees on matis were very very low and I was surprised that um very few people actually bothered like discussing that from like first principles right I mean what what are the security uh you know trade-offs there and is that scheme actually viable to consider for anyone um the community was largely silent like are you talking about I looked at I looked at metis profile on LTB before this talk and the idea is to use a data availability challenge right but the problem is with data availability challenges is that that's that's um like they have as far as they haven't actually there's no challenge because finalized or purpose yet but in general the general problem with data availability challenges is that they didn't solve the data availability problem um like that was actually the first thing that people like vitalik looked at to sort of the availability problem but it doesn't solve it because of something called the fisherman's dilemma which is where um because data and availability is what's called a uniquely accurate unattrovisable fault which means that the Challenger who's challenging the data availability it might be their fault that they can't access the data it might not because maybe the network is not is not good um and that basically creates like a dilemma where yeah it's like explained on uh on like the ethereum wiki but it's like um you either have a situation where you have a dose attack or there's not enough incentive to make a challenge there's no incentive to make a challenge in the first place because the um the the a publisher might only release the data after we make the challenge and so like you're basically griefing you so there's no way to make it economically sound basically yeah it's basically as Mustafa described because um default is not interpretable it's impossible to attribute the default here you would essentially let's say if I create a challenge and you post the data you can both withhold it and uh you you could have withheld it before and that's why I initiated the challenge or I was just not in sync with the network and that's why I didn't receive the network and that creates a long-term griefing Vector where essentially you start the challenge you stake a certain amount I post the data when you initiate the challenge you get slashed and then you continue to get slashed and until nobody is willing to challenge you anymore because basically every time you slash the sequencer just reveals the data so there's no reason why you would long term continue challenging it and therefore it's fine if you consider the optimistic case but but in a worst case scenario this whole security basically breaks apart um okay so um I have another question like from a completely different angle uh so you guys uh mentioned and this is the thesis that we've been hearing and will be hearing uh especially in this conference uh it will be very easy for anyone uh very soon uh to launch their own uh construction uh on roll up with very Custom Security parameters and as it is even today for users to understand the security assumptions is extremely hard for us as an Orc trying to understand that you know we're like really uh for us it's a moving Target that you know we need to chase how do you see in the future with potentially thousands of Roll-Ups being launched uh uh how do you envisage that users will be actually able to tell uh what are actually the secrets and assumptions and how do you see the role of you know people like like us in this whole Space I'm very curious because I'm frankly terrified about the future you're describing from my perspective ah so so the way I think about it is you know like there is like a wide variation in terms of roll-up orchestration today uh but as you're increasingly seeing all the roll up Stacks are also tending towards standardization right like so for example you know like ZK VMS in terms of the implementations you know like are going in a you know like a similar direction as such for example uh so that process will also apply to the entire roll-up stack pretty quickly I mean uh better than anticipated and as I said right like a lot of these uh the major players are pushing then there then standardized stack to ah you know like this rollover service providers or such for example and so what we feel is rather than having a wide variation of Roll-Ups there will be a set of roll-up Stacks that will be pretty much standardized and the deployment itself also will be really not it will not be like a thousand different role of configurations it will be like maybe five ten different configurations but like a lot of instances in that sense and so it's it will be easier to kind of look at it of course it's not going to be as easy as I make it out to be but yeah yeah anything people should just use LGB like that's that's the solution you know I'm relying on you guys you guys are doing a good job so far um to be incredibly neutral and uh like um people trusted the way for people users and developers to get quick information about the security trade-offs between different the a layers and roll ups and ltus I think we open up opened up a can of Wars by by uh John specifically I blame John Char but new for everything by writing that article about social consensus or how Roll-Ups are real because now for the last three weeks or a month I've been hearing people making ridiculous claims about like Anatoly I love him but like his argument that if I post data on ethereum I all of a sudden become an L2 it just doesn't really make sense and so I feel like first before like we start discussing about how many Roll-Ups are going to be built on it on a different protocol we need to work together to Define what our rollup actually is because I don't really buy it as a whole social consensus thing because you can literally attribute it to anything and therefore like all the concrete properties that we have about consensus protocol signatures Etc can be just hand waved away through social consensus and so I think we need to start working on that first before we discuss what's going to be deployed where and we've just ran out of time so you know that might actually open the discussion for an entirely different panel so thank you so much guys I mean it was lovely to have you all here unless you've got like one last closing comment for Stuff I mean you've got to ask a specialist question which is like what's the report you know but I guess we are applying for that unfortunately all right thanks so much thank you very much thank you [Applause] 