uh i think i'm missing a slide but um hello everyone i'm alex evans as a dt set i'm one of the partners at bain capital crypto we're uh early stage crypto focus fund we spend a lot of our time on research protocol design engineering we have a particular obsession with modular architectures and blockchain designs and i guess in this presentation i want to share maybe some of the excitement with you in particular i want to talk about some of the scalability properties that mustafa sort of alluded to carefully in terms of what types of scalability blockchain modular blockchains enable and what types of scalability they don't and i want to spend a little bit of time talking about flexibility and the acceleration of vm and execution environment design innovation where i want to start is not by defining uh monolithic versus modular blockchains i think mustafa did a great job doing that but just to point out that like the concepts of monolithic versus modular exist along a spectrum and that most blockchain designs are positioned somewhere along that spectrum most uh smart contract platforms as we understand them today uh are becoming or at least adopting some of the ideas of the modular paradigm over time so in some ways you could see them becoming more modular and that i think paints that world in less of a binary perspective and more than one where these ideas of separating out uh d.a from uh execution are permeating into the broader blockchain space and i guess the argument for this talk is that there's a very good reason for that um and so the reasons for that are number one scalability uh the ability to have greater performance in our systems uh as well as um the ability to support more users while still allowing them to be first-class citizens of these networks and the second one is that modular designs unlock rapid experimentation across the stack in terms of da layers in terms of execution layers and so forth splitting up these monoliths into components allows us to experiment on each one and iterate on the best possible designs so i'm going to take each of these in turn i'm going to start with scalability it's a little bit of a pedantic approach to scalability just to make sure that in the time that we have we cover it precisely because there's a lot of confusion around what scalability means in these ecosystems and so forth just with a very simple rough definition of scalability we want to increase the capacity of our systems to handle more transactions cheaper but we want to do so in a way where users are still able to catch up to the tip of the chain or able to validate the transactions that have happened thus far are able to reconstruct the state or request the state with very minimal trust assumptions such that they're able to author their own transactions validate the transactions of others and importantly do so without trusting anybody else in the system that includes miners it includes other nodes that gives users full autonomy of these systems mustafa mentioned really eloquently this is one of the core values of the modular design ecosystem it also turns out to be a core value what makes to us at least blockchains and public elections in particular interesting so oftentimes we hear that this chain or that chain is more scalable oftentimes we're simply referring to cost um that we i guess the point of this part of the presentation is that we also need to consider what the requirements are on users be they full nodes or like clients that comes with that decreased cost right so sometimes we hear roll-ups scale ethereum or roll-up scale some other chain and i think we want to be a little bit precise about what we mean that is true that roll-ups are a way of scaling these platforms uh however not often for the reasons that are discussed typically uh people point to some graph like this they show the l1 cost being significantly higher than the l2 cost for the same transaction here is just the uniswap swap on ethereum versus optimism and they say okay one is roughly an order of magnitude or more more expensive than the other therefore we have scaled ethereum uh of course we're only talking about cost we haven't considered uh what actual users have to do to validate these computations that are happening on there too right another thing to note pretty briefly is again this is data from for optimism the vast majority of the actual cost that users pay in these roll-ups actually come from the data footprint on l1 in particular in this case ethereum uh today uh that is projected to go down uh through all sorts of things that are in the pipeline including making data availability cheaper on ethereum and so forth all this is telling us is that gas is really really cheap on layer two right um and we have to reason about whether that can continue to be the case or what we anticipate these gas price and gas market dynamics to be down the line and the way to do that is we want to think a little bit carefully about what the skip precise scalability of splitting up computation from data availability uh is right so just for the sake of argument let's take just like a standard monolithic blockchain let's create a carbon copy of it just as a thought experiment that does the exact same execution and just split it up into two different layers one is going to handle data availability and we'll also process state commitments commitments to the state as well as proofs that that state is valid in different ways and then all the execution is going to happen on another layer and the question we're going to ask is have we scaled anything just just by doing this and intuitively it would seem no because we're executing the exact same computation so for instance if we're running the evm on another rollup it's just another blockchain we're simply validating the same thing we need to run a full node on layer one to validate the data to download data and validate you know state commitments and so forth it doesn't allow us to reconstruct the state of the roll-up if we wanted to reconstruct the state of the roll-up we would have to run a full node that entire roll-up re-executing all the compute thus therefore not inducing a scalability benefit right um however we do get some other really important things just by the way uh we do get um uh cheap uh trust minimized like clients right uh we can just run our full node on layer one and if we just trust one of n nodes on layer two they can provide us that state we can verify that it is indeed the correct state it can allow us to do fast syncs it can allow us to do trust minimize bridges between rollups and lowers the work hardware requirements for users to participate in these systems meaningfully but in terms of running a full node so far we haven't achieved any scalability the scalability comes from two factors that are a little bit more subtle than just the pure cost of looking at layer two the first of these is horizontal scale and the second one has to do with resource pricing right so let's take these in turn this is indisputably a more scalable system than a monolith for the simple reason that it's essentially a form of sharding right so if i want to run a uh if i'm going to validate zk sync i don't need to also validate a bunch of execution on optimism right so nodes are splitting the work between them uh this is a form of scalability right we have increased the strictly increase the capacity of our system without increasing the hardware requirements on any one of the nodes in the system or splitting up the work between us now that scalability benefit is somewhat muted given the fact that a lot of the cost as i showed two slides ago is coming from layer one right so we still need to validate this really heavy evm we need to make sure we're downloading data that corresponds to a whole bunch of other rollups and so forth right uh over time as da cost come down on layer one call it eip4488 call it data you know data only shards and data availability sampling and so forth we get a lot more horizontal scale than you also consider uh pure data availability chains that don't have heavy vms to validate and the horizontal scalability benefits can become very very substantial that said they don't come for free they often come at the expense not always of composability and what we mean by composability just the fact that applications are co-located can be atomically composed with each other it's a really magical property of public blockchains wherein as a developer i can take different components from different underlying pieces of software and stitch them together into an end user experience such that the end user sees these applications almost as if they're functioning as a unit as one right this is really magical it's kind of what's enabled a lot of the innovation in d5 and other areas of web 3 that we find very exciting it it's we don't take it lightly to compromise i suppose on this property it's as we understand at least one of the unique properties of building applications on public blockchains right on the other hand um we have a lot of benefits to building splitting up execution and having multiple different roll-ups handle different tasks right including this horizontal scalability as well as sovereignty and flexibility in terms of being able to customize your execution environment to your end goal right so we have a pure classical trade-off as application developers inside of the modular world between composability and horizontal scale on the other side and i guess the question is going to be how do we navigate trade-offs like this one is to take the twitterverse approach which is to take really extreme positions and say hey you know we we want all applications to be on the same chain uh and not compromise on any composability ever of course that will mean that our chain's gonna be either really expensive to use or uh really centralized uh and then the other approach is that every single application you could imagine should have its own layer one blockchain right or sovereign rola right uh empirically we know that neither of these extremes are truly tenable and that the reality is somewhere in the middle and in fact the optimal point is somewhere in the middle and the question is going to be how do we find that optimal point incidentally by the way monoliths can also do sharding except that you know you need to pre-specify what each char does a priori the modular approach to navigating dilemmas like this and trade-offs like this is to let application developers is to let users decide with their money with their time what trade-offs work for what applications so you can imagine a game developer may want a very streamlined self-contained experience and they're very happy building a software and roll-up or a layer one chain specifically tailored to the experience that they want their own users to have maybe a defy yield aggregator is going to want to co-locate with a whole bunch of defy apps might be a little bit more expensive as a consequence but the composability they get out of that greatly outweighs any slightly increased cost to users right the modular approach is to run a whole bunch of these experiments in parallel and for each application iterate towards a more optimal design when we find those designs we can continue to iterate on them and continue to explore and run experiments and get better over time as an ecosystem okay so this second scalability property of modular blockchain designs comes without that many trade-offs it is essentially a pure increase in capacity that comes from more efficient resource pricing so recognize that monolithic blockchains price resources as a bundle so you get some gas you can then redeem that gas for certain bytes of data availability you may also redeem it for certain opcode executions right those resources have a fixed price relative to each other what does that mean it means if demand for one goes up um even if supply and demand for the other does not go up the price of it will go up which strictly decreases the capacity of these systems right on the other side on the modular paradigm where you split up data and execution if demand for instance for uh running certain operations inside of i don't know zk sync goes up uh data availability for optimism does not go up which is a reasonable thing to say because those things are completely orthogonal resources so we strictly get more capacity in our systems through more efficient resource pricing without increasing node requirements in so doing in fact this property is the more congested the system the more powerful this property is it's another way to think about it is we have a constrained optimization problem to allocate resources in such a way to maximize the scalability of our systems we simply remove a constraint from that optimization problem right with the constraint that the prices are fixed removing constraints from optimization problems very often especially when when constraints are hard and sad and uh and met for instance in case where there's a lot of uh demand for the system greater than the capacity they can handle at any given time strictly allows us to achieve greater values for the objective function right so relatively trade-off-free way of skating scaling blockchains all right i want to change gears a little bit and talk about experimentation and optimization and this is really maybe the meat of the talk it's what excites us the most about modular ecosystems and designs that we're starting to see percolate throughout the space it's that by splitting up monoliths and allowing individual components to be optimized we get a lot more experiments run in parallel and we get a lot better data layers we get a lot better execution layers as a consequence we get more scalability but we also get new capabilities in these systems that we couldn't have imagined otherwise maybe those are about privacy or maybe they have to deal with application specific concerns so just as a prequel we've seen an incredible amount of innovation on public blockchains a part of it is the and more of it happens higher up right a lot of iteration happens at you know the ui layer you can push changes every hour if you want to smart contracts move a little bit more slowly you have to audit them hopefully at least and you uh you need to make sure that you know whatever you build you know there's hopefully some users on it and so forth it's not as easy to just iterate um you know on a day-to-day basis maybe on it's on the scale of months but even in spite of that there's an unbelievable amount of innovation that's happened and the reason for that is is for instance if i am curve and i see unit spot v1 and i'm excited about that design i can say hey i think i can do that better for stable coins i don't have to go and fork all of ethereum underneath me i need to fork the consensus layer anything i just build a smart contract i run it that experiment if people tend to like it maybe i'll get a bunch of liquidity uh and people will like my product right similarly sushi swap and fork unit swap and add an incentive unispot v3 can take ideas from everything else that's been built including ads some of their own and we're running a bunch of experiments in parallel figuring out what the right way to do decentralized exchanges and then when we find ideas that work we double down on them in contrast the underlying infrastructure consensus data and execution moves glacially it moves kind of closer to the speed of hardware than the speed of software and there's actually very good reasons for this it's like you know there's like hard fork release timelines you know on a year scale kind of similar to how quickly i you know apple releases iphones um and the reason for that is is let's say i want to make for instance a change to the execution environment of ethereum maybe i want to enable greater parallelization in the evm i can either launch my own layer one uh from scratch right i can build my own consensus system or fork the consensus system and recruit my own validators then build the execution environment that i want and building layer ones is really resource intensive or i need to uh get you know an improvement proposal salon improvement proposal ethereum proposal push through the core developer community there's reasons why that's structurally hard to do the main one is the fact that like a lot of people use these systems and we can't just try random crazy ideas with low probability of success because people's livelihood depend on these systems actually being resilient right we don't want to break them so we want to be a little bit conservative with how we iterate on these designs because everything could because again they're tightly coupled and so one change for instance in the execution could screw up consensus and maybe performance improvements in one part of the system can cause performance degradation elsewhere right so structurally these things are slow moving uh but we still want this capability to be able to run a bunch of experiments kind of a little bit more similar to what's happening at the smart contract layer and i think we would benefit a lot as an ecosystem with a lot of parallel innovation happening in these systems okay so i guess my point here is that modular blockchains enable this uniquely uh by splitting up execution from data availability and consensus it is now easier for for me to launch a new execution layer should i choose to right so if i have an idea maybe i want to enable access lists in the evm to enable parallel reads and writes from storage uh maybe a lot of people disagree with this decision but i want to run the experiment i now don't have to fork the entire da and consensus system i can simply launch a parallel execution environment now it's still a little bit harder than launching a new smart contract i need users to validate i need nodes and so forth to be running on this and for instance if i need to build my own sequencers then i need to recruit a validator network it's maybe not worth it if i just want to kill the self-destruct instruction in the evm that everybody hates but it might be worth it again for something like parallelization which is just an interesting experiment to run right if i happen to be right users will use my chain they'll validate my chain they'll run nodes on it they'll liquidity will migrate to it and if i'm not nobody will do that but the cost of experimentation have been brought down materially relative to launching a new layer one from scratch it's easier to launch layer two it's easier to launch a sovereign modular execution environment um in these contexts the other thing is that as a developer of an execution environment i only need to optimize for the precise use case that i care about somebody else is handling da for me somebody else that might be handling consensus i need to think about who my users are who the developers on my execution environment are likely to be and how do i best tailor to their needs right similarly for vda layer the a layers can specialize and focus on providing the best possible experience for people building execution layers on top of them right so what does this imply it implies that at the same time we're able to run a whole bunch of experiments on execution layers that means we could get execution layers that are more performant for general purpose applications maybe special purpose execution environments for different use cases my colleague wei from bcc has a proposal to add an additional instruction to the evm to process sapling circuits and enable anonymous uh transactions uh and composable defy anonymously there's all kinds of different things we can try some of them are going to fail some of them are going to be terrible some of them are going to be centralized scam execution environments that claim to solve everything without any trade-offs the point is it's going to be really fun a lot of people will try many things maybe your nephew will shield you execution environments at your next family reunion uh maybe you yourself will build an execution environment and if you do we hope you call us thank you very much pause now for questions [Applause] do you need to get your mic or yeah there you go beautiful on the note of orthogonal resource pricing if you have an l1 that supports on the central planning when it comes to essentially creating efficient markets to maximize resource allocation for l2s is it the case that you can get quite a lot of benefits when you have multi-dimensional eip 1559 or you reduce central planning and you say like you have individual gap code operations um to be free floating or is that only a half-hearted solution and you really need to have complete orthogonal uh separation of orthogonality on the different layers versus to allow a free market for the usage of layer one resources by layer twos um i think both of those achieve a similar thing right so like if if you are able to price resources separately in every instance and that you have fixed absolute prices that are given by supply and demand for scarce resources then you've accomplished this goal it just a simply much easier goal to accomplish in a modular architecture because in a monolithic architecture there's other concerns to doing that right and even eip-1559 is not trade-off free right and i'm not saying it's trade-off free in the modular world either it's just it's a different way of exploring what those trade-offs might be by just running a bunch of experiments in parallel right it's a great question and then quick follow-up in terms of the trade-off between having dap chains or very specific l1s for applications with a limited composability and then the other sense of having a extreme gas limit universal l1 for instance that everything is operating on with limitless composability or far fewer constraints there do you foresee there would be multiple optimal points rather than say a single optima and then as time evolves and these interact would you imagine it would converge to say a bifurcation or a single optimum point or do you imagine say there would be benefit for extremely low gas limit l1 to live alongside say for instance very secure bridging in adapt chain environment right i think we're likely to see that uh we're i mean there's gonna be different optimal points for different applications right by definition some will choose to co-locate some will choose for instance to break off um and then for instance they could break off and then start to build other applications on the same chain because there happens to be a lot of liquidity there and then these these optimal points are not fixed in time like are not fixed across time right there might be something that works now then there's a whole bunch of users tomorrow and there's an optimal point in the future it's just the experiments are running in parallel we're already starting to see this right there's different uh roll-ups with that are exploring different trade-offs there's different ones with different trade-offs there's application specific l1s this is the world that we live in right and we're starting to see increasingly like what types of applications might work in one environment versus another we haven't fully explored that space yet that is for all of us here to do awesome any other questions nope oh so there we go all right these are pretty evenly distributed across the room i know right to make you exercise here you go thank you hey um could you you've spoken a lot about the technical benefits of a modular blockchain architecture but if i were to build my own l1 one of the things that i want to do is hold value in my token and for something that's going to provide me this data availability layer to replace the need for my consensus has to be paid for do you see that as being something where we can have a lot of different tokens something like terror where you get fees paid in all the manner of tokens that are being used on the network or is that something you see more in the kind of ethereum space where you're paying an eth to write back to the chain um there's a couple questions in there just to make sure i understand it right so one is uh application developers or roll-up developers wanting to have their own asset but if there turns out to be some use for that asset then presumably it has a reason to exist maybe it does sequencing or so forth that you care about maybe there's some property to sequencing that you care about if not then you know might not have a reason to exist and so you know these are the debates we have on ethereum all the time right with different applications anyway and then in terms of for regular users i expect like so for instance if there's a roll up that has its own sequencer or you pay in that native asset then you just outsource it to the validators right to go pay celestia or ethereum or any one of these data availability uh solutions right so or some some some of these operators have users paying whatever token they want right there's different trade-offs that have to do with ux i suppose my personal bias um obviously different people will have generalizations like to abstract some of the complexity away from users as they're transacting you're thinking having to think about 50 tokens and the relative prices of each other is just not a fantastic experience does that answer your question yeah it does thank you i think first yeah as you say abstracting away the complexity okay you use our l1 token but it's not an l1 it's it's actually provided by the data availability layer and then we handle the the costs and the gassing on on the underlying consensus layer through the validators right that makes sense yeah i i didn't get that you could do that on the validated layer because obviously they're a bit more smart than uh than i initially thought so thank you any other questions i think you're done thank you thanks alex yeah [Applause] 