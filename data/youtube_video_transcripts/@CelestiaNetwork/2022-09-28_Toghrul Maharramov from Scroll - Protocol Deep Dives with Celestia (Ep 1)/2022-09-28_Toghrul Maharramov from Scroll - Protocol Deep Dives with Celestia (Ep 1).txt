hi everyone this is John Adler um Mustafa al-basam from Celestia labs and we're joined with uh toguru a researcher at scroll we'll be presenting a scroll ZK roll up photograph would you like to introduce yourself and give us an overview of the technology behind scroll hello my name is tall girl as John already mentioned I I do research at scroll mostly on the protocol side don't really do the cryptography and scroll is a ZK VM based zero knowledge roll up that is going to be deployed on ethereum at some point in the future so what a scroll scroll is a zero knowledge roll up that is built on top of ethereum and as zkvm based uh what is cqvm ziki so you might have heard a lot of different things about zika in the past few months and there are also quite a few flavors of zikivium so what is scroll ctvm so firstly the scroll ctvm has co-built together with ethereum foundation's privacy and scaling Explorations team it's been an ongoing collaboration for more than a year and it's an OP code equivalent implementation of evm and what I mean by upcoded equivalent is that the implementation follows the specification of the evm up codes from the ethereum yellow paper with the only one missing is self-destruct we're not going to add it but bear in mind that self-destruct is going to be removed from ethereum or substituted at some point in the near future so it doesn't really affect us so we're gonna just add the the opcode with which it's going to be substituted and we're type two and a half according to the famous vitalik post about Z kvms and different types of gtvms despite the fact that in his article it says that scroll is going to be taught too but we're not because type 2 requires an equivalent gas metering to ethereum and we just don't think it's practical for a couple of reasons one is because if we were equivalent in terms of gas metering that would open up quite a few DDOS vectors because let's say Computing a ketchup hash function in circuit is much more expensive than Computing addition inside the circuit and therefore the difference between the out of circuit pricing and inside circuit in circuit pricing is going to be quite drastic and secondly even if we were to support it and attempt to mitigate it that would require us to sustain the same ratios between the fastest and the slowest uh up codes as there are in ethereum which means that we would have to essentially add domain gates to the faster op codes to make them slow and so to retain the ratio which doesn't really make sense so we're not going to be type 2 we're going to remain type two and a half which means that we're going to retain all the properties of evm aside from the fact that we're going to have different gas prices per of code and what Scrolls Mission so scroll is this main mission is to help ethereum on board the next billion users so we know that ethereum has there's quite a demand for ethereum blog space but ethereum just can't facilitated so and we think that with more block space there's going to be more users and also more different ideas of how to use that block space which ethereum doesn't really allow now because of the pricing and we want to achieve that by offering the most ethereum-like experience to both the users and developers and what I mean by that is that when you're deploying an application or just playing around with an application ideally you shouldn't feel the difference between using scroll and ethereum I mean there are there are going to be minor differences but we're going to strive to be as similar to ethereum as we're technically allowed to be and within open sources day one and we're also Community oriented which means we're open to collaborations and we welcome people to contribute to our code or our protocol in general so why did we we choose to build a roll-up so in our case so so in in case of ethereum a robot can have two purposes obviously if you're building on Celestia or building somewhere else you can you can build for other reasons but for ethereum specifically you would either build to extend the throughput of the base layer in a trust minimized manner or you would build it to introduce new features to the protocol Enterprise minimized Manner and what I mean by trust minimizing this case is that in order to bridge between the roll-up and ethereum you need the minimum assumptions possible so I hope the assumption is that the zero knowledge proof used to compute the validity proof is secure and in case of an optimistic grow up you just need to assume that there's one honest operator of a LT over optimistic grow up node is capable of challenging it within the window within the dispute window and why ztvm so firstly it complies with our vision of offering the most ethereum like user and developer experience and secondly as Alex govski from ZK sync livestref refer to it evm is the lingua Franca of general purpose or contract platforms and what it means is the majority of the developers in in blockchains are familiar with solidity most likely and if you're a newbie and you're just a developer from traditional of with a traditional background and you hear about the smart contracts for the first time if you Google developing a smart contract the most likely thing that you're going to encounter is the tutorials on how to build on solidity and the most of the documentation is available on solidity so uh yeah and what's what's the current stage of scroll so we just launched a pre-alpha that's not a couple of months ago a pre-alpha test that in our case means that uh we basically allow users to sign up and we let them into the system one by one also the deployment of contracts is permissioned which we deployed a couple of forks of unit swap Etc on on on on the on the test net to let the users play around with it but users can't deploy their own byte code and that's where the public test that comes in so it's currently in the works I don't really know the exact date on when we're going to launch it but it should be in the near future and the public test that will be completely permissionless meaning that you can play around with it as much as you like you can deploy whatever you want on it etc etc and now for the future so the mainnet is found to be released in two phases so phase one will have a centralized sequencer and a decentralized proverb and phase two is going to have decentralized sequencer and decentralized proverb so now a bit about the design decisions that we we have agreed on early on and we're going to stick with so firstly we're going to publish transaction data on change router than the state divs and the reason for the there are two reasons one we believe that publishing transaction data will allow us to decentralize the sequencer easier and also it allows us to have a system similar to how optimistic Roll-Ups work from the perspective they as long as you run an L2 node and the data is finalized on chain you can essentially consider the data to be finalized so the only reason why you would need on-chain finalization is for withdrawals which is not really possible if you go with state divs and secondly because you need to read one quick question but does this transaction data include signatures as well or are they just the transaction payload So currently it includes the signatures but we're thinking about compressing the signatures with a validity proof and just publishing difficulty proof so long term we're thinking about doing it that but currently we don't have that implemented and uh the second reason why it's it it makes sense for us is because if you don't have the transaction they don't change you have to trust some external data provider to provide you with the transaction data because if you call you can't really derive the transaction history from the state disk whereas with transaction data it's all there and you can just derive whatever you want and the second one which we think essentially defines a roll up and roll up without this feature is not really a roll-up is that the user should be able to force transactions into the rollup chain and what I mean by that is you can submit the transaction through you know one validating Bridge and it will force the sequencers to include it into the follow-up at some point so let's say you can give it a few hours and then if no batch just include that transaction within the couple of hours you know more batches are accepted by the welding Bridge without that transaction because it's while there's some Roll-Ups take an approach of withdrawals in case you're being censored uh firstly withdrawals our non-trivial to design in a system where you have arbitrary computation because you can have a scenario let's say when a transaction is locked uh your phones are locked let's say in an LP or unit Swap and you can't really withdraw directly you need to First wind down your position and then withdraw and secondly we just think that this is a more elegant approach so uh the architecture right so in Phase One the architecture is going to comprise of the following things so we're gonna have the ethereum uh validating which will be which will comprise of the bridge contract and the roll-up contract and I'll explain what they are later and we'll have two different nodes one is going to be the scroll node uh and the other is going to be the prover which we call the roller and what a sequencer is what we refer to as a scroll node and it's comprised of three components a sequencer a coordinator and a layer so the sequencer is a modified version of gafs which has essentially the similar responsibilities to an ethereum full node we just modified a few things for example we changed the Merkel Patricia tree to a Merkel tree and we use Poseidon hashing everywhere outside the evm itself so so there are a few more changes but overall the the code is quite so the the difference is not really messed up with with the original Gap implementation and we also have the coordinator which is essentially responsible for propagating the execution Trace of a bashed and elected Brewer so how this would work is you execute a batch let's say comprised of 20 transactions you extract the execution Trace after you complete the execution and you send it to the proverb and that's the responsibility of the coordinator and then their layer is responsible for monitoring the status of the robot blocks and deposit withdrawals events on the on the roll-up and ethereum so and for the prover so so the prover is architecture is comprised of these things and uh basically first it receives the execution traced uh it then builds the inputs from the execution Trace that it receives and the ztvm circuit is split into seven different circuits so uh so one for storage one for Ram etc etc one for cat check and it computes all those circuits and then all those proofs and then it Aggregates it into one proof that is basically deeper for a block and so the the prover is responsible for the Computing as I said before and once it computes the volatility proof it sends it back to the coordinator and we refer to the proverb as the roller in our documentation and uh I've already explained this and now the welding English the validating approach is comprised of two contracts one is the roll-up contract that handles the canonical chain and the non-finalized step so let's say if you commit the the new batch there's no finalized yet so the Roll-Ups responsibilities to handle it until the validity proof is submitted and it verifies it and then it's just assumed that that tip is finalized and then the bridge contract which handles the arbitrary messaging between the ethereum and the roll up so you can pass both the branch build the standard erc20 tokens etc etc and also just any messages you would like to send and this is how the workflow is for the phase one so essentially as you can see the sequencer commits data to the raw contract uh and then through the coordinator uh it propagates the execution trace for that particular batch to the roller so let's say a ruler one in in case of number one it computes the proof and then it sends it back to the coordinator but instead of the coordinator directly submitting it it's the roll-up contract through the sequencer which would be quite expensive what it does is it waits for a few more proofs from for different blocks batches and then propagates it to another ruler which Aggregates all those validity boosts together returns it to the coordinator which then submits submits it to the sequencer which allows us to save some cost on verifying every single batch on chain ability proof for every single batch on chain and now the phase two which will have the decentralized sequencer so first thing to note this is still open area of research so a lot of things may change by the time it's released but this is what our current thinking is uh we're gonna deploy PBS style model in which sequencers perform the roles of the builders and the previous perform the roles of the proposers we're going to add sequencer quantities which enable the economic finality guarantees prior to Unchained finality so let's say you could commit the data on chain and if you have enough signatures you you assume that it has economic finality I'll explain what what that means later and uh an on-chain challenge mechanism and I'm going to be challenged to dispute the volatility of a batch commitment I'll also explain that later and as I said it's still under active research so things may change so the pfps model is the goal of it is to solve the incentive imbalance between the sequencer and the previous and what that means is because the sequencers can extract as much and maybe as they like if we don't implement the model like that there can be a scenario where it's not as profitable to operate approver as it has to operate the sequencer and so the incentives would dictate that it makes more sense for somebody to become a sequencer rather than approver and we want to avoid that and by deploying the PBS model we can maximize the pro the profits of the previous and essentially lead to a scenario where the incentives are more balanced between the sequencers and the Brewers so how it would work is we would pseudonymly extract multiple sequencers per slot and then they would propose the candidates which essentially bids with the hashes of the block to singles that are randomly like the proverb and the proofer select the one most likely with the highest bid because there's no reason why it shouldn't and then the sequencer would propagate the block for for the hash that it previously propagated and then we will go to the next stage after the previous the candidate and receives the block the the sequencers all the sequencers vote on the selected box so it's essentially a mini consensus and if you get enough votes there is economic finality because let's say if at some point somebody challenges the that commitment all those sequencers can get slashed so essentially it's a system where if you vote for an invalid vouch you can get slashed through your point and how the challenge mechanism works we still it's still an area of active research we still don't know if that's going to be efficient enough to put everything in circuit but ideally how the challenge mechanism would work if you would just input the control compressed batch and the circuit would deal with the compression and everything and even if you just input an arbitrary blob and then put the previous state it will just prove that the output state is equal and to the input State and so essentially if let's say you commit an incorrect batch that has an invalid transaction in it and you say that the state route is going to be this but then you can be your disability proof and it says that no actually the state route is going to be completely different you can just put the validity proof on chain the contract would verify it and it will basically dismiss the commitment batch and also slash all the nodes that include all the sequencers that are voted for it and that's basically the whole presentation and if you have any questions feel free to ask that was awesome thank you toguro uh first I'll mention that based on based on history maybe it's not so good to call your last phase phase two because that didn't go well for the last project I tried that uh so I do uh well first of all I'm very excited uh at the kind of first CK evm DK roll up uh to be to be introduced uh so this is all super exciting from someone who's been following the Rob space for a while uh I have a few questions the first one is with respect to uh the gas schedule that you were mentioning earlier that the your instantiation of the ZK evm has the same functionality in in the instruction set except for gas except for the gas schedule uh do you see this as kind of a fundamental limitation of ZK I don't want to necessarily says you can evm but just like these kind of tune complete ZK systems uh do you see it as a fundamental limitation of them that they will basically for the foreseeable future have a very different gas schedule Than A system that is not DK based yes I I I don't see how you could basically have an equivalent gas metering system to um ethereum in a GK in circuit currently um OPC there might be some breakthroughs and we things can change but currently the difference between let's say addition and hashing getcha caching and circuit is so drastic that doesn't make any sense to follow the schedule of uh of evm and the gas metering schedule of AVM and can you give us kind of maybe an order of magnitude approximation of how different that ratio is compared to the evm I'll be guessing at this point don't really have have the numbers on the top of the top of my head but it's going to be quite different in terms of cat check or most of the other op codes should be quite similar it's mostly to storage and and ketchup that is that is the problem okay everywhere that hashing is involved is an issue for us and everything else should be not that problematic there might be a few exceptions but most of the most of the time everything else shouldn't be that problematic all right and do you guys by any chance provide something like a beside and hash uh pre-compiled or something or are there no extensions to the evm So currently there are no extensions so we are discussing internally about adding a few pre-composed that evm doesn't currently have in case people would want to use it for one reason and another but currently it's basically the plan is to have the same functionality of ethereum has how do you um currently verify that so you said that you mentioned that you posted transactions on chain how do you verify inside the Decay roll up that the input data was posted on chain to ethereum because presumably you need to use some kind of commitment scheme right you mean uh inside the the roll-up chain or or when you compute the validity proof and I'm not sure I'm understanding inside the roll-up chain how did yeah how do you how do you know how do you check that the uh transactions inside the roll up match the on-chain data so the assumption is that if you're a sequencer or approver you run both an L2 node a theorem node so you can just basically check that your data for roll-up matches the data on ethereum committed to ethereum but wouldn't you need to feel the pre-bro with the easy to download all data and yeah we yeah so in the current model what we do is because the sequencer is centralized and it's trusted in a way and it's operated by us at the previous just trusted the sequencer commanded the correct data but uh uh for a decentralized sequencer yeah you would I would we would either have to have some form of stateless client where you would just propagate the uh the the the branches that are relevant and then you would just check that the oh no actually no that one actually I mean so I guess what I mean is like actually forget the people on the ethereum resp contract so you put how does the Ethan smart contract know that the Unchained data post to that smart contract is the same data utilized or outputted by the ZK roll up but then you have to like hash today or something like using 256 or something like that oh you you mean you mean when you put the data on chain uh yeah you you store the hash in contract and then when you when you append the validity proof to that badge you input the the hash of of the of the data that you originally the batch commitment that you put in circuit and then you prove in circuit that the data that you're proving the validity proof for is equivalent to the data that you committed to on channel so when you saw the hash on in in storage what hash function are you using uh catch up the one that ethereum uses I see and then you also have to use catch act inside the roll up as well to check okay I see okay and and this is gonna be that's that's a that's a a significant source for the proofing cost I guess yeah that's that's going to be quite expensive here and it's just a single hash not a Merkle tree no it's just at the moment it is a single hash I mean we could change it in the future but it doesn't really make sense to use remarkable tree rather than the hash because you need all the data from from from the document essentially there are applications for instance if you have a an off-chain light client uh then they would want to get for instance a miracle proof that their transaction was included in a block uh without having to download full block data oh fair enough yeah yeah okay which aren't relevant at this stage for a lot of Roll-Ups uh but in the future I imagine that we'll have a lot more things like off-chain like clients being being a thing but that's the problem for futures yeah I guess uh also bear in mind if if we work the computer Miracle tree rather than a single catch up it's going to be much more expensive so in circuits so it would just add additional costs that is unnecessary to all this at the moment yeah yeah I get to be like as twice as much as expensive because you have to do twice as much hashing roughly it will be more than twice because because in case that you're hashing once you're you're just basically you're not doing the entire process recursively you're just adding the the inputs into the sponge whereas here you have to basically you get checked recursively for as many times as you need to compute the Merkel 3 so it's going to be much more expensive than twice right I see do I are asking as I mean asking because um we're also look especially also looking about how um CK Roll-Ups can use Celestia use easy to use this last year data route for the a we use shot 256 for the medical route so the Roll-Ups will have to be verified six Miracle tree but it would have to verify a bug which we rather than a take linear like a single hash I'm currently thinking about all the trade-offs there presumably the bigger the leaves are there and the leaves are the less overhead um because nonetheless the smaller the tree is so you know that's something we're currently um kind of exploring internally as well yeah uh okay I guess next next question which is uh currently uh are you guys only doing CPU approving now we have a GPU implementation of the program and we're approving on gpu's mainly okay even today you have a parallelized GPU approver yep awesome and you foresee in the future uh the usage of things like fpgas or Asics for proving zkevm circuits so we we are talking with a few fpga manufacturers that specialize in like in zero knowledge proof and we would like to experiment with it but we're not sure if it's going to be that the benefit of it is going to outweigh the costs essentially so we're not sure that the the Improvement in terms of efficiency wrote the gpus is going to be drastic really can you provide some intuitions on why that would be the case because Asics can often be substantially more performant than GPU so in the in the in the current proof system we need a lot of RAM and and so essentially we're more constrained by Ram rather than processing let's say yeah we're trying to minimize it but but the current I think the current uh uh Hardware respect that we're running the proveron has like 512 gigabytes of RAM so yeah Abram so that's that's before getting to like the gpus or are you talking about like the total GPU while proving uh yeah got it okay well that's I guess that doesn't it does make sense if if it's all Ram bottlenecks you won't you probably won't gain too much by using an Asic okay uh so I guess follow not follow-up question but subsequent question uh you mentioned earlier in the slides that you had uh the current phase a centralized sequencer and a decentralized set of provers uh can you maybe expand to it more on intuitions around things like proof or markets and so on but there's a pretty nuanced topic that's different than say approval for mining yeah yeah like um some of the approaches that I've seen taken by other ZK Roll-Ups is essentially make improving each slot a complete each batch of computation between multiple Proverbs but and it's essentially advertised by some as proof of work but it's not because in this in proof of work you have Randomness so basically your probability of winning is uh basically uh corresponds to the to your hash rate relative to the to the total hash rate in the system whereas in here the the the the your probability of winning is almost 100 if you have the most efficient Brewer and so uh if you if you do something like that it has a centralizing effect because it will descendivize others to prove and our thinking is that it was inspired to some degree by Celestia by the way because in the last day you have this idea the more light nodes you have in the system the bigger the blocks you can have and so our idea is that the more you can parallelize the proving the bigger throughput you can have because you can just publish let's say 500 batches assign them to different proofers and then just aggregate all those validity groups into one validity proof that goes on Jane and so uh our thinking is that we need to incentivize the reverse to essentially have larger throughput and there are a few ideas that we're exploring currently the one that I described already is a PBS like model where you share the Mev profits with approvers but there are a few other things that we can think we're currently thinking of yeah awesome and uh to tie into this my understanding is that based in the description you use some sort of like recursive proof scheme or you have like different components of the evm and then you kind of have this aggregate proof that has recursive proofs could you maybe contrast this with the Cairo model which I think has like a more monolithic CPU style thing with with a single proof uh uh so as as I described our our proof system has uh RNG KVM circuit has seven sub-circuits so for example catch up is complete it's a completely separate circuit and then we have a separate circuit for storage for memory Etc and we have essentially two layers of proofing first we prove the sub circuits and then you aggregate those sub-circuits into one a single facility proof whereas in Cara we just compute the proof without needing to compute the sub-circuits first and are there any benefits last trade-offs of doing one versus the other I think if we could do it in one circuit we would but it would just be too inefficient because aggregation does isn't free it it costs time and also computational Cycles so if we could do it in one circuit we would do it but it's just it would just be too costly to do it in a single circuit and therefore it makes more sense for us to just split it into multiple sub-circuits and then aggregate them together makes sense uh okay I guess then following up on my earlier bench off point uh which is that you talked about having a centralized sequencer and then decentralized Rovers and then in the next step to have decentralized sequencers along with also decentralized Hoovers what are kind of your plans slash considerations around the decentralization of sequencers so firstly we want to minimize the overhead as much as possible so I I know that other protocol projects are exploring adding tendermint or hot stuff or other consensus protocols on top of it and we we are thinking about minimizing it as much as possible so we we still would want to have Commodities but ideally what we would do is we would have the ethereum Drive the consensus essentially so you would commit the signatures to ethereum and it would take care of the timing and everything uh uh but if we were to have blocks that are shorter than the blocked the slot times for ethereum we would need the consensus essentially because there's no avoiding that it's a current model we're aiming to minimize it but we're open to the idea of adding a consensus at some point in the future got it so it'll be something along the lines of approval stack and census protocol like tendermint or something something simple like that yeah yeah temperament or hot stuff or something along those lines yeah okay I mean seems pretty reasonable no no complaints here uh another question oh go ahead so you mentioned before I think like so one of the complaints about the evm is that it's it's hard to parallelize what you mentioned before like uh there's a ways to paralyze it but maybe you could expand on that because like at the moment like the bottom like for current evm equivalent or compatible roll ups is the fact that um ebm is not paralyzable and you know optimism and arbitrim I think oh optimism like reached like like limitations about armor correctly because um you know they just the execution notes just exceed in all transactions like how would you parallelize it because I think you mentioned before that there are ways to paralyze it so you could have optimistic parallelization where you enforce the access lists that are currently optional in ethereum and uh just uh so the model would be similar to how Solana does it where you where you list all the slots that are you you're touching in the access list and then if you if the transactions try to touch a slot that is not listed you essentially in two transactions so I should try to attach the same slot you refer to sequential execution I I think there's somebody has done experimentation with this a year or two ago and there was a even an implementation of GitHub and in their test in their rudimentary test they achieved like five x execution uh throughput um but but I think it could it could probably be optimized more but yeah so that's that's the lowest hanging through food that you can basically uh take in terms of how you can parallelize execution but we there are other approaches but that would probably require modifying evm which we don't know like would there be a gas penalty if you try to access something that's not an access list yeah I see yeah I think I think that makes sense because like most like assuming that you have different contracts they're probably not going to access the same state anyway so uh I I think um in most cases that's fine and in the cases when it's known we can just fall back to the sequential execution which should happen very unless there is one dominant contract that everybody is attempting to use on one specific slot in that contract that everybody is attempting to write and read from yeah that makes sense I guess we have to maintain like locks for each like State uh yeah yeah I think it was Brock Elmore who developed episode on alchemistic concurrency I did talk to him about this not too long ago I have concerns around optimistic concurrency because it's optimistic in the sense that it's fine when things are a happy path but a lot of things in blockchain you have to optimize based on worst case performance and and the non-optimistic case someone could you know potentially have some denial of service Vector for instance you know they could have something like transactions you know a bunch of transactions that all call the same sequence of contracts so now you like execute them parallel and then okay then it's like I screwed them back and and in series or versus parallel and then potentially you have to attempt this multiple times and it opens the door for lots of potential denial service vectors if you're not very careful you could take an optimistic approach so I'm not saying it's impossible but it's uh definitely something that's in the early prototype stage right now and that there's still there's still a lot of research in there uh okay speaking of implementations uh you mentioned earlier that you took guess forked it I guess uh and then replace the uh Merkel Patricia tree with a Merkel tree yeah uh what kind of Merkle tree is that is it just a regular binary Merkle tree or is it a sparse Merle tree oh we had a couple of implementations I'm not sure what we're currently using because I'm not really involved in in the programming side of things but I think we're just using a standard Mercury the users Poseidon housing instead of get check for the state tree yeah I should definitely look at look into this more because Merkle trees are generally not updatable in place like you can't like do an insertion of a key uh in in place you can I guess update keep you can't insert a new key in the middle of the tree without incurring a lot of cost so I wouldn't be surprised if it's something more along the lines of a sparse welcome tree but there's it might be it might be because I think we uh we yeah it might be a spherical tree I I because I remember there was we wrote a document a document about how we implemented the miracle tree and I remember going through it but I don't remember the details so I don't want to mislead anyone here all right well I mean if it required a whole document then it was probably it was probably something like espresso Mercury as opposed to regular entry okay well not answered that that answers that question okay mystery solved uh you mentioned earlier that uh you're not implementing the self-destruct instruction one of the reasons was that it will be remote soon I mean those are some famous last words uh based on based on the previous Cadence of modifying the ebm uh I'm not sure I'm not I'm not sure I'll be so optimistic about the timelines but regardless uh this kind of points to potential limitations of uh ZK EVMS can you kind of elaborate on any potential shortcomings or limitations or challenges around why you couldn't implement the self-destruct or was it purely just because it was just extra work that you didn't think you needed to do because it was going to get moved soon but there was nothing fundamental preventing this a self-destruct could have opened up uh the those vectors because let's say if the contract has thousand slots you need to prove the removal of thousand slots from from the state tree or if it has one slot you need to prove the removal of one slot and but because self-destruct is priced the same no matter how many slots you're removing from the from the storage you still have to prove the removal of each of them individually and so it will be an old vsd dose vector that may be the case for traditional commitment structures one thing that we kind of talked about as part of our because last year is this lab is developing as far small country for usage uh uh some some parts of the cosmo stack and whatnot that is a bunch of nice properties but one thing that has been discussed is the notion of like removing an entire subtree at once uh there's no reason that you have to necessarily remove each individual leaf you can always use an operation that says okay here's the root of an entire subtree and then just remove it out yeah I mean yeah how obviously there's checking us around the underlying implementation but in terms of like the commitment and stateless clients and stuff for those is very easy because they just remove just one node from the whole tree obviously you know implantations would have to go out and remove each Leaf uh like full notes rather but for things like light nodes or whatever uh just a single node removal but this is kind of still a kind of a more research thing it doesn't exist in production yet uh so I think I think that would be possible though for this if he's a splash Malco tree because spark Sparkle or three keys are hashed so each so even if the South even if the um like the nodes that you want to delete are sequential they're going to be in different random parts of the tree however if you use a different tree like if you use I iavl for example then you could do that because then they would be in the same sub tree gotcha yeah so essentially ivl is something you can you guys can look into as a potential tree commitment uh if you guys want to support that uh but regardless I don't think many people use self-destruct nowadays it was I was all the rage with a counter factual contract I think back in the day but no one uses those anymore I was talking to somebody the other day and they said that there's a possibility that it'll be added into the shelf life work EIP uh what's this what's this called 4758 we move self-destruct but I'm not sure because I think the priority right now is withdrawals and I'm not sure if the developers would want to add other things into it yeah Mustafa did you have any further questions um I think that's all from my side uh yeah I would say me as well uh talk about did you have anything to kind of add or should we wrap up this uh this amazing presentation I mean I I don't really have anything to add so awesome okay in that case we can wrap it up uh thanks to girl for presenting Scrolls DK raw technology it was very enlightening uh and uh where can people go to keep updated uh both with yourself and with scroll I mean uh both scroll uh would be probably the best place to follow us is on Twitter it's scrolled underscores AKP for for scroll and talk real maram without the last two letters of my surname for me on Twitter so you can follow us down and then you can also visit our website scroll.io and read our articles and also you can sign up for the pre-alpha test out there awesome thank you thanks for having me 