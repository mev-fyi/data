[Applause] cool um so i've known two of these people for many years but tomorrow it's really nice to meet you nice to meet you too yeah uh welcome to the back to the conference circuit you know how's it going um all right why don't we start out with like introductions um who are you and a little bit about uh you know zk sync and scroll and uh i think erie gave a good introduction to start that so sure happy to start my name is alex kulhovsky i'm ceo and co-founder of metalabs which is the company behind zika singh uh we have a ezekiel we specialized here all up on mainnet now for payments and uh swaps uh but now we're working on the uh evm compatible uh zq roll up for uh which is the version two of ezk sync which we have now live on testnet so it's the first zka vm live and testnet um i'm tomorrow maharamov i work for scroll i'm a senior researcher there so what we do is we're building an evm equivalent roll up zk roll up which uses zkvm which is implemented by us an ethereum foundation as a collaborative effort and we're planning to launch the test nets so we're around the end of the year and hopefully the main by next year um so i'm louis i'm the ecosystem at stockwere um i'm going to repeat still what my overlord uri said before so starkware is um as your large company we do is scaling through the usage of proof systems specifically stocks and uh we have been you know settling over 1.5 billion dollars wait what's more 350 billion dollars on sorry was it like yeah one or two out of magnets um uh through the through our customers dydx mutable solar and diversify and we have been launching the um first production ready uh vk roll-up in production on ethereum today so if you want to try it out have fun awesome yeah um okay so let's like kind of get into it um i i one of the questions that we were discussing in our in our chat is this question of decentralization and i think uri like sort of started this question with us you know startnet has a single sequencer how do each of you think about you know the sort of trajectory towards not just having scale but decentralization in sort of uh general purpose zero knowledge execution uh so maybe we can begin with the with the with the question what is decentralization why it's important so for sure absolutely uh at zika sync uh we we are following the philosophy of uh bitcoin ethereum the decentralized public blockchains where decentralization is important to ensure that no one has complete control over the network there is no single point of failure there is no single point of leverage so we don't want the blockchain world to devolve into something that happened to internet which began as a decentralized network of networks but now it's essentially controlled to a very high degree by like five corporations uh because of the natural process of centralization of power of amassing of resources so we we need to prevent that and the the way to prevent that is to put the control of the system in the hands of the users and making sure that the the community always have the leverage over the developers of the protocol over the the operators of the protocol so like if something happens to ethereum uh and it's uh like let's say miners or like validators and proof of stake after the merge become malicious and and like have a malicious intention to control the network extract value from the users the community can always coordinate and fork away and the the this for me this is the the very fundamental thing uh which we want to preserve in ezekiel sync [Music] by having permission on a software license by having the network governed by by the community but also supporting even though we hope it never will never happen uh ability for mass exits mass transfers to a fork of zika sync or to any other roll-up which will preserve which will keep this philosophy uh but it's also important to keep decentralization in in layer 2 itself which is why we're working on a decentralized consensus for inside layer 2 um to to make sure that the transactions also remain sensitive i guess maybe it would be a bit better to start this question why is why is it particularly hard to uh decentralize sort of execution with the zero knowledge technology why you know like we've had decentralized blockchains you know for 10 years why what what is the unique challenge that comes out of zero knowledge that like makes decentralization more of a a challenge do you want to um yeah i guess i can take it um it's more that i mean there is no real challenge it's an engineering challenge you know you know it's only engineering as like you know people say uh except that you know it takes time uh so there is we we're gonna make it right we're gonna make it uh it's gonna we're gonna make decentralized making open source everyone will be able to run it what's complicated though is that the you have less flexibility when you build an l2 than when you build an l1 because as opposed to an l1 you know at the point of the l1 is playing relay on the l0 and you can always roll back there's like you know you can do things and then l2 the problem that you have is that you have this bridge and when something is a bridge valley the proof and the money is out the money is out you don't control anything anymore and so you really the bridge is the most scary scary piece of software when it comes to l2s and for the funny story um the reference paper and that no one thinks about it the reference paper on altus it was written by patrick mccoy and it's the name of it and i would think about it scaling scaling blockchain through no validating bridges as a scaling solution because at the end of the day everything is about the bridge and so the the now there is a bunch of challenges also with the proof system which is you know you need to have also you don't have only like executors and your sequencers you have like this proving and the experience is latency and you don't want you want to make the sequencing as distributed as possible in the proving also and you have a bunch of challenges that exist elsewhere and so there is a bunch of interning challenges that still need to be solved and we just want to get it right so it takes time can i add uh so uh i think the problem with l2 specifically is that because you we're already outsourcing the consensus to an l1 uh the goal of an l2 is to minimize the overhead which is consumed by the consensus so adding a consensus on an o2 doesn't really make a lot of sense because it just will add the same overhead that you already are trying to outsource to an l1 so the point is how do you decentralize in a way which also minimizes the overhead and it's quite difficult to achieve especially with zk roll ups because the prover efficiency is not great so it consumes quite a bit of computational energy to compute the validity proofs and we need a way to make sure that the system remains efficient but at the same time people can just join in and participate without an issue so yeah i just want to add we're here at the modular summit so the we should think about the decentralization also modular way uh there are multiple layers that can be decentralized in with regard to layer two uh the first is obviously the the bridge itself and like the the general ability to uh to control it to upgrade to exit uh and so on uh then there comes execution which is actually easier to decentralize than layer once we have less challenges because we can always tap into ethereum as a court of the final appeal to resolve disputes so our consensus mechanism is actually simpler than the one required for layer one and we can also have uh some significant advantages we can reduce the number of validators because we don't depend with zero knowledge proofs on on them for security so we don't need and like for yeah we wouldn't need such a large number we can have delegated proof of stake so we can reduce latency a lot and we can have very fast confirmations for transactions in in layer 2. but then there comes also zero noise generation which is a layer on top of that which is uh which can be decoupled from the execution layer which can happen asynchronously in some time which is probably going to be like much lower than what we currently have like it's going down like under minutes like some seconds but it still needs to happen it needs to happen in a decentralized way we don't want to depend on a single provider on single cloud or in a single powerful player for generating the proofs uh not to have the supply chain as a point of failure well so do you imagine that there's going to be economic incentives eventually in these systems for generating proofs like how are we going to convince somebody to spend you know i think we're going to have money i'm going to troll my colleagues here so we have a start net research group internally where we discuss those things and you know this question about how you decentralize this as has been an open you know something making joke we have ideas in direct direction but an open question for nine months to a point i'm i drop the meetings just for the joke but uh no there will be obviously generated like incentive like there would be today we only pay the sequencer either through leader elections through proof of work or proof of stake but um the the there will be the same kind of mechanism now you're perfectly right the challenge there is for instance if you let's give you a random idea which is oh let's make a competition and then if you make a competition that's great it's like you know just the fastest gets to the network we're all happy whatever so the main issue you get with that is that it's so it basically brings down to centralization because the fastest guy the most efficient guy would always win and there is no incentive for the others to make it and so all of a sudden now you're relying on one or two single point of failure and your system is not redundant and while he cannot of course steal money because it's a zealous proof system whatever because of the bridge it still can store the system and you don't want that property and so you need to come up with new design to and right find the right trade-off on this latincy decentralization which still is an open question to some extent cool but to come back to the original question like like if there is a work that needs to be done if there's something people need and they were willing to pay for they will always be found some market mechanism to provide this value and and to get compensated absolutely i think um what do you so you know i think the progress in uh uh in like the fact that like we even have test nets and main nets of general purposes or knowledge computations is just like insane from like the first time i saw the zero cache paper in 2014 it's just been like i can't believe that like we've made so much progress in eight years but what do you think what have been the challenges in building execution environments um like you each have like sort of very different models of of sort of like how the zero knowledge like is actually executed how do you differentiate them or like think about them as being different so um for us our our main goal is to be evm equivalent which means that you can copy the bytecode smartcon by code from ethereum and just plop it on a scroll and it'll just work without any need for transpilation or any other fancy trickery to make it work and the challenge was with that is that evm is not really circuit friendly and zero knowledge proof friendly so a lot of the up codes and a lot of the functionality inside evm needs to be made in a way which minimizes the overhead in inside the circuit and that's quite challenging to do while also remaining efficient so that's one of the main challenges for us that we're tackling right now and the way we're trying to solve it is by using hardware acceleration so we already have a gpu acceleration implementation accelerated implementation and we're working on a pgas as well and we're just going to see which doesn't that make decentralization more challenging to some degree it does but it depends on the incentives if you provide enough incentives there are going to be always enough people who are who are going to participate in it i mean about the decentralization challenge i just want to bounce into things here um the first one is we all have the same challenges like we basically have the same frame we want to separate the sequencer we want to separate the approver ideally if we don't we could we're just going to merge if there is no other way but basically we have the same technical challenge but uh the sort of now the trade-off that we all have is different like target um you know the starkware just took the took a different bet to say you know what evm is great we all love it but seriously it's a piece of crap and let's let's do something that is optimizing for for for proof right i mean i i'm going to rephrase that it's a great software and uh we could be improved uh um and making jokes but uh but uh yeah it's dark we just decided to say you know what let's zkp have a different computing program computing paradigm just deal with it like just accept it and you know embrace it and do be as fast as you can using that paradigm and then you can retrofit stuff if you want afterwards but optimize for the performance and scroll obviously is going for pure evm like um even compatibility or like the ability to actually prove blocks on the only theorem and dk sync is optimizing for simplicity of uh development developer experience based on previous experiences like just use if you summarize it briefly i would let you guess on that but um the the the question now about decentralization is the decentralization as we say also is less important for zika roll up than it is for ethereum or for l1 and because decentralization matters on the on on the you know decay on the on the real one because you need it for consensus you need to know that the state you're holding is a true one you need that otherwise the guy everyone could lie to you when in the vk roll up you're like i just need for censorship resistance i'm not going to be lied to because they cannot lie to me because of the proof so i just need like a a bunch of you know incentivized people probably you know i don't even give numbers because we want as many as possible but it could be smaller and it's not necessarily a big deal so i think the the zk sync cpu target is kind of very unique you want to sort of explain sure so as uh louis correctly said we've chosen the middle ground between the the two extremes of having evm equivalence and having a completely separate like from scratch execution environment what we're trying to maintain is a balance between the like not the balance not the balance uh we're not making compromises we're like we want to get the best of both worlds of the super efficiency which zero knowledge proofs can provide if you make them if you optimize for them specifically and having a fully evm compatible chain where you can take essentially any application written for evm chains on in solidity and viper and other languages you support it on zika sync and it will work out of box with full like a web free api access with with your deployment scripts with all the tooling that you depend on it must just work but we are not willing to take the compromises like in in like the penalty in security orders of magnitude more sorry not security um performance that you would do if you went for full evm efficiency so the challenge for us was uh there were a lot of challenges like you just go them one by one and you tackle them once you set your priorities but i like one specific example would be we need to follow and fully preserve the evm security model and what we're building is a is a hybrid solution it's evolution between zika roll up and ziki porter which is a data availability of chain solution which is connected to secure all up so you can users can choose the whether their account is fully secured by ethereum or has this external data availability layer which makes some assumptions about security but the challenge was like how do you maintain the security uh model of ethereum because the roll-up contracts cannot depend on uh the the portal contracts like if if their data becomes unavailable there how do you make sure that the users can still always withdraw their funds from their roll up and this can be enforced by ethereum so there were multiple layers of of of thinking there one is that we need to ensure that every transaction is executable in zero knowledge proofs even if it fails we don't only accept transactions that that are valid we can accept invalid transactions as well without coming through priority queue on ethereum and we will always execute them no matter what and just if they fail we will prove that they fail uh but there was also a challenge of like how do you design this interaction between roll up and porter in such a way that to protect to like 100 secure roll up users no matter what happens on the border we found the solution we're going to be happy to present it once uh porter is out um so louis the challenge with having your own sort of cpu target is you've had to build a developer community right around zero knowledge right so what is how is that so um to be honest uh so you know like let's say that at nine months ago or four yeah you know before since i've joined stock where and we've been talking about cairo everyone say if vm compatibility even compatibly like you know it's like the gold uh and and um and the truth is um like it was at first we were very worried like of course you know we we have to start greeting from scratch and the counter-intuitive thing is that it actually helped the counter-interesting thing that you know we are not gathering i mean the whole thing we're going after is not to gather to the what let's say 20 000 30 developer worldwide 30 000 different 3d developers we are all the new guys they are you know willing to accept something new they are trying something and even worse you know they found they they they feel that they're bounded by fire because cairo is hell uh and so cairo is our cpu and a language so so the truth is we have seen a surprisingly interest from the dev community from people who came from existing solidity there from other places in in the world just you know popping up and learning it and improving the tooling and having extremely dedicated community and you know without sort of buying their way we just you know just explain them talk to them give them ideas fostering the community basically helped and worked and as of today i would say that worldwide right now on the daily basis there is around 300 to 500 dev nine months after inception so that's pretty successful and the growth is never it's not stopping at all so for now we are pretty considered pretty safe with our bet so you say you people can are willing to run you things but i certainly see you tweeting some new exciting egg like every single uh every single time i wake up in the morning yeah i know i know i know i mean i at this point i'm like uh cheerleader like a echo chamber like i have to say if you're a dev and you want i always tell to the dev wants to actually break into the space that the the most important thing they need to do is i have two two things that i tell them the first one is um it's better to be early than to be great which means the following thing you're better off being early in ecosystem than coming from you know deep mind and going make solitty dev right now why because you are all of a sudden facing people who have been doing that stuff for five years and you're like you don't know the food the guns the food guns you don't know all that stuff and you're gonna be like competing with people that are just you know better than you because they've been around on the other hand if you come only in a environment that is a bit hellish but at least the fundamental is very interesting then the competition is a lot easier you you're gonna everyone is new no there is no great dev there is no sort of magic trick that you know and so that's what i'm saying i'm seeing people you know realizing oh the tech is super cool and web 3 looks amazing but solidity is already kind of a crowded place it's not fully but it's a kind of quiet place here i am there is 500 people worldwide if i'm bad i'm still i'm better than anyone else and so that's the frequent one the second one is i always tell them you know whenever you do something in this space you're gonna break in by being visible and the only way you do to do that and that's magic of crypto is that everyone's dealing with twitter and twitter is a very easy marketing tool so every time i'm telling you oh you do this paper from paradigm just tweet it and retweet it and the committee is being what it is is basically retweeting and a lot of excitement and you know in i got like at least five or six people getting hired just because they posted three tweets uh i open c higher various places because because they just become visible so yes i have i have i've become an echo chamber and i don't do much in my life except tweeting so people should just follow you i actually don't know i don't think you should follow all three of them yeah you should but like it's really boring not him though no not me it's very boring it's like it's advertisement advertisement advertisement you know just don't do that but if you want if you go but in your life feel free okay i think we're supposed to get off the stage around now right you have time okay i thought we were going to transition so now i have to like come up questions yeah any questions i have one question for louis oh god one token i saw you first i'll give it to you by the way sorry guys uh hey so uh we discussed at some point uh zk over consensus right so like uh one of the properties that we have for off chain computing is that we're not time constrained so we can perform any proof of that and just use this in the chain so for the consensus this is more complicated since we're time constrained and maybe we cannot even benefit from some state-of-the-art techniques such as recursions so how do you guys see that like should we expect like some improvement on the current state of the research or the implementation to make this feasible so to summarize the question i mean if i may you're asking about ezekiel ones and recursion as a tool for improving what i'm saying consensus consensus okay i think those are actually orthogonal problems like you you don't have consensus you improve uh other things you can you can significantly improve decentralization if you implement like l1 fully like let's call them fully succinctly one uh what mean a protocol is doing for example all right so like we you it's a lot easier for everyone to verify for full nodes you don't have to re-execute all the transactions you just verify once your knowledge proof which is which will take you 30 milliseconds or something and um the rest is just data availability right so like it's it's a great approach if uh you can uh you like the challenge would be to compete with big established still once but at least that's something where you can differentiate from them but i don't see it affecting the consensus mechanism as such in any way also bear in mind that a lot of consensus mechanisms are synchronized or partially synchronous and when you use validity proof ability proofs don't have a sense of time so there is no way you can use validity proofs to optimize consensus in most cases yeah but i mean like uh the the other time to prove uh statements for the consensus so the proof is far more expensive than the verification so like to verify it's trivial to prove it's not so uh that's the main problem so so the uh um alex said the proof generation can improve some bits in some proof of proof of work stake like in weeks there might be something about the weak uh uh subjective so yeah absolutely weak checkpoint or which objectivities checkpoints yeah thank you yeah like there may be some improvement there but um the main thing that you break when you have a vk the l1 level is that you are improving the topology of the network meaning um the all of a sudden you have validators that can give you know node non-mining no non-sequencing node that can be on your phone and the miners or sequencer can be on the data center for the care and so all of a sudden you can be increasing your throughput dramatically so the but you still have decentralization that you're hoping for for anyone so that is not the consensus level though it's at the topology of network topology cool okay perfect thank you all right thank you very much guys this guy this question is for louis because you were doing good on evm [Laughter] what would you change about evm uh hola you want all the all the we're gonna be here for a long time let's give it a top three dub three uh i mean okay uh you win 256 uh as default uh kit check as default hash function uh which is like why from the get go uh exam i mean this is not technically evm i guess but the xzml3 for uh storing state top three those those are all in vitalik's list too so what those are also on vitalik's list right i mean i mean i can write the rest in peace right what else um oh the choice of 80 80 bytes addresses is a mistake rlp rlp i that that's something i don't even know right like there is a bunch uh you can continue there but okay uh yeah i mean i'm not the even the expert in the vvm just the top three that come because when you work on zk you're like why did they use k-check why did they do that like could they just like shout like oblique 2s or something something simpler like you know or standard preferably poseidon yeah but like you know whatever like uh actually we don't use poseidon for the story we would have yeah yeah no we don't otherwise start start for the story story by that actually the hash function so star query is very conservative in this operative function choices and so you have to understand that the biggest thing you have the hardest thing to prove or the most consuming the proof is usually the the hash function and so uh most of the industry have been using poseidon and we are using pedestrian hash which is provably secure but ten times worse to prove um and so we kind of like to shoot ourselves in the foot with like uh being conservative sometimes so here's one example uh conservatives doesn't mean like better or different it's just like it's different that's such a choice and i mean i kind of hope one day that i would convince my engineer that position is good enough we can adopt it but for now we're still struggling with that so we can we gotta wait for someone else to settle like a trillion dollars on poseidon hashes right right exactly here yeah yeah anyway so just uh thank you very much my pleasure any other questions you had a question ah uh yeah okay yeah right sir okay i said it's interesting it's a question for all three of you uh or four exactly you want to throw in um what proof systems are you guys gonna like most i don't know interested by or kind of like where it could go you know take it bulletproof smarlin hilo the list goes on like what of those zk proof systems are you kind of most stoked about yeah i mean i uh i saw it i think we know your answer it's on your shirt starks i saw i saw ariel uh yesterday and i was like you won like everyone is using plonk except for the starks people and uh you know the earth monetization inside of plonk and starks is very similar so it's just like really the last step of the verifier but yeah congrats to ariel for for winning stark timber but i mean now just uh just a very i mean i just want to react to this uh it's a it's a very interesting question um my name is obviously stark but obviously that's not the answer for everyone uh it's obviously a trade-off what people don't know though is that every time you talk about staggering snarks they always care about like the theoretical differences like you know the trusted setup or the non-trusted setup the quantum resistance or not the reality of actually why i care more about starks to be honest is because of the proving time uh probably like by default and which is like why scroll i mean you said though they're looking into acceleration it's because it's a lot harder to make a snark proof because it's n log n but it's a logarithmic operation when the start pro a star proof is analog game it's a hash function so the the constant here is pretty significant and does have an impact on the proof generation so um this is also why i believe that me i mean polygon basically when the only the stark words because it's they believe that it's more scalable at least in the short term but you should also bear in mind that long term we're probably going to be limited by data availability or rather by improving times well i mean we'll see i mean you know i'd like to say i i like to quote kent in these cases in the long term we're all dead so from there i don't know i'm just a normal guy i don't know i mean i mean of course you know i'm not i'm not dumping a dunking here at all i'm just saying that this is why i kind of care more for starks cool all right you guys are done thank you thank you [Applause] 