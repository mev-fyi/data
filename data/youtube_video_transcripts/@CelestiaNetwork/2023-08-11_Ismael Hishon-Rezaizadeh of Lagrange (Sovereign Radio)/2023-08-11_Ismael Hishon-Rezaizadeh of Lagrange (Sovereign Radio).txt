all right tell me about yourself how did you get into the whole space and what your involvement with the range so I'm the founder of LaGrange Labs we build computation that increases the expressivity of how State can be used on chain and between chains by being able to prove computation on large data sets and data parallel computation in particular in a zero knowledge context I was previously a venture capitalist and before that I was an engineering leader for a financial services company focused on digital asset strategy in layman's terms what does LaGrange do so what we do is we enable you to run the same types of data Rich computation that you traditionally run on top of a database or a data Lake in web 2 on top of on-chain data between applications and from applications things like SQL things like mapreduce things like spark and rdd where if you treat data on chain as if it was structured that you would treat it the way with a webto database okay awesome now if you were to give a VC the elevator pitch of what this does what would you say so what I would say is that there's a lot of D5 Primitives game five Primitives and social five Primitives on chain that can't really be built in a trust minimized fashion the example I like to give is something like volatility if you have a modular roll up with a conical bridge to ethereum and you wanted to take volatility over a period of time to do a boxrolled model for instance and you wanted to do that in a way that doesn't incur any additional trust assumptions associated with the ethereum state beyond that which is guaranteed by the conical Bridge we allow you to run that computation on top of the underlying data commitment in a way that is fundamentally as secure as the data that's contains in it okay so how is it complementary to the tool set that already exists yeah so when we look at Cross train in general there's really three-way three components of how we see it you have what is an assertion of State at the base layer it can be a proof of consensus it can be a assertion by a validator set or it can just be an awful then you have the transporter State moving the state from chain a to chain D and then you finally have the computational state and so most crosstain protocols typically messaging bridging they focus on the assertion of State the transport of state to check in a simple property the existence of a transaction the existence existence of an event Etc what we allow you to do is take that same infrastructure and use that to pass expressive properties of data between chains AI whether something is true or not yeah because what if a sequencer lies to you what if like a relay or whatever yeah so it doesn't work on trying to say this is the correct data that isn't the correct data it says that if this block header is correct which you've implicitly opted into as part of the security design of your protocol then you can prove all of this other Rich information about the chain in question you can prove moving averages of price you can improve volatility of assets you can prove the ownership of of nfts you can proved the result of SQL cores on top of on-chain data as if it was a database assuming that the relayer and the assertion of state is correct so it inherits from any arbitrary protocol if most of these theorem Network depends on a single single a single validator for State yeah like slash Bots yeah then and if flashbots turns malicious then it would it would still inherit the security of that the network so this is not designed to try to tell you what the correct statement is in the correct State what this is designed to do is when you have a cross chain when you have a protocol you typically have opted into a security model or a security design by by just the network you're in and the relationships that you have with other chains if you're on a modular chain that has a canonical bridge to ethereum you have secure State access to ethereum that's secured by that conical bridge and now we allow the computation to inherit that if you for example are building with axillary or layer 0 a polymer or polyhedra or many of these other other protocols you typically are already opting into the security model what we allow you to do is to retain the same security model you opted into for bridging and messaging and now use that as part of these more data rich computations so what what's yeah so the use case there's a lot of them right I think when you think of general purpose computation access to data it's important to think of like what are the New Primitives that can be built and will be built one of the things we think is very interesting is complicated pricing both for collateral as well as for options being able to derive features of how assets are being valued on chain within existing exchanges and existing protocols and then being able to leverage that to remove dependencies on off-chain data feeds another thing we think is very interesting when you build game five autonomous worlds and somebody's more more intricate on-chain compute heavy game fine uh and so on and nft based applications you now can query and interact with data that doesn't require you to to compute and loop over huge amounts of launching information on chain and so with respect to General message passing how does it relate back to something like interboxing communication protocol or how uh axler has its General message professional protocol yeah we we think that there are a lot of terrific designs for message passing and for the movement of State between chance I think IBC is a great example of that Axel is a great example of that layer zero is a great example of that hyperlink is a good example of there's a lot of omni's a great example there's a lot of great protocols that do this I can vote on the list what we enable you to do is to select the one that's best suited for your application and now do more with that state and you don't have to now rip out your frosting protocol and pick a new one if you want to access the moving average of the press you can opt into the one that you would like to use and have your knowledge based and secure computation sitting on top of the data that protocol starts to someone we're trying to query some kind of data from the entire blockchain ecosystem they would select what their preferred transport layer is yeah and the secure assertions of State I see okay do you think that that hurts progress towards standardization of what that transport layer may be or do you think that um we're unopinionated when it comes to what a transport layer should or or will look like we think that you need to have the ability for applications to have agency over the transport layers that they would like to operate as well as chains themselves and at that point you need to have tooling that enables chains to interact and treat data as it should and it's not the job of a protocol like me or for me protocol like ours or or a developer like me to say this is what your protocol should opt into and I'm going to not support you if you don't gotcha what about a transport layer compels you to support it rather if they move States between chains we can support them and we will I see so so so every transport every transport layer that moves State between chains we will support okay our s so what was the opportunity you saw in the space to convey to build something like a grunge yeah I think when you have built a lot of on-chan applications you traditionally have run a lot most developers have traditionally run into limitations with how data can be accessed and treated one of the examples I'd like to give is if you have an nfq contract and you say I want to know all NF cheats or nfts around like Shango how do I do that well what you actually have to do is is to to go on the the mapping and to Loop through every single mapping from your contractor from whatever contractor needs to access this data look through every single slot and figure out whether or not there's an nft that is owned by you there and so you can't just very simply core your search over the data filter through it the same way you would if it was an off-chain data structure and so what you should be able to do is to be able to treat this fundamental morass of data that we all are creating the fundamental multi-chain data Lake as if it was a data Lake the computer first okay zooming out to the 20 000 foot view given that our entire world is moving closer to Ai and not everyone understands what's real or fake anymore yeah and the problems will only get worse does something like LaGrange helped to mitigate some of that uh in terms of proving provenance identity that kind of thing yeah like who who really signed what that's a really good question I think one of the most exciting things that we're seeing being developed right now is zkml and and zki and Associate Technologies there and I think one of the areas that people often Overlook when looking at zkml is what is the provenance of the input data that is being used by the model if you can't ascertain the veracity of the input data you really can't ascertain the veracity of the computation on top of something because you can always just change the input data you can put adversarial input into there and then the result of the model is going to is going to not be what you think it is um or could be militias as well and so what we enable you to do especially with with this type of distributed and data parallel computation is to really start building ZK ETL pipelines or what is that extract transform load like the way data will be processed when you were when you would be ingesting it to a model in a more traditional web 2 context you can now do with the types of data that you'd like to be inputting into your and here's your camel model so you say I would like this user's input activity for the last two months so I can underwrite a credit instrument to them let's say um you start with the model that's going to be used to underwrite the credit instrument is going to be ml based well if I have to trust some off-chain party to put the data into that model there's no way I can trust the computation that model renders or provides so you'll need you have two problems that you have to you have the proving of the model then you have the proving of the input and what we enable you to do is to is to now compute over huge amounts of on-chain data as if as if they were as if there were in your web 2 Data like what if a machine like an AI we're able to generate a whole bunch of different addresses it doesn't necessarily correct for that right to understand whether or not it is a human behind of an identity like an address identity you know or if it's an AI behind it so this this is not an inference-based inference-based technology it's a it's a data parallel computation and data transformation data porting technology and so the model itself likely should hopefully be tuned to be able to discern between between fake fake user personas and real users who are looking for credit instruments but once again if you have an AI that generates a bunch of fake addresses and yeah I can just you know compromise whatever the off-chain data feed is That's supposed to extract that yourself you've now incurred two sources of attack one that the model can discern which is what it's hopefully been developed for which is being able to determine on chain whether or not this address belongs to a user and should be issued a credit instrument and then the second thing which it can ever be able to discern is whether or not there's been some off-chain hack that it has no no knowledge of yeah and it's also not perfect because an AI could also employ a human to create these on-chain identities as well yeah on its behalf it's not perfect okay great um any less thoughts about the modular stack and why you're here at modular Summit yeah so you know one of the things that we're very excited about in the modular ecosystem in general is the ability to have modular chains that can have access to State and storage proofs of other modular chains without latency or overhead and generally we're seeing a proliferation of General message passing protocols in the modular stack but what we're not seeing and what we don't have access to right now is ubiquitous ways to compute and to transform and interact with the underlying State between chains Beyond event emissions and transaction inclusion and so with the efficiency of computation that we can provide we can start deploying we will start deploying the stateless contracts on modular chains that make it very very easy for them to be able to verify the existence and the inclusion of some property of other chains when they receive block headers from cross-stream protocols or Chronicle Bridges and so what this does is that this allows you to now in the modular space in general be able to have more expressive access to data okay great so is do you see do you imagine that this thing is going to have longevity in 10 years from now are people going to be using this to query archival blockchain data that's produced today yeah I mean I think I think SQL and Spark and rdd and mapreduce and the way we handle data at scale is not going away we are actually creating more data on chain than we really have the ability to effectively process from any any of our state machines that we're developing I think broadly speaking when we think about the the progression of the development of expressive State machines we often focus on the machine aspect the execution layer we don't really think about how State machines are fundamentally constrained by the prefix state and so the ability to access prefix state by the prefect by the first word of the state machine State because if you're developing an expressive State machine right an execution layer then you typically say okay there's some data commitment the storage uh storage route and we're going to execute computation over this within within our station layer which you don't often think about is well how about all of the other data that I could potentially access data that typically you don't have access to at the point of execution of some function of State transition on chain and so that is where we sort of think that the ability to now have additional types of data available at the point of your execution of State transition allow you to have applications that are much more expressive with respect to the underlying data that they compute over what kind of data is currently available on chain that is not expressive enough for the use case that you're you're thinking of yeah um for example moving average is the price let's say I would like to compute a moving average of price on optimism or an OP stack roll up over three months and I would like to factor out the top 10 about lies and bottom 10 percent of my players and I wanted to block my block what I have is 19.2 roughly million blocks over three months I need to do do two storage proofs per block let's say so I have about 40 million storage groups I have to do and then I have to compute over all of that data to figure out what the price is to do a very simple computation of what's the moving average of price over three months and then you have to update that every two seconds whatever new block comes in so that's the type of thing you can't really do with on-chain state right now because you just don't have the execution language having access to that historical data or that depth of data nor do they have the the computational ability to actually Loop over all of it and this is the type of thing we now have um compute that that is verifiable it's data parallel that you can start doing so my understanding of that problem is that there's a lack of archival nodes there's also a lack of incentive to run archival nodes so how does the range solve that yeah so it's I would say it's less about the archival mode and more about the computation on the data so if you have an archival node you can you can extract the data off chain right you can do the compute off chain but it has to do with being able to say from your contract I would like to execute some action based on price and then you have the question the problem also that data right if I knew that compute off chain from my account we'll know the feed it back on chain there's an agency assumption and the trust assumption associated with who I am and what I'm input so what this allows you to do is to prove compute at a very large scale on on chain data and so it's that's only a problem in stateful networks in improve the state Networks proof of work any any blockchain that has state that has a statement right yeah um but you know we actually haven't explored much on like utxo base models and whether or not this would be this would be applicable there um like Bitcoin I say if you have a proof of work Network that has as smart contracts and and a programming language and you can you can write and and maintain a stateful uh representation of all the all data on the 300.5 then than what you have you still have the same rules I wonder how layer zero does this layer zero I believe they use hashes to to prove States but not to a deep level um so I wonder if they actually address this problem this is a perfect question this is a really good example we think Super highly the layers of rotating um and so what they do is they have two components of their infrastructure broadly they have oracles and relayers oracles make assertion to this is the state of the of some Source chain on some destination chain at some point in time so you know an oracle would say at block height X this is the state of the theory and then the relayer moves a proof that contained within the receipt route of that block header that the Oracle asserted to that there is an event and they run that on chain as a local Patricia track proof um very similar storage proof a lot different route that being said you now have a block header that you're checking a single property the question is why why don't you take something like 100 why can't you take something like LaGrange and allow it to compute over that underlying data and to now prove a different property instead of there's one transaction here's the moving average of price over three months the same security as the check of the checking of the transaction the same security that underlies and integrates with all of these protocols today but now you can do more without the same security to the cosmos ecosystem by proxy of Celestia is there when are your plans to support IBC yeah um I would say that we will likely be supporting Cosmos chains by and large by early next year there is no limitation for us in our underlying constructions and supporting them it's just an order of operations and a rollout thing we think very highly the cosmos ecosystem and you know we we have well I'd say we're very bullish on it and we have been for quite a while okay that's great the problem with Cosmos chains though is that not very many people if at all from archival nodes so then how would you how would you solve for that if you don't have access to that data yeah I mean the beauty like the data that's not even available for over over a month yeah the the beauty of compute improving compute is that you only need one node so in theory to support a proof of some historical data approval computational data for a given chain you need one archival node and if there's a single archival mode for a chain then you can support it um and you know I think I think there's obviously a question of incentivization of running our panel nodes long term but if you can now run an archival node and take a fee whenever people use this proof on chain as part of computation you've now have a cash flow running back to the person running today fair enough so you now have a better economic incentive for it which is a consumption of the data great well thanks for coming on the show thank you very much 