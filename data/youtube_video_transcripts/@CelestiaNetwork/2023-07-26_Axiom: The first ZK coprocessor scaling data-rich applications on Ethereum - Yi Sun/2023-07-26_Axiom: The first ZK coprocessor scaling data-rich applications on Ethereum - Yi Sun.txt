all right so the title my talk today is scaling data reach applications on ethereum with Axiom and the starting point of this is the realization that if you're writing smart contracts today on ethereum or really on any blockchain VM you're really operating in a very data starved environment if you look at the listing for this cute penguin on openc and you try to identify of all the pieces of on-chain data on the page what can actually be used on chain you'll find that the answer is only the owner namely Zac Efron all the other Rich information on the page like the transaction history the historical prices and all that good stuff that openc users get to see is simply not available to your smart contract and this is not just an inflammation flaw of ethereum any blockchain that wants to be decentralized can't impose the requirement that validating nodes can access history otherwise that would require all full nodes to become archive nodes now of course developers are very creative and they work around this in many ways today they have this trade-off between putting more data in state and paying for that or by reducing the security somewhat and relying on trusted oracles which in many cases is a fancy way of saying that the team itself puts the relevant data on chain in a fully trusted way and so developers who want to scale data access on chain today really have to trade off between increasing their cost or reducing the security of their application we're thinking about whether we can scale data Rich on-chain applications now on blockchains we have a special tool namely in any blockchain the current block always commits to the full history of the chain and that means we can use cryptography instead of consensus to access on-chain history so how does this work on ethereum the current block is linked to all pass blocks by a ketchak chain of block headers and of course every pass block commits to all the information in that lock namely the state of ethereum at that block as well as all transactions and receipts the problem though is that if you try to decommit all the way back to a million blocks ago in ethereum that's going to be prohibitively expensive you can never do that in the evm so what we realized at Axiom if we is we that we can shove all these verifications into ZK we can check a Merkel Patricia tri-proof as well as a chain of blockheader hashes and make that feasible to verify on chain this has a couple side advantages of providing scale and accessing the historic data in composition so what we've packaged that into is something we're calling a ZK coprocessor for ethereum smart contracts can query Axiom on chain to do a combination of historic data reads and verified compute over that data we generate the result off chain and also provide a zero knowledge proof that everything we computed was valid once we verify that proof on chain you can use that result in your smart contract however you like and because of the zero knowledge proof every result that we return from Axiom has security that's cryptographically equivalent to something you're accessing on chain in ethereum itself so let me talk through what the two components of Axiom give you the first component reading arbitrary historic on-chain data means that you can scale your application while interoperating with existing applications that are already on the chain unlike something like a rollup you don't have to move your state or really do anything to access more data on the compute side we envision supporting computations that really cannot fit in any blockchain VM either today or in the future you might imagine running some sort of local neural network inference that's never going to happen on a Time shared Global computer and so I've told you what our ZK coprocessor is and I now want to talk about about what it can enable so I've drawn a rough graph on the x-axis is the amount of data you're accessing and on the y-axis is the amount of compute you're using to process that data obviously they're correlated if you have a lot of data you're going to use more compute so in the beginning we think that ZK co-processing will make the devex for certain operations that are already possible but at Great cost in the evm much simpler this will be things like Computing a trustless volatility Oracle verifying a user's account age or Simply Computing a consensus level randomness but where I think it really gets exciting is when the data and the compute both get ratcheted up you might imagine accessing fully trustlessly the historic balance of any erc20 token at any historic block or if you're running and designing an on-chain protocol you could Define objective slashing conditions over the entire history of your protocol participants that cause them to be punished or rewarded and once ckml gets sufficiently fine-grained you can imagine adjusting your parameters of a D5 protocol based on machine learning algorithms applied to the historic data on chain in this way we think you can bridge the gap between traditional web 2 applications that take in vast streams of data and process it to the current trustless on-chain applications that are data starved so let me walk through what the state of ZK co-processing is today so we just went live on mainnet with trustless access to any historic block header account or contract storage variable two weeks ago and this week at ECC we've launched transactions and receipts to test net so in this way we allow smart contracts to access any piece of execution layer data on chain today to do compute over that data we offer the ability to write custom ZK circuits to come to a result for a developer all of that can be verified on chain fully trustlessly now what does that mean for your actual application I'm going to walk through a few examples in a very concrete way suppose you want to access a user's account age what you can do is trustlessly read the historic nonce of their account at two different blocks then you compute the first block that has a non-zero knots and deposit the age of their account on chain we have this running live in a demo on our website today suppose you want to enhance your governance in traditional corporate governance there's a very complex governance structure it's not just one stock one vote most governance today is simply one token one vote I think the reason for that is it's very hard for governance to actually know any other information about the participants to have a more complex voting weight scheme with Axiom all you need to do is trustlessly read the history of your users voting by looking at the on-chain events then you can compute derived quantities like the number of times someone has voted their voter participation or even things involving when they vote and how reliably they vote you can then compute the custom voting weight using that and really tailor your governance to incentivize whatever you'd like fi you might imagine adjusting fees for historic participation and standard exchanges like finance and NASDAQ obviously if you're a higher volume Trader you get a fee rebate in D5 today everyone gets the same fee and we think that kind of violates the fundamental law of economics the only reason it hasn't happened yet on chain is that amms actually can't know how much their users have traded to implement that with Axiom all you need to do is trustlessly read the trade events of your users on chain add them up and then apply the appropriate discount to fees so all of the applications I just talked through are possible today on mainnet with Axiom but I want to talk through where we're going so we started by giving smart contracts access to the execution layer data and we think ultimately there want access to all data once Cancun lands in September we'll be able to access consensus landfill data on ethereum and perhaps through Bridges like succinct we can access data from other blockchains and roll-ups after that we think developers want to process the data they're getting in the most native format possible for ethereum that's going to be simulating the result of view functions through zkevm proofs once we're able to do these first two steps we'll essentially have a ZK version of an archive node or indexer which caches these values and returns them to Smart contracts that with lower latencies finally we think developers will want to use different forms of computation that exceed the bounds of blockchain VMS and we think the best way to provide that will be through a ZK native VM so we've started on the first piece of this roadmap and shipped it to mainnet two weeks ago and we're excited to continue over the next months and years if you want to try out Axiom today you can check out our developer docs at docs.axiom.xyz and my remaining time I want to talk a bit more meta level about the usage of ZK in blockchain applications I think a lot of developers today are very excited about ZK as a concept and Technology to be frank they don't really know too much about it and this week I've been talking developers about using ZK and they view it either as a black box which either they like or are very afraid about and so at Axiom we started something we're calling the open source program to educate developers about how they can develop CK circuits and what ZK can do for their applications so in the first round of our program we had a number of community written open source circuits for things like fixed Point arithmetic ed25519 signature verification and BLS signature verification so we're opening up the second round of applications next week you can go to the URL above to apply and we'd love to see what you can build using ZK thanks so much guys [Applause] 