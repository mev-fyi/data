[Music] so how did you get into crypto and like i'd love to kind of hear your story of uh what what how did you ended up ended yeah i feel like that's a very standard intro question in the crypto space uh which is interesting because you don't really hear it in other podcasts or at least from my limited experience it's probably because there's so much diversity in terms of background in the crypto space uh but yeah uh my origin story is kind of not that interesting uh but essentially i was in grad school um this i think this was around 2016 maybe uh yeah maybe early 2016. and my grad school advisor was very keen on cryptocurrencies and especially on this thing called ethereum because uh he was involved in kind of the uh in the very early stages of ethereum uh and in fact he's been a big he's been following cryptocurrencies for a while uh he one of the pictures that he likes to show to all his grad students is him next to the mount gox guy in japan uh you know the day mount gawks happened and then there's a guy with a sign going you know give me back give us back her bitcoins and stuff in japan uh he was there when that happened and he took a selfie with the guy uh so he likes showing these pictures to students so he's been involved for a while he talked to his students about this thing called ethereum and you know me being the the youngling that i was i didn't really know anything about blockchain at the time uh and this was you know when the theorem was like one dollar or something something along those lines uh and then you know after i looked into it i got interested it all seemed very interesting and the reason i think i got hooked on hooked on cryptocurrencies and you know this kind of permissionless consensus uh blockchain stuff like that as i saw there was a lot of overlap between uh the things people are doing and virtual machines and compilers in the in that area and it turns out that a lot of the ways that you do things they scale blockchains and whatnot revolves around building better virtual machines and it has very little to do with the things people were talking about at the time you know things like better consensus protocols and stuff like that none of these things really make a huge difference it's really about you know can you make a nice virtual machine uh can you have to go with a virtual machine you know a nice language that compiles down to it and stuff and i happen to be very interested in the domain of you know compilers interpreters virtual machines so i saw i saw there was like a large overlap in interest over there and that's kind of where i kind of got my initial in into blockchains from there i joined consensus to do layer 2 scalability research and then about a year ago i left to start working on lazy ledger full-time man that that's uh awesome story who was this advisor or can you not like share the name i mean i'm pretty sure this is public because i went to a public university so like my advice the fact that i have a thesis out there is public i'm pretty sure it's publicly searchable but my advisor's name is andreas benares oh cool and that's awesome that you got into this like from the virtual machine side um because that was like one of the first things that i thought was totally wrong with the stacks that people are building both the ethereum side and the hyperledger side were like like what were like they'd be barf but i was like no this is not how like there would nobody like qualcomm would build you this thing or anyone that's like an hft trader or like any anyone that really had to like deal with like you know milliseconds you know let alone microseconds you know what those things matter um so yeah i thought that was a kind of interesting entry point um so lazy lodger i like kind of i i love it because it's one of those places where i feel like you know there's a pareto efficient curve and you guys like picked an extreme point uh and to me that's like always an opportunity to see like what breaks in the world when you do that like can you kind of like tell tell me like i guess from your what is your vision for it and like how do you think about it sure i mean you're not wrong that it's kind of taking an extreme point uh do would you rather i start with a description of what lazy ledger does rather than its vision or do you want me to start with a vision um let's start with what it does okay so what it does uh so one thing that's interesting to note in the original bitcoin design paper written by satoshi nakamoto is one of the earlier sections describes a timestamping server and if we had a way of timestamping transactions uh trump transactions in the database not necessarily monetary transactions then we would have a way of ordering potentially conflicting transactions and if we have a way of ordering them then you can just ignore all the conflicting transactions that come later uh but to do this we need some sort of timestamp server and unfortunately in a you know completely permissionless trustless setting you don't really have you know some you don't have a notion of time there's no there's no time which which i think also there's no some overlap here with regards to time and stuff with the selena's celina's approach to using the proof of history so your time is the time is the big thing uh you know how how do you how do you timestamp these transactions and you know what he he devised at this blockchain thing uh but one of the implementation details of the bitcoin as it was deployed as opposed to bitcoin the idea described in the design paper uh which you know the bitcoin that is described in the design paper and the bitcoin that is currently deployed are like very different because the one in the paper is a very high level approach right it describes the high level some high level ideas and it completely doesn't go into implementation details while the bitcoin that is currently you know out there is obviously it's implemented so it has implementation details and one of the implementation details is that invalid transactions are not in the chain uh in the bitcoin chain you cannot have two transactions that span the same utxo in the ethereum chain you cannot have two transactions that are from the same account with the same nonce and you know some similarly for other chains that use either the utxo or data model or potentially some something else right uh and this is kind of uh it's implementation detail that was carried over it's an artifact but you don't actually need this as satoshi describes in the design paper as long as you can timestamp transactions then you can and that means you if you took a timestamp with them you have an order if you have an order then you can simply ignore the conflicting transactions that come later right yep yeah so this is kind of the kind of the core idea of lazy ledger is well let's not execute transactions let's just order them so the the core idea is rather than having an execution layer however rather than having the blockchain be an execution layer uh it is only an ordering layer uh so currently what blockchains currently provide the contemporary blockchains like bitcoin and ethereum and so on you have they order data they make it available and then they also execute the transactions lazyledger only orders data and makes it available it doesn't execute transactions and as it turns out as i said before as long as the data is available and ordered and you have the consensus on that and you know some time stamping then you don't actually need to execute transactions at the consensus there you can do this at the application layer yep and uh this is this you can kind of like base this on the assumption that like computers unless there's like some weird alpha particle that hits this thing that run the same code will come to the same conclusion and that's all you really need right is the input right because the side effects of the state everybody will generate the exact same thing that's exactly correct and if a transaction is about invalid the virtual machine will just say this transaction is invalid and it'll like make it a no-op essentially yeah yeah this is like i think a wonderful idea because i think it really challenges like a lot of assumptions that people want to make about like i think like code is law and like all these other things that are on top of it like what i think is really neat about it is that like you kind of end up with a situation where um you can like maintain just the this this like bandwidth pipe where it ingresses as much stuff as it wants and anybody can really catch up and read it um and it's available like you can kind of actually easily much easier shard computation that way but like all the folks working on sharding are should be building lazy ledger and then some transaction isolation to follow whatever chart they want yeah i mean i call lazy ledger a universal blockchain because you can implement any execution system on top of it unlike for example ethereum right and ethereum you could due to gas limits despite the fact that it's you know in theory you're incomplete without a gas limit with a gas limit it's very much not turing complete right you can't do anything you would want to do and this is especially true for your cryptographic operations you know even like simple some you know verifying some signature that is in the pr pre-compile might take you you know a few million gas which is just like it's impossible to actually do anything with that with lazy ledger you don't have this constraint because you don't have execution at the base layer you don't have a concept of gas it's just pure data so you can actually have you know a bunch of data availability the throughput of you know way more than you could do in a charted blockchain uh and then just build a sharded execution layer on top of that so what do you lose like what do you lose uh i want to say nothing i i tend to agree with you but like what do people say that you lose well so i can so kind of the the laser ledger system to contrast it to uh to systems like uh you know like sharding uh i guess i should kind of go over a bit more detail into you know what we do specifically as opposed to just you know ordering data because that's that's the that's the core idea but there's you know some other techniques that make it even better than just that right uh there's uh two components of lazy ledger that distinguish it from [Music] the first for the first facet distinguishes it from regular blockchains the second one distinguishes it from charted blockchains as they currently are designed so the first one is data availability checks there's you know some some math behind it but essentially what you can do is you use erasure coding and plus random sampling so that a light client can convince itself with very very high security guarantees like just an arbitrary number of nines uh that the block that the data of the block is available without having to download the full block and in normal blockchains you know a full node will fully download a block and a light node will you know just assume the data is available but that's not very secure because a majority of hash rate or stakers or whatever could create a block and not show the data behind it which is you're very bad you don't want you don't want that to happen and you know light nodes would get tricked without these data availability checks so lazy ledger adds these data availability checks uh now shorted blockchains also need some form of data availability check because shorted blockchains always have committees of some sort right like say you have serenity uh commonly called d2 uh in serenity that you have you know your set of global validators whatever you know 100 000 let's say and then they get split up into tiny committees of you know size 100 or something and then each committee uh votes and creates blocks on a single shard and then they get rotated but it means if you could corrupt you know the committee just one time on one charge you could produce a block and not show the transactions in that block and then no one would ever be able to produce a new block for that chart and you wouldn't really be able to penalize anyone uh because you know the data's not there of course now one thing you could do is you could say hey every single node in the network and every single validator and serenity go download that shard block but then now that's not starting anymore everyone's just downloading everything all right so all sharded systems need to have this you know sub these sublinear data availability tracks and your laser ledger does this at the at the base layer uh so and these kind of distinguish charted blockchains and lasers that are from your current contemporary blockchains is that it allows for secure light clients uh that that's actually not not too far off from like performance forced us to go down down a similar path for data propagation yeah erasure coding is really good because you can do uh it's like it's very resilient and very robust as opposed to full replication uh which is just this is terrible but you know temporary blockchains do full replication as opposed to as opposed to error error correcting yeah so the now uh proponents of sharding will say well lazy ledger you take your big block and you razor code the whole big block and in sharding you kind of razor coat each individual block right and the razor coating an individual block is less computationally intensive right that's kind of what they would say this but this isn't like really accurate because uh if uh because if the if the nodes don't check if the razer code is correct uh then they have to rely on some fraud proofs or potentially validity proofs in the distant future that the erasure coding was done correctly uh so if you if you rely on those on those fraud proofs then you know what the difference between if you're one validator in serenity then you're basically saying okay the other 63 is three shards i trust that the range of coding was done correctly i wait for fraud proof and i do you know i do my work on this one chart versus lazy ledger where you know you have the equivalent of say 64 shards and that you would you know either trust that they're correct or that you would you know validate it yourself so there's not really any difference here right the the if you trust that 63 shards are correct versus 64 uh there's not really any difference uh because as soon as you have cross-shard transactions you trusting your one chart is correct is meaningless right as soon as you have crushed our transactions it means that you're you know taking in your your your shard is not isolated it actually you know takes an input that could be malicious uh there's also the fact that you know in general doing the 2d erasure coding scheme is highly parallelizable uh because each to kind of give you the the one sentence overview is each row and each column of a square is erasure coded independently so this is a completely you know true it's like embarrassingly parallelizable to do and you know in the modern modern day computers with many cores uh i think you know there's this uh the the ryzen 5000 series just came out you know how many cores do they have i can't even count them on my on my on my digits uh you know there's no there's enough cores there to do to do these things and you know massive amounts of of uh parallel work here uh so there's kind of concerns about uh you know it'll if if it's a single monolithic chain you won't be able to produce a block but that's just not true at all uh you will be able to produce a block and the reason is that if you're a very small block producer you can produce a small block you don't have to produce a big block right um how do you guys deal with like um like in terms of actually coming to agreement that this data is shared for the whole chain like and stake and state and like like actually like deciding that which fork of the chain to go with uh that is a very good question uh could you could you clarify what you mean so like without um you know general blockchain that does i guess nakamoto consensus right i have some electricity that decides which blocks are which oh i see okay what the consensus protocol is so the consensus protocol yeah yeah how do you define what what is what is the super majority okay so well so the so there's kind of separate things here the three types of things the first is the execution system right the second is the consensus protocol and the third thing is the civil resistance mechanism so for our execution system we do as i said we do we do no execution uh which as you said is an extreme and it's in fact optimal right you cannot do less execution than no execution and as uh the kind of moderately uh moderately known but not too famous barbara liskov would say the bottleneck for these consensus protocols in in practice is the execution system and not the actual consensus protocol which is something that i've tried to you know bang into the heads of several academics uh but they don't they don't really want to listen and i think we know what we're talking about here uh so so you know the the first component is the execution system lazy ledger there's no execution uh well star in your theory it does no execution uh in practice as we'll get to the other two facets maybe it does a small amount of execution that's you know very very tiny so the second component is what is the consensus protocol and then the third one is the civil resistance so we would have liked to use proof of work because you know proof of work it's completely permissionless within the protocol there's you don't need anything like stake or anything like that you just join with hash rate you you know you you you produce what is essentially a signature over itself uh you know just by mining and finding a nonce and then you just you just do your thing uh but your proof of work has many downsides you know it has uh it is uh not it has a variable log times it is high variance in block times and of course you know there's electricity we would prefer not to spend a bunch of electricity because it's bad for the environment so we the current lazy ledger design is using proof of stake uh specifically we're using a tendermint based consensus protocol with proof of stake for a sibling resistance mechanism but how do you know what is the stake without any execution well that's why i put a star is that in pr in theory you would have no execution if you say proof of work in practice you need a very tiny amount of execution and the only thing this execution layer does is manage the validator set but this is like to me the bottleneck in censorship resistance is actually like if how do you scale this to 10 000 validators oh well we're using tendermint which scales to 100 validators right but it's a hundred enough for censorship resistance i would say so yes uh well it depends how you define censorship resistance right uh it depends how you define censorship resistance how would you define it it's the the number of validators that control the ordering of transactions or the ordering of blocks so to me this is the liveness threshold the minimum set that adds up to the lightness threshold this is kind of the true measure of censorship resistance how many nodes do i need like to bribe to just for my hedge fund to prioritize my usct transfers between finance and coinbase yeah i mean there's no there's not really an easy way to do this is that if you have a very very large validator set uh you know let's say you have a million million individual validators then you would need to manage those you know million validators which necessarily you need some sort of state there right yep you can't really you can't really avoid this uh so you know if you if you really care that much about the number of validators over anything else uh which you know maybe that's a good metric maybe it's not uh some might agree some might disagree and we can talk about you know and we can shortly talk about maybe a few other metrics uh but you know if that's what you cared about then you could either use proof of work where you have you know unlimited validators block producers there's no limit to the number of block producers you can have in proof of work or you could have uh there's another consensus protocol that is that works with the proof of stake that supports a large number of validators i think it's called avalanche but they don't replicate the state yeah but they have but but that has its own set of problems that i i brought up in various twitter threads uh so you know it's an interesting consensus protocol in academia uh in practice it introduces a number of issues so you know if you if you want if you really wanted you know a huge number of people being able to contribute to block production you could use proof of work uh but this kind of leads nicely into the next point which is that technically anyone in the world could you know do could do proof of work but if you look at bitcoin which is supposedly you know highly decentralized and highly censorship resistant you know there's about three four h mining pools that control 51 of the hash rate right so so there's like i think like in my mind there's two different kinds of censorship resistance one is the 4k ethereum classic rollback that's a beautiful example of like a moto style censorship resistance where eventually no matter what you do the heaviest fork wins and do you like it doesn't matter what you do right if there's somebody like in the dark producing a bigger bigger chain that's the thing that's gonna win um but that's not like important for inner financial like things that are connected to this network for them to transfer goods and services right like i have finance and coinbase and i want to transfer usdt between them i don't give a about the 4k [Applause] like rollback that censorship resistance depends on who gets to like kind of like decide what what block gets approved and what block does it and that is like the minimum set of like the liveness uh yes and no uh so i think there's a disagreement here and in what consensus protocols actually provide right and i would say that a consensus protocol you know the permissionless consensus protocol on its own the straight up doesn't provide censorship resistance uh you need something else uh and in in nakamoto consensus uh this was an incentive right nakamoto built in an incentive that incentivized people to not censor and the the sentence that's important uh from my reading is you know he ought to find it more profitable to play by the rules that will favor him with more new coins than everyone else combined than to undermine the validity of his own wealth right so there's an extra incentive here and with that incentive you know if someone tries to do something like you know a 51 censorship attack and just stop a transaction from being included on chain a penalty will be applied that's the undermining the validity of his own wealth so the consensus protocol on its own like you know let's say nakamura consensus that's how you completely just take away the incentive it doesn't provide censorship resistance it does not guarantee that you know a transaction will make it into the chain it does not guarantee that if you join the mining network that you will be able to produce a block that's accepted on the canonical chain correct yep yeah that that i agree with you i think that that's where like yeah but uh but the incentive doesn't really have anything to do with the number of nodes right it's in fact it has nothing to do with hash rate or just taking validating power at all it's completely independent it's a it's a social coordination thing right but the end result and proof of work is that you have a large kind of concentration of hash power to a few mining pools even if those pools like there's a difference between those pools corrupting the state or doing like double spent or doing something catastrophic to like preferring one hedge phone to another well a censorship attack is not detectable objectively correct uh you need some you need subjectivity to detect a censorship attack so that the censorship attack i would say is you know a case of favoring one one hedge fund over another is essentially right yeah would you agree with that yeah yeah yeah i i this is like an impossible problem right well it is that's the thing is that it is an impossible problem which is why the consensus protocol does not provide censorship resistance censorship resistance comes from social coordination it comes from the incentive but if your consensus cannot handle like one approach to solve this problem is you just make it an extremely large set sure that's one approach so i i suppose there could be other solutions right but like i think tendermint is pretty limited at that capacity well so the other solution that i would suggest which is what we've seen in practice with bitcoin and it's what we've kind of come to understand with modern proof of stake protocol design is that uh these permissionless consensus protocols are really just tools for social coordination and none of them are objective even proof of work because it requires some subjective component to enforce penalties like no one would agree to changing the bitcoin proof of work hashing algorithm and make that new coin bitcoin unless there was some subjective agreement among people to say you know miners are currently attacking the system we need to break their asics by changing the hashing algorithm this requires subjectivity right so proof of work was never objective in the first place because it requires a subjective component for the incentives and if we have that then we don't need so things like a large number of mining pools or as large number of validators what do we need is the cost of a 51 attack must be high which means that the cost of us going from automated consensus which is what the consensus protocol provides to social coordination which is required for all these permissionless consensus protocols to work that cost we want to maximize it and that's the only thing that matters so long as you maximize that cost you could even have three people doing the consensus protocol if you really wanted to um i love that definition actually and especially when applied to like cross chain bridges because there's a lot of like weird cert like weird attack vectors where some things are automatic and something some things are not but like the surf like collusion surface itself seems similar um so that's like a an interesting way to phrase it yeah because let's say you know let's say banks uh traditional banks nowadays okay let's say you know let's say there's five of them i don't know how many there are in the states but you know let's say there's five of them i think in canada there's five or whatever uh you know let's say they start colluding and they say okay you you john uh you can't open a bank account in any of any of these five banks well what can i do i can't do anything right if some if you know a majority of validators and some tendermint consensus protocol decide to start censoring one guy you know this one guy goes and sends out his transaction to everyone and says hey look i have a transaction here it pays enough fees it hasn't been included in a month these guys are censoring me you can actually slash them and you can fork the entire system you cannot fork uh traditional finance you can fork blockchains and that's like that's where all the power is is if these people you know i'd say the miners tried to 51 attack if validators decide to you know 67 attack and do some censoring you can just fork the whole system and make some change with proof of stake you can burn their stake with proof of work you can change the hashing algorithm and you can just make them lose money you can't do that to banks nowadays and this is where the power is it makes the cost of a tax very high that makes the cost of us having to move from automated consensus to social coordination very high do you think that um the that like i'm kind of seeing this play out in in uh in u.s politics right now where the automatic consensus is failing is that like is that enough like i i'm like worried that like it's not i think like proof of work has some meat to it because at the end of the day there is something about like just the electricity backing it and the hash power wars when picking one side or another being a form of kind of like much much simpler social consensus yeah i would say that proof of work has a lot of interesting properties specifically that it's uh to to move from uh you know automated consensus to social coordination you can't really fake it in any way you need to spend the money while with fruit of steak you can do some do some trickery it's not easy trickery uh you know people who say you know proof of stake is just fiat is like they don't understand this thing like i just said that you can't fork the fiat system you can fork uh proof of stake protocols and burn the attacker's stake they're fundamentally different uh but you know the you can do some trickery it's very difficult you know it requires things like prolonged clips attacks and whatnot but you can do some trickery and proof of stake to you know get someone to for a short time uh believe something that's false without spending too much direct resources one proof of work you have to spend the money uh that being said uh neither system is really you know perfect against state actors right if china wanted to since the majority of hash rates in china if they wanted to 51 attack bitcoin for the next few months or whatever they could do it and it wouldn't really cost them that much money since you know they just take the hash rate uh so you know neither of these systems is inherently perfect against state actors uh so so in that context does does it really matter yeah yeah in the context of state actors i think we're all scripted um do you uh i guess like have you guys thought of like dumping the data into our weave or like using something similar to of like like to rv as as like the consensus mechanism because it seems like there's like a like if you could make that work it would be kind of a beautiful combination of proof-of-work style system and data availability for this like base layer so it would be interesting uh but one thing to note is that uh systems like r-weave or scia or or filecoin they might be better with things like data retrievability but they don't necessarily provide data availability guarantees given the current context specifically by data availability guarantees i don't mean is this data available period uh because that's you know fairly easy to do you just try to download it right if you want to know if the piece of data is available you just download it but specifically when i say data availability i mean data availability by doing sublinear work uh you know traditional blockchains you know if you're using bitcoin i can know the data is available because i run a bitcoin full node but that just fundamentally doesn't really scale because you can't expect every single person in the world to run a full mode you you would expect eventually you know a lot of people will run light clients and if the light clients can get tricked by without blocks that is very bad there's a very bad position to be in and you know systems that deal with file or you know just data retrievability or storage things like you know cya coin i guess r wave to a certain extent uh file coin don't really provide data availability in other words they don't allow a light client doing sublinear work to know or to convince themselves that the data is available or not what about like zero knowledge systems zero knowledge systems also don't help with this uh because in like let's pres let's you know let's assume that you have this blockchain that you know recursively proves that the contents like the transactions in every single block and the structure of every single block all the way from genesis up to the current tip it's all valid right so you could essentially have the you know of one blockchain that you just verify a single block and it tells you the entire history right but that's not true you can't do this because if you don't know of what's inside these blocks you can't produce a new block you could get censored so if the whole network kind of agrees okay this is the tip but no one can produce a new block except you know a small handful of people you can't you can't you can't make new blocks right you'd have to like revert maybe you know a million blocks later later on down the line when you figure this out so you know all blockchains need some data availability at least for full nodes uh and ideally you want to have data availability for light nodes too does that make sense yeah that makes sense um so what like i i've like tried to like struggle with this problem too because i think like the way we've been building solana is that i don't really care about the history what i care about is the final state and we do this like bfd style agreement on the computed state and then slash everybody that disagrees basically right like you can kind of like build like clients on top of that by if i if i know the set of validators that i care about and they all agree on the thing that i care about i can slash anybody that produces um like uh invalid attestations of the thing that i that i'm asking about um but everyone always asks about like it's not a it's not a blockchain because we don't care about the history so i almost started calling it a replicated state machine um which is funny it was just funny given that you know blockchains or replicated tape machines exactly but like just leave me alone i don't i don't know if the history is important but i feel like there's a opportunity for use cases that don't care about it to use something that is tailored for but i wondered is it possible like to build something where if not for data availability i can at least have like guarantees that this is part of the main state like so you know we could run like an rsa accumulator that records every transaction um then anything that you pull from history you at least can check hey is this thing part of history you could but i mean technically a blockchain also allows you to do that and if you store the restore the hashes of previous blocks in the in a merkle accumulator also like a merkel mountain range for example then that also gives you the same property or again it gives you the property of being able to provide logarithmically sized proofs of a transaction being included using only a single block like at the tip yep is that like good enough for for like people that care about the historical data set because uh what what what is like what do they actually care about when they when they want to see the full history in your in your mind what does that care about uh the i guess what now that's actually a very good philosophical question and this kind of brings up the point which is what's even the point of doing all this why are we doing all this right uh presumably the the well here's the thing not everyone needs uh you know the philosophy the not everyone needs to fully validate a blockchain right it's entirely possible to find use cases and applications for you know blockchain that doesn't have the same philosophy as bitcoin uh you know do you just you know just an example the centralized financial system right uh well that works just fine and they don't even use a blockchain right and you can have systems in between uh things like you know let's say permission blockchain uh that is just there for transparency right imagine if all government spends were done through a blockchain uh then you would have you know some accountability and authentication uh for you know holding uh public servants accountable and that would be strictly better than the status quo despite the fact that it wouldn't be you know full on bitcoin so there's like a spectrum there and everything within the spectrum has use cases uh but you know if you wanted to go you know full bitcoin bitcoiner philosophy of you know you want to be able to fully validate the blockchain uh not that you do do it uh but that you are able to do it uh then uh you know you you want to have the the historical blocks around so that you can reconstruct the current state and make sure that nothing bad happened in between so this is where i feel like the objectivity maximalists if you want to call them that barf and anything that like is like throws away all data or i think in your case like doesn't do the computation because they get scared that how do i know the result of this thing if i have to download a big pile of data with unbounded compute that i have to execute oh oh actually so there's a subtle point here which might have been missed uh which thank you for bringing that up or you might be talking about this uh but you know a few of the few of the people in the ethereum resource space have also completely misunderstood lazy ledger uh probably because as it was originally described in the academic paper it wasn't clear on this specific point uh but when you kind of synthesize it with the optimistic roll-up construction which i i created uh you get some you get some nice energy which is that lazy ledger is a universal blockchain as i said i said earlier that's what i call it because you can implement anything on top of it and you can implement any execution system and actually one thing you can do is you can implement another blockchain on top of it so instead of just ordering transactions you can actually take whole blocks and put them on lazy ledger so in that context a block is consists of a block header plus a block body so if you want to know the state route of you know your application on top of lazy ledger you just download the block header for that application and the the only thing you need is that verifying whether or not someone had the right to produce this block must be exponentially cheaper than verifying the whole block and as long as you have this property then there's no problem um but i still have to like if i'm running it like evm right or a big pile of execution somebody still has to go execute all of this like how do i how do i trust the late the lazy laser like tendermint validators and any applications that use this okay uh yeah so okay so we can actually go go through like a more practical example let's say you know you have a laser laser system someone creates a ethereum virtual side chain as we call the ethereum application that it runs on top of laser ledger this would look like you post just four ethereum blocks and you just post them to lazy ledger and you post them to a particular namespace id uh this is the second facet that distinguishes lazy ledger from from charted blockchains which i forgot to talk about earlier but i might might as well talk about it briefly now we have this construction called the namespace merkle tree which is really just like a tree that lets you do range proofs uh there's an augmentation on things like you know the merkle sum tree uh and what this what this allows you to do is if you have multiple applications in a single lazy ledger block each of them can be assigned to a namespace id and then you can provide efficient range proofs to say you know here are all here's everything inside this lazy ledger block that consists that that is from this application and here's some proof that i'm not i'm not uh i'm not withholding any of that data that you know here you know this is the entire set of of data for that application on this lazy ledger block right yeah so once you have that now you have to ask yourself okay what's now let's say you do it ethereum as it currently exists so you have proof of work you know what's stopping someone from spamming a bunch of blocks uh the answer is nothing just like there's nothing stopping someone from spamming the current ethereum peer-to-peer network with a bunch of blocks right the like someone could create a block with an invalid proof of work like they could just create a block with you know like a tiny amount of difficulty it clearly shouldn't be there if they send it to a node what will the node do well let's say well i guess like i guess i'm done i guess i have to download the full block and execute the full block well no they don't they just download the block header and check to see you know does this match the proof of work if it does then you know they can process it if it doesn't they just ignore it and potentially they ban the node right but there's you can just do you can do that on lazy ledger too you don't have to give you a full node for this application for this ethereum application you don't have to download the fake ethereum blocks you just download the block headers and check the proof of work so you're kind of thinking that lazy ledger is going to be just a pure data availability layer laser ledger itself yes it does not ledger does not provide consensus on application state it only provides ordering and availability of data blobs so kind of like like imagine if i had like a censorship resistant s3 bucket yes with time stamping like does that three do that's three times i guess it does if it runs like linux or whatever right so yeah sure it's got versioning right and you can see if everybody rice is the same as street bucket no data is lost you can actually see the order of whatever amazon decided sure it's a trustless s3 bucket modulo you know consensus protocol caveats yes but why would i ever need to like download the whole thing uh so the interesting thing is you wouldn't need to download the whole thing unless you really wanted to produce a new lazy ledger block or unless you wanted to be guaranteed that you know every single laser ledger block was valid now here's the interesting thing uh we've implemented a sort of or we've designed a sort of like segregated uh segregated system so it segregates transactions that modify the laser ledger validator set from what we call messages which are you know these data blobs so if you wanted to like produce a new lazy ledger block and you know if you wanted to make sure the lazy that your coin did not you know undergo undue inflation you actually don't have to download the full lazy ledger blocks you only have to download the very tiny set of transactions from these large blocks uh and then you know use use those small trends in small number of transactions to get the new state so you there's not really a case where you would really really have to fully download every single laser ledger block you know even if you wanted to make sure that lazy dodger coin was not inflated because unlike in bitcoin where the op return is embedded into the transaction or ethereum where the call data is embedded into the transaction or bitcoin sv where you know the op return which is huge in their case is embedded into the transaction we implement a segregated scheme so that our message data is actually segregated from the transaction which means you don't have to download them you can just ignore them so you know if you know 99.99 of the block is message you know 0.001 i'm sure i'm sure i got those number of zeros wrong as you know transaction data then really you're only downloading a tiny tiny amount of the block to get the transactions um when uh when are you guys gonna be live uh months not years um uh we so it's on the serious answer we expect to have a test that out next year and we would expect maybe mainnet within two years or so i'll i'll tell you it'll be substantially before serenity launches phase one um yeah that's actually fairly decent i think uh a time horizon well unlike unlike systems like selena the nice thing is we don't have to make a huge amount of really intense optimizations into a virtual machine uh we just and we don't have to design a new consensus protocol because tendermint works just fine uh given our interpretation of proof of stake protocols uh so you know we don't have the design you can censor to call we don't have to design a super optimized virtual machine like you guys had to do which means our life is actually on the easy side yeah and i i think um our lives our life is on the easy side because we don't have to deal with sharding so yeah like i think there's there's much more painful paths you can take to build the layer one but i like honestly like i i don't fault people for trying because i think the it would be cool if it worked like i think there's like if you could actually build a network where you have like low availability systems just pop blink in and out of existence but somehow add anything liveness or security um that isn't nakamoto electricity-based proof-of-work um i think that's a really cool thing um i don't know i don't think it's possible outside of what was done with nakamoto but um you know if i'm proven wrong it's only the world is getting better right like so yeah i think there's a paper there's a paper i think by the trifecta guys uh i want to call i want to say it's prism uh i think they wrote prism and trifecta but if a remember is prism that uh uses uh sortition i do the proof of work plus sortition uh so it lets you do some sort of sharding with proof of work uh without having to worry about you know the usual you know a bunch of hash power goes on to one chart and it dominates a shard now you don't have to worry about that because it uses sortation uh so sorry does it have any pentagrams in the white paper i don't know but so this is not an endorsement of prism but rather it's i read it and i thought it was interesting and i couldn't find any flaws immediately so it actually looked fairly interesting uh that being said you know unless you unless you add things like you know data availability plus you know like client sampling and all this other stuff proofs then you know just just doing sortition doesn't really help you you know having a charge doesn't help you unless you also do statelessness plus uh plus data availability checks with like clients not playing if you don't do those things then charting doesn't actually help you because if you don't do those things every node has to fully download everything anyways yeah i think like the scary part is that like most of these systems are going to be run by the same validators which are all good actors just trying to like keep this thing going um there's about 100 of them so that's it right then like they can the intersection between all the chains is the super majority in every chain yeah you're not wrong and this is one of the reasons that you know and actually initially maybe like two years ago i was very against tendermint because specifically you know it can only support up to a hundred validators i'm like oh that's terrible but you know you're thinking about it more and it's like you know you have maybe a hundred people that have the majority of the stake what's the big deal and again it's like concentration does not imply control yeah yeah um i think this is this is where like i think i i want to see somebody else try pick those crazy points in the pareto curve and like try things that haven't been tried before um and like i'm i'm disappointed with how slow like charting research is going and like actually getting these systems deployed um and that because it's you know easy and people are dumb it's because it's really really hard yeah it is and i mean if i can rant for a minute about you know sharding and how how i don't think it's viable or how i think it's a terrible idea in general at least in the context of blockchains is uh you know they make at least the currently contemporary sharded designs uh make a lot of assumptions around having a network that does certain things uh which you know seem benign if you don't think about them or maybe that you don't even know about them because the assumptions aren't outlined but like they push a lot of the properties that you would expect from the consensus protocol and they push them onto the network and then it makes it look like oh our consensus protocol can support you know thousands of validators look at that it's amazing you know the started system it can survive roller three you know where geniuses and all that uh but that's because they push a lot of the complexity and not just complexity but like security guarantees and stuff from the consensus protocol onto the network stack and the network might not actually do everything they need to guarantee this uh one example is things like you know fast rotation of gossip subnets uh you know if you have like committee based shirting and then you have you know a bunch of committees that need to shuffle around then maybe you're asking people to you know quickly change their gossip subnets or gossip subscriptions you know every six seconds or something and it's like do your does your gossip network actually do this right if it doesn't do this then you just assume that the gossip network is a black box that can do this uh but it doesn't right you've you've moved complexity and guarantees away from the consensus protocol into the network but the network doesn't actually provide these guarantees so like your consensus protocol is now also now also screwed there's all other issues around you know if you have a global passive adversary they can essentially snoop in on uh they can snoop in on network traffic and figure out or they can link validator ids to ip addresses and if you don't have you know systems like how tendermint has you know with where you expect validators to maybe be a bit larger you expect validators to run century nodes and so on uh you know if you expect users to run you know and then open a port in their in their home firewall and run off of the raspberry pi or worse yet if they're running off of their home computer uh you know with all their passwords and stuff on that uh with you know with no protection you know a global passive adversary can link things like validator ids to ip addresses just by snooping network traffic and now you get into the problem that's very easy to denial of service the system right stuff like that yeah yeah i i i don't disagree with you there um do you know of any anyone trying like i guess there's kind of like a couple families that are springing up um like i think avalanche and dags avalanche is probably the the dag that's gonna survive um maybe well i mean there's there's iota and there's nano i think the other two or nano is a black lettuce but i i owe it as a dag uh but yeah i thought the tingle got untangled and it's now like he uses like some kind of like coordinator to linearize everything uh wait no it always use a coordinator to literalize everything it always has i mean i explained this to one of my friends like a while ago but essentially if you have a dag and then there's no there's you know if you have two transactions that are conflicting across the dag how do you how do you join them right because they don't have a consensus protocol for this uh the answer is they use a blockchain they use a linearizer to linearize transactions to order them and that thing is called the coordinator so the diag doesn't actually do anything it all depends on you know what is what is the what does the coordinator spit out in the newer blockchain um i mean to me all those protocols are basically a vector clock like a i mean i guess iota is is a not a vector clock it's like a timed pla it's a single server but like avalanche and uh hashgraph yeah but that's the one right like you like get a set of of uh versions for everything from from everybody and that that's the vector clock for that particular message yeah dags are interesting i think conflicts also uses a transaction dag of sorts uh they do linearize it post facto but they when they build up they build it up as a transaction tag uh i think avalanche also does linear does linearization uh but this comes with certain synchrony assumptions between the p chain and the x chain uh but you know these the the systems do like after the fact linearization of some sort do you have time to read all this stuff uh basically all i do from the time i wake up to the time i sleep is work on laser ledger work on optimistic roll-ups and read papers and blog posts and also i troll on twitter um what is your take on on optimistic roll-ups i think optimistic roles are good but obviously i'm biased because i'm the one who originally came up with with the idea of optimistic roll-ups so i'm a little bit biased uh but i think they're pretty good not because they scale the evm to 10 000 transactions per second but because they're essentially a way of doing like a pseudo-sharding on top of a common data availability layer right you can charge your execution without shorting the data availability layer so so in that context like you can essentially get all the benefits of e2 just by running a bunch of optimistic roll-ups or a bunch of the benefits of serenity just by running a bunch of optimistic roll-ups on top of ethereum today so my view with uh roll-ups was that like it almost seemed like a perfect bridge for at least for like acid transfer data trend like not not like arbitrary message transfer but for like moving tokens around because if i can if if like ethereum looks at like evm state that's executed in solana when solana comes to settlement on that thing all the other systems all the other programs external to that roll-up can trade against that state because it's guaranteed internally but then when ethereum gets the data they can actually like trust that whatever was executing in solana didn't just you know mint tokens or fake signatures that that can be verified so to me it seems like almost like you know like what bridges should have been designed like yeah the funny the funny thing is i had talked to james prestwich about this also around the time that i was developing optimistic roll-ups i don't remember the exact dates uh but he had come up with an interesting way of doing a bitcoin bridge optimization that was along the same lines of doing you know optimistic execution uh which as you're correct this should have been how it's done well bitcoin is a bit cheaper because of the fact that you know there's a sha26p compile on ethereum but things like let's say the near bridge uh i think selena has a bridge too to ethereum does it uh we have a very very dumb one that we could ship very quickly which is uh like a multi-sig yeah that was it you guys you guys did have an uh uh an initial one and i think uh kyle salami also talked about two other designs that you guys were working on or something along those lines but you're near also have a bridge uh and i assume some other protocols are bridges uh lazy ledger is being built so that you know we can natively bridge to ethereum so that you can use lazy ledger as a data availability layer for rollups uh which you know some some fundamental design goal here is that we want to have we want to be able to support uh not only native applications on top of daisy ledger but roll ups both optimistic and zk on top of ethereum and things like cosmos zones and potentially other systems that want to use lazy ledger for you know shared data availability security uh but you know there's a bunch of systems building bridges and you're correct that there's no point to verify all the signatures when you can just do it optimistically yeah because here's the thing all the blockchain problems come down to data availability and ordering that's why you can build these optimistic systems yeah for the most part and then the application problems are execution um there's there's no way to like cheat the devil right like they're an implantation detail we don't worry about those yeah um cool i mean we kind of ran over uh it's been a fascinating conversation it's awesome they were on the podcast and like um it's really cool to see people try crazy things um i think like we're we're still so early in the space that there's a ton of room for people to go and and like take these crazy ideas and see what can happen yeah thanks for having me i'm you know a big fan of the do you have no sharding and i'm also a big fan of the insane work that you guys have done into building out an optimized virtual machine and your parallel execution and all this other stuff you know dealing with concurrency and member state accesses and caches and stuff it's like it's top-notch and i wish more people in the blockchain space understood these things as being fundamental to blockchain scalability like you guys do thank you yes [Music] 