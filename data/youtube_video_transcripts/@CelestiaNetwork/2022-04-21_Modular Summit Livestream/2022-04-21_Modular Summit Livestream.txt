[Music] i want to me just like [Music] and this you leave you for me [Music] [Music] [Music] please [Music] just like they sitting on the say of the roof the bridge is on my steam minions roll by the bridges fall down and so do my dreams oh you hear me calling your name the bridge is your time your engine rolls hot if the bridges fall down don't lose [Music] boy i didn't make it too far but baby you are the family star i'll tighten your seams don't lose [Music] young [Music] hey [Music] [Music] is new whatever you do is up to you but do me this dude don't lose your head of dreams young man [Music] [Music] young man young man [Music] [Music] [Music] can you get me to the other side [Music] don't lose for me to find my truth is [Music] [Music] these days these days three years remind me [Music] sit down together [Music] [Music] ten [Music] [Music] oh [Music] [Music] bye so [Music] [Music] [Music] [Music] my baby [Music] [Music] [Music] all right hello welcome to the modular summit uh what an incredible turnout we have um and and we're expecting a lot more people so this room should fill up in a few hours um i'm one of the mcs for today a dt i work at celestia this event is about mapping the next generation of blockchains since the agricultural revolution humans have relied on the specialization of increased efficiency and productivity blockchain is no different um modularity is becoming a huge paradigm shift in the way we'll build and scale blockchains and today we'll have representation from all the modular players in the ecosystem including roll-up teams bridges other l1s and more i'm excited to kick this event off and i'm going to pass it off to baldur to do a proper intro thanks guys a little late sorry yeah so hello everybody um very very warm welcome here in amsterdam uh i think everybody has so far has had an more than amazing week right i'm very surprised by the turnout this week during death connect and also the quality in the amount of people good lesson we shouldn't start a crypto event at 9 30 right that's clearly a little too early for a lot of people in the in the crypto scene um and also what's great is that i think amsterdam weather is slowly sort of rock pooling the crypto the crypto scene right i mean we've never had this weather and now for a full week we've had 20 degrees and sunny sunny skies normally it's like a lot of clouds and very bad weather so great to see come come come over more often we're happy to happy to host here [Music] very happy to kick off this amazing event uh celestia team and ourselves in maven 11 have been working around the clock last eight weeks to get you set up in this amazing location and start chatting about modeler designs modularity optimistic uh execution environments share knowledge execution environments like the dtset bridges um and we um celestia made 11 teams go quite a long way back we battalion myself i remember very vividly we read mustafa's paper at the time called actually lazy ledger for the ogs in the room and we were very excited about about what he has obviously been doing at the time so we go almost three three four years back um and i think i wanna thank mustafar for all the time and how she spent with us in making us understand how this date availability and modularity should look over time and so without further ado i would like to get mustafa united states and i wish you a great day from the maven 11 and celestia side here today [Applause] hello everyone a quick little background about myself as was mentioned i'm co-founder at celestial labs which is a modular data availability layer previously before that i was doing a phd at ucl where i was focusing on layer 1 scalability and i worked on the first smart contract sharding research paper called chain space which was later kind of spun out into a startup that was that was acquihired by facebook but i was the only like team member that did not join facebook and i ended up working on lazy ledger and celestia instead um which i think was a very interesting like alternative design to blockchains so i'm going to talk about three main points today the first point is we're going to actually set the record straight on what is a modular blockchain stack uh and define it clearly and what is not what it is not and then we're going to give some examples of of configurations of blockchain stacks and we're going to talk about the key benefits of modular blockchains so when bitcoin was first proposed in 2008 um it proposed a model where it was a monolithic blockchain where effectively the full nodes and the validators they do consensus on the chain and they also execute the transactions in the chain so they provide consensus and computation on the chain and for the 10 years after bitcoin most blockchains followed the same model so when ethereum came around in 2013 it followed a very similar model to bitcoin except that it replaced the execution environment of bitcoin with a smart weather with a general purpose uh smart contract environment known as the ethereum virtual machine but effect but the actual like architecture of the blockchain was was similar architecturally in that it was monolithic and full nodes did the same general tasks but obviously the problem with this approach is that um if a validator is effectively and full nodes to verify the chain have to do effectively two things the first thing is they have to check if the chain has consensus and the second thing they have to check they have to actually process and execute every single transaction to check that all the transactions are valid and that obviously doesn't scale and it's quite limiting because obviously it doesn't scale if full nodes have to execute the entire state and transaction history of the network but if you think about what is like the most basic blockchain you can create like what is the most minimal layer one if you if you took bitcoin or ethereum and stripped it back to its core components what would you get and what is the core functions of layer one if you remove the execution from a layer one what you would get is a blockchain where full nodes do two things the first thing is consensus they take transactions and they order them and the second thing is that they ensure that the data of those transactions are actually available and the reason why you want to ensure that the data of the transactions are available is because that's all you need to have a to build an application on the blockchain to allow clients and users to know what's on the blockchain to compute the state of the application and i wrote a paper about this in 2019 called lazyledger which was the former name of celestia well i proposed a paradigm for blockchains where the blockchain is only responsible for consensus and data availability and all the execution happens of chain which i call client-side smart contracts or client-side or off-chain execution and that's basically the same model of how roll-ups work so for the off-chain computation you can use a roll-up um you know zk roll-ups burst in 2018 and optimistic roll-ups was proposed three months after i released a lazy ledger paper and in the roll-up paradigm the roll-up execution happens of chain but you're only using the layer one chain only for data availability and consensus which is exactly what the lazy ledger kind of paradigm does so in this kind of model your effect what you're effectively doing is separating consensus and execution whereas previous blockchains like bitcoin and ethereum nodes did consensus and execution in this new model layer 1 nodes only do consensus and data availability and the execution happens on a layer above above the stack um off chain not on layer one either as a roll up or as a or as a different kind of layer two solution and this actually makes a lot of sense if you think about it um so you know in computers you have this thing called the osi model that proposes a modular kind of like stack for computing where at the mo at the bottom of the stack you have the physical layer which is actually the actual cables and wiring and hardware of the of the um like internet or or on the network and then if you go all the way up the stack you get the application there you know like facebook and google but in between you have all of these different layers that can be swapped out and and you know i think blockchains are kind of shifting to this paradigm because it makes a lot more sense and i'm going to talk about why so let's talk a little bit about what are the actual layers in a modular blockchain stack once you decouple them from a monolithic blockchain so the first layer is consensus and consensus the consensus layer is simply responsible for taking arbitrary data or arbitrary messages and then providing an ordering over those messages so a developer would submit a bunch of messages to the network and the blockchain or the consensus layer would tell you in which order those messages are serialized and that's at a fundamental level that's what consensus does then above that you have data availability and what data availability is trying to solve is once the consensus nodes or the consensus layer has decided or determined what the ordering of those messages should be they usually commit to that as a medical route or any kind of other commitment but they also need to actually tell the network what the actual data in those messages are and because if they don't actually publish the data that they agreed on then users and and applications would not be able to have know what the state of the application is because they don't know what the transactions are in the application and they wouldn't be able to generate things like fraud proofs in the case of optimistic roll up for example um so with data availability there's some cool techniques you can do for like such as data availability sampling which lets you verify that the entire data is available by only downloading a very small percentage of the data then the third layer is the execution layer an execution layer takes the messages that have been agreed on ordered in the blockchain and have been made available and then does some computation or processing on those messages so in this example um on the board you can see you know you take a bunch of transactions in and those transactions might be payments and then once you actually compute those payments you can know what people's balances are and in the execution layer uh nowadays people typically use roll ups whether optimistic or zk roll ups and you can prove to users uh the execution was done correctly by either using zk proofs or fraud proofs the final layer which kind of sits in between the execution layer and the consensus layer technically is a settlement layer a cell element layer is basically like a special case of execution layer it's basically an execution layer that bridges other execution layers together so as i mentioned you would typically prove in a roll-up whether a computation was valid using a fraud or zk proof a settlement layer basically does basically verifies this floor or zk proofs um on the settlement layer itself and provides dispute resolution on it and that would basically allow different um execution layers and roll ups to bridge each other to bridge with each other and transfer assets with each other in a trust minimized way now having listed all of the main components of a modular blockchain stack like how do we define what a modular blockchain is so i define a modular blockchain as a blockchain that outsources fully at least one of the four components like ie it does not handle that component so so for example like solana wouldn't be a modular blockchain if you just added roll-ups to solana because solana's l1 still has a smart contract environment and its re and its validators are doing data availability consensus and execution and they're not specialized in a specific task and therefore it's not it's not modular so what is not a module blockchain there's often some confusion on this a module like a blockchain that handles all the components but has a modular software design is not a modular blockchain a modular software design can be helpful to build modular blockchains like we use tendermint uh the abc i construction in tenement um is very helpful to build modular blockchains but the software like but deploying a blockchain using this modular software library does not make that blockchain modular in itself and secondly a network of blockchains where each blockchain network handles all the components is also not a modular blockchain for example avalanche subnets are not modular blockchains because each chain in that subnet in the network handles all the components that i just described therefore it's not modular let's go through some examples of what a modular blockchain stack could look like so as i mentioned there's these four components data availability consensus settlement and execution so you have one you have monolithic blockchains um like bitcoin or ethereum as it is you know traditionally where you basically have a like a general purpose smart contract environment and the validators and the full nodes handle all of those four components then you have roll-ups and in roll-ups um the l1 handles the first three components data availability consensus and settlement but but not execution and the roll-up itself handles execution of chain and then you have validiums and validiums are basically roll-ups or but they do not have on-chain data availability or they do not use um like the same layer 1 as the settlement or consensus for the availability and therefore they are not roll-ups but more like side chains um or by lydiums as as they're turned so this is kind of like the kind of modular design in the ethereum space that has been kind of like discussed so far but then there's also a more of a celestia-centric modular design or configurations of the stack so in celestia we're quite interested in this idea of sovereign roll-ups and in a sovereign roll-up um the sovereign roll-up uses the l1 only for data availability and consensus but it does not um have an enshrined settlement layer instead it does its own settlement and the reason why it's sovereign is because it can hard fork and it can upgrade without permission from the cell from a higher execution layer so for example if you have an ethereum roll-up your roll-up is effectively like a baby chain to the ethereum settlement execution layer and you can't really hard fork it without you know requesting without convincing the ethereum kind of social consensus to do so but with a sovereign role up because it does its own settlement it's effectively like a its own layout one chain it's effectively like deploying your own layer one you can hard fork it and it can have its own social consensus you've also got this idea of settlement roll-ups and i started metro lock is basically you know like a standard roll-up except that it only is it's only optimized not for general smart contracts or computation but for um settle for but for settling other roll-ups on top of it and i think uh yuri is going to give a talk about this today with this idea of l ones and l twos and l threes so like for example you can have l2 that has l3s um but the l2 might only be up might only be supposed to be used for settlement for other rollups but you're not supposed to like post like um host actual like applications on that roll-up and then you've also finally got this idea of a celestion which is basically a validium that uses celestia for data availability it's it has similar on paper has similar security tradeoffs as a validium except that it has slightly higher crypto economic guarantees because of celestia's data availability sampling and slashing here's a different kind of like current players in all of these different um like stacks as you can see there's currently a higher focus on the execution layers in this stack and i think that makes a lot of sense because there's a lot more kind of innovation or divergence that can happen on the execution layer of the stack than the lower layers which kind of do more of a kind of like simpler role simpler but more important role let's talk about some of the benefits of a modular blockchain stack so first of all um obviously the big one scalability so like intuitively marginal blockchain stacks are more scalable because there's a separation of resources and that means each node in each layer of the stack can be more specialized to a specific function so for example on the data availability layer the nodes don't do any computation they just do data availability and that means they can optimize their resources to like having high bandwidth resources rather than computation and they can be specialized just on that one task and the main benefit of this kind of comes from the resource pricing you're separating the resource pricing for different resources and network like you have different resource pricing for data than computation and finally in a modular blockchain stack it's quite common to use technologies like data availability sampling and fraud proofs or zk proofs and this is very important for scalability because the whole um like the whole all of the scale blockchains you can't just increase throughput you also have to increase throughput while enabling users to still gain assurances about the correctness and validity of the underlying chain and in traditional blockchains the only way you can do that is if the users actually re-execute every single transaction which most users cannot do but with technologies like data availability sampling and fraud proofs and validity proofs it allows ordinary users to effectively be first-class citizens of the network and have almost the same level of security as a full node that is actually downloading all the transactions without having to have the same resource requirements as a full node secondly flexibility is a major advantage of multiple modular blockchains we can see there's a kind of a cambrian explosion of different execution layers that are innovating in different ways in the ethereum roll-up space for example and these different layers these different execution layers have different advantages for example you know fuels execution layer is parallelizable for example and this is very important because um if you think about the history of the web and this i like before before the clouds or virtual machines were popular people used to just use shared web hosting providers you know like dreamhost or or you know geocities and that really limited the innovation of the web because you were limited to whatever like execution environment or programming languages that that host provider had on their server but nowadays no one uses that nowadays people just deploy like virtual machines on amazon ec2 and they effectively have their own operating system and they can install whatever they like on it and experiment with all kinds of different like programming languages and technologies finally i think this is more celestial specific but if you think about the different layers in a blockchain stack it's quite you know we have layer one which is commonly known as the consensus layer but the consensus layer only has value because people agree that it has value like people agree that the current ethereum chain or the current bitcoin chain has value opposed opposed as opposed to some fork of ethereum because people because people have just agreed it by social consensus you know like if i fork ethereum no one's gonna say that's the real ethereum so to define the real ethereum that requires you know social consensus and but applications on shared smart contract platforms and smart contracts do not have that same property instead they kind of um borrow the social consensus of the layer one that they're using or the execution environment they're using so like if you wanna if you have a smart contract in ethereum you can't fork it without asking without convincing the social consensus of ethereum that's to some people as a feature but to others that's a bug and but with sovereign rollups you can actually uh have your own layer one execution environment that your community can hard fork and that it therefore has its own social consensus and i think this is a very interesting kind of model to me the whole point of blockchains is that it's basically a social coordination mechanism for uh off chain social decisions that have been made i think there's kind of three main values um from my perspective of a modular blockchain stack or blockchain or the idea of blockchain modularism in general firstly users are first-class citizens citizens of the network thanks to technologies like data availability sampling and dk proofs and fraud proofs those technologies allow users to have the same level of security as a full node or similar level of security as a full node without needing the same resource requirements and hardware as a full node and so you're allowing end users to actually be your first class participants of the network secondly i think a second important value is um you know this idea of layer one fighting and layer one maximalism is kind of it's kind of you know getting old and i think modularism is much more interesting than maximalism because it's not a zero-sum game and if you allow develop developers should have the freedom to build their application how they want according to their use case and the more you know the bigger the more players there are in this module blockchain ecosystem and the more value there is to be created and finally i think an important value is that if they want to communities can choose to be sovereign and by deploying sovereign roll-ups and the main difference with that with other without with deploying your own l1 is that it's much easier to deploy a sovereign roll-up than it is to launch your own new um like layer one chain with its own consensus because you have to bootstrap you know a secure and decentralized validator set using something like proof-of-stake but with sovereign roll-ups communities for the first time have the ability to create a sovereign blockchain very quickly within minutes without having to worry about the overhead of maintaining and creating their own consensus network with a secure validator set because they can effectively have shared security with the data availability layer without and without losing the sovereignty of their execution layer i think the reason why i think it's so important to kind of move towards a modular blockchain stack is because over the past 10 years we've kind of been stuck in this kind of cycle of like layer 1 chains being created all the time because the previous layer 1 chains could not scale and you can see this is not ending well because you know all of the chains here have also run into performance issues and gas is transaction fee issues and uptime issues so clearly we kind of need to escape this cycle of new layer ones with something that actually kind of works i think the best way to do that is to have a modular blockchain stack and use technologies like roll ups and zk proofs and fraud proofs and data availability proofs so if you want to free yourselves from the kind of limitations of monolithic layer ones then build modular and gain your freedom thank you everyone do you want to take a question oh yeah uh we have time for questions cool all right thanks mustafa all right next up we have alex evans who is a partner at bain capital crypto he will be talking about why modular is now a central thesis in their new fund so alex take it away do i have a clicker i think mustafa thank you i think i'm missing a slide but um hello everyone i'm alex evans as dt said i'm one of the partners at bain capital crypto we're early stage crypto focus fund we spend a lot of our time on research protocol design engineering we have a particular obsession uh with modular architectures and blockchain designs and uh i guess in this presentation i want to share maybe some of the excitement with you um in particular i want to talk about some of the scalability properties that mustafa uh sort of alluded to uh carefully in terms of uh what types of scalability blockchain modular blockchains enable and what types of scalability they don't uh and i want to spend a little bit of time talking about flexibility and the acceleration of vm and uh execution environment design innovation uh where i wanna start is uh not by defining uh monolithic versus modular blockchains i think mustafa did a great job doing that but just to point out that like the concepts of monolithic versus modular exist along a spectrum and that most blockchain designs are positioned somewhere along that spectrum most smart contract platforms as we understand them today are becoming or at least adopting some of the ideas of the modular paradigm over time so in some ways you could see see them becoming more modular and that i think paints that world in less of a binary perspective and more than one where these ideas of separating out uh d.a from execution or permeating into the broader blockchain space and i guess the argument for this talk is that there's a very good reason for that and so the reasons for that are number one scalability uh the ability to have greater performance in our systems as well as the ability to support more users while still allowing them to be first-class citizens of these networks and the second one is that modular designs unlock rapid experimentation across the stack in terms of da layers in terms of execution layers and so forth splitting up these monoliths into components allows us to experiment on each one and iterate on the best possible designs so i'm going to take each of these in turn i'm going to start with scalability it's a little bit of a pedantic approach to scalability uh just to make sure that in the time that we have we cover it precisely because there's a lot of confusion around uh what scalability means in these ecosystems and so forth just with a very simple rough definition of scalability we want to increase the capacity of our systems to handle more transactions cheaper but we want to do so in a way where users are still able to catch up to the tip of the chain or able to validate the transactions that have happened thus far are able to reconstruct the state or request the state with very minimal trust assumptions such that they're able to author their own transactions validate the transactions of others and importantly do so without trusting anybody else in the system that includes minors it includes other nodes that gives users full autonomy of these systems mustafa mentioned really eloquently this is one of the core values of the modular design ecosystem it also turns out to be a core value what makes to us at least blockchains and public elections in particular interesting so oftentimes we hear that this chain or that chain is more scalable oftentimes we're simply referring to cost that we i guess the point of this part of the presentation is that we also need to consider what the requirements are on users be they full nodes or like clients that comes with that decreased cost right so sometimes we hear roll ups scale ethereum or roll-up scale some other chain and i think we want to be a little bit precise about what we mean uh that is true the roll-ups are a way of scaling uh these platforms however not often for the reasons that are discussed typically people point to some graph like this they show the l1 cost being significantly higher than the l2 cost for the same transaction here is just the unit swap swap on ethereum versus optimism and they say okay one is roughly an order of magnitude or more more expensive than the other therefore we have scaled ethereum uh of course we're only talking about cost we haven't considered uh what actual users have to do to validate these computations that are happening on there too right another thing to note pretty briefly is again this is data from for optimism uh the vast majority of the actual cost that users pay in these roll-ups actually come from the data footprint on l1 in particular in this case ethereum uh today that is projected to go down through all sorts of things that are in the pipeline including making data availability cheaper on ethereum and so forth all this is telling us is that gas is really really cheap on layer two right and we have to reason about whether that can continue to be the case or what we anticipate these gas price and gas market dynamics to be down the line and the way to do that is we want to think a little bit carefully about what the skip precise scalability of splitting up computation from data availability uh is right so just for the sake of argument let's take just like a standard monolithic blockchain uh let's create a carbon copy of it just as a thought experiment that does the exact same execution and just split it up into two different layers one is going to handle data availability and we'll also process state commitments commitments to the state as well as proof that that state is valid in different ways and then all the execution's gonna happen on another layer uh and the question we're gonna ask is have we scaled anything uh just just by doing this and intuitively it would seem no because we're executing the exact same computation so for instance if we're running the evm on another rollup it's just another blockchain we're simply validating the same thing we need to run a full node on layer one to validate the data to download data and validate you know state commitments and so forth it doesn't allow us to reconstruct the state of the roll-up if we wanted to reconstruct the state of the roll-up we would have to run a full node that entire roll up re-executing all the compute thus therefore not inducing a scalability benefit right however we do get some other really important things just by the way we do get cheap uh trust minimized light clients right uh we can just run our full node on layer one and if we just trust one of n nodes on layer two they can provide us that state we can verify that it is indeed the correct state it can allow us to do fast syncs it can allow us to do trust minimize bridges between rollups and lowers the work hardware requirements for users to participate in these systems meaningfully but in terms of running a full node so far we haven't achieved any scalability the scalability comes from two factors that are a little bit more subtle than just the pure cost of looking at layer two the first of these is horizontal scale and the second one has to do with resource pricing right so let's take these in turn this is indisputably a more scalable system than a monolith for the simple reason that it's essentially a form of sharding right so if i want to run a if i'm going to validate zk sync i don't need to also validate a bunch of execution on optimism right so nodes are splitting the work between them this is a form of scalability right we have increased the strictly increased the capacity of our system without increasing the hardware requirements on any one of the nodes in the system or splitting up the work between us now that scalability benefit is somewhat muted given the fact that a lot of the cost as i showed two slides ago is coming from layer one right so we still need to validate this really heavy evm we need to make sure we're downloading data that corresponds to a whole bunch of other rollups and so forth right over time as da costs come down on layer one call it eip4488 call it data you know data only shards and data availability sampling and so forth we get a lot more horizontal scale than you also consider pure data availability change that don't have heavy vms to validate and the horizontal scalability benefits can become very very substantial that said they don't come for free they often come at the expense not always of composability and what we mean by composability just the fact that applications are co-located and can be atomically composed with each other it's a really magical property of public blockchains wherein as a developer i can take different components from different underlying pieces of software and stitch them together into an end user experience such that the end user sees these applications almost as if they're functioning as a unit as one right this is really magical it's kind of what's enabled a lot of the innovation in define other areas of web 3 that we find very exciting it um it's we don't take it lightly to compromise i suppose on this property it's as we understand at least one of the unique properties of building applications on public blockchains right on the other hand um we have a lot of benefits to building splitting up execution and having multiple different roll-ups handle different tasks right including this horizontal scalability as well as sovereignty and flexibility in terms of being able to customize your execution environment to your end goal right so we have a pure classical trade-off as application developers inside of the modular world between composability and horizontal scale on the other side and i guess the question is going to be how do we navigate trade-offs like this uh one is to take the twitterverse approach which is to take really extreme positions and say hey you know we we want all applications to be on the same chain uh and not compromise on any composability ever of course that will mean that our chain is going to be either really expensive to use or really centralized and then the other approach is that every single application you could imagine should have its own layer 1 blockchain right or sovereign roll up right empirically we know that neither of these extremes are truly tenable and that the reality is somewhere in the middle and in fact the optimal point is somewhere in the middle and the question is going to be how do we find that optimal point uh incidentally by the way monoliths can also do sharding except that you know you need to pre-specify what each char does a priori the modular approach to navigating dilemmas like this and trade-offs like this is to let application developers is to let users decide with their money with their what trade-offs work for what applications so you can imagine a game developer may want a very streamlined self-contained experience and they're very happy building a sovereign roll-up or a layer 1 chain specifically tailored to the experience that they want their own users to have maybe a defy yield aggregator is going to want to co-locate with a whole bunch of defy apps might be a little bit more expensive as a consequence but the composability they get out of that greatly outweighs any slightly increased cost to users right the modular approach is to run a whole bunch of these experiments in parallel and for each application iterate towards a more optimal design when we find those designs we can continue to iterate on them and continue to explore and run experiments and get better over time as an ecosystem okay so this second uh scalability property of modular blockchain designs comes without that many trade-offs um it is essentially a pure increase in capacity that comes from more efficient resource pricing so recognize that monolithic blockchains price resources as a bundle so you get some gas you can then redeem that gas for certain bytes of data availability you may also redeem it for certain opcode executions right those resources have a fixed price relative to each other what does that mean it means if demand for one goes up even if supply and demand for the other does not go up the price of it will go up which strictly decreases the capacity of these systems right on the other side on the modular paradigm where you split up data and execution uh if demand for instance for uh running certain operations inside of i don't know zk sync goes up uh data availability for optimism does not go up which is a reasonable thing to say because those things are completely orthogonal resources so we strictly get more capacity in our systems through more efficient resource pricing without increasing node requirements in so doing in fact this property is the more congested the system the more powerful this property is um it's another way to think about it is we have a constrained optimization problem to allocate resources in such a way to maximize the scalability of our systems we simply remove a constraint from that optimization problem right with the constraint that the prices are fixed removing constraints from optimization problems very often especially when when constraints are hard and sad and met for instance in case where there's a lot of uh uh demand for the system greater than the capacity they can handle at any given time uh strictly allows us to achieve greater values for the objective function right um so it's relatively trade-off-free uh way of skating scaling blockchains all right i want to change gears a little bit and talk about experimentation and optimization and this is really maybe the meat of the talk it's what excites us the most about modular ecosystems and designs that we're starting to see percolate throughout the space it's that by splitting up monoliths and allowing individual components to be optimized we get a lot more experiments run in parallel and we get a lot better data layers we get a lot better execution layers as a consequence we get more scalability but we also get new capabilities in these systems that we couldn't have imagined otherwise maybe those are about privacy or maybe they have to do with application specific concerns um so just as a prequel uh we've seen an incredible amount of innovation on public blockchains a part of it is the and more of it happens higher up right a lot of iteration happens at you know the ui layer you can push changes every hour if you want to uh smart contracts move a little bit more slowly you have to audit them hopefully at least and you uh you need to make sure that you know whatever you build you know there's hopefully some users on it and so forth it's not as easy to just iterate um you know on a day-to-day basis maybe on it's on the scale of months but even in spite of that um there's an unbelievable amount of innovation that's happened and the reason for that is is for instance if i am curve and i see unit swap v1 and i'm excited about that design i can say hey i think i can do that better for stable coins i don't have to go and fork all of ethereum underneath me i need to fork the consensus layer anything i just build a smart contract i run it that experiment if people tend to like it maybe i'll get a bunch of liquidity and people will like my product right similarly sushi swap and fork unit swap and add an incentive unit swap v3 can take ideas from everything else that's been built including ads some of their own and we're running a bunch of experiments in parallel figuring out what the right way to do decentralized exchanges and then when we find ideas that work we double down on them in contrast the underlying infrastructure consensus data and execution moves glacially it moves kind of closer to the speed of hardware than the speed of software and there's actually very good reasons for this it's like you know there's like hard fork release timelines you know on a year scale kind of similar to how quickly i you know apple releases iphones and the reason for that is is let's say i want to make for instance a change to the execution environment of ethereum maybe i want to enable greater parallelization in the evm i can either launch my own layer 1 from scratch right i can build my own consensus system or fork the consensus system and recruit my own validators then build the execution environment that i want and building layer ones is really resource intensive or i need to get you know an improvement proposal salon improvement proposal ethereum proposal push through the core developer community there's reasons why that's structurally hard to do uh the main one is the fact that like a lot of people use these systems and we can't just try random crazy ideas with low probability of success because people's livelihood depend on these systems actually being resilient right we don't want to break them so we want to be a little bit conservative with how we iterate on these designs because everything could because again they're tightly coupled and so one change for instance in the execution could screw up consensus and maybe performance improvements in one part of the system can cause performance degradation elsewhere right so structurally these things are slow moving but we still want this capability to be able to run a bunch of experiments kind of a little bit more similar to what's happening at the smart contract layer and i think we would benefit a lot as an ecosystem with a lot of parallel innovation happening in these systems okay so i guess my point here is that modular blockchains enable this uniquely by splitting up execution from data availability and consensus it is now easier for for me to launch a new execution layer should i choose to right so if i have an idea maybe i want to enable access lists in the evm to enable parallel uh reads and writes from storage uh maybe a lot of people disagree with this decision but i want to run the experiment i now don't have to fork the entire da and consensus system i can simply launch a parallel execution environment now it's still a little bit harder than launching a new smart contract i need users to validate i need nodes and so forth to be running on this and for instance if i need to build my own sequencers then i need to recruit a validator network it's maybe not worth it if i just want to kill the self-destruct instruction in the evm that everybody hates but it might be worth it again for something like parallelization which is just an interesting experiment to run right if i happen to be right users will use my chain they'll validate my chain they'll run nodes on it they'll liquidity will migrate to it and if i'm not nobody will do that but the cost of experimentation have been brought down materially relative to launching a new layer one from scratch it's easier to launch layer two it's easier to launch a solver and modular execution environment in these contexts the other thing is that as a developer of an execution environment i only need to optimize for the precise use case that i care about somebody else is handling da for me somebody else that might be handling consensus i need to think about who my users are who the developers on my execution environment are likely to be and how do i best tailor to their needs right similarly for vda layer the a layers can specialize and focus on providing the best possible experience for people building execution layers on top of them right so what does this imply it implies that at the same time we're able to run a whole bunch of experiments on execution layers that means we could get execution layers that are more performant for general purpose applications maybe special purpose execution environments for different use cases my colleague wei from bcc has a proposal to add an additional instruction to the evm to process sapling circuits and enable anonymous transactions and composable defy anonymously there's all kinds of different things we can try some of them are going to fail some of them are going to be terrible some of them are going to be centralized scam execution environments that claim to solve everything without any trade-offs uh the point is it's going to be really fun a lot of people will try many things uh maybe your nephew will shield you execution environments uh at your next family reunion maybe you yourself will build an execution environment and if you do we hope you call us thank you very much pause enough for questions [Applause] do you do they need to get your mic or yeah there we go beautiful on the note of orthogonal resource pricing if you have an l1 that supports on the central planning when it comes to essentially creating efficient markets to maximize resource allocation for l2s is it the case that you can get quite a lot of benefits when you have multi-dimensional eip 1559 or you reduce central planning and you say like have individual gap code operations um to be free floating or is that only a half-hearted solution and you really need to have complete orthogonal uh separation of orthogonality on the different layers versus to allow a free market for the usage of layer one resources by layer twos um i think both of those achieve a similar thing right so like if if you are able to price resources separately in every instance and they you have fixed absolute prices that are given by supply and demand for scarce resources then you've accomplished this goal it's just a simply much easier goal to accomplish in a modular architecture because in a monolithic architecture there's other concerns to doing that right and even eip-1559 is not trade-off free right um and i'm not saying it's trade-off free in the modular world either it's just it's a different way of exploring what those trade-offs might be by just running a bunch of experiments in parallel right it's a great question and then quick follow-up in terms of the trade-off between having doubt chains or very specific l1 for applications with a limited composability and then the other sense of having a extreme gas limit universal l1 for instance that everything is operating on with limitless composability or far fewer constraints there do you foresee there would be multiple optimal points rather than say a single optima and then as time evolves and these interact would you imagine it would converge to say a bifurcation or a single optimum point or do you imagine say there would be benefit for extremely low gas limit l1 to live alongside say for instance very secure bridging in adapt chain environment right i think we're likely to see that i mean there's going to be different optimal points for different applications right by definition some will choose to co-locate some will choose for instance to break off um and then for instance they could break off and then start to build other applications on the same chain because there happens to be a lot of liquidity there and then these these optimal points are not fixed in time like are not fixed across time right there might be something that works now and there's a whole bunch of users tomorrow and there's an optimal point in the future it's just the experiments are running in parallel we're already starting to see this right there's different uh roll-ups with that are exploring different trade-offs there's different ones with different trade-offs there's application specific l1 this is the world that we live in right and we're starting to see increasingly like what types of applications might work in one environment versus another we haven't fully explored that space yet that is for all of us here to do awesome any other questions nope oh so there we go all right these are pretty evenly distributed across the room i know right to make you exercise here you go hey um could you you've spoken a lot about the technical benefits of a modular blockchain architecture but if i were to build my own l1 one of the things that i want to do is hold value in my token and for something that's going to provide me this data availability layer to replace the need for my consensus has to be paid for do you see that as being something where we can have a lot of different tokens something like terror where you get fees paid in all the manner of tokens that are being used on the network or is that something you see um more in the kind of ethereum space where you're paying an eth to write back to the chain um there's a couple questions in there just to make sure i understand it right so one is application developers or roll-up developers wanting to have their own asset but if there turns out to be some use for that asset then presumably it has a reason to exist maybe it does sequencing or so forth that you care about there's some property to sequencing that you care about if not then you know might not have a reason to exist and so you know these these are the debates we have on ethereum all the time right with different applications anyway and then in terms of for regular users i expect like so for instance if there's a rollup that has its own sequencer or you pay in that native asset then you just outsource it to the validators right to go pay uh celestia or ethereum or any one of these data availability solutions right so or some some of these operators have users paying whatever token they want right there's different trade-offs that have to do with ux i suppose my personal bias um obviously different people have generalized like to abstract some of the complexity away from users as they're transacting think having to think about 50 tokens and the relative prices of each other is just not a fantastic experience does that answer your question yeah it does thank you i think first yeah as you say abstracting away the complexity okay you use our l1 token but it's not an l1 it's it's actually provided by the data availability layer and then we handle the the costs and the gassing on on the underlying consensus layer through the validators right that makes sense yeah i didn't get that you could do that on the validated layer because obviously they're a bit more smart than uh than i initially thought so thank you any other questions i think you're done thank you thanks alex yeah [Applause] all right oh you're already up next up we have zucky co-founder of many projects also one of the founding fathers of cosmos i'll let him take him away take it away he's going to talk about the modular past and future of cosmos cool doesn't seem like it's working is it working now cool awesome sweet here we go all right so uh what does cosmos have to do with celestia um this i think is the hardest question to answer because no one knows what cosmos is and cosmos is my best way of describing cosmos is cosmos is the emergent properties of we going to die are we going to die imminently cool um cosmos is is sort of this emergent bottom up uh uh effort or like emergent bottom-up system that like emerged out of like a bunch of work um on building blockchains uh in this modular way that started in 2014 um and so uh you know i kind of uh i'm frequently the like face of explaining the system architecture to the world but like uh credit to jay for like basically uh foreseeing like a lot of the problems that like mustafa and alex just talked about which was we want to run a lot of experiments uh we want infrastructure to enable running a lot of these experiments um back in the early days we were extremely capital constrained um you know how do you like we we could you could kind of understand the scope of the problem of how do you build like a global uh financial system but like you know it was just like a bunch of people who were like you know it was like small groups of people who were coding we had very little money um with very little resources and to build all this stuff how could you actually achieve uh the system uh another sort of sort of anecdote um just about like how this is all destiny um it's all it was all destined it was all foretold um the original idea uh that jay and ethan came to me with uh was which was their idea for a public blockchain was called super tanker supertanker was basically the idea of have like one tendermint chain where you write the block headers from other tendermint chains too and then you would do fraud proofs if those those tenderness chains uh uh if those validator sets for those other tendermint chains like uh produce an invalid state transition so doesn't this sound like awfully familiar um but so my objection back then this is this is 2016. um my objection to the supertanker design was we don't have a solution to data availability that scales um like tendermint blocks can only be so big um we can't write like the transactions for hundreds or thousands of blocks uh to the tenement blockchain like we should do something else um and so i kind of like was very much pushing a bunch of other ideas about how you would get sovereign interoperability that i found in like mark miller's work from the 90s uh and that's how we sort of got to the the architecture that we sort of launched in 2021 um we had no idea uh that it would take that long but you know uh like there were many hijinks and adventures along the way so cosmos though has been this like uh so like mustafa mentioned uh abci um abci is sort of a cosmos's modularity i'll get to a little bit of the origin story of that later but largely i would say the modularity that we built into the system has had a number of big success stories um so uh there are a number of chains that have used tendermint uh to augment geth um going back all the way back to 2016 uh reca uh maybe 2015 where like kobe shows up in the tendermint slack he's like i have guest running on top of tendermint let's call it ether mint um and then like you know all of this code was open source a lot of people ran with it we got and you get things like polygon and binance which are sort of elaborations on this original idea of augmenting death with tendermint um you know the sort of team the cosmos the tendermint team like focused on the cosmos sdk and we've had a bunch of big block chains now that like have billions of dollars of uh of value that use the cosmos sdk chain the sort of raw cosmos sdk and interoperate with the whole cosmos sdk ecosystem of wallets and block explorers and all these pieces and you get this like shared component architecture around the cosmos sdk with strong network effects so like terra luna uh osmosis atom uh that all works um then we also have this like explosion of uh smart contract vms that have emerged in sort of this ecosystem so like you have agorik evmos cosmosome uh and probably there's going to be more it's very exciting um and then you have people who have built like entire alt alternate application frameworks like oasis and penumbra and enoma and nomic so this to me is like hey like this is the this is the shining example of like sort of the social and like adoption benefits and how sort of making these choices to build tendermint through this abci interface as consensus separated from application has like yielded like enormous benefits uh to our entire industry um and so this is sort of to me like why modularism has been good um why i am really happy with like how we went about building tendermint and cosmos because i think these all of these things have been massive wins so our sort of layers of modularity are abci and abc i plus us application blockchain interface so there's when you build on top of tendermint you get this like list that's probably about a page of apis that you have to provide in your application and it's it's like sort of obvious things it's like okay like begin a block so like what should the application do and when a block is commit like when a block comes in uh deliver all the tx's so execute all the transactions uh end the block uh commit the block uh these are like the sort of standard apis it's pretty simple um you see you uh they're like if you like look under the hood of many other blockchain stacks you see like the same design patterns where like basically the same application consensus interface but what was what was unique about the tendermint approach was that we took this approach of uh sort of standardizing that api it hasn't changed that much since like 2015 2016 though i plus plus is a new thing that's coming out of the tendermint team um to sort of enable all of the things that we didn't foresee um and like you know hats off to dave ojo for like sort of like really extending the api in a general way um but this is like enabled things like optiment and celestia to use like uh uh abc in this really powerful way but like all of those people on the all those chains uh uh on the previous slide were are all abci users um the cosmos sdk has like also this modular interface where you can extend it with various modules um this has allowed like very diverse use cases like thor chain uh axillar like all of these things like really have like very different systems interfacing throughout this module architecture we also built ibc the inter like the sort of built-in bridging protocol to the cosmos and tendermint ecosystem in a modular way where you could actually pull out uh where you can actually extend the uh the client interface so like the transport layer like how do you authenticate like a message coming from which blockchain so we actually could have multiple security models um and then we've always had like support for many different signing algorithms and many different sort of cryptographic primitives uh inside of the cosmos sdk has also been a huge enabler and sort of uh innovation and integration with different ecosystems like you see all of these things coming out of the evmos ecosystem uh that support like the ethereum signing out algorithm and like people have tried with uh uh to build in support for like polka dot signature system as well okay so these are all sounds really great um but we've been doing this a long time and obviously hasn't all been beautiful um and and perfect um so tendermint itself uh is a bit of a monolith it has like excessive interdependency between things like consensus and peer-to-peer layers um and the number of teams that have tried to build experiments sort of like within that system have failed because of just like sort of the overwhelming complexity of that uh task we have a lot of like large-scale users that are also like sitting on ancient versions of the system um you know like the uh the version of the cosmos sdk that polygon is using for their tendermint chain um is i think from 2019 um it's it's kind of terrible um we've also seen like a number of teams like kind of mostly binance like as the biggest one like take tendermint make a bunch of changes to it and then never open source or upstream their changes um phantom built uh there was like the first so like the dream one of the dreams of of of cosmos was not only could you have many applications built on top of tendermint but you could have many consensus implementations because we didn't we don't believe that tendermint is like the end goal and state of consensus could also implement abci and phantom was the first blockchain to do this but it never really shipped and it's sort of sitting on the shelf um and then uh we basically abandoned the idea though there's some community members are trying to keep it alive of like sort of like making the uh sort of like uh local database of tendermint plugable um because modularity frequently imposes like a lot of sort of uh maintenance cost on the core team okay so how did this all start um so tendermint's modularity came about because of an ip dispute um uh there was one set of tendermint contributors who really wanted to gpl the entire software seat and another set and jay wanted uh uh t apache licensed it um and uh in hindsight i think apache licensing tendermint was like a huge reason why the modular systems uh uh like why this has become such an important modularity layer and it's like one of the things that like worries me about sort of the modular blockchain stack is like we're playing too many games with like the licensing uh and the ip system and it's going to hold back innovation as people like do like business source licensing and stuff like that i think getting tendermint to be apache2 license was like a huge huge uh uh innovation enabler especially in those early days of blockchain so what we did was like jay came up with abci as a way of separating the gpl components from what he wanted to apache license uh and this is how we got the modular system that sort of formative thing sort of turned into like the philosophy of cosmos which is like sort of this like extremely modular uh system architecture um that i think has has really proved its worth so this is the this is the start of modularity in cosmos uh what's the future hold um so um abci plus plus is this very composable thing or this very powerful addition to like the modularity system of tendermint um you can do a lot of things you can do mev mitigations uh i think one of the things that you've you see from like for instance starkware has this proposal about using tendermint to like decentralize the l2 sequencer um and like you can use abc i plus plus to require like a set of sequencers to agree on external data availability before producing like another uh another state update um and so like we are trying we have like we sort of uh somewhat intuitively somewhat informed by all of these emerging requirements as like sort of the modular stack evolves and we want to meet the needs of the modular sort of builders of modular blockchains we are somewhat we are like building these functionalities sort of in the core tendermint to like sort of enable people to do this without forking tendermint um one of the other questions that we've been thinking a lot about um and there's been a lot of conversations is can we extend ibc itself to be sort of fit within the roll-up uh uh paradigm um ibc is the sort of like sort of tr is like it's certain currently like somewhat trustless in the sense of hey like we have this thing where you where the packets are authenticated by the validator set of the uh of the sending chain so like if you already trust those validators which you needed to because you were using that blockchain there's no additional trust assumptions but we could build um and we've sort of always wondered about like approaches to building like actual state machine verification so you actually could move away from trusting the majority of like a minority of those validators to just being able to say hey like the bridge itself authenticates or has fraud proofs and all of this stuff actually composes very nicely into like sort of the overall design of ibc um and i'm very hopeful that like one of the things that emerges out of the celestial work is sort of like an ibc ecosystem that does um sort of trustless settlement or like state machine verific verifiable settlement um but across like in this sort of like uh multi-polar ibc world rather than in this like sort of like hierarchical like settlement layer up world um we've been you know going back to the super tanker story we've been thinking about fraud proofs for the uh cosmos sdk also like if you kind of uh if you go back to the the historical split between polkadot and cosmos uh it was like we wanted to do like interoperable high multi-channel blockchains and gavin was like gavin and rob were like yeah we want to do all that too but we want fraud proofs um though they didn't end up actually doing fraud proofs um uh but we actually i think we have like a relatively good architecture for like adding fraud proof support to the cosmos sdk um and that'll allow it to sort of fit start like you to build applications in the cosmos sdk um actually i then so open questions um we still really haven't like i've been like working in the zero knowledge proof ecosystem um for many many years but we still haven't been able to figure out exactly where zero knowledge proofs like fit in the cosmos ecosystem um and i still think that's like an open question it like it continues to be like a sort of uh a constant error of discussion i like talked to the mina team about using tendermint for data availability we talked to the star ward people about using tendermint for uh sequence or decentralization it seems like there's some piece that like tendermint is supposed to plug into like the zero knowledge ecosystem but we still haven't actually figured that out yet um so a little bit of predictions about the future um modular and monolithic blockchains are going to coexist for a very long time um there are still like they're they're uh uh there is a kind of uh uh uh purity to being able to like reimagine your entire system from the ground up um and uh there are things that you that i don't know how to build in the modular paradigm that i still think are useful in the blockchain space um like um but modular systems will outperform on decentralization i think like one of the coolest things about cosmos is the sheer number of teams that contribute at every layer of the stack um and it is going you know what you will see what i expect to see with celestia is now you will start seeing teams that are building settlement layers and roll-up sdks and like zero knowledge proof you know execution environments they're all built on top of celestia but they're also going to contribute to like the core data availability layer they're all also going to find bugs they're going to improve the like uh because like as you build these module architectures it's very friendly to sort of modular organizational structure um because you can have different organizations like own different parts of the stack they kind of can like uh uh and while you also have incentives because you're building it one layer to like fix problems or improve performance of other layers of the systems um finally like i have a project called similier and sommelier is to be like the best example of a thing that doesn't work in the modular environment like it is a blockchain that is sort of coordinating defy strategy execution on other blockchains and really the only way to do this is like if you own consensus up in like sort of your core application stack um and so this would be my like counter argument to like the the the modular blockchain thesis is going to rule them all um there's still going to be a place for monoliths uh i certainly have a lot of experience building them uh and uh that will be that um yeah and the the other thing is i think stuff like uh sort of like the direction that like many of mustafa's co-founders on uh on chains uh chain space uh have kind of gone down with like uh uh mistin and swee is another great example of like how you are building like highly uh like very interesting monolithic systems are still to be built so thank you very much questions you and your questions take some questions yeah any questions can't see all right time for my walk excuse me we go can you come on on the use of optimum with the celestia stack in terms of modularization yes so optiment is an alternate abci provider so like tendermint is like the normal abci provider an optimum is an alternate abci provider and the expectation is like the way i think it's going to work is we are going to add new apis to abci on optimant that add what's necessary for fraud proofs in the cosmos sdk and then we'll add them on the cosmos sdk and then the dream is that eventually gets upstreamed into like sort of the tendermint provider um and so we continue to have like one abci but now we have these like fraud proof mechanisms um the other thing that like optimum is i think you know we need to figure out is exactly how we're gonna um because right now leader election is owned inside a like one of the questions i think is about this is um entenderment leader election is owned inside of tendermint it's not owned inside of the application um and we either need to change abci to like move that over or optimate will just have like a standard uh a leader election system for rollups and like sequencers on top of celestia does that make any sense and was that yeah thanks yeah thanks anything else guys any other questions ah-ha all right so yeah oh there you go uh thanks for the presentation my question is do you think that in the future monolithic uh chains will be more of a niche product that solves like let's say only one use case and if it is the follow-up question is how individual you think model exchange can scale um so there is sort of the reality of the situation is is like there's plenty of room to scale monolithic chains um like it you do have to like exponentially increase the amount of resources that are involved in like sort of the core engineering um but like there's plenty to uh uh there's plenty to do in the uh in sort of monolithic scaling to get like you know an order of magnitude towards magnitude um throughput the the real reason why i think um modular winds and the or like modular is sort of so like fundamentally important and like is basically like the future of finance layer um is because what we need is we need the infrastructure to become public goods um we need to like disperse the knowledge of how to maintain and build and improve and bug fix this to a very large community you need lots of contributors and really the only two systems that i think that have ever done this are like the tendermint ecosystem and the ethereum ecosystem um everything else like the the the knowledge of like how to maintain the core system how to improve the core system how to change the core system is really like siloed in a very specific group of people typically a company um and like the modular architecture is the only reason way to do that and that's the only way we're going to build a system that is like fundamentally different from like the legacy finance systems that we're trying to replace um and so my my general bet would be like you may see monolithic systems really succeeding in like gaming uh like you know uh gaming nfts like all of these other uh entertainment more entertainment applications but i do think that the ethereum sort of team and this you know is is very right and they're sort of directionally right that like if you want to build the future of finance um modular is the only way thank you questions no no right behind you where ah go ahead um how trivial or non-trivial would it be for a monolithic blockchain in the future to evolve into part of a modular one as sort of a a roll-up um so like alex was basically trying suggesting that it was it is like more of a content it is like more of a continuum than like an absolute and there's an extent to which that's true um but like i really think about these things as the social process of building them as being like is like what over determines like the future technical direction rather than like what the actual constraints of the software because like and so what i what i've yet to see is ever someone a project that started out building like sort of a monolithic architecture ever ending up with the social organization to do a modular to like build modular um they they just don't they don't know how to run a multi-stakeholder process right uh if you like the success of a of a of a monolithic blockchain comes from doing the opposite like not running a multi-stakeholder process being like we are going to build a blockchain we are going to make these technical trade-offs like if these don't work for you screw you um that is the that is the way in which you successfully build a monolithic blockchain if you want to build a module or blockchain you have to run this like multi-stakeholder multi-entity process um which is very messy and it's like harder to like figure out what's going on you cut it requires a lot of engagement we see it in cosmos we see it in ethereum like happening in real life um and uh that is a very difficult dif uh difficult social shift um so i think if innovations from the monolithic blockchain world actually make it into the module blockchain world it's mostly going to be like forking the code and like rebuilding the social infrastructure rather than like people like the monolithic people being like okay we like have gotten to the end of our monolithic story and we now are going to build a modular system thanks very much thanks all right anything else no questions ah okay there you go that was really interesting a really interesting talk i wanted to ask you um you mentioned that there are some features or functionality in monolithic blockchains that you think would be or you don't know how you would build in a modular blockchain so i was interested in like like what are the most challenging things to build in a modular system that currently exists in a monolithic um so what are the things that make some of these things really hard um one is where you want like so basically so there's like like the story basically comes down to situations where you can like tightly couple the properties of consensus with the properties of the application so if you can like you want to change the rules of block production uh in a very specific way that's like about like observing external systems like what you what you need is like sort of monolithic total control you want to change uh you want to have encrypted transactions that like uh uh that are decryptable you want to uh uh you want to be able to control your scheduling in such a way that like you can execute that you uh uh like react to and execute to events like autonomously from other blockchains uh all of this kind of like tightly bound uh uh integration between consensus and the application still represents something that's like very hard to build in my mind in the modular ecosystem all right i think that's it cool thanks ducky all right we're going to be taking a break until about 11 20 um which is when we'll hear from ori from starkware on l1s l2s and l3s so thank you all [Music] 10. [Music] me [Music] hello [Music] [Music] [Music] i'm last time [Music] my [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] left [Music] but [Music] oh [Music] so [Music] [Music] [Music] uh [Music] uh [Music] [Music] [Music] [Applause] wow [Music] so [Applause] [Music] foreign [Music] [Music] [Music] [Music] [Music] [Music] [Music] so [Music] foreign [Music] [Music] [Music] [Music] me [Music] foreign [Music] [Music] um [Music] [Music] [Music] time baby [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] this [Music] left [Music] foreign [Music] foreign hey all reminder we're starting in 10 minutes [Music] oh [Music] [Music] [Music] [Music] all right guys we'll start at 11 25 please make your way back into the atrium and get seated thanks [Music] uh [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Applause] [Music] [Music] [Music] [Applause] five minutes left to the break please make your way back into the atrium we'll start at 11 25 with uri [Music] [Music] [Music] [Music] [Music] [Music] hmm [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] thank you [Music] two minutes left guys we're gonna start real soon [Music] [Music] [Music] so [Music] [Music] [Music] all right hope everyone is caffeinated and ready to pick up again cool all right um next up we have a packed schedule until lunch which is at 1pm i'm going to kick it off to uri who is the ceo and co-founder of starkware to talk about l1s l2s and l3s uh mike can you hear me do i need this pacemaker does it make me look slimmer so uh um three tokens to the gun so okay so um so from layer one to layer two and on to layer three hi um so so so first there was nothing right um we were all wondering aimlessly and then satoshi showed up and um some people were doing some useful stuff but not many and then ethereum showed up a few days a few years later and with this beautiful concept of for the first time allowing general computation on on a public ledger and that was very exciting for a lot of people and this provided two massive benefits and delivered as promised unlike many many other projects a truly decentralized network um and a secure one what's going this thing is messing with me um okay so can can can you hear me now there we go so i i want to donate this to science thank you um okay so so delivered on security and decentralization so that's good um it failed massively to deliver on scale and then you know the years 2022 and uh repeating the same numbers and 15 tps and the the whole moving from 10 uh uh 10 million gas per block to 12 and a half to 15 the sort of the gradual erosion there around decentralization and the uh high-tech efforts involved in launching an evm compatible chain with 150 million gas per block or one and a half billion or 50 inch mazillion and you know but scale wasn't provided so layer twos were born and so layer two was introduced as this new concept that conceptually says let's try and solve scale and of course the base layer or ethereum was retrospectively renamed layer one and uh we came on the scene in the middle of 2020 our first mainnet deployment with starkx which is a scaling service it's a sas business that supports a bunch of use cases and that does solve scale we'll give a few examples in a second we started out with uh validium where data is stored off chain and this is very pertinent an event hosted by celestia we think that data availability is a massively interesting design space and we'll talk about that in a second but we launched starcx in uh validity mode and uh in valydia mode and we launched starkx in rollup mode one is with off chain data with a data availability committee the other is with uh onshade on-chain data and as of this week we have a new mode called volition where the choice whether data resides on chain or off chain is actually made by the user not the application at the single transaction level okay so the nice thing about these dark x deployments is that they you know they actually achieved scale to give you some numbers starcx over the past year has settled over 150 million transactions on ethereum uh over half a trillion dollars in terms of scale we've reduced say dydx's gas per transaction from 250 000 300 000 gas per transaction to 480 gas so a 700 x reduction these we're talking about tens of millions of transactions on ethereum mainnet this isn't uh this isn't crypto twitter and this isn't some testbed and um and for so rare and immutable for example we reduce the cost of minting nfts from about 200 000 gas per minute to less than 10 gas per minute so a 20 000 x reduction so scale you know that was demonstrated in a very sort of powerful way but a few things were sort of uh not achieved in the process so first of all this isn't general purpose star cache star kick supports a bunch of use cases out of the box so payments and transfers spa trading minting and trading of nfts perpetual contracts now a new use case called defy pulling they're actually folks in the crowd here building uh a business around that but not general purpose not composable you could think of these dark x instances as these islands run by these businesses dydx so rare immutable no composability so this beautiful vision of composability was not achieved and of course not permissionless you have to sign a contract with us uh there's an actual legal document in place uh so all these things were not achieved now for starkx we developed a programming language called cairo there were about five druids on the planet who could take a given computational statement and translate that to a set of polynomials all of them on the starkware payroll guess what they were terrified of their own occupation because when you said how about we modify the business logic from this to that they said well let's not do that anytime soon okay so or maybe the other guy can do that i don't want to touch the uranium and the core of this nuclear reactor and you know it's just too dangerous we need a programming language we need to turn this from sorcery into just plain good old programming and that's when cairo was born and starkix is powered by cairo now the minute we had cairo and we said this thing is becoming increasingly higher level increasingly more uh uh secure just by merit of of sort of being battle hardened uh the obvious idea was to say well let's externalize this to everyone everywhere and so starknet was born we announced this in january of last year the public test net went live in june of last year and the alpha went live on ethereum mainnet in november and it's getting a fair bit of uh of attention in the ecosystem these days and this does give it gives general purpose it has composability and how it well right now much like our our friends at uh arbitrary optimism the first with the proverbial crutches the the first phase of the network is in fact permissioned we want to make sure that the whole thing doesn't fall apart but the intent is within a few months time to make it truly permissionless the public test it by the way is permissionless for both deployment of contracts and sending transactions to the network um so all these things were achieved but still stuff was missing okay what was missing um so first of all scale is never the thing where you know no one ever said okay that's perfect that's enough skill for me thank you very much right bandwidth being sort of the obvious example but beyond that i think it's control and privacy being one example of control so how how how did this whole thing sort of evolve in our minds how did this realization evolve in our minds so often you know when you the narrative of these things you come in and you say well we thought about a and then we realized b and then we you know said c is really a forced move and reality is far messier than that and the reality for us was that a particular project came and said we want to do uh derivative trading we're in exchange and we said that's terrific uh you know the the you know we can build for you something like what we built for dydx perpetual trading perpetual contracts and they said no we want every form of you know all sorts of derivatives and we said well that's terrific go do whatever you want on startnet you know you don't need our permission for anything and they said well the problem is that uh we're an honest honest-to-god sort of regular exchange and there are legal contracts god forbid with banks on the other side and those are denominated in that pesky fiat called us dollars and we can't have revenue coming in in dollars and costs fluctuating with uh gas prices they said if this network of yours is anything as successful as you expect it to be you know there's the you haven't sort of invented new physics this is going to supply and demand is going to dictate the prices of gas here and we may not have a business and so at that point we said well how about you run your own instance of starknet okay so think of this as a walled garden okay and the beautiful thing about these metaphors is the minute you have the metaphor and you start using it it becomes a very sort of relatable way to sort of process these ideas so we started talking to all sorts of folks about this and it turns out that this concept resonates in a very powerful way with web 2 companies and web 2 companies understand that something massive is happening okay but this kumbaya view of uh and excuse the kumbaya folks here above you know everyone going full unit swap and it's not going to happen it's not going to happen not because we don't want it to happen it's it's just not the way of the world there's companies exist for a reason and and economic forces are in play and all these things will continue to exist so control for companies is something that they need they need this because the regulator requires them or their business partner or their users um and uh and so that's where layer three was uh was born now the one thing we can immediately do in terms of layer three is move the stark x instances up there so what does that even mean so stark x much like the public stark net the transactions process there we create proofs and those proofs are verified on ethereum on layer one if instead of verifying those proofs on ethereum you verify those on the public's darknet and layer two conceptually you could say well this thing just moved one layer up okay and that's exactly what we intend to do this will be in production in a short number of months uh the the poc in-house is already done um and so that's one very interesting thing but just like the the anecdote i told you you know about the sort of the birth of this idea uh this will also allow for private stark nets to be born and private stark nets will exist for single companies or for all sorts of consortia of all sorts and and we expect a lot of experimentation to go on there because say in the context of data availability the stuff that celestia is working on this is the space where you can you can knock yourself out you can move faster than the public chain you can move slower the public chain you can do all sorts of alternative stuff without building sort of the broader uh consensus that is needed for something like eap 1559 and we we firmly believe that that kind of experimentation is actually critical for this ecosystem to uh to succeed okay in my opinion if that doesn't happen then this this whole experiment will fade into the haze so you know of course you can have multiple instances of these things um as many as you want you would have you know all these desired properties you can of course recursively apply this and say you know let's go higher up if you want there are of course trade-offs right the final settlement on the base layer gets delayed the further up the stack you move and the final step is to basically say well just like we're settling on ethereum just like there are trade-offs in the context of data availability for different applications so on and so forth there are going to be trade-offs in the context of the base layer not everyone and everything needs security the security offered by ethereum and we think that other layer ones are going to eventually adopt these uh tools i think that the basic uh the the solana approach of building more lanes is um well just like we see with highways that sort of typically ends in tears and so they need proper scaling solutions in other layer ones as well and i think that's sort of the way things will unfold so this is our view sort of our story where we came from how layer two was born and how we're sort of moving to building out these layer threes in the uh in the coming months and years uh that's it i'm happy to answer any questions any questions ah-ha all right take it away you very much for your presentation uh at which layer does it stop how can you go um it there is no so what does it mean a layer three in this context it means that i prove a computation that takes place at layer three and instead of verifying that proof on in by a verifier smart contract on ethereum layer one i verified in a smart contract that's deployed on the public stark net now you can build this as high as you want um you know at some point the question is why are you doing this sir but but do you then think that layer 3 3 is sort of the general optimization point for layers or it's it's i say i say this with a fair bit of caution and these things go in waves right so zaki a few years ago he described basically this notion of you know these different ecosystems have different names but zaki in 2018 i was new to this space this view of of these autonomous environments and the need for those you know he articulated that several years ago and then sort of everyone went sort of fully unit swap composability is the only thing that's the only and i you know when we started servicing dydx some of our investors who actually became investors of dydexslater said the walled garden once again that metaphor that's that's so 1990. that's gone there are no walls and it turns out there are walls and sometimes people you know want them at different heights um but that's sort of it this sounds like a human needs uh the metaphor i heard uh from one of the web 2 they refer to this as noisy neighbors i thought that was a cute uh expression they don't want noisy neighbors thank you yup i see you hold on from l1 to l3 what do you think is the most under appreciated impactful economic force what is the most underappreciated economic force something that might be decisive so say like in kosa's theory of the firm how he identifies transaction costs as leading to large companies and small companies in the dynamics there and why everything might not be decided on an open market you might want to internalize transaction costs so for instance do you think there's something other than transaction costs such as privacy mev that might be decisive in the l1 to l3 landscape or the possibility of higher level roll-ups that we're not quite paying attention to yeah i mean i i i think that the the the the basic statement around uh this will all exist on a public chain first it was layer one then it was this will all exist on a layer two period no modifiers or qualifiers about it uh and people will say well there is a way to achieve privacy you can do true zk roll-ups you know like our friends or at-stick have already implemented yes you can that is true zk privacy but in many use cases there are all sorts of other grades that are of interest and value and far simpler and computationally more efficient etc and and those that level of control control is the word uh is is offered in layer three now how this will unfold in terms of additional layers i have no i don't know beautiful thanks any other questions hi so for projects that are considering deploying on their own private stark net or on the public stark net um beyond just having you know the scalability primary reason that you discussed what are the specifications and factors where you would push a project towards one or another so um i'd like to think that we don't push them meaning meaning that that we're truly agnostic and i think the intent is to build a software stack that that turns that into reality meaning it would be perfectly fine for you to spin up your own private start net and then if one day because you've concluded that composability for your business is in fact not terribly important interoperability is good enough you don't need the synchronous calls the async stuff is fine and in fact you have a beachhead you exist and there are three but you have a beachhead which is your smart contract on there too for the stuff that you do want to be composable and then your business changes and you say well i actually need to be part of the public network here and um you basically plug the logic everything is implemented in stargate contracts you can at the switch of a button uh become part of the public network so so the the intent is to to reduce those switching costs to the to the greatest extent possible hey thanks for the talk um could you talk a little more about recursive rollups and hyperscale when it comes to l2 and the limitations that exist sure so um so recursive roll-ups are actually intimately tied to this uh concept and i'll explain uh the concept of layer three and i'll explain why um right now for example we're minting batches of six hundred thousand nfts in a single proof which is a fair number of uh of nfts but that requires reasonably powerful machines in the cloud to do so and that is of course conceptually you know this is sort of goes against decentralization you'd like to sort of move to lower requirement on hardware you'd like to cut down latency right when i when i batch 600 000 transactions two things happen a it takes me time to batch 600 000 transactions two transactions came in and another two it's going to take a while for 600 000 to accumulate okay that's one thing the other is that once i have 600 000 transactions proving that is a very big computation so generating that proof takes a long time all this builds up adds up to significant latency which affects the ux so for recursive proofs what you could do is you could take let's say a hundred thousand transactions instead of proving a hundred thousand transactions in one proof i'd say let's split it up into a hundred batches of a thousand transactions and so we now can create in parallel and the emphasis here is an in parallel by much smaller machines um a hundred proofs and these proofs from a computational they prove the same computational statement i verified the transactions in this batch they're all they all abide by the logic that we want to enforce here now you have a hundred proofs and now is the recursive step okay now is when you say well let me prove something else to you i'll prove something that actually has nothing to do with the application specific logic i'll prove to you that i have verified 100 stark proofs for these 100 uh proofs that i've generated okay and now i have a single sort of masterproof and of course you can do this in multiple steps so this is how recursion would work and this is something that we intend to deploy very very soon this is my stand-up meeting in israel now so a reminder so um yeah and so this ability is of course needed in order to support uh the verification of proofs generated at layer three on the public starting yeah so these things sort of go hand in hand yeah thanks that'll be the last question by the way we have to move on to the panel thanks um so in your diagram you've got l3's as sort of the the hyper scale point but um to the question you answered just earlier um if composability sort of continues to be a thing very valuable more of these private statements start to move on to l2 do you think you will do you think that's a concern in the long run that you will ultimately need to make the l2 the public start net sort of ultimately have be the place for all the throughput instead of effectively having sort of inverse upside down pyramid where you've got slow ethereum then public stock and then hyperscale at the top with that middle layer become really fat fat and what's yeah like i i i i i think i had until just the very end the subtext i don't like it oh oh oh but i mean how about this heavy-bodied right an asterisk yeah he's referred to as heavy-bodied so um i'm not sure i understand the cons fat in what sense as in so your burden with a lot of computation correct because of the desirability of composability at the l2 so i i don't know what the equilibrium will will end up being so for i for one i i think we at starkware are sort of of the opinion that ethereum eventually will be the place where proofs are verified period that everyone else will be priced out of the market like the the actual transactions that they want to conduct there now whether there will end up being a public stark net whose purpose is to support the layer threes versus a public start net which is sort of geared at composability i don't know we'll see thank you all right thank you all right thank you very much everyone cool thank you all right next up we're going to have a panel on scaling zero knowledge execution layers moderated by zucky with alex from zk sync louis from starkware and toggle from scroll all right sweet hey guys how's it going hey how's it going cool we can i think we can sit down [Applause] cool um so i've known two of these people for many years but tomorrow it's really nice to meet you nice to meet you yeah uh welcome to the uh back to the conference circuit you know how's it going um all right why don't we start out with like introductions um who are you and a little bit about uh you know zk sink and scroll and uh i think erie gave a good introduction to starknet so you're happy to start my name is alex kulhovsky i'm ceo and co-founder of matter labs which is the company behind ziki singh uh we have a uh ezekiel we specialize in your role up on mainnet now for payments and swaps but now we're working on the uh evm compatible uh zq roll up for uh the which is the version two of ezk sync which we have now live on testnet so it's the first zka vm live on testnet um i'm tohru maharamov i work for scroll i'm a senior researcher there so what we do is we're building an evm equivalent roll up a zk rolob which uses zkvm which is implemented by us an ethereum foundation as a collaborative effort and we're planning to launch the test nets so we're around the end of the year and hopefully the main it by next year so i'm louis i'm the ecosystem at stockwear i'm going to repeat still what my overlord uri said before so stockwear is a azure launch company we do a scanning through the usage of proof systems with philly starks and uh we have been you know um settling over 1.5 billion dollars oh wait much more 350 billion dollars on sorry was it yeah one or two out of magnitude um through the through our customers dydx mutable also air and diversify and we have been launching the um first production ready uh vk roll up in production on ethereum today so if you want to try it out have fun awesome yeah um okay so let's like kind of get into it um i i one of the questions that we were discussing you know in our in our chat is this question of decentralization and i think erie like sort of started this question with us you know startnet has a single sequencer how do each of you think about you know the sort of trajectory towards not just having scale but decentralization in sort of uh general purpose zero knowledge execution uh so maybe we can begin with the with the with the question what is decentralization why it's important so for sure absolutely sync uh we we are following the philosophy of uh bitcoin ethereum the decentralized public blockchains where decentralization is important to ensure that no one has complete control over the network there is no single point of failure there is no single point of leverage uh so we don't want the blockchain world to devolve into something that happened to internet which began as a decentralized network of networks but now it's essentially controlled to a very high degree by like five corporations uh because of the natural process of centralization of power of amassing of resources so we we need to prevent that and the the way to prevent that is to put the control of the system in the hands of the users and making sure that the the community always have the leverage over the developers of the protocol over the the operators of the protocol so like if something happens to ethereum uh and it's uh like let's say miners or like validators and proof of stake uh after the merge become malicious and and like have a malicious uh intention to control the network extract value from the users the community can always coordinate and fork away and the this for me this is the the very fundamental thing uh which we want to preserve in ezekiel sync [Music] by having permissionless software license by having the network governed by by the community but also supporting even though we hope it never will never happen uh ability for mass exits mass transfers to a fork of zika sink or to any other roll-up which will preserve which will keep this philosophy uh but it's also important to keep decentralization in in layer two itself which is why we're working on a decentralized consensus for inside layer two uh to to make sure that the transactions also remain sensitive i guess maybe it would be a bit better to start this question why is why is it particularly hard to uh descend now for 10 years why what what is the unique challenge that comes out of zero knowledge that like makes decentralization more of a challenge do you want to um yeah i guess i can take it um it's more that i mean there is no real challenge it's an injury challenge you know you know it's only engineering as like you know people say uh except that you know it takes time uh so there is we we're gonna make it right we're all gonna make it uh it's gonna we're gonna make it decentralized making open source everyone will be able to run it uh what's complicated though is that the you have less flexibility when you build an l2 than when you build an l1 um because as opposed to an l1 you know at the point of the day everyone is playing relay on the l0 and you can always roll back there's like you know you can do things and then l2 the problem that you have is that you have this bridge and when something is is a bridge valuable proof and you the money is out the money is out you don't control anything anymore and so you really the bridge is the most scary scary piece of software when it comes to l2s and for the funny story the reference paper and that no one think about it but the reference paper on l2's it was written by patrick mccoy and it's the name of it and i would think about it scaling scaling blockchains through no validating bridges as a scaling solution because at the end of the day everything is about the bridge and so the the now there is a bunch of challenges also with the proof system which is you know you need to have also you don't have only like executors and your sequencers you have like this proving and the spreadsheet has this latency and you don't want you want to make the sequencing as distributed as possible in the proving also and you have a bunch of challenges that exist elsewhere and so there is a bunch of interning challenges that still need to be solved and we just want to get it right so it takes time can i add uh so uh i think the problem with l2 specifically is that because we're already outsourcing the consensus to an l1 the goal of an l2 is to minimize the overhead which is consumed by the consensus so adding a consensus on an o2 doesn't really make a lot of sense because it just will add the same overhead that you already are trying to outsource to an l1 so the point is how do you decentralize in a way which also minimizes the overhead and it's quite difficult to achieve especially with zk roll ups because uh the prover efficiency is not great so it consumes quite a bit of computational uh energy to compute the validity proofs and uh we need a way to make sure that the system remains efficient but at the same time people can just join in and participate without an issue so yeah i just want to add we're here at the modular summit so the we should think about the decentralization also modular way there are multiple layers that can be decentralized in with regard to layer two uh the first is obviously the the bridge itself and like the the general ability to uh to control it to upgrade to exit uh and so on uh then there comes execution which is actually easier to decentralize than layer once we have less challenges because we can always tap into ethereum as a court of the final appeal to resolve disputes so our consensus mechanism is actually simpler than that the one required for layer one and we can also have uh some significant advantages we can reduce the number of validators because we don't depend with zero knowledge proofs on on them for security so we don't need and like for yeah we wouldn't need such a large number we can have delegated proof of stake so we can reduce latency a lot and we can have very fast confirmations for transactions in in layer two but then there comes also zero knowledge generation which is a layer on top of that which is uh which can be decoupled from the execution layer which can happen asynchronously in some time uh which is probably going to be like much lower than what we currently have like it's going down like under minutes like some seconds but it still needs to happen it needs to happen in a decentralized way we don't want to depend on a single provider on single cloud or in a single powerful player for generating the proofs not to have the supply chain as a point of failure well so do you imagine that there's going to be economic incentives eventually in these systems for generating proofs like how are we going to convince somebody to spend you know i think we're going to money i'm going to troll my colleagues here so we have a startnet research group internally where we discuss those things and you know this question about how you decentralize this as has been an open you know something i think we have ideas and direct direction but an open question for nine months to a point i'm i drop the meetings just for the joke but uh no i mean there will be obviously like incentive like there would be today we only pay the sequencer either through later elections report from work but um the the there will be the same kind of mechanism now you're perfectly right the challenge there is for instance if you did let's give you a random idea which is oh let's make a competition and then if you make a competition that's great it's like you know just the fastest gets to the network we're all happy whatever so the main issue you get with that is that it's so it basically brings down to centralization because the fastest guy the most efficient guy would always win and there is no incentive for the others to make it and so all of a sudden now you're relying on one or two single point of failure and your system is not redundant and while he cannot of course steal money because it's a zero-knowledge proof system whatever because of the bridge it still can store the system and you don't want that property and so you need to come up with new design to and right find the right trade-off on this latency decentralization which still is an open question to some extent cool but to come back to the original question like like if there is a work that needs to be done if there's something people need uh and they were willing to pay for they will always be found some market mechanism to provide this failure and and to get compensated for this absolutely i think um what do you so you know i think the progress in uh uh in like the fact that like we even have test nets and main nets of general purposes or knowledge computations is just like insane from like the first time i saw the zero cash paper in 2014 it's just been like i can't believe that like we've we've made so much progress in eight years but what do you think what have been the challenges in building execution environments um like you each have like sort of very different models of of sort of like how the zero knowledge like is actually executed how do you differentiate them or like think about them as being different so um for us our our main goal is to uh be evm equivalent which means that you can copy the bytecode smartcon by code from ethereum and just plop it on a scroll and it'll just work without any need for uh transpilation or any other fancy trickery to make it work and the challenge was with that is that evm is not really circuit friendly and zero knowledge proof friendly so a lot of the up codes and a lot of the functionality inside evm needs to be made in a way which minimizes the overhead in inside the circuit and that's quite challenging to do while also remaining efficient so that's one of the main challenges for us that we're tackling right now and the way we're trying to solve it is by using hardware acceleration so we already have a gpu acceleration uh implementation accelerated implementation and we're working on a pgas as well and we're just going to see which doesn't that make decentralization more challenging to some degree it does but it depends on the incentives if you provide enough incentives there are going to be always enough people who are who are going to participate in it i mean i about the decentralization challenge i just want to to bounce into things here um the first one is we all have the same challenges like we basically have the same prime we want to separate the sequencer we want to separate the approver ideally if we don't we could we're just going to merge there is no other way but basically we have a same technical challenge but uh the third of now the trade-off that we all have is different like target uh you know the starkware just took the took a different bet to say you know what the vm is great we all love it but seriously it's a piece of crap and let's let's do something that is optimizing for for for proof right i mean i i'm going to rephrase that it's a great software and we could be improved uh and making jokes but uh but uh yeah it's dark we just decided to say you know what let's zkp is a very different computing program computing paradigm just deal with it like just accept it and you know embrace it and do be as fast as you can using that paradigm and then you can retrofit stuff if you want afterwards but optimize for the performance uh and scroll obviously is going for pure evm you know even compatibility or like uh the ability to actually prove blocks on the only theorem and dk sync is optimizing for simplicity of uh development developer experience based on previous experiences like just using if they summarize it briefly i would let you guys bond from that but um the the the question now about decentralization is the decentralization as we say also is less important for zika roll up than it is for ethereum or for n1 and because decentralization matter on the on um on the you know decay on the on the on the real one because you need it for consensus you need to know that the state you're holding is a true one you need that otherwise the guy everyone could lie to you when in the vk roll up you're like i just need for censorship resistance i'm not going to be lied to because they cannot lie to me because of the proof so i just need like a a bunch of you know incentivized people probably you know i don't even give numbers because we want as many as possible but it could be smaller and it's not necessarily a big deal so i think the the zk sync cpu target is kind of very unique you want to sort of explain sure so as louis correctly said we've chosen the middle ground between the the two extremes of having evm equivalence and having a completely separate like from scratch execution environment what we're trying to maintain is a balance between the like not the balance not the balance uh we're not making compromises we're like we want to get the best of both worlds of the super efficiency which zero knowledge proofs can provide if you make them uh if you optimize for them specifically and having a uh fully evm compatible chain where you can take essentially any application written for evm chains on in solidity and viper and other languages you support it on zika sync and it will work out of box with full like a web free api access with with your uh deployment scripts with all the tooling that you depend on it must just work but we are not willing to take the compromises like uh in in like the penalty in security orders of magnitude more sorry not secured here performance that you would do if you went for full evm efficiency uh so the challenge for us was uh there were a lot of challenges like you just go them one by one and you tackle them once you set your priorities but i like one specific example would be we need to follow and fully preserve the evm security model and what we're building is a uh is a hybrid solution it's evolution between zika roll up and zik porter which is a data availability of chain solution which is connected to zika roll up so you can users can choose whether their account is fully secured by ethereum or has this external data availability layer which makes some assumptions about security but the challenge was like how do you maintain the security uh model of ethereum because the roll-up contracts cannot depend on uh the deporter contracts like if if their data becomes unavailable there how do you make sure that the users can still always withdraw their funds from the roll up uh and this can be enforced by ethereum so there were multiple layers of of thinking there one is that we need to ensure that every transaction is executable in zero knowledge proofs even if it fails we we don't only accept transactions that that are valid we can accept invalid transactions as well without coming through priority queue on ethereum and we will always execute them no matter what and just if they fail we will prove that they fail uh but there was also a challenge of like how do you design this interaction between roll up and porter in such a way that to protect to like 100 percent secure roll-up users no matter what happens on the border we found the solution we're going to be happy to present it once porter is out um so louis the challenge with having your own sort of cpu target is you've had to build a developer community right around zero knowledge right so what is how is that so um to be honest uh so you know like let's say that like nine months ago or four you know before since i've just started where and we've been talking about cairo everyone's saying vm compatibility even competitively like you know it's like the gold uh and and um and the truth is um like it was at first we were very worried like of course you know we we have to start green tea from scratch and the counter intuitive thing is that it actually helped the counter interesting thing that you know we are not gathering i mean the whole thing we're going after is not to gather to the what let's say 20 000 30 developer worldwide 3 000 different 3d developers we are all the new guys they are you know willing to accept something new they are trying something new and even worse you know they found they they they feel that they're bounded by fire because cairo is hell uh and so cairo is our cpu and a language so so the truth is we have seen a surprising interest from the dev community from people who came from existing solitude from other places in in the world just you know popping up and learning it and improving the tooling and having extremely dedicated community and you know without sort of buying their way we just you know just explain them talk to them give them ideas fostering the community basically helped and worked and as of today i would say that worldwide right now on the daily basis there is around 300 to 500 devs nine months after inception so that's pretty successful and the growth is never it's not stopping at all so for now we are pretty considered pretty safe with our bets so you say you people can are willing to run you things but i certainly see you tweeting some new exciting things like every single uh every single time i wake up in the morning yeah i know i know i mean i at this point i'm like uh cheerleader like a echo chamber like i have to say if you're a dev and you want i always tell the dev wants to actually break into the space that the the most important thing they need to do is i have two things that i tell them the first one is it's better to be early than to be great which means the following thing you're better off being early in ecosystem than coming from you know deep mind and going make solitty dev right now why because you are all of a sudden facing people who have been doing that stuff for five years and you're like you don't know the food the gun the food guns you don't know all that stuff and you're going to be like competing with people that are just you know better than you because they've been around on the other hand if you come early in a environment that is a bit hellish but at least the fundamental is very interesting then the competition is a lot easier you're you're gonna everyone is new no there is no great dev there is no sort of magic trick that you know and so that's what i'm saying i'm seeing people you know realizing oh the tech is super cool and web 3 looks amazing but solidity is already kind of a crowded place it's not fully but it's a kind of quiet place here i am there is 500 people worldwide if i'm bad i'm still i'm better than anyone else and so that's the frequent one the second one is i always tell them you know whenever you do something in this space you're going to break in by being visible and the only way you do to do that and that's magic of crypto is that everyone using twitter and twitter is a very easy marketing tool so every time i'm telling you oh you do this paper from paradigm just tweet it and retweet it and the committee is being what it is it's basically retweeting and a lot of excitement and you know in i got like at least five or six people getting hired just because they posted three tweets uh are you open c higher various places because because they just become visible so yes i have i have i've become an echo chamber and i don't do much with my life except tweeting so people should just follow you actually don't no don't think you should follow all three of them yeah you should but like it's really boring not him though no not me it's very boring it's like it's advertisement advertisement advertisement you know just don't do that but if you want if you're brought in your life feel free okay i think we're supposed to get off the stage around now right you have time okay i thought we were going to transition so now i have to like come up we can take questions yeah any questions i have one question for louie oh god one token i saw you first go to you uh hey so uh we discussed at some point uh zk over consensus right so like uh one of the properties that we have for off chain computing is that we're not time constrained so we can perform any proof there and just use this in the chain so for the consensus this is more complicated since we're time constrained and maybe we cannot even benefit from some state-of-the-art techniques such as recursions so how do you guys see that like uh should we expect like some improvement on the current state of the research or the implementation to make this feasible so to summarize the question i mean if i may you're asking about ezekiel ones and recursion as a tool for improving what i'm saying consensus consensus okay anyone wants to take it or do you want to i i think those are actually orthogonal problems like you you don't improve consensus you improve uh other things you can you can significantly improve decentralization if you implement like l1 fully like let's call them fully succinctly one uh what mina protocol is doing for example all right so like you it's a lot easier for everyone to verify for full nodes you don't have to re-execute all the transactions you just verify 100 knowledge proof which is which will take you 30 milliseconds or something and um the rest is just data availability right so like it's it's a great approach if uh you can uh you like the challenge would be to compete with big is established still once but at least that's something where you can differentiate from them but i don't see it affecting the consensus mechanism as such in any way also bear in mind that a lot of consensus mechanisms are synchronized or partially synchronous and when you use validity proof ability proofs don't have a sense of time so there is no way you can use validity proofs to optimize consensus in most cases yeah but i mean like the the other time to prove uh statements for the consensus so the proof is far more expensive than the verification so like to verify it's trivial to prove it's not so uh that's the main problem so so the alex said the proof generation can improve some bits in some proof of proof of work stake like in weeks there might be something about the weak subjective so yeah absolutely week checkpoints or weeks activities checkpoints objectives yeah thank you yeah like there could maybe be some improvement there but um the main thing that you break when you have a vk the l1 level is that you are improving the topology of the network meaning all of a sudden you have validators that can make a you know node non-mining no non-sequencing node that can be on your phone and the miners or sequencer can be on the data center for their care and so all of a sudden you can be increasing your throughput dramatically so lifestyle but you still have the decentralization that you're hoping for for n1 so that is not the consensus level though it's at the topology of network topology cool okay perfect thank you there was one more question right yeah oh perfect all right thank you very much guys this guy this question is for louis because you were doing good on evm [Laughter] what would you change about evm uh hula you want all the all the we're going to be here for a long time let's give it a top three top three uh i mean okay uh you win 256 uh as default uh kit check as default hash function uh which is like why from the get-go uh exa i mean this is not technically evm i guess but exactly similar tree for uh storing states top three those those are all in vitalik's list too so what those are also on vitalik's list right i mean what do you regret i mean i can write a rest in peace yes it's just copy paste exactly they're the top three right um what else um oh um the choice of 80 80 bytes uh addresses uh it's a mistake rlp rlp i thought that's something i don't even know right like uh there is a bunch uh you can continue there but okay uh yeah i mean i'm not the even the expert in the evm is just the top three that come because when you work on zk you're like why did they use k-check why did they do that like could they just like shout like oberlic2s or something something simpler like you know or standard preferably poseidon yeah but like you know whatever like uh actually we don't use poseidon for the story we would have yeah yeah no we don't otherwise start start for the story if they started by that actually the hash function so star query is very conservative in this british function choices and so you have to understand that the biggest thing you have the hardest thing to prove or the most consuming the proof is usually the the hash function and so uh most of the industry have been using poseidon and we are using pedestal hash which is provably secure but ten times worse to prove um and so we kind of like to shoot ourselves in the foot with like uh being conservative sometimes so here's one example uh conservative doesn't mean that they're different it's just like it's different that's such a choice and i mean i kind of hope one day that i would convince my engineer that position is fair is good enough we can adopt it but for now we're still struggling with that we gotta wait for someone else to settle like a trillion dollars on poseidon hashes right right exactly here yeah yeah yeah anyway so just uh thank you very much my pleasure any other questions you had a question uh yeah okay yeah right there okay it's a question for uh all three of you uh or four exactly you want to throw in um what proof systems are you guys of like most i don't know interested by or kind of like where it could go you know take it bulletproof smarlin hilo the list goes on like what of those zk proof systems are you kind of most stoked about yeah i mean i uh i saw it i think we know your answer it's on your shirt starks i saw i saw ariel uh yesterday and i was like you won like everyone is using plonk like except for the stars people and uh you know the earth monetization inside of punk and starks is very similar so it's just like really the last step of the verifier but yeah congrats to ariel for for winning stark timber um but i mean no just uh just a very i mean i just want to react to this uh it's a it's a very interesting question um my name is obviously stark but obviously that's not the answer for everyone uh it's obviously a trade-off what people don't know though is that every time you talk about staggering snarks uh they always care about like the theoretical differences like you know the trusted setup or the non-trusted setup the quantum resistance or not the reality of actually why i care more about starks uh to be honest is because of the proving time uh probably like by default and which is like why scroll i mean you said they're looking into acceleration is because it's a lot harder to make a snark proof because it's n log n but it's a logarithmic operation when the start a start proof is analog gain but it's a hash function so the the constant here is pretty significant and does have an impact on the proof generation so this is also why i believe that me i mean polygon basically when the only the stark words because it's they believe that it's more scalable at least in the short term but you should also bear in mind that long term we're probably going to be limited by data availability or rather by proving times well i mean we'll see i mean you know the i'd like to say i like to quote kent in this case in the long term we're all dead so from there i don't know i'm just a normal guy i don't know i mean i mean of course you know i'm not i'm dumping a dunking here at all i'm just saying that this is why i kind of care more for starks cool all right you guys are done thank you thank you killed it good job all right uh last panel before we break for lunch which will be at one right here um we have sean from medi-keck taroun from gauntlet um alex from flashbots and guillermo who's enroute from banecap crypto to talk about mev on modular blockchains so you guys can welcome them up [Applause] all right hello friends i think we're we're supposed to be four people but we're currently three people so in the middle of the panel it's going to be interrupted by someone rushing to the front and sitting down and joining the conversation we could also view it as uh my alt actually is my alt the proof where it's finally being revealed so this is the uh panel on mav on modular uh i think we're going to try and split this up very plainly into into three sections or questions that the panelists are going to kind of hopefully run wild with and they're related to sort of uh security on modular then incentives and finally uh winners and losers in the preventative game of mav so before we get started uh maybe we could do some i mean the reason i'm excited uh to sort of host this panel is because actually all three of you wrote the paper on crosstrain maybe right i think not guillermo but yeah you're two out of two two out of three two three so the people who literally are writing the book are sort of present so we can get their perspective but maybe they have more profound sort of personal details that are relevant that they want to introduce themselves very briefly yeah i'm tyrone um founder of gauntlet but also written a lot of papers on uh both mev and oh i'm alex researcher at flashbots wrote a paper with tarun as a co-author on cross domain mev and did some research on mev on different fronts um yeah before carlson enemy i was looking at proof of stake and mev on proof of stake chains on ethereum in particular well so what we wanted to do to start the panel even though we're kind of midway through the the the day is um start out with sort of some definitions of modular just so we all use the same uh words when we mean the same thing i mean the same thing when we use the same words i think that'd be helpful and we want to sort of do that in the in the context of security so when i guess the biggest we could start with like the worst case of mav is uh these sort of long-range time-banded attacks that are sort of hopefully prevented by uh some kind of economic security so maybe we could start there with asking like what is the model like what does a modular blockchain do how is it secured what are the assumptions and what does that have to do with the overall stability of the system um i think the best way to think about module modularity is sort of the classical computing analogy of you have many different computers they have different resource access so they have different compute and different sort of state that they're executing and they have some method for communication that method for communication might be some type of message passing system it may end up being some type of deferred computation system uh but effectively they have a way of communicating yet operating somewhat independently while synchronizing anything i miss because i you wrote the definition for domain in that paper so yeah no i think i think that sounds good i i saw there was a talk earlier by mustafa um and i think you were defining what a modular blockchain is in your context um so i don't know there's like multiple definitions floating around and i'm not entirely sure which one we should let's use the simplest one well i mean the simplest one is what tarun mentioned i think it's the the kind of like the architectural one yeah but then in practice i think you have different applications i mean different approaches to uh where you want modularity on your on on your chain and so there's like some approaches that have been stated i think earlier today um yeah then there are other approaches as well i think what tyrone is referring to to some extent is the modularity even in like parallel comp computing right where you have like multiple threads to some extent there's like modularity there and how you manage communication between these threads sure i think like if if we had slides and we don't there would be like some kind of diagram and there'd be delay actually the last presentation from the guy from zk snarks had this this photo you know where you have like and there's a transaction and someone executes it and then someone agrees on the order and they're not in the modular architecture they're not necessarily the same people right um which of course you could start to envision how there'd be sort of a different as soon as there's communication as soon as there's asynchronous as soon as there's layers and two things that's kind of where mav lives sort of at the intersection of varying security models at these at these layers you could say so talking about security now we have an idea like if we were on bitcoin and hopefully no one's using that anymore you'd have some notion of like how expensive it would be to um you know rewrite the network right and it would get more expensive do those security assumptions hold true in the marginal does the marginal architecture first of all say anything about that is it indifferent to sort of uh proof of work professor or whatever or does it does it really imply certain new security assumptions um i i guess each unit has its own local security assumptions but then there's sort of some synchronization guarantees that are you know uh held between the different units so and and maybe it's because i'm i'm older than the space but i think of everything in terms of like old-school parallel compute and you know when you write kind of like multi-threaded code or you write multiprocessing code you oftentimes will have this sort of notion of like independent units that have some amount of minimum shared state and they're only allowed to kind of communicate through that channel and one of them could flood that channel and then halt execution somewhere else because it kind of ddosed the other chain or other process uh you kind of have the same same issues here where effectively you need to ensure that the communication channel is sort of resistant to some extent to being ddosed that the it's expressive enough that it can actually send all the messages that are needed to be concisely communicating between the two chains or two processes and so you basically have three sort of between in any pair of communication you have three security kind of models you have one for each of the two sides the two processes and then you have one for sort of the synchronization mechanism um and one of the biggest you know if you if you look at something like the wormhole attack uh you know these kind of like mint bugs come from the fact that like the two sides may not agree on the security model for the communication layer and that's usually what's exploited but you could say that in in some ways the offering of a project like celestia or um or one instance of this modular architecture is to kind of share security so that there are sort of like something that some component that does the data availability and then state machines and executions are kind of separate from that and there's this idea that um maybe if security is cheaper right you could do you could do more things you can push innovation to other layers so in this modular stack you could have one layer that is maybe super secure and uh but not very useful in general sense and then other stuff that is like much more useful uh and doesn't have to worry about security but from uh from an mev perspective you could imagine that at some point someone has to decide on the order and uh and that's the bit that i would imagine you would want to be let's say the most secure yeah so if we actually think a little bit back to linux's development it took almost 10 years for linux to actually have reasonable smp which is like its multi-multi-core multi-processing unit so that when you write some piece of code the linux scheduler could decide how to sort of like shard the memory that for each process and allocate the process cpu manage the page tables all that stuff and the reason it took ten years it's actually really hard to get this communication part right like and this is without people having economic incentive to ddos your cpu it's actually that scheduling algorithm is actually really hard to write because you need some notion of shared state and common state to all these processes which is where i think something like celestia becomes useful is actually having that initial kind of agreement on on sort of data before you start trying to do these kind of like mutexes lockless cues things like that this is this is completely off topic but interesting to me so i'm going to say it like in a pre-bsd are you familiar with what happened in freebsd and so there was like there used to be this freebsd and then it became dragonfly bsd because they had different models uh or different yeah different models of parallelization that was sort of sufficiently different to sort of warrant the project sort of um working do you think that this is something or these kind of disagreements like is the intellectual space of modular blockchain sophisticated enough that there are sort of like multiple models and how do you think those models will sort of maybe evolve and parallel compete or eventually integrate yeah i mean i think one one important piece of interoperability between all unix-like systems and post-sex-like systems is that their file systems are very similar right like they all can you can you can basically install any formatting uh on any of those devices versus like the windows version where like you effectively had to do all this emulation and it took forever for linux to actually be able to read windows formatted devices um but i think the second thing is that uh the memory the memory models while different have some notion of translation uh and you can get some sort of bitwise identical behavior on both sides and that's the key thing right you want bitwise identical execution and bitwise identical sort of storage and agreement on that and and as long as you have that as a standard then you can get a lot further than uh say you know like what ended up happening in windows where like parallel compute never was able to take off and to some extent i would argue that windows missed i mean there's many reasons microsoft missed the cloud computing world but one of them is this is that it was extremely hard to actually interoperate with these and sort of alex from from your understanding and and research on uh i guess on the ethereum space right which is let's say more uh diverse it's a pretty big ecosystem many examples of this modulus like split there's many l2s in in ethereum they're already um sort of competing i would say in a way um what's your what what's your sense and like uh like which one is best for me what's it oh and why which one is best for me yeah which one which party i mean no but i was going to bring it back to what turin was talking about if we bring it back to like the space today uh i think tarun is kind of talking about standards to some extent as well for communication for interoperability to some extent i think we kind of need that um for security i think if you have completely different security models i think it's much easier to so take take like let's take two different worlds one where you have like one big blockchain right where if like a homogenous security model where you have like you know security model can be understand holistically understood holistically and then say you have like a patchwork of chains that have different security models different like however you want to define your security model etc intuitively it feels like the patchwork of chains is easier to attack because you can attack like smaller uh units of it grow in strength and then be able to attack more um whereas the first one i think is harder to attack maybe upfront but if you find a flaw you can like the whole the whole system can fall i think currently we have these like patchworks of different uh chains and different models even in the modularity approaches there's like different modularity approaches but also in where the security assumptions lie same thing for like the bridges whatever this is all over the place right now and i think it makes it much easier to attack to a sophisticated attacker that wants to like attack the overall system um so what i think and also for for usefulness of the system if there's some standard that's agreed upon that people can coordinate against it can be a very simple tool but something that people can coordinate with cross domain but there are sort of like concrete examples like i know that polka dot has xcmp cosmos has ibc right uh there's there's a few i agree i agree but i think i think maybe we need that um that's more universal or more general i think ibc is hard to integrate in some uh chains as far as i know as hard as integrating a like client yeah like you know non-trivial um so maybe that's that's the thing related to mev right the more you modularize the more your stack is complex the more there's like new attack vectors that emerge and the more there's complexity in being able to reason about the economic security of that stack i think that's important in the case of modularity where you use something like celestia as your data availability layer there's also the the security budget you need is kind of dynamic right so the more like domains you add on top that use celestia for data availability maybe you have to think about how that affects the security of that layer um so i think it's also dynamic in that sense and to me trying to reason through this the more simple that is and how we reason about the security of these different domains and how we compose the security together the more often we can build like a strong resilient system but what are the sort of like to switch topics now onto sort of incentives like as like a an l2 creator or a rollup like if you were just launching i don't know sushi swap clone as a as a roll-up like beyond the incentives of like the settlement layer being sort of secure you wrote a little bit about how there is this competition from fees to move between the base layer and uh and this sort of roll up or the application or higher up the stack so how maybe you could elaborate a little bit like how do you see like vowel value accrual sort of evolving with you know potentially deeper stacks you know l3s and and and so on and what impact do you think that will have on sort of stability like not just having something secure at one point but sort of long-term security predictable security so maybe two answers and i'll pass it to roon as well one in terms of like value or cool from a perspective of designing the system you want value to be where you need like security right so like value related to security budget um so i think there's like some relation there of like where value could accrue and then the second thing is like owning order flow and owning where the customers are i think to some extent there's like value there because you own the touchpoint that creates economic activity uh so there's like two ways to look at it so to some extent the order flow part is like whoever runs the ui that users interface with because then they can you know redirect your transaction to some other stack very easily but at the same time a stack that isn't secure won't be used for long and so you need that stack to be secured properly um whatever that means in terms of value that has to back it how much has to be staked on it how much does it cost to attack it relative to the economic value that's on top of it is there an incentive to attack it because you make more money by the value that you secure on top of it et cetera but there's also cases when uh the protocols are incentivized to sort of like be attacked like in the sense like if you're a dex or whatever you're trying to bootstrap you're trying to get liquidity like you know you want to pay the liquidity provider as much as possible if someone is sandwiching or more adding more value like people are sandwiching attacking consuming the spread technically that money is going from users to lps right so there is this incentive play where it isn't always clear and probably changed over time like hey here he is [Applause] oh well i was expecting music while i was coming in but it clearly doesn't work out what happened yeah well sorry very nice to meet you all right sorry yeah i guess uh i'm just a paid actor actually i'm not really uh i'm in stand-in really so it's great anyway sorry no but we were talking about incentives but i think i interrupted uh uh terred if you wanted to speak sort of about the incentive question yeah i mean one very high level view of like sort of the [Music] game theory difference between single chain mev and and multi-chain mvp and obviously it's a stylized simplified version of this is that in uh in a single chain muv is really a congestion game there's just like a fixed number of opportunities there's a bunch of people who are all competing to basically capitalize on those opportunities and auctions are can be designed to be like a reasonable way of dealing with congestion in like most economic situations however in the multi-chain world it actually really becomes like a routing problem right of like how do i go between these different chains and maximize my value and the routing problem is actually a much larger sort of state space of like possible ways that things can go wrong which means you need to coordinate the incentives along the whole chain so like let's suppose i have a sandwich attack that's a sequence of sandwich attacks across bridges that on each side use an amm well then i actually need to make sure all the lp to your point earlier i need to make sure that all the lps kind of end up having like similar incentives but there's not really a great way of guaranteeing that in in a sort of multi-chain world right you you actually would have to synchronize those constantly across all of these or just take the risk like or be atomic and then you say oh i assume that i'm using flash bots and that i'd zero risk but if you're like again bain capital or someone with serious money or like a market maker oh sorry you're here i forgot [Laughter] you'll just you'll just execute the trade your short-term prediction model will say whatever is going to happen and you'll do it and you're going to be right you know enough times to to be bank capital you know so so here's a fun little alpha leak actually um so it here you have a model of uh that's probabilistic yeah turns out that problem is convex too so actually you can you can solve it um but anyway sorry i didn't mean to detract uh shouldn't maybe that's too much of an off leak um but but i i guess the point is like you have to coordinate these incentives a lot more right and and right now we don't have you know the fact that like flashbots can be isolated from the incentives that are people using to bootstrap pools uh is is not really possible when you when you think about it in terms of modularity is that to somehow suggest that smaller players are going to have a harder time in this fragmented liquidity world absolutely yeah you need more capital to be able to do this right yeah it's kind of it's kind of rough because i was talking with uh with exactly about this and it's like it's like oh when you have like a million fmo space chains or whatever maybe not a million like let's say a thousand and we're trying to ask this question like oh should like will it actually be possible to do this kind of things like do you think like jump capital would have any problem putting a million dollars in a thousand place i don't think so i think that most market makers are big enough to like make the spread to take the inventory risk and sit there and get like the juiciest part when it arrives so if someone is writing this complicated model and like even if they are like say everything is connected over ibc and you actually can there is sort of some kind of secure transmission between these things like it's not like an exchange where they can like stop withdrawals or something it's like an automated permissionless interrupt thing even if the people who are trying to move liquidity around are always going to have a harder time than someone who just keeps liquidity everywhere i mean i guess this gets to alex's point about like who owns the order flow owns a lot more than what you than you think and the question is in this multi-chain world will the order flow actually be owned by an aggregator or will it be multiple kind of like separate front ends that end up fragmenting the order and then right now it looks like the ladder right no one's actually built a very good cross chain aggregator and part of the reason is that the front ends don't want to take the inventory risk uh right like the market makers do but the market makers also don't make products so they're not like getting user flow but it could be just because it's just not worth it yet like the markets might be too or i don't know there's some bridges that are backed by market makers right wormhole being you know notable for sure for sure for sure i mean we we get a sense of the size of their ambition that by the fact that there was a hack and the next day they you know paid 350 million back to the bridge but they also raised that again right after let's not let's not forget that it did get covered in this fairness or fundraiser i think the overflow cross chain uh is at the bridge level in my opinion i i think when jump started wormhole there was kind of this sense uh amongst people that like this was their version of like the microwave tower and like you want to win the new york chicago microwave tower there's a really horrible movie that if you want to watch on the history of this called the hummingbird project worst acting i've ever seen because it's a movie about like hft so it's like can't be good acting uh but but like honestly jesse eisenberg isn't that yeah yes exactly and still a horrible movie but i liked it but but it is factually accurate that like the bridges the effective war we see with the bridges now in the normal world converge to one thing but here it actually seems like it's not going to be true right like it really does feel like there's going to be quality of service versus bridge security trade-offs and you're going to have like the spectrum of that and then people are going to allocate to that and that that'll have a totally different microstructure than normal markets um yes i i think that makes a lot of sense but even in a permissionless system like some people use bridges but not everyone needs to and i guess the question is when is it eventually gonna or will it will it happen that the bridges will sort of like if you wanted atomicity across the bridge you just have to be two like if you're if you're validating like who's to say that you can't operate on both on multiple networks simultaneously but as like an infrastructure provider i guess like i'll talk from like the cosmos case because that's what i'm most familiar with a lot of validators operate on multiple networks there's no reason if the blocks are like uh the leader election the person proposes lock is predictable if you could predict the time on both networks right then you could actually do a risk-free cross-chain thing and go around the whole bridge dilemma so how do we think about the sort of the incentives for not just jump but but even hopefully smaller valid or smaller operations to start doing multi-chain operations to sort of benefit from multiple or simultaneous order flop one one thing there uh that's part of the paper the cross domain muv paper right so we talk about that to happen but then we talk about the fact that if um say you're a leader on chain a i mean i'm a leader on chain a you're a leader on chain b and you want to collude or collaborate however you want to frame it yeah in order to get that like economic atomicity across ourselves we don't trust each other so we need a way to trust each other we also need a way to communicate right i think you can model all of that as some like additional economic cost that a player that is the leader on a and b at the same time doesn't have so i think you can maybe point to an incentive for someone to control both of these like validator sets for example um and that's what's problematic because maybe the the pursuit for you know alpha is such that the system equilibrium doesn't go towards many smaller validator sets that actually have a way to collude with one another trustlessly at minimal overhead but rather a very large player that starts owning more and more of the validator sets on many sides so that they get these leader election slots and they can reduce the overhead of communication there's still some communication but it's internal communication and you don't have that trust cost as well because you can trust your internal systems to work to some extent and so you can lower the cost to do some interactions and you can maybe even seize pockets of opportunities that for us who don't trust each other the cost of coordinating for that opportunity would be too too high well you can put an auction between it right i think that's what you're sort of alluding to yeah i just think like the the the lack of atom atomicity actually changes the outcomes of these auctions right so so you sort of have to like have an auction where you coordinate like the like let's just even in the simplest case like i have two first price auctions of like on on both sides and i go across the bridge i actually have to coordinate that the reserve price and the floor price and stuff like that the initial setup is the same across both places uh and there is a cost to that like if eventually you still have to pay some amount of money to to synchronize those things and that might go that might go to zero over time though ideally it does i think i i believe we wanted to or at least not zero but i like i yeah i think zk stuff will be the only way to get to the point that that cost gets arbitrarily small in the long run because what because it lowers the communication complexity yeah exactly but you would still have like a back and forth to some extent right you know the question is like i only have to send you one packet and you can verify that like it's correct at the same time that it's like i did the computation correctly on the other side so i don't have to like do any interaction so okay in that sense that's good question for you both uh right what if say we go back to that same example right and we send each other the packets but now we need to split the profits how do we split the profits i mean that you can have with a contract on both sides right the profit you store the final profit on one side right right it's always on one side it can be on both sides contract that's that's doing that you could also do like i mean the question i don't know if the question you're getting at is like how much profit do you split yes this is like the key how does the optimal profit split yeah yeah so this is actually a well-studied problem in uh okay in game theory which is called the the cake cutting problem the pie cutting problem depends on who you ask and what your aesthetics are so it you could actually there is like a weird set of equilibria that you can talk about here um and often they're actually quite stable too so um it is surprising depending on who the first person is who's like taking some of the risks okay but but and how do we think about like adversarial things we're like say we're splitting and like we agree on 50 50. but maybe uh there's someone else who's willing to do 60 40 or or maybe over time if you like in an iterated game instead of this particular opportunity it starts making more sense for me to own more of the network that's rather than split with him right because then it's like 100 0 instead of 50 50. i mean this is why proof-of-stake networks that have uh that draw the entire valder set over an epoch before the epoch where you know know the blocks like are just you can't avoid this problem right because you already see this in solana in avalanche right where like people are basically just paying validators ahead of their block number and co-locating like for liquidation yeah i mean even decks are people they're just basically paying do we need to avoid like any kind of uh leader election that's in the future do we need like proof of work like leader election yeah and that's the zk thing right the single secret leader election kind of type of stuff is is only enabled by like homomorphic encryption and zk stuff so i think this cost goes down with cryptography gets better it's not it's not a purely economic thing to use the like algorand leader election model that's like vrf based with the committee i mean you still the there's still these like poison committee attacks that are known you know so often grind too so sorry to cut you guys off do you guys want to take one question from the crowd before we uh get off super interesting conversation i just want to open it up a little bit to uh if anyone's been waiting has a question i don't know how this works i just point to the person and then mike goes yep yeah yeah guys we have lunch one um so this will probably be one of the last questions where was the person there there you go this is a question for alex about flashbots architecture and how you see it changing in context of multi-layers and modular blockchains so like flashbots right now and the way it's designed is very practically helping democratize mev on ethereum with like enabling many searchers to participate and minors but in context of like what you guys were talking about with like you know people with the most capital being able to get mev on like cross-chain world and multi-layers and modular blockchains how is flashbots intending to like change its architecture to help further democratize mev and um yeah in context of like obviously you guys are also trying to like become censorship resistant and decentralized so you seem to have a lot of like competing priorities but i think this is like a big one coming up so curious to know yeah uh great question um we think a lot about the risk that i mentioned earlier this like centralization of validator sets across many domains and how this kind of world can be attacked we also see it as like it's inevitable that the world at least for the next few years is gonna be like multi-chain or whatever even if you focus only on ethereum you have many layer twos that have raised like hundreds of millions are going to be able to subsidize activity for a while even if it's artificial um and so we need to prepare for that asynchronous world and what that means there i don't know if looking at it as simply as like oh players need more capital to participate makes sense because even today on ethereum you know there are flash loans but if you use flashballs today flash loans are not gas efficient compared to having the capital directly so a lot of players need the capital to be able to execute a lot of opportunities but does that change massively in the cross domain world i think what changes is maybe like the inventory management logic that you need to have and also there may be economies of scale that if you have more capital you can warehouse risk a lot better and so you can maybe take more opportunities or take more risks so we think a lot about that we think a lot about like this dystopian future of like very centralized blockchain world in general where you have like a few validators that run validators on like 100 different chains and you know run bridges where they provide liquidity to users uh that's like the worst thing for us and we need to adapt right for what i just mentioned now um we're gonna announce more on this relatively soon uh so you'll have like extra details there but we're we're worried about the same thing that we were worried about when we started flashballs a year and a half ago which is we want to keep the system decentralized we want to keep it transparent as much as possible because the more you have opacity the more it's difficult to understand what happens within it what else do we want to keep it we want to keep it like accessible to to most individuals um yeah i guess i'll add one one point on uh the fairness aspect which is that even though these bridges that have synthetic assets tend to be the ones that get exploited first uh synthetic assets and like staking derivatives especially in proof-of-stake networks do actually lower the cost of capital for the actual validator because validators basically smaller validators can lever up around opportunities that they want to kind of subsidize and so one real question is going to be how do we balance this uh centralization effect versus like should we allow like more kind of derivative assets across bridges and that's obviously going to be this big security question because you know wormhole is the greatest example of like what happens when the staking derivative blows up uh but you know it is clear that that actually does lower uh validator centralization so um yeah long run in the long run that there's going to always be this kind of trade-off between like how much we want derivatives for uh equalizing access versus why why does it lower the cost yeah so like let's say i'm a small validator and i have like one percent of stake i can borrow against my stake and basically at least for a short amount of time get to say like 3x leverage on my existing stake and now i can go restake that get three percent increase my number of blocks and try to basically but surely surely if you own 50 you can borrow 3x leverage as well right no you're you're actually if you how do you borrow three times oh you said there's no liquidity for it there's no difference so so the the there's this kind of liquidity versus security trade-off that naturally comes up and sacred derivatives um so it's always going to end up being this war between like how much do we want these kind of very hard to price derivatives that can blow up versus how much do we actually want centralization cool i think that's a great place to end it thank you very much panel all right all right guys that wraps up our first half of the modular summit uh lunch will be in the overflow room so right over there um yeah go ahead and enjoy it's gonna be an hour long lunch so we'll meet back here around too [Music] me [Music] you [Music] foreign foreign [Music] lunch will also be served in the atrium so if you guys also want to hang back here and pick it up that also works up [Music] [Music] [Music] [Music] black [Music] left [Music] oh oh [Music] [Music] oh [Music] [Music] uh [Applause] [Applause] hmm [Music] [Music] [Music] [Music] do [Music] [Music] [Music] [Music] [Music] my [Music] [Music] [Music] green [Music] [Music] me this [Music] me um baby [Music] this oh uh my [Music] oh oh so [Applause] [Applause] uh [Music] [Music] [Music] [Music] [Music] so [Music] [Music] so [Music] [Music] [Music] my [Music] [Music] [Music] hmm [Music] [Music] so so mmm [Music] my oh me content me baby [Music] uh oh uh [Music] so oh so oh [Applause] [Applause] foreign [Music] [Music] thank you [Music] [Music] [Music] [Music] [Music] [Music] [Music] so so [Music] [Music] [Music] my [Music] you [Music] [Music] um [Music] foreign so huh um uh oh left [Music] uh so oh [Applause] [Applause] foreign so [Music] [Music] [Music] hey everybody we're about to start up the second half so please come and sit down [Music] me [Music] uh intro oh no i'm [Music] [Music] so [Music] [Music] [Music] is hey everybody um come and find a seat we're starting up the second half of the modular summit i'm nick white i am ceo of celestia labs it's my pleasure to be here um i'm so thrilled to see so many people in our ecosystem the modular ecosystem here today and the the quality of conversations has been phenomenal so we have a action-packed second half of the modular summit i want to make a few announcements one is that there's a research track in the in a room over here and we have a lot of really if you're more like research oriented and you want to see some of the most cutting-edge things with roll-ups interoperability data availability schemes um that's happening over here but don't don't walk through here go around the corner past the bathrooms um so hopefully some of you go there because i don't want them to be in an empty room uh aside from that we have uh networking drinks at the end of this uh segment so at 5 30 and we'll also be handing out t-shirts so i hope you guys stay around until the end and hang out and talk to us um so what do we have coming up we have a bunch of really cool stuff we have a data availability landscape panel that will have feature ethereum polygon avail celestia we also have a debate between mustafa and anatoly from solana about modular or monolithic and that's going to be right at right up until 5 30. um so anyway it's my pleasure to announce or introduce the next guest um he goes by a few different monikers some people know him as a altcoin slayer some people know him as erica the blogger but he is really one of the most insightful  posters on twitter uh and uh yeah so without any further ado i want to introduce eric wall he's going to talk about roll-up security paradigms thank you nick thank you so much hello everyone my name is eric erika combination of those i'm a swedish crypto fund manager slash blogger slash vc uh but i'm also uh recovering bitcoin maximalist and i've been recovering for about three years now and i want to i want to tell a story like um i think this is a story of why i'm here on and on this stage and the story of how i grew out of bitcoin maximalism and why i think this is one of the most important things that is happening in the blockchain space right now and why i couldn't think of any more crucial place to be so i want to congratulate all of you guys that are here and i want to share my story with you with how i came to that conclusion so like nick said i used to be called the old coin slayer in this picture you see some of the coins that i supposedly killed they're still out there i mean it's really hard to kill coins but um supposedly i i killed iota neo header hashgraph and in this journey of killing altcoins i always had this belief in from the beginning that bitcoin was going to be the currency that was going to the network that was going to power everything else in the blockchain ecosystem and the rationale for that is that if you go back we're gonna go way back in town now we're gonna we're gonna go all the way back to 2014 when the company block stream was found so if you read the vision statement of why block stream was created it is actually kind of beautiful it's a beautiful story so it reads that the altcoin the altcoin approach of creating a new cryptocurrency just to introduce new features creates uncertainty for everyone looking at cryptocurrencies from the outside there seems to be no natural stopping point each fork can be forked again at infinitum this creates both market and development fragmentation we think that for cryptocurrencies to be successful as a whole we must build network effect not fragmentation we believe everyone should enjoy enjoy freedom to innovate too without seeking permission from us or anyone we needed a different way to get there than by attempting to to disrupt our own success it's kind of beautiful to accomplish this we propose technology to enable new cryptocurrency networks that do not need new cryptocurrencies delivering on this vision will require continued investment and in cooperation with the bitcoin ecosystem as a whole along with the full-time support of many people with broad and specialized backgrounds we felt that in the bitcoin ecosystem and in the world at large there's there is a shortage of companies working on trustless cryptographic infrastructure that's why we along with our other co-founders who share our vision came together to establish blockstream so this this was the picture that they shared like their vision for how the blockchain ecosystem would grow and i like this picture so much this used to be the banner picture of my linkedin profile this was i was so um hyped up about that vision uh for for for blockchains and for side chains to to interact with each other and these sidechains are they're not your they weren't originally proposed as you know what we see today where there's uh proof of authority chain with a bunch of validators like these were supposed to be merge mind side chains reusing the proof of work strength of bitcoin to secure the transaction order so and you have the developers here speaking like yeah merge mining is an option you could also have proof of stake and proof of authority but merge mind side chains that you reuse the hashing power of bitcoin to secure these side chains were a crucial element for this vision and you can even go back and you see greg maxwell are talking about using snarks to interpret the programs running outside inside of side chains which is basically what what we have today with with zk roll ups so this was already envisioned back in 2014 by the blocks and block stream uh developers and what was the purpose of of being able to run a snark to interpret other programs like what was the vision well it was division the vision already then to build things like dexas to build trustless peer-to-peer marketplaces options contracts all those things that we see blossoming in d5 were already the vision uh back then and in 2015 they said you know we're in the coming weeks we're gonna publish a proposal for a fully decentralized two-way peg the peg is how you transfer an asset from the main chain into the side chain and back that would be completely decentralized and the side chain itself would be merge mined but it never materialized but in 2018 i still believe that this was the vision for bitcoin that we would see this ecosystem of side chains emerging around bitcoin that would be trustless merge mined um so i would go around writing these twitter threads saying like the mo in architecture it is a no-brainer that every structure of importance that we build whether it be religious monuments skyscrapers pyramids starts with the establishment of a rock solid foundation to persist through the tests of time and with the billionaires to come so i was really in this vision that bitcoin asked the strongest most solid uh consensus engine would power everything else and around that point in time we were thinking in terms of drive chains drive chains were the latest iteration of how to build these hashrate-backed side-chains to bitcoin um and this is a pretty funny quote here so i believe that the only way to compete with bitcoin is by figuring out how to make a more decentralized robust consensus engine and the problem that i had with all coins what the was that all of them were competing in bitcoin in the exact opposite way so they were trading away their core robustness and simplicity for smarter contracts and this is before i understood the concept of modularity because if you think about celestia celestia really is an even more simple base layer than bitcoin is bitcoin bundles settlement execution data availability together in one monolithic piece celestia is that piece that abstracts the set the the data availability and the consensus away and focuses on just that most simple piece but this was before such constructions existed so anyway i was really hyped up about drive chains i wanted to drop everything that i was doing at the time to build a drive chain and even the bitcoin core developers saw a lot of them were on board with this vision they wanted to also see drive chains come into fruition they thought it was a good idea but something happened this is just one year later just one year later something changed and i think it was the block size wars that got so intense and what a lot of bitcoiners realized is that the miners are not our friends the miners had previously been involved in every upgrade in bitcoin using their hash rate their sig they signaled their support for upgrades and the miners were working together with the community but during the block size wars we kind of realized that miners weren't our friends and we wouldn't want to give because in a merged mind side chain the problem is that the miners can always steal the coins from the side chain because everything is relying on the hash rate so something changed in the in the uh in the sort of conversation around drive chains uh we wouldn't we didn't want to give more control to the miners and one thing that happens when you have a merge mine side chain is that um if you are not validating these merge mind side chains you're not uh you're not mining those side chains you don't get the fees from those side chains so if they have a lot of mev only the miners that have very high capacity uh bandwidth are able to extract those revenues so something changed and and even even peter todd is also one of the core contributors to bitcoin said that merged mind side chains were gregory maxwell's biggest biggest mistake so 2019 kind of realized that it wasn't going to happen with bitcoin bitcoin was not going to become this piece that powers all of block of the blockchain ecosystem and then roll ups enter the scene and the the the powerful thing with roll ups the the really intelligent part is that the roll up uh the miners who and the validators that process the base layer they don't need to to validate the roll-up they don't need to re val they don't need to process the transactions they can only look at the fraud proofs and the validity proofs to make sure that those systems are safe so this in if you are a bitcoiner and you're thinking that the only only problem that we have with drive chains is that the they cause centralization around mining roll-ups are that exact mechanism that you need to get away from that problem so i sort of wanted to bring this to the bitcoin community and say hey look at the ethereum researchers look at what they have come up with that actually solves that core piece that we've been looking for all along this is should be a very interesting interesting architectural design for how to build blockchains that can scale that can have different execution environments that does not call uh calls centralization for the validator set so i try to convince other bitcoiners like look at this look at look at the architecture of this thing just look at the construction if we can just learn from this mechanism we can integrate something similar into bitcoin and then we would be able to realize that vision that was the what the uh block stream company was built on that core vision was now within reach it didn't go that way so i tried to bring it to the uh to the bitcoin community and i asked like i'm i'm reading this stuff on on the ethereum research page i'm reading on vitalik's blog post i'm finding new uh ways that we could reach that goal and what does that make me and i got told that it makes me potentially dangerous um and a lot of this is this is also a block stream employee um so i i basically fell out of favor with the bitcoin community just for having the idea that maybe there's something interesting going on in the ethereum community that we should learn from so people were starting to make bets around how long i would last in the bitcoin community and this vision that was originally proposed about how bitcoin would power these side chains all of what it all came down to at the end was just this the only side chain that blockstream ever created was a single proof of authority chain that had no crypto economic guarantees that was the culmination of half a decade of research that blockstream was supposed to deliver to bitcoiners so eric the old coin slayer turned into eric the potentially dangerous coiner and i was feeling miserable at this time i i started to write my own eulogy i was it was kind of a sad time where i didn't know what what to do because you know i had been building up my reputation and my community my friends within the bitcoin community for for seven for seven years um and and this is what it all came to but i found new life in the roll-up design space there are so many things that you can do with roll-ups you can experiment with completely new vms you can experiment with the sequencing logic the fee logic the mev you can you can make combinations of zk roll-ups and optimistic roll-ups it opens a whole new paradigm for innovation experimentation one of my favorite roll-up constructions uh is the zkopro construction which allows you to make zcash style private payments inside of an optimistic roll-up so you can get almost perfect privacy within an optimistic rollup just by leveraging the fraud proof system on the on the base layer of ethereum another thing that i think is usually interesting is the the idea that if you have a roll-up and you have perhaps a more centralized sequencing mechanism uh that creates an opportunity for mav to be generated in a more centralized way and you can do things with that mav for example one of the one of the biggest problems that we've had in bitcoin is how do we fund developers to keep contributing infrastructure to bitcoin how do we fund developers how do we fund public goods so this idea by optimism that we could use the mev in a chain that the sequence service extracts to fund public goods is an incredibly interesting idea and following along that path if you have the ability to create an entirely new vm just by because if you if you're running a rollup uh by using the fraud proof mechanism you don't actually need to be able to interpret all the internal logic within the roll-up itself all you need to be able to do is validate a fraud proof or validate a validity a validity proof those things allows you to create entirely new vms so inside of fuel which is one of my uh favorite roll-ups you can you can you can you can run smart contracts using a utxo data model and that allows for parallelized parallelizability so you can parallelize transactions and get basically slanted type efficiency inside the roll-up so that's a whole new paradigm of of an execution environment that is enabled through the separation between the main chain and roll-ups using fraud proofs or validity proofs so i want to talk a little bit about what does this mean for security so there there is a difference between roll ups and side chains even the merge bind side chains that we had roll ups are superior because like i said the role in the roll-up paradigm what you can do is you don't need hash rate to secure a side chain because the fraud proofs that get posted to the mainnet and the validity proofs that's the only thing that you actually need to validate for the validators or the miners to do their job so roll ups present a way more attractive security model inherently than side chains were ever ever able to produce but they still come with some drawbacks there are some people who say well you know roll ups they're layer two and the layer two in her inherits the security of the layer one that is you know roughly accurate it's not 100 accurate because there are of course some things that there are some there's some level of complexity that comes with a rollup that creates security risk so it's not it's not like one-to-one perfect uh equivalency between the security of the of the layer one and the layer two there are some differences there and i think that you know it's good to perhaps get a grip of what those differences are so i mean the most obvious one is that of course you're going to run into some implementation risk the roll-up the roll-up has a roll-up note that roll-up note has its own software there can be bugs in that software it's just no way to get around that uh if the roll-up isn't implemented properly even the on the contracts under layer one uh a bug can arise like if you look at the most of the hacks that have happened to side chains only a few of them actually happened because someone compromised the keys in many cases there are simple implementation bugs that can cause complete failures for the roll-up system for for a sidechain system for any type of system so every time that you use an external software component that can have an implementation uh bug and and you can you can lose your money by us by using a rule up this way there's no way to get around that the nice thing is that we can theoretically move away further and further away from that risk by improving our systems uh with the sidechain for example you can never get away at the inherent risk that either the miner steals the coins or in a proof of authority side chain you can never get away the risk that the well that the validators steal the coins in the side chain but you can get away from that risk if you design a rollup really really well you also have a sequencer risk and roll up so uh rollup doesn't use the same uh validators they don't use the same block producers as they the as the layer one does they have their own set of sequencers that produce the roll-up blocks and there are some some risk that like the the most common risk that you speak about is well what if the sequencers start to censor transactions on the roll-up now for optimistic roll-ups like arbitrarium and and optimism there's a way around that there's you can post transactions directly to the layer one track to delay one contract on the on the base layer and that makes the roll-up process those transactions so you can get away that centers that's censorship it's not as easy on on start net for example i haven't seen a design that allows transactions to be posted on the layer 1 that will automatically be included into the start net roll up you also have mev risk i actually prefer to think of the the mev difference here as an advantage like i showed with optimism that you can actually use the fact that you have a different set of sequencers that can be permissioned or have priority and they can generate mavs and you can you can they can decide that this mav is going to go to uh benefit the overall system uh but there there are still differences in how mav works inside of a roll-up compared to the to the layer one chain so that's also a risk that you gotta think about uh another thing that i think that people don't talk that frequently about is something that i call state access risk so a lot of people believe that well in a roll up uh all the transactions data are posted to the layer one and because you have all you have full data availability provided by the layer one system that means that you're never going to have a problem to access the state and you need to be able to access the state if you want to be able to withdraw your funds from a rollup you need to have access to not only the raw data itself you need access to the full state and the risk here is that okay well what if there aren't any nodes who are providing me with the state the risk then is that you and if you haven't produced that state for yourself if you're not running a rollup load node which in a roll-up they their execution environments are designed to be more performant so it's going to be more resource intense to run your own roll-up node versus running a layer one node so if you're not doing that which most people won't do and for some reason there is no other node that is providing you with what the state looks like then you can run into a situation where you actually have to generate that state yourself and if the state takes a very long time to uh to be computed then let's say for example you're getting liquidated in a in another roll up and now you got to withdraw your funds from that roll up into the other roll up and that takes time now you got to regenerate the state the time that it takes to to generate the state is a risk it can cause you to get liquidated it can cause you not to access your funds in time maybe those funds lose value like maybe there's something happening in the cryptocurrency space and your your assets are losing value you need to be able to get those house and sell those if that takes time well then then that's that's a risk i'm coloring these in yellow because these are not like catastrophic risks they're more like medium mediocre risks and then you all in in most of these rollups the the reason that these two are wide is because these problems are not inherent to roll-ups themselves but it's something that we're likely to see with most roll-ups so most roll-ups in the beginning are going to have admin keys the ability to make emergency upgrades uh that of course is a risk um the idea is that we will move away from those uh training wheels with time and we won't really have these admin keys that can change the entire rollup system once these systems are mature but they're probably going to be with us for a couple of years these types of admin case and a lot of these rollups are also planning to add governance tokens and governance tokens also provide a an attack vector like if the governance tokens has the ability to vote on something crucial for the system then uh if someone is able to acquire a large amount of those governance tokens well they get control of a part of the roll-up so that's also a risk the most secure roll-up would probably be a roll-up that doesn't use like if we had no implementation bugs then the most secure roll-up would probably be a type of roll-up that has had as little as possible of these types of admin keys or governance tokens or at least limit their ability to do anything material to the system at all so those are some inescapable things that you sort of get with rollups that make the security different from the layer one the layer one itself so then there are also differences between zk roll-ups and optimistic roll-ups i'm not going to go too to get too bogged down into this but it's good to have at least the the basics of it i think that most people in this audience probably do know the the basic security assumptions that trade-offs that you make between a zika roll-up and optimistic roll-up the zk roll-up of course uses starks or snarks those have a significantly higher complexity and a lot fewer people that actually understand those systems in detail to be able to audit them to a level where we have uh an assurance that the system is going to work as intended so they are more complex the the probability that they have implementations bugs are significantly higher even though in a system like arbitrary for example the arbitrary virtual machine is also quite complex there's also an added risk of complexity there but i don't think it's fair to say that these risks are comparable i think it's probably fair to say that the zk roll ups do add a more material layer of risk but in the optimistic rollup you have the assumption that someone is watching the chain that someone if the if the sequencers post an invalid merkle root than a merkle root commitment then someone needs to be able to to produce a fraud proof and invalidate that update so roll up has these other trust assumption that you need to trust that someone is watching the chain either that has to be you or that has to be someone else and zk rules don't really have that this assumption and then if you are using another system or another component to provide data availability then you're always exposed to the underlying risk of that system we used to say that validiums for instance avalidium is a zika robot that does not actually post the data on the layer one um it uses another another entity or a federation of entities to guarantee that that data is there we used to say that that these validiums had a lower risk profile because the only thing that they could do is freeze the funds they could freeze the system but they couldn't uh steal the funds but we've sort of come to the conclusion that you know if you're able to freeze the if you're able to freeze a roll up it's very easy to turn this into a random situation where you're ransoming the funds and uh and even if you have emergency keys which allows you to roll back the state of the ability of the validation that opens up another attack vector which allows the attacker to double spend their funds because when you roll back the attack then no one can then the attacker can just do that attack again so you create a double spend risk um so no matter how you want to wrap the burrito if you're using other data availability systems you're always going to run into this issue that you sort of have to trust that those components are functioning correctly so i also have this picture i saw that mustafa had this one earlier i want to make a joke about this this is the modular conference so we're outsourcing other people's uh diagram skills to the ones that are most suitable to create those so that's my excuse for for using other people's diagrams in this in this talk um so we sort of we sort of covered covered everything except these two so far we're talking very very broad stokes but i don't think that you're going to be able to to learn everything about every single trust assumption between different roll-ups in in a 30-minute talk but let's talk about the broad strokes here so if you're running a roll-up on celestia there is a difference between sovereign roll-ups and settlement roll-ups so the sovereign roll-up has its own settlement and execution environment so that allows for a lot of flexibility because you can hard fork or soft fork the roll up itself and change the rules of that roll up without having to depend on any other community celestia is not going to stop you for changing the execution environment of your roll up so if you want to change something of index inside the execution environment you can in a sovereign roll-up that gives you flexibility that that's that's why they're called sovereign roll-ups uh but that does mean that uh the roll-up can change if as long as the roll-up community decides that the roll-up should change and that means that you know if it's a small community if it's a small roll-up and they are malicious they could hard fork and perhaps you know steal your funds from that from that roll-up so you you have this different risk profile in a sovereign roll-up versus a settlement roll-up the the benefit of a settlement roll-up is where you share an execution environment where many different types of roll-ups uh can settle their fraud proofs and validity proofs and if they are all batched together in one layer then one roll up can't go rogue and hard fork their system because that makes them incompatible with all those other rollups that they have enjoyed the ability to to to interact with so if you're bridging if you're using a settlement roll-up for the the uh compatibility with other roll-ups now you if you can't hard fork one of those so it sort of ties all of those roll-ups into that specific settlement layer so using a settlement roll-up with many roll-ups sort of batches their security together um in a different way um last thing i wanted to steal another uh diagram here from from maven rain and coffee made this excellent chart i think that shows a little bit easier how these different roll-ups interact with the celestia system so on celestia you can have these you can have a sovereign optimistic roll up you can have a sovereign zk roll up uh and then you can have these restrict restricted settlement layers for rollups uh like sevmos but you could also have other other other uh settlement roll-ups like that on on which you can build other roll-ups that sort of gives you a better picture of what the ecosystem looks like i think and if you want to get really bogged down into the very nitty-gritty differences between an optimistic roll-up and a valet validium or a zk roll-up i think that matter labs did a quite good deep dive into the nitty-gritty differences in terms of security but also performance usability and other aspects that you can look at so i would recommend to read uh from this website it's not 100 perfect you got to realize that matter labs uh they have a zk roll up themselves so they might be a little bit more biased towards zk roll ups uh with that that was the end of my presentation uh thank you very much great to see you guys so many of you here [Applause] i don't know if we have any time left for we are out of time all right but thank you eric that was awesome cool um well i'm very glad that uh eric recovered from his maximalism because um in celestia and in the modular ecosystem we say modularism not maximalism so i think he's now in the modularism camp which is good to see the next panel is about modularity emerging in different areas of the blockchain ecosystem so not just modular protocols like celestia or rollups but also modular bridges or modular software stacks like the cosmos stack that zucchini was talking about earlier today so without any further introduction i'm going to bring on our panel guests so we have ismail who's cto of celestia he's a moderator we have marco who is the head of cosmos sdk um we have james prestwich from nomad beau from polymer and of course alex north from uh protocol labs so give a round of applause [Applause] hello can you guys hear me yes um so why don't we start with a brief round of introductions michael uh yeah i'm marco i work at uh interchain gambeha and i'm the current product owner of the cosmos sdk i've worked from tendermint all the way to the cosmos sdk so just having fun with it i'm james prestwich i'm currently cto at nomad which is a modular bridge focusing on smart contract chains and arbitrary message passing i am beau i'm co-founder at polymer labs and we're working on a universal ibc interoperability hub hi i'm alex i'm an engineer at protocol labs i've spent the last three years also working on filecoin as a software and protocol engineer thank you so mustafa made this very clear distinction this morning what like a modular blockchain is and what like modular software is and where the distinction you drew a clear line so um let's let's also like briefly summarize what like modular design in general is like a modular design means that you basically break up the system into smaller parts so-called modules they can like be used in other contexts that can be um changed or yeah basically can plug them into other systems as well and then modular software or like modular programming you uh basically subdivide your program in modules uh where the modules only execute the part that that um that you care about so and yeah modular blockchains is when you basically outsource core functionality of a blockchain i think that was made very clear throughout the day so i wanted to ask the panelists like where do you see like modularity emerging basically in your like in the in the fields that you work on like in the cosmos sdk i think zaki took most of it away but uh maybe you have more for us um yeah unfortunately i didn't see zacky's talk so i don't know if i'm going to say something wrong now but the cosmos sdk was always built or the cosmos ecosystem was always built with the mantra of like there'll be many chains in the ecosystem kind of this interchain world with ibc and so the idea of like modularity in a framework to rebuild to build new blockchains and make it faster to scale has always been at the core of cosmos and so with the cosmos sdk it's evolved over the last it's been around for like six seven years now and now at the most latest i think it's the most advanced and it's truly modular now we're seeing chains pop up in the cosmos ecosystem that go from like inception and then one month later they have they have mainnet and we really haven't seen that and building a modular software stack allows us allows people to accelerate their products thank you right now in bridges we're seeing a lot of modularity in the separation between communication channels and the applications that they run between how you actually pass data between chains and between what's done with that data like token bridges nft bridges governance systems uh one of the things we built that nomad that we're very excited about is a strict interface for how a communication channel connects to an application and this should let applications like bridges swap out ibc for nomad channels for anything else they want in general we think of modularity as finding the right place to separate a concept into two smaller pieces and finding the right interface for that cons so finding the right interface for that separation and defining the contract around it how people use it and what they can rely on yeah that's a polymer we see obviously as defining those interfaces like james said uh in a really nice way kind of separating the like transport authentication ordering layer from some of the application semantics you can kind of see it in the interchange standards there's clear distinction between the two layers they have a number of specifications that they review by the community and across different organizations and one more thing i would like to say about what mark was talking about in terms of the cosmos sdk is that i think the cosmos sdk does a really good job of making the components so modular you can replace certain components like with what celestia did was able to do come and replace tenement core with optimum and basically do what they do and be able to innovate and that's a really important thing that i see here is that it allows different organizations to come together build together and innovate very quickly uh yeah i mean modularity of software is very core to how protocol labs tries to build things uh ipfs for example has a sort of strong brand as a thing but really ipfs is a collection of protocols and components that are put together a data model in ipld uh some network transports from lib p2p a few different ways of exchanging uh information uh and so really we don't think of there being a protocol as ipfs but a node can participate in an ipfs network uh if it if it uses a similar network protocol and if it adopts one of the transport protocols one of the data exchange protocols uh that you know that one of our ipfs nodes will use um and then similarly those pieces can be taken by other by other projects and so other projects will usually p2p for their networking that automatically puts them a step closer to interoperating with other projects that do the same thing uh and the ipld data model again is not specific to ipfs and is a foundation for uh other applications data models thank you yeah so um in the so the cosmos sdk was mentioned several times like in the cosmos sdk and particularly in tenement do you see anything like that is currently more monolithic that should be more modular marco um thankfully no one here is from who works on tendermint core i'm very opinionated on this uh tendermint was always built as a library so kind of when jaquan and ethan buchmann wrote wrote it the idea was you have um you have consensus you have p2p and all these things but it's like if someone wants to switch out consensus like from tendermint consensus to like hot stuff it should be easy enough to do it it was always meant as like a consensus framework a consensus library and over the years building modularity is very difficult especially at a low level because you're kind of making assumptions of how that application developers will use it how the clients will use it is somewhat easier for application developers the deeper they go the more freedom they need in developing their software and so at the lower level tendermint has kind of shifted more less from like being super modular to being a bit more opinionated because if you make too many uh opinions or you don't make that many opinions then you it's it becomes really hard to build software um but intended in general like i just wanted to be more modular i wanted to be to switch out p2p with like a click of a button instead of having to fork the code and rewrite some stuff yeah that's that's what we do at celeste um so one aspect that was also mentioned several times is like ibc um is that also would you also describe it as modular or is it something specific to the cosmos ecosystem only like maybe james uh i really enjoy working with ibc uh it is modular in a sense that it can be applied to other ecosystems i think that the amount of effort it takes to reach other non-tendermint ecosystems right now is very high uh and so so actually it's actually a lot easier than we than we first thought okay uh i'll talk to you after that i'm interested um you know it took a number of years to develop ibc for tendermint in the first place uh it's going to take a little while longer to productionize it outside of the tendermint ecosystem uh which and you know in like the eth2 model where they have a proof-of-stake system that admits some reorgs we have a lot more work to do to parameterize ibc and make it work for the casper-based consensus system which is why we focus on not just the ibc connection but what applications can we build today that can be eventually connected by ivc once it's ready and what channels can we build that can be used today that can be swapped out for something more secure or faster once it's ready uh this is one of the great parts of modular design is it allows us to go you know build scout modules that go fast break things uh colonize new ecosystems not colonized but uh explore explore that's a good word translator tracer wander around new ecosystems find what works and then we can bring in these heavier and you know better things like ibc in the long run once we have figured out what an application developer wants in a specific chain ecosystem modular modularity allows progressive development and upgrades over time let's talk a bit more about bridging and ivc in that sense so modular blockchains does it make your life much harder as a bridging provider or like as a as someone building bridges thinking about bridges um i think so sorry you can go i think so i think there's more considerations to make for example like when you separate consensus from or perhaps execution from the data availability and consensus layers now you have to say like well on this and we have like data availability headers on this other end we have state proof headers we have to somehow combine these things when we want to you know perhaps produce a fraud proof um but yeah i think there are more considerations to make and how all the like pieces fit together it's not fully clear yet but i think i think we're going to be able to figure those things out yeah one of the interesting challenges is that the bridges like ibc which are kind of the ideal most secure model you can get they mix the execution and consensus layers they have to cross that boundary they can't be confined to one of the others the bridges that we work on and that can be iterated fast they stick in the execution layer they don't really touch the consensus systems so modularity might make this more challenging to reach you know the long-term goal of having these consensus layer bridges everywhere uh because there will be more consensus modules that we have more execution modules which will also play into these kind of bridges um yeah let's let's let's take a step back and talk more about modular software as well so um one aspect so first of all do you see any like downsides of building building your software more modular um maybe alex or marco uh yes there's always a trade-off i think to building something more modulary um to build a module to be reused by others is in a sense to take on an obligation to serve others needs and you don't know who they are when you start building uh and they don't know who they are when you start building but eventually you'll have to uh in order to serve some needs well you'll have to optimize towards those and necessarily be less good for some other use cases and so at some point someone else coming along with a new novel application uh wanting to plug it together out of modular pieces will find that they just don't they they can't get the performance or the features that they need uh and so at the other end of the of the stack is sort of vertical integration and there is a great place in the world for vertically integrated software and vertically integrated systems you know car manufacturing is probably a you know timely example now where for a long time uh car manufacturers were outsourcing and modularizing and buying and putting together components from other lower level manufacturers and there was this old joke in the auto industry that you could see the org chart of the car company from the layout of the dashboard and you could see like there was a climate control team and a radio team and a navigation team and so on but then tesla came along with entirely new needs and so they are a vertically integrated car company and they make almost all of their own components and they use their own manufacturing processes and can produce a different thing uh at a you know at a price and performance uh that could not be met by stitching together the modules that existed beforehand and so um i wouldn't be at all surprised if we see a continual sort of back and forth between modularity and vertical integration yeah in five years time maybe that we will be at the vertical integration summit uh about all these special purposes this thing to kind of add on to that it's like when you're building modular software you're also providing a guarantee of like security and kind of the assumptions you made when building the software but in especially like you can't predict what a developer will do when they use your software and so you providing a guarantee for this software and then maybe them taking and changing a few variables kind of breaks that guarantee but for them it's like if they say we are using the cosmos sdk and we got hacked it's like no you guys are using a fork of the cosmos sdk that you guys like rewrote a bunch of stuff like this shouldn't really fall on us because cosmos sdk we provide a security guarantee of the software that's in the code base yeah you kind of have to teach developers not just about the system that you built the module itself but about the interfaces and the boundaries of that module and what they have to do to keep the module working with other modules correctly uh so there's a lot more developer you know mental overhead for these things yeah and like you end up talking with a bunch of users like how are you going to use the software and i think in the current blockchain space in the past year or something we've kind of run into this cycle of like we're kind of rinse and repeating many similar products and then so it's like you build this you build your modularity to like serve these products then you can also get a team that like does does something out of the box and you're like okay wow this is where we want to be going because this is like pushing the boundaries this is like something that's next this is the place that we're comfortable in right now yeah i think that brings up an interesting point that i want to connect to alex over here is uh we always like start off building bundled projects like everything's integrated in ethereum and it's because we just didn't know how to do any better at the time like we built ethereum the way we did because we had no idea what modularity was or what the right boundaries between different parts were or where the bitcoin is even more extreme to that like it's more monolithic like more integrated yeah we built it monolithic because we didn't know what the right module boundaries were now that we know we can build modular systems and then eventually once we have modular systems everywhere people are going to want to go back to monolithic to eke out those you know those little gains they can get by eliminating the edges between modules yeah it's a typical problem yeah exactly like uh in in web development we've seen it go from monolithic services to microservices and back to monolithic because people get frustrated with whatever the current model is yeah that's basically what what alex also said previously there will be a constant back and forth yeah we're in a great space now where they're like it's it's all about the pace of innovation and modularity is fantastic for you can quickly plug together a few things and maybe take some trade-offs but you can learn really fast but once we really know what we're doing um you know james is right we didn't know what we were doing to start with the ipfs is just the same lit bitter p didn't birth as an independent project it was extracted from ipfs and so were some of the other protocols over time and then we then we realized what we're doing and what we want to plug together and then when someone really knows what they want to do they're going to go and do it from scratch in a vertically integrated way again when they're going to sacrifice the ability to change it later yeah and we we had to do it that way because we didn't know any better totally we had to experiment with it first as a monolithic thing to figure out the right way to build modularity so some things some theme kept popping on the like recurring theme that was mentioned on the panel is like the interaction with users and people using your modules potentially different in like different ways that you didn't anticipate maybe so um one thing that i wanted to ask and like i don't have an answer to it at all myself but like how do you basically uh maintain these modules from like a governance point of view like it's it's you you have these potentially have these like you have tendermint maybe you have the cosmos sdk and its modules and you have something like lip p2p which is widely adopted across the the blockchain space or like in another non non crypto projects as well it's like how do you make sure that this is some like is it is it like in any other open source project where um you you basically maintain it and you discuss things on github and this is like you you submit rfcs and you discuss and you discuss and eventually you come to an agreement and you implement this or that feature or this and that addition to the protocol is it exactly like this or are we in a space where i feel there's like all these projects that are kind of like collaborating using the same software using the same open source software and they at the same time collaborating but at the same time somehow also competing it's like do do we need other means of governance and do we do we need to align incentives here or is that something um that that is unnecessary i know i know it's like a tough question i could probably do a whole summit about this topic but i i just want like off the top of your head like what what what what what are your thoughts on this particularly marco and uh alex maybe um i'm gonna have to think for a second yeah sure as far as i don't have anything that says it's it's different to other open source software yet maybe it will be maybe we'll learn about something uh or perhaps some better way of doing things but i think it basically comes down to open source software development and most of my ideas here i'm like shamelessly parroting things uh from nadia igbal and her book working in public which i think really sets the stage for the discussion here um software the code itself once it's produced is a public good it can be taken and used by as many people as possible with with uh you know marginal cost but software development and the the software development resources the people and the time and the attention is a scarce resource that is competitive and so it sort of comes down to discussion about at what level do you want to do you want to be open and decentralized and permissionless it doesn't work for it doesn't scale for a team building a critical software component to accept contributions from any from everywhere to answer every question to allow anyone to have uh their say on how the software should be written uh that the team will immediately grind to a halt and be unable to make any progress on the things that are important but we can sort of add the robustness add the anti-fragility back at a higher level by agreeing perhaps on a protocol and then having multiple implementations of it so there are multiple implementations of with p2p and they agree at the protocol level but then are free to make all of their implementation decisions independently uh and so it's robust in the sense that uh individual software libraries can fail most open source software libraries go nowhere they're you know pet projects they don't gain traction they don't go anywhere and it's fine and that makes the ecosystem of open source software incredibly robust because individual project projects can fail on the way to us discovering uh and supporting the best most useful projects and then one day forking them when they you know when we need different different different needs from them so at least in the in the cosmos space um we're starting to get to the point where we have to put a lot of like parameters that are kind of like feature flags inside modules for things that are changing to replicate the similar behavior in the in the early days when the cosmos have first launched and there's maybe a couple other chains it was easy to coordinate like we go talk to these three chains do they want this change some of them put it to government some of them don't some say it's fine and we go forward with it right now there's roughly 35 chains and if we introduce like a consensus breaking change that they may not read the change log for and all of a sudden have this irregular behavior that they weren't expecting causes a huge huge problem and so it's like feature flags are kind of like the worst thing you can do in software and just adding a bunch of them just because you end up having to maintain so much extra software that you don't know how many people are using but in an environment that you don't know how many people are using your software because they may be rebranding it as their own or they may be like um forking it and saying something else then it's kind of hard to be like okay we're making this consensus breaking change if you don't like it like you have to you have to go do it on your own because we want to also avoid people just forking the repo just to do little changes yeah this is a little embarrassing for me because i have an unmaintained cosmos [Music] that needs to be updated for the breaking changes exactly i'm feeling a little called out so but you don't think none of you think that there needs to be basically like some doll structure or something like that in to to maintain like because it's like there are these there's always like in this space there's always these incentives right like you have stakeholders you have maybe like very like even competing projects that use your stack right yeah it's it's a super hard question to answer and in terms of like governance in like the cosmos sdk land and tendermint i it's unfortunate to say but there's like a finite amount of people who actually understand the protocol and there's a lot of people who use it and so you if you involve end up involving all these people who just use the software and maybe like they write applications with the cosmos sdk then never dove into tendermint and all of a sudden you have to involve them in like tendermint discussions on like oh are we changing um to like lip p to p or are we changing to this are we changing to this then the process gets dragged out like alex said like you end up hitting a wall and then the spec keeps moving and the developers kind of get burnt out because it's just kind of like you're trying to implement a moving spec and then it just becomes an infinite loop so it's like you don't like democracy basically but yeah yeah i got i got the point and it makes sense but like it's a it's a recurring thing and you talk about governance yeah sorry oh i was like i think we're talking about interfaces and layers and also governance and consensus over those interfaces and layers and alex made a really good point about if you define certain interfaces for example with the cosmos sdk you have the avci it's evolving to the abc i plus plus there's some interface changes at like the o2 client level to support uh the development of different light clients i think the idea is that you want to you know have some sort of consensus over these uh layers and how they're defined and then be able to have protocols come in and innovate and make trade-offs beneath between the interfaces themselves make sense do we still have time for like short a few questions maybe from the audience as well if there are any i always ask a few questions uh before and i was like keep them for the keep them for later but uh not sure if people are asked to hear any questions there's one over there where up there here we go people are running away you can introduce yourself hi i'm max um can you comment on maybe there'll be an overlap in the future where the communication mechanism the bridges overtake the actual chain uh a future where the communication layer in the bridges overtake the chain so we're looking at modularity especially like with celestia where it sort of enables cross-chain applications would you see that the bridging technology has become more dominant than say on-chain activity colonization um that's an interesting question uh typically the bridging technologies can't provide what users want out of a chain users want to store their assets to you know trade them to lend them borrow against them all of these things that bridges are poorly suited for we the way we look at this future is that bridges are going to be asynchronous communication channels between applications that are honed on individual chains and i don't see that changing in the near future i do think that there will be some bridging standards that are just on every chain implement the same standard in solana rust and near rust and polka dot rust and solidity and everything else but i don't wouldn't say that they would dominate the chains they might just be widely deployed on chains another question um more questions there's another one over there hi um my question is more to the interoperability rather than modularity of parts we're seeing now a lot of the cross-chain protocols are trying to build essentially similar um cross-chain messaging protocols as ibcs or xcms for the substrate do you see this becoming they're becoming like a standard to allow them to interoperate or it's more of which ecosystem wins out ibc is the standard ipc is a standard tell this to the xcm people you know we've talked with the web 3 folks um and the polkadot folks about ibc uh we're working with a couple teams implementing ibc uh ibc on substrate via grandpa like lines and [Music] mountain there's some new consensus that they're doing um and so we're working with people who are implementing both and the idea behind like cosmos and ibc and everything is just kind of like interoperates with everyone because it's like a connected ecosystem of blockchain not of a specific like smaller ecosystem just makes everyone better and so in a way lift everyone up ivc includes a lot of different parts of this bridge stack as well so when we talk about ibc we have to distinguish between which icss we're talking about which touch different parts of the modular like chain stack and bridge stack so some ics's will be the standard and some of them won't make sense in specific chain environments uh so thinking about this a little more to answer max's question i think a little more in depth you could look at ibc as an example of the communication standard dominating the chain and that cosmos chains are designed the way they are in order to support the current set of ics's like cosmos zones are designed to participate in ibc and that is one of the defining most important elements of cosmos yeah so you can use it off the shelf without adding another ibc client that makes sense yeah there is uh i think another question over there last question yeah okay hi friends uh so basically i i think like sales like modular stuff plays out um two years from now what are your thoughts on what this like post modular world looks like what things are we going to like focus on is more like application layer stuff um yeah it's a great question to end the panel all right start um i i mean to step back and like look at why we're here uh we're trying to build the reason that blockchains and execution layers and so on are interesting is because we're providing a platform for applications to be built software for people to use to achieve things and in our case ideally things that are cooperative or collaborative in some way so there's a there's a big there are a lot of modules to be built out uh in this uh platform you know with a nod to mustafa's attempt to claim the definition of modularity earlier today um in a your development platform needs you know the execution layer the cpu in the computer and it needs some ram which is the state and it needs a you know i o bus which is the the data availability layer and also needs storage which is file coin and it also needs uh networking talking to other computers which is bridges and there are lots of pieces here that need to all come together into a platform and i think we will see start to see some standard ways some standard groupings of these modules in much the same way as like the ibm pc defined uh a standard platform much to ibm's detriment uh uh but which then provided then provided you know the green field for application developers to go nuts on this platform that was well understood and widely implemented uh and so i'm hopefully in two years time from now that kind of platform is understood and we're talking much more at the application level thank you okay that was it thanks thank you guys yeah this way good job thanks cool that was that was really interesting um thanks for the good questions it's clear that you know modularity has a role to play in many different ways not just modular protocols and i think there's a reason why we're seeing modularity emerge across the stack because the the whole i think blockchain ecosystem is starting to mature so that was very very insightful thank you again to the panelists our next panel uh is going to be about how we're scaling execution layers using optimistic constructs so earlier in the morning we talked about zero knowledge roll-ups now we're talking about optimistic roll-ups so leading the the panel is john uh who is from delphi and we have uh proto who's the highest iq pineapple on twitter and josh who's representing celestia and emily who's representing fuel so give them a round of applause mic check check how's my voice can you hear me nice um all right so um why don't we start with a brief intro of ourselves and um i'd like to also hear an short elevator pitch of what you're working on your project i'm sure many are familiar but it would be nice to have your own definition of it so i'll start with myself i'm john from delphi digital we provide cutting edge uh crypto research and we are strong supporters of this modular paradigm hello i'm proto-lambda i work at optimism as a researcher um we have optimism we're working on a new upgrade called bedrock and this simplifies the protocol by modularizing by taking apart the roll-up logic from the exclusion logic the vinod's gaf and uh as part of this we also have this new fault-proof deck that will basically secure the withdrawals from the roll-up to the later one all right hi i'm josh i'm an engineer at celestia labs where i lead the sevmos team um so i'm not participating on the l1 data layer of celestia i'm working on if you recall back to most office slides or eric slides the sevmos section which is a sovereign roll-up focused um for settlement so for roll-ups to actually settle onto and as part of that we're using fraud proofs to um actually kind of guarantee the security of that software roll-up i'm emily i work at fuel labs and we're building fuel which is the fastest modular execution layer and we're building that using three different principles that's utxo based parallel transaction execution the fuel virtual machine and then a really superior software development experience thank you guys um and emily so yeah so i think um like the the now that we're in the role of paradigm the difference between fraud proof and validity proofs those are kind of overplayed and well known um by a lot of people but i think um what's under discussed is how dispute resolution mechanisms fraud proofs and particular implementations of those differ from each other so i'd like to start with that and have your opinions on like what are some design choices you've made when um building these um dispute resolutions mechanisms and if you may uh if any uh can you point out to some like unique aspects of your implementations of dispute resolution right so if you think of the early rollup design everyone approached it starting with the fraud proof and then later looking like what can we fit in into the execution i think this really hurts ux over time you've learned that users want some like they want on ethereum at least they want their hearing features so they they're looking for an epm or they're looking for some specific execution environment without workarounds and so to get rid of these workarounds you have to approach it differently you approach it with this this type of vm that can run arbitrary code maybe it can run something else than optimism and then do a fraud proof over this this more generic binary of instructions yeah so i kind of view fraud proof there's a relatively more limited design space i think even than like zk roll ups um because you really have two ways of doing a fraud proof you're either going to have re-executing a transaction or you're going to have this like bisection interactive verification game um for us we're just using re um re-execution of a transaction and that's you know our kind of first attempt here and that's for purely practical reasons you know we're building fraud proofs into the cosmos sdk the modular software architecture that cosmos sdk provides that you can have consensus on one layer and then you have your state machine on another layer and so it's a relatively clean software design to just add that execution layer that state machine cosmos sdk layer and say okay now light clients just have to have the ability to take your fraud proof set up the state the pre-state that you give them right from your state witnesses and then actually re-execute the transaction just as any other cosmos sdk based execution layer would do it so from it's just an engineering practicality question here at fuel we use a hybrid approach so on one hand we use a utxo based fraud proofs and then on the other hand we also use interactive verification game fraud proofs that are at the vm level and we combine both of these to produce an outcome that works for our execution model and we feel that this offers benefits because it means that we only need to necessarily rely on say a specification and other designs might rely on different technologies like interpreting to wasm and then doing fraud proofs with wasm and those technologies would then rely on the limitations of what the wasm team wants to implement for the language and at fuel because we take this hybrid approach we're just relying on the specification and that gives us some freedom interesting uh i didn't know that about fuel um so just to like move on that um how far are do you think uh the as an industry we are in like uh fraud proof design space do you think like in near future do you think like many of the problems are figured out and these like implementations are going to like converge on on one design get more commoditized or do you think we will see more and more like some different implementations getting divergent like some different flavors of of solutions here so in terms of research i think we're getting there but in terms of products there can be so much more so when you think about all this new tech that enables fraudproof offer some arbitrary execution you could do a lot more than just an evm or just some specific type of smart contract vm and so think of indexing services or other types of things that take layer one data and do these very expensive like huge computations and then just prove that the execution is correct this can be very meaningful to say adding an indexer to a chain that wouldn't otherwise be possible with a smart contract yeah from my view i think we're pretty far along in research and like what i mentioned before right we have these two modes and you can kind of figure out how to implement those modes from an actual like implementation perspective i think it's a wider range because if you want to re-execute transactions right you just pick that mode of fraud proof then it's completely depend on what execution environment you're using here right if you're going to use an evm execution environment you're going to have to have the ability to re-execute evm transaction same for cosmos i'm same for fuel i'm assuming right where you're boxed in by what your execution environment is and so in that space we're actually pretty narrow in what we've researched because quite frankly you know blockchain right now we have a pretty minimal number of general purpose execution environments i mean look we have like solana we have cosmos them we have fuel we have evm right those are kind of the big four that cover most of these things and then we have cairo right but you know even cairo team right is working on an evm transpiler so we're very small number of execution environments and that's going to actually drive what kind of fraud proof you need to be able to generate right right yeah i think um we're going to see some general theoretical ideas shake out and become more standardized but then in terms of the actual implementations i feel like those will be specialized to the tech particular technology stack um got it yeah so we basically cannot uh like consider fraud proofs in isolation from the execution models uh i i i think you everyone would agree with this um so i'd like to hear your experiences and like get more familiar on like what kind of constraints um each pose on each other so you're fraught proof posing on on your choice of execution model and vice versa some what are some like constraints that you witnessed um if you could share your experiences here um yeah right so we started with the wrong approach we have a lot of constraints and over time we like kind of freed ourselves going for more and more generic fraud proof and so we're not being really a fraud proof over the ethm but rather we're doing a fraud proof of our mips execution you can target a go program to different instruction sets most commonly is x86 that maps is this simple instruction set that you can do a fault proof offer and using this approach you can compile the evm implementation from gaf to these types of this type of binary but it doesn't mean that you cannot compile another type of golang program to a binary to do a fraud proof offer so over time i really do think that we like we'll think and we'll start thinking outside of the box something that's not a regular smart contract vm and do fraud proofs are for more interesting things would you would you think mips could become a standard here um well so unfortunately at the time of development we didn't have the ability to target risk fi with go but by now in recent updates in go this risk fire instruction set is now supported so there is this more elegant instruction set that we could support and the differences are not that large it's the same kind of family of instruction sets very interesting yeah so in that vein right you know i mentioned like we're limited by like the execution environment we have to re-implement the execution environment and the kind of elegant solutions the optimism team has gone with you know and as i understand the nitro vm is somewhat similar that where they essentially take the the evm which is somewhat hard to understand virtual machine it's somewhat complex right and they compile it to you know this reduced instruction set computer right you know which is mips and then risk five would obviously be like the ideal more modern one but it's just not quite kind of ready yet from like an infrastructure perspective right and that simplifies our problem we still need to be able to have the ability to deterministically like execute this thing but then you're reduced to kind of a simpler problem because really what you're trying to figure out with a fraud proof right is how do i execute the minimal amount of state transitions and give essentially the smallest amount of state to i'll say my like client right and then allow them to do the smallest amount of work to prove that i gave them an input and an output and they did an execution on that input and they got a different output right um and i think we'll see this general purpose fraud proving hopefully kind of become the standard and then what we have to see is you know this kind of software architecture on light clients being able to execute these you know general purpose things right where right now you have to run a light client for your optimistic roll-up that has to be able to run this mips vm right and it has to tie into that execution for us you know like clients in a cosmos chain normally don't have any execution part in them they don't have a state machine we have to attach a state machine to them and have an ability to start up this state so i think we'll see it at this engineering level of how can you easily tie in to these optimistic roll-up chains to actually do more general purpose execution of a risk you know risk five or a mips based you know fraud proof i see i see yeah so i touched on this a little bit earlier but because at fuel we have this hybrid approach it allows us to be based on specifications instead of being based on running fraud proofs based on say wasm or something like that so i feel like in general the fraud proof schemes that we'll see are going to be tied uh to execution right yeah and so so in your hybrid model would you say that it relates to your like data model like you the utxo model and and like would that be relate like your choice of a hybrid fraud proof would that be related to your choice of like utxo model or those um irrespective of themselves um i would say that our choice of doing a utxo model is what helped us choose to do the hybrid approach okay yeah i see so [Music] um and so a lot of focus on on the roll-up space is and rightfully so is focused on um reducing this l1 footprint right when we like move from uh single single round uh fraud proof to to to a bisection game we try to like simplify the the basically the problem to a single instruction set that should go on chain so this reduces the l1 footprint but when we think about scaling as a holistic thing um really the the l1 footprint is a part of it and arguably it's not the it won't be the biggest part of it in the future um with with solutions like uh like dank sharding like with celestia and like um so um uh a part of scaling is obviously the data model and execution model so what i'd like to know what are some optimizations that you do uh in this respect in your particular implementations right so i do think optimistic roll-ups are in favor here of seeker roll-ups where if data duke does get very cheap then we reduce our costs by basically 100 percent like this this is our primary cost execution only happens during the fall proof in the the unhappy case so in the happy case you'll get very very cheap transactions and like in the best world you would combine seeker roll-ups and optimistic roll-ups if we had like a zika proof for arbitrary computation like we can do from the throat proofs they can lag behind the optimistic rollup and reduce the dispute periods by showing a validity proof to basically confirm the execution um short term i think the optimistic roll-ups are more powerful here with better arbitrary execution and uh so i'll have to deal with the dispute period for now yes i think answering your specific question of like what are we doing to kind of specifically optimize our thing outside of the data layer you know as you know a project inside of celestia we're not really focused on that right now you know we're taking a somewhat naive approach to these roll-ups to these optimistic roll-ups and the fraud proof where we're just trying to generate a fraud proof and we're assuming that the size will come in at such that we can post it on our data availability layer and we're assuming our data availability there gives us cheap enough data that we're not too worried about that i think one of the interesting things is as data becomes cheaper right you know as proto mentioned like optimistic roll-ups start becoming more favorable because they have a higher cost of you know a quantity of data that needs to be posted on the l1 as an optimistic roll-up right um and when that kind of problem becomes better like the actual cost of generating a fraud proof also or the complexity of that can become simpler because in this generic you know execution you know the the nitro vm is somewhat complicated to understand the bisection game it's much easier to explain to someone oh i'm just going to give you a pre-state you're going to do an execution you're going to get a post state the reason you might want this you know interactive verification game is because you can get a smaller fraud proof and if your data is expensive you know you could touch a lot of state in this fraudulent transaction you could have a very very large fraud proof theoretically too large to even post in one block right and that's kind of an unacceptable um situation you know that's what the arbitrary people were talking about right they can cover fraud proof for transactions that are essentially larger than the total block space of ethereum but if we start having larger blocks on a data layer or really really cheap data you can kind of remove some of the optimizations actually in the fraud proofs because the assumption is you're not frequently posting fraud first if we're posting fraud proofs like once a day once a week once a year something is kind of wrong with the economic incentive system here right so we can actually get simpler things and less optimized systems that are then easier to understand easier to audit and verify if we get cheaper data right yeah so in terms of optimizations there's really two things that come to mind for fuel and that's one we have the utxo based model that allows us to eliminate the need for a global state merkle tree and this allows us to scale quite generously and the second thing is in the actual implementation every contract has a corresponding address and then to refer to these contracts you refer to them using an address and this allows us to take advantage of some of the properties that having a global state merkle tree would allow you while still meaning the utxo based system where we don't actually need it thanks for these um yeah i'll have one final question and then i'll open it the panel to audience questions so um oftentimes in this crypto projects there's like misconceptions so what would be can you think of a common misconception related to your project or um optimistic execution in in general um if you have any if you don't uh feel free to pass this one well this is a spicy question everyone thinks different things about rollups i think in general like you could say there's a misconception that um non-interactive or interactive develops one is better for some weird reason in the end i do think it really depends on the dispute periods like if you have this kind of dispute periods then you'll see differences or if you don't have this dispute period you'll see differences so like in the sovereign roll-up model or things closer to celestia it will actually matter to to be able to do a non-interactive fraud proof whereas on ethereum mainnet you always have a limited epm and having interaction is actually a good thing because if you have a week anyway to do like this anti-censorship throughout proof like if you take the time then you might as well use that to reduce the the cost on layer one so when in the case where like the the fraud proof is distributed over the p2p layer am i getting yes if you want to be off-chain you definitely want to be non-interactive right or something close to that whereas if you want to do this on-chain interactive is actually more optimal makes sense yeah so for like the celestia model if we go into like the l123 right we're deploying that settlement layer which is itself a sovereign role so in this way we're boxed into using this non-interactive fraud proof because we don't have a place where a smart contract can live that would essentially mediate this interactive game um but to answer your question on like misconceptions i think there's a little bit of like this assumption that like a 14-day like latency window or whatever to like um the verification um or the the trust of the um roll-up transactions like some people don't know i think that that's like a completely arbitrary number that was picked like there was not there's not good backing research on like this is what happens if you extend it to 21 days this is what happens if you shorten it to seven days like people just picked like 14 days and that got good enough that seems fine right but we haven't seen like a live network generating large quantities of fraud proofs to see like is 14 days actually long enough and if you think about the amount of transactions going through you know these networks right we're in you know if we look at like stark where like we're in like tens to hundreds of billions of dollars of transactions the reality is you just need one honest full node in the network to be re-executing all transactions they should be able to find an invalid transaction this latency window doesn't really need to be that long you know for you to have you know a pretty large set of people that are you know economically incentivized to check these transactions like you could probably be you know order of like days or or even hours because like these liveness fails you'd have to have the assumption of there is no honest watcher in your network executing the transactions but you're still producing transactions it's just not a real world situation in like the modern world of how trivial it is to spin up software if right the thing that can generate a fraud proof is open source software if all of them are down on the network and you have a 24-hour window there's like 10 cloud services you can go to at any hour of the night buy a vm download the software and execute it against the chain in that period like i just don't see liveness issues being something where we need 14 days to resolve this right so i think the 14 days number or seven days for some uh it really depends on the censorship ability of the against the fraud proof so if you have a fraud proof of chain you have a very different model than if say if you have a fraud proof that depends on miners including the the challenges yeah and i guess so we're biased you know in this sovereign roll-up environment right where we have a relatively easy work around to the censorship resistance because you can pass it to the data layer and the assumption is that the data layer is not necessarily incentivized to you know sensor transactions for um you know your roll up above it obviously that's still you know an area where you can have censorship if there's sufficient money to be gained by censoring right yeah but good color i i i like the angle that you you bring on this emily do you have any um yeah so uh speaking particularly about fuel i feel like there's this misconception that fuel is an optimistic roll-up first when in reality it's a modular execution layer first but i don't think that this is a problem that's specific to fuel i think because modular layers and the modular blockchain is this new exciting thing that's becoming more prevalent in the space that we are knowing and working with um i think it's there's there may be misconceptions about what the definitions mean and i think that's why days like today are very important so that we can get excited about it and bring everyone together with the community so um if anyone have any questions now's the time questions don't be shy no one this shows that fraud proofs are already answered we've already solved all the problems all the problems i think there's one oh here we go uh thank you for for the panel uh i have a question here i was asking fuel ups folks uh on the background but still question for you all let's say i have a celestial like client on the phone it's just a disability layer okay and there is a dex deployed on a full node basically unit swap and i want with my phone to take like to plug any execution layer and talk to that thanks is it do you think it is viable in this year or a year after or it's just like a real after five or six years thank you so i guess when you say a phone here that's a relatively nebulous thing because i i don't think hardware limitations of phones right now are going to be a limiting factor i mean like what the hell does like an iphone 13 have from a processing power perspective that's insufficient to do validation or or execution of transaction um i think your actual problem is going to be kind of from a software perspective like can i get my software onto ios probably not can you get your software onto android probably if you root the phone yeah then you can probably you know jam it into linux somehow but then actually an interest problem is you know something that i don't think we talk about that much it's like cross architecture things you know you can just transpile it to like arm but you do get layers of different architectures in here but i don't see any technical limitations for that i think we're into the limitations of apple and google control you know what 98 99 of all phones and they're relatively restrictive on the kind of software you can deploy i don't know if anyone's gotten blockchain execution nodes onto a phone i don't know where that falls into an app store category well so [Music] if you have a phone obviously you might want to run a full node but this is what light clients are for to reduce the resources so if you want to submit a transaction to a dax from your phone and if you want it to be confirmed on the data layer then there will probably some be some software roll up that will try and specialize in giving you like a soft confirmation very very quickly for the decks to have like a good ux and then to show you a proof that the data is being confirmed and like i think this is really just a it should be a software problem and i think this is like a question for like maybe a sovereign rollup panel or like basically like the execution layer more so than the execution fraud move tank we have time for one more question yeah we got one over oh they did work nice um so like i often think about uh like app specific chains as not really being like whole chains but just like a rollup that's built on top of like a celestium or or something like that and like each application will just run their own roll up and etc i wonder how much like you guys have like thought about that i think especially with the people that are building more like general purpose solutions yeah so that's pretty easy for us to answer as like like trying to do like fraudulent cosmos sdk right like it's just a cosmos transaction and as long as you know we can generate a fraud proof based off of like you know the state tree that you know we'll get in the cosmos sdk then it should be fine for absolute change i think that should go for kind of all of these it's just as long as you have the ability to re-execute a transaction for that app specific chain which really just involves a light client being able to load the state and execute whatever software makes up the app specific chain it shouldn't be more difficult than that right let's uh call it there so thank you guys um that's an awesome panel thank you so um we have a 15-minute break now so there should be some grab some tea grab some snacks hang out and come back in uh 15 minutes we have an investing panel we have a data availability schemes panel and of course the debate on monolithic or modular so see you guys in 15 minutes uh [Music] do um um um [Music] [Music] [Music] oh my uh do [Music] oh oh [Applause] [Applause] hey hey hey everybody we're about to start up um the break is wrapping up so please come sit down [Music] [Music] [Music] [Music] guys please come sit down we're starting up again [Music] [Music] okay guys welcome back hopefully you got some refreshments and you're ready for the last segment of modular summit so we have two panels and then we have a debate to wrap things up um so again the debate is going to be about modular or monolithic between mustafa the co-founder of celestia and anatoly the co-founder of solana so that's going to be really exciting so stick around for the end and of course we're going to have networking drinks starting at 5 30. so i think that's going to be some of the most fun time here because we all get to hang out and connect with each other so stick around for that and i can there's going to be a swag drop happening at 5 30. so more incentive to hang around and also of course there's the research track again don't forget it's cool stuff happening there tarun was just talking about d5 there's going to be some more technical data availability and roll-up presentations so i encourage you to check it out okay introducing the next panel is about investing in the modular stack so it's you're going to hear from a group of different investors who are really betting heavily on modular as a category um for their funds so um we have joe from framework we have matthias from maven we have alex from blockchain capital we have eli from polychain and we have will from galaxy digital so give a round of applause help me welcome up our panelists [Applause] all righty hello everyone uh i hope all of you have been enjoying today uh as much as i have i know i've been really looking forward to the summit and appreciate maven and celestia inviting us to speak today um but my name is joe i do investing over at framework and very excited to be joined by this group today as we kind of discuss our various perspectives on supporting the modular thesis through our work as investors in in the space so i think we'll kind of start just by kind of going around uh introduce yourself uh where you're from uh and then as well maybe the first concept or idea uh that introduced you to kind of the modular paradigm yeah cool um so i'm mathis i'm a partner with maven 11 and a part-time event these days we're co-organizing this so incredibly happy you're all here um i think i first ran into modularism or modular blockchain design when i ran into the lazy ledger paper written by mustafa in 2019 i reached out to john edler with some questions and we then helped bootstrap lazy ledger with their initial sheet round hey everyone i'm uh i'm alex i'm a partner with uh blockchain capital um i was first introduced to uh to the concept of the modular blockchain paradigm probably in 2019 just reading each research forum so the the first concept i i don't forget the specific project that i first read about but was the optimistic roll up so it was either optimism or fuel or arbitrarium hey i'm will um i work at galaxy digital and invest on behalf of the team there um obviously paid attention to the optimistic roll-up stuff in 2019 but i think the moment where the shift to the modular sort of approach really clicked was watching in like october 2020 when vitalik was posting about the sort of rollup centric roadmap and ethereum's shift from east 2.0 sharding to a to sort of like a data shard centric approach was the moment that it clicked that these like different functions of a blockchain are going to be split up and then started reading mustafa's papers at that point and um got you know started to work with the team around that time and then you know proceeded with that so cool uh my name is eli i'm a partner on the polychain investment team i think the the first time that i started thinking about kind of the modular stack i think it's hard for me to trace back exactly but it really evolved from conversations just with brilliant people in the industry uh one of them being james prestwich who i was kind of like trying to work with i guess when i was like a struggling or a mediocre developer just on interoperability uh interoperability um and so this evolved into this idea of or or this exploration of a trade-off space of you know really you have like composition which is like you know how do you compose different functions within a the same execution environment like how the evm operates today um and then you have interoperability you know and and the various kind of permutations of that with ibc or xcmp in the polka dot ecosystem um and then you know we we are also thinking about scaling um and so this this evolves from you know all these ideas around charting and how do you how do you appropriately communicate between these charts like how do you make them interoperable how do you um how do you compose these functions because that's a really interesting you know kind of thing to to you know discover with with you know flash loans or what have you um and i kind of came to the conclusion that like you know maybe the best way forward is not to you know put such an emphasis on on composability or or like kind of sync uh synchronous composability rather um and so you know after evaluating this for a while um you know you can you can kind of like parallelize it with shards or you can kind of compress or commit data in a roll-up and just after exploring a lot of these ideas um i i actually realized that what i had kind of discovered or rediscovered was a pitch that i heard a year before with the lazy ledger white paper and so for those who are not aware lazy ledger is the you know the academic name for or behind celestia exactly um so that that's really what led me to you know yeah this modular kind of way of thinking i love it let's uh i think one thing that i'm like really curious around to get some of your guys perspective on is you know in a broad sense like what makes you know yourself or or kind of the investment thesis that you hold you know why is it so bullish specifically on on this modular stack versus monolithic constructs uh yeah i think it's it's of course about the various parts of the stack and that you can optimize for say data availability or execution etc but maybe deeper down and especially with data availability sampling it goes about giving users or end users and their lite clients the ability to be first-class citizens i think they're words for use of these crypto networks so that i with my mobile phone have similar security guarantees or almost similar as a full mod whether that metals or not this may be the bigger question and i think we'll get into that later with mustafa and anatolia but that's i think that's why we're building web 2.0 um to have end user verification absolutely about you alex yeah i mean for me it comes back to like why we're here in the first place which is we want to build um you know we want to build open incorruptable censorship-resistant networks for the web in our view we see you know the majority of the world's economic activity moving on to blockchain networks over some period of time that you know is probably over the next couple of decades and in order for that to happen i mean you look at what the end state is and that's millions of transactions per second so what we have right now is definitely not going to get the job done and like what is the best proposal for how to get to that end state and and to do so safely and by safely you know that would that would mean without breaking the guarantees that decentralized blockchain networks give you today so censorship resistance and security primarily and the best proposal that i've seen for that is to scale via rollups and to scale via a modular blockchain stack where instead of introducing a an honest majority assumption every time you scale horizontally you can scale vertically and via some combination of fraud proofs validity proofs and data availability data availability proofs you can introduce only an honest minority assumption and you can significantly increase the throughput and the express expressivity of new execution environments that all share some decentralized foundation so i mean that to me makes the modular blockchain stack um the the only viable approach right now to an end state where the world's economic activity is happening on blockchain actually there what about yourself well i mean it's always i think for us comes back to scaling we just simply haven't seen like rollups can work but you know we're not operating at scale yet today right so you know the kind of scale that can onboard you know a global and you know full user base you know across across the world so when you see i think why we're bullish and we believe in the modular thesis is because you know when you split out these layers data availability from execution from settlement um you you can optimize the three separately right and you can let them when they're bundled together you fundamentally can't you know you know perform the optimizations um that each one is able you know capable of being on their own right so when you it gives splitting out data availability in particular gives you the ability to go from you know basically data available availability proof and you know in o of n right complexity to to o of you know square root of n right and that's a phase shift in in the ability to scale blockchains right and we're looking i think you know and sometimes at times particularly investing in technology it's like you're looking for phase shifts as an investor and like going from oven to of square root of n is a phase shift and so i think we think this kind of stuff will continue to emerge as this you know sort of thesis is played out and technologists continue to develop along these lines anything to add there eli uh i think i covered a lot of this in kind of my earlier point kind of jump the gun but um i think the only thing that i would kind of add was that you know if you look at the history of computation you kind of like you know maybe you started with like uh something more primitive like uh abacus and then you kind of worked your way way up through like application machines like you know there were these um kind of machines or kind of like contraptions that you know could uh solve a fourier series or something right but they were very application specific that were very bulky and then we worked our way up to mainframes or what have you and then worked our way all the way down to you know kind of modular modularizing it right with you know cpus gpus whatever is kind of in your desktop or laptop and i think that you know we're seeing a similar evolution in the blockchain space um but i do think that there's like you know there's a little bit of ambiguity about like you know what modularity actually means right i think that so far we've seen modularity in the cosmos sdk or the the substrate uh design for for polka dot but i think that you know celestia is really really interesting to me because um you know you can have all of these modules you can build custom execution environments but you still uh inherit the security of the underlying chain through roll-ups or whatever and i think this extends to other kind of you know other categories of investing whether that's you know bridges or or what have you it's like really important to you know further modularize some of these concepts but also still inherit some of those um you know security principles elements of the kind of the security foundation i got you what do you all think is kind of like or are some of the larger distinctions when it comes to uh you know investing in a modular thesis versus the monolithic one maybe we could start with yourself ela on this one sure so i think um i mean uh we've been pretty bullish on you know uh kind of this modular paradigm for quite some time um but i think when when thinking about kind of monolithic versus uh versus modular architectures you know if you abstract modules in celestia or just like roll-ups on ethereum um or what have you really you just kind of end up with like a bunch of different chains if you abstract it away enough um and i think that you know like our name is polychain right like we've we have like a bunch of application specific chains in our portfolio um and you know whether you're exploring that because uh you want to implement some kind of new execution environment or um maybe you want to uh incorporate you know cheaper fees for some kind of privacy based feature or whatever it may be um you know we don't live in a monolithic world it doesn't really make sense to even from a portfolio construction perspective to invest in one monolithic chain if you believe like in that kind of thesis right so uh yeah i mean i i still really like solana and everything i really like ethereum um and i don't think that they're going anywhere um but i kind of view them almost as their own modules interesting well what about yourself what do you think makes kind of the largest distinctions between investing in these two separate thesis yeah it's it's actually i think maybe more challenging in theory i think to invest in modular not like it's hard to decide to that that should be your thesis but but thinking through like what parts of that stack are um going you know are going to be valuable is actually i think harder to do as an investor frankly i think when you have you know that like a sort of monolithic l1 set whether it's ethereum or something else right fundamentally the you know tokens on the l1 are being used to pay for you know secure you know sorry you know uh compute bandwidth like memory and solid state storage fundamentally they're provisioning those resources on the network and but that's okay because those things are all bundled together and you know obviously you're paying for it at the op code level but in like at the end of the day like you know when you're submitting a transaction you're paying for all in all in one bundle fundamentally and so it's like and that creates a surplus you know that you know a transaction fee that produces a surplus above and beyond what the miner takes in that's kind of the eip1559 model i think this is more challenging right because now you have different layers where there's different fundamentally different resources being sort of like provisioned at the end of the day and understanding which of those resources like uh whether that's computer bandwidth or solid state storage or memory like understanding who what people are willing to pay for each of those and then who exactly is going to pay for for each one i think is pretty non-obvious to me at the moment maybe these guys have answers but um that i think it makes it more challenging frankly to like really evaluate um these products i think these like modular projects as an investor alex i know we've talked a lot about this kind of what are your thoughts maybe on some of what will was speaking on are just kind of the distinctions between this yeah i mean for me i think it comes down to end user and developer choice and iteration speed um specifically so um when when you build a monolith like blockchain you are you know you're marrying absolutely consensus settlement data availability execution and that constrains the directions that you can move in it's much more difficult to upgrade a system where all those pieces are coupled together whereas in a modular blockchain you can deploy a new roll-up when you want to experiment with a new execution environment and so over time like like you know what i'd love to see is like a wallet where i can have almost like a slider for like what's the level of security you need what is the level of privacy you need what you know what what is the um the speed of transactions that you need um for for whatever your application is or even just as an end user doing payments and you know i think if we're if we have wallets switching between a bunch of monolithic blockchains that are specialized for each of those things that introduces a massive security hole in bridging assets between them and so when you have roll ups you have these like natively trust minimized bridges that are baked in and you can achieve you know very like highly customizable um security highly customizable privacy etc um for for applications without compromising on security so i i think that approach is more sound long term almost giving developers kind of an additional choice by kind of modularizing that yeah exactly i mean at the end of the day like this is these are open networks it's open source software and so like the more experiments we run the more iterations we produce the better the results are going to be and the faster we're going to move as a space and so i think one of those approaches lends itself much more effectively to experimentation to iteration and i think that it's very likely that that'll produce more interesting applications and developer experiences long term anything to add on that one no i agree with almost everything that has been said but two maybe more general points will said earlier right these rollups aren't live fully yet so i think there's still a lot of ongoing research and when looking to back founders i think you have to accept that there can be quite radical pivots based on the latest research so you need to look for people that are willing to do that that aren't like set in stone and i'm going to build just this and when a new insight comes they ignore it in addition and i think this is something we already see with the rise of bridges between monolithic chains i expect these communities of the various parts of the stack to get increasingly intertwined so we see that with avalanche and terra now right their communities are sort of splitting uh combining through bridges i think in the modular stack that's gonna happen so much more yeah and it is an interesting thing to think about when you you know not only are you modularizing the technical components of these but also the underlying social consensus of each of these layers and it does lead to kind of some of these interesting uh kind of ideas to think about on a much longer term basis as well as people are kind of choosing where to where to go and which layers to use under them um you all think there's maybe a particular layer within the modular stack that you believe is maybe like underappreciated under discussed or maybe even potentially undervalued yeah yeah i i think right now we mostly focus on execution environments that are evm focused i think different types of execution environments virtual machines i know fuel was on here earlier they are doing something with that are fastly undervalued because it brings in a whole new wave of developers and i think that's something that excites me a lot about modularity i'm sorry frontrunning you i know you're yeah i mean maybe just to like expand on on the intuition there like the i guess the the most dominant development standard today is based around the evm which i don't think that a lot of folks would disagree is is suboptimal in a number of ways like the way that it prices storage the way that it handles memories these things like create limitations in the types of applications that you can build and the level of scalability that you get like the amount how efficiently you use the node resources at the end of the day and so br like allowing the space via modular execution environments to break away from the evm and run experiments on like can we optimize a vm for a specific application like to me that is that is like like we've more or less saturated probably development efforts on around the evm and that's important we should continue to you know to improve it and make it better and make evm proliferate more but like alternative vms are very important and have a large place in in the world and so those experiments i think are very welcomed yeah we're we're all going to be on the same page on this one i mean ironically it's like data availability sampling and data availability that like unlocked all of this right so in some ways that's the most you know like that should be what we're most excited about but at the end of the day like the real exciting part of the modular stack fundamentally is that um i you know i i think the design space for the execution layer has been blown wide open by splitting out execution optimizing the data availability and consensus layer i think like we haven't even started to explore the design space and what's cool about it is it you can iterate a lot fast i mean obviously cosmos and cosmos sdk and other projects have provided like some outlet for this in the past but still i mean there's a lot there is a lot involved in scaling a validator set uh you know and some of the work you have to do to spin up a call you know sort of like a cosmos zone so i'm not their credit to where credit's due to people who have you know gone down this road in the past i just think the pace of you know developing new rule sets for execution environments or execution layer is going to be um way faster on top of celestia and in this sort of roll up in a modular mo you know modular you know sort of approach things that like i mean we could talk there's a number of parameters we could sort of imagine tweaking you know but you can there's there's just so much you can do whether that's increasing block intervals trying something with longer block intervals where it allows you to do longer sort of like compute right so that you can do like write longer programs if compute is really cheap and you can write longer programs you know people might try doing something like that you might try you know i was i you know parallelization of transactions uh was mentioned before like the there's so many parameters you can you know sort of modify and it's just a multivariate sort of like design space and i'm excited to watch people explore this over the next couple years because i think that's where a lot of the exciting stuff around the modular stack will happen yeah i i think that there's like just this kind of elegance uh around the like minimal design of celestia specifically um and when thinking about you know how the execution environments on celestia or any other chain will evolve does get really interesting naturally the the first step is to have this kind of evm environment maybe slightly modify it maybe it's a canary network or something uh like that um but i think that the next step gets even more interesting where you can take a cosmos sdk app and maybe decouple some of the staking economics from the the core business logic right you no longer or or maybe you don't have to subsidize uh valid the validator set as much right or or you know you can defer some of those costs or or just cut costs in general i think the the the the step after that uh gets even more interesting where you know you can kind of have this ide where you what you're really doing is deploying separate roll-ups of sorts um you know and you're no longer competing to include a transaction in you know this uh this order book uh this uh order matching logic uh or and you know and and that's separate from uh voting logic in a governance module or something right so you have like a matching engine module and then you have a governance module or something right and these things can seamlessly uh uh communicate between one another um using celestia um or or you know uh you know even ethereum can do this on a long enough time verizon i'm sure absolutely um i think like one of the ideas that makes me think a lot about is just like you know i think we've covered a lot of you know what this stack or what about this stack excites us i'd love to hear some more about like you know what are the implications of that on applications on end users i think there's an aspect of security that could get discussed here but also just the new design space that this opens for applications would be awesome to hear your thoughts on yeah so um [Music] so i think that um you know even taking a step back from just thinking of celestia as the modular stack i think that other applications um have been really interesting to see uh to see evolve so you know there are these different kind of layers that we've seen as more chains just come into existence um you know you don't have to you no longer um you know are just loyal to this like l1 eth right and you know using a lot of layer twos uh feels very similar to layer ones that have deployed evm compatible environments and so we're seeing you know um new kind of like liquidity layers like um connects where that can act as like a clearinghouse between all of these different uh environments and i think that um you know when you have this clearinghouse you don't really have to worry about um you know certain security concerns because you know it's modular modularized in a way where it inherits the security of you know the the local instances where it's deployed or you have something like nomad that can be used for uh passing messages between these different modules or environments um and then you know at the application layer you know you know outside of just um uh outside of just you know kind of communicating or transferring tokens between uh all of these environments you have like core applications right it's like something like dydx um you know they're they're deployed on stark net and i think like the majority of the reason they can even function is like solely because of kind of the scalability increases yeah exactly right will what do you what do you think about like or what excites you kind of about the application space when it comes to yeah i think you know we've made a lot of points uh and gone there's just a lot i think that's that generally excites this group one thing that i want to flag as well while we have it's like some of the work cello has historically done on plumo which is the sort of light client that's optimized for mobile i think there's some elements um you know celestia's data availability is very different but data like the modular paradigm and data availability like a sort of like a like let's call them like an eager blockchain versus a lazy blockchain like lazy blockchains are like fundamentally pretty good at uh for like fundamentally better for sort of mobile users and like if mobile users want to be self-sovereign because like clients have stronger you know sort of censorship resistance guarantees um so i'm excited to see if we can center you you know use the data availability paradigm and the modular paradigm to approach you know new applications for mobile first users i i it's a little bit further down the road and that's a little more speculative but i think that's always should be a goal of the crypto and blockchain community is like how do we build things for different sort of like sort of computing platforms like mobile and um so i know i know that's a bit of a digression from some of the stuff i was just talking about but i wanted to flag it as something i find exciting and we'll see if that can you know that can sort of like emerge i'm definitely aligned with you there like i think there is an end goal for web3 where you know we need to had to have or at least provide the optionality for end users to be you know verifying on and then making that easy on it on yeah because device like a mobile phone otherwise like inevitably somebody will build it you know we'll build an application for a mobile phone and you won't have the option to do to have like you know sort of self-sovereignty via a really you know censorship resistant like client um so you know that for me that's that's pretty cool awesome i know we only have a few minutes left but i'll let you guys hit on this one anything particularly um interesting that you guys are kind of foreseeing when it comes to the application layer uh sure um so my a colleague of mine uh ryan likens it to uh you know i guess like the shift here to a breakthrough in hardware so it's really hard to say like what does this new paradigm enable um but i think we have some instincts on what it might enable so for as an example fuel processes transactions in in parallel so they can run at a significantly higher speed than a monolithic blockchain so that enables things like micropayments which aren't practical or economical on on blockchains today they also have a significant memory expansion um at the node level and so that allows things that are memory intensive like an on-chain order book dex for instance which is not something that you can really practically run on on ethereum today so yeah without any trade off yep ties what about yourself that polishes yeah i think the order books have been mentioned a few times right totally agree there also on the application layer i think we will just see more radical innovation because it's easy to just deploy an application specific chain and try something out another but this is maybe more infrastructure if you will as bridges that's their shared state between optimistic roll ups is an area i'm very interested in i haven't really seen that yet so if someone is building it do reach out but yeah apart from that agree with what they all said awesome guys well i think we are at time but really appreciate you all joining us and thanks everyone for coming out to the summit i hope you all have enjoyed gilbert thank you guys good job all right we have one more panel and then we have the debate so um this next panel is about different approaches to solving the data availability problem so um you guys may be aware that there are a few different projects who are building data availability layers namely on this panel represented celestia uh polygon avail and ethereum itself building out dank sharding so this panel is meant to explore what are the different trade-offs to these to the different data availability scheme constructs so um on this panel we have john adler from celestia we have matt garnett from ethereum we have onrog avail and then we have sriram kanan who is a researcher at university of washington uh who has written a lot of very interesting research on the topic so please give a round of applause and welcome to the stage good evening everybody welcome to this panel discussion it's a great pleasure to be here thanks to the celestia organizers and the modeler summit organizers for bringing us here it's a pleasure to share this stage with these illustrious gentlemen here okay maybe we get started with a brief introduction of the data availability stack that uh each of you are building i'm just here to moderate this uh discussion please yeah uh so i'm anurag i'm representing polygon avail polygon avail is a data ability focused blockchain so we began thinking about this problem in around 2020 celestia or lazy ledger you know like i think mustafa had written a paper regarding that and you know like we were interested in exploring that but you know like they went with the fraud proof approach and you know like when kzg polynomial commitments you know really uh became better known i would say you know like we thought that you know we could uh structure our data ability focus solution on using kzg proofs uh so uh the architecture is that you know of the data ability focus in the sense that we do focus on transaction ordering um and not execution and specifically we use validity proofs um you know yeah proofs for that and the first use case which we are focusing on because you know like we have uh three different uh projects under the polygon umbrella namely polygon hermes which is focusing on zika evm for example polygon zero and polygon maiden so we have three zkvm solutions within the polygon suit and so the first focus was to you know kind of working on validium solutions for these three solutions uh so that's the focus and then of course we will move to like sovereign app chains going forward so that's the uh rough brief thanks okay is this on okay uh so hi everyone my name is john uh i'm the co-founder and chief research officer of celestial labs and we're building celestia the first modular blockchain network this was formerly known as laser ledger and it's a data availability only uh chain that is overhead minimized which means in order to use the chain uh for data availability in order to verify that the chain is valid you have to do a very small amount of work ideally as minimal as possible the particular data availability scheme that we use is actually essentially completely identical to the one that was first proposed by my co-founder mustafa in his earlier paper on fraud and data availability proofs uh to secure uh trust minimized my clients effectively completely unchanged from that paper which was three four years ago at this point uh so there aren't many cases in the blockchain space where you know you don't have to fundamentally pivot your technology over that many years but in this case it did happen oh thanks hi my name is matt i am a researcher working on ethereum and you've probably been hearing a lot about the ethereum scaling stack called dank sharding and dink sharding is a new take on the ethereum sharding roadmap that exploits some of the observations that people have been making over the last couple years related to the centralization of block builders in the ecosystem and bank sharding relies on the fact that in the future we expect that the people who are building blocks are going to be highly sophisticated more resourced actors and the people who are proposing blocks are going to be lower resourced actors that's coming from a more decentralized set and so with think sharding we are able to avoid some of the clunkiness that existed in previous sharding proposals where there were 64 shards 64 sharp proposers there was latency between creating a block on a shard and that block being check pointed onto the beacon chain and so now we're taking advantage of the central more centralized building actors and they're able to aggregate 64 shards worth of data into a single 2d kcg scheme and provide that to the network and it's also inspired by this lazy ledger paper that was written four or five years ago where people are sampling the data elements from this super 2d encoded block i would say also for the the encoding scheme for the the blocks is um definitely more efficient because you're able to only do 75 samples of the block rather than 30 samples for each shard block so it is a lot better than the previous proposals for how much data needs to be downloaded from each validator that exists in the validator set thanks so the main agenda of this discussion is to understand the different data availability architectures so what we will do is try to set up the axes of comparisons one axis i think is obviously security what is the security model that each of these different projects are trying to optimize for one way to phrase this question is what would it cost for me to get an unavailable data and pass it as being available right so that's something maybe we can discuss about the different schemes in the kzg commitment scheme the the scheme is set up as such that i think safety of data is not a big problem uh the problem i think would be maybe uh insufficient number of light lines for example sampling the data uh because there's no incentive as such so the idea is that you know like uh if you are building like an app specific chain you would have the incentive to you know like have light lands sampling the data so i would imagine um having insufficient number of light lines sampling the data or part of the data is available not from the block itself but if you consider the entire app history so that is that could be a case but you know in general due to the construction of the scheme um it's uh pretty nicely laid out that you know like either data is available or not available so so one point i want to add here is we call this data availability sampling and when we call something data availability sampling what we mean is you any node any light node like arjun just mentioned anurag just mentioned is any light node should be able to sample an arbitrary subset of the chunks of a given block and then verify that they are available a very important underlying assumption here is private random sampling that a light node should be able to do check a random sample of different chunks this i think you know goes back to the security question and maybe we can address this but the the question is how do we think about private random sampling how do we make sure that light nodes actually when they're sampling it is private and it is random because there are all kinds of de-anonymization attacks for example if i know that the same ip address is requesting these 32 chunks maybe i can just send those 32 chunks to that guy and withhold the rest of the data so private random sampling is a very important model and i think this is not something that is part of any other blockchain architecture prior to this so we need to take special attention as you know data availability builders to this so i think that that goes back to the light client point that you mentioned whether they're able to sample correctly yeah or you want to add anything to that please uh what is the specific questions though the security and then a part of the security is private random sampling so that's how i'm saying sure so i guess you know this is the you know the first data availability sampling scheme proposed and the assumptions are pretty concretely laid out in the paper right you need a certain minimum number of nodes not necessarily light nodes but nodes in general to perform sample requests you need a synchronous communication in order to reconstruct the block if there's no single node that starts with the entire block and you have to reconstruct it from light nodes and you also need as 3ram has said you need under certain adversarial assumptions you need private uh sampling so that the attacker can't link together who is trying to sample because if they can then they can just trick that one particular node into believing that the block data is available when it's not and that's essentially i mean borrowing bugs in the code and stuff uh assuming that that's all correct and that's essentially the failure modes for the scheme so a certain number of light nodes is a pretty weak assumption because you know there's thousands of bitcoin nodes there's all 10 what 10 000 ethereum knows there's even like something like six or ten thousand solana nodes right there's people will run nodes even if they're fairly expensive uh just purely because they want to have access to the chain you know so an assumption that there are a hundred light nodes sampling or 200 light nodes is like a really easy assumption to satisfy we can essentially guarantee that the synchrony assumption for being able to rebuild the block is something that you unfortunately can't avoid and this is one of the reasons why validity proof schemes aren't inherently better because yes you avoid a synchrony assumption for a fraud proof but you still need synchronous communication to rebuild the block anyways so you know if you're not actually if you're not fixing the worst case you know you still need that synchrony assumption so you don't really get too much out of validity proofs except potentially some lower latency happy path cases which is debatable uh and yeah that's that's about it those are essentially the failure mode so as you can see uh these are fairly weak assumptions which is why we tend to classify uh date the process of data availability sampling as being trust minimized because these are fairly weak assumptions now in terms of in terms of private sampling this is something that i don't think any chain that's implementing data availability something has done yet mostly because this would require a fairly well-resourced attacker but there's if you don't want to go through you know fairly advanced uh schemes like mixnets or whatever or tor there are other alternatives for instance you can have users run full nodes in multiple cloud service providers and unless every single one of those cloud service providers colludes to reveal that you know the same user running those nodes a third-party observer can't actually connect that those nodes are actually run by the same person so you can even if you don't directly solve it through some advanced networking you can solve and practice fairly easily against an attacker that is just one person observing the network rather than literally every single cloud provider in the world all colluding together because that's a much stronger adversary which is much less practical thank you john uh matt do you want to address how ethereum is thinking about private random sampling right so in the first uh iteration of dink sharding their plan is to not do the private random sampling and the reason being is is that being able to quickly access random chunks of this data is actually a really difficult networking engineering problem to solve and so the assumption that we'll make is that with an honest majority we're going to be able to assign validators certain rows and columns that they'll go ahead and download and be able to you know with this mechanism you know propagate this information over the lib p2p gossip network in a lot more efficient manner than having to push things into a dht and then retrieve them but this does have the assumption is if you are a client who is just watching the network you don't have a strong you don't have a strong grasp of if that block is valid or if that block is available or not you're just trusting the honest majority of the network to tell you that block was available because the validator said it was available and this is where the private random sampling goes in where even if you have a legit a malicious majority of the network saying that a block is available you're able to do the samples yourself and with high confidence determine whether or not the block was actually available uh would you say in uh in bank shouting if somebody indeed wanted to do random sampling they could just join those particular peer-to-peer networks where those chunks are propagated and thus check you know whether that is available in those like peer-to-peer networks and then you know verify for themselves that the data is available is that something that you guys are planning for or so the way that the the the rows and columns will be encoded is not going to be as amenable to like the proper sampling structure that people want to do because you'll want to have more granularity when you do these random samples and the rows and columns won't provide that level of granularity but you will be able to connect to as many you know of the gossip channels that you want to download as many rows as you want okay so the this is basically a level of cla the level of granularity that you can achieve in the random sampling the peer-to-peer network and what goes on a certain peer-to-peer network dictates that granularity but it's still not everything you know as a light node you don't have to necessarily download everything is that correct correct okay um excellent so that's the first axis that we talked about is security and as a subset of security was uh the private random sampling which is a radically different networking model than what was needed before the next axis that we can think about is how do we think about the scaling ability of these systems how many nodes should sure to make it concrete let's say there are n nodes all of them have a certain bandwidth what is the uh net commitment rate or data availability rate that a system like yours can achieve is there some um heuristic idea on that yeah so uh we are still uh playing with those around but in devnet for example uh the configuration that we have set is like mb uh block sizes uh like 4mb 2mb like coded in so and blocks block time yeah that's around at the devnet config is set at around 20 seconds at the moment but we are still you know kind of we still not optimized it too much we have just benchmarking we will do a lot of benchmarking in general we are also in the future we will also do things like commitment generation uh optimization you know like so those kind of things are there but the current configuration is like 2 mb block sizes at 20 seconds that is that is what we think we'll experiment more in the upcoming testnets awesome so when you look at your uh avail network do you think of this as like a peer-to-peer network and you're doing both consensus and data availability or is it a pure data availability network so of course i mean the validators uh need to compute the commitments uh so that is certainly uh so we are using substrate stack for that which we have like grandpa babe for the actual commitment generation for example and at the moment the scheme is such that all validators compute the commitment uh individually as well so after the proposal proposes then all together on its majority consensus so everyone has to commit it but we are looking at certain alternatives in the future which might optimize that as well so then you know we can experiment with block times so yeah john uh so to clarify what's your question around the uh you know the bigger uh complexion throughput the data availability throughput that you can achieve using this kind of like an architecture is there a question around throughput and not the size of the commitment no not in the states of the country okay i must administer it my bad so the throughput i think i have to refresh numbers it might have been something like 1.4 megabytes per second uh i don't exactly remember but you know we're going to have a respectably large block size in the order of several megabytes and a respectably long block time uh we aren't really aiming for a short block time the reason for this is that the longer block time the larger your block even if the throughput is the same but the larger the block the more you can take advantage of the fact that the size of the commitment isn't linear in the size of the block but rather is the square root of the size of the block and therefore you get more advantage like you have more scalability benefit if you have larger blocks and a longer block time than if you had really really tiny blocks and a short block time uh longer block times for the base network uh well a lot of people say you know you need really fast finality for the for the base network right you need 400 milliseconds or you need one second right but it turns out that as people have used have used rollups uh users are perfectly happy with the fast instant soft confirmations that roll-up operators provide and as long as you're happy with that which is a perfectly fine assumption right as long as the robot operator is honest they will include the transactions they promised to include in the in that order then the base layer doesn't actually need blazing fast block times so you can have slightly longer block times and take better advantage of this square root data commitment scalability property oh matt do you want to comment on that right yeah so for the ethereum dink starting proposal the throughput will also be around 1.4 megabytes so that's the magic relationship i didn't want to say 1.3 because you had just said 1.5 yeah so the throughput is supposed to be 32 megabytes for every 12 second slot and of the 32 megabytes 16 megabytes will be the erasure encoded data so the actual data that will be useful will be the 16 megabytes and so that works out to 1.3 megabytes so something interesting here is you you are in bank sharding every node doesn't have to download all the megabytes per second you know each node can download much fewer at least the quorum that you're certifying is not every node downloading everything whereas in a celestia i understand at least as of now all nodes are downloading everything but both of you have the same bandwidth is there i'll let matt answer that first i have actually a lot to say about that but i'll let matt answer that first i didn't catch what you use no i'm just asking just clarifying first that in bank sharing every node if even though your total you know throughput is 1.3 megabyte per second each node each validator does not download data at like one 1.3 megabyte per second is that correct right exactly so in the first phase where we are not doing the random sampling the current parameter is to do four uh samples from the rows and columns so you would download two rows and two columns and i forget exactly how much that is per validator but um that would be the current the current parameter and um after that yeah we need to look to see what the numbers is for like the exact uh row and column throughput awesome yeah so for uh for celestia we have kind of a few different like a few different ways to answer that question essentially which is that the first is we don't just have full nodes and light nodes and nothing in between where you either download all blocks like all the block data or you just do data availability sampling but we have a node in the middle called the partial node and these partial nodes rather than downloading the full blocks they can either download a subset of all blocks or they can download oh sorry they can download like entire blocks but a subset of all the blocks or they can download a subset of each block this is configurable up to the end user but these partial nodes still contribute to the security of the network because they can still produce fraud proofs and more importantly you know even if you eliminated fraud proofs and just had you know validity proofs you still need that worst case of being able to reconstruct the block and partial nodes contribute to that because they will do things like fully download either a for block or full row or full column or however you want to configure it there's an implementation detail so there's no type in the middle now with respect to our validator nodes if you're a validator the initial implementation will require validators to fully download the block and the reason for this is not that this is a fundamental thing in our technology because you know we use fraud groups or anything because again you still need a synchrony assumption for reconstructing the block and this is kind of the key is that by requiring validators specifically to download full blocks it means we avoid having to deal with the complexities of dealing with the synchrony assumption if you had validators sample blocks to determine their availability you totally could do this but then the networking and the assumptions around this and how tight your timing constraints are become much more complex to reason about because you have to worry about the case where the block is available if the whole network can reconstruct the block by communicating together and that is like a much harder process to reason about so i wish the ethereum researchers well on implementing this but subsequent implementations of celestia know if these problems are ironed out will not require validators to uh fully down the blocks they'll allow validators to simply sample blocks for their availability yeah good point there in the sense so our header structure is also um constructed in such a way that you know when we want to increase the block the commitments the number of commitments you know kind of increase in the block which is not a very uh it's not a linear increase in that sense right like so so we have not experimented with uh those at the moment but you know like once we get to more better testing conditions we will uh experiment with those second thing is we i mean like uh john said like you know we've also not from engineering we have not studied that but like we have experimented with the idea of columnar full nodes uh because uh you know like that is also another entity taking pla play you know part in the network and you know like that will also i think be more prevalent in the future that could be a scaling path i mean basically you have validators and today you have letters and you have full nodes and you have the light lines and this is like another participant into the mix storing some of the data great um i think we're coming to the close of the panel so i want to switch into a broader question on all of you have taken very deep bets on this modular uh world and maybe the celestia team coined maybe the modular paradigm and polygon has a variety of projects building a whole stack and ethereum is shifted to a roll-up centric world i want to just get your outlook on where this paradigm is headed and what you think the uh interesting opportunities and challenges are so i think what we've learned a lot from our polygon journey is you know we have got the opportunity to talk to so many devs uh so many users uh and you know like what we have finally i mean so the broader polygon thesis is in general uh not um focusing on one particular um you know like uh so if we have a sheet of scaling solutions right like so we've we've invested in you know like zk uh evm's m3 in fact and uh you know like so we we believe that you know like uh in general developers want like different developers different users want a variety of different use cases so for example when you talk to enterprise clients right like so they want some some version of the blockchain you know with different properties they don't want us maybe they don't want to have the data on a shared chain for example they want some people want private uh data as well some people you know i mean so there's a variety of use cases enterprises have different you know like uh dabs are different there are like there's a wide variety in terms of you know what requirements is right like and so what we thought is you know like we should focus on getting uh customers or users or developers you know like their choice so if they want to run a roll up for example on ethereum uh that is their choice uh if they want to run validiums yup that is their choice if they want to run silver and app chains that is their choice right like so so yeah yeah it's not a one size fits all um john yeah so uh you know throughout the year since we started building this has definitely been kind of a journey of learning uh when we first started a lot of uh people who first heard about the project didn't really understand what it was about and didn't really understand the use cases how to be used and so on so it took quite a while to get to the point that we are now uh that you know there's a lot of excitement around this notion of modular blockchains and the modular blockchain stack uh yeah oh i guess i just i should talk about kind of you know from the from the early days when i was you know not exactly clear what applications we had to now when there's like much more uh concrete vision of applications so we have things like uh you know there are pos there are ways and mechanisms to use celestia for data availability and provide the service to roll-ups on ethereum and we call these celestiums i gave a talk at eat denver earlier this year i think if anyone is interested in learning more and i think a colleague of mine evan gave one yesterday or the day before also uh here at youth dev connect and there's also the uh the notion of submos which i think mustafa talked about earlier uh where you have a settlement layer that runs a solvent roll up on top of celestia and then roll-ups that as they exist on ethereum can use this as a kind of drop-in replacement to ethereum and then finally we've really been pushing this notion that i really believe in is the end game four roll ups this notion of sovereign roll-ups where it's roll-ups that run directly on top of a data layer and therefore have the ability to harden soft fork to change their consensus rules to change their block validity rules with off chain governance which is not something that applications can do if they're built directly into an execution layer you know like gaps on ethereum or roll up smart contracts on ethereum uh they those to upgrade require something like a multi-sig or a dao some sort of on-chain governance and you know the entire you know the entire ethereum and bitcoin ethos is that on-chain governance is no bueno because on-chain governance can be captured by whales right so it's kind of weird that you have you know this entire notion that the the core blockchain layer can't be captured by whales it does often governance but then literally every single application on it does have on-chain governance because it needs to upgrade uh these sovereign rules are really i think the endgame of roll-ups where you can have the best of both worlds you have off-chain governance and you have shared security so much um matt yeah so um for ethereum i think that there's just different choices that ethereum has to make being a chain that already exists in a chain that's focused on decentralization at the very core but what i am really excited about is that even though dink sharding has many things on the critical path to eventually reaching that end goal we're working on a eip-4844 called proto-dink sharding hopefully will provide a lot of the facilities for rollups to start using what will exist at some point in the future which is basically there will exist some oracle to provide a route that says this data was available and as long as the rollups understand that this route was made available by the protocol then they can trust and they can build uh you know their chain based on that or if they're a zk roll up use that for their zka roll up to do synchronous communication um within that and so i think that with you know 444 which is actually very similar to what john proposed in 2019 with 2242 um i think that you know as things continue to change over the next couple years at least we can start providing that facility the parent that primitive for rollups to build on this data availability layer that will start to exist in the next couple of years awesome i think that brings us to the conclusion of this panel i think it's the right question to finish to lead into the next one um thank you so much to the panelists for taking the time to be here uh anurag john and matt thank you thank you thanks everyone good job guys thanks man all right guys this is the last uh i guess segment of the modular summit we are having a debate between mustafa from uh celestia and anatoly from solana about monolithic or modular so uh and and moderated by yours truly tyrone uh the mad scientist of crypto so again after this stick around we're going to have networking drinks right here and um yeah i encourage everyone to just hold on and if you're talking like move outside and give your full attention to the debate so let's give a round of applause [Applause] i see that anatoly and i both have our our usual uh drinks in hand uh you know i i i i i'm sure you've seen all the memes of anatoly drinking uh so uh well great to see you virtually um great to see you guys too yeah i i uh this might be the only section today that needs no introductions so can kind of go straight to it um so i actually want to start with this intro question that uh i think was teased online yesterday uh of sort of you know before we kind of get into the details of modular versus monolithic i think we should actually just say let's suppose each of you uh were kind of stuck in an elevator for 30 seconds you had a 30 second pitch and there was an engineer who just quit netflix because well i mean stock price crashed and uh they're like i wanna i wanna like understand like why i should build something um either on slano or celestia what's your 30 second pitch um and let's open me okay so this netflix this former netflix engineer is presumably a web 2 engineer so i would use a web 2 analogy if you're a web 2 engineer and you wanted to build a web application um you wouldn't want to build it on a shared hosting provider like let's say if you're familiar with dream dream host or geocities or google pages if you use a shared web hosting provider you're restricted in what you can deploy because you're stuck with the same execution environment that's provided to you by that shared execution provider and also because you're set you're sharing the same server as everyone else you're also limited in scalability that is what that is very similar to deploying your application on a shared smart contract platform which that is that is a monolithic blockchain nowadays web 2 developers build uh use virtual machines on on on systems like amazon ec2 or aws and that allows those developers to have full have their own virtual server and have full control over the execution environment and scalability of the application and that is what building a rollup is like you effectively have your own blockchain that you can do whatever you like on it and have any execution environment you want on it and you can scale better because you're not sharing the same execution resource pricing as everyone else great i think we were traveling slightly close to speed of light right there because we went a little over 30 seconds on our pitch but anatoly um don't use a blockchain unless you want um unless you critically need finality in a shared context with a lot of other financial applications like exchanges stablecoin providers all those things if you don't need that don't even use a blockchain to begin with but if you need that then solana is the cheapest fastest way to get to the most the largest set of these as quickly as possible well just so that we get uh kind of both of your individual definitions um in your own words how would you describe the other architecture so mustafa how would you describe monolithic and uh anatoly how would you describe modular and the answer you want to start um i would i would describe modular as an attempt to break down the different functions of the of the chain with some clean interfaces and build them as separate components i would describe monolithic as trying to build a global shared computer similar to the original ethereum world computer vision cool all right so let's start by uh actually starting with with this idea of like developer costs developer ux startup costs so if you're talking to someone who was you're just getting their their feet wet with developing on either salon or celestia how would you kind of describe the startup costs of going from you know you know just a hack a kind of toy project to actually like running something in production to like maybe having a dao having kind of longer term sustainability and uh you know how would you describe things like how long does it take to get familiar with the language the architecture development environment you know just imagine you're someone who who is just getting started um antoine you look like you well i'm assuming all the cool devs already know rust if not it's a it's a language that we didn't build it's a it's a language that uh amazing other folks have built with its own community and you know people out of things are publishing open source ros code so that part is pretty easy for anybody to pick up because it's a general purpose what language and the development environment is really just as close to native rust as we can make it so you just use cargo and use anchor to glue all your programs together so we've seen folks go from a hackathon to a project with like eight billion tbl in um like six weeks shout out to the saber guys who slept on the floor of the solana office to get that done um so i think the general goal for model blockchains is that we want to make it we want to make it so that deploying your own like roll-up chain or your own blockchain should be as easy um as deploying a smart contract and like we're practically like almost there like you know with the cosmos sdk you can deploy a blockchain in seconds um with like roll up technology you can just fork that roll up it can flip the smart contract and have your own roll up and the idea is like if you if we make it deploying your own chain as easy as playing a new smart contract then why wouldn't you do that uh like you you would have much more flexibility over the trade-offs in your execution environment and you can tailor it more to the application that you need so i i to to that point i think um you know one thing historically uh is that the linux took a very long time to actually get sort of the multi-processing multi-app multi-tenancy environments to work correctly uh it took a long a lot of pain and changing things in the kernel and but however once it was done it was like a huge floodgate kind of opened up a ton of app development opportunities like node.js would never be able to run on a linux machine without smp and linux 2.6 basically uh so do you think it'll take a similar amount of time for us to get to the point where you have this sort of automatic scheduling of roll-ups automatic kind of deployment uh things like that and and that the monolithic sort of version is is sort of a crutch on the way there or do you actually think there's sort of a fundamental difference um and obviously you know you both probably have different opinions on that yeah so i mean i think um if you look at the modular if you look at if you take a modular blockchain stack um like even if you take each module in isolation you know each module is simple but there is indeed complexity in the way that the stack integrates so yes module blockchain stacks um if you zoom if you look at the big picture have more complexity than a monolithic stack not on the individual module level but throughout the whole stack but but that's not an issue because the whole point of modularity is that you don't have to like know or care about what's going on in the entire stack you're reaching a situation where for example you know core dev in in guess already uh said that like guess there's now guess it's circle the guest client is so kind of has so much complexity now that no guest developer actually knows what's going on in the entire guest node and now i think that's a reason that's part of the reason why modularity is important it does allow you to do more and have more complicated and blockchain architectures that do more without needing a single person to understand the entire analytic stack and so yes to answer that question directly and it does it will take us longer to get there and then like it is it does take us longer than building a monastic blockchain but you know since 2008 2020 people have just been been building new layer ones and we've ended up in this like cycle of new layer ones so it's now time to kind of break that cycle and evolve into a stage where um now we have like so much traction in blockchain space let's modularize it and make sure that all the different innovations that people are building can actually be useful and holly um the irony is a two-six once it was stabilized killed the microkernel design and the monolith the kernel one is true the the rust mock kernels have never really caught on um and that's so i kind of think that it's going to depend what the use cases are if you have dependencies on composition between applications and these applications are you know like not static right you have constant kind of churn between the composition set then you really need one giant state machine to glue them all together and because what the users are going to care about is how quickly do i access all the state whatever that may be maybe it's like positions in serum manga and like uxd or whatever and how quickly it's finalized and how quickly it's connected to all the other financial institutions that i care about you know from stable coins from two exchanges and that network is not something you can spin up in a smart contract um that that's actually kind of the the living breathing glue of the the chain itself so you can have that in a modular architecture right there's a bunch of tendermints a bunch of them have different pieces but if you have use cases that are run even 20 percent worse right you have 20 percent higher fees 20 higher latencies that's like your phone running 20 slower people are going to notice and applications and everything else will shift to that environment where everything is optimized to the hilt and to do those optimizations you don't really care about the design or how complicated it is right what you care about is like how fast can we achieve these goals for for the use cases uh and this is why you know modern operating systems are all monolithic um even when somebody tries to build like you know a microkernel design when it actually shipped you look at like the 80 of the use cases that people use the frame buffers everything else it's all punched through directly to the um you know directly to the kernel without without using any of the um you know these modular features so how this plays out is going to be largely like use case dependent i personally believe that what these chains are good at is running uh like a dex right it's they're good for price discovery they're good at managing like offers and bids and all this other stuff for digital items and that under that like market function what these things provide allow everyone else to build unpredictable use cases like nfts which are you know their own financial uh you know financial contract but at the end of the day like what you need to do is synchronize offers and bids and like do this as fast and as quickly as possible and connect them to all the rails yeah i want to challenge the microkernel analogy and i don't think analogies are bad so rip it apart i would say small closure i would the knowledge i would use is that in linux in the linux operating system you have kernel space you could have code that runs in candle space and user space i would say having a monolithic chain is like running all the code for user applications in the kernel space but the way i see it is like having modular blockchain is having a user space which is like the l2 and having like actual applications wrong in there rather than in in the kennel yeah i mean that's the i i i i i feel like there's sort of merits to both of these if we look at history uh history has sort of preserved both models um you know there's obviously an intense amount of work on extremely low latency code kernel bypass like all the types of stuff that antoine was mentioning but then obviously you know much of the value of the consumer internet has come from realistically making it so that i don't need to know what a page table is to write a piece of code right like what percent of javascript developers have ever heard the phrase page table probably sub two percent so there is a sense in which developer friendliness is also quite important uh and to that kind of point uh you know another thing that of course people love talking about nowadays a lot is is uh extractable value from from validators and miners and this is something that you know both developers and users of these systems have spent a lot more time thinking about and one thing to kind of compare and contrast between monolithic and modular architectures is the idea that uh modular architectures have a lot more uh sort of things like you know you have to wait for certain transactions to go through there's certain like predictable latencies that allow people to sort of uh find newer forms of extractable value there's sort of this uh you know sense in which the tana tolle's point earlier heterogeneous fees make it perhaps more easier to extract value so one question is is do you think this ability perhaps to extract more value from modular architectures for you know for arbitraries and and sort of market makers do you think that's a feature or a bug because it could actually be something that drives liquidity as well as also kind of can parasitic towards users and sort of like where do where do where do both of you kind of see that mean it depends on how your execution environment works specifically if your execution environment has a sequencer so like in ethereum l2s like the mev is basically just moved to the l2 you know sequencers like in optimism for example in optimism for example the optimism roll up they see that as a feature and not a bug because they see that the sequencer can extract the mev and use that to fund public goods and that also kind of goes to um to kind of respond to hillary's point about finality um it's not necessarily the case that you need layer one finality for users to have sufficient finality on the transactions going through so for example like arbitrarily inbox model or like optimism sequencing model you can get a finality by looking either at the inbox or asking the sequencer immediately in in a second uh like faster than the ethereum block time so like there's different like uh like guarantees or finality that's more of a spectrum than just relying or like on layer one finality anatoly so those are all kind of compromises right because for it to be a true l2 it needs to be able to fail at any moment and that means that whatever partial finality you get is then in question when um you know the whatever however failure recovery works in that setup and that tradeoff is probably okay for applications and users in like a sandbox but you know like what is the whole point of a blockchain right is it is it to build an application that could run it's on a database or is it a a global you know replacement for a large chunk of finance so you know we'll probably see both of these models survive because of that because we don't really know why why these things exist yet i think with regards to mev um i tend to agree that so far we're probably the only thing that we see that um actually funds and create you know generates like value in in these chains is mev and you know in the best of light you have very fancy algorithms that try to predict the future and they want to improve prices and offers and everything else on this chain and you know they run their racks at gpus and they extract mev and in theory it should get competitive to the point that they should be bidding for users to send their orders directly to them you see some of that with flashbots and some of that with like uh jito network on solana so those are all good things right because in theory a user that doesn't care about mav should in fact see a better price and maybe even a rebate for their transactions instead of a fee but that's still kind of i think pretty far away um like we're still not yet at like the level of adoption where i could you know say that yeah this is true and this is how these chains run um so a lot of a lot of questions still remaining um but for the for the sake of the debate my uh my strong opinion is that mev will be the thing that's actually driving development and hardware upgrades and you know actually going after after users uh in these spaces and on on blockchains yeah i mean i guess to that point if we think about the something that you said in terms of replacing existing financial systems or sort of having the ability to uh service sort of a larger spots in the market mev in in in solana to some extent right now and then so on avalanche in particular uh where there is a lot of like validators advertising like hey we'll sell you this much space on in this block at this time and people basically are co-locating do you know do you think that this is sort of uh deleterious and that it'll just like reduce to the existing sort of model uh of sort of you know co-location around exchanges co-location around validators or do you think that you know you could actually sort of make this more sustainable with a different fee model or you know potentially better cryptographic solutions because i i do think one thing from the monolithic world is there's a lot more probabilistic uh mev like there's there's sort of two ways of thinking about it mev on single chains is a congestion game right there's like just a fixed amount of opportunities and way more bidders than opportunities and it's more about just like constructing a mechanism that matches those mev on on in modular architectures is much more probabilistic right it's about routing figuring out which path to take and so i guess from both your perspectives do you do you think like there is a way in which the economics model can can thwart kind of that becoming all of the volume on the network or do you think that you know effectively there's there's no way of avoiding that and we're sort of stuck with the existing system um i can go first so the way i think about it is that we want to make it as competitive as possible and as easy for anybody to enter you know the the network and offer like my my my whatever map solution because that should drive the you know the amount of like actual like profit extracted by these things to as close to to zero and that should be a good thing right like what you should see in the end is better prices uh prices that reflect the true value of things you know globally all synchronized across the the same state machine and to that regard at least in solana it's possible to run multiple block producers on the exact same state concurrently at the same time um doesn't do that yet but in theory that that's something that is doable because of the the way the vdf works we can effectively splice the final ordering using the vdf ordering between all of these so what that should hopefully do is that instead of only having one block producer at a time some position in the world you now have n and then you go talk to the closest one geographically that reduces that latency and then that thing is then running whatever out goes and whatever it needs to optimize its order flow and it's doing that competitively with everyone else and in theory that should reflect you know the lowest price of any good offered on chain globally you know within within the speed of light latencies around the world um and if we can do that then in theory price updates moving through solana are moving at the speed of light through fiber which is as fast as news travels and then it's competitive with something like nasdaq and cme so that's kind of the dream the horizon and this is how we've been building like everything you know looking at that final state how do we get there and what are the pieces we need to get to and when you kind of take that perspective you don't really care if the architecture is modular or monolithic what you care about is can we synchronize like information globally as close to the speed of light as possible and usually at least so far there's no gains to be had using a modular architecture that right now everything that we've been doing is around this like monolithic state because that's what's going to get us closer to that final state of you know global information symmetry all right well i'm excited for mustafa's response to that yeah like i think this discussion um kind of highlights the key kind of philosophical difference between like solana and systems like celestia or ethereum which is i think solana kind of systems like solana view blockchains more as a distributed computing you know platform where you can like you know have a global settlement whereas like we like celestia and also other systems like bitcoin and ethereum we see blockchains fundamentally the main the the fundamental purpose of blockchains is that it's a verifiable uh competing platform not just distributed but very verifiable and and that means there's a there's a focus on you know decentralization in that setting means that you need to allow light nodes to verify the state of the chain uh otherwise like if your goal is just simply to have you know fast like like fast agreement on the state how does that goal like fundamentally differentiate from you know what web 2 achieves with essentially with a centralized or distributed database even um like you could argue you know that the fundamental difference is that you have a global set of distributed parties agreeing on that state but without the ability for end users to verify these dates in my opinion that takes away one of the key properties of blockchains which is that like well-resourced actors cannot violate what the social consensus of the chain has agreed what the rules of the chain are like like like if this lana valde is for example wanted to change the rules of the chain how would end users like verify that without running a you know a well-resourced node yeah you teed up the next question which is you know to to to those i haven't spent time like sort of in the in the weeds on these modular and monolithic architectures force very different uh separations of storage and execution uh for light clients versus full clients so for the audience uh first let's just start with describing kind of how uh like clients work um for salon and celestia and sort of what where the division is and sort of which parts uh the light client has to store and execute versus not and name the stuff since you brought up like lines so like light nodes effectively um like allow end users ordinary users with like whether you know a cheap laptop or mobile phone to have almost the same level of security as a full node because they can get assurances about the state of the blockchain using technologies like fraud proofs or zk proofs and like this is the fundamental reason why the bitcoin community has decided not to increase the block size limit like in theory bitcoin could do a zillion transactions per second if they increase the block size limit and optimize the node software um but the bitcoin community has fundamentally decided not to do that because it would increase the resource requirements for end users to run full nodes and validate the chain which is like the pretty much the critical or the critical aspects of what a blockchain is supposed to um guarantee that the state of the chain is correct effectively and but with like with new tech with new light color with new light node technology that has that uses like fraud proofs and data availability sampling and zk groups you can now increase the block size limit without compromising the ability for end users to validate and verify the chain analytic like clients on solana the so the devil's in the details obviously here um when you look at like uh eth2 like deployment like go to nodewatch.io there's about 6500 machines that run the 300 000 e2 nodes eth2 validators there's only 6 500 actual boxes um so the real world deployment is that you take a solano validator and then you load it up with like a few hundred two nodes right e2 validators and that's your that's your eth2 deployment so the real world is that machines are getting bigger and the solana validators are not uh stacks of xeon processors they're like 32 core systems which is what you get out of a you know like a data center right now for you know 800 bucks a month which is not insane by any means it's not building racks and racks of of of systems like you would for you know like a centralized service to handle all those users um so there's a bit of like kind of i think uh like i don't know um like you have to actually look at the details of how these things are deployed so it's it's uh i think uh it would be misinforming to say that the amount of hardware necessary to run solana is that significantly different than um any of the other chains when you actually look at how these things are are being deployed in the real world but like clients themselves when you're talking about like a different system for verification there's obviously trade-offs so whenever anyone brings this up the question that i have is assuming all the other nodes everything else has been destroyed how many like clients do you need to re-re-hypothecate the network to reconstruct it so how many nodes are you trusting when you run this thing and those trust assumptions are important to users because the end state of like a monolithic chain is when you don't run any nodes you're trusting that at least one out of the whatever 3000 boxes that run the replicate solana at least one of them is honest so when you're on a like client you partially participate in validation and then you assume that at least x number of other like clients are honest to help you to actually help you get those very like get those guarantees so it's up to the users to pick which one and if they even care yeah i think like one thing that's actually been an interesting development is you know end users have actually had to learn the difference um in like client design sort of by uh spam and ddos type things right where recently sort of solana's had some some issues for end users using wallets and like clients um which you know every chain if we if we actually look at their history has had had similar issues i i guess i just want to point out that the end user does notice this but they may only notice it after they've already adopted something so how how do you how do you kind of view the pros and cons of uh you know the like client design for your chain and its impact on the end user who is you know their friend told them to download a wallet to buy an nft um is this follow-up to me yeah well since you said it's up to the user to care i feel like i think so you can like first of all it takes a hell of a lot more resources to stay on tip and to run solana at the latest blog if you're talking about just about verification um you can look at like how solana deployment works out of the you know 32 cores there's only four threads dedicated to the entire banking stage and that includes all the votes as well so if people don't know how solano works the execution of smart contracts it is optimized to the point that we can use it as our uh like the rail for voting and consensus so consensus runs just as a smart contract so out of like 300 million or so transactions per day 80 of them are just votes and it doesn't matter because they're still out of those 32 cores only four threads are dedicated to run all the all the transactions so validation is actually the smallest part of the entire stack it's just like the true cost of running solana is how do you synchronize globally all of this information that's being propagated and that requires a big pile of of ratio coding and signatures and everything else to propagate this data simultaneously around the world to all to all the machines so that part is the expensive part if we're talking about a world where users actually care about hey i want to make sure that the wallet and the thing that i hold hasn't had like these big validators committing an invalid state transition you can do that much much cheaper and you can do that with mn of n schemes and a whole bunch of other stuff and you can build those applications without actually impacting anything else about how solana runs and how it finalizes and those are different concerns these are not concerns about i want like fastest finality for trading and price discovery this is i'm holding my assets and i want to make sure that the big validators don't rug me um you can serve both of those and that's something that you know if the users demand it people will build it that's basically the answer but what users are demanding right now is i want the fastest cheapest finality because what we care about is distribution of these nfts to the largest number of users mustafa yeah so i think there's kind of two things there's unpack first of all there's this question like there's this point to be made that users don't care about violating the chain but you can also say like if you built a this if you built a completely centralized blockchain with one violator like from the user experience it looks it looks the same it's still got a wallet well we'll we'll use this care maybe they wasn't my point wasn't that users care about validating is that with a like client you have honesty assumptions that you need at least and other like clients to be honest right and that's an honest minority with with a normal decentralized network like a monolithic one you only need one out of the end so that's a real trust trade-off that users will make and it's not about going fully centralized or not right i'm not trusting facebook i'm like assuming okay there's a permissionless network at least one one under the end is honest or there's a permissionless network on a like client and i need at least 40 out of the you know 50 000 to be honest but it's not one-off end because you need to trust the honest majority to make sure there's no invalid state transition uh no any one of them can detect it and flag it and say hey what the just happened one of the entire majority of the chain just did an invalid state transition and i have the record to prove it and i'll provide the data availability for it you only need one out of n yeah and any on any like monolithic network yeah but that is food proofs what you're describing is food proof and that's what rollups do no it's simply here's the data to prove that the majority went uh like went rogue everybody can download it and then validate it locally yeah but that is the basically the definition of a fault proof like you prove like you give someone one transaction that's invalid and then they can check for themselves that's invalid um they would have to check the entire ledger history so this is a very big very big fraud proof but every every chain that has a full replication like tendermint bft whatever they all rely on this idea that if one out of the n detects an invalid state transition that they can pager duty goes off and then somebody yells what the happened so it sounds like what you're describing um is similar to the concept of alerts uh in the original bitcoin white paper but the problem with that is that it kind of simplifies back down to in a worst-case scenario you have to run a full node to verify the history of the chain of course it doesn't really fix the problem with that like you need to be able to prove light nodes that have low resource requirements that something is invalid yeah i mean i think this is when this happens right so one of the end one of the invalidators say something went wrong everybody in the community that's holding assets says okay give me the data and we'll validate it so like what is the cost of that versus the cost of me running a white client which then depends on at least some minority of like clients being honest and running but what's right but like what's the path there like if i'm running islander like client if that's if that is supported in the future and then someone sends me alerts like i'm not gonna do i have to like buy new hardware just to validate that that specific alert and that alert could be spam as well um maybe it's up to you right i think that validation is like i said is much much cheaper than running a full node because even in our current deployment right now only four out of the 32 threads is dedicated to smart contracts and they're not saturated so do you need to buy new hardware do you can use your laptop well i mean fundamentally that still doesn't really help like users like users might want again like light nodes need the ability to validate verify the chain with low resource requirements like they don't want like maybe some users are fine with buying new hardware but i'm pretty sure a lot of them aren't not a single light clone like client which where the rest of the chain is completely dishonest cannot validate anything so a user with a like line is not is validating in an honest minority so that trust assumption exists like it or not it's there yeah but unless minority assumption uh is inherently a significantly weaker trust assumption than the honest majority assumption because it's it's one of the things it's not an honest majority it's one out of n all i care about is that one of the one out of end of these validators is honest and then i can validate whether their a claim that there's been fraud in the network is drawn up but it doesn't fix the fundamental problem which is that you need you need to be able to have light nodes that can verify something went wrong with low resource requirements because in your scene they need to upgrade their resources to validate that so i don't believe that fundamental problem exists because the actual amdahl's law of execution looking at purely the smart contract side after everything has been validated is 20 of the actual running a validator so your requirements are just naturally at least 5x lower yeah but the 20 percent is still significant my laptop is not 20. i have a i have a question for you actually so like i have a question for you um like so like what is the like so the the the the goal of lana is to have like have a world computer model where you have like a single settlement layer it's like what's the end game like what like how do you scale that are you just going to like keep increasing the nerd resource requirements because like right now you're saying it's 800 and that's fine but like what's like there's going to be more demand and what's your type of end game so uh that happens naturally simply because for the same cost you get offered twice as much hardware roughly every two years so without even asking for it the new machines that are being deployed right now have dual 25 gigabit next um cost didn't change it's just everybody upgrades that's just part of the cycle in two years my laptop that i have right now is going to be twice as powerful you believe in perpetual motion machines for for moore's law like it's going to continue i believe really if so i believe that if we stop doubling the amount of uh you know vector dot products that we can do every two years then we should all be working on bunker coin i mean i i to some extent that's true but there's also this this problem right like from uh from a hardware design standpoint of you know we're already at like euv lithography there's not actually that much further we can go before we're like set like electrons tunneling between no tires bandwidth is going to keep doubling maybe single core performance is going to get saturated but you're going to keep doubling the amount of bandwidth and what we care about is bandwidth right we care about can we handle 20 gigabits in a single box right that would be really really hard right now but we can do one in two years we can definitely do two or you know maybe even eight depending on how architectures change and within 10 years you can definitely do 40 gigabits on a single machine but that's a that's an instantaneous transfer how much storage do i actually need locally because like one thing i guess from kind of your argument of uh hey look people will be able to to effectively submit this fraud proof by validating the entire chain is this idea that the storage requirements are somehow like static like yes we're only using four cores to process but you know if i have to actually validate the entire history of like 300 million transactions a day um you know that's basically like 30 billion transactions a year uh you know at some level the storage requirements also go up with bandwidth required so like how do you how do you kind of combat that in the long run you care about that happening within a certain uh fixed time period right within an epoch because all proof of stake chains have an epoch if you go beyond the epoch leaked keys can generate a fork right so you have some assumptions about like within this fixed amount of time somebody's going to say hey what the hopefully i mean it sounds to me like until he's relying on the assumption that moore's law will go on permanently which i think is an assumption that's like is not holding like it started it's not really has not hold true for all types of hardware over the past few years uh it has so far over the last 20 years the amount of raw throughput compute wise that you can do on a chip push through a neck push through an ssd push through the gpu that's been doubling at a steady pace but not for the hard drive we're talking about it's not cycles per second on a single core it's like raw bandwidth across like an array of uh you know like cores or dsps or whatever cuda threads i'm pretty sure it hasn't held true for hard drives if you look at the data but the other the other thing that's missing here i think it's like you're assuming that i think you're assuming that people like you you're ignoring the fact that people also have to sync this sync the chain from scratch like there's also like you need to be able you need to have people to be able to sync the chain and bootstrap from scratch yeah it's like 30 terabytes of uh transactions per year is what we're seeing right now 30 tb is not intractable i can and what's that going to increase you but you don't need the whole year right you need it within the epoch so what is it over like two days maybe 20 gigabytes or whatever is that like so fundamentally difficult that i could do it right and a very large number of users could do it so is that assumption like my point is is that assumption and that like hurdle to actually do this verification is it so much harder and such a big differentiator versus i have to have a light client constantly running plus i have to assume that there's an honest minority of light clients so that's the difference right like you can do that or you can rely on at least one out of end to provide you the data so the larger the network gets the more likely that one out of end is honest and this is where there is a clear difference between something like solana or tendermint where most tendermints run to like you know 150 or so validators solana is at you know close to 2 000. so i think like we're we disagree on what is a large number of users like if we're talking about 38 terabytes a year like we're talking about like reaching the stage where like as a you as a user i have to effectively buy a whole bunch of hard drives you know i put it in my living room to verify the chain whereas what we're targeting is like you should be able to do it on your laptop you shouldn't have to buy specialized hardware just to do just just to have authentic just to be able to verify that the chain is valid if you're not you're participating in validation with an honest minority assumption which is different we thought this majority assumption is a much weaker assumption than this majority assumption but like like i said we're not talking about honest majority assumption we're talking about hardware costs and one out of n assumption but so the user the user's trading and i i totally own that it's trading it's the user's trading hardware costs plus relying on at least one out of n of the validators to provide them the data so i i think uh you know this is probably where it's very clear the philosophical differences between like what it means to be a user and what it means to be a validator um are kind of hidden here uh but i i do before we kind of uh wrap up kind of want to talk a little bit about cross chain compatibility um you know obviously at this point i think it's pretty much impossible to say we're not going to have a multi-chain world with different use cases different qualities of service different types of guarantees for users and one kind of advantage of modular architectures is that they do spend a lot of time standardizing interfaces for cross-chain communication and they effectively build in messaging standards or rely on actually specified messaging standards whereas for monolithic architectures it can be quite uh different as we we've kind of seen you know trying to basically translate between different virtual machines and get sort of bitwise reproducibility is can actually be quite hard especially when there's synthetic assets involved so how do how do kind of like monolithic architectures compete with uh platforms that have focused on building in native messaging and do you think it's essential or do you think that you know eventually there will just be a communication center that everyone just agrees on i mean well i can start i guess but i think like one like paradoxical thing of like the vision of like having a shed like a single settlement layer that solano relies on is that we already don't we already don't have a like if you look at the blockchain system we don't have a single segment layer like we do have a multi-trained ecosystem with bridges like you have wormhole bridge between ethereum and solana like it the market has already shown like we can have multiple settlement layers like we don't need a single settlement chain so if that if the market has already decided that then and and and right now we have these insecure trusted bridges that can have that have multisigs then why don't we just embrace that and have a like a modular architecture that uses roll-ups and has these more secure trust minimized bridges anatoly um bridges are man are scary um and the part that is the scary part is not the collusion uh which something like a native like roll up architecture fixes it's the smart contract risk and implementation risk because of the just the even the simplest piece of code can have a bug and that's that's really the scary part um to that extent though i think a natively baked in like roll up where this is the standard implementation that is owned by the layer one is probably likely to get um to a point of like extremely high security and confidence in the code um much much faster than something where every user does their own thing every developer does their own thing um so that part is like you know who knows how that will play out um the stuff that does work pretty easily and pretty well are actually like the very very dumb simple things like usdc where that's effectively bridged across multiple chains and there is a very heavy trust assumption on on circle right to make sure that there's dollars in the bank um and you saw that be pretty successful with rap bitcoin as well on ethereum um i can't tell honestly if this is how things will play out over the next like five years but i feel like that's probably a model that if there's demand for cross chain assets in large volumes that's a model that's going to stick around for for users and i don't know i don't know why would users would switch from that to a more permissionless one unless it takes much more time to get assets listed in in these centralized systems which is you know obviously a hurdle so i'm i'm uh i i see a lot of advantages so this is like a point that's really hard to uh to uh to fight against if you have very native very baked in roll up model that is totally uh part of the of the layer one um i would tend to think that's going to have a lot more confidence in terms of like code security and that is the biggest surface you know that that's the biggest attack surface in all the bridge solutions that we have out today yeah and and a lot of ways i think there's there's definitely been some truth in that the optimism bug that was found was you know very similar to the wormhole bug um and it's mainly this idea of like having the synthetic asset on both sides and like ensuring that they say synchronize supply stay synchronized price stays roughly synchronized that seems to be the source of these these kind of bugs um so so definitely i think the we're still not at the point of hey like the contract flash node bugs are uh not kind of an issue um so you know before we wrap up um i guess you know if you were to uh give one compliment to the other type of architecture uh and say something nice about it what would you say i like salon linux i like slalon's execution environment i think parallelization is a very good idea um i just don't like the division of a single settlement layer uh basically hey something nice was still said oh still method i really like i really like the idea of actually separating data availability and focusing development effort on that because i think that's easier to make it secure and the biggest hurdle to iterating is security so there's actually i think an opportunity for modular architectures if they can iterate faster because they've broken down the the pieces into something more manageable um so that that's like a honest honest compliment well mine is honest as well [Laughter] um awesome i don't know are we doing questions or no we're good all right well hey thanks everyone for for listening in uh thanks anatoly i'm sure it's like i don't know 5 a.m for you no it's 9 a.m okay it's perfect time it's still too early thank you guys still too early thanks for waking up um yeah all right thank you thanks tarun for moderating that was a really really good job um all right guys that concludes the modular summit thank you all for coming i want to thank all our sponsors and speakers um again we have drinks right now so hang out and um we're gonna we there's swag as well if you want one of these t-shirts build modular be free we have them on a table in the back so thank you again [Music] [Music] [Music] you 