well great to see you virtually um great to see you guys too yeah i i uh this might be the only section today that needs no introduction uh so can kind of go straight to it um so i actually want to start with this intro question that uh i think was teased online yesterday uh of sort of you know before we kind of get into the details of modular versus monolithic i think we should actually just say let's suppose each of you uh were kind of stuck in an elevator for 30 seconds you had a 30 second pitch and there was an engineer who just quit netflix because well i mean stock price crashed and uh they're like i wanna i wanna like understand like why i should build something um either on slano or celestia what's your 30-second pitch um and for me okay so this netflix this former netflix engineer is presumably a web 2 engineer so i would use a web 2 analogy if you're a web 2 engineer and you wanted to build a web application you wouldn't want to build it on a shared hosting provider like let's say if you're familiar with dream dream host or geocities or google pages if you use a shared web hosting provider you're restricted in what you can deploy because you're stuck with the same execution environment that's provided to you by that shared execution provider and also because you're set you're sharing the same server as everyone else you're also limited in scalability that is what that is very similar to deploying your application on a shared smart contract platform which is a monolithic blockchain nowadays web 2 developers build use virtual machines on on systems like amazon ec2 or aws and that allows those developers to have full have their own virtual server and have full control over the execution environment and scalability of the application and that is what building a roll-up is like you effectively have your own blockchain that you can do whatever you like on it and have any execution environment you want on it and you can scale better because you're not sharing the same execution resource pricing as everyone else great i think we were traveling slightly close to speed of light right there because we went a little over 30 seconds on earth pitch but anatoly um don't use a blockchain unless you want unless you critically need finality in a shared context with a lot of other financial applications like exchanges stablecoin providers all those things if you don't need that don't even use a blockchain to begin with but if you need that then salon is the cheapest fastest way to get to the most the largest set of these as quickly as possible well just so that we get uh kind of both of your individual definitions um in your own words how would you describe the other architecture so mustafa how would you describe monolithic and uh anatoly how would you describe modular and the answer you want to start um i would i would describe modular as an attempt to break down the different functions of the of the chain with some clean interfaces and build them as separate components i would describe monolithic as trying to build a global shared computer similar to the original ethereum world computer vision all right so let's start by uh actually starting with with this idea of like developer costs developer ux startup costs so if you're talking to someone who was you know just getting their their feet wet with developing on either salon or celestia how would you kind of describe the startup costs of going from you know you know just a hack a kind of toy project to actually like running something in production to like maybe having a dow having kind of longer term sustainability and uh you know how would you describe things like how long does it take to get familiar with the language the architecture development environment you know just imagine you're someone who who is just getting started um and totally you look like you well i'm assuming all the cool devs already know rust if not it's a it's a language that we didn't build it's a it's a language that uh amazing other folks have built with its own community and you know people out of fangs are publishing open source ros code so that part is pretty easy for anybody to pick up because it's a general purpose what language and the development environment is really just as close to native rust as we can make it so you just use cargo and use anchor to glue all your programs together so we've seen folks go from a hackathon to a project with like 8 billion tbl in um like six weeks shout out to the saber guys who who slept on the floor of the salon office to get that done um so i think the general goal for model blockchains is that we want to make it we want to make it so that deploying your own like roll-up chain or your own blockchain should be as easy um as deploying a smart contract and like we're practically like almost there like you know with the cosmos sdk you can deploy a blockchain in seconds um with like roll up technology you can just fork that roll up it can fold the smart contract and have your own roll up and the idea is like if you if we make it deploying your own chain as easy as playing a new smart contract then why wouldn't you do that uh like you you would have much more flexibility over the trade-offs in your execution environment and you can tailor it more to the application that you need so i i to to that point i think um you know one thing historically uh is that the linux took a very long time to actually get sort of the multi-processing multi-app multi-tenancy environments to work correctly uh it took a long a lot of pain and changing things in the kernel uh and but however once it was done it was like a huge floodgate kind of opened up a ton of app development opportunities like node.js would never be able to run on a linux machine without smp and linux 2.6 basically uh so do you think it'll take a similar amount of time for us to get to the point where you have this sort of automatic scheduling of roll-ups automatic kind of deployment uh things like that and and that the monolithic sort of version is is sort of a crutch on the way there or do you actually think there's sort of a fundamental difference um and obviously you know you both probably have different opinions on that yeah so i mean i think um if you look at the modular if you look at if you take a modular blockchain stack um like even if you take each module in isolation you know each module is simple but there is indeed complexity in the way that the stack integrates so yes modular blockchain stacks um if you zoom if you look at the big picture have more complexity than a monolithic stack not on the individual module level but throughout the whole stack but but that's not an issue because the whole point of modularity is that you don't have to like know or care about what's going on in the entire stack we're reaching a situation where for example you know core dev in in guess all the way uh said that like guess there's now it's circle the guest client is so kind of has so much complexity now that no guest developer actually knows what's going on in the entire guest node and now i think that's a reason that's part of the reason why modularity is important it does allow you to do more and have more complicated and blockchain architectures that do more without needing a single person to understand the entire analytic stack and so yes to answer that question directly and it does it will take us longer to get there and then like it is it does take us longer than building a monolithic blockchain but you know since 2008-2020 people have just been building new layer ones and we've ended up in this like cycle of new layer ones so it's now time to kind of break that cycle and kind of evolve into a stage where um now we have like so much traction in blockchain space let's modularize it and make sure that all the different innovations that people are building can actually be useful and holly um the irony is a 2-6 once it was stabilized killed the microkernel design and the monolith the colonel won is true the the rust mock kernels have never really caught on um and that's so i kind of think that it's going to depend what the use cases are if you have dependencies on composition between applications and these applications are you know like um not static right you have constant kind of churn between the composition set then you really need one giant state machine to glue them all together and because what the users are going to care about is how quickly do i access all the state whatever that may be maybe it's like positions and serum manga and like uxd or whatever and how quickly it's finalized and how quickly it's connected to all the other financial institutions that i care about you know from stable coins from two exchanges and that network is not something you can spin up in a smart contract um that that's actually kind of the the living breathing glue of the chain itself so you can have that in a modular architecture right there's a bunch of tendermints a bunch of them have different pieces but if you have use cases that are run even 20 worse right have 20 percent higher fees 20 higher latencies that's like your phone running 20 slower uh people are gonna notice and applications and everything else will shift to that environment where everything is optimized to the hill and to do those optimizations you're you don't really care about the design or how complicated it is right what you care about is like how fast can we achieve these goals for for the use cases and this is why you know modern operating systems are all monolithic um even when somebody tries to build like you know a microkernel design when it actually shipped you look at like the 80 of the use cases that people use the frame buffers everything else it's all punched through directly to the um you know directly to the kernel without without using any of the um you know these modular features so how this plays out is going to be largely like use case dependent i personally believe that what these chains are good at is running uh like a dex right it's they're good for price discovery they're good at managing like offers and bids and all this other stuff for digital items and that under that like market function what these things provide allow everyone else to build unpredictable use cases like nfts which are you know their own financial uh you know financial contract but at the end of the day like what you need to do is synchronize offers and bids and like do this as fast and as quickly as possible and connect them to all the rails yeah i want to challenge the microkernel analogy and i don't think um analogies are bad so rip it apart i would say small closure i would the analogy i would use is like in linux in the linux operating system you have kernel space you can have code that runs in candle space and user space uh i would say having a monolithic chain is like running all the code for user applications in the kernel space but the way i see it is like having modular blockchain is having a user space which is like the l2 and having like actual applications running there rather than in the kennel yeah i mean that's the i i i i i feel like there's sort of merits to both of these if we look at history uh history has sort of preserved both models um you know there's obviously an intense amount of work on extremely low latency code kernel bypass like all the types of stuff that anatoly was mentioning uh but then obviously you know much of the value of the consumer internet has come from realistically making it so that i don't need to know what a page table is to write a piece of code right like what percent of javascript developers have ever heard the phrase page table probably sub two percent so there's there's a sense in which developer friendliness is also quite important uh and to that kind of point uh you know another thing that of course people love talking about nowadays a lot is is uh extractable value from from validators and miners uh and this is something that you know both developers and users of these systems have spent a lot more time thinking about and one uh thing to kind of compare and contrast between monolithic and modular architectures is the idea that uh modular architectures have a lot more uh sort of things like you know you have to wait for certain transactions to go through there's certain like predictable latencies that allow people to sort of uh find newer forms of extractable value there's sort of this uh you know sense in which the tana tolle's point earlier heterogeneous fees make it perhaps more easier to extract value so one question is is do you think this ability perhaps to extract more value from modular architectures for you know for arbitragers and sort of market makers do you think that's a feature or a bug because it could actually be something that drives liquidity as well as also kind of can parasitic towards users and sort of like where do where do where do both of you kind of see that i mean it depends on how your execution environment works specifically if your execution environment has a sequencer so like in ethereum l2s like the mev is basically just moved to the l2 you know sequencers like in optimism for example in optimism for example the optimism roll up they see that as a feature and not a bug because they see that the sequencer can extract the mev and use that to fund public goods and that also kind of goes to um to kind of correspond to this point about finality um it's not necessarily the case that you need layer one finality for users to have sufficient finality on the transactions going through so for example like arbitrary's inbox model or like optimism sequencing model you can get a finality by looking either at the inbox or asking the sequencer immediately in in a second uh like faster than the ethereum block time so like there's different like uh like guarantees of finality that's more of a spectrum than just relying or uh like on layer one finality anatoly so those are all kind of compromises right because for it to be a true l2 it needs to be able to fail at any moment and that means that whatever partial finality you get is then in question when um you know the whatever however failure recovery works in that in that setup um and that trade-off is probably okay for applications and users in like a sandbox but you know like what is the whole point of a blockchain right is it is it to build an application that could run it's on a database or is it a a global you know replacement for a large chunk of finance um so you know we'll probably see both of these models survive because of that because we don't really know why why these things exist yet um i think with regards to mev um i tend to agree that so far we're probably the only thing that we see that um actually funds and create you know generates like value in in these chains is mev and you know in the best of light you have very fancy algorithms that try to predict the future and they want to improve prices and offers and everything else on this chain and you know they run their racks at gpus and they extract mav and in theory it should get competitive to the point that they should be bidding for users to send their orders directly to them you see some of that with flashbots and some of that with like gito network and solana so those are all good things right because in theory a user that doesn't care about a mav should in fact see a better price and maybe even a rebate for their transactions instead of a fee but that's still kind of i think pretty far away um like we're still not yet at like the level of adoption where i could you know say that yeah this is true and this is how these chains run um so a lot of a lot of questions still remaining um but for the for the sake of the debate my uh my strong opinion is that mev will be the thing that's actually driving development and hardware upgrades and you know actually going after after users in these spaces and on blockchains yeah i mean i guess to that point if we think about the something that you said in terms of replacing existing financial systems or sort of having the ability to uh service sort of a larger spots in the market mev in in in solana to some extent right now and then solana and avalanche in particular uh where there is a lot of like validators advertising like hey we'll sell you this much space on in this block at this time and people basically are co-locating do do you know do you think that this is sort of uh deleterious and that it'll just like reduce to the existing sort of model of sort of you know co-location around exchanges co-location around validators or do you think that you know you could actually sort of make this more sustainable with a different fee model or you know potentially better cryptographic solutions because i i do think one thing from the monolithic world is there's a lot more probabilistic uh mev like there's there's sort of two ways of thinking about it mev on single chains is a congestion game right there's like just a fixed amount of opportunities and way more bidders than opportunities and it's more about just like constructing a mechanism that matches those mev on on in modular architectures is much more probabilistic right it's about routing figuring out which path to take and so i guess from both your perspectives do you do you think like there is a way in which the economics model can can thwart kind of that becoming all of the volume on the network or do you think that you know effectively there's there's no way of avoiding that and we're sort of stuck with the existing system um i can go first so the way i think about it is that we want to make it as competitive as possible and as easy for anybody to enter you know the the network and offer like my my my whatever mav solution because that should drive the you know the amount of like f actual like profit extracted by these things to as close to to zero and that should be a good thing right like what you should see in the end is better prices uh prices that reflect the true value of things you know globally all synchronized across the the same state machine and to that regard at least in solana it's possible to run multiple block producers on the exact same state concurrently at the same time doesn't do that yet but in theory that that's something that is doable because of the the way the vdf works we can effectively splice the final ordering using the vdf ordering between all of these so what that should hopefully do is that instead of only having one block producer at a time some position in the world you now have n and then you go talk to the closest one geographically that reduces out latency and then that thing is then running whatever out goes and whatever it needs to optimize its order flow and it's doing that competitively with everyone else and in theory that should reflect you know the lowest price of any good offered on chain globally um you know within within the speed of light latencies around the world um and if we can do that then in theory price updates moving through solana are moving at the speed of light through fiber which is as fast as news travels and then it's competitive with something like nasdaq and um cme um so that's kind of the dream the horizon and this is how we've been building like everything you know looking at that final state how do we get there and what are the pieces we need to get to and when you kind of take that perspective you don't really care if the architecture is modular or monolithic what you care about is can we synchronize like information globally as close to the speed of light as possible um and usually at least so far there's no gains to be had using a modular architecture that right now everything that we've been doing is around this like monolithic state because that's what's going to get us closer to that final state of you know global information symmetry all right well i'm excited for mustafa's response to that yeah like i think this discussion kind of highlights the key kind of philosophical difference between like solana and systems like celestia or ethereum which is i think solana kind of you know systems like solana view blockchains more as a distributed computing you know platform where you can like you know have a global settlement whereas like we like celestia and also other systems like bitcoin and ethereum we see blockchains fundamentally the main the the fundamental purpose of blockchains is that it's a verifiable competing platform not just distributed but very verifiable and and that means there's a there's a focus on you know decentralization in that setting means that you need to allow light nodes to verify the state of the chain uh otherwise like if your goal is just simply to have you know fast like like fast agreement on the state how does that goal like fundamentally differentiate from you know what web 2 achieves with essentially with a centralized distributed database even um like you could argue you know that the fundamental difference is that you have a global set of distributed parties agreeing in that state um but without the ability for end users to verify these states in my opinion that takes away one of the key properties of blockchains which is that like well-resourced actors cannot violate what the social consensus of the chain has agreed what the rules of the chain are like like like if this lana valdez for example wanted to change the rules of the chain how would end users like verify that without finding a you know a well-resourced node yeah you teed up the next question which is you know to to those that haven't spent time like sort of in the in the weeds on these modular and monolithic architectures force very different uh separations of storage and execution uh for light clients versus full clients so for the audience uh first let's just start with describing kind of how uh like clients work um for salon and celestia and sort of what where the division is and sort of which parts the like client has to store and execute first not and mean mustafa since you brought up my clients so like light nodes effectively um like allow end users ordinary users with like whether you know a cheap laptop or mobile phone to have almost the same level of security as a full node because they can get assurances about the state of the blockchain um using technologies like fraud proofs or zk proofs and like this is the fundamental reason why the bitcoin community has decided not to increase the block size limit like in in theory bitcoin could do a zillion transactions per second if they increase the block size limit and optimize the node software um but the bitcoin community has fundamentally decided not to do that because it would increase the resource requirements for end users to run full nodes and validate the chain which is like the the pretty much the critical or the critical aspects of what a blockchain is supposed to um guarantee that the state of the chain uh is correct effectively and but with light club with new tech with new light color with new light node technology that has that uses uh like fraud proofs and data their body sampling and zk approves you can now increase the block size limit without compromising the ability for end users to validate and verify the chain analytic like clients on solana um so the devil's in the details obviously here um when you look at like e2 like deployment like go to nodewatch.io there's about 6500 machines that run the 300 000 eth2 nodes validators there's only 6 500 actual boxes um so the real world deployment is that you take a solano validator and then you load it up with like a few hundred ety nodes right e2 validators and that's your that's your eth2 deployment so the real world is that machines are getting bigger and the salon validators are not uh stacks of xeon processors they're like 32 core systems which is what you get out of you know like a data center right now for you know 800 bucks a month um which is not insane by any means it's not building racks and racks of of systems like you would for you know like uh centralized service to handle all those users um so there's a bit of like kind of i think uh like i don't know um like you have to actually look at the details of how these things are deployed so it's it's uh i think uh it would be misinforming to say that the amount of hardware necessary to run solana is that significantly different than um any of the other chains when you actually look at how these things are are being deployed in the real world um but like clients themselves when you're talking about like a different system for verification uh there's obviously trade-offs so whenever anyone brings this up the question that i have is assuming all the other nodes everything else has been destroyed how many like clients do you need to re rehypothecate the network to reconstruct it so how many nodes are you trusting when you run this thing and those trust assumptions are important to users because the end state of like a monolithic chain is when you don't run any nodes you're trusting that at least one out of the whatever 3000 boxes that run the replicate solana at least one of them is honest so when you're on a like client you partially participate in validation and then you assume that at least x number of other like clients are honest to help you to actually uh help you get those very like get those guarantees so it's up to the users to pick which one and if they even care yeah i think like one thing that's actually been an interesting development is you know end users have actually had to learn the difference in like client design sort of by spam and ddos type things right where recently sort of solana's had some some issues for end users using wallets and like clients um which you know every chain if we if we actually look at their history has had had similar issues i i guess i just want to point out that the end user does notice this but they may only notice it after they've already adopted something so how how do you how do you kind of view the pros and cons of uh you know the like client design for your chain and its impact on the end user who is you know their friend told them to download a wallet to buy an nft um does this follow up to me yeah well since you said you said it's up to the user to care i feel like i think so you can like first of all it takes a hell of a lot more resources to stay on tip and to run solana at the latest block if you're talking about just about verification um you can look at like how solana deployment works out of the you know 32 cores there's only four threads dedicated to the entire banking stage and that includes all the votes as well so if people don't know how solano works the execution of smart contracts it is optimized to the point that we can use it as our uh like the rail for voting and consensus so consensus runs just as a smart contract so out of like 300 million or so transactions per day 80 of them are just votes and it doesn't matter because they're still out of those 32 cores only four threads are dedicated to run all the all the transactions so validation is actually the smallest part of the entire stack it's just like the true cost of running solana is how do you synchronize globally all of this information that's being propagated and that requires a big pile of uh of ratio coding and signatures and everything else to propagate this data simultaneously around the world to all to all the machines so that part is the expensive part if we're talking about a world where users actually care about hey i want to make sure that the wallet and the thing that i hold hasn't had like these big validators committing an invalid state transition you can do that much much cheaper and you can do that with mn event schemes and a whole bunch of other stuff and you can build those applications without actually impacting anything else about how solana runs and how it finalizes um and those are different concerns these are not concerns about i want like fastest finality for trading and price discovery this is i'm holding my assets and i want to make sure that the big validators don't rug me um you can serve both of those and that's something that you know if the users demand it people will build it that's basically the answer but what users are demanding right now is i want the fastest cheapest finality because what we care about is distribution of these nfts to the largest number of users mustafa yeah so i think there's kind of two things there's unpack so first of all there's this question like there's this point to be made that users don't care about validating the chain but you could also say like if you built a this if you built a completely centralized blockchain with one violator like from the user experience it looks like it looks the same it's still got a wallet well we'll use this care uh maybe they wasn't my point wasn't that users care about validating is that with a like client you have honesty assumptions that you need at least and other like clients to be honest right and that's an honest minority with with a normal decentralized network like a monolithic one you only need one out of the end so that's a real trust trade-off that users will make and it's not about going fully centralized or or not right i'm not trusting facebook i'm like assuming okay there's a permissionless network at least one one out of the n is honest or there's a permissionless network on a like client and i need at least 40 out of the you know 50 000 to be honest but it's not one-off end because you need to trust the honest majority to make sure there's no invalid state transition uh no any one of them can detect it and flag it and say hey what the just happened one of the entire majority of the chain just did an invalid state transition and i have the record to prove it and i'll provide the data availability for it you only need one out of n yeah on any on any like monolithic network yeah but that is food proofs what you're describing is food proof and that's what roll ups do no it's simply here's the data to prove that the majority went uh like went rogue everybody can download it and then validate it locally yeah but that is the basically the definition of a fault proof like you prove like you give someone one transaction that's invalid and then they can check for themselves that's invalid um they would have to check the entire ledger history so this is a very big very big fraud proof but every every chain that has a full replication like tendermint bft whatever they all rely on this idea that if one out of the n detects an invalid state transition that they can pager duty goes off and then somebody yells what the happened so it sounds like what you're describing um is similar to the concept of alerts in the original bitcoin white paper but the problem with that is that it kind of simplifies back down to in a worst-case scenario you have to run a full node to verify the history of the chain so this is of course so it doesn't really fix the problem with that like like you need to be able to prove the light nodes that have low resource requirements that something is invalid yeah i mean i think this is the main i only need these resources when this when this happens right so one of the end one of the invalidators say something went wrong everybody in the community that's holding assets says okay give me the data and we'll validate it so like what is the cost of that versus the cost of me running a white client which then depends on at least some minority of white clients being honest and running but what's so like what's the path there like if i'm running as long like light client if that's if that is supported in the future and then someone sends me alerts like i'm not gonna do i have to like buy new hardware just to validate that that specific alert and that alert could be spam as well um maybe it's up to you right i think that validation is like i said is much much cheaper than running a full node because even in our current deployment right now only four out of the 32 threads is dedicated to smart contracts and they're not saturated so do you need to buy new hardware do you can use your laptop well i mean fundamentally that still doesn't really help like users like users might want again like the light nodes need the ability to validate verify the chain with the resource requirements like they don't want like maybe some users are fine with buying new hardware but i'm pretty sure a lot of them aren't not a single like y client which where the rest of the chain is completely dishonest cannot validate anything so a user with a like line is not is validating in an honest minority so that trust assumption exists like it or not it's there yeah but unless minority assumption uh is inherently a significantly weaker trust assumption than an honest majority assumption because it's it's one of the things it's not an honest majority it's one out of n all i care about is that one of the one out of any of these validators is honest and then i can validate whether their a claim that there's been fraud in the network is drawn up but it doesn't fix the fundamental problem which is that you need you need to be able to have light nodes that can verify something went wrong with low resource requirements because in your scene they need to upgrade their resources to validate that so i don't believe that fundamental problem exists because the actual amdahl's law of execution looking at purely the smart contract side after everything has been validated is 20 of the actual running a validator so your requirements are just naturally at least 5x lower yeah but the twenty percent is still significant my laptop is not twenty but i have a i have a question for you actually so like i have a question for you um like so like what is the like so the the the the goal of lana is to have like have a world computer model where you have like a single settlement layer it's like what's the end game like what like how do you scale that are you just gonna like keep increasing the nerve resource requirements because like right now you're saying it's 800 and that's fine but like what's like there's going to be more demand and what's your turn game so uh that happens naturally simply because for the same cost you get offered twice as much hardware roughly every two years so without even asking for it the new machines that are being deployed right now have dual 25 gigabit next um cost didn't change it's just everybody upgrades that's just part of the cycle in two years my laptop that i have right now is going to be twice as powerful you believe in perpetual motion machines for for moore's law like it's going so i believe that if we stop doubling the amount of uh uh you know vector dot products that we can do every two years then we should all be working on bunker coin mean i i to some extent that's true but there's also this this problem right like from uh from a hardware design standpoint of you know we're already at like euv lithography there's not actually that much further we can go before we're like set like electrons tunneling between no wires bandwidth is going to keep doubling maybe single core performance is going to get saturated but you're going to keep doubling the amount of bandwidth and what we care about is bandwidth right we care about can we handle 20 gigabits in a single box right that would be really really hard right now but we can do one in two years we can definitely do two or you know maybe even eight depending on how architectures change and within 10 years you can definitely do 40 gigabits on a single machine but that's a that's an instantaneous transfer how much storage do i actually need locally because like one thing i guess from kind of your argument of uh hey look people will be able to to effectively submit this fraud proof by validating the entire chain is this idea that the storage requirements are somehow like static like yes we're only using four cores to process but you know if i have to actually validate the entire history of like 300 million transactions a day um you know that's basically like 30 billion transactions a year uh you know at some level the storage requirements also go up with bandwidth required so like how do you how do you kind of combat that in the long run you care about that happening within a certain uh fixed time period right within an epoch because all proof of stake chains have an epoch if you go beyond the epoch leak keys can generate a fork right so you have some assumptions about like within this fixed amount of time somebody's going to say hey what the hopefully i mean it sounds to me like until he's relying on the assumption that moore's law will go on permanently which i think is an assumption that's like is not holding like it started it is not really has not hold true for all types of hardware over the past few years uh it has it's over the last 20 years the amount of raw throughput compute wise that you can do on a chip uh push through a nick push through an ssd push through the gpu that's been doubling at a steady pace but not for the hard drive what we're talking about is not cycles per second on a single core it's like raw bandwidth across like a an array of uh you know like cores or dsps or whatever cuda threads i'm pretty sure it hasn't held true for hard drives if you if you look at the data but the other the other thing that's missing here i think it's like you're assuming that i think you're assuming that people like you you're ignoring the fact that people also have to sync this sync the chain from scratch like there's also like you need to be able you need to have people to be able to sync the chain and bootstrap from scratch yeah it's like 30 terabytes of uh transactions per year is what we're seeing right now 30 tb is not intractable i can and what's not going to increase you but you don't need the whole year right you need it within the epoch so what is it over like two days maybe 20 gigabytes or whatever is that like so fundamentally difficult that i could do it right and a very large number of users could do it so is that assumption like my point is is that assumption and that like hurdle to actually do this verification is it so much harder and such a big differentiator versus i have to have a like client constantly running plus i have to assume that there's an honest minority of like clients so that's the difference right like you can do that or you can rely on at least one out of end to provide you the data so the larger the network gets the more likely that one out event is honest and this is where there is a clear difference between something like solana or tendermint where most tendermints run to like you know 150 or so validators solana is at you know close to 2 000. so i think like we're we disagree on what is a large number of users like if we're talking about 38 terabytes a year like we're talking about like reaching the stage where like as a you as a user i have to effectively buy a whole bunch of hard drives you know i put it in my living room to verify the chain whereas what we're targeting is like you should be able to do it on your laptop you shouldn't have to buy specialized hardware just to do just just to have authentic just to be able to verify that the chain is valid if you're not you're participating in validation with an honest minority assumption which is different assumption is a much weaker assumption than this majority assumption but like like i said we're not talking about honest majority assumption we're talking about hardware costs and a one out of n assumption but so the user the user's trading and i i totally own that it's trading it's the user's trading hardware costs plus relying on at least one out of n of the validators to provide them the data so i i think uh you know this is probably where it's very clear the philosophical differences between like what it means to be a user and what it means to be a validator um are kind of hidden here uh but i do before we kind of uh wrap up kind of want to talk a little bit about cross chain compatibility you know obviously at this point i think it's pretty much impossible to say we're not going to have a multi-chain world with different use cases different qualities of service different types of guarantees for users and one kind of advantage of modular architectures is that they do spend a lot of time standardizing interfaces for cross-chain communication and they effectively build in messaging standards um or rely on actually specified messaging standards whereas for monolithic architectures it can be quite uh different as we've kind of seen you know trying to basically translate between different virtual machines and get sort of bitwise reproducibility is can actually be quite hard especially when there's synthetic assets involved so how do how do kind of like monolithic architectures compete with uh platforms that have focused on building in native messaging and do you think it's essential or do you think that you know eventually there will just be a communication center that everyone just agrees on i mean well i can start i guess but i think like one like paradoxical thing of like the vision of like having a shed like a single settlement layer that london relies on is that we already don't we already don't have a like if you look at the blockchain system we don't have a single settlement layer like we do have a multi-trained ecosystem with bridges like you have wormhole bridge between ethereum and solana like it the market has already shown like we can have multiple settlement layers like we don't need a single settlement chain so if that if the market has already decided that then and right now we have these insecure trusted bridges that can have that have multi-sigs then why don't we just embrace that and have a like a modular architecture that uses roll-ups and has these more secure trust minimized bridges and totally um bridges are man are scary um and the part that is the scary part is not the collusion uh which something like a native like rollup architecture fixes it's the smart contract risk and implementation risk because of the just the even the simplest piece of code can have a bug and that's that's really the scary part um to that extent though i think a natively baked in like roll up where this is the standard implementation that is owned by the layer one is probably likely to get um to a point of like extremely high security and confidence in the code um much much faster than something where every user does their own thing every developer does their own thing so that part is like you know who knows how that will play out um the stuff that does work pretty easily and pretty well are actually like the very very dumb simple things like usdc where that's effectively bridged across multiple chains and there is a very heavy trust assumption on on circle right to make sure that there's dollars in the bank and you saw that be pretty successful with rap bitcoin as well on ethereum um i can't tell honestly if this is how things will play out over the next like five years but i feel like that's probably a model that if there's demand for cross-chain assets in large volumes that's a model that's going to stick around for for users um and i don't know i don't know why would users would switch from that to a more permissionless one unless it takes much more time to get assets listed in in these centralized systems which is you know obviously a hurdle so i'm i'm uh i i see a lot of advantages so this is like a point that's really hard to uh to uh to fight against if you have very native very baked in roll-up model that is totally uh part of the of the layer one um i would tend to think that's going to have a lot more confidence in terms of like code security and that is the biggest surface you know that that's the biggest attack surface in all the bridge solutions that we have out today yeah and and in a lot of ways i think there's there's definitely been some truth in that the optimism bug that was found was you know very similar to the wormhole bug um and it's mainly this idea of like having the synthetic asset on both sides and like ensuring that they say synchronize supply stay synchronized price stays roughly synchronized that seems to be the source of these these kind of bugs um so so definitely i think the we're still not at the point of hey like the contract flash node bugs are not kind of an issue um so you know before we wrap up um i guess you know if you were to uh give one complement to the other type of architecture uh and say something nice about it what would you say i like salon linux i like solana's execution environment i think parallelization is a very good idea and i just don't like the division of a single settlement layer uh basically hey something nice was still said oh still method i really like i really like the idea of actually separating data availability and focusing development effort on that because i think that's easier to make it secure and the biggest hurdle to iterating is security so there's actually i think an opportunity for modular architectures if they can iterate faster because they've broken down the the pieces into something more manageable um so that that's like a honest honest compliment well mine is honest as well um awesome i don't know are we doing questions or no we're good all right well hey thanks everyone for for listening in uh thanks anatoly i'm sure it's like i don't know 5 a.m for you 