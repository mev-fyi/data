[Music] because i wanted to begin with introductions and arjun you are our guest and so why don't you quickly introduce yourself um tell us a little bit more about connex and then we'll move to the others uh yeah absolutely um i'm arjun i'm one of the founders and project lead of canucks um let's see i've been in this space since 2016. i became interested in the space because i wanted to find ways to build public goods that are both non-sovereign and non-corporate i think it's a it's really important for the world if you can have um uh some types of things that exist so sort of sort of like the internet itself that exists that are accessible by anybody and that are really untemperable um by any specific entity um i think that's that's really how we get to a world that makes sense um in in light of things like globalization um i was always really interested in infrastructure so i started was working on like infrastructure startups in the space in 2016 and 2017 um and then in may of 2017 i started connects with my co-founders and rahul um and the goal was always this technology ethereum specifically is is at the time is like extremely important um and uh and let's figure out ways to get it into people's hands faster and so pretty quickly that led us down the path of scalability because that was really the biggest bottleneck for everybody um and uh and we became scalability researchers in in like early 2018 we built one of the first ever l2s on ethereum this was a state channel network in partnership with spank chain because spanx chain was the only place that that actually had an attraction in the early days of the ethereum community um and then uh we uh we ended up uh you know continuing to do research alongside the uh the the kind of core scalability community um that's how i met john for instance uh and then as that as that community started to focus more and more on rollups we became interested in uh interoperability basically communicating between these different roll-ups um and between potentially other sovereign chains uh finding ways to to make that experience seamless if we're moving execution into many paralyzed environments which is actually a really good segue into into this into this talk awesome and then from our side we have john and mustafa mustafa please introduce yourself and then john hello i'm the co-founder and ceo of celestial apps which is a which is the first modular blockchain network and the basic idea is that we're building a modular consensus and data network to enable anyone to very easily deploy their own blockchain with very minimal overhead without them having to worry about deploying their own consensus network and they would do so they can do that by deploying a roll-up or more specifically a solver and roll-up on top of celestia so i've been thinking quite a lot about um like bridging in a multi-chain world and i've written some things about topic and this idea of trust minimize bridging and also committee-based bridging so i'm looking forward to have discussed these topics more in this in this space sweet john hello everyone uh my name is john uh also co-founder of celestia labs i'm the chief research officer so i generally do protocol research and specification previously i was at consensus doing general layer 2 scalability research where i met a lot of the fine folks that are on this call today great and we'll introduce james when he joins in the meantime i will set the stage for the conversation and let's get right into it so if you are listening to this twitter space you know something about where the world is going you can likely see that we are in the midst of a complete tectonic shift in blockchain infrastructure much of this innovation is driven by deep frustrations with monolithic chains in a single word that change can be summed up in the word modular and many are saying that modular is now a movement where we're seeing early signs of a new culture values and even customs lately what has caught my interest is the intersection of interoperability and modular blockchains i found myself asking the question what will the future of secure bridging and cross communication actually look and feel like in the modular world gents i'm intensely curious about the changes that are going on right now at this exact moment in time in regards to that intersection that i just described specifically i want to go deep on the trends and forces at play that are obvious and not so obvious and i want to cover a wide range of topics for example i want to go into clusters optimistic bridging token fragmentation bridge failure we'll talk a little bit about modular interoperability systemic risks the whole nine yards and what better way to just explore all this stuff through a true meeting of the minds with arjun from connext james from nomad and then mustafa and john arjun on the 24th of january you and nomad announced a strategic partnership introducing the modular interoperability stack why don't we start off with you um can you introduce the concept of modular interoperability and how it fits into the modular blockchain thesis yeah absolutely um so before i do that let me just give like a really really high level background on like what connex is right now and then what nomad is right now and then how those two things really like fit together um so connext at the moment is a liquidity network um crosstalk in the query network basically what this means is that we have a network of lps called routers that hold pools of funds on different chains um and using a technique that is very similar to atomic swaps uh allow users to swap you know kind of like one for on assets across chains um but from a from a user experience perspective this doesn't look like a swap it just looks like a transfer of funds um but the the goal is that the user ends up with the right asset on the right chain uh on the right destination chain for a fairly minimal amount of uh of fees and and almost no trust um and this is this is a model that has been like explored for a very very long time um we have optimized the  out of it but um it is it is something that like we've understood the trust considerations around for for quite a while um uh nomad on the other hand is a is a is a completely novel mechanism for uh for interoperability and it's it's specifically targeted around more generalized message passing so that uh you know what we do with k'next which is atomic was sort of more of like an atomic swap style thing um what we call like a locally verified mechanism for bridging that that mechanism is trust minimized and it's easy to deploy places but it's not we can't really use it for like arbitrary data passing only some kinds of data passing whereas nomad on the other hand is the other end of the spectrum where it is specifically designed for to allow for passing any kind of message between chains so allowing for one contract to read the state of another contract for instance which is which is a really really important and difficult problem now the way that historically the way that this has been done is by using something called uh well one of two ways either through like light client header verification like ibc um where you have one chain actually running a light client of another chain and of course this is difficult to do because it's custom to each chain and each time you have a new consensus model or anything like that you have to to come up with an entirely new strategy for how to do it um or through what is pretty much the most dominant paradigm right now in interop which is uh using an external set of verifies so using like a pos network or an mpc network or uh you know uh in in like layer xero's case uh like a q uh pos networks um but the the idea is like um you have some external set of parties that are is responsible for relaying the data across trains and verifying it now what nomad does kind of differently is that it it says uh instead of instead of uh just trusting some set of third parties to relay this data across chains it uses an optimistic pattern similar to an optimistic roll-up where you relay the data across chains optimistically and then you have a period of 30 minutes within which people can dispute and and if they dispute and find that fraud has actually occurred uh when relaying that data then the the entity that signed off on relaying the data the updater and nomad system um ends up getting slashed um so the the security model is like more similar to to that of something like a roll up then to something like a side chain which is true for most of the other kind of externally verified mechanisms now the the trade-off for nomad there are always trade-offs the trade-off for nomad is latency right you have this 30-minute window within which uh you can't really do anything with that data because you can't really trust that it's accurate and and uh you know on the other hand you have connects where um you know these transactions are near instant but they have limited functionality they have limited uh they have you have a limited ability to pass around data between chains so our thesis around module interoperability is that uh similar to the modular blockchain stack where you can take different parts of the system uh different like kind of pieces of of what make blockchains important and what make them work um and separate them out into their own independent networks um so separate out data separate out execution and by having all of these things be built in a modular way you can kind of offset the trade-offs of you know trying to build this like single monolithic blockchain that has this really complicated tradeoff space around around like the the scalability trilemma that vitalik has talked about and similar to that um we think we can we we can do the exact same thing with with interoperability where we use you know nomad as the the core message passing layer to pass uh data between chains and then you use connect as the liquidity layer the the that gives you the ability to get the right kinds of assets on the on the receiving chain um you're able to you know while nomad messages would have historically taken 30 minutes you're able to short-circuit them in any user-facing case and make sure that they happen almost instantly through connects and but you still end up getting the like core security guarantees of nomad and and really by extension of the underlying change thanks i want to kick it over to the celestia guys uh guys react to me like tell me about how you feel about this modular interoperability thing that arjun is talking about any trends and forces that play that we'll have to take into consideration or things that we cannot ignore but just give me your general points of view um yes i think i think one thing that we might be helpful listeners here and is to kind of explain this difference between what connects a number they're trying to achieve in the sense that um there's this idea of a lock and mint protocol and a swap um and connect me correct me if i'm wrong origin but so like with connects you have instant transfers right because it says you can swap tokens so with connects you can you can basically swap tokens um like that already exists on two chains it's like if you have two chains like chain a and b and some tokens exist on there you can basically swap those tokens instantly um yes but but what but then the question is like what if tokens don't what if the token what if you want to mint new tokens into that chain so let's say you have a bunch of tokens on chain a and you want to move them to chain b and but like uh there's no that there isn't any existing tokens of your type on chain b so you have to kind of like transfer you can't just swap something on chain b to get to move those tokens to chain b you have to actually move them to chain b like mint new of those circuits on chain b and um that's referred to as lock and mid particle because you have to lock you have to lock those circles on chain a and then mint them on chain b and you're basically moving them chain b rather than swapping them for existing tokens and as far as i can understand that's where nomad fits in and that's where um like you have this um 30 minute with like latency because you would only use nomad if there's not enough liquidity on both chains so that you can just swap those circuits instantly exactly yeah and i'm actually really oh sorry go ahead go ahead no no definitely finish your thought yeah so i was saying um that actually fits in really well with the kind of like a roll-up centric or modular uh blockchain stack because the way that i see because with roll-ups the way i see that um like roll-up bridging work and working in the future and like at the moment there's quite a lot of like people com like critical of optimistic roll-ups for example uh because they have this like one-week withdrawal period um for example like if you withdraw the tokens to ethereum um like even when we're talking to ethereum from a roll-up you have to wait a one week for proof period but with this like but you don't actually have to do that if you just swap the tokens um like a naive way of doing that would just be like uh deposit the tokens to an exchange uh swap them for the tokens and then withdraw them to the chain that you want using the exchanges with the durable feature um and connex is kind of doing that but in a decentralized and more trust minimized way by having like a kind of this um like a cryptographic swapping protocol which basically does the same thing as if like let's see what let's say you want to move the 3m to polygon and instead of using official polygon bridge you can deposit ethereum into like coinbase or like maybe not coinbase i don't know if coinbase supports polygon but let's suppose it did you just deposit the e3 coinbase and then you would draw it from coinbase to the polygon chain um so that's basically that's basically what connex does uh okay but a decentralized way and so i see and like with roll-ups and with like uh multi-chain bridging in the future the way that i see it working is that you have like a like one official or enshrined like lock and mint bridge where you move the tokens across those assets that is completely trust minimized and that has a highly high latency period might be expensive to use but um you only have to but but only like liquidity providers would use that bridge like the actual users themselves don't actually use don't actually swap tokens to the official bridge but just use atomic swap and between chain and chain b rather than going through the official bridge which is only should be used by like liquidity providers and whales effectively exactly yeah and i'm actually i'm really glad that you brought up this example of of tokens and like token transfers and token minting because that is that is actually the the simplest most quintessential example of the sort of uh the the kind of domain space that connects to nomad occupy where like you have uh with connects you have you know uh like the the fundamental assumption again in the in the token transfer case specifically the fundamental assumption is that like there is some liquidity exists on the receiving chain you have to have some minted representation so if you know if you're transferring if you wanna you know if you're on uh polygon and you want to get to you know xyz chain and xyz chain doesn't have a usdc representation uh there's really no way to use connects to swap usdc into that chain we we need somebody to mint those assets first and that is really where nomad comes in is that like nomad has the capacity to with minimal trust uh mint um representative assets on a chain where they don't exist yet um and uh and so for from from the you know from our perspective for instance with moonbeam like this was a core part of how we've like collaborated is that like exactly as you said you know liquidity providers market makers other institutional people who are trying to mint you know 10 20 million dollars of liquidity at a time will do that through nomad um they'll migrate that liquidity from ethereum or from some other chain to moonbeam and then uh they will now uh you know now that liquidity is available some of it can end up in connects and that can be that can be swapped over for users and so the users still get this like less than two minute turnaround time very cheap swap that can go from any other chain um but you still have this this like safe mechanism to mint tokens now it definitely gets a bit more complicated than that um i think we'll we'll certainly touch on like inter and truck lester communication in the future and like at some point on this in the space um and i think there's like a lot of really interesting things that we can do there around you know like nomad may be quite possibly the best inter-cluster communication option that there is um and but at the same time there are certainly instances where you may want to use like something more native um for the intra cluster piece so like you know use a rollup ambi um in order to achieve the best possible security guarantees and then the other the other dimension along which it gets more complicated is is if you're doing something that's more complex than simply transferring tokens um and and that is entirely possible with nomad and in certain cases it's also possible with kinect um and so you know those those pieces we can kind of dig into in the future as well james welcome can do we have your audio i made it you made it okay cool um real quickly introduce yourself james and then give us one of two lines on nomad sure i'm james prestwich one of the co-founders of nomad i've been working on you know cross-chain communication and bridges for uh for probably four and a half years now i started off the cross-chain atomic transactions with bitcoin back in 2017. um so uh nomad is a new cross chain protocol that leverages you know kind of some of the techniques from optimistic systems uh in cross-domain communication so you get cheaper you get better throughput at the expense of some latency awesome we had opened up with the concept of modular interoperability i mentioned the announcement on the gen on january 24th where connext and nomad formed this d partnership and i asked how does the concept of modular interoperability fit into the broader modular blockchain thesis and so that's where we're dialoguing on now john we'll kick it over to you your point of view i think it's a particularly important subject because uh as we've seen from the rise of various uh new blockchains in the past couple years things like solana for example there is a need and a demand for systems beyond just a single chain with a single virtual machine with a single set of constraints and limitations there's a demand for exploring the trade-off space not just in terms of security and decentralization like maximus would have you believe but the trade-off space in terms of things like complexity in terms of novelty uh and so on there's a large shade of space that doesn't necessarily involve harming the user's security or harming this notion of decentralization or sovereignty while still being different than a single monolithic chain uh and we are moving towards a world that is increasingly multi-chain that involve multiple blockchains ideally we want these blockchains to share security for example if they use the common uh consensus and data availability layer such as celestia or some other mechanism and these chains need some mechanism of communicating with each other and this is where cross-chain communication protocols such as what connects the no matter building are going to be very important in the future james one of the one of the things i i love about this is that what what people don't realize is that multi-chain systems uh have always been and will always be the only scaling roadmap uh ethereum called them shards we have cosmos zones we have polka dot parachains but it's all you know different flavors of multi-vein system uh when we talk about cross-chain and cross-domain communication this isn't something new this has been an inevitable consequence of the ethereum roadmap from the beginning yeah i mean there's definitely um like in this long ecosystem like until these views basically like we should have a single synchronous like world computer blockchain that processes transactions synchronously um but i think like anyone who's kind of built you know a large scale system before we'll no that's very difficult to do like if you look at the internet itself and like it that's not a secret like the web itself is not a synchronous system it's a bunch of uh web servers communicating with each others asynchronously and i think we will end up in the with a similar kind of um architecture for web 3 as well a set of blockchains communicating each other asynchronously and does anyone disagree i was gonna ask does anyone disagree with what mustafa said or does anyone see it differently i we agree to the extent that um and i think james and i have talked about this that like effectively what connects plus nomad is really offering is like is asynchrony uh not really as a service but really just asynchrony to this to this world right um like async coms to this world that it just fundamentally doesn't exist yet it's going to be completely in your paradigm uh people are going to have to completely change the way that they think about things in the same way that we have to completely change development paradigms to be able to build web applications um uh we we have we'll have to do the same thing in blockchain development which is certainly going to be a giant pain in the ass james i think you were saying something yeah well i was mostly agreeing in adding colors the the web is you know not a synchronous system and it's certainly not a homogeneous system the internet is composed of you know thousands and thousands of independent networks with slightly different uh software and slightly different semantics and network address translators and all of these messy like systems just to resolve the fact that the web is not synchronous it's not homogeneous you need uh cross-domain communication in the internet and that happens you know 10 times every time you make a web request without you thinking about it question for the group celestia and rollups are examples of a modular blockchain protocols while nomadic next are examples of modular cross-chain communication protocols why do you think that modularity as a design principle has emerged in both of these contexts and do you see this emerging in other areas of the blockchain stack can anybody um well i think um i think there's several elements there i think arjun also kind of uh elaborated on that well by by talking about the trade-offs and like uh from a from a questioning perspective uh like every like question particular every question mechanism has trade-offs uh so i mean obviously like in the connex case uh like the atomic swap protocol doesn't work if there's no uh like liquidity and and that's why you need nomad if there's liquidity but the trade-off there is like no matter has higher latency um so in that kind of world it's a way to kind of like beat the trade-offs by having like kind of the best of birth worlds in any way in the sense that uh the product like because the protocol is modular in the sense that it supports um multiple uh like cross-chain mechanisms depending on the use case or what's needed at the time and you can kind of like the the user can effectively has more has more like flexibility over over the use cases that it needs and i think from a celestial perspective that's also similar in the sense that celestia decouples consensus from execution and it allows developers to define their own execution environments and different execution environments have different tradeoffs and so the beauty about it is that because celestia doesn't define it it doesn't enshrine or enforce a specific execution environment for you the developer has the freedom to choose what whatever execution environment they want based on their use case and what trade-offs they need like whether they want to use the evm for example or or use the cosmos sdk or some other or fuel or some other execution environment i think to that point modularity is kind of a inevitable response to these unavoidable trade-off we see in consensus systems and cross-chain communication systems uh you know the response is rather than choosing one point in the trade-off space and planting a flag there and telling everyone they should come join it we try to design these systems so that users can move throughout the trade-off space based on their use case i think also just to echo uh james's point from earlier this is something it's like modularity is a term that we have started to use in the last year and a half around describing these different systems but like fundamentally what this is is just like layered architecture for for distributed systems right um that's something that we've we've sort of understood for a very long time that like when building networks um the this is true on the internet as well by the way when building networks it makes sense to have each protocol or each like level of the stack be hyper-specialized into doing one specific kind of activity rather than trying to build a network that tries to do everything and the reason for this is that like it's extremely difficult to constrain risk if you have a lot of like for instance if you know tcpip was also handling uh you know network address translation um if it was also handling some of the higher level functionalities of things like http requests um you would end up with a just a much larger surface area that you have to think about in order to do things with the protocol and in fact you would also end up with a lot of this like extra stuff that you're not doing anything with most of the time um and i think that's sort of what's what exists with you know the whole modular blockchain thesis and the whole module and our operability thesis is that like fundamentally what we're doing is we're we're separating everything out of layers we're saying the settlement layer is responsible for for economic security we're saying the data availability layer is responsible for storing data we're saying um the execution layer is responsible for you know executing uh like a in a virtual environment and then like translating that into proofs that can be stored in a data availability there and by separating out the responsibilities of each of these different pieces of the puzzle we are not only constraining the risk that anything goes wrong but we are also limiting the the need for uh each of these pieces to always be used and this is really where the scalability piece comes in i know this is john something that john has talked about a bunch which is that you don't actually need to have a general purpose vm uh the full stack general purpose vm and chain running every single type of thing you cannot you should and in the future we will hyper optimize vms to be specific to certain kinds of activities and as long as the the data layer itself and the settlement layer itself are are the same you can do that freely with minimal risk and still have the same level of security and that is how you eke out the best performance right is is allowing allowing people at higher levels to optimize without affecting any of the core properties of the system and without having to rebuild an entire system john i want to give you an opportunity to weigh in could you repeat the question to refresh uh the listeners memories yeah celestia and rollups are examples of modular blockchain protocols while nomad and connext are examples of examples of modular cross-chain communication protocols why do you think that modularity as a design principle has emerged in both of these contexts do you see this emerging in other areas of the blockchain stack yeah so i would say that uh before the separation of the actual blockchain process uh kind of became popular uh into what we currently see today as being the modular blockchain stack we only have your consensus layer data availability layer your execution layer and your settlement layer potentially wrapping some or all of them together into different different layers uh there were a number of projects that did modularity at the implementation level so an example of this was the cosmos sdk uh and the cosmos uh just the entire tendermint cosmos development ecosystem where essentially they you know black boxed the consensus process uh and they made uh specifying the execution semantics modular right they allowed you to just define your own execution semantics bring your own vm essentially uh and consensus would be provided there's also other projects that are also doing something similar uh avalanche and subnets for instance is smg copying the cosmos model where they have you know consensus consensus stack their avalanche consensus stack and then they're providing some for now relatively primitive but i guess they're in development uh mechanisms for actually developing your own virtual machine and they have a few currently application specific vms that are that are being played around with uh i think they're launching some new ethereum uh evm based vms uh with some tweaks and so on uh but that's an example where uh the implantation uh rather than the like fundamental design was built around a kind of modular modular design principles and with the rise of the modular blockchain stack we've kind of extended this notion not just to the implantation but fundamentally to the design and this kind of takes the benefits of modula the modular implantation and makes them even better and it takes them even more to an extreme and what what are those benefits why would why would the cosmos sdk be designed like this uh it's because uh when you make something modular it means that whoever uses that system or whoever builds the system doesn't have to know and care about everything they can only care about a subset of the features and therefore it allows specialization and as we know specialization is one way to really eke out maximum performance or maximum anything really it's just an amazing way to min max because if someone has to keep in their head how an entire blockchain stack works that's not just fundamentally not scalable right the i don't think there's anyone in the world today that knows how the entirety of a go ethereum node works uh just it's just not humanly possible it's just too complex but it is possible to know for instance how the evm works and not care about the peer-to-peer networking not care about the consensus protocol any of those edge cases or anything like that many people know entirely how the evm works so this uh allowing specialization allows uh allows us to get maximum performance and this is why i think we've seen the rise of modularity not just at the design layer but also even at the implantation layer i want to move towards clusters what are the biggest security differences between inner cluster bridging versus intra cluster bridging and how do we see that changing in the next 12 months 24 months mustafa why don't we go with you yeah so this idea of clusters was set out in a blog post i released in october where i was trying to kind of make sense and categorize the communications in the multi-chain ecosystem landscape and the basic idea is like there isn't really such thing as layer one or layer two or the three all there it really is is blockchains that bridge to each other like conceptually what is like the layer 1 and layer 2 aren't really useful terms and nowadays it's more useful to think about what are the trust assumptions between different blockchains because layer 2 is basically a blockchain layer one is basically the blockchain but what different what why it's called layer two and what differentiates it is how they bridge with each other and the trust assumptions um used by that bridge so i came up with this idea of like um inter-cluster and intra-cluster communication so i i i defined communication between two chains as and the two kind of entities categories the first category is this idea of a committee-based um security and that's kind of like the more traditional bridging mechanisms you would use for example like the bridge between ethereum and solana that uses wormhole for example that requires a committee and requires you to trust a committee to not steal your funds because effectively it's a committee the wormhole bridge community committee that signs messages to uh that from with that bridge to and from um ethereum and and that like that committee could lie about to ethereum for example about how many funds there are lana or what your balances are in solana and vice versa um it's like it could theoretically steal all the eth in that bridge same with polygon right the ethereum to polygon bridge and that's also a committee based bridge if i remember correctly there's like a multi-city of like a few people that if they're compromised all the ethan polygon could potentially be compromised and that's why that's different that's why polygon um is kind of not is not usually referred to as layer 2 to ethereum it's kind of referred to like polygon it just it's just another layer 1. even the polygon for example the value proposition of polygon is that it helps scale ethereum all polygon really is it's just another blockchain it's a fork of ethereum with tendermint on top of it consensus with a kind of like committee-based bridge that's committee-based security and um and then you have the idea of trust minimized bridging and that's where roll-ups come into play and so with roll-up chains if you deploy a roll-up chain on ethereum for example um and you like have a bridge between that roll-up chain and ethereum then that's what i would describe as the trust minimize bridge because even if the operator and the nodes that operate that roll up misbehave and go completely rogue then in theory they can't steal your funds and the reason for that is because the ethereum chain runs a like like client as a smart contract for the roll-up and if your roll-up chain has a malicious block producer that produces invalid blocks then that can be proven using either a four proof um or a zk proof to prove that the block was valid and so this is what i call i kind of this is what i call um chains that communicate with each other using trust minimize assumption i call that as an intra cluster communication mechanism and i call chains that communicate with each other using a committee-based assumption as an inter-cluster communication and this is kind of named after like in the internet and intranet so like with the internet with the intro the intranet is basically communication across the your local network so like if you have an office um and like all the computers in the office are connected to the same like local network to the same router and they can communicate with each other using the intranet but to communicate to other computers that are not in the same office for example or that are not on the same local network um you would use the internet and so i kind of applied that concept to um like the multi-chain ecosystem and you have like intra cluster and interclassic communication good overview would anyone else like to add on this topic of clusters inter verse intro and the changes that you expect to see yeah yeah definitely um back in like 2017 2018 when people were first investigating and talking about these kinds of things we came up with a terminal hierarchy which was essentially you have what mustafa would call committee based which is usually a multi-sig um and you have uh the intracluster the shared security which we referred to as usually dependent or merged consensus where the secondary chain is dependent on the primary chain uh so an example this is roll-ups uh where the roll-ups consensus security is dependent on the l1 and the l1 is running a fully verifying rollup client under some assumptions and in the middle is where we would put things like ibc which is based on a relayed light verification you know ibc relays don't do full state verification of the remote chain they only do a header chain verification so i think in mustafa's terminology the relay and the you know dependent consensus would both fall as intracluster while the multi-sig or committee based would be inter-cluster so i would still define um so so with committee based uh i was told to find like like clients that only verify their headers like like clients that verify the signatures in the headers to do to check that the block headers have consensus and i i think that's still theoretically covertly based because the only difference is that the committee is simply the chain itself um but the reason why i make that distinction is because like if you create a new cosmos zone with one valve layer it's like it's potentially even less secure than like a third-party committee that's true so good um the so that by the way the impetus for this is um just to kind of like uh give a little bit more context for everybody who's listening to this um the impetus so this is this is the same thing that was kind of talked about in battalions like crosston multichain post um where he was saying like the fundamentally there are limits to the security that you can achieve when you have like two chains that have independent validator sets that are uh inter like communicating with each other and even if you built the most theoretically secure communication mechanism between each other like a you know like like ibc for instance um the the bridge security is might be foolproof but the fact that you have two independent chains means that you now are still adding another risk there and that risk layer is that the the one of the chains could get 51 attacked um you could force uh you know a fake update on that chain um that then gets translated to fake funds being sent over over the bridge onto the other chain and so this is a way to kind of like spoof malicious data in uh and send it over a bridge and into like a higher security environment like ethereum um and fundamentally that is true there is there is this fundamental limitation that exists when you have uh multiple different chains that have their own uh independent validators this exists within cosmos and and ibc uh when you create new cosmos zones uh this exists um you know this this that wouldn't exist in the case of something like ethereum and roll-ups or uh um i guess in any any sort of situation where you have shared security because in those cases uh you would have to the the the risk that you're taking on um is is really the risk of the single validator set uh being attacked um um now i do i think that there is a fundamental limitation there but i don't i don't necessarily agree with the like vitalik kind of conclusion which is okay we should just not have multiple validator sets we should have just one like we should just not have inter-cluster communication because the reality is people are just going to do it like this they're going to find ways to do it and even if those ways are completely trusted because are idiots and also because like the market wants it and like you know i think i think james you kind of i think you had a tweet about this where you were just like it's man in the mountain energy clearly people are gonna do it usdc exists wbtc exists like we're not going to avoid these things let's find ways to actually lean into it fundamentally you have to look at the market and say people are using usdc they're using wbtc these are functionally uh committee-based cross-domain systems and there's such incredible market demand for these committee-based cross domain systems that you have to admit that you know people will always use them and there will always be huge amounts of value locked up in them you can't have this mountain man single validator set for everything uh the market has rejected that i have a spicy analogy for this which is uh which is that um uh you know sex is risky uh it can give you sdt stds it can uh it can lead to pregnancy but the solution is not just to tell people don't have sex and be absent abstinent because they're never gonna listen to you they're still gonna have sex um the solution is to make contraceptives extremely easy uh to get so we just need bridge condoms um yeah yeah but you know i think the idea why i think like like the logic of you know metallic saying we should not have a multi-chain ecosystem just a question ecosystem i think the kind of ethereum roadmap expectation is that like ethereum will scale kind of like infinitely to some extent using this roll-up centric roadmap um like it will have you know very like high data availability capacity and like so you can support lots of roll-ups under there but i think that's still basically an important part like even even if threateningly speaking you had even if you started speaking you had a chain like ethereum that could support an infinite number of roll-ups an infinite number of data so you don't need to spin up a new chain i still think that's kind of missing an important part of um why we need the multichain ecosystem um and that's because that's because that's the solved reason for that like the most important underrated reason is like these different like clusters of chains have different values and not just both from a technical perspective but from also a social perspective like maybe you don't want to have it maybe you don't want to be like limited to the evm um as a supplement layer um like maybe you want to have a paralyzable virtual machine like solana for example but also you might have different social values like look at ethereum and ethereum classic um you know different different different values there so i think either way like we will have a multi-chain ecosystem one actually this i so stuff i definitely agree with you um by the way i want to couch this and say i agree with you but i want to also play double advocate because this is this is definitely an important question um and i think it gets asked a lot and the question is if the the the thesis of having a you know the economic security uh associated with a settlement layer being as high as possible to resist you know sovereign corruption to resist corporate corruption if the thesis is you want to make that as highest like make that as secure as as you as you can why not just have a single sediment layer why not have you know you could potentially have diversity at the execution layers and the the data availability there but like from a objective world outcome uh point of view isn't it much better for security to say uh let's just stack all of the security in one network of validators so that's directly possible if as long as you are happy with the idea that the resource requirements to produce a block or to participate in the production of blocks in that settlement layer could potentially be very high and so like as a thought experiment like let's see let's say you want to take salamat to your extreme because london's value for example is like to to produce blocks and solana we're going to have you just need very high resource requirements and we're just going to scale that we're just going to make it like a web scale blockchain where you have like unlimited resource requirements to produce blocks like how far would you take that like let's say in theory you could take that to something that eventually looks like a mysql database like web 2 by sql database and you have like something like google scale and and you somehow figure out how to like localize that database into like a single multiple route so it's basically a blockchain the question is the question there is would the community be happy with the fact that the requirements to produce blocks would be so high that you basically need to be like a like a corporation to produce those blocks and like the answer might be yes to some extent because if you've looked at vitalik's recent like end game post his his entire thesis there is that you will have centralized block production but decentralized verification using things like data availability sampling and fraud proofs and sdk proofs but then the question is like still how far do you want to take that to an extreme like these do you really want to take that to the extreme where only like google level companies can produce blocks and if the answer is no then you do need multiple settlement layers and multichain ecosystem uh you know i think at this point we've we've basically all agreed that there will be a multi-chain ecosystem with multiple settlement layers uh and i want to bring it back to talking about you know modularity in bridges because that's fundamentally a very difficult thing to reconcile in cross-chain communication how do you deal with so many different chains so many different execution environments and so many different choices for cross-domain communication if we want to use this intracluster communication as much as possible but we're forced to occasionally use inter-cluster communication you know how do we resolve that for any application um the way we've been approaching this at nomad is to completely abstract the communication mechanism from the application and to have any application be able to enroll any number of communication mechanisms so the same logical like token bridge application can use ibc or nomad channels or whatever other inter-domain communication mechanism is available we're still in really early stages of rolling this out and it's very complex in practice i was wondering if anyone else had uh strong opinions on what standards in that should look like i mean like everyone's i think like there's i know there's ibc for example and then there's also like polka dot standard um but like would you consider like ibc as a standard like for example as they like at least packet prod like packet format protocol well uh for example i like you know packet format protocols um you know i like as a developer just getting some message in off the wire and deserializing it locally and you know um i guess my question is like uh we have to separate the packet protocol layer from the application layer um you know how does your token bridge handle these packets and how many different channels can it get so we need standards for you know enrolling channel you know connecting the channel to the application and passing packets from the channel to the application and handling the packet uh and i'm still not sure what those look like in the long run like this uh we're talking about modularity a lot what we're uh this is you know what do you do at the layer boundary and what is the correct format for that interface so is the problem there if i understand it correctly like one where like if you have different execution environments on your chains you have to have different like libraries to like process those packets effectively and like to for example like deposit individual assets in those written like compatible with those execution environments like with the evm for example like for example with the evm you can have a solidity contract uh or with like it's lana you have a slanted contract am i on something that correctly took a question uh so suppose that you have a uh consider like a evm ethereum of mos and osmosis for example uh this is something that we think about today uh you can connect ethereum to esmos with solidity on both ends you can connect esmos to osmosis with ibc hypothetically you can connect osmosis directly to ethereum with some other channel how do you reconcile the unified application you know that you want moving tokens between those three chains and the different uh communication layers underneath them each of those three connections might have different semantics or different layout and we need to connect them to essentially the same application on top which might be expressed as a go you know a cosmos module or a solidity contract uh so there's kind of this you know just interfacing between the application layer and the communication layer is very difficult and kind of not yet standardized yeah i guess i guess the main difficulty there is that like all these chains have different like execution layers and so like like for example osmosis does not have does not support like generalizable smart contracts um so like hypothetically speaking like if osmosis like if ethereum or if ethereum evmos on osmosis all supported like general evm contracts then you can probably have the same kind of communication between all those chains uh if you just like deploy the same smart contracts incidentally and so i think it sounds to me like the bottleneck there is like the fact that you know in osmosis you've got a hard-coded like you have you have to use ibc for example if you want to use official bridge um but there's no generalizable like smart like environment where you can like add like deploy code into it yeah cosmosome soon i hope uh but we we have to like uh for each communication channel you have to build it in the local you know uh the local language in the kosumosam or in the evm and then for each application you have to connect it to those channels in a predictable understandable way and you have to build the application so that it is you know abstract over many different potential channel types um this is a huge like challenge uh and we are you know kind of slow rolling our approach to it you know today we have this very abstract handler pattern where you know some channel calls into the handler with a bunch of raw bytes that have already been authenticated but we don't know whether that'll continue to work for ibc or for polka dots xcmp or for other like communication uh patterns um in the future uh so we're still in like very early stages of building out the modular bridge stack and getting the interfaces between all the different modules correct and usable yeah there's also a lot of like other pieces other open questions that exist around this too so like you know we have we have the you know liquidity and messaging layers that that are live and in production and kind of now being like figured out uh how those how those will interface with each other but there's also like a lot of open pieces around like okay i think you need some sort of gateway protocol that allows you to connect between nomad to other localized communication mechanisms like ibc i think you probably need some sort of not protocol that allows you to translate addresses between these different systems nomad has a concept of chain agnostic domains but we will need to translate those into like localized addresses everywhere um in order to actually like execute anything to to any destination contract um so these are these are the kinds of like interesting challenges that remain where there's a similar to the internet itself there's just like a there's a whole host of uh of protocols like bgp and things like that that will that will need it really will kind of need corollaries in this space we will need to have that kind of functionality exist and we'll need to make sure that that functionality is simple it's very well specified and it's it's available for everybody these are open protocols that people can then start building again so that we end up so that we we do end up in a in a world where like there is a single path for users to get from you know uh contract on chain a to contract on chain b even if those two contracts uh even if those chains are in totally different clusters uh and if you're having to go through like a few different hops of messaging protocols in between to get there i'd like to transition a bit here you guys are already somewhat discussing this the question is part of the value proposition for modular blockchains is that we can have many different execution environments running side by side however having a multitude of execution environments poses difficulties in bridging between those chains how do we resolve this conflict is this where modular bridging protocols come into play you know we're we're right here right now uh and i touched on this a little bit uh for any like good cross chain communication system you have to implement it in the language both ends of the communication channel so if you want to connect evm to solana you have to write it in solidity and in solana's rust and you have to write you know the same application on both ends so that it can commune so that they can communicate together right now you know we have planned ahead for that but we haven't yet done a second implementation of the nomad protocol uh what we've planned ahead for is you know allowing extra space for addresses because you know not all chains use 20 byte addresses like ethereum most of them use 32 putting as many things as possible in very agnostic very general patterns so for example we don't use abi encoding because that's a solidity pattern not a solana or a near or a cosmos pattern and making the interfaces you know as simple as possible we kind of expect that we're as we spread out to more chains we're going to run into more and more things that we neglected to consider in the initial design uh so really excited about all of the fun uh bug fixing and resolution we have to do for the protocol in the future so i think um like one thing that might or might not help with this in the future um so uh so um optimism recently released this uh like inter interactive floor proofing system called canon and what they've done there is they've like implemented um like a mips machine on in the evm and then you can they've compiled like go code into mips and so like this mips machine kind of acts as like a virtual machine on the on the ethereum virtual machine and you can you can compile arbitrary like go code and other code potentially causing muslim code or like wasm code and into mips so that so in theory you could potentially run any kind of like execution environment on top of the evm as a virtual machine um like in the same way like you could run like a like a docker container with a different operating system um on your like own computer to run other applications that might not be supported on your machine instead of having to necessarily like write rewrite the application in every single like language or like execution environment that needs to be supported and so that could potentially help in the future with like preventing the need to write multiple code for for bridging in multiple different execution environments yeah um optimism's uh mips machine you know kind of based on arbitrom's earlier wasm machine doing the same thing these uh really open up a lot of possibilities for more direct optimistic full verification of remote chains in the evm um i'm excited to see these productionized over the next probably like two to five years uh it's gonna take a while but they're gonna do really cool things in the long run i want to begin taking q a from the audience pretty soon here so if you have a question raise your hand before we take that first question uh arjun i wanted to ask you something specific can you explain for the audience what is optimistic bridging and what makes it different for most other bridges that operate today and why is connext so bullish on this um yeah um so uh i mean i'm sure james is probably gonna give a better explanation than i can but um i'll try to give like uh the best explanation that i can and then james let me know if there's anything that i missed or that that is important that i've missed um so uh last year we wrote about something called the interoperability trilemma which is that there's there's at least with with where the market was at the time every single interoperability project that existed kind of fell into one of three categories and that was either it was basically came down to who is verifying the system who is actually uh responsible for deciding what data gets relayed between chains and how that data gets related um and uh and there are three three real options there the first was native verification which is um you know uh the chain's own validator center the receiving chain zone validator set is responsible for for validating the data that actually gets posted to it um and this is this is you know things like well there's two options here there's there's things like xcmp where you have uh you know the the core validator set of polka dot is responsible for um for validating the data that goes across chains um or you have things like cosmos uh and ibc where uh the receiving chain validator set is is like running a like client of the sending chain and uh and then uh validating like the block header of something that comes gets passed then you have externally verified systems uh which are which as i talked about before are like multi-sig npc uh systems layer zero is another example of this um where really you just have like a third-party set of validators doing this um that has its their their completely own set of security tradeoffs um that is different from either the two chains this one adds a lot of trust assumptions in some some cases it could be okay so like one case where it could potentially be okay is if like the security of that third-party validator set is much better than the security of either of the chains in that case it's totally fine um but of course it can be hard to really figure that out and and in for at least any of the larger clusters like ethereum um you're realistically not going to end up in a situation where your interoperability protocol has better economic security than ethereum does it's just very very highly unlikely at this stage um and then the last one is is kind of what connects does currently uh which is local local verification so you instead of having this like big end party problem where uh you have some set of people that are responsible for passing arbitrary data between chains you do something like an atomic swap you have you isolate to two people um the two parties and then you have them just communicate with each other and uh taking a two-party communication mechanism and making that trustless is something that we do actually understand quite well and it is quite easy to make that as at least very very reasonably trust minimized more than most other options out there um now the the assumption at the time when we wrote that post was that there was this trade-off space that exists between these things and you you could really only have like two out of the three core properties of like trust minimization uh uh generalizability which means uh being able to pass around arbitrary data um and what we called extensibility which means the ability to take this this core product and like copy pasta across many different chains um which unfortunately would like you know like client header relays things like that it's not it's not very easy to do because you have to build custom implementations um the reason that we're really really bullish about optimistic verification is it's the first sort of it's the first like approach to interoperability that is genuinely novel uh beyond the the three prongs of the trilemma that we have explored in the past and and we think it's interesting because fundamentally it changes the trade-off space so that rather than having to choose between um you know being trust minimized being uh generalizable or being extensible you are able to actually get all three of those things fairly easily and instead you choose this other trade-off which is latency now latency is bad from a user perspective but from like an institutional perspective or from a security perspective or even from like just an implementation design perspective it's really not a huge problem and latency is something that we can fix latency uh you know the the the the march of technology is very very good at making things faster we're very very good at optimizing on speed uh and on cost we're very bad at optimizing on some of these other core properties like security um just through engineering work um and that's the the reason that we're extremely bullish on on uh the optimistic verification model is that in combination with the way that connects works currently we can get pretty darn close to what looks like a perfect model of course there are still instances where you'll have the 30 minute nomad latency but aside from that aside from the rare instances that happens and most of those are like dow facing use cases institutional facing use cases aside from those cases uh you have a system that is actually gets the best of all worlds um i hope i explained that well i can definitely answer more questions about it in the question section and also james if i missed anything let me know i want to take q a um i see some cool names here uh justin drake joined uh angela lou d5 frog shiram kanan would love to field some questions but we'll go with seg as the first question let's see if he can join here yeah it just takes a couple of seconds to connect so for future uh attendees as well um i just had a general question for whoever wants to answer it about um a more modular blockchain world one was i heard arjun speak about the liquidity layer um fragmented liquidity is probably going to be a pretty big issue uh in the in the future and one thing one solution that i can see potentially is to have an application specific chain or roll up for example on celestia that specifically deals with liquidity that talks to all the other rollups that are trying to do general purpose stuff do you see this as the solution to liquidity fragmentation and also do you see a a world where you have more application specific blockchains than general purpose blockchains because i guess you can you can design a a virtual machine that's more efficient for some applications than others and you see do you see them talking to each other more than you just have a single dumb general purpose computer trying to process everything kind of like cpu versus gpu type anyone who wants to answer can answer thank you uh i you know i don't see uh fragmentation going away anytime soon um there really aren't many good solutions to this adding new application specific uh liquid amm chains or stable swaps between them uh really just embeds the fragmentation deeper in the ecosystem and adding new domains to try to resolve it just provides more routes for uh fragmentation to creep in you know fragmentation is a function of having many domains and many bridges between them and i don't think that we can prevent that from happening or uh you know unspill the milk on this one uh i was gonna say there is there is an interesting incentive though which is that um so so basically uh like just a quick take a quick step back so fragmentation exists because um you know you have uh you know potentially five or six different bridges that are all connecting the same chain producing their own like uh flavored asset and that nobody has really quite decided which asset is canonical and nobody's been able to like take the time to figure out which asset is canonical the chains themselves are of course trying to be agnostic um if if they can or in some cases they don't and they'd say like no this is this is the right bridge but even then these are open systems so you do end up having people coming in deploying stuff anyway and giving you the wrong assets and that's a giant pain for for users um i agree that like it's a problem and it's going to be a problem i think there is something that's kind of interesting though which is that like having a stable swap um the one the one piece of incentivization here which is interesting is that like having a stable swap necessarily means or even having some sort of like swap chain necessarily means that you are going to have uh one bridge which is the the one that is canonical ends up becoming over time becoming the embedded version because if you have a stable swap every single version of an asset except for the one that you're swapping into is going to have slippage um as as a fundamental part of transferring so the the assumption is that if you're again you know obviously markets are messy and it's not clear if and when this would happen but the assumption is that if we are heading towards a world where you know everybody tries starts trying to optimize on price everybody starts trying to optimize on slippage and user experience um there is this really interesting thing which is that you can go to like asset issuers and say hey um why don't you just like explicitly tell everybody this is the officially supported asset um and if they use this version of the officially supported asset they can transfer between chains without any support at all um i think that's that's an interesting economic level lever i'm we're not sure yet what whether it's going to work but it is interesting one of the one of the fun fun side effects of stable swaps is that uh when you have a stable swap between like three or four different fragmented versions of the same asset typically that stable swap is going to incentivize creation of more fragments first of all to go into metapools and it's going to incentivize the minting of each of those fragments at about the same rate uh so your ideal stable swap pool with three fragments is going to be 33 percent of each one so by introducing this additional you know mechanism to try to fix fragmentation you're gonna end up incentivizing you know like a higher tvl for all of the fragments um and you're going to end up with a huge stable swap pool that exists just to resolve the fragmentation issue that you kind of created with the stable swap pool in the first place um so as long as you know stable swaps are still incentivizing liquidity at the rate they are i think this is going to remain true and as long as we can make money by launching a stable swap i think we're going to see fragmentation uh get worse i'd like to quickly interject here for our next question justin drake please go ahead and ask oh hi there um i i was listening to the to the bankless episode today uh on on celestia with nick and i love the name of celestia i mean i guess i have a couple questions about the lessons um one is would it be fair to say that the security of this of a celestium is kind of uh at best a degraded version of of celestia and the reason being that when you have a settlement layer for example ethereum that settlement layer can't do data availability sampling um itself um and so basically you you lose data value sampling as as a security layer of for your celestion i guess my second question is that um nick mentioned that the the celeste validators can make attestations then these attestations can go on the settlement uh layer and there would be some form of slashing if there's some incorrect associations and i guess my question is how does this slashing mechanism work yeah so like first of all it's definitely the case that celestions do not have the same data availability guarantees as roll-ups for the reason that you stated which is that the ethereum chain cannot do data availability sampling of third-party chains including celestia and so if you want to if if if you want to see if you want ethereum smart contract to check data availability on the third party chain um you have to use a committee based assumption where some committee signs the block to the test to its availability kind of like a data availability committee but the difference with that you can think of celestia as the data is a data availability committee with crypto economic assumptions because in celestia the data availability committee is the kind of validator set of celestia and white this is better than like a standard like normal like uh like closed data committee um because you have this crypto economic like assumption where because celestia itself is kind of like a normal blockchain used by other like used by other applications and the purpose of celestia isn't solely to provide data availability for some specific ethereum roll-up it's just a standard blockchain in its own right and with with with the proof of stake in the validator set and so on and so forth and because lecture has data availability sampling and we have data verbally something like clients so that if a block producer produces it invalid or attests to a block that's not available the like clients can detect that the lifelines and celestia can detect that thanks to their availability and they can help the chain and then slash the stake of the validators that sign the invalid block so you can kind of think about as you can you can kind of think of it as a more like secure like a slightly more secure version of a plasma um or a validium in the sense that you have this crypto economic security i would actually like to i'll go go ahead i was just kind of summarizing in my own words which is basically that you're using the database sampling as a mechanism for the like clients to enforce the slashing yeah of the the committee of validators exactly that's the plan yeah i'd also i'd also like to add some some nuance here uh which is that most committee-based bridges not just most but like all the committee-based bridges that have been deployed so far essentially don't have slashing uh which is why analyzing this new thing where there's actually a way of penalizing the the committee uh in this case the celestial validators that through slashing if they do in fact misbehave essentially you know either making an envelope block or equivocating or something like that uh there's kind of this subtle this subtle thing in the analysis which is that uh if you can if you model it uh where the cost to corrupt the bridge is simply the cost the cost of two-thirds of the state because you know say 100 of the misbehaving validator stake will get burned so you know these two thirds have to be malicious so the cost is two-thirds of of the stake which you know could be in the billions of dollars or whatever who knows okay so this would already be an enormous cost but you know if you say that's the cost of corrupting the bridge that's true if there's no slashing if there is slashing things get a bit subtle because you can say what is the cost of corrupting let's say ethereum today what's the cost of corrupting ethereum for from the perspective of an ethereum application you could say that it's you know 51 percent of the hash power ma proof of work is a bit weird that's the we consider proof of stake lens right you know two-thirds of the e2 stake uh to corrupt the ethereum chain but that's not exactly true because if two-thirds of the ethereum validator set were to try to make an envelope block everyone would simply reject it so like the cost of actually making something invalid is infinite it doesn't actually matter how much is actually a stake in terms of dollar value like the actual cost of corrupting the chain is infinite because it cannot be corrupted uh so so in that context uh it's not exactly uh it's not exactly like you can't analyze it in the same way that you can analyze uh your classical committees because there's this notion that you can slash something you can slash the you can slash the committee out of protocol you can slash them uh in in a way that you can't do it with regular uh committee-based bridges uh so this isn't really presenting a conclusion or anything but it's more that there's some nuance here that is not a simple analysis of you know here's x amount of money you can corrupt the bridge here's x amount of money you can corrupt the l1 there's some there's some subtle differences here because you should be modeling the cost of corrupting an l1 is essentially infinite okay that's all i have to say justin i'll give you another opportunity if you want to say something before we move on to the others no that's all good thank you so much for the answers thanks sri ram we'll go with you next um hi uh uh thank you actually i want to pick up on a thread started by justin here are trying to understand the crypto economics of slashing when data is unavailable it seems there are fundamental differences between uh what happens when uh you have something invalid when a committee signs an inmate message versus when the data is unavailable when the data is unavailable and a two-third majority committee is actually uh signing a block in favor of this unavailable block it seems it's quite difficult to actually and for slashing even if light nodes are observing this because uh that committee could temporarily withhold data and then later reveal it and i think the point that john adler just made on uh the cost being infinite is the cost is able to fool one of those light clients but really as this the twitter space is really secure bridging in a modular world you could still fool the other side of the bridge because the other side of the bridge is ethereum or something else which simply cannot parse anything more than whether there is a two-third committee signature so the broader question is how do we enforce slashing for data availability because that's while it's perceivable there's also ways in which you can hide data withhold data for some time and then later reveal it as well as how do you uh protect it across a pitch thank you yeah so with with this attack that you mentioned where you can like you can you can withhold that data temporarily and then release it there so in celestia there's this big subjectivity assumption which is kind of embedded into the tendermint protocol anyway so because we use proof of stake so there's this kind of like on there's this three week unbonding period in the cosmos group of state portugal and and so we rely we rely on this big subjectivity assumption which basically says that like if you gonna go offline for three weeks um then in order to kind of think the chain from scratch you have to ask a friend to let you know what's like the truth like the trusted or true hash of the network the block height of the network is with the with the block hash and so which means like if that's first thing so which means like firstly if a block producer like uh withholds the block um and then that client sees not available but then but then the chain somehow moves forward and then it actually releases the historic block in four weeks that would that that block will still be ignored even though it's available uh in the future uh because of this big subject weak subject of the assumption but secondly the second thing so we're gonna say something no uh go on i think i was gonna say that the weak subjectivity period therefore bounds the period of withholding yes that's the first thing but but the second thing is that um because tendermint is a is a fork free protocol which means that if like the the validate set misbehaves and forks the chain and creates two conflicting block headers then that's considered to be kind of like a safety failure and what happens is that all the nodes and clients in the network will simply help the chain and so like they will stop processing your blocks and abort like the node and then you have to kind of go back to social consensus um in order to kind of recover the chain uh which means that if you have if if if the block producer has withheld latest block um you can't really kind of proceed to the next block until that block is released so and so there's there isn't really any because there's no concept of forking there isn't really any concept of like a historic block on a on a different fork of the chain that's longer has now become available and now that's the real that's the real fork of the chain should you show your army follow-up to that yep uh it seems therefore that all i need to do to attack the chain is i get a two-third committee sign a block withhold the block and then i have a three-week period in which i can release that block i don't produce any conflicting blocks i just withhold that block so the cost of actually executing this attack rather than being infinite is actually zero but that would violate the weak subjectively assumption uh only if i don't release the block within three weeks so okay if you release the block within three weeks but why why why why would that violate the assumption be that that's isn't that isn't that um like why does that violate the why does that validate the protocol as long as that's like the latest block it just means all that all that means is that it takes three weeks to finalize that block but the block is saying on ethereum nobody could contest that block if i keep the contesting period less than the week's subjectivity period oh i see yeah so in which case the week's objectivity period well the challenge period will have to be longer than any such than the week subject of the period that's right so it seems like fundamentally the uber available finality is over this week's subjectivity period and all crypto economics is related to that yeah i think maybe john has something to add as well uh i would have to [Music] think about uh your question a bit it's a bit unclear exactly uh what your model is so if you could ask us offline uh that would be great absolutely sriram thank you so much for your question we will now move to angelfish uh please ask your question hello everybody um i've been thinking a little bit about how a modular world would give rise to things such as cross-train mev and thinking about how the most popular way that it could manifest would be in terms of multi-domain arbitrage but this could serve as the centralizing force because people that could would could capitalize on this arbitrage opportunity would either have to have money on two chains or three chains where the arbitrage opportunity is and swap them at the same time or the sequencers or like would be incentivized to collude to get these opportunities so for nomad and connects i'm curious whether you guys modulizing bridging could be an avenue to democratize cross-chain arbitrage opportunity specifically and for celestia i'm curious how in a more general sense how you guys deal with ordering and could data available layers be a good hub for somebody like flashbots to build on top of to enable multi-chain block building it's a really good question um so wait can you hear me yes we can okay cool um so uh there's on the on the interrupt side um there's a couple of really important points here um so so there is arbitrage in general as as a space uh crosstalk arbitrage particularly and there's there's some really interesting stuff there um but there's two things that i think are kind of like potentially problematic um the first is you talk about costume mvp which is like allowing front runners to basically front runners extracting value from a transaction that goes across chains um uh you know for for example say you do something like in a single transaction swap on uniswap on ethereum uh bridge those funds and then swap on uh some other you know uh decks on uh ethmos um and uh because this is happening in an asynchronous paradigm it is possible for some front-runner to see the the transaction happened on the sending chain um and then like uh front run the transaction see it like see the event that happened on sending chain front run the transaction on the receiving chain um unfortunately there's not a lot of ways to get around that one thing that we've explored is potentially like uh you know the the data that goes across chains to call the receiving chain contract encrypt that and then decrypt it um uh you would you would basically decrypt you'd have some sort of like uh you'd yeah it's messy you'd have to like encrypt the data that is being emitted in the event and then decrypt it on the receiving chain and then have like push that into the contract you then you would still be subject to like on-chain mev but you would not be subject to like this cross-chain front running a track but that one is is super messy and i think uh unfortunately it doesn't seem like there's a lot of ways to get around it the other one that's even more messy is uh is actually the the paradigm that is currently being followed by things like synapse um and this this is actually where i think the the bigger problem rises which is um a situation where you actually have an amm itself that straddles multiple chains um and so you have a liquidity pool on one chain and then a liquidity ball another chain and then you are determining pricing between those two chains based on uh you know the size of those liquidity pools when a transfer occurs and the fundamental problem there is that you're introducing asynchrony to this to this model that is not designed to be asynchronous um there is a uh there's a kind of two approaches to building distributed systems there's uh uh you know the the approach of of like uh you know having asynchrony in your distributed system and then there's like uh what are called crdts where you just have like reconciliation happen uh after the fact in a in a conflict-free way um and i i'm kind of bearish on the whole concept of like let's build an amm that has like pools on different chains uh for exactly this reason because as as is currently happening with synapse it is extremely possible for uh people to just manipulate the price of the amm itself directly uh as it go as like liquidity goes across chains by by simply sending transactions in the opposite direction uh and front-running users um yeah so one more one more future avenue that we're really really interested in uh right now mev largely relies on these miners running specialized nodes that uh you know the flash bots nodes uh because cross-chain communication is asynchronous building that kind of thing for it is much more difficult and more complex but it's still possible a lot of these tendermint zones for example share big chunks of the validator set and hypothetically those validators can use their leeway in the tendermint protocol to bring those chains into something resembling synchrony that only the validator can access so because tendermint is fork free and because it has one block finality they can in effect if they are validating both of those chains under certain circumstances bring those chains into synchrony make transactions on both chains in a synchronous manner and have whatever effect they want to uh so we're really interested in this in the future it is a much more long-term research avenue and again it relies on running custom node software on dozens of validators across dozens of chains mustafa john anything to add on angelfish's question nope james gave a pretty good answer okay cool well i think um we can wrap up here uh james uh arjun you guys are our guests we will leave you with closing thoughts arjun why don't you go first uh as it relates to the modular blockchain thesis and uh most of the stuff we talked about what are your your closing thoughts on this twitter space and what would you like to leave the audience with um oh man um yeah i mean i think um yeah there's so many closing thoughts um so i think like modularity is a term that is probably going to be used more and more as we as we like explore this space um it does seem to be the direction that a lot of uh that a lot of decentralized application decentralized network development is going because i think people are beginning to understand that it is better to um have hyper specialization at each layer of the stack um and uh and so yeah i i you know i would love to see more people kind of getting involved in that research and getting involved in this conversation um it's it's fantastic that like we have celestia here is fantastic that like the modular thesis is is playing out really well in in like a bunch of different ecosystems um so i think just getting more researchers involved is certainly going to make it much more um is going to help us get to solutions faster yeah also thanks for having us james thank you yeah definitely um so the the great part of the modular blockchain thesis and modular bridges we get to specialize with the ice things and the domains and we get to you know work at the boundaries uh the whole project of bridging and the whole project of cross-chain communication is to work at the boundaries between these different ecosystems to reconcile their differences and build something coherent on top of them that interacts with them directly that's what i really enjoy doing and that's what like keeps me coming back to it for so many years as this ability to work with so many systems uh learn so many things about each execution environment in each chain and then bring them into practice and write code for them so if that sounds fun to you definitely uh reach out all right everyone we'll conclude at this thank you so much uh one last piece celestia is hosting the modular summit at devconnect in amsterdam next month um and i believe anatoly from solana will be in attendance uh with a one-on-one with mustafa it will be exciting and there will be more on twitter for that thank you so much for your time james arjun we'll see you guys when we see you guys thanks for having us mustafa john thank you thank you thanks thanks so much [Music] 