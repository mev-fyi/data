hey everybody thank you for coming um i'm starting off this research uh series but i i wouldn't call myself a researcher so um i'm sure the the next people will you know give you much better research than i will be presenting but um i'm really glad to have this opportunity i recently got involved with celestia and um i actually came to it from an interest in researching dows and i had been doing a lot of um research on how decentralized methods of organizing labor can create higher levels of productivity and what's really cool is that the modular design that celestia has can actually create a very high amount of innovation in the execution layer so i'm gonna i've made this argument before and i've posted it online i'm going but so what i'm going to do is i'm going to talk about a few things adjacent to this point and then i'll try to show why i believe that a lot of of innovation will happen on celestia but first i'll just talk a little bit about more about myself and what i'm working on so i'm building a cloud services company um kind of like infura for celestia so what i'm doing is i'm re-implementing a celestia node i'm listening to the networks and indexing the new blocks into aws and then i'm replicating the endpoint using serverless technology and this can allow people to access celestia nodes without running it on their own there'd be no upfront cost for them no minimum spend no devops effort and they would have extremely high speed scale and reliability for any um any time they need to query a node so if you're interested in using a service like this or potentially a future service cloud service offering based on celestia data please um let me know and i we haven't launched yet but um i would love to work with you so um yeah and i get it um aws um people some people don't like using that but i'll talk more about why i don't think that there's any problem with a crypto company using aws in the future um so okay here's the talk so i want to talk about this concept that i find really interesting it's called software 2. so this is something i think it was coined by andre carpathy who's the director of ai at tesla and basically and i'm not an ai expert by any means but my understanding is that back in the 80s people when approaching a problem such as visual recognition but also other types of problems that we now use ai to solve they tried to decompose the problem manually so they would try to you know detect edges and they would build all these very complex systems and so you find a it's a person if they have an arm and a head and eyes and you detect those because they have this pattern of pixels and stuff like that and these approaches didn't work for building complex software such as visual recognition and over time um ai and i promise this will get to blockchain eventually um but over time they started introducing machine learning into the process of increasingly and they would engineer features and they would put that into a pipeline where they would train on a data set and you would start getting you know better results that way because it's very hard there's so much variability in the type of um work that that that machine learning does and and so the traditional engineering approach which which is called the approach by decomposition um where you basically break down the problem into a series of steps and you hard code what what the software does that doesn't work for very complex for very complex software and as you can see here on this on this chart and this is all from andre carpethy's talk this is not something that i originated but over time they basically discovered that it's more effective to step back and do less work and what you want to do is um i mean like here's the quote it's difficult to take millions of numbers that describe an image and turn it into a concept because there's so much variation we want to step back and not design those features we actually want to lay out a skeleton of the architecture and leave much more to the optimization so we are stepping back designing less and things work better so the the so more modern ai models are actually you're running an optimization on a program space so like if you look at this dot on in this circle that's an implementation of of some code and it's not a very complex application it's something that a human was able to write themselves but there's also this large area here um that this would be what software two would do is you would carve out a space metaphorically in in the um list of possible program spaces and you would run an optimization uh on top you would optimize on the program space to find an implementation that um that is the best and so for example alphago was done this way they use this at tesla and there's all kinds of things you can do um with this um and like so one interesting example of this and again i promise that this will get the blockchain um one thing you can do with this and this is cool as dolly it just came out um it i thought it was kind of funny that just as nfts started giving illustrators a revenue source they got basically completely automated um but it's um but what's what's important here to note is that with the engineers are building here is software that creates software and there's and they're creating software that searches for the software that they're trying to create so okay so how much can this be generalized because this is actually really interesting you want to step back and say can i create something that can create something better than i could create the thing that was going to be created so this is what andre says about it he says anytime you have an evaluation criterion such as winning a game of chess or go you can apply this methodology you can search the program space and you can actually discover through optimization by paying and compute the much better algorithms in that space um so it's so this is something that requires a large amount of compute to do um so my kind of point here and this again this this is just something that i'm thinking it's not there's only so much that um i can substantiate this claim but web3 is a form of software too so let me explain what that means and what the implications of that are but first before i do that we need to like go to in a political analogy so central planning is a way that people have run an economy and they've they've you know used this to allocate resources and organize people um at scale and so countries have adopted this and when they have it's it's never worked out um there's been you know massive shortages or overproductions and they would make too many of one thing and not enough of another thing and the problem there's a there's actually a reason behind why this happens and it the reason is that one that knowledge is distributed in society across all the individuals so if you think of this algorithmically you you if you're running a computation and you need to access knowledge access data that's stored with throughout a network you're going to need some way of retrieving that information for your calculation and when you don't have prices you're also unable to calculate trade-offs so what they do is they basically make a and they make it impossible to um compute the op the a they make it impossible to compute the relative trade-offs between different things that could be produced like for example if you want to build a car or a house both of them contain some amount of metal how much should go to each one without prices you can't calculate what the trade-offs are between cars and houses and furthermore you without the knowledge of everyone in in society you can't know like what people would prefer to have and and what very and you know they wouldn't know things about what they know about the environment in which the house is being built or car and um so it basically when you go to at a large scale you you lose your ability to allocate resources um and a connection that i made is that product management meaning deciding what you're going to build what what engineers are going to build is a form of central planning so but there is a solution to planning an economy and this solution is um to use markets so markets are the are the greatest decentralized system that has ever existed um they yield amazing results and this is a log scale graph as you can see it just goes up and to the right and um no matter how big or small the market is you can um it will deliver compounding growth and you get this really valuable price information when people are able to coordinate at scale and as you can see this is a decentralized process um so like here's some structural properties of markets that kind of support this um take on it like if you compare it to bitcoin mining or bittorrent which are both things that are considered to be um decentralized processes um they have these really great characteristics that make them very good on the internet which meaning you can have untrusted people participate you can you can require incremental contribution and they can be uh they can contribute in parallel so when you're thinking about this doing something at an internet scale you're thinking of the possibility of many many people participating um and you don't know who these people are you don't trust them and they may not have a capacity that is um very high in in their contributions so with all of these systems like for example you can join you know bittorrent and there's cryptographic guarantees that what you're sending is what you're saying you're gonna send um you can um if you're you know you're in bittorrent you can just seed a few bytes of data and then you can go offline um and many people can see different data or the same data and it all works together beautifully so similarly though a market has this type of these characteristics and in it practically speaking it is the only way that people have been able to coordinate to work together at scale and i've written a newsletter article about this um so web 3 allows us to and the reason i'm using the term web3 is just because of the symmetry with software too but um web 3 allows us to program markets so tokens and blockchains can create designer markets that can achieve specific goals including building more software and that would in this case it would be software too since it is software that creates software and the mental model i like to think about is basically that a human is a neural engine and you can have a cluster of neural engines and and that would be you know the human colossus and um we have you know a lot of people coming online entering the economy all around the world and so um it's it's very exciting to think about the possibilities um when you can you can use a market to incorporate labor from many different people um and yeah humans are and just like ai humans are you know they are optimizers they optimize for profit um and um these are called incentives we don't think about them at this as an optimization problem but that's because i don't think we've entered the era in which you can in any meaningful way um you know program markets and design markets but now we're at the stage where we can start to think about like tokenomics is basically the field of engineering markets and using that to harness toward harnessing people towards a productive end so so basically i think i kind of already covered this but um yeah i mean i guess i kind of already i kind of already said all this stuff but um it fits this fits the criteria you can if you design a tokenomics scheme that can allow people to profit that is an evaluation criterion and it can scale so i've done some research on this topic um and how this could be used um for building build building social apps but i haven't used this uh i haven't pursued this in terms of how you can build roll-ups or blockchains but i think that this is a valid research question and i think that people should be looking into it and i there's like for example there is a lot of discussion about how certain certain projects have tokens when maybe they um don't need a token i mean people say it's just for the sake of funding but that kind of starts to look more reasonable uh when you think about that that um it's basically um funding the improvement um of us of a search inside of a program space of possible implementations um and i'm of course of course someone would need to flush out that argument but it's it's that that could be a start um but um more specifically um i think that there's already good reason to believe that there's going to be a high level of innovation on celestia even without taking into consideration these types of more advanced tokenomics plays and this is again because of the type of infrastructure that it provides to a market so i was hoping that i would be able to you use my computer to show this next part no i think i think i can just do it from here okay i actually already have this up so can you see this okay sorry i just wanted to sh like i wanted to walk through this this thread that i had written on celestia so basically the idea oh no it's not scrolling sorry about this guys i'm not sure it's gonna work well i can go here one annoying thing about twitter is if you don't have i think it's if you don't have your phone number on your account um they restrict every image for you and stuff like that even though this is my own account i can't even view it um so i'll just try to run through this basically um let's say that you're making a change to the evm um there's basically two ways that you can do this one is you can submit a pull request to to the evm or yeah to the code base but the problem is that you you would need to convince people to accept your change um and that can take a lot of time um the other option is if you don't want to you know be dependent on and but by the way they may not accept your change but the other problem though the other option is you could um you could just hard fork it and then you have the complete capability of doing you know whatever you want but when you do that unfortunately you're segmenting you know your project from the ethereum network and so you you don't have users are not going to necessarily want to come on and use your implementation and um under a traditional model before celestia this would come at a security cost to anyone who wants to come and use your project so this is a disincentive um for people to to um do your change so if you want to make a targeted change that that just simply optimizes the evm you're gonna you're gonna be at a severe disadvantage and ethereum is gonna have a significant moat in in its current implementation sorry the evm is is going to be basically stagnated because there's going to be a moat for people switching to any sort of competition so there's there's a lot of problems that this um that this causes it since since hard forks aren't really viable even if you think that you could convince people to adopt your change um the problem is that um you can't accept all the changes so um even if they were really fast at accepting changes and they made all like really good choices at accepting the changes you can have a conflicting change that conflicts with someone else and you can be having different trade-offs um and um if you have an implementation someone else does they're going to have to choose between your implementations so this again causes um it prevents changes from being experimented within the market and so this is a problem because what what it means is that you there's a significant barrier to trying things and some things will never it's impossible to try them and as a result you don't you end up being way more conservative in your choices of what code gets merged and what gets used and so and like what you have here is like a distribution and these are the outliers there's going to be very few changes but these very few changes can have a very high impact and this can be the kind of crazy idea that revolutionizes um revolutionizes the evm um and like just an example is bitcoin i think was like a great example of something that everyone thought was not going to work when they heard about it at first i'm sure that i mean i know there's people who didn't think that who did think it would work but like most people that heard about it in the beginning thought who would ever want this internet money and when it was actually brought to market satoshi had released the code and the white paper at the same time and it actually got adoption and that proved that it was a good idea so you don't get any of these types of ideas when you have just this one monolithic code base that needs to be um i mean it needs to be conservative so um and so going to this um article that i had written there's this important concept in that startups that startups have known about for a long time and it's called the the rate of iteration and basically the faster you iterate as a company the more compounding growth you're going to experience um so and the reason for that is because every time you iterate on your software you're taking into consideration the um you know new information that you have so you're acting from a superior point of knowledge and you're also making a small bet that can have an asymmetric payoff and you can create a kind of s cur like a bunch of little s curves that can um go and that can turn into an into exponential growth and so when you are able to hard fork um when you're able to hard fork uh um software what's really cool and this was my the original research direction that got me interested in this is that um hard forks are done completely in parallel and and it follows that um that chart that i had shown earlier where you don't have to um trust the person who's hard forking you and you and the person who forks the other person doesn't have to doesn't have to trust them um as well as so it's untrusted um if someone you know does a bad job that's okay um or they can do a good job and then that will be a success but it doesn't drag down anybody it doesn't it doesn't hurt you to be hard forked um and um and so basically it's a way of parallelizing software development um and so when you parallelize software development you're increasing the rate of iteration because multiple iterations can be ran in parallel and since this is a comp um since this is a driver of compounding growth you can actually you know you can by since a lot of code is in common and ideas are in common you can build off of each hard forks can build off of each other's work and merge back upstream or hard fork downstream and that can lead to an acceleration in in the uh i mean this is just obviously i just drew this it's not like real data but um in in theory right as your iterations increase your output will increase exponentially and unfortunately what happens though is as i was saying earlier how um central planning is a product management is a form of essential planning there you you get an iteration bottleneck here um and this causes a stagnation in output over time so as an organization you're only five more minutes so as an organization um scales you're not able to allocate resources as effectively so anyway i only have five more minutes so i'm going to skip ahead here um yeah i mean i think i think i pretty much covered all of this conceptually so i'm just going back to the presentation um yeah so um i guess i just wanted to return to that point on cloud services um i i just i i don't um i i don't think anyone actually is thinking this but um i just it is something comments that i get a lot about you know you don't want to depend too much on amazon um and i think that um one one thing based off of what i was just saying is that um it's great there is a moat in the ethereum ecosystem to new chains and um there's a lot of cloud services that support development on on those ecosystems and so providing um cloud services to them can help providing cloud infrastructure that can help new chains roll out cloud service offerings similar to that of ethereum can actually increase decentralization and the rate of innovation because there's there's less reason for for people not to switch to them and it's it's normal to have you know a big provider like amazon is a big provider of hosting that doesn't mean that the internet is necessarily decentralized it's possible to have an over-reliance on something but there's also always a power law distribution in any network so someone is going to be servicing a large number of requests and a large amount of compute that's a natural phenomena and if you know if something does go down if people are relying on a service and it goes down that doesn't mean that the network is necessarily centralized if as long as the network can recover markets are decentralized and there's depressions there's market failures of different sorts but the market does clear those and recover so i don't think that that's necessarily a problem and yeah and if there is you know an over-reliance on some piece of infrastructure that's definitely something that the protocol should take into consideration and they should try to design that out so anyway that that's all thank you so much i appreciate it [Applause] 