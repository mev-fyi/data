okay so uh we have a pretty unique set of teams here today um for the most part over the last few years as people have thought about CK snarks they've thought about how they relate to Roll-Ups and validity Roll-Ups but I would say in the last 12 to 18 months there's a handful of teams that have started exploring what it looks like to use Starks for Creative applications Beyond Roll-Ups um and I think this panel represents a lot of those teams um maybe we could talk a bit about um some of the unique opportunities that introduces why we think it's interesting and compelling but also some of the challenges that come with it so one of the first things I think we can start with is just like a motivating question around ZK and and the trust assumptions sort of inherit in your interest in ZK is the belief that like trust is very important in blockchains for years we've had Solutions like off-chain data processing that sort of either happen optimistically or just with an assumption of trust what about trust assumptions is important to you and why do you think it's so important that we do it cryptographically instead of relying on some of these weaker assumptions what I like about ZK is it gives you an opportunity to establish a point of trust and then inherit computation from that point and I think the the notion of trustlessness is oftentimes misleading so you know when we think of a ZK roll-up what you're functionally doing is you're inheriting trust in a state transition from the consensus level collateral and when we think about co-processing or we think about any type of access to on-chain data from a contract that can't be accessed to the vm's native execution what you're trying to do is get as close as possible to your computation running on the same trust assumptions as the underlying base layer so it's not trustless per se as more as much as it is the inheritance of trust from a very specific designated point yeah it's a different boundary box a little bit than where we were before but I think if you look at the base layer of no trust this moves it uh it up a level it is any other any other thoughts on that why is it important for you guys yeah we've been talking to a lot of application teams and really trying to educate them on what ZK can do for them and as I'm sure everyone in the audience knows although we talk about these very secure ZK based systems in reality a lot of things on chain today rely on development teams being honest even a lot of the systems in production will eventually become you know fully secure optimistic Roll-Ups but today have permission sequencers and I think where in the next year or so I see ZK really being valuable is in cases where social consensus will not accept a trusted Oracle and those are typically cases where composition is very important so if you're a protocol team maybe your users will accept your trusted Oracle but where it really gets difficult is when another protocol wants to compose on top of that protocol well that protocols users trust this random other protocols team I think the chain of trust gets very tenuous I think ZK really helps in establishing more clear trust abstractions and boundaries yeah to add to that I think there are some existing situations where we've already seen trust be super problematic abuse and then eventually leads to things like Bridge hacks and results in like very tangible amounts of money getting taken from users and protocols I think we're all aware of all the bridge hacks that have happened uh very recently and I think in those situations it makes it really clear why the existing trust assumptions of multi-cigs or whatever other mechanisms they currently use aren't acceptable because it has already led to material loss I think that's a great one composability is often overlooked but if you have like a key trust assumption in the in the middle of a chain of trust then you can't really build another highly valuable protocol on top of something that has weak trust assumptions so it's a good one um yeah and so it's sort of implicit in that conversation it was um an attempt to do it a bit differently than we've done in the past to avoid some of the bridge hacks that have have happened help improve composability and protocols how are your teams thinking about doing it a bit differently uh maybe starting decentralized or enabling that very early in your product's life cycle yeah so what we focus on at LaGrange is supporting data parallel computation on top of large chunks of on-chain data to be provable efficiently so broadly speaking when we think about the space of on-chain data we're functionally constrained by the ability to access from the execution layer the majority of the state that has been created in the the canonical history of a chain and this is even more prevalent when we think about the modular context when you have a series of different execution spaces with a series of different state structures transaction trees receipt trees and block histories and so what we focus on is to allow you to treat this what is more or less massive on structured data as if it was your data Lake in web 2 to run SQL mapreduce rdd massively parallel processing computational models on top of this to be able to derive and extract properties that are relevant to your applications function yeah maybe pulling it in a bit to focus on decentralization and yeah and so I mean I think functionally decentralization in the context of this question has to do with a few relevant vectors so firstly like you have to think about the decentralization's approver which confers a liveness assumption on the overall protocols that inherit computation from it and moreover you have to also think about the the Assumption of where you're deriving data from and a single chain context it's very straightforward when you get multi-chain some of the work that the sync does it gets a little bit more opaque you know you have to derive data ideally from the underlying consensus of a source chain or from a closer process like a like client is you can get to that yeah I would say one really important piece for this is for users to be able to audit what every team is doing and what's actually been deployed like I think that's the the four of us on stage here can talk all we want about how we our teams have a security mindset and all the things we're doing but ultimately users actually have to be able to verify and put our systems to the test so I think there are a lot of things that go into that one is just standing the test of time in a production environment the second is having a lot of transparency around code base being open source having a reproducible build for any verifier you Deploy on chain and third is adopting cutting-edge security techniques like formal verification or fuzzing to give a higher degree of guarantee to users so I think all of these systems are going to be difficult to trust at a super high level until they've been running live for years and whatever we can do to allow users to audit that more quickly is going to move the space as a whole forward yeah to talk more about security I think vitalik Advocates even in the context of zkevms for this two-factor approach where maybe you have one computation that's done in ZK and then maybe you have a trusted tee sgx based two-factor or maybe you have many different implementations of the same function I think one really interesting thing about ZK is that at the end of the day all the functions we are Computing are just f of x equals y and so it's very clearly specked and so you can actually have multiple redundant implementations of the same computation uh very easily because it's just inputs and outputs and the circuit has to be the same and I think that will be really powerful in the security story of actually getting ZK adopted and for people to feel comfortable using it in their DAP in a very critical context yeah those are good points do you how are you thinking about I mean multi-proofer kind of implies a separate software stack do we do any of us think maybe we would push in the direction of a full separate software stack for another prover I mean the t's may be a shorter path where you you don't necessarily need a whole new stack I think you can already see that there's a few Primitives that are implemented across a few different proving systems at the end of the day I think all of us are on stage are doing pretty similar stuff with hash functions signature schemes and other cryptographic Primitives that are fundamental and it's really not too hard to take the same primitive and re-implement it in a new stock and we already see multiple implementations today so I think it's quite feasible to actually have multiple implementations and something we can push towards so I'm not advocating for a different prover for the same proving system but more multiple redundant implementations and even different proving systems I I think it also requires being thoughtful over you know the the scope of what you are proving I mean I think if you're proving something like a like client where you have a very fixed set of parameters over what a correct execution of that is it's it's more straightforward than if you're building a general purpose VM when you're incurring a significant amount of technical debt when you're anchoring to a specific proving system potentially if there's a change in the state of the art and you can't have your your back end be agnostic your front-end be agnostic to the back end and so there's complexities there I think that are inherently incurred as you as you develop applications that have more and more zero knowledge intrinsic to their corporate their core purpose and I think in those situations having having multiple back ends becomes an imperative if you're trying to ensure that you can stay up to date and you can your performance can stay relevant yeah that makes sense what are you doing in the meantime like we're not quite there to a multi-prover world I think most most all of you are working on getting into production pretty soon are you putting in Gates or checks in your contracts or you know every proof needs a signature along with it for now um how are you thinking about that in the short term make sure you don't have a catastrophic bug in prod yeah we deployed to mainnet two weeks ago and we put in gating on the prover so if there is a circuit bug in soundness we certainly will not trigger that bug we've also put in time lock upgrades on our verifiers so that we can actually fix any issues that come up we do feel like this should be a temporary phase until we're able to introduce stronger security techniques like formal verification yeah I think we can follow a lot of the in production ZK Roll-Ups today they all have similar setups where they have approved provers you have time locked upgrade and governance over the verifiers I think all of those things are very reasonable because again if they are in the critical path and there is a hack that can potentially be catastrophic I think we think about following their lead and their design choices and we think that's like a very reasonable short-term trade-off before we get multi-prover and trusted execution environments we agree I mean I think there's good precedence over teams who have have pushed large production code bases with complicated underlying circuits and have done so in a way that has to date been more or less secure I actually had a conversation with a an auditing team that will remain unnamed that had done some zika evm Audits and they were nervous they said to themselves like I don't know if we understand this well enough to really put it in production we've like done our best here um are there things Beyond audits that you're trying to do internally at your companies maybe to have a culture of security or or help ensure at the code development time that it uh when you go to production you don't have problems yeah I mean I think I think having a culture of security is very important and I think you need to be very clear on your code reviews and your best practices as you're implementing and developing your underlying infrastructure I'd extend that broadly and say like I think just from a business operations standpoint right now you should be resourcing and hiring people who have an understanding The Primitives that they're working with especially when you're building these highly complicated systems it's important that that the work that's being done is done by people who have an awareness and a context for how the things they're building works yeah we think that of course Standard Security practices are very valuable but actually I think there's some very obvious things which are very helpful one would be having less code just having a more minimal and well-designed system so you don't have a lot as large a security surface area the second would be looking at the interfaces between ZK and blockchain systems where the two systems are really of quite different natures and we think these boundaries are places where issues are more likely to arise so of course circuits might have bugs but I think it's much more likely you just completely mispars part of your circuit and do something quite obviously wrong yeah you can even see this in the bridge context where a lot of bridge hacks have been due to trust assumptions getting violated but then other Bridge hacks are simply due to Smart contract bugs I think goes to support use point maybe zooming out a little bit and thinking about modularity given where we are um and Uma I guess you you just announced that you're working on a Celestia Bridge or any of the other teams thinking about sort of modular DNA layers in their environments or are you mostly focused on specific chains we we focus very heavily on modular with how we're developing our infrastructure I think being able to permissionly permissionlessly support data access in a modular context is very important I think especially as we see a proliferation of new execution spaces whether those be you know roll up as a service providers or l3s on top of existing scalability Solutions I think it's very important to ensure that in terms of State access that you're not constrained by by the ecosystems that you're interrupting with or interacting with principally yeah we're currently focused on evm but I actually want to point out another aspect of the word modular that I think ZK is very useful for if you are able to use ZK introspection onto the pseudo chain as part of your application you can sometimes dramatically simplify the on-chain architecture of your smart contracts basically you don't need to be recording a lot of extraneous information to state that you could later read using ZK and so we think that this can contribute to a trend in smart contracts actually getting more modular yeah I agree with that I think State access on chain has led to development practices that if you had principally better data access and principally better compute on top of that data could be alleviated and I think if we look at like optimistic Roll-Ups and we look at the the the the bisection game there there's things there that can be simplified drastically I would argue reducing some of the implicit security assumptions to it cool um let me think if there's anything on that topic I I guess yeah one one of the challenges um of having like a a proof or a light client built off of a given chain I mean in L1 we can kind of um assume there's a lot of economic stake behind this uh this root of trust if we when we go into a modular chain ecosystem where you have a root of trust that maybe has lower economic state or longer times to finalize I think that introduces a challenge in in the level of trust you can you can put in that how are you uh alleviating that problem I think LaGrange I know you've published some thoughts on state commitments and I think this challenge will also impact Axiom as you guys try to look at other chain ecosystems yeah I think the core challenge is essentially although let's say an optimistic roll-up might have a longer finality period let's say seven days users really demand some sort of weaker guarantee that can hold much faster and so we think it's actually more appropriate to leave that sort of guarantee to the application if you're trying to withdraw 100 million dollars from optimism maybe you should wait seven days before someone else accepts it if you're trying to play a game on optimism who cares just accept it and so we think it's important that for the end user the guarantee that you're precisely offering is extremely clear I think part of the complexity with having a clear guarantee for the end user application is that it opens the design space up for Less transparent infrastructure providers to have opacity over the underlying design decisions of their protocols and this is I think the concern with a lot of cross-champ protocols today that originate messages from optimistic execution environments and so one of the things that our team works on is using existing ethereum like existing ethereum valid asset collateral with eigenlayer to be able to assert a early degree of economic attestation behind the validity of a reported State transition by for an optimistic rollup and the reason that we think this is very valuable is you can have Bridges permissionlessly consuming state from a shared layer with a clear amount of economic trust and economic security behind the state that they're using and it means that if you want to understand how much security is behind a given attestation estate you can very quickly look at the size of the committee and the stake within that committee and you can derive that assertion from there and you don't have to worry about you know whether or not a k of n assumption for an arbitrary bridge that could be used in some intermediary protocol has the a sufficiently decentralized underlying validator set we think of this as like very much a public good great maybe we could spend the last few minutes here um zooming out and and thinking about uh sort of ZK beyond the blockchain I think snarks in general just receive no attention uh or minimal attention outside of the the crypto domain and I think a crypto serve it is a good incubator for this technology to kind of grow up and get mature um but over time there's probably an intersection with zika snarks and and the broader internet at large is there anything kind of exciting and interesting that starts to happen as we see more verifiable computation used throughout the internet are we all just crypto Maxes so I've done some academic work on zkml putting some of the largest known machine learning models into ZK and as consequence I've had to explain to some pretty well-known machine learning professors like what is ZK their reaction is always the same number one is that's impossible like you got something wrong number two and this takes varying periods of time for different faculty members is okay maybe it's possible but it's useless for us and so and number three some of them are like oh maybe in this like Edge Edge case it might be useful and so I think it's a lot of Education in finding the places where having verifiable computation actually makes sense in a non-hostile environment typically their response is hey like why would I use a snark I could just run it or I trust Amazon more than your weird crypto system and so I think it's very sobering in in showing us that as a space we need to be delivering real world value that's exogenous to this relatively insular crypto world that said I think one Trend that pushes a lot of people I know who've been bearish on crypto bearish on ZK forever to be very interested is the rise of AI people are very worried about spoofing about deep fakes and they really want a notion of provenance to exist just the other day I called my bank to verify my identity for a wire I'm pretty sure that's just not going to be a thing in a couple years and I think people are very hungry for a solution and I do think ZK can play a role in that finding use cases like zkml and worldcoin are an interesting uh case where they're not fully they are in crypto but they're also kind of Bridging the real world and it's a very real world application of zkml but of course also World coin is going to be settled on an OP stack roll-up and so they are actually using a lot of crypto properties as well in their system I think use cases like that that straddle the real world and the crypto system are really important and what really excite me um and so looking forward to more of those I'm sure there's more ZK ml use cases for similar or new sorts of products like that and one thing I'd also add is I think when we think of verifiable computing crypto we we have a tendency to think principally about the succinctness of the computation and we don't I think most of us here are not talking most about about the the ability to compute verifiably on a private set of inputs and in the web 2 context there's a lot of examples of where that is and will be highly viable and I would say you know if we think about like Enterprise transfer of data and the inability for most for a lot of webto companies to have effective effective orchestration of computation across shared data assets to mitigate fraud to have better user experience and customer experience like the the fragmentation of data within major companies today makes it very difficult in financial services in the healthcare sector for there to be applications that can act in the best interest of of the underlying user base while still being able to preserve privacy of each of the Enterprises that has that has that data cool are we doing questions I don't know okay I think I think we're pretty close to time but I also think we're wrapping up the end of the conference so maybe we could do a few questions from the audience there's the one talk after this but okay sorry we can do all descriptions of course anybody okay I know maybe you want to say something we don't bite what's a storage proof yeah just a general question about like see like how much more efficiency gains do you think is do you think there's left for ZK proofing systems like from here is it a 10x 100x is like a 2X because you know people are still kind of skeptical about like how practicals UK is you know from approving from a computational perspective yeah I mean I think that's a good question but I think if we were to assess the the trends in computation over the last 20 30 years there there are a number of buoying factors that will result in ZK becoming increasingly performant irrespective of the underlying proving systems that we're talking about I'd say that you can you can discuss improvements in the proving system as well as improvements in underlying Computing infrastructure that both likely will have have positive effects over time yeah I think to add currently uh I think all of us perhaps run our circuits on fairly commodity hardware and there's a lot of ZK Hardware companies that are out there trying to make Hardware level improvements to these proving systems and make it faster so that's one Frontier to push and then of course there's also new proving systems all the time I think Nova came out this year and perhaps there will be more in that line of work that make a algorithmic Improvement so I'm very optimistic that the algorithmic Improvement Plus the hardware Improvement is going to result in huge gains to ZK yeah if we want to think about the theoretical limit Justin Taylor put out a blog post I believe last year discussing this essentially there are two sources of overhead one is in converting a normal computer program into a ZK circuit there he thinks that the limit is maybe a factor of 100 and the second is in the actual proof system where maybe you lose another factor of 10 in the limit so that would add up to a 1000x overhead over a normal computer right now we are nowhere close to that so I think we can easily get 100 to a thousand X Improvement without any hardware and one last thing I'll add on that is that doesn't account for the hardware side of things which typically is between 10 and sometimes up to 100 times Improvement um depending on the specialization of the hardware so fast proofs of the future all right I think that's do we have time for one more yes we do okay thank you so as someone who's been on a team who's seen you know developers probably not elegantly put data on chain and you know they're just pushing a lot there I'm very excited for this idea of what you guys you know calling co-processors any predictions on how long it'll take to make the switch from these very inelegant on-chain protocols to you know these in my mind much more elegant protocols a year two years three years like how long do you guys think that that switch will take I I think the principal constraint there is twofold I think there's developer adoption and there is whether or not developers feel comfortable altering the Paradigm and the the function of their application to now include new primitive irrespective of how secure those programs may be I would say secondly there there's the the question over whether or not there are clear instances on chain now where there are applications that could leverage better data access and we'll leverage better data access in um changes that they will make to their underlying infrastructure in a shorter period of time I think another factor that comes into play is really social consensus I think right now it's frankly still pretty difficult to have trustless versions of a lot of the things that developers just putting up the number on chain for I think once it becomes easy enough users will start demanding it and it's something that we'll see slowly and then suddenly okay thank you everyone for your time and enjoy the rest of the conference thank you [Applause] 