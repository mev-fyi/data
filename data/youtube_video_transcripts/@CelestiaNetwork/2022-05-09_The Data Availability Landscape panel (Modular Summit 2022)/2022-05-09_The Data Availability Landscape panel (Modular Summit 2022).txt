good evening everybody welcome to this panel discussion it's a great pleasure to be here thanks to the celestia organizers and the modeler summit organizers for bringing us here uh it's a pleasure to share this stage with these illustrious gentlemen here um okay maybe we get started with a brief introduction of the data availability stack that each of you are building i'm just here to moderate this uh discussion please yeah so i'm anurag i'm representing polygon avail polygon avail is a data ability focused blockchain so we began thinking about this problem in around 2020 celestia or lazy ledger you know like i think mustafa had written a paper regarding that and you know like we were interested in exploring that but you know like they went with the fraud proof approach and you know like when kzg polynomial commitments you know really uh became better known i would say you know like we thought that you know we could uh structure our data ability focus solution on using kzg proofs uh so uh the architecture is that you know of the data ability focus in the sense that we do focus on transaction ordering um and not execution and specifically we use validity proofs um you know your kcg proofs for that and the first use case which we are focusing on because you know like we have uh three different uh projects under the polygon umbrella namely polygon hermes which is focusing on zika evm for example polygon zero and polygon maidan so we have three zkvm solutions within the polygon suit and so the first focus was to you know kind of working on validium solutions for these three solutions uh so that's the focus and then of course we will move to like sovereign app chains going forward so that's the uh rough brief thanks for watching okay is this on okay uh so hi everyone my name is john uh i'm the co-founder and chief research officer of celestia labs and we're building celestia the first modular blockchain network this was formerly known as laser ledger and it's a data availability only chain that is overhead minimized which means in order to use the chain uh for data availability in order to verify that the chain is valid you have to do a very small amount of work ideally as minimal as possible the particular data availability scheme that we use is actually essentially completely identical to the one that was first proposed by my co-founder mustafa in his earlier paper on fraud and data availability proofs uh to secure uh trust minimized my clients effectively completely unchanged from that paper which was three four years ago at this point uh so there aren't many cases in the blockchain space where you know you don't have to fundamentally pivot your technology over that many years but in this case it did happen oh thanks hi my name is matt i am a researcher working on ethereum and you've probably been hearing a lot about the ethereum scaling stack called bank sharding and dink sharding is a new take on the ethereum sharding road map that exploits some of the uh observations that people have been making over the last couple years related to the centralization of block builders in the ecosystem and think sharding relies on the fact that in the future we expect that the people who are building blocks are going to be highly sophisticated more resourced actors and the people who are proposing blocks are going to be lower resourced actors that's coming from a more decentralized set and so with dank sharding we are able to avoid some of the clunkiness that existed in previous sharding proposals where there were 64 shards 64 sharp proposers there was latency between creating a block on a shard and that block being checkpointed onto the beacon chain and so now we're taking advantage of the central more centralized building actors and they're able to aggregate 64 shards worth of data into a single 2d kcg scheme and provide that to the network and it's also inspired by this lazy ledger paper that was written four or five years ago where people are sampling the data elements from this super 2d encoded block i would say also for the the encoding scheme for the the blocks is definitely more efficient because you're able to only do 75 samples of the block rather than 30 samples for each shard block so it is a lot better than the previous proposals for how much data needs to be downloaded from each validator that exists in the validator set thanks so the main agenda of this discussion is to understand the different data availability architectures so what we will do is try to uh set up the axes of comparisons one axis i think is obviously security what is the security model that each of these different projects are trying to optimize for one way to phrase this question is what would it cost for me to get an unavailable data and pass it as being available right so that's something maybe we can discuss about the different schemes in the kzg commitment scheme the the scheme is set up as such that i think safety of data is not a big problem uh the problem i think would be maybe uh insufficient number of light lines for example sampling the data because there's no incentive as such so the idea is that you know like uh if you are building like an app specific chain you would have the incentive to you know like have light lands sampling the data so i would imagine um having insufficient number of light lines sampling the data or part of the data is available uh not from the block itself but if you consider the entire app history so that is that could be a case but you know in general due to the construction of the scheme um it's pretty nicely laid out that you know like either data is available or not available so so one point i want to add here is we call this data availability sampling and when we call something data availability sampling what we mean is you any node any light node like uh arjun just mentioned android just mentioned is any light node should be able to sample an arbitrary subset of the chunks of a given block and then verify that they are available a very important underlying assumption here is private random sampling that a light node should be able to do check a random sample of different chunks this i think you know goes back to the security question and maybe we can address this but the the question is how do we think about private random sampling how do we make sure that light nodes actually when they're sampling it is private and it is random because there are all kinds of de-anonymization attacks for example if i know that the same ip address is requesting these 32 chunks maybe i can just send those 32 chunks to that guy and withhold the rest of the data so private random sampling is a very important model and i think this is not something that is part of any other blockchain architecture prior to this so we need to take special attention as you know data availability builders to this so i think that that goes back to the light client point that you mentioned whether they're able to sample correctly yeah or you want to add anything to that please uh what is the specific questions though this is the security and then a part of the security is private random sampling so that's how i'm sure so i guess you know this is the you know the first data availability sampling scheme proposed and the assumptions are pretty concretely laid out in the paper right you need a certain minimum number of nodes not necessarily light nodes but nodes in general to perform sample requests uh you need a synchronous communication in order to reconstruct the block if there's no single node that starts with the entire block and you have to reconstruct it from light nodes and you also need as 3ram has said you need under certain adversarial assumptions you need private uh sampling so that the attacker can't link together who is trying to sample because if they can then they can just trick that one particular node into believing that the block data is available when it's not and that's essentially i mean borrowing bugs in the code and stuff assuming that that's all correct and that's essentially the failure modes for the scheme so a certain number of light nodes is a pretty weak assumption because you know there's thousands of bitcoin nodes there's all 10 what 10 000 ethereum nodes there's even like something like six or ten thousand solana nodes right there's people will run nodes even if they're fairly expensive just purely because they want to have access to the chain you know so an assumption that there are a hundred light nodes sampling or 200 light nodes is like a really easy assumption to satisfy we can essentially guarantee that uh the synchrony assumption for uh being able to rebuild the block is something that you unfortunately can't avoid and this is one of the reasons why validity proof schemes aren't inherently better because yes you avoid a synchrony assumption for a fraud proof but you still need synchronous communication to rebuild the block anyways so you know if you're not actually if you're not fixing the worst case you know you still need that synchrony assumption so you don't really get too much out of validity proofs except potentially some lower latency happy path cases which is debatable uh and yeah that's that's about it those are essentially the failure mode so as you can see uh these are fairly weak assumptions which is why we tend to classify uh the process of data availability sampling as being trust minimized because these are fairly weak assumptions uh now in terms of in terms of private sampling uh this is something that i don't think any chain that's implementing data availability something has done yet uh mostly because this would require a fairly well resourced attacker but there's if you don't want to go through you know fairly advanced uh schemes like mixnets or whatever or tor there are other alternatives for instance you can have users run full nodes in multiple cloud service providers and unless every single one of those cloud service providers colludes to reveal that you know with the same user running those nodes a third-party observer can't actually connect that those nodes are actually run by the same person so you can even if you don't directly solve it through some advanced networking you can solve and practice fairly easily against an attacker that is just one person observing the network rather than literally every single cloud provider in the world all colluding together because that's a much stronger adversary which is much less practical thank you john uh matt do you want to address how ethereum is thinking about private random sampling right so in the first uh iteration of dink sharding their plan is to not do the private random sampling and the reason being is is that being able to quickly access random chunks of this data is actually a really difficult networking engineering problem to solve and so the assumption that we'll make is that with an honest majority we're going to be able to assign validators certain rows and columns that they'll go ahead and download and be able to you know with this mechanism you know propagate this information over the lib p2p gossip network in a lot more efficient manner than having to push things into a dht and then retrieve them but this does have the assumption if you are a client who is just watching the network you don't have a strong um you don't have a strong grasp of if that block is valid or if that block is available or not you're just trusting the honest majority of the network to tell you that block was available because the validator said it was available and this is where the private random sampling goes in where even if you have a legit a malicious majority of the network saying that a block is available you're able to do the samples yourself and with high confidence determine whether or not the block was actually available would you say in in bank shouting if somebody indeed wanted to do random sampling they could just join those particular peer-to-peer networks where those chunks are propagated and thus check you know whether that is available in those like peer-to-peer networks and then you know verify for themselves that the data is available is that something that you guys are planning for or so the way that the the the rows and columns will be encoded is not going to be as amenable to like the proper sampling structure that people want to do because you'll want to have more granularity when you do these random samples and the rows and columns won't provide that level of granularity but you will be able to connect to as many you know of the gossip channels that you want to download as many rows as you want okay so the this is basically a level of the level of granularity that you can achieve in the random sampling the peer-to-peer network and what goes on a certain peer-to-peer network dictates that granularity but it's still not everything you know as a light node you don't have to necessarily download everything is that correct correct okay um excellent so that's the first axis that we talked about is security and as a subset of security was uh the private random sampling which is a radically different networking model than what was needed before the next axis that we can think about is how do we think about the scaling ability of these systems how many nodes should sure to make it concrete let's say there are n nodes all of them have a certain bandwidth what is the uh net commitment rate or data availability rate that a system like yours can achieve is there some um heuristic idea on that yeah so uh we are still uh playing with those around but in devnet for example uh the configuration that we have set is like 2mb uh block sizes uh like 4 mb 2 mb like coded in so and and blocks block time yeah that's around at the devnet config is set at around 20 seconds at the moment ah but we are still you know kind of uh we still not optimized it too much we have just benchmarking we will do a lot of benchmarking um in general we are also in the future we will also do things like commitment generation uh optimization you know like so those kind of things are there but the current configuration is like 2 mb block sizes at 20 seconds that is that is what we think we'll experiment more in the upcoming test nets awesome so when you look at your avail network do you think of this as like a peer-to-peer network and you're doing both consensus and data availability or is it a pure data availability network so of course i mean the validators uh need to compute the commitments uh so that is certainly uh so we are using substrate stack for that which we have like grandpa babe for the actual commitment generation for example and at the moment the scheme is such that all validators compute the commitment uh individually as well so after the proposal proposes then all together on its majority consensus so everyone has to commit it but we are looking at certain alternatives in the future which might optimize that as well so then you know we can experiment with block times so yeah john uh so to clarify what's your question around the uh you know the big o uh complex throughput the data availability throughput that you can achieve using this kind of like an architecture is there a question around throughput and not the size of the commitment no not in the state of the country okay i must administer it my bad so the throughput i think i have to refresh numbers it might have been something like 1.4 megabytes per second uh i don't exactly remember but you know we're gonna have a respectably large block size in the order of several megabytes and a respectably long block time uh we aren't really aiming for a short block time the reason for this is that the longer block time the larger your block even if the throughput is the same but the larger the block the more you can take advantage of the fact that the size of the commitment isn't linear in the size of the block but rather is the square root of the size of the block and therefore you get more advantage like you have more scalability benefit if you have larger blocks and a longer block time than if you had really really tiny blocks and a short block time uh longer block times for the base network uh well a lot of people say you know you need really fast finality for the for the base network right you need 400 milliseconds or you need one second right uh but it turns out that as people have used have used rollups uh users are perfectly happy with the fast instant soft confirmations that roll up operators provide and as long as you're happy with that which is a perfectly fine assumption right as long as the rob operators are honest they will include the transactions they promised to include in the in that order then the base layer doesn't actually need blazing fast block times so you can have slightly longer block times and take better advantage of this square root data commitment scalability property oh matt do you want to comment on that right yeah so for the ethereum dink charting proposal the throughput will also be around 1.4 is megabytes some magic relationship i didn't want to say 1.3 because you had just said one point yeah so the throughput is supposed to be 32 megabytes for every 12 second slot and of the 32 megabytes 16 megabytes will be the erasure encoded data so the actual data that will be useful will be the 16 megabytes and so that works out to 1.3 megabytes so something interesting here is you are in bank sharding every node doesn't have to download all the megabytes per second you know each node can download much fewer at least the quorum that you're certifying is not every node downloading everything whereas in a celestia i understand at least as of now all nodes are downloading everything but both of you have the same bandwidth is there i'll let matt answer that first i have actually a lot to say about that but i'll let matt answer that first i didn't catch what you use no i'm just asking just clarifying first that in bank sharing every node if even though your total you know throughput is 1.3 megabyte per second each node each validator does not download data at like one 1.3 megabyte per second is that correct right exactly so in the first phase where we're not doing the random sampling the current parameters to do four uh samples from the rows and columns so you would download two rows and two columns and i forget exactly how much that is per validator but um that would be the current the current parameter and um after that yeah we need to look to see what the numbers is for like the exact uh row and column throughput awesome yeah so for uh for celestia we have kind of a few different like a few different ways to answer that question essentially which is that uh the first is we don't just have full nodes and light nodes and nothing in between where you either download all blocks like all the block data or you just do data availability sampling but we have a node in the middle called the partial node and these partial nodes rather than downloading the full blocks they can either download a subset of all blocks or they can download oh sorry they can download like entire blocks but a subset of all the blocks or they can download a subset of each block this is configurable up to the end user but these partial nodes still contribute to the security of the network because they can still produce fraud proofs and more importantly you know even if you eliminated fraud proofs and just had you know validity proofs you still need that worst case of being able to reconstruct the block and partial nodes contribute to that because they will do things like fully download either a full block or a full row or a full column or however you want to configure it there's an implementation detail so there's no type in the middle now with respect to our validator nodes if you're a validator the initial implementation will require validators to fully download the block and the reason for this is not that this is a fundamental thing in our technology because you know we use fraud groups or anything because again you still need a synchrony assumption for reconstructing the block and this is kind of the key is that by requiring validators specifically to download full blocks it means we avoid having to deal with the complexities of dealing with the synchrony assumption if you had validators sample blocks to determine their availability you totally could do this but then the networking and the assumptions around this and how tight your timing constraints are become much more complex to reason about because you have to worry about the case where the block is available if the whole network can reconstruct the block by communicating together and that is like a much harder process to reason about so i wish the ethereum researchers well on implementing this but subsequent implementations of celestia you know if these problems are ironed out will not require validators to uh fully down the blocks they'll allow validators to simply sample blocks for their availability yeah in the sense so our header structure is also um constructed in such a way that you know when we want to increase the block the commitments the number of commitments you know kind of increase in the block which is not a very uh it's not a linear increase in that sense right like so so we have not experimented with uh those at the moment but you know like once we get to more better testing conditions we will uh experiment with those second thing is we i mean like uh john said like you know we've also not from engineering we have not studied that but like we've experimented with the idea of columnar full nodes uh because uh you know like that is also another entity taking pla play you know part in the network and you know like that will also i think be more prevalent in the future that could be a scaling path um i mean basically you have validators and today you have letters and you have full nodes and you have the light lines and this is like another participant into the mix storing some of the data great um i think we're coming to the close of the panel so i want to switch into a broader question on all of you have taken very deep bets on this modular uh world and maybe the celestia team coined maybe the modular paradigm and polygon has a variety of projects building a whole stack and ethereum is shifted to a roll-up centric world i want to just get your outlook on where this paradigm is headed and what you think the uh interesting opportunities and challenges are so i think what we've learned a lot from our polygon journey is you know we have got the opportunity to talk to so many devs uh so many users uh and you know like what we have finally i mean so the broader polygon thesis is in general uh not um focusing on one particular um you know like uh so if we have a sheet of scaling solutions right like so we've we've invested in you know like zk uh evm's m3 in fact and uh you know like so we we believe that you know like uh in general developers want like different developers different users want a variety of different use cases so for example when you talk to enterprise clients right like so they want some some version of the blockchain you know with different properties they don't want us maybe they don't want to have the data on a shared chain for example they want some people want private ah data as well some people you know i mean so there's a variety of use cases enterprises have different you know like uh dabs are different there are like there's a wide variety in terms of you know what requirements is right like and so what we thought is you know like we should focus on getting uh uh customers or users or developers you know like their choice so if they want to run a roll up for example on ethereum uh that is their choice uh if they want to run validiums yup that is their choice if they want to run silver and app chains that is their choice right like so so yeah yeah it's not a one-size-fits-all um john yeah so uh you know throughout the year since we started building this has definitely been kind of a journey of learning uh when we first started a lot of people who first heard about the project didn't really understand what it was about and didn't really understand the use cases have be used and so on so it took quite a while to get to the point that we are now uh that you know there's a lot of excitement around this notion of modular blockchains and the modular blockchain stack uh yeah oh i guess i just i should talk about kind of you know from the from the early days when i was you know not exactly clear what applications we had to now when there's like much more uh concrete vision of applications so we have things like uh there are pos there are ways and mechanisms to use celestia for data availability and provide the service to roll ups on ethereum and we call these celestiums i gave a talk at eat denver earlier this year i think if anyone is interested in learning more and i think a colleague of mine evan gave one yesterday or the day before also uh here at etham devconnect and there's also the uh the notion of sabmos uh which i think mustafa talked about earlier uh where you have a settlement layer that runs a solvent roll up on top of celestia and then roll-ups that as they exist on ethereum can use this as a kind of drop-in replacement to ethereum and then finally we've really been pushing this notion that i really believe in is the end game for roll-ups this notion of sovereign roll-ups where it's roll-ups that run directly on top of a data layer and therefore have the ability to harden soft fork to change their consensus rules to change their block validity rules with off-chain governance which is not something that applications can do if they're built directly into an execution layer you know like gaps on ethereum or roll up smart contracts on ethereum uh they those to upgrade require something like a multi-sig or a dao some sort of on-chain governance and you know the entire you know the entire ethereum and bitcoin ethos is that on-chain governance is no bueno because on-chain governance can be captured by whales right so it's kind of weird that you have you know this entire notion that the the core blockchain layer can't be captured by whales it does often governance but then literally every single application on it does have on-chain governance because it needs to upgrade uh these sovereign drops are really i think the endgame of roll-ups where you can have the best of both worlds you have off-chain governance and you have shared security thanks so much um matt yeah so um for ethereum i think that there's just different choices that ethereum has to make being a chain that already exists in a chain that's focused on decentralization at the very core but what i am really excited about is that even though dink sharding has many things on the critical path to eventually reaching that end goal we're working on a eip-4844 called proto-dink sharding which hopefully will provide a lot of the facilities for rollups to start using what will exist at some point in the future which is basically there will exist some oracle to provide a route that says this data was available and as long as the rollups understand that this route was made available by the protocol then they can trust and they can build uh you know their chain based on that or if they're a zk roll up use that for their zk roll up to do synchronous communication um within that and so i think that with you know 444 which is actually very similar to what john proposed in 2019 with 2242 um i think that you know as things continue to change over the next couple years at least that we can start providing that facility that perm that primitive for rollups to build on this data availability layer that will start to exist in the next couple of years awesome i think that brings us to the conclusion of this panel i think it's the right question to finish to lead into the next one um thank you so much to the panelists for taking the time to be here uh anurag john and matt thank you 