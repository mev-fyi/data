at some level the storage requirements also go up with bandwidth required so like how do you how do you kind of combat that in the long run you care about that happening within a certain uh fixed time period right within an epoch because all proof of stake chains have an epoch if you go beyond the epoch bleak keys can generate a fork right so you have some assumptions about like within this fixed amount of time somebody's gonna say hey what the hopefully i mean it sounds to me like until he's relying on the assumption that moore's law will go on permanently which i think is an assumption that's like is not holding like it's started it's not really has not hold true for all types of hardware over the past few years uh it has so far over the last 20 years the amount of raw throughput compute wise that you can do on a chip uh push through a nick push through an ssd push through the gpu that's been doubling at a steady pace but not for the hard drive we're talking about is not cycles per second on a single core it's like raw bandwidth across like an array of uh you know like cores or dsps or whatever cuda threads i'm pretty sure it hasn't held true for hard drives if you if you look at the data but the other thing that's missing here i think it's like you're assuming that i think you're assuming that people like you you're ignoring the fact that people also have to sync this sync the chain from scratch like there's also like you need to be able you need to have people to be able to sync the chain and bootstrap from scratch yeah it's like 30 terabytes of uh transactions per year is what we're seeing right now 30 tb is not intractable i can and what's not going to increase you but you don't need the whole year right you need it within the epoch so what is it over like two days maybe 20 gigabytes or whatever is that like so fundamentally difficult that i could do it right and a very large number of users could do it so is that assumption like my point is is that assumption and that like hurdle to actually do this verification is it so much harder and such a big differentiator versus i have to have a light client constantly running plus i have to assume that there's an honest minority of light clients so that's the difference right like you can do that or you can rely on at least one out of end to provide you the data so the larger the network gets the more likely that one out of n is honest and this is where there is a clear difference between something like solana or tendermint where most tendermints run to like you know 150 or so validators solana is uh you know close to 2 000. so i think like we're we disagree on what is a large number of users like if we're talking about 38 terabytes a year like we're talking about like reaching the stage where like as a you as a user i have to effectively buy a whole bunch of hard drives you know i put it in my living room to verify the chain whereas what we're targeting is like you should be able to do it on your laptop you shouldn't have to buy specialized hardware just to do just just to have authentic just to be able to verify that the chain is valid if you're not you're participating in validation with an honest minority assumption which is different we thought this majority assumption is a much weaker assumption than this majority assumption but like like i said we're not talking about honest majority assumption we're talking about hardware costs and a one out of n assumption but so the user the user's trading and i totally own that it's trading it's the user's trading hardware costs plus relying on at least one out of n of the validators to provide them the data 