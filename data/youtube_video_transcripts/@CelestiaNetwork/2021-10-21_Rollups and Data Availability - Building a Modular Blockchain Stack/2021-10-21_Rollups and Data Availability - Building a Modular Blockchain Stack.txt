hey everyone uh please find a seat if you don't want to sit on the ground floor there's overflow seating up above also fred if you're here from off chain labs please come down put on a microphone uh you gotta put on your microphone welcome everyone um the title of this event is roll-ups and data availability building a modular blockchain stack and it's a joint event sponsored by first i want to feature maven 11 who was a seed investor in celestia and has kindly you know really helped with the organization of this event um i think you'll see baldur there's a bunch of them here matthias miguel i think i can't remember anyway um there's also celestia which you'll hear about from musafa fuel and off chain labs and starkware and the format of the event is that each speaker will present for 10 minutes about their individual project and then after that we'll have a 45-minute panel led by james prestwich and we'll have 30 minutes of the panel followed by 15 minutes of audience questions so stick around for the end and i think there will be a really interesting conversation so first i want to introduce mustafa albassam from celestia hello everyone uh yes my name is mustafa i'm the co-founder and ceo of celestia labs and i'm going to give you a brief introduction on celestia and what we're building so what is celestia um celestia is what we're describing as a plugable consensus and data availability layer and the key goal is to allow anyone to deploy their own blockchain very easily without the overhead of having to do deploy their own consensus network using something like proof of stake because they can plug in celestia as their consensus later now the kind of the key kind of key thesis here is and what this event is about is building a more modular blockchain stack if you look at traditional blockchains like ethereum for example okay so if you look at traditional blockchains or existing blockchains for example like ethereum have used the same kind of paradigm since 2008 that bitcoin has introduced which is that a big blockchain provides everything so you have an ethereum you have the consensus layer and the consensus layer provides everything it provides the execution environment which is the ethereum virtual machine it provides data availability and so on and so forth and you have the smart contracts which build on top of ethereum now the idea of a more modular blockchain stack is to decouple the consensus and execution layers so the idea is that a blockchain or layer on blockchain does not it can only be responsible for the consensus which is basically arriving on consensus on the ordering of transactions but the execution can happen on a different layer above the consensus layer and that's also a key intuition of rollups for example and above that execution layer you have the smart contract uh you have this actual applications which can be smart contracts and so the key idea of building a more modular blockchain stack is to decouple consensus and execution and we can think about why why do this and why does this make sense and we can draw a lot of parallels with how the web developed so if you look at if you look at the early 90s when the web was still forming and if you wanted to create a website you would typically host that website on let's say your university server or you might have a physical where you host that website on your computer or you might have a physical machine server you know in your in your workspace or your home or your company's office and so there was a higher barrier of entry to creating a website you had to have a physical you know machine but then you know in the 2000s this idea of you know shared web hosting came about you know geocities dream host bluehost you can upload your html code or php code using ftp or ssh and your website runs on the same server as everyone else's but obviously that was quite limiting because you're stuck with the same with whatever programming languages the um you know that service provider provided but then if you want to but then the modern web came about where virtual machines and the cloud became popular and that really gave you the best of both worlds because for the first time it gave you the advantages of having your own physical server without having the overhead of having to actually have a physical server and because you have virtual machines and you can deploy those virtual machines within seconds and have functionality equivalent having a physical server you know right next to you without all the overheads of having to maintain the physical server and you can draw a similar parallel to to to the blockchain space in web3 um we can see in 2008 um you know we had bitcoin which was a single purpose blockchain for cryptocurrency application then it has also other blockchains like namecoin litecoin you know and so on and so forth and then ethereum came about in 2014 and decided to provide you know a shared blockchain with a shared smart contract environment so that all of these different applications could use the same the same chain and yeah this was great because it allowed a new you know flurry of new decentralized application developers to very easily deploy you know blockchain-based decentralized applications but it was also very limited in terms of flexibility and scalability obviously does not make sense for every single smart contract to be running on the same world computer as every single other smart contract both for scalability and flexibility reasons and then if you look at what's happening now in the crypto ecosystem uh we're returning back this idea where applications should have their own blockchains um so for example you have cosmos political dot and roll ups because roll-ups are effectively blockchains um that into that you know deposit and withdraw assets to a parent chain like ethereum and that kind of similar to virtual machines that gives you the best of both worlds because you can effectively have your own blockchain but without having to maintain the consensus layer of that blockchain like for example if you play a roll-up on ethereum you know you're using ethereum for security to and what and what celestia provides is what we're building is a is the world's first general general purpose consensus and data availability to understand how that works if you look at traditional blockchains um in a normal traditional blockchain if you want to check that a block is valid then you have to do two things you first of all have to check that the block has consensus using the proof of stake or proof of work algorithm and secondly you have to actually process every single transaction in the block to make sure that the block is valid because if there's an invalid transaction in that block then the block has to be rejected but what we're saying at celestia is but let's we're building a blockchain where anything goes so you can post any message and there's no such thing as an invalid transaction so it's just a blockchain where you can post arbitrary messages onto it and so that that that means that if you want to check that a chain a block is valid then you only have to do two things you have to check that the block has consensus by using the proof of stake algorithm for example and then you have to check that all of the data in that block is actually available and has been published to the network but you don't actually have to care about what the data actually is or verify the data and that makes it much more scalable because to check that data is available and you can use a primitive called data availability proofs and the key idea of the availability proofs is that you can check that the entire block is available by only downloading some very small random pieces of that blog i'm not going to go too much in detail here but to provide some you know brief properties of this um with data availability proofs the the number of pieces in the block you have to download to check that block is valid and it's constant so if a block if the block size increases that does not increase the cost to validate that block but the way it works is that there has to be enough clients in the network such that they're downloading enough pieces from each block such that they can collectively reconstruct the entire block and so um that has two consequences like first of all to validate a block or to check that block has consensus and you can do that very quickly in subliminal time because you only because you don't have to process every transaction and secondly um the network the scales are scalable because the more users you add the bigger the block size you can have securely now the reason why this is interesting is because um if you look if you think about what's the world's most decentralized scalable protocol um it's effectively bit torrent because at one point bittorrent handled a quarter of the internet's traffic and the reason why it was so scalable is because obviously there's no execution in bittorrent it's just a protocol for sharing data and making data available so our key intuition is that if you simplify block verification to checking data availability then you can effectively achieve similar scalability properties to peer-to-peer file sharing property systems like bittorrent or ipfs um and so to build applications on top of celestia you effectively implement them as a roll-up and because the whole idea of a roll-up is that it's you have it's effectively its own blockchain that uses um a data evaluator to post its blocks in to and that's what the next three talks are going to be about how much time do you have so if you're interested in learning more then go to our website celestia.org and we're also hiring so you can check out the our angel page if you're interested in joining and we're looking for more go to lang engineers thank you everyone nice mustafa our next speaker is going to be brandon kite from fuel hey oh everyone is the right ah here we go hey everyone i'm brandon kite and i'm here with fueled apps to talk about fuel in the modular blockchain world so uh i got my start in blockchain um working at a project on disney in seattle and i've consulted on several blockchain projects over the years recently i was leading a protocol dev team at transparent systems which is a paul allen based company and now i'm leading the client in infrastructure development at fuel labs [Music] so fuel labs and fuel have been around for some time and actually shipped the first roll up to mainnet ethereum last year and uh one big kind of difference or differentiating factor fuels that it uses a utxo based model so fuel can be an execution layer two data availability layers like celestia um we're also targeting ethereum of course um but it could also be used in execution layer on top of like virtually any other blockchain that provides consensus so you know that could be polka dot or you know anything that has an evm basically um so you know [Music] mustafa already kind of covered all the main points about what makes a good data availability layer but essentially you know something that provides or has minimal execution overhead and uh has very high capacity storage because you know the more data we compose to it the more roles we can run more transactions the better so essentially just needs to provide consensus over the data for the roll-ups to function [Music] so what makes a good execution layer um so i kind of break this down into two categories one is kind of the lift and shift approach where you just want to speed up your existing layer 1 apps so it's taking like you know the evm putting it into roll up porting over your solidity apps and just running them and a roll up and then the other approach is kind of where we're going is a more layer two native approach where you have developers building your applications from the ground up for layer two to be optimized to take advantage of you know the best performance features you can and so one thing that we're doing with the utxo model is essentially maximizing parallelizability so that essentially allows us to kind of maximize our throughput compared to kind of more sequential solutions like evm so as an execution layer you know you get basically full ethereum style smart contracts uh you know but we get a bunch of extra features that we can support because we're not kind of tied to being compatible with the evm um and uh yeah it allows us to basically run a lot more faster and efficiently so the fuel vm uh has several like built-in extra functions so you know things like extra math operations um that you know can be efficiently verified on layer one um we support out of the box and built into the language uh you know so you know like some of these things like mem copy really expensive to implement in solidity but super efficient to verify from a fraud proof perspective so you know that's something that we can do very fast and efficiently in the field vm and that's a benefit to writing our own vm versus just being compatible with evm um and then you know there's other kind of advanced features we get where we can let people fully tap into the utxo capabilities such as like multi-user transactions [Music] so building on fuel is uh similar to working in rust so we have our own language uh called sway and uh the reason why we have our own language is because it uh you know if you're trying to write rust on a blockchain um you know you have the whole overhead and barrier of you know using web web assembly usually and dealing with no stood issues there and all of your dependencies making sure they're noticed and then you probably have a lot of macro garbage that has to be put on top of that to deal with all the blockchain specific primitives so rather than you know putting people into that macro world we decided to just build something that's basically identical to rest cuts out the things that aren't needed for writing smart contracts and so it simplifies some of the more complexities of rest but um allows you to like write uh you know basically smart contract native code with all of your api generation and tooling kind of built in so yeah we don't have the issues with like you know more than 16 local variables and all that kind of stuff because it's tuned for our vm um and for the you know contract sizes you know basically you can have them up to like several megabytes uh and not have an issue there so um yeah another the big thing is like you can have in-memory hash maps you know so essentially you can have this extensive standard library of functionality that people can just you know use like they're writing in a normal language basically and then to go along with that we have uh tooling so essentially rather than trying to be similar to um like or reusing pieces from the solidity ecosystem with like hard hat and all this um and then trying to use it for a specific layer too everything comes out of the box ready to go for writing your apps on our vm so you you're gonna have you know code formatting uh you know all the ide support you know autocomplete you know that kind of stuff uh debugging you know if you want to like fork the chain and debug at a certain point um that's all baked in and uh yeah so that's you know essentially trying to focus on building a good developer experience where it's all cohesively integrated together instead of kind of piecing together a bunch of different tools out there so that's pretty much it um if you want to follow us we're on twitter and all the major like you know we have a github we're launching our network pretty soon so we'll be open sourcing that and um if you're interested in like learning more about our project or working with us we're definitely hiring so [Music] [Applause] thanks brandon so next up we have fred lax from off chain labs who's going to talk to us about arbitrum [Music] thank you i'm fred nice to meet you all and i'll be talking about arbitrarium and l2 scaling with arbitrary and how it fits into this modular blockchain idea so i'm fred i'm a software engineer at often labs and a big question is what is arbitrary right uh we have arbitrary one which is our flagship chain right now which has been live in maine since i believe may this year for developers and open to end users for a bit over a month almost two and since then we have processed over 2 million transactions with over 200 different accounts interacting with the chain and a bunch of contracts deployed a couple that i heard are just for all different like devs uh deploys a lot of contracts sorry i went on a real tangent uh it's because i was hearing earlier that maker dow went on a weird rescue recently and deployed a lot of contracts on us so until maybe last week this number was around 11k and then it jumped to around 15k but that's a story for the drinks after a weird tangent but yeah and we spent 67 billion our gas which is how we account for computation on our layer two and yeah uh i mentioned that we are an optimistic roll-up that scales try attempts to scale ethereum and why do we say ethereum right we're talking about this modular blockchain idea and as mustafa was saying earlier we there's this idea of having a consensus chain which is the base chain where you make the data available and where you're able to send your send your assets to make either a deposit or a withdrawal and where you actually enforce the correctness of this layer 2 protocol 2 and we use ethereum and why do we use ethereum well there's security there's a wide pool of miners and validators block producers that enforce the chain security has generalized computation which actually allows us to write our proving system which means it allows us to prove what happens in arbitrary is actually correct so that you can enforce it and it has adoption the sad face shouldn't be an adoption should be a bit lower am i bad for that but by adoption it means there's a strong community there's a lot of users there's another a lot of knowledge that was built up about how ethereum works and how you can have a safe experience with ethereum being from auditors helping developers through best practices and just what patterns work and what patterns don't and this was something that was developed over a long time but there's a bad side right which is congestion and that's why we're here we're trying to alleviate the congestion and try to make this blockchain idea more scalable and yeah what's up with arbitram right we try to inherit the security from ethereum but we try to keep also this generalized computation so that means if you're able to express a contract on ethereum you're able to express it also in arbitrary and we wanted to go a step beyond not just general computation but actually keeping this evm compatibility and what that means is if your contracts work on ethereum on the layer one it also works on arbitrage and that means that not only we inherit the security but we inherit this this like body of knowledge of like how to write safe patterns and what works and what doesn't and that's something that we're committed to to keep this compatibility and to offer like scalability on top of that so why layer two right uh we want this idea of like uh inheriting the security from where you're based on and still being able to branch into different states so the idea is that you have this main ethereum chain and then you have a roll-up or different execution environments that kind of like branches off from it trying to be like we're kind of still the same but we branched off and we can come back and have different environments and this isn't precisely correct on a technical term but it's an interesting intuition around how it works and yeah uh we com we keep on talking about like we have this layer two like arbitrary virtual machine that can be proved but what's up with that when we say we have a virtual machine it's pretty much virtual machine in the traditional sense that takes inputs and gives you outputs but the special thing and one of the important things here is that this virtual machine is fully deterministic so it means that if you always put the same inputs in you always get the same output and yeah that's a key characteristic of our l2 virtual machine but we want to get this l2 virtual machine and have it inherits the security from the layer 1. so we can think about the layer one virtual machine the ethereum virtual machine as being side by side with our virtual machine and if we bring back the idea of the inputs is if we like lay it out a bit differently we can actually try to enforce the security to actually hold and how we do that is instead of you sending all your transactions all your inputs straight to arbitrarium what you do it is you send it to the consensus layer you send it to ethereum the data all the inputs get posted into ethereum and from there anyone is able to read that and actually execute on their own l2r between virtual machine and everyone will have the same output it's a fully deterministic virtual machine you know the set of inputs you execute it and you get an output but yeah and the idea is that anyone can get this output but you still need to prove it back to the layer one right so you have the data available anyone can know what the result is the challenge is how do we convince ethereum itself that this is the results and this is where our protocol comes in in which uh the optimistic roll-up parts uh proves this and i'll get into that a bit later but yeah i blast over a very important thing here which is the sequencer which i won't get into very much but i need to at least mention it that is in this layout there is one level in direction with the product we have shipped which is the sequencer which has one superpower that it orders transactions for the users so it lets you you as a user have a much fast much faster ux and experience when interacting with the chain so what it does is it gives you soft confirmations in terms of like the ordering and then you get a harder confirmation when it actually goes through the l1 chain so what i mean by that is before we have the inputs going straight to the l1 to ethereum or whichever consensus layer and now we actually have this interaction of the sequencer and the cool thing is that you can have these fast soft confirmations but they can never actually steal your funds they can just front run you and extract value which can be very bad as well but it's limited in that sense and yeah but so i was mentioning before right uh roll ups uh we're roll up and a key thing that we need is this date availability which is uh when you're talking about a rollup we're getting many transactions together all those inputs we roll them into one single bing transaction and we include it to the layer one ethereum and then the optimistic part of it is how we prove it back which as you hear later there are other ways that people do it for example star prayer will be talking about their zero knowledge proofs and how they'll actually also leverage that and yeah uh i have quite a few con quite a bit of content on the slide so i'll power through because i think i'm getting close to the time limit but yeah optimistic roll-ups is the idea of like how we prove it back and we take an optimistic approach which means whenever you have an output of executing on this l2 virtual machine we let anyone assert and be like hey this is the output of execution this is what i believe is the result and you do that but you also put a stake along with it being like i'm willing to put my money and if you prove that i'm wrong you can take my money or part of it and we optimistically accept these assertions like people saying this is the outputs which they are saying and we have this notion of fraud proofs which means since all the data comes as the inputs anyone can execute and see the difference on the outputs and and then you play a bisection game on the layer one ethereum being like we disagree on these outputs let's see where we actually disagree on this and the way we do it uh in a nutshell it's kind of like a binary search in which you have all the options that were executed on the l2 virtual machine and we asked was it on the first half or the second half and then we keep going on until we narrow down to a single op code of execution and then you actually prove that on the chain itself by executing that and since all the data is actually already there on the chain anyone can do this uh right now we don't have this fully permissioned set yet uh since we're in beta but this is part of the design of the system and how the protocol can be enforced but yeah what's actually there and what's deployed for arbitrary so we have this proving system that allows you to prove the l2 virtual machine on the l1 we have the roll-up part of things which means when you're rolling up transactions and having the game between different outputs being disputed by people that's the logic that's there and we have the layer 2 vm and yeah here's a disclaimer we're in beta these systems are live are functioning but they're upgradable and for for a while we're going to keep these upgrade keys uh but we have a route towards permissionless at permissionless system and decentralization which you can read on our blog posts yeah but so i said what's there and the interesting question is what's next right uh we've recently announced arbitrary nitro which is just changing a certain parts of the system as i said before just like before we have the prover the rollup and the vm what we're doing here is we're changing the virtual machine that's actually running on the l2 we're going to have like a wasm a virtual machine and a proving system that is able to prove why some execution and what happens in the wasm execution is fully verifiable on the l1 chain itself and a big question that we get is well if you're gonna have this big upgrades should we wait until we deploy so once you do that upgrade and the answer is no you can deploy right now it's gonna be a seamless upgrade that will go live on the network and users won't have to migrate we will maintain the evm compatibility and your contracts will continue working fine yeah two other big things that are on our roadmap are ethereum two because as mustafa was saying earlier these data availability shards and just having like uh the capacity to handle more data on whatever your consensus layer is always benefits roll ups making the cost more cheaper and that's a big thing on our roadmap and the other one's arbitrary channels which is what happens when you get this idea of data availability and you make certain specific relaxations which is for example instead of posting all the data on the layer one what if you trust someone who actually give you this data and it's a trusted party and you trust that reduces the cost of the system a lot and it works very well for certain use cases and that's a very interesting flavor of roll-up well wouldn't technically call it a roll-up but that's a very interesting flavor of tech to play around with it's good but yeah if you want to learn more uh now he regrets not putting the actual links instead of hyperlinks but you can check out our documentation discard our twitter or just talk to me later on in the event we're also hiring that's our hiring page this one actually got right if you want to type it in and yeah thank you for coming that's my twitter and i guess we don't really have time for questions but later on after the panel thank you fred super interesting and our last speaker is avihu from star queer here he is [Music] and then after this we're going to have the panel hi everyone can you hear me will okay so hi i'm a view i don't have slides except this one single slide with my well skinnier version of me um but i do want to try and speak a couple of minutes about starknet um just before i get to starknet i do want to mention what we did in starkware so far so we started in starkware by developing a scalability engine that we call starcx starcx is basically well it based on validity proofs it it based on generating and running stark proofs and it works for a specific operator and specific application and fast forward we deployed the first version on mainnet i think more than a year and a half ago and fast forward for today there are a bunch of applications running on it with almost one billion dollars locked in the contracts and something between few hundred thousand to a million transactions per day so it works pretty pretty good it works in a very high tps and it support application like dydx immutable diversify and sewer but at some point we realize that we want to get to something that is bigger and by bigger i mean it's more general it supports um all the nice features that you're used to from ethereum like composability and the ability to um to split logic to contracts to have the same structure in contract like um like functions and this is where we created starknet so stagnant is basically well as the part of the as net in the name says it's a network and the idea the idea in stark net is that we will get to the same or high scale of zika roll-ups but with the ability to write general computation in a way that is very very convenient and and very easy and still get the scale that that we all wish to achieve in as a ziki rollup that is letter to on ethereum um i do want to explain how i i think the easiest way for me to explain in what way you get scale in starknet is uh the following so i think of it as a as i think of starknet as an extension in two different dimensions those i mentioned are computation and what i call weakness or basically the data that you need for the computation so why do why do i want to point on these two specific uh things well because startnet is equivalent is a very important advantage there in every other solution that is not based on validity proofs every node in the network eventually have to re-execute the computation and because it has to re-execute the computation it has to know the data that is needed for the computation so every time that you have a heavy computation or that you rely on a lot of data to execute it in other any other solution is going to cost a lot but in starchnet there is just a single entity at the end of the day is the sequencer that responsible on the existing on the current block that takes all the computation and generate a single proof for all other for all the other nodes in the network so now those nodes only need to verify the computation they don't need to re-execute everything and i think this opens the door for a variety of new applications that we haven't seen today on ethereum and i can dig like way more than 10 minutes on examples but i do want to point on just a single example i actually chat right before the the the talks i chat with um one guy in the audience here and he told me that he is implementing a primitive that is called storage proofs on starknet so i just want to give storage proof as an example uh on why where is the advantage in scale of stark net lays um so think about the following let's let's talk about voting for a second in voting you basically want to know in a specific point in time what is the balance of every user and you want to make sure that every user that wanted to vote he voted and he signed on his vote right so if you have like 1000 people that wish to vote on ethereum is going to be almost impossible to verify if it's going to happen each and every single time what you basically would like to do is you would like to take the state of ethereum prove it and prove that you basically have um i don't know for every person who voted it has some specific balance and still be able to run the entire computation of the votes and verify that everyone has this balance so for storage proof for example one way to do it is to take a lot of data in the form of miracle passes and verify them on ethereum or verify them on starknet if you verify them on ethereum or on any other solution it's going to cost a lot of data and it's going to cost a lot of computation to do it if you verify it on starknet you don't need to relate this data to any other node and you don't need to um to recompute everything in every node so every basically every application that is computational heavy or requires a lot of data is something that is very interesting to try and create in stagnant because it wasn't possible up until start net and on any other scalability solution that isn't a zika roll-up we have a very active i think community of developers and if you are interested in coming and building any any type of application a new application something that wasn't possible to be to them any other solution then you are more than welcome to join our discord and try and build your application on starting thank you thank you avihu and now we're going to move to the panel portion of the event and so i want to introduce james prestwich who is a cross chain engineer who is going to be leading the panel and just give us a few minutes as we set up the stools so hang in there and after this we're going to have more food and drinks upstairs let's let's try with them lapel mics and see if did you get one okay well here you just use this i seem to be missing someone okay let's see we good everyone's good it's good to see you all good uh so quick funny story is uh i ran into fred on the street yesterday and did not recognize him uh and he just told me right now that that was him and now i have to be super embarrassed about it for the rest of the panel um uh i thanks you all for coming out uh we have about 45 minutes for the panel 30 of which will be you know question prepared questions conversation with panelists and then we'll open up for audience questions for the last 15 minutes if i stumble over my words because i'm still extremely jet-lagged i just got in two days ago and i believe everyone here is already given an introduction of themselves as part of the presentations just now yeah so we can go ahead and skip that part of the panel and dive right into questions so we've all been talking about you know our tech and how it works presentations uh but the the name of the event is building a modular blockchain stack so i want to give each panelist a minute to talk about uh what does it mean to build a modular blockchain stack um speak up a little bit i can do this all right uh i want to give each panelist a minute to talk about what it means to build a modular blockchain stack uh mustafa do you want to start sure so um well what what me what it means to build a blockchain stack is to take all of the components that make a blockchain separate them out into different components and make and connect and make those components connected to each other and be in a way such that you can replace each component and the main advantage of that is intuitively if you make each component replaceable then there's a lot more flexibility in what you can make those components are and so that unlocks a lot of new potential innovation and like i say if you make the execution part of the stack uh replaceable you get a lot a lot of uh a lot of new innovation in what the execution stack is like we've seen with roll ups for example but also outside of the ethereum ecosystem you can have different execution environments that don't have to be evm compatible like you have wasm or ras based programming languages and so on and so forth yeah i think mustafa expressed it pretty well and another thing is for example if you try to solve all the problems you need to solve for execution and for consensus on the same part you end up being much more limited in terms of what you want to do because of the trade-offs that come along with that so when you actually separate them and you're clear about the boundaries of where each one goes and what are the requirements and the limitations you can actually make more explicit trade-offs that end up like i guess you would know best but i the trade-offs you can make are much better because for example if you want the execution to be something that is highly efficient and paralyzable in that sense it's something you can do and also just the flexibility of having many different like flavors that go on top of this shared security layer is very like desirable and it allows us as like just builders to experiment with different ideas without having to actually make this trade off of having to make for example a different chain with independence uh with independent consensus and independent security that actually allows us to these experimentations with in a higher security environment yeah i mean oh sorry that was a little loud um yeah just building on what you guys said i think um you know the separation of concerns between data availability and execution just allows the execution layer to focus on you know what's most important to processing things as fast as possible you know you don't need like a team full of blockchain experts who know a lot about consensus to just build a virtual machine implementation that you know is very optimized for you know running on native hardware or you know taking advantage of what computers can do so i think that abstraction is definitely a powerful one that's going to enable a lot of new use cases thanks and i think for us the most important part to to build in a modular way is actually the data availability part um when it comes to execution we don't have the exact same issue with all nodes needs to run execution because in any case it's more important for us to optimize just a single machine and everyone else can just verify but on data availability side it's very important for us to offer not just a single layer of data availability but probably more than a single solution with possibly various trade-offs because we understand it's important for some applications to choose their data availability not necessarily be with the same security or the same cost as other applications so i think that's like modular part that is important uh so it sounds like we've identified data availability and execution is the main parts that you know people are working on people are making modular right now are there any other parts of the blockchain stack that should be modular that should be swapped and replaced um yeah so i guess i already spoke about it for us the the main focus is the data availability layer i think yeah i i can talk about which part i think that shouldn't be a modular but maybe i'll keep it for later yeah i mean i guess you know from the uh execution layer side i wouldn't say that there's a ton of modularity because it's already very modular on top of you know the data availability layer you can just like like you said try out a new thing experiment but on the data vility side i think there is value in keeping modularity there in terms of you know maybe you want to upgrade your consensus protocol or you know change different features about how the network functions and works over time or just have different flavors of data availability networks that have different kinds of properties that are you know have different trade-offs like for example um you know what kind of finality do you want and you know how does that impact the roll-ups when the clients are trying to decide you know what happens when a fork occurs and you know dealing with all those problems so yeah well there's one point that i kind of hinted at during my presentation which is for example the change we're doing for nitro right now for arbitrary nitro which is i wouldn't say necessarily modular blockchain but modular components that make our layer 2 blockchain so for example we have this idea of a virtual machine how to prove this virtual machine and the role of protocol that helps anyone challenge and do for example the bisection and to get to an actual execution proof or the the fraud groups in part and having this part of the protocol and actually the optimistic role of be modular offers a lot of flexibility because you're able to have for example a lot of there's a lot of codes that we are not going to change what we actually need to change is the vm running on the l2 and how you prove the op codes on the l1 everything in between this is still the same and it's very useful to to be able to like for example plug different vms and different flavors and everything that comes from that so one thing that is modularized in practice that we haven't discussed so far is this um idea of dispute resolution for the roll-up and so if you look at ethereum roll-ups right now all ethereum roll-ups don't only use ethereum for consensus and data availability but all ethereum roll-ups also enforce a mandatory cross-chain bridge with ethereum but our review or the celestial view of rollups is kind of more flexible than that you can even have a roll-up that is a standalone chain that does not necessarily need to be bridged to another chain but can still use a data availability like celestia or ethereum and this is particularly relevant in um you know the cosmos ecosystem for example the idea of cosmos is that there isn't a single you know there's a cosmos hub but there isn't a single parent chain like the beacon chain like ethereum but you can have like an internet of blockchains that have where the cross-chain bridges are kind of like a network and so what one project we're working on is we're trying to make it possible to deploy cosmos zones as roll-ups so you can deploy a cosmo zone as easily as you can deploy you know a smart contract or roll up on your ethereum and you can and your cosmos zone can connect to any other cosmos zone using a roll-up based bridge that you choose to that supports that yeah i think this point of bridges is very interesting and something that is different on rollups especially the ethereum flavored rollups is that the way the bridge works is different from the traditional sense of when a user hears like i'm bridging my assets and that's because it's actually a protocol enforced bridge that means when you're sending your assets from l1 to an l2 it's it's not some cryptoeconomic incentive standalone to the bridge it's actually if the protocol itself holds the messaging between these this bridge actually holds as well and actually having this characteristic in a non-roll-up world is is very hard to maintain you you have different ways you can set up protocols but for example i guess optics is a very interesting example to this to put you on the spot i'm sorry i'm trying to avoid talking about myself sorry then i'll shift it back here and yeah i guess cosmos does have ibc for communication and all that but at least in the ethereum style flavor of rollups i don't think ibc actually translates what we actually want for bridges and yeah i'm curious to hear your take yeah to clarify when i mean roll up bridges i mean roller bridges i don't mean trusted abc bridges like we're working on editing we're working on adding ibc sorry working adding off on fraud and led proofs ibc so you can use you would be able to use ibc to connect um a rollup to another chain which could be a rollup um using rollup base thrust assumptions so like the roll-up does not have to be connected to some parent chain like ethereum a roll-up could be even connected to the other roll-up and that roll-up would be connected to the roll-up um it without without these um on this majority trust assumptions and i talk about this a little bit more in a blog post um i posted recently about something the idea of clusters where you can have clusters of chain and you can communicate between those the chains within the cluster in a trust minimized way it's like ethereum and all of its rollups are one cluster for example i i think um you know talking about rollups and ibc and cosmos zones and smart contracts brings up an interesting question which is in these modular systems who's writing the module and what does one of these modules look like is it an iv is it a cosmos zone is it a roll-up is it a smart contract like who is writing these and what do they do um i can't go unless anyone else okay so i think that question kind of goes to like what kind of apis do these like modular components have uh because it's all well and good seeing that the stack is modular but for it to be truly modular you need to be able to replace the components in those stacks and for that to be possible you need to agree on some kind of shared api or shared interfaces so that you can replace the different clippers in the stack at the moment i don't think we have a shared we don't have a shared interface at the moment different communities have consensus on interfaces like for example the cosmos community has this ab something called abci um application blockchain can be i forgot what the c stands for uh interface um and that does that decouples are already the couple's consensus and the state machine so you have like the abci client which i if i remember correctly is is the consensus and then the abci server which is the state machine and we're using that because that was already a very natural separation between consensus and execution and on ethereum i guess that's not so much the case because you use ethereum anyway so if you wanted to use ethereum roll up on a different database layer you have to kind of create interfaces around that um yeah so the question was about like the who's writing the you know these modules or you know okay so you know i mean there's obviously like a stack here right so there's you know essentially the protocol developers writing you know the roll up themselves how the fraud proofs work the clients you know all that kind of stuff in the vm um and so that's typically like the layer 2 protocol team and then you're going to have the actual application developers who just you know you know i mean both in you know most layer twos you have some kind of contract oriented language similar to ethereum you know where people can write their apps and they don't need to like build their own blockchain basically they just you know write their their code for their application um yeah i think for us one interesting place to look if if we talk about data availability i think we will start in stark net in a completely zika roll-up mode where all data goes to layer one and you always every time you change a state in r2 you pay the price in uh the layer one which is a theorem data layer and we feel that it's not ideal like we feel that at some point we want to let other option of data layers to be activated so i don't think it's in the level of the application or the developers of the smart contract but i do think and hope that the protocol will offer by the time a different layer of data that the consensus can be run just on the data and it offered by the protocol designers and at some point after that i think that we have the the ability and there are some interesting design around enabling other entities to write their own model of data availability that is not necessarily part of the protocol or part of the layer one that the roll-up run on top but it's a completely different layer but as long as users and applications are convenient with using this layer and it doesn't break the protocol then we can we can we would like to edit as well i think with time these lines are going to blur a bit more to be honest i i see what all of you are saying but at the same time if we're building like modular parts it's still they're going to be repurposed for other things and i think a great example is like what we're doing with wasm and for upgram nitro we're actually building a proving system for wasm and even though you won't see application developers necessarily interacting with this on a protocol level for arbitrary itself i can see many like application developers leveraging these modular uh aspects of modular blockchains for other things so for example if you have the ability to actually prove a wasm code on the layer one uh that opens the door for a whole lot of other applications that can leverage that so you start seeing a blur's line of like application devs actually leveraging these protocol tools for and repurposing them for different things um interesting uh so it sounds like right now almost all of this work on modularity is being done by the teams you know involved by starkware or by fuel or arbitrary more by celestia is that about right what do you mean by the modular when we talk about like building a modular blockchain stack we're each working on a system that has modular components and there are relatively few other people contributing modules right now well i would see it more as like we are the modules that are contributing to the border like modular stack rather than we're building something itself that is modular if that makes sense interesting uh so each of us is trying to build the module that uh takes up that slot in the stack yeah that's that's how i see it interesting um what we're kind of talking about what we're building concretely are there any parts of your stacks that are not modular that would be extremely hard to replace or to substitute another technical system for so celestia um we describes us we describe ourselves as a consensus and data available player um now in theory you know you could potentially decouple consensus and date availability as well um and some projects like software for example have gone somewhat down this route so you can you know start with stock for example you don't have to use um ethereum as availability layer you can use a volition which is kind of like a trusted data availability oracle but that doesn't have consensus and the reason why we're not decoupling data availability of consensus is because if you just have a data availability layer without consensus it's not censorship resistant unless you have multiple data availability layers that people can kind of use maybe in a round-robin kind of way but we're not there yet um the problem with making the stack more and more modular is that um you start to introduce more complexity that has that and you need to have developer tooling to address that complexity like if you look for example like 10 years ago it would it would never have been practical to do what we're doing today which is decoupling consensus from execution simply because like there's a lot of work to be done in making role like developing roll-ups and tooling around that like this even today there's so many teams working on roll-ups to make them practical and usable and they're only possible today because of that but like for example five years ago or ten years ago it might not have been as easy as possible uh yeah so i mean specifically for fuel um i'd say some of the things that aren't very modularized are specifically things around like our you know vm op codes and kind of our transaction data model because those are the kind of the core decisions we made to be able to achieve the you know throughput and scalability and parallelism that we want so you know be hard to just be like oh you can use utxos or accounts right i mean you can write a layer or something like accounts on top of it but just to like swap something out at that core of a level it doesn't really make sense because that's kind of what makes you know the whole product work the way it does so um yeah i mean i think outside of that you know there's lots of other things you can make modularized like you know for example we're using you know our own language but you know it just compiles to a byte code for a vm so you could use other languages that's fine but yeah do you have anything yeah i think for us one thing that is not easily going to be modular is the well the execution layer in particular cairo um eventually we optimize uh not the re-execution but the generation of the single proof so while i can think of a verifier that knows to read different languages or even different proof systems i think it doesn't make sense at this point so it's probably going to stay in variant so yeah that's that's i think one piece that is going to stay for a long way i think for me maybe invariants would be a very strong word to use but i'd say the core module for arbitrary at least right now is this idea of the interactive prod proofs and what it allows to do especially like the evm whenever you need to prove something on top of not necessarily the evm but another vm you always have limitations and it allows you to put a us to put a upper bound on top of the the work that goes into doing all that and i love our secret sauce and what actually makes arbitrary work and be efficient comes from that i believe which like a lot of our approach when even thinking of the theoretical sign behind actually came from starting with this driving principle of idea of like how can we prove this back and prove this efficiently and the other modules around this uh are somewhat like interchangeable but i'd say that's the core so uh it sounds like for you know three of you the answer is the execution model is uh the vm or the proving system or cairo that is kind of driving the execution of smart contracts i guess uh mustafa building a data availability level is that layer is in a bit of a different position so for you know the people building roll-ups why did you choose this specific piece of technology to build around why choose the multi-layer the interactive prover or the fuel vm or starks specifically so i guess the interactive fraud proofs have a couple of different benefits but the idea is that for example an alternative would be just re-execution on the l1 for example but that comes with limitations for example you would be bound by whatever you can do within a span of a single transaction when you have interactive fraud proofs you actually can put that along the span of a couple transactions or various and you narrow down and this goes back to what i was seeing before that you have predictable amount of work of what you do to actually create these proofs and yeah i'd say that's one of the things there's quite a bit that goes into this but actually we did a very good like deep dive around interactive fraud proofs that express it much better than i can now if you look for it on on youtube i think it's also linked to in our documentation yeah so with the the field vm you know we have uh no global state lock between each transaction so you know essentially the more capacity we get from the data layer the more we can you know post to it um assuming there's more cores so you know that's something that you know really what makes is the motivation for building the fuel vm the way we did as well is also training it to do you know the verification games so you know changing making that modularized would be probably more overhead than it's worth in terms of the trade-offs because that's you know i don't know if people would really want that they want whatever is going to be the fastest so you know um you know you can add other layers on top to abstract those details from people but at the core level you know that's what you need to have to make make the products good yeah i think for us the main question is is about cairo like why did you chose cairo and like cairo in particular and um so i i i want to say that cairo is something that we started to develop a while ago and it's actually went into production with stark x already so it's already running on minutes for a while and we've learned a lot from that and building a language that is capable of like perform and still uh help you generate proofs for any computation is not something simple and in in all kind of levels so cairo is something we invested a lot of time in and it paid off in the speed that now you could we could we can develop stark net because darkness then eventually is just implementing an operation system with cairo um so i think replacing cairo will be extremely extremely hard at this point and also we invested a lot of time making sure that cairo is secure safe and efficient and this is why this is this is why starting is based on this language and there will be obviously support in all all kind of other languages but they're going to be probably less efficient if they would be in the core okay and one more question before we move on to a new topic mustafa you mentioned earlier that it is you know extremely difficult or impossible to separate consensus and data availability while maintaining censorship resistance uh are there any other like interesting uh difficulties in modularizing the stack or other places where it really you know doesn't want to become unstuck like that um i mean that's that's as far as you can modularize it i can think of but one like okay one other component is this idea of the compulsibility and like i think the more you modularize the stack the more difficult it is to for different applications to communicate and effectively like there's a trade-off between scalability um and compressibility like we see this even in an ethereum ethereum right um with ethereum roll-ups ethereum roll-ups are composible with each other but they're not they're not as easily composable as just having every smart contract on the same blockchain you can just do a cross-contract call and so there's always this trade-off between compressibility and modularity and the other thing is um i recently wrote a blog post about this idea of clusters and as you said in order to for two chains or two roll ups or to have systems to communicate with each other um in a trust minimized way without trusting like some custom committee you have to implement a roll-up based bridge but to do that those the chains there's two chains you're committing communicating with have to understand the state transition system of each other to able to verify the fraud proofs or validity proofs of each other but then if you want to create a different kind of execution environment that is not compatible with the execution environment of those two chains you cannot communicate between that chain and those two chains in a trust minimized way you instead have to use a committee based bridge which is less secure and so the way that i envision this ecosystem looking like is a cluster is like a cluster of chains where each in each cluster the chains communicate with each other in a trust minimized way but you can but clusters communicate between each other in a trusted trusted way and that's totally okay the important thing is to make sure that we maximize the scalability of each cluster before spinning up too many clusters um so we are coming up to the end of our time the one like quick thing that i want to talk about before we go to audience questions is you know the idea of data availability uh three years ago no one was talking about this now you see it everywhere and it's considered you know incredibly important to protocol design uh do we have like a brief narrative of how we got from there to here um well in the theory well the history of like the importance of that availability in blockchains and it goes back to i think at least 2014. there was like there was you know emails on the bitcoin mailing list of people discussing you know what is a blockchain like what really is a blockchain what is it what you know the people were arguing this and you know peter todd argued that fundamentally at its core a blockchain is something which he referred to as a proof of publication system which is like if you post a message you can prove that you've published it to the internet and that's what we mean by data availability in today's terms but it was only the idea of that availability and became popular when in in in ethereum scaling research communities and that availability was important in sharding because if you want to if you want to chart the blockchain each shard has to effectively make sure that the data of each other shard has actually been made available and the reason for that is to make sure that you can process the fraud proofs of each shot but in today's but in today's um world um to give you some context before before we had roll ups we had a different kind of layer two um kind of strategy called plasma and plasma was this side chain technology that was very very similar to roll ups except that the data was not posted on chain and it was just controlled by that whether by the operators of the um chain of the side chain but the problem is that if the side chain operators don't publish the data then the users of the side chain are screwed because they can't prove that anything bad happened and then for years and years people try to come up with a solution to this and ultimately people realize that any solution is just to post the data on chain and that's why that's why we have roll ups yeah i think you covered it pretty well uh and the idea of plasma i think it came with two main problems right uh one was the mass exit schemes that happens that when whenever someone thinks that this coordinator is actually withholding the data the incentive incentive is for everyone just trying to run away together and i believe the other one is fisherman's dilemma i think that's the name right there's there's a nice name exactly that is the idea that if if this person is actually being malicious and withholding the data from you well me as a user what can i do right i can ask for him the data he's not going to send to me what do i resort to i actually post it i need to act on the chain and this means that the cost of like this the coordinator being malicious falls on the user and that's very undesirable and this way like with roll-ups the idea is like we just take that out of the equation and the data is already there so like for example when you're designing a protocol you can always make this assumption of like the data is available and people can act on it and that just simplifies from there on whatever you're designing yeah i mean coming from you know originally like truebit and all that kind of verification game stuff uh you know i think people kind of realized what we really need other consensus is just ordering of the data and you know consensus over that it exists um so you know with that basically you know it solves a lot of the problems with um you know having data off chain so you know i think it's been sort of in the works for a while but yeah it's nice to see everyone you know making it happen so yeah um yeah i think for me the one important point for when data availability became very interesting so when we started to build star kicks we were actually working on it i think at the beginning with 0x and for me it was clear that you know for a long while using layer 1 data would be good enough and it turns out that it's not the case like star kicks showed it in with 0x and then with other systems that run on top of it people were just like they wouldn't accept the level one cost or the level one linear cost as this solution for data and for me this was like i don't know exactly what was at this point in time but somewhere like two and a half years ago that that we realized that this is a problem that we need to solve and it cannot just rely on le 1 forever and this is hence everything that happened since in stark x and everything that we plan for stark net um i think it's interesting that you know so many different threads of research converged at just about the same place um anyways i'd like to open it up to audience questions uh nick do you want to handle the microphone for that or all right guys now is your chance to ask a question um yeah go ahead or if you want to shout or i'll hand it to you oh and also for those who are watching on the live stream um feel free to drop your questions on the chat okay i have a question for celestia specifically i hope it's not too stupid do you have a token and if you don't have a token how do you ensure that the data on the data availability layer is not spam we plan to have a token for our proof of stake vertical so because celestia has a con is a consensus layer and for that we're using tendermint proof of stake and and so we do have a feed market so there will be a block size limit and so to prevent and so if the if people start spamming blocks the fee market will kick in and um spam will become economically you know impractical effectively the same as same it's the same the os resistance story as ethereum or bitcoin okay so a very small follow-up to that does that mean that the core data availability layer of celestia also has to process transfer transactions for the token the call yeah the core you know the main chain of celestia has to process transactions that pay the transaction fee um for applications that post data on celestia but it's not so it's not necessarily the case that it's like one transaction has to be processed for each one for each roll-up transaction it's more the case that you have to process one rollup transaction for each roll-up block right so the so it's more like the more applications you have the more transactions you have basically rather than the more transactions you have the more on the roll-ups the more transactions you have on the main chain if that makes sense cool thank you more questions hey i was just curious is there a future in which we will see all of you guys working together in a sense that i don't know maybe stockholm will become an execution engine for celestia and arbitrarium will provide the uh throughout proof system uh of sodes right because right now it just seems to me that yeah i mean fuel is working with celestia obviously but all the other projects are trying to do more or less the same thing but separately yeah so there's definitely a future that's possible where you know we're we're talking and working with fuel labs fuel labs has plans to use celestia as data of the boat layer um like stock celestia would also be very natural um alternative to volitions um because the evolutions already have trust assumptions and if you can if you use celestia as a evolution that's even better from a security perspective now the problem the problem with using celestia as a data availability layer um for ethereum rollup specifically is that ethereum does not support off-chain data availability proofs um so you have to basically use a committee-based assumption where you check that like the the celestial consensus has signed off on the block and therefore is available so technically it's off-chain that of off-chain and that availability because ethereum does not currently support off-chain data availability proofs but that being said like you could use celestia and for example in stock where context as a volition or in other contexts for as an offshoring data availability in the case would it be kind of like a celestial light client as the oracle for the state availability yeah i think one other interesting direction that i don't know if exactly that's what bartek meant with this question is that like you can also think of data availability [Music] fraud proofs can that can be replaced with data availability proofs so let's just think another topic where some collaboration can bring to potentially improvement on on both sides and we definitely think that other data availability solutions are interesting for us cool if you're on the top row by the way and you want to ask a question if you could just walk down so i can see you we have one over here just to follow up on what you just said that ethereum does not support off chain availability proofs so does that mean that um it's not so you could not build a rollup which does not basically write write the data required for the fraud proofs on ethereum so it's not enough to just have the data but you actually have to have the data on ethereum mainnet is that what you mean yeah so if you want to build a roll-up with the security properties of a roll-up um which means that you don't have to trust the roll-up operator or any kind of like committee and you want to completely rely on and inherit the security of the of some main chain like ethereum then you have to post the data on on ethereum the problem is is that if he puts a data on an of chain on the third party of a build layer um it becomes like plasma it becomes like it is basically a plasma rather than roll up or in the case of a zk roll up it will become a validium which is fine because the the core problem is that um ethereum cannot verify that data is available on some third-party chain without relying on a committee [Music] because in order to verify that data is available on a third-party chain you have to use something called data availability proofs but data availability proves that you can't implement you can't verify them in like inherently within the smart contract because they require some interactivity that just aren't possible um within like a a general purpose execution environment like a smart contract so if you wanted to if if you want to implement if you wanted if ethereum wanted to support the verification of third-party chains using data availability proofs and we have to effectively fork ethereum to add data availability proof verification as a kind of like a pre-compiled op code john adler has a proposal for this on the on the ethereum research forums does that answer the question or yes yes thank you and just a small one um is there a particular reason why in celestia you you're using a tendermint and not casper is that just because it's simpler or well well this casper exists as like a standalone river of stake like like library at the moment i don't know if that's because but the reason why we're using tendermint is because when we first started building celestia at the at the time like tendermint was the only like you know production ready modular proof of stake uh sorry the consent bft protocol at the time and so like that was really the only option back in like fifa late 2019 and i think now maybe there's some other options like i know i have like maybe this code for casper that could be used in in a more applicable way and another like avalanche has released a source code for the proof of stake protocol okay thank you any more questions we have one question on the live stream which i'm going to go to first [Music] it's steph is asking what are the trade-offs between zk snarks and zk starks and optimistic rollups yeah [Music] how much time do we have ten minutes okay um i think at this point i'll just say that the differences are like it's not just theoretical differences there's they're already uh teams and project building very specific solutions so i would focus on that um we started with starks because we this was first our expertise and we also know that it's operate faster and that we can rely on it on the long term um and we got to very nice performance there but right now the most important part is that it enabled us also to create cairo which enabled us to develop a really fast and basically really fast and and and quality solution that that is darkened um so i think the difference is there lie mainly on performance and and the expressibility of the language and the removing the need to use circuits and this enable all kind of things that relate to scale and throughput um with regard to optimistic roll up i will let my friends here to speak one one thing that i mentioned in my short talk before is that one advantage that we see is that the ability to post much less data on chain which is still very very important in all kind of use cases and yeah so that's one thing i will mention well that's true uh but also that as you said at the start it's very use case specific right because different use cases have different requirements which fits two different models differently so for example reducing the data you post on chain to for example state dips is a very interesting approach but many times many applications actually needs the intermediate data in the middle of execution and that's something that if you're just posting the state divs you're more limited because you actually need to trust this intermediate state but many applications actually don't need that others do and there is a trade-off based on use cases and yeah i believe that just having many different flavors of many different scaling solutions is the best thing we can have right now so everyone can actually experiment and we can see how these systems actually react in production and yeah there are many different like community resources that show the differences of what it's live and many different papers that show the theoretical differences of what can be live and yeah i'm going to keep it at that otherwise it's going to be half an hour yeah i mean you know that's part of the beauty of the uh modular stack is that you can have all these competing solutions and you know it really depends on the implementation of each approach to see how it stacks up so you know let the numbers talk i guess but um yeah overall there's you know there's a lot of different considerations to take into you know accounts um you know depending on you know what kind of proving time you want or you know liveness of your transactions so you know those are all different you know things that probably deserve like a full conv full presentation but i think that just about wraps it up for our panel discussion tonight uh thank you all so much for your time uh we have food and drinks starting pretty soon yeah food and drinks are upstairs um they made a really nice spread this is a really great restaurant and there's dessert it looks amazing so please hang out and mingle and chat and let's learn more about data availability and roll-ups thanks for coming everyone uh you 