hello welcome to the bay two of 61 point x meetings so what we're going to do now is that we have about four different presentations first will be Frederick then there will be KC then there will be a Remco then I will I will present after that I presenter very shortly the things that have changed since yesterday and after then we're gonna have a breakout session okay and then following that we'll have individual time and more presentations all right good morning so we had some good discussion on chain pruning in the relatively small group yesterday I can't say that we got to any sort of implementation details or figured out exactly what to do or what an EIP or anything should look like but we had a lot of discussion around what we need more feedback on and what we need to figure out before we can write an EIP and a lot of this comes down to like community feedback and actually figuring out what users want and need so I think the first point that we need to establish is what do people actually want and when we're talking about a prudent node this is you know we can talk about it in different context but essentially imagine it's a node that just has a full copy of the states and headers and nothing else who wants this like what is this for and you can imagine a use case like for myself where it's someone that wants to use apps and used apps regularly they don't necessarily want to use the like lines and pay the network cost of going out over the network for our every requests to this step so they want the full copy of the states and but don't have 150 gigs free on their laptop so it's sort of a thing in between no my client and a full note and like I can imagine that user and like I would be one of them myself but how many of those users exists in the ecosystem and it's really hard to tell we've already you know pushed people to accept centralization and just just use in Fura everything is fine and can we really take this step back and and like bring those users back to care about data validity and other things like that and it is something that I don't have the answer to and I think we need more people saying that they actually want this before we should just dive headfirst into doing stuff this the second question sir why yeah why run this over all our clients do people care about those Network costs how hard is it maybe we should focus instead of focusing on building a proven node we should focus on like clients and sensitization actually making like client the like client experience a first-class experience that works really well if we solve the in sensitization problem that's also used for useful for you too and something that can further like both the current ecosystem and future so that's yeah another question of like where is the efforts best spent and given that we have limited resources and the final question just here is this is almost like west question to Hudson which who's not here right now but the question is like how do we find out the answers to these questions I don't really know we no need to like engage the community to find out what they actually wants what they actually need what they're what they would use but if we assume that this is something desired how do we build an MVP to prove these assumptions so I think the the first thing that is really easily implemented like I said yesterday something that we can have like next week if we're going into is a feature flag on CLI to just say you know proven everything beyond and blocks and I think there's a discussion here around if this behavior is default versus non default and in Peters proposal I think he has written up most of this is like right up in assumptions and sort of how this should work with the expectation that it's the default behavior and I think going into default behavior is really dangerous and something we shouldn't do as an MVP if we just ship a feature flag that says hey run this and pruned mode then and and have a non-default then we can sort of gauge you know how many people are actually using this is it something that's solving a real problem and we can talk about you know bringing this into a default behavior if we can see that it's necessary and have figure out this the solutions to the availability problems with that comes to the question of like do we actually need to ensure availability so a lot of Peters post goes into how do we ensure availability you know like Bitcoin has this mode as an optional thing that you can run and they don't seem to have availability problems I think it's relatively safe to assume that we won't have availability problems if this is just an optional known default feature that we shipped I think there's also much less of a coordination burden between clients if it's the non default behavior and a sort of quote/unquote insignificant amount of users using it but if a significant part of the network starts using it then coordination becomes more important and we actually need to figure out like how do clients sings together and things like that so I think my proposal here would be that we actually ship a pruned mode feature flag and just see if this is something that people want and people use the last thing that we discussed was how do we like what is the best way to ensure availability into the future Peters sort of top suggestion was to put blocks on ipfs comes with a couple of additional problems like we we're not actually storing ipfs hashes in the header so to be able to look up blocks we'd have to either introduce another hash or have some sort of gateway service that converts one hash to the other and it can be you know somewhat complicated it's figure out and we can we can add another hash to the header I in protocol or out of a protocol depending on implementation and it's not that big of a deal but it's a little bit dirty and yeah it requires cross client coordination so if we go with IPS ipfs we need to make sure that all clients have ipfs capability to be able to read from ipfs etc that adds another level of overhead so I think something that we talked a little bit about was you know what level of availability is required what's high probability of availability and what incentive structures can we build around having availability but I didn't hear any really great suggestions on actual incentive structures so if anyone has ideas still open to that and finally I think something we talked quite a lot about in especially related to the discussion of ipfs and where do we store things there is this or a general proposal that brought parody made a couple years ago that we just shard up the history by identity key and you can do that in arbitrary way way sort of but we currently don't have any discovery method of finding notes that store this section of blocks so we're sort of dependent on discovery b5 which development of is not that active or fast and so we need to be v to be in place and sort of deployed widely across nodes before we can use and a mechanism that is inside the theorem network to just like distribute these blocks but I think everyone in the circle sort of agree that this would be the best scenario because we keep everything in the theorem Network we don't have external dependencies on things like IP FS and it paves an easier path for sure for proper and sensitization in the future so yeah there's an open question there of like how do we get discovery v5 ships as fast as possible but I don't think we need that to start experimenting and I think we can build our MVP and have a feature flag that enables to promote pruned mode and basically say if a lot of people want this a lot of people use this then we can look into you know how do we ship discovery vv5 faster or whatever else that's necessary to get higher probability of availability on the network so those roughly what we discussed yesterday and I don't really think that we can have much more like productive discussion with just the people here today my hopes is that everyone kind of goes away and thinks about this a little bit and especially we try to reach out to the community try to see what people want and is this indeed something that is desired and people are asking for then we can pretty easily build a prototype to serve that need that's about it yeah so discovery b5 is I don't know exactly everything that's proposed in so we're currently on the before like version 4 of the dev p2p discovery protocol and so this is the next version of dev PTP discovery it includes one important thing called node records which is just a fixed length string that becomes available like Mazar Batory contents and it becomes available in the DHT so you can once you've downloaded that DHT you know which nodes have which section of the blocks so you don't actually need to go run the network pinging everyone to see who has what so question is what's what an incentive structure look like I don't have any way suggestions so basically I mean that the core premise is you get paid for storing a section of blocks or even you get paid for storing all of them so if you're a full node and you can prove that you have all of the blocks available you get paid somehow this sort of goes back to proof of storage and proof of like retrieve ability and proof of availability which is like a lot of the stuff that file coin is working on and we could probably lend some stuff from them but borrow some stuff from them yeah III agree I think in Peters proposal there were some comments there were some from metallic of like why why wouldn't we use this and Peters response is basically well they're not production ready and yeah I think the argument is like ipfs is something that exists now and we can put in next week if we wanted to like there's great bindings or go and there's we have already have a bunch of stuff in rusts for the peri client so we can get it done really quickly and making Sorum production ready is a longer-term project that we don't really have any estimates of how much effort that would actually take but I would argue that we don't need to solve this problem right now we can still introduce a poun mode and then we start working on that problem and solve it properly yeah yeah no programming why so we could do quickly but socially I'm not sure yeah it has so it was considered as well it just has some additional problems no but we could come up with a standard for it relatively easily but currently parody and get a different serialization well I mean there's there's always our LP so could always do that but I think so question is can you incentivize both disk and bandwidth usage and I think you can know but I've like swarm and pylori and a couple others are working on dit and since I storage and that's a hard enough problem I haven't actually even seen one anyone trying to approach incentivizing bandwidth it's it's a lot harder yeah it's wrong maybe yeah yeah so in a like client incentivisation scheme you would basically charge per requests and account for bandwidth in that request [Applause] used to what yeah I use this computer yeah can you do that yeah the URL yeah okay so the idea can you hear me okay with roadmapping in Reverse we know what we want to achieve we know what the final product is which is compiler engines for both the major clients Gethin parody and preferably the engine for goal would be written in pure go as the go team likes the codebase likes their codebase to be volunteer goal not pulling in dependencies written in C++ or Java or various languages just pure go and that would be ideal so that's the ideal situation what we have now is we have interpreters in every steering plane it's not very hard to write an interpreter you know so one number team Paul wrote the interpreter and Python for go we're using an interpreter called wagon interpreters are easy but compiler engine prototypes there's the the serious ones come from the browsers so in chrome there's it's written in C++ so there's a decent engine in C++ there's also good efforts for engines and rust because Mozilla is trying to rewrite basically Firefox in rust so for parody it's integrating the web assembly webassembly engine in rust is it's there's a lot of progress towards that yeah I was joking this morning that if if Google was decided to rewrite Chrome and go then it would probably solve all of our problems because we would have a decent webassembly engine written pure go but they don't seem to be doing that so without having a good compiler engine in and go then one workaround we could try as compiling the pre compiles using just ahead of time compiler whichever one works the best and then deliver the binaries you know the exe is to to the clients so that you know guess would import and executive all they might not like this well we we already know they're gonna hate it but this is one when approached short short of having a full engine full compiler engine written go maybe if this is viable you know to get seem could bite the bullet and we'd be able to have webassembly pre compiles before a web some the engine is available short of that we can use an interpreter we can just start out with an interpreter this was a proposal from Alexi where rather than trying to jump straight to having awesome compiler engines if we just start with integrating an interpreter engine then other teams will be incentivized because they'll know okay well web assembly is definitely in this area so if we work on a fire engine for theorem then you know we know it'll actually be used and adopted on the main net or e 2.0 whichever without having even an interpreter base engine inside the theorem clients it's it's a big risk to start working on on a compiler engine that you know may or may not ever get into etherium so the downside is this using a web summary interpreter for free compiles it only worked for a few frickin files a lot of some pre compiles are prohibitively expensive prohibitive prohibitively slow when ran inside an interpreter so like the snark pairing free compiles are way too slow in an interpreter hash functions might be fast enough hopefully they'll be fast enough when ran inside an interpreter because you know it's kind of dependent on the input size so for small input sizes the free compiles might be usable inside an interpreter engine so even another workaround even less ambitious is to simply use webassembly as a blueprint for free compiles so we just be analyzing this webassembly blob to generate a gas roll and this way the gas roll is simple enough to be implemented natively this is basically how the existing free compiles are done this it doesn't this doesn't help get introduced webassembly into into the clients very much at all it makes it maybe slightly easier to add new free compiles but still like one of our estimates is maybe if we only do this then adding pre compiles the according to the way the old way we can maybe do like two new pre compiles in 2019 when there's probably demand for like five or seven new pre-compiled that people want but doing it this way is a lot of work for each one so yeah that's I mean that's it these are three approaches that we're kind of working on all three at the same time trying to prove which ones are viable and hopefully get us to the the final goal but you know eventually yes well we can probably I mean using an interpreter and and a blueprint but I mean we could specify the the changes for for you know a bunch of pre compiles but we're not confident that the testing that we could test them all thoroughly and that clients could implement them by you know say the end of 2019 only only the interpreter using as the as the engine I mean you can probably specify this by yeah by me but um it's not going to be usable for all the briefing files that people will want so be patient yeah I mean what will see if the client maintainer you know algorri that it's a realistic time timeline yeah we're pretty close with concepts as it is yeah yes alright let's try that so so my question was yesterday we talked about how difficult it is to estimate gas and we identified a couple of scenarios one is that it's constant gas the other one is that you can cap it at an upper level and the final one is that it's an undecidable problem depending on stuff so my question is really is there any pre-compiled so we can start with which which which are easy to do according to this increasing ladder of difficulty you guys want to jump in here yeah things that are constant runtime for example if you know your elliptic curve you know multiplying to whatever points or whatever you're doing is constant runtime then that's the guest role you charge them constant gas so someone some are have structure like hashing they do it in blocks so they they your input data is sort of arbitrary length but they hash it in chunks and they you know they do you know digest digest ideas and then finalize so that has sort of a simple structure so yes the pre-compose we're looking at have this structure that we can analyze arbitrary code hopeless as you know the halting problem but as you mentioned but yes for what we're doing we think we can have reasonable guess rules for the pre compels that people are interested in I don't know one more point I just wanted to say Casey said that the interpreters might be too slow for certain things like pairings I think that might not be the case I think we need more benchmarks for that kind of stuff yeah just to repeat the the upper bound gas rules don't help with that that's unrelated to pre compiles that are simple enough to run inside the interpreter if we can maybe do to your new pre compiles that are too complex to run fast inside the interpreter and for those we yeah we want to use the web assembly as a group blueprint to generate the gas rules but still doing more than two seems unrealistic with all the testing and an implementation that clients would require to add those pre compiles okay so we were just my previous question was about so I just looking at the objectives because I'm determined to kind of keep people and objectives today so we stab lished sort of vaguely the initial set of changes for the May 2019 so the first is the second objective is established as framework for designing evaluating and comparing that sort of the change proposals so the question to you and to your your team is that if somebody wants to sort of propose something for your for your you know for a Eve as an introduction so so water's gonna be maybe you could write it out or something like this so what are the things that they need to consider so that they don't have to they don't throw like really and thought on thought through the proposals at you so what kind of what are the steps that need to go through to to describe like do you actually want people to come up with alternative things and sort of the framework that I'm talking about the framework so so what are the questions people need to answer what are the things that they need to consider before they come to you and says hey I've got I've got an idea I've got an alternative proposal to what you're doing maybe or something like that I know it's my deep way I think the biggest one that people take for granted is client code bases like to keep their code bases pure so saying well why don't why doesn't every client just use this you know C++ engine or something so to to the so you might remember yesterday I put up the like of four questions write to you so if we were to keep those questions and obviously added a third thing today so that another consideration you would add to that is the is the client code and they are sort of some of them being opinionated about what should go in and not what are other things that you would you would say are important off the top my had nothing really comes to mind yeah so if you have any ideas through today or tomorrow to add to because I think I'm gonna start with the the questions that are asked yesterday I'm gonna add what you're done today and we can add some more things about all the considerations that people have to think about if they want to you know to participate and give you our alternative proposals okay cool I just wanted to add some comments on specifically this and some of what you said about clients and code bases as well so something we have to keep in mind with pre-compile is that there are some pre compiles that will never be anything about x86 assembly like it's just anything else is too slow so it doesn't matter if it's wasn't compiled to assembly like that's gonna be too slow so there are there's like a certain class of pre compiles that are assembly only that oversight whoo hi pervs you know specific optimized assembly code then there's like a second class that I think this fits really well for where it needs to be fast but not extremely fast where it needs to be compiled as a web assembly and then there's a third class where it just needs to be faster than EVM and if it's a really good interpreter an interpreted webassembly might actually be faster than EVM because we have 32-bit math instead of 256 bit math so if you're doing some sort of like relatively simple crypto then webassembly might like even if it's interpreter it might actually be faster pairing is the first one it's assembly yeah so you think that the pairing whatever we do here the pairing have to be an assembly right to be efficient enough yeah I mean for the ones that we already have yes but when like when we're adding new ones you know it will definitely be faster than EVM version so it's it's sort of what about like if we if we think about this is the G minus two but did you start with G is this the so if we think about G right was it G yeah so if we have a very good yeah if we have a like a compiler maybe ahead of time compiler or something which can compile let's say pairing code written in a web assembly into machine code do you think it's going to be fast enough or not where it does it have to be handcrafted assembly the bans are what I mean by fast enough but like maybe then done what did you want to say something about it the click the pairing implementation in depth right now it's like 10 times faster whereas the just a quick question does the pairing require a careless multiply and does webassembly actually have this instruction in the first place careless multiplies something you need for yeah exactly exactly so my question here is that okay so the do we need pairings to be super fast or can is their use for pairings where which are not as fast because we might maybe we we don't need to have them handwritten assembly like are they can i still be useful enough if we release they are right now like that that's why people are pushing to get like the parity implementation a 5x faster because the guide the gas costs of pairings right now is no no this is a different thing so there's a gas cost and there's the actual performance but the gas costs currently mirrors the actual performance so what to reduce the gas costs we actually need to improve the performance okay I would still I would still like to separate these things because if you know if we can improve increase the gas limit or something to make those things useful so then maybe it's it's even even because what it looks like what you're saying is that some of us some of the pre compiles the whole approach is probably kind of flawed right the whole approach to trying to do the pre-compose and Eva's ohm is flawed I actually before we sort of flow throw the the white flag on this I want to say I want to ask if even if it's slower than hand-crafted assembly like why why what Martin but what but what factor like 10 or so could it still be useful for a real real stuff or not there are applications where you would need to do pairings infrequently but I think the issue is if you price the gas costs relatives are the worst performance you're discouraging developers from using it so it's it's probably not the best approach to take in the short term just because it's not gonna encourage use but actually just following up on that maybe I'm misunderstanding if so all the clients already have an optimized pairing implementation yeah what is hard about calling that from-from otherwise I'm interpreter not much it's doable it's just the whole point of there was am thing is to remove the pre compiles or like in in this scenario so it would you gain nothing but I think there's so we will always have this like the super optimized like pairings stuff I don't think it's going like you like it will discourage use like even currently it's use this limited because it's too expensive or like too slow and I think that will remain the case for those but there's a whole sea of other pre-compile that people want to add like they want to add different hash functions they want to add a ton of different stuff that is not that doesn't have to be hyper optimized crypto code so there are definitely other types of recompiles that don't need to be hand-coded assembly yes so the just to clarify so we they're one of the reasons why what the e azam was included into the why we're trying to get it earlier than a Tyrian 2.0 is because we see it as a meta feature whereas instead of developed core like a client develop of developers working on specific pre compiles and it's really a lot of work we basically deliver one engine which could implement any pre compiles so that's like a it's like a optimization of our time sort of instead of try to optimize things which might be used by quite a few contracts we actually just got the whole class saw the whole class of problems probably not the most optimal way but like a which is kind of we were going for the breads was the one paring function we have now you know the the BN curve and then as soon as that was implemented and adopted you know by the time was adopted C cash was already switching to you know the new curve so anybody else have a question for now I think Frederick made the best point where there's some class of problems that are reasonable for interpreters there's some class of problems that are not reasonable for interpreters there's some class that we can't even use with aetherium now because we would need FPGAs or a64 so there's sort of classes of problems the interpreters give us access to some sort of problems like hamstring like Blake's and certain things but perhaps not other things but this is sort of a process as we go as we as we improve as we shift our gas prices from interpreters to compilers when our into a compiler infrastructure improves we have to take a first step this isn't going to just magically appear our compilers are you know audited everything isn't going to just appear we have to take a first step and that's the that's the reason we want to start with interpreters to tell people hey look you know we have a need for compilers we need to audit them we need to you know we want them to perform we want guarantees from them without doing this interpreter step maybe there's no incentive to do the compiler so I think that's the benefit of doing it yeah I completely agree with that yeah and it's just a matter of time I have an adapter yeah yeah but you can use it awesome that's not the right screen okay weird where did you go why does this not work found it so I just want to have a brief meta discussion so me immune Greg are here from the perspective of tap developers which which means that we're not as much in the know of the real problems that the core developers are having but I think it's good to just step back a little bit from these specific implementations and and look at the higher-level problem that we're trying to solve here because I don't think there's a lot of clarity right now about what specifically is the is the problem we have a clear goal we want to keep a theory m1x running but one of the things I learned yesterday is that for sheree theorem performance is really really complex and for example something like storage has nothing to do with actual on disk storage but has complex you I'm suddenly become more expensive so it's a very crude signal that we have and any sort of discrepancy between this gas cost and the actual resource consumption of the system that could in theory grow out of like without bounds over time will inevitably become a huge performance problem in the system so we need to make sure that whatever the actual cost of something is and the gas cost of something is it needs to say within some finite bound of each other otherwise this will inevitably lead to problems so now that we sort of defined the problem space the other thing we need to do is define the solution space so what are our options what are we willing to do what are we not willing to do where are the trade-offs how much pain are we willing to suffer in order to solve certain things how urgent are things so one thing I realized is that at least in some respects notes are not fully optimized yet probably in terms of the peer-to-peer network there is a lot of optimizations that can still happen to avoid bandwidth consumption so it might not be necessary to break the protocol in order to reduce bandwidth consumption because there are lower impact changes we can do in India wire format first and we need to be a little bit careful here when we start breaking the protocol for things that are not heavily optimized yet in the notes because we might end up breaking making changes in the protocol that we need to carry forward stored in in the future that might end up actually being necessary as we start developing the notes more so this is this is something I'm a little bit worried about fortunately it seems that notes in many critical areas are already heavily optimized and where they aren't we can we should be able to estimate how much of an improvement we can expect from future development here and how much the impact of certain protocol changes would be here another goal that we have is to remain implementation agnostic we kind of want to give all the notes the freedom implement things however they see fit but this is directly at odds with having accurate gas cost and solving concrete performance problems because they are always dictated by a particular style of solution so these two these two points in the solution space are actually its trade of what we need to collectively agree on where we want to be between one and the other another another question is what do we want to break in a protocol so the protocol has a lot of things it has historically guaranteed by virtue of the yellow paper and it is obvious now that we're going to have a series of hard Forks that are going to break some of the things that were to fight in the yellow paper entirely clear where do we draw the lines here what what the boundaries of this are what for example it seems that we are completely fine with destroying gas token like the storage also and some of the other changes are going to utterly destroy us okay and don't think anyone has a serious problem this because the assumption is that gas token it's based on something however we were kind of talked when something that you cannot do is storage for 2300 gas was broken and started working and it cost a big planet so this was an assumption tween core developer resources and urgency so this sort of defines the solution space and again I'll like to give the dab developers perspective here the main thing is how do i as a dab developer and know which things I can reasonably rely upon to be there for the future and which things I should consider yes this is what the protocol does now but this is likely to break soon because it was not meant to last forever this this is not like we specified the current behavior really nicely in the yellow paper but what is not clear in the yellow paper is how does will evolve going forward and it would be nice to add this now big shops like let's say maker 0x you name it honestly break whatever you need to break we can migrate we have designed our systems to be upgradable we have developer resources we have the skills in-house to deeply understand the and the changes and if you just give us at least a path to a migration we will be able to find it implement it and take it so don't worry too much about about us well you should be worried about I presume is the medium scale developers and the individuals because they might not have the resources to implement migration paths or they might have locked themselves in the position where a migration part is really hard let's say for example that I'm an individual who thought it was fun to become an absolute 0x Hudler and I locked all my 0x tokens into a contract that locks them for three years and now storage rent comes and this contract gets slowly drained what happens to my tokens is there a migration part here so that's that's the sort of thing that I I would assume is a bit more challenging so yeah my key takeaways here are we need to we need to define what our performance targets are for the system in order to define what the problem is that we're trying to solve and we should be documenting any invariants that the system currently gives and to what extent the app developers should be depending on them thank you would anybody has questions today I'm cool you very much I'm gonna just do you so you probably saw this yesterday but I just gonna highlight what I've changed since yesterday in my slides so it shouldn't take a long time based on the feedback that I had received including this one as well okay hello so um you probably remember I showed this yesterday so my kind of premise was that we had four main performance degradation that would be caused by the growing state size and so it's numbered one two three four but actually you it's more than an hour for example which it could do then by the time the sink is complete the piers would would have pruned the state that the this new pier wants to sink and that clearly is getting worse with the sort of with the with increase in the state size so what we want to do is that we want to do this emulation which to determine what that function is so we kind of think that this function has three arguments state size bandwidth and pruning threshold and so what is the success rate in this case and maybe we could try to see investigate the dependency there and so what happens when you when your state's I start improving and so the next thing what we could do is that for other things for the number 1 2 3 although we we have an idea of what the function is but we don't know the coefficients so for example you probably intuitively can think that the just simply reading the state is cheaper than then sealing the block when you modify the state but how much cheaper is that so we again using some emulations we can we can put some really rough coefficients on these functions like and after that so once we've done it so we have a very rough sort of functions with the with the relative coefficients for each of this performance degradation and then after that we use we use simulation already when we have functions and then we we try to simulate the events that we kind of want to predict when we're going to be hit by these problems and the problems could be new guest nodes mostly unable to seduce a snapshot sync or new priority nodes because they have a different syncing mechanism they probably will have experienced this problem at different times at the different size of the state so then an accidental thing to look for is that when is the block processing going to be 10% of the interlock time when is it gonna take half of the interlock time because all these things will start affecting more and more facets of the of the performance so that's kind of the the the plan so next thing I'm going to show you is is the additions that I've done to the to the state rent since yesterday ok so just scroll to the pieces I've changed yes so here remember yesterday I was trying to explain you the hello cups with the with this analogy so I think Andre came up with a different analogy which I didn't put any pictures but yesterday I used analogy where the contract is like a glass separated into the sections here we've got the six sections so basically this contract has six storage items and two of them are currently have low cups on them so the idea is that after the introduction of low cups all the contracts of the new contracts will always be full so in this example where you created the contract it's got no storage and then you have an explore which sets some values and it keeps growing and then you see that at some point when you simply change the value the storage size doesn't change and there's no lookups necessary so you could see that takes origin doesn't need to pay anything but when subsequently the ciock surgeon women subsequently you free element of the storage the excess lookup gets returned to detects origin and in the case where we have a pre-existing contracts so again this picture demonstrates that we start doing some changes which could be either add in a new item which needs to be needs to have a lookup with it or even if we just modify the item in a second transition we still have to put up the lock up although the the storage size doesn't change in then in then we'll do another modification and then it new leaf I so so in a second so in the last transition we reduced the storage size but we don't get anything back so a good question oh yeah yeah sure can you give a microphone I wonder why are you deducting balance from TX origin instead of trying to use some opcode for that well assuming new opcode yeah well the new opcode which will be like you it would be like somewhat code that increases the DOE cups like pay rent and if the reason for that is because the reason the whole reason why the lookups were introduced is to provide the existing contract with the migration path so and of course if you need if you need to put a new opcode in a means that you can't have to rewrite the contract so the we have no choice if you want to keep the existing contract for a while while they migrating we have no choice than to modify the semantics of the current things and actually something came up this morning which I didn't put into the presentation yet we had this little chat and there's I want to explore a bit explain a bit more widely we need this migration path and shortly the explanation is that if there are multiple contracts which have a synergetic relationship then like let's say three of them using each other then these three contract will have to upgrade kind of step wise sir the first one so the first contract rewrites its code but it keeps my gradient of clients but in the meantime then the contracts which uses it has to use the old version and then they say okay we've done migrating now the the contract number two can switch to the new version and then start migrating itself and so forth so this could be like a whole chain of migrations and we need that's why we need this mechanism to keep them alive while they're migrating because it cannot happen in a one BIGBANG cool thanks just a yeah let's discuss later but the exhibition may not be enough for those kinds of situations okay so what else I changed here since last time okay so here is the you know the discussion we had how big should the lock up price be and at the moment one of the methods which were thinking about is that we think about a target storage size we could rediscover it using simulations or something that I just mentioned at what sort of storage size we can everything is gonna stop working then we also have some sort of target lock up so how much easier do we want to be locked up so completely reach that target storage size and we just divide one another so if we for example take five hundred million items which is about three times more than we have at the moment and which we say okay in order to completely fill this up we will we will require 10 million easier to be locked up then we arrive at the lock up price of 0.02 east per item which is quite expensive I would say and so what else did I aren't so I fixed this diagram from yesterday so in this diagram yesterday I had the potential modification to Rend balance and balance but now in here I cleaned it up so it's now should be clear that the during the eviction check if the eviction check is not not successful then nothing gets modified so and another thing okay so here I clarified at a pay rent opcode actually has two arguments which is the target contract and the amount which means that you you can call it from the third party contracts and to to top up somebody else's a balance rent balance and the lastly again we had a discussion yesterday what if we get the constants wrong so in this proposal therefore for different constants that have to be introduced which is the account rent code rent storage rent for different aspects of the rent as well as the lock-up price so if we realize that we have under price to over priced rent then we could probably just change it by a hard fork and don't at the moment I don't see any major problems with that we're but the local price is a bit different because we we currently have one a simple counter that account accounts how many lockups do we have so if we decide to change the local price with a very hard work what we'll have to do we have to introduce the second counter into the each contract called lockup - in this case the condition of like remember there was a condition that which determines whether the contract is full which means that it doesn't have to pay a storage rent so this condition will change from the simply storage size equals two lookups to something a bit more complicated but still manageable which includes two prices the old one and the new one which is basically say that we were we were tracking both of these things and if we need to do the third hard work then of course we can make this formula a bit more complicated so but basically yes so I say that this is the trade-off so obviously introducing the variable lookup is will eliminate this problem but it has other the the variable lookup introduces even more complexity into this mechanism okay so that's it from so these are the updates from yesterday that I had any more presentations for now or should we just break through break out okay cool break out so we can stop the livestream for now and we do the breaker 