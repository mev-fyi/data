[Applause] in sweethearts I can have some kind of any tool here and then we have a really trusting talk by PC of a benchmarking I'm sure you guys are here for the bench waiting to hear what bench ranking of what numbers being victims we realize and as I'm gonna explain briefly but he wasn't one pointer when he was in 2.0 is said this is disclaiming electrography has an elegant look concludes with a bunch of questions regarding wasallam blockchain and there will be plenty of time to ask questions during these talks so he wasn't even a tea person in a bunch of different ways to to write the project name and and a current one the current one is the last one so please only use that I still see people using the first one it's not to be put in this form any I actually started a really long version of this talk with a lot of stories but I realize you guys may not be trusting stories you want to get your facts here so kind of over some facts but if you're interested news stories you can come to that thing um and it's been long but it seems to be I think 45 minutes so this one gonna be much quicker okay be kept the motivation of II wasn't for BMC general we want to have a really fast execution but it seems that we want to have an extensible architecture so you can kind of feel something sensibilities fine are we gonna see it may not be on the speed and lastly we want to have a lot of languages supported it's great to so these were the three motivations wanted and eventually by looking at a lot of different architectures and VMs was M was selected so easy we kept of the original prototypes and in 2016 nothing any of you guys were following he wasn't since 2016 and you cannot confuse a give us what's going on it's been 19 today so in 2016 we started off with the motivation that wasn't should coexist with EDM on the cereal and milk that you use we passed was an engines those which are interpreted it is all these four that I submitted and Jasper I'm entering JavaScript because is going to be important and we didn't shield a couple of key points back then and we had metering ejection prototyped and working so this was proof that you can meet her other bytecode not just email it was an important milestone and he also managed to write any version of the helium twasn't compilers sentence is important because we wanted to have a VM and wasn't on the same chain and eventually wanted to place out EVM so you could just use it was an engine but still having compatibility with even particles we have some kind of acetyl team but it was so early for wasn't that we actually had to write a lot of stuff by hand in awesome s70 Foreman so just by taxi it was fun but it wasn't really sure that this was the point where we had the first benchmarks and we're gonna talk a lot of a benchmark so that was that film - he wasn't at perform CPM super excited we achieve what we wanted but there's an asterisk there it was at performing it on VA a really fast a fire engine who wasn't and we were comparing that against is no interpreter opt Emma fun fact it's actually not so slow of an Indian interpreter what's another compare to go achieved but it was still not on par with v8 and then we had to test all of the seven twenty sixteen and then we had a full year of haters in 2017 the team only consisted of two people and nothing really happened we were occupied with other things so that's why you haven't really seen anything but he wasn't in 2017 and the only restarted work in 2018 after death country the main goal was to launch he was a Methodist on each one however this coincided with Deaf country which was focusing quite a bit already on serenity and some of you may ask what is said serenity is the maiden name of e to bunny so e to Brno was already on the table at that point we did make some progress and he wasn't any police suspects and they also had to update everything to Byzantine the heart worked at the time we made a bunch of test cases of course it was not a percent but we did a lot in the test cases and people to Brenda knew he was an engine in C++ called Hera so the previous brother had presented JavaScript but at this point we had an engine in C++ and this was using an interpreter of wasn't called webbot it wasn't that the fast VA it wasn't the browsers to read fast engines it was an interpreter for water so then we went on to add more interpreters and also have entire engine genera we can expected at this point in time which was two years after what was proposed initial workman wasn't restarting it was two years after death and became expected that all these fast engines from the risers could be usable standalone but we were kind of wrong that wasn't the case and that it's still not it is the case you cannot use any of these fast engines extend a little bit at the Ransom and our work on benchmarking we began at this point because we have these different engines were able to benchmark them with different templates and we came to two major key realizations a combination of time on the fast engines so the way these fast engines work is you take the modified code you translate it to machine code and then you run the machine code in this combinational face compiling that code to machine code turned out to take a lot of time and if - maybe more time than the be spent on actually executing that code and in many cases this time the total time was actually more to running it on an interpreter the main point here is a lot of the to be run on aetherium runs for quite a short time we don't do a lot of stuff you know in in the box we only want to spend you know less than a second and I need to point it would actually weigh less than a second in completion time a take way longer than set compared to that interpreters may actually take less but we did find out that interpreters at least well it was extremely slow at least so we thought with extremely low it was restore 256-bit stuff which DVM does but it was kind of late for 64-bit stuff so we want any kind of really unhappy at this point when they sign up on was dropped it's a lot of people are asking what kid pumps be an issue and I guess what chief pumps are anybody else okay yeah so basically you want to have a short input which can blow up this compilation face and we took this question really seriously and we spent some time to validate this plane and I came was true you were able to find bombs one example in support from beings we had this chip bump off anyway it was taking a lot of time and that's me that's really worried okay in fairness though we cannot choose these hostage in stand alone and their suspect about these two these cheese bombs so we're in a really bad position at this point so basically be turned briefly to you trying to defend ourselves against jet bombs so he's still kinda committed to use these fast engines because well to the speed but we didn't want that to be suspect ability to cops so we came up with some really cool ideas how to do that but they turned out to be too complicated so what was the next step then we try to reduce you volume to a safe subset first which can be quickly introduced as the same subset would be the precompile because of a so really this kind of safe because they're introduced to a hot pork there's consensus what the truth is they're likely should be you know a lot of scrutiny for that clear that should be an audience so whenever people might get said it should be really well reduced code it may not be the case but you'll become expected that that would be case and we definitely introduce wasum a safe subset of wasum and then we could improve things in the background we also a desert if you mentioned the fear than others ambitious ideas or welcome blueprints and I'm gonna go into them internet but it's retesting the talk I mentioned the beginning and has the slide on it but I think you can also google it it was really ambitious it's cool but happy too complicated so as you see it was this was a terrible point of time for us nothing really started to work that we needed a breakthrough and the great truth that I'm becoming it in my case man then I'm gonna come back with some some more cell phone wasn't he wasn't she fantastic work oh you're gonna kick my Casey crisis is the one question and everyone question okay so you said the food but you can't use the so the question was there's some services today which offer standalone we seem to ever stand alone wasn't engine stuff I am a clarification said that thing was actually like 16 months ago and he only looked at three big engines in the risers that's like Firefox and Chrome and that's still the case there's some work trying to pull them out some types of work to have a standardized API in C and then you can alter that and use them through the standardized API but since then a couple of other out there actually fight at a project to project really fast engines some of them are better than others and one thing which surfaced I think was tuned ASCII has a new engine that's the the one I think you were referring to so today there are a couple more options is it working Casey no to do it though [Music] no I don't take more questions later if you have any questions about the methodology or if you can't see something on the screen please don't hesitate to interrupt so it's sort of an accidental discovery the outline first explaining what recompile is and how this motivated main use cases for you awesome recap where we were then discovery that flipped world upside down and brings her so freakin pile is EVM code that's treated as a special case so you know you charge less for this special code in theory all precompiled should be pay the same cost as any contract that's deployed but because PBM is thought to be slow these special cases are added and the gas cost is subsidized most of these pre compiles that are around cryptography so EC recover is signature verification there's a couple test functions here and then once added in October museums over 2017 is Antione also cryptography so this will be important later most of the prequels that people want seem to be related to cryptography with the exception of a occasional professional Channel and people think some people think Creek Mountain is mine and it's easier to have creaking piles than it is to make the VM go faster but so what's wrong with just adding more freaking piles well it expensive trusted computing base so the trusted computing base right it's in in the etherium context it's like the consensus print book oh that is in each client so if there's a bug in that code and you get a consensus bug then in this consensus critical zone with people files come custom guest roles that are very complicated so there was a consensus bug found after the test net rollout so as found before the main' that hard work before activation I've made that but on the test net in in my Jeff speed representative there was a consensus bug phone then in the pairing check over here and this div I don't know if this is still say the test cases of to be written because everybody wants to propose new recompiles but they don't want to write test cases and when I took this home maybe it still says this to be written but just once kind of near and dear to me because I actually found that that consensus booking and that one the notes after it was activated on May 9 also there is a social governance burden up you know it takes up time on Cornell calls it takes up it over time and most importantly I think is the philosophy I mean Francophiles actually go against the etherium philosophy in my opinion if you know all codes should be treated equal and no special treatment just because you know you're like connected and you know you're buddies with metallic or whatever you know well tough you pay the same guest post as everybody else so here's where we worked last time tongue which Alex pretty much went over but we had some interpreters they were too slow compilers Alex talked a lot about the optimizing compiler the solution for Jim poems was like a single pass compiler but there really weren't many of those so yeah like light beam which is being worked on by the parody guys a single best compiler but you know doesn't support memory yet maybe it doesn't know I don't know but in brief nothing really suitable and so they problem with interpreters so these are like hash functions down here and you know the time for the hash function says okay these the issue is the pre compiles that people really want to wrong which is elliptic curve you know cryptography stuff snark verification this is like 12 seconds 17 seconds 25 seconds 30 seconds so when you look at these benchmarks on interpreter do you think it's hopeless there's like even if you have like a 10x or 100x veto you're still too slow so this is kind of like the mindset that you're in the you know that we were in so and in all the way of the 1x discussions and between DEFCON last year and you know the like the January meeting we face this problem of if compilers engines aren't ready look we want introduced well saman etherium then how do we do that and it was a bloke a well we could start with an introvert just to get the door open so even if it is too slow to do you know the really useful stuff that you want to do at least you would have you know la semana theory om and it would incentivize people to you know teams to work on compiler engines and bring compile engines above up to we don't production readiness in your great dream client so that was the plan and then we noticed something almost by accident so just thing I can't emphasize can't overemphasize like the dogma that has overcome that interpreters are just too slow even so I asked this question you know in a discussion at the meeting in January and this this conversation sticks in my mind because as I asked you know what was the worst premature optimization you know or the theory and the design before you know the genesis walk was launched back in 2015 and so the two candidates you know what I suggested it was the merkel patricia tree because as a consequence of this large red connector no proof sizes for students clients are way too big and which i like actually responded the he thought maybe it was the IBM having a 256-bit stack size because well computer architectures are actually 64-bit so I mean um you know that's gonna make EVM slow and really Oh some credit here to critical then the you know wise long pre-beard Gandolfo aetherium for training this this benchmark and you know arguing that will actually under 56-bit workloads diem is going to have an advantage so go back to this so these two are blossom engines so I mean these are wising interpreters this is rabbit which is the fastest while interpreter at one point eight seconds and then Jeff p.m. 162 milliseconds very DBM aparna 16 and then EVM one which was a C++ engine that probably said it to break on his Christmas break which is even like 10 times faster than you know over 10 times faster than the other than parity so when you base with this kind of number I mean even it's over a hundred times slower so you have to ask yourself what is going on here and I mean the short story is you know this overhead with to do a 256 bit multiplication using 64-bit instructions it requires like something like 25 instructions so you get a 25 X slow down right there but so even when you see this kind of number you're like okay well yeah 256 big workload CBM excels it's well Samanya fixed welcome but what do you be on the stove plus or the native as everyone knows so this was the other yeah lucky lucky circumstance that led to this discovery was this this contract called wire strudel and so we're screwed all that implements elliptic curve scalar point multiplication or the N 1 xx so it does the same exact thing as the EC mall frequent file that is on mysterium already and so it's written by a zachary blame center BAM and it's highly optimized highly optimized for minimal gas cost all right actually beats you guys cost of the native precompile now when I first heard this you know my initial thought was wow the native precompile gas cause must be priced like just terribly priced yeah my initial thought wasn't wow EVM is really fast right is that ok well something up with this native freaking power pricing let's just confirm that and you know compare it with the native runtime and this was really like the the shopping result was when so it was the the combination of having this wire strudel example and having an optimized interpreter that power Oh because if we didn't have it we would have said okay well yeah that's pretty fast you know it's like 200 milliseconds but that's still you know ten times slower than native so you know interpreters are so slow even though this sort of just expecting foot when you run the optimized you should be on bike boat in an optimized interpreter you get a number that is pretty close to native I mean so in this one it's five hundred eighty microseconds and here that you know the native rust is three hundred the native got this you know is almost 100 but I mean it's even further optimized but if you compare it to like the average native and so the thing is it they could be even faster right because it was optimized for gas cost so it uses these moments when charge lead slower and EVM so if it was implemented to be fast Union would be 2x faster so it's not really be like right on par with with the mean of parity speeds so once this song end we guess they realized that maybe amateur bursts can be fast and there's perhaps no speed benefit from introducing blossom on East one at the same time progress on execution on these two is picking up and we'll hear more about that later but just motivating us to introduce the proposed yet be 2045 particle yes boss um so we so we we have you seem all performing paths and an interpreter but you know if you can still kind of say well yeah I hope this you can use scalar multiplication what anyone can do scalar multiplication interpreter can you really can you parents his parents are the most intensive freaking pile right and unfortunately I mean there's no implementation of parents in EVM so we weren't able Spencer marked up until web snark was released and so from from 30 billion in here and I mean web snark was the least a while ago with hearings were just added so this gives us an opportunity to benchmark bearings in interpreter by using post functions for the the 256 bit math so this gives the same effect as like BBM's 256 bits to excise and so when we do that though here's like in a lot of interpreter you know for half second Savannah the eights interpreters slower so it's 13 seconds in the single facets 200 milliseconds and the interpreter with big knobs it's 200 milliseconds that's very close to optimizing wasn't father would be in a hundred milliseconds there's an interpreter getting like basically the same speed as an optimizing compiler so this is just a well these are signature verification which is scalar point multiplication we already knew we could do that what pairings can you the same thing with pairings and it ends up yes so these are like very fresh we just finally got to this point this week there's like 230 milliseconds and bring to an interpreter without putting on most functions and we'll zoom in here too his in the single-pass compiler both milliseconds optimizing fire 7f milliseconds interpreter with big nut most functions 5 milliseconds native rust is for 20 milliseconds so this is an interpreter doing pairings that pretty darn close to native speed burning trying time so having to rush through here and okay um I mean so the thing about the thing about preparing it to native speed right the goal of awesome isn't to you know simply run code at native speed is to safely run untrusted code you know Matt near native speed so that's a lot harder problem so when we compare like what one is not as best as Native well yeah that's me this the native you're ranked trusted code with awesome you're running untrusted code so it's not like you really I mean native is a nice you know ideal subscribe or but it's different so there's a performance gap so that performance gap you know listen of research and I'm sure you know yes Paul I'll have more to say about it so the new reality the old dogmas interpreters are too slow you can Pilar engines well these benchmarks show that we can actually do a lot of the things people want to do with interpreters some caveats the thing to make guy you know you know here stuck a lot about the big no maybe I don't have a specific proposal yet that is something we want to come up with also 64-bit workloads that's not as fast as native you know so that's still challenge we'll see if it becomes about like that's what we've been you know prototyping things and benchmarking but interpreters even Firefox is introducing a human fastest JavaScript interpreter so not a waz an interpreter but it gets the same ideas and the cost of optimizations that we would like to do to as an interpreter they also those numbers we saw they're not even the best we can do there's some interpreter speed ups this one so here I did this in like a couple of leads and I'm got almost you know a 30% speed-up on rabbit and it required bringing like some C++ code and I don't even know like I'm not a C++ programmer so when when I was able to achieve this you know the kind of this kind of speed up on rabbit unlike well somebody who actually knows known C++ and you know his actual expert on optimize interpreters they be able to do a lot more than then I've been able to do so far what I did in breed was this just think of Sumer instructions if you research like optimize interpreters it's pretty standard stuff so instead of do n instead of taking three instructions to execute this little clock that would execute in one you know has one super instruction in an interpreter loop and there's yeah there's more you know optimizations on them on the table that are waiting to be sped up and you know have even better numbers clarification on an interpreter is compilers to know when you start talking about the you know how we're running everything on interpreters and we're really good that we're doing stuff quickly on interpreters people say well does that mean we're never going to have compiler engines no it doesn't mean that father is still an option but in our opinion they're not ready for lunch right now so you know if we're going to launch on January 3rd then I would recommend going with an introvert interpreter engine and we can do a lot of stuff with it so next steps propose a specific big name API or March other speedups metering so we know like see that we got good progress on the injection algorithm premiering but the gas cross-table that will be used is still an open question and it's easy to you know throw up any old gas cross-table but it's difficult to thoroughly analyze it so I mean even eat me out and still have a subject of much analysis and in research so we need to do the same thing and that's it so epsilon cast it back it is there like an AC switch at the door so you guys having question on the benchmarking before you leave because there's no happy so I remember two items regarding white universities in 2016 in Germany like the whole batch comes around as they also apply to give our support I mean this is why [Music] get this one so basically said Oh woloson why fight us slowly yes sorry um buffers so the u.s. uses it in a way that's not deterministic because there's yeah so they have like a time cut off and it's up to you know one of the twenty one block producers can make whatever block might see whatever transaction so they don't face the same challenges as when you're trying to determine is tickly executed but the conclusion was like maybe 256 be slow so a small team absolutely also find your x-ray yeah yeah come on take a look yes so you said that the instruction yeah so it takes 25 I 64 like 64-bit multiplications so on the processor at the processor level you know the processor would be executed using for being executing on 25 you know multiplications but the loop inside the interpreter is only one mol 256 operation and then it executes the native you know CPU assembly for these other 64-bit locations it's stiff because the email was defined with the 256 bit stack size so the mall opcode on anywhere's yeah you know it's the same as its yeah it's all I mean if it's you're running it on a 64-bit CPU then I know that you're still using 64-bit multi divisions but it's just the you know may have the interpreter overhead of having a new instruction for every single 64 bit multiplication yep yeah when it comes to in parlors versus interpreters are there any implications regarding say a constant time limitations of the current record primitives you're trying to avoid up side channel attacks most kinds of things that compiler versus interpreter haven't okay um you know in our context so the the side channel attacks in the constant time stove that's mostly your own when you're dealing with private keys you know in this context these are contracts executed on you know on the blockchain so it's not everything's all transparent there's North there's no private keys you know to worry about maybe one more so on your copy of design spec have the 256 Italian dinners sorry so like only a potato spike you have a lot of sites for this so I've done sentences there's a probably types that are just in the Watson memory they're not so you're not extending the instructions that's right you know that's fine that's very reasonable yeah so we're using two big dumb host functions rather than adding like new op codes - awesome - that way - he wasn't is just awesome alright we're not modifying Watson at all so they said it wasn't because right now we're yeah the goal is to stay compatible what want some it's just a quick recap cases talk which was really interesting so basically be if he had these two it just coincidentally happened this year that he may want to release and white students released and they showed that EDM interpreters can be extremely fast whereas he's very close to didn't and he had a decides that the case maybe wasn't interpreters can be the same then the whole point was that they don't we came to this idea that maybe wasn't interpreters community and we managed to to kind of prove that and that was a real break proof or 40 wasn't itself since it's just a recap record what Jesus said that the main and you think we have to introduce is a big Nam library it doesn't change things tracks and it's just a library and the debt we can get very close to two negative in many cases these are Puritans used by by depths are covered and if you guys have some other primitives needed you know probably be kind of a look at that many months so basically this this just leads him to do once hear that he wasn't 1.0 this kind of ready for to be released with interpreters because we have disliked fast wasn't interpreters but at the same time we also have this really fast EDM interpreter so why don't we just optimize the EDM Ito better send on excited you know we could do that and he actually took that stance that that should be the next step for it on my own people should the client helpers should have a look at the authorizations medium one they should take those optimizations and they should be balanced the prices on India because it seems um can be really fast and there so that's the FE 245 almost we did it to assemble but maybe for the next hard part so if of course these optimizations into EDM won't really happening in Iran in each one we can look at introducing he wasn't on each one but can be optimizing the benefits of blossom on each one isn't anything so that's basically realization we ended up with after all this benchmarking work now as 5th change to each of our own so we mentioned this quite a few times in the past thingy we were quite close to the internal teams and for love why but he really said that to to get coding and get more interacted with them May of this year and and e 2.0 itself needs a really fast execution and Jim and based on these findings we can start with a really fast interpreter but s compiler and Jim's going to be more mature going to be better we can't seem less thing to do Stowe's to be 2.0 so we can start with this this really fast interpreter but we are not missing out of compiler engines if they do turn out to be mature in the country so we have a focus mostly together with the the quill team does grass on the home bedside on from typing different execution designs you need to play off and they also love dance creating some of these testing tools there is one close head and we're writing a couple of execution environments and optimizing them so they're going to be a really long discussion tomorrow at 2:15 and HPD room over there done dear friends so that is really longer the session it's it it's a two-hour stuff but it's not gonna be long talks we gonna have a couple of 15 minutes lightning talks explaining all these different execution environments basically started the long interaction what Ichikawa execution is and then we go into details of the testing tool and all these different exhibition memorized so really invite you guys to come there tomorrow and learn more about it to learn but I do give you a tiny packet here just to understand the gauges do it one different question a question just a monthly six capsules and effects on the execution environments are neither like I said the intention of that kind of converge at some point yeah yeah definitely but in actually started with with scared that the goal was to get scattered to lighthouse and that's kind of what's happening but we are actually converging into to move into a separate tool entirely because things going to be much more rapidly developing so we may not want to be attached to like this okay put primarily in 2.0 because you guys need to be prepared of this in the coming months and years so contract anymore because it's just confusing but yet this is even more confusing term called execution environments well short it is you're gonna hear these I guess a lot of times there is even today some kind of defects or so this is specific to one proposal but they're more proposals as well IES are executing the charts and the proposal and these shards only store really tiny amount of data for each of these environments or contracts and the only story statement which is only 32 today if I'm gonna be clear it's actually could be thing up to 96 bits but if it's stored yeah it's yeah it's it's 506 kind of producer but yeah there's spaces for more but you know it's definitely not something you could store all your daytime it has to be just a quick summary of what you have and and basically the execution what's going on on these charts is only proving that so you have to supply your proof and you have to be verifiable proof that it ends up with the same stapler and you also supply steady supply of proof and you have to end up with the same state which is stored in this chart and next to that proof you also supply your new transactions and things you want to do and so first you verify that you actually supply the right place then you make the changes calculate the new value and sort of you make that's all that's happening on each well as these random to execution is it clear any question on that great so we have enough it actually mean to be possums we have this equals a 1 for now which is the state for the TV incompatible but it also comes about the legacy of India and we want to below which is stateless and is really just about this pure execution and then these are the final points we had on that that we can launch II was at one point oh honey tompa no but you guys can all see you want one for your contracts within these execution environments because I think everybody understands stateful contracts may be more than the understand stateless so you probably want to keep using a stateful model so you may want to use he wasn't on Twitter which of the staple model which can be encapsulated into this he wasn't - pedo sharks does that make any sense to you I think it was a bad explanation but so that's what we have now I think there's time for some questions and then we can jump into some really hardcore stuff on wasum you know questions regarding wasn't unboxing there a lot of questions but do you guys have impression for this some carriers because when you execute on the m1 it's the state that causes the most troubling terms of your opponent's because he noted on this end of day again how much impacting them performance improvements have from yeah that's one of the things why why didn't you consider doing too much on monthly know because execution doesn't seem to be the big issue it's run everything else and if this famous model definitely have this access you have to supply does it answer your question and I think it was one more there yeah just brother I don't know if this will make sense but photographer named mark team in the last few months were playing around trying to get in play contracts to be walls amendment wrong on the we both report could go ethereal impurity BMC and we kept trying to use the latest and greatest and sorted out it was really difficult so we kept going back to milestone one which dates back to the never 2018 and I wondered if you guys are done publish a milestone to maybe so that you he wasn't 1.0 stuff is a little more caught up and you know what kind of debating this for quite a while if you ever find a bit of updates on there I think we could discuss this also flying probably there could be some some updates me if there's enough demand so if you really had stand on finding distribution to the speed questions and the 2.0 step but we have I think tiny things polished before too much in it but it's me so to do this lesson for the next few years so what we're doing is like Russell and from a physician an icepick exactly there was a point to one for though then you know but it's nothing yes much support case some better the our job where's the spit example like there's no party poori yes a tsuyoi but you said you're working on a symbolic execution and engine - but you said translating EMTs what what yeah what we're doing is right now we're at the end of a fog machine the new temporary employer wasn't somebody won which means instead of Li are you guys as an enterprise and right now we're doing in 2011 but like in the next few months or so but he wasn't likely so we can actually license my projects maybe hopefully our goal was to get them ready existing smart contracts and transpiler the EPS backup it was mandala is done by C so this that way we can analyze others already seen smart contracts using your spec like the walkie wasn't respect to one per dog where's the support off spec it just requires hard work in-house additional stuff such as distance between difference of course customer wasn't the the the metering is kinda the same as of today at least the only difference is the ideas so of course in tough cases here would be to retrieve Kalina and you have to be to store and weed stuff but this is where it's really different it wasn't one point you have pictures like esto en esto and you can store into data entered data as much money you have he wasn't 2.0 you only have to be fixed size of Dave Mingus and the other big difference is that when he wasn't 1.0 you are in the natural state then you can interact with other addresses better contracts you can add anything like that you have to supply everything pretty much so my question is kind of experience yeah I think which is reading and device pain points what's your own fault the point is so public enough what's your comment thank you what are questions now we have the hardcore stuff with a problem I think that's gonna be nothing hello my name is Paul Burzynski I guess I was introduced as you know we got some parkour stuff I don't know if it's hardcore but you can decide for yourself the question is is Watson so this is a bunch of questions I'll ask this structure first I'll talk about the census the number one stereotype has bugs I'll get smooth sorry I'm sucking sample buns that Alex radiates over to some others metering is a big topic for us so is America the metering leader is slowed down according to their our interaction between mountaineering then I'll talk about execution speed which is extremely important this may be the most important thing maybe one of the two most important things on their interaction between metering there are some subtle interaction between metering and execution speed I'll talk about bytecode size in their interaction between micro sites and execution speeds so there's a lot of interactions between these times then I how I might not have time for other than topics consensus so blossom I think the best thing about Lausanne is they advertise some sort of mathematical definition and they have some proofs for something called type soundness they claim that there's no undefined behavior they have guarantees and that's very important for blockchain the verifications people like this and they are there it's very sort of constraining what you can do the control flow and things like that so these are three books I won't read them you can read them yourself but there are things we can do with it there isn't determinism it was not at the back so we have to get rid of floating points the host functions might do something strange and there are some system resource limits that we might have to fix upper bound on things sort of consensus you think wasn't is great for that's everyone's favorite topic my favorite topic is it's funny too you know so I define a bomb it's an input to a lots of managing what you saw some times our size limits so when we say this is a bomb we have to say this is a bomb with respect to aetherium size and time limits so it's a bomb with respect to whatever an example that out said earlier it's a JIT bomb we don't break wrecking bus tested in generated a twenty kilobyte Waze invite code which took compiler from v8 2.5 Sciences compile this is a bomb because in a theory and we we want to have up to whatever 24 kilobytes and it should take whatever is a short amount of time under whatever 20 milliseconds compile so this is this allows so this was started at you know the whole chip on discussion this is nested the reason I'm showing this is a quasi code actually and a sort of pathological this loop nested loop nested message mess invested as a lot of nested loops so maybe the compiler has some trouble actually it does have some trouble because this turbo things for this turbofan compiler has some signals an algorithm that creates a graph and then there's some computational blow-up I think it's quadratic and in a number of loops so and I think we can make it even worse this is just fuzz tested 2.5 seconds but who's to say we just make it you know just so many nested loops it might be you know tormented exactly rewarded but this was an important example there are other buns it's a salt one but the the binary format that there are two formats there's a text format for web assembly and there's a binary format in the binary format we have a shorthand notation for local so we have a function it has it has an arbitrary number of local variables and in the binary format there's a number and the text format we have to say I 64 by 64 for every single one so if we wanted to find whatever a billion locals we have to say I 64 a billion times then the binary format assembly there's a shorthand notation for this so this is what what was something my Filofax format you can Sanchi that you can you know execute it it's just a module with this function enough and it creates 1 billion 64-bit locusts so we have to somehow be aware of this because if this module gets uploaded to their having starts running in them we had to this function then we try to instantiate 8 gigabytes or whatever there are this good even worse you can recursively call it something and you know it gigabyte z+ thank you good bye teach names so we have to be very careful with things like this and these things are solved with the binary Department I think I was the one that found this this one another problem that so this is a bound with respect to any size limit so that's gonna crash our computer there's a recent paper that executes statement this is an EVM bound 8 million gasp in 72 seconds so it's using a lot but there's certain things I think Cobbold said that's it's it'll be easy to fix but there's some somehow codes that are not optimizing since of the etherium because sent me two seconds unacceptable you can't wear any when it's one second for anything luck execute so this is something recent so we can't now we have to somehow meter that's really it's metering so we have to meet her and such that there's no chance that something like say two seconds so this is a segue to my next topic metering I think most people know this cost and somehow know that each we assign a cost to each opcode and then we execute and then we subtract from some whatever and when we reach zero we revert everything depending we were familiar with aetherium yes the idea is to do something similar use this cost in some model for web assembly as well we have one optimization so this is algorithm we inject we inject the cause I think a lot of people already know this but for those that don't we inject these use gas calls into basic blocks the basic block is guaranteed to execute all the way through so we inject above it the contract correct for this whole basic block and then we have a call to use gas but this may have a slowdown there's a there's a metering slowing I mean people will start asking well you know sure you have the best fastest engines and things like this but you're still calling use gas for every basic block so what you know so there there is a slowdown there's a concern for this there's some execution cycle so the metering slowdown we have the pre compiles for even for theory m1 I've written and web assembly and we found that they have between 1.05 and 2.4 times slow them after we're running with metering in junctions so at the worst case we have 2.4 times slow down now Toros 500 times slower I think Casey caught that it was it's just a fly have laughlin nested examples there are nested blocks and there are how many nests of oxen 300 nested blocks so I decided in ellipses but you can imagine this is just so many Destin Blackson before each one these are this website but forgive me if you don't know if it doesn't look familiar but there are just so many at least use guests calls that the runtime is dominated by it so there's the 500 times slow down in the hole you know execution because of this use gasps sweet this is unacceptable so as a programmer I don't know prova contract writer and Iggy writer you have to be aware of this if you're generating code that is dominated by use guesses that might be a good idea to write long basic blocks and don't take so many branches if it's impossible so Casey what Casey's idea was to lift he calls the super block I caught lifting he lifted all of these because we have a guaranteed it's a kind of subtle because sometimes you could read if there was a loop in here or if there was some branch within one of these but it that wasn't the case it was just nested block after nested block so he can pattern match at metering tag so that's a place hang and he can lift it all so that there's just one use gas instead of 387 of them so this is a huge optimization that from 500x slow down which was dominated by this use gas it's a two-point Barcelona but it's real the metering slowdown is real work we have some other ideas to improve things I mean it's a big deal for your bottlenecks if if you're doing a loop you know each time but for the I guess business we call business object it's no big deal because it's just you know you you know at one at this token balance subtract this token balance that's no big deal but for this sort of crypto that's nothing sort of these expensive parts of your code it's good to be aware of this metering Slonim there's another idea loops if we have guarantees this is an awesome quote as well that inside of each loop we use this huge guess but if we can do some static analysis at the ploy time that we have this sort of loop variable that we increment if it's once it equals five we stop branching and then if we if we can notice that in pattern match this use gas you get lifted some sort of similar idea for metering lifting and this is actually you know realize we have examples working examples for cap check with this so catch hike has it operates this Loup and I think it's twenty four generations so you lift it outside so that we say a breech-block 24 iterations per block there are many blocks many more but the problem is I don't know if we want to do this pattern mansion what if there are two different patterns that collide with each others might cause problems but it may be in certain cases that are highly used we might consider doing this and it might have a cost you know this isn't freeware pattern learned and preparing lights and things like this but there are there are examples and we're working towards removing this but as a again the perfect writer to be aware so there's another optimization that we we did this use guess is call to use guess it could either be some sort of external function what we call host function that gets color that goes out where it could be inside of that tool it could be wising code that does this but depending on the engine we found that doing it one way or another way is faster so that helps the metering slow down as well but that's not the implementation side and I don't know if we're gonna circular problem that's that all of these optimizations being done by the developer before to point none of this is named I'm a bit small so some of them are some things that the developer sorry attention I said that's why I was just lying to clarify with you know it's the ones that are all being done by the developer like there's not a huge worry 20 different optimizations best one yes that's yes so I'm talking about both the question was are these developer optimizations or clients size so a client developer opposition or a contractor optimizations these are both these are you know have to be implemented by the mine developer but the contract developer and you know experiment and play with these things but for example this one is optimization to Boston engine itself yes directly is circular these guys holes are injected by the that light let's have the ploy to him okay sure but if Johnny they're not being certified for maintenance okay all right new clock just to bury that so we certainly thought put into using already existing optimization matters like LLVM has a lot of these kind of hoisting primitives and constant computation stuff is that something that if you're thinking about building on top of or is that too much to ask - so is this related to speed or is this related to recycle optimizations so like these look very similar to optimizations that the pilots already performed so are you looking on top of a framework that already exists that's such a framework I don't think that I was not interviewing I will be able definitely sound like this okay so sure if there's an optimization and I'm not worth these optimization passes but yes I yeah I think you for tip should just really optimizations are being it has to be part of this fact is all the client s exact same optimization exactly there's consensus so if we have some pattern matching there must be consensus exactly which pattern matching there's you're thinking of doing that's one option okay we're committed to other other cost metering yes yes yes yes yes the idea is that the metering will actually be the metering injection and the validation and things like that will be done in wasum and that will cost gas as well it's like sorry to discuss that so earlier you mentioned like learned some things that was them apply back to UVM and they would get it back to you Kamau feet multiply home any thoughts about you know doing some of this super block metering or metering poisoning the stuff implementing that you have one clients nothing tomorrow and if you did premium stuff like accessory one more question no I'm nowhere acts Brown does but I've always been under the impression that like any pay-as-you-go like you went once you actually won by code then you put the gutter but with this kind of love the optimization will that be a promise say hey I haven't gotten to the point where it's Sarah but then the gas is ready so the pattern matching includes there's some ellipses here that we have to have guarantees that something doesn't happen we can halt her home but there are some thing kind of happened maybe expensive in case you are using it up you have access for every hopeful but and people to just and stable and that's it you do not need well you'd want to put and I would be even better yes that's the first version and we do that we can do that as well and everything will be correct then someone will say well if we have a use guest and we have a function call if we have a function call them and and inside of this function you know we have a staff framing nothing screen and then inside of this function we have a branch which is which might happen you know there's french conditions so there's a pipe where I installed possibly everything so there was someone after doing it so this is an optimization week that we do it over the whole basic authentication a bit more but I think of some media mentions do it this way that they do it / / out code I think Pavo that tomorrow Pavel will had this affirmation compensation for the basic block and I think it is even possible for prepping you have access on the C++ book or heavy equipment to just a lot so you can tell him about Darren that's it yes for implementation for just hacking together and go to sleep it's perfect yes thank you but I don't think it's faster I think it'll get slowed down I think there you can avoid the whole stack frame for calling a functions with that like yeah I don't need to both of us so when this happen here's your right so if you have in the interpreter loop you have a jumper or whatever you go to and then before each one you actually have that actual line of code or whatever or in the whatever if you're jumping it's an interpreter prop the part where you know guess the next topical things like this you can just go over there yes that's correct you don't have a function right sort of an inline function maybe you could think of it does but still you're doing French the conditional branch that's still not free they're being good you read yes if you're not there's one was an interpreter which I think by the Wassenaar people who they decided for the practice the metering of the interpreter itself so just like it's done comedians as Sedaris there's a there's a the Wasik interpreter which does to be trained in the interpreter just like EDM engines are going and i think that's an option what we want to to do here is just step set some minimum rules which can be either implemented in an interpreter if you wanted to do that or they've optimizations like the panel is doing or just rely on this between injection to only use it with lots of engines which are not the graduated one note if we have consensus that we use gas has a charge we have to you know gates it for each basic block and we have consensus on this charge then that has to be taken into account to us or have now just mentioned this blueprint metering and we have this we have an example of us ketchup we went and catch a 256 that blossom and this this independent axis is input length 8 bytes so we created inputs for kept check that are zero bytes line 1 byte long to wait long 3 bite long all the way up to 6 thousand bytes long this this connect function is passion and the pending actions axis is the number of blossom alcohol executed and this is experimental data so we planted it and you can see it steps so that you know the cache functions to operate on block so you do you know update block up they block update Lex you're getting the next 136 bytes here been twiddling on it that your 36 bytes block block block and then finalized experience so you can see that there's there's a step like the stop function at every 836 more input bytes and this has a nice structure and for this we actually had a movement we have a Python program that was like 3 oh that gave you the that matches exactly this experimental data but we can only do this for certain programs why why can we only do this for others they undecidable yes undecidable various moments are very funny that i think it's maybe the most interesting thing that's we you can't decide if I give you a lot of love you can't tell me I know you can't tell me whether it will halt to begin with so if you can't even tell me if it'll halt to begin with you can't tell me how much you guys know constants so in the imaginary case we have no chance that this blueprint stuff for arbitrary code that's given to us look for certain examples of my structure we can have some formulas and I think Gregory was and I think Everett about this as well that you can maybe automate this process of getting this gas formula but sometimes maybe this automatable process might increase in you know the gas formula might be so huge and computing the gas formula from this formula might be more expensive than just doing the injection so I think that the guest formula if there's some pathologies that can grow late you know exponentially this formula can be really fast in a number of I mean all these different branches and things so it's very funny the halting problem we're limited by you know laws of the universe for this one this is like symbolic execution yes there's it's really it's about executions but that there's limitation you know that you blow up - so this is all this costing some meteringmode but there are other earring battles one which we're using right now which is here's my pre-compile here's the gas I proposed and then we say okay you know that seems reasonable and everyone heart calls it that's in one metering model that we use that's and then the bottom one consensus metering is what I call it link parity wants to do this life you know on deploy code there might be some consensus but I think they have a certain you know number of there's a finite number of this deployed code and maybe some consensus on metering so they avoid the speed or slow down by having some consensus to begin with they made a human being to propose this and you need human beings to you know cooperate with each other and there might be some problems with that another metering model is this is fresh so this is a new idea counting cycles is what it's called so someone would think you know this is costing some you know each our code this isn't really how computers work you know they don't take one app code to process it and you know finish processing it and then take the next opcode you know process it while the electrons settle the gates and then they you know finish pressed it's actually you know there's tons of hardware acceleration there are registers pipelines branch prediction multi-issue order execution speed so just go there with hardware key factorization memory fix multi-threading and more of these things so why not just take a we can compute cost by counting cycles executed on open source instructions are so people implements instruction sets for example x86 there's some definition of this and some parrot language may be very loud how many people took a computer science Carson this is obvious okay so yeah so we'll actually have either physical Hardware maybe a pair of lager VHDL whatever is some very large file and we'll have a cycle accurate simulator and we'll execute the code and we'll count the number of cycles then the contract writer would take in second ok so this multiplication is going to take four cycles then I'll hide its latency by doing some things on the ALU and things like this so those are people those would be free and the card metering model doesn't even take into account these things so this is sort of a more fine-grained metering the metering models so this is something freshmen it's just being Colonel techno so next when is speed what are the bottlenecks is the first thing we have to know the current bottlenecks are io at least for f1 the disk i/o and then AC Network IO expensive trick though KC just talked for a while out populo a lot expensive crypto our focus is fast crypto so this big numb I'm just in the cold for everything so these are all the proposals that that are possible all at one time maybe I should go I kind of want to jump around with them because I don't know but I think I should just go an organ just for this sanity so compilers single passed the Firefox and Chrome has one there's another one called web which we found some some boxing and we fix there's some there's another one called that Guillaume fixed by the way on our team maybe there are more bugs lightning is a development by parity and the next task is to work on metering helping me meter compilers I'm in subtle because this costing some model is works well on interpreters and people wonder in with less variance because there's an interpreter and then there's some you know get the next out code you know you go to or you know switch whatever dude says some to some you know instruction and execute it and then you know have to obviously with with compilers we're doing all of this stuff away and there's hardware acceleration things are hidden ly latency is hidden and there's a question how well can we neither it's we hope we're hoping that we can meet her it with this costing somehow there might be some variance and that might hurt meter so this is a subtle interaction between the last set of slides that section which was metering and execution speed so so that's what we're working on that's what we're starting to work on now as metering and we're hoping that compilers will be ready in time in these metering will work well there are some other proposals the fastest subset of blossom or I don't know maybe I should stop for questions along the way in smoke or there may be some fast subset of us might just read about microcosm that doesn't like something about blocks being able to leave something on the stack blossom because this is something about wasum so if they don't allow that then you know they built this whole compiler this whole like you know intermediate stuff and then they said this one these few parts are just so awkward to do if we just exclude them they will do things much faster so they did it allows you know that are easier to compile and some better optimizations and then there's an so we have subset of Rossum and then media fast superset maybe I should have said a fast superset of Awesome but there might be some annotations our compiler hints Donald Knuth you can read this close yourself but there are automatic you know fully automatic here take this cone and pilot it sort of has limitations and then the idea that talked about is which should give compile maybe even interact with the compiler as it's going so I can make some better optimizations so there are some proposals to put some annotations it allows people to give hints to the pilers making the post functions I'm just repeating but we'll set a few plants but this is among this execution speed proposals so the thing them is the big bottleneck by far the multiplication but the big number is is definitely coming even with compilers compilers are too slow for the for some you know things like this even so it would be better to hand right hand write it there's a there's a fresh idea I just want I have to say that I'm biased because I'm the champion of this idea so everything I say to my might sound really excited but other people might not be as excited as me so please don't you know interpret this so it's called universal assembly motivated by qqh ASM go assembly this great big packet filter and then this annotations proposal motivated as and the proposal is for all the pre compiles all these bignum functions to be written in a universe or what is it universal assembly there x86 arm verse 5 omits all these sort of instruction sets there's their similarities to them and these you know huge as we go somebody already noticed this we're not doing anything so the proposal is to hand rights in this sort of a universal something language that applies to all you know all of these instruction sets that's right I'll be picking on post functions and then we'll meter them so the interaction with the metering stuff will meter them by counting the cycles so there's a benchmark okay so there's a there's an example in a benchmark multi 56 this is just for one limb of the whole commutation this is in walk in the wasn't syntax of this Universal assembly this isn't max 86 syntax this is just register this just you know assembly hendren multa arm and risk 5 and the benchmark is in this universal assembly 660 milliseconds to do 64,000 iterations of multiple 6 but was native we had 64-bit we had 15 milliseconds so why the floor floor XD no we had 64 bit multiplication and add with carry and the universal may not be able to support it but Pavel gave me a good tip that all these instructions tests not blossom but all the other ones do support a 64-bit times 64-bit - 128-bit so with that I'm hoping that these are going to be much nicer so the universal assembly went competitive and the beauty of the universal some please there's a lot some syntax to it - it's awkward because it wasn't as a stack machine so you have like you know instead of mul you have a mole local get right this register will get register and local set some other register so you're doing the same thing as this mole in arm you're you're multiplying two registers and putting the results in another register but this is the first time that we can you know have black people hand write you know code and this is the proposal is to do these big functions in this syntax and then so the pre-compiled to make the whole tree compile system fully automated so if you want to do a frequent pal give it to us in this you know while some syntax or one of these syntax will meter it automatically in the psych lab here at simulator and it'll I think it'll simplify the the paper path of puzzle process okay so if you're a programmer this may be is it this sort of more you know I'm a site of the implementation if you're a programmer bossom compilers are still being too so you know getting better and better but you have to be very careful in that you know with resources I think the best thing is the invited people are there any people that are interested invited systems yes so when you're writing into smart contracts things like an invited systems keep the person with your finite resources in your cursor every cyclist is practicing like this I think because the compilers if you do like all this abstraction and it's those things now so there's a concurrency proposal charting itself is concurrency in itself but then there's also I gave a talk last year that for paralyzing several transactions which are independent so if you have guarantees which in the Salus model we have guarantees is there we could have guarantees for passing all the addresses we can see that these two transactions are independent of each other they don't touch the same things so we can execute them concurrently I conf I'll shoot by weekend's inside of transaction there's a new proposal in Washington and Matheson's dangerous there could be race conditions and deadlock see if there might be some currency which we can use so we can allow but there's still cost there's a spawning you know you have to do a system call you have to you know trade up in process we had we have everything has to be metered everything that is not obvious and then there's some you know contact switches and instruction system scheduler and you know some strange things can happen for example you know if everything is waiting for you know you know a bunch of threats - you know there's some block that waits for all the rest - it's all right and then one is context switched out we're waiting for the slow pull thread so what that's a serial partner there's um bells law so what was supposed to be 8x speed is actually like one point five nine speed-up instructional with this Universal assembly we can hide latency multiplication of addition you can have it too free and you know that's a lot that's one type of concurrency and then there's an optimization for the plant developer a startup costs you can leave the module instantiated and no need to read the revalidated you just validate it once Michael sighs so this is the last sort of sequence of things and I'm gonna rush through it just telling a story so by Michael signs is very important because now we're in a stateless but everything's a Target stores out there making enemies this sort of immediate change that's one of the proposals so Blake to be I took the reference implementation and CI compiled it and this number number of bytes on the left of some thousand one-third bikes for Blake to be referenced at Lawson then I looked at it as I said wait it's enlightening aggressively and it's unrolling loops so I told it thank you Miss Flags not to so we went through pretty lights huge improvement and this is huge because unchain you know if we're gonna store this this is just the master it's much smaller but then I used that I can do better with compression and there's another so we got it done another 2x here the best compression and then there's another idea split stream compression which compresses that's splits the opcodes from the operands from the immediate and then the outpost can be you know better it's all sort of the best one is dictionary based but the problem is these these medians to the opcodes are just arbitrary values and they can be whatever so if we split let's bring the outpost themselves from the from the medias then we can better press the the optical experiences so it's called split stream compression and this is a proposal for the General Assembly you know this is more general than he was but I don't think anyone's working on that right now and then another another proposal is on the clients we might be able to recover Watson binary from from instance so you right now maybe you've got the store both you have to store the binary for consensus reasons and then you have to store then stay module but maybe you can if there's if we boost certain things we can restrict the binary format in a few small ways then we can jump back and forth we can recover the binary from the instance we can instantiate us we only store one of them that both other topics I'm running out of time anyway so there's some feature proposals of multi-member will be great for us multi return to be great for us but that's that's going to come soon I think that was recently found the champion the multi man global raise I don't think that's yeah remember resize the page size is 64 kilobyte that's too big it feels much smaller of the about address contracts you know you might want to share you know you don't want to be able to Blake because it's already on the change so hungry share code so people don't have to be upload things contracts call each other you can import the function you can have table which is something that it's really a two function pointers but we can selections are using this a lot of people being though their post functions so you know that available is function called call not you know call contract so we're going like beyond wasn't how to communicate between contracts you have arguments returns shared memory call data returned in us we're going through the post again we can have some shared buffers all these things are being discussed I'm just I'm just sort of getting you sort of a sketch of logic team is doing a lot of good work I just want to leave you know there's all these all these different concerns all these projects going on you want to build the best system house and then no parity we're putting up with most people and we want everyone to share infrastructure so you can compile your stuff and then when annex black sheet comes out you can deploy [Applause] 