they want to contribute to something I know enhance in some way it's really good though Alex still love it it's from GitHub great yeah ERS are welcome um and then of course like the users we want useful apps utilizing the likewise so what are those be nice to have some Minds collaborate on that what are these apps and what are people building these are the kind of things that we should get a little bit more coordinated on and we'll try to make more of an active effort to do so to really push this forward so that's my Twitter up there if you want to get in touch actually I forgot the underscore there's an underscore there that will lead you to some other dude that I don't know yeah I lost the nft on that one uh that's it and um any questions at all otherwise I'm going to bring a ton up here he has a few slides that just goes into a little bit of the protocol um and with some proposals yeah yeah technology okay all right we are back at the left of that bird um I put a quick uh recap about how the Leica and protocol Works um so if we have a beacon block from uh Beacon chain it has two signatures the proposer that signs the main signature but that's when we cannot actually confirm on the like client because we don't have the data like we would have to verify not just the signature but also all the attestations so it's like several gigabytes of data that we would need so what was added in our there is this additional signature down here from the sync committee and this one actually signs the parent hash so it's like a block before this and the like client then trusts it with more than two-thirds of it sign the same message why two-thirds um it's sort of arbitrary but the main Network operates under an honest majority assumption and the string committee is just sampled randomly from that so the same security guarantees apply to it as well this is what the like client data looks like it sort of describes a set of three different blocks the main block is this a tested block that is the one that is being signed in the signature block using the sync aggregate and then it also points using Merkle proof to this finalized uh locked up up there and this allows a like client to follow both the finalized header and the latest optimistic header so this is the chain header and this is the latest finalized block on the network these structures look like this it's like this a tested header um and then it has Miracle proof through the to the next twin committee and the finalized header and then the overall signature there is also a couple smaller objects that describe the same thing but don't include all the information because for example next to in committee only changes once a day and finally only changes every 12 minutes or so and then there is also this bootstrap structure down here to get the initial sync committee from a trusted block um the entire protocol or a like client then looks like this you start from a trusted block route that can be for example the merge transition block and that would be the first question can we sort of get the clients to ship a agreed on socially agreed on checkpoint periodically like every four months and from that and the lag fan can download the initial zinc committee and then follow the next thing from it is using those like lined updates that we have seen before and from that you can obtain the latest block and the latest finalized block and in there is the El root hash against which you can then run like proof all sort of groups of the entire ethereum so this has been standardizing rest and in lip P2P those protocols have been merged into the official specifications Nimbus is implementing them like it's working there a load star has them I think mostly done and uh Tech who I think opened an issue to start implementing it as well so progress is going fine and then there is also the portal Network which is getting more important over time as it also stores like historic data for ethereum it so that's basically where we are today um I was wondering is there anyone in here who doesn't know about these structures yet or is it mostly people who are working on like lines already so that is for doesn't know the structures yet okay so so most of us know the structures already um so there are a couple problems that are still not solved yet this is a relatively new protocol and the first of them is for the security and technically if um if you assume that only validators who exited the chain normally and sign conflicting messages then it is safe to use four months old data to stay in sync with the network so if you if you start from a checkpoint that's four months old and then sink forward that's still safe according to this research so the problem is right now there is nothing that prevents a student Committee Member from just signing multiple chains at the same time there is no slashing defined and it's also kind of difficult to define a slashing like if you want to slash on any non-canonical finalized block the problem is that if someone sings to an incorrect chain or even if someone signs something that later gets orphaned it's uh becoming a problem and so we can make this a bit weaker and sign only if you um slash only if you sign multiple finalized headers so so you have to sign the correct chain and also a conflicting chain and then we can sort of Traverse the history and see how you signed two blocks from a different history that are analyzed according to you um that's also kind of difficult because right now slashings do not have this sort of History so it maybe it needs to be bounded to a day or so or two days I don't know um but what could work is to slash on signature of conflicting finalized texting committee so this chain that we had before here if you can break this you can basically send someone to an arbitrary chain and with this slashing we can at least prevent that so if someone signs um to the correct chain and also signs to a conflicting chain we can submit Merkel proofs that tell that this is on these are two different chains and two different extreme committees that are according to you finalized so it's statement so then the next problem is um this data structure it doesn't contain anything about the El yet but many use cases for like clients actually want to prove ethereum data that is part of the state so we have to embed it somehow in here and for that um there is a couple use cases for example the top one therefore Choice updated that one is basically instead of a beacon note you use a like and to drive your execution layer um right now it's not possible because the block cache is not in there so you still have to download the full block which is like two megabytes per 12 seconds um then the results of the problem that get right now I don't know about the others they still required a new payload call and a new payload also requires the full block but I actually talked to yes yesterday and they actually only only need the block hash the block number and the parent block hash but um that's more like a it would be very difficult to engineer it the other way but technically it would be possible to also just start with the block cache and someone else was like mentioning that maybe they need the entire execution layer payload header like everything instead of the accepted transactions yeah and then for all the proof um endpoints for example if you have a wallet and want to prove that you have a certain token balance you need the El stay true which is part of the block mesh as well so question here is how do we want to incorporate this information into those update structures for the laction I have measured um the sizes of those messages so right now we are at 280 bytes for the most frequent message that is happening on every single slot if we were to add the full execution payload header it would basically quadruple in size it would be a kilobyte and if we only want to put in the block hash right now um it's still like a doubling of the current size because the El block hash is rooted quite deep in the block so we have like the beacon block and then inside there is the beacon block party and then there is the engine execution payload header and in there there is the Yale block cache so one suggestion there it would require a consensus change as well is to just move that El block hash to the beacon block header this way this structure can be kept around roughly 312 bytes um but the challenge there is it would be like the first ever addition to the beacon block headers so no one has done it yet so it's probably a lot of work to make that one work aware the reason why this message should be small is that light clients enable also use case for real world devices for example internet of things so you can have for example at door lock where you push all these like client updates and any proof that you can have a certain nft in your wallet to open the door and that can happen via Bluetooth or via Ultra wide pen and there you really don't want to have very big messages so yeah not sure which one is the most simple here to minimize size here if it's important to Upper discussion so then the third one is the engine API that one is the API between the is between the beacon node and the execution layer how it informs the execution layer about the latest head right now the spec says that if you passive work to is updated within hash that the execution layer doesn't know yet but it is sort of optional that the execution layer actually does anything with so proposal here is to just make it a shoot so that in general it should be okay to just use Fork Choice updated to sync and yell the El already has capabilities to download blocks because when you pass hit the latest look at then it can download all the parents from that so the data is there it just requires more engineering and then also for Accelerated thinking it would be great if the deposit contract would be prioritized during the sync because then you can sort of have a situation like this where you have like the optimistic sync that sinks forward and then it jumps into the light client data and brings it all the way to the wall slot so can be multiple months of data that is skipped there but um then you have this like client head and you have the latest talk but you may not have all the validator keys to verify that so if like currently we have to pass that block to the El and then wait for it to sync it's still faster because the El already knows about the latest talk um but then we have to sort of import a deposit contract all the logs build up our set of validator keys and then we can sync backwards and validate all the proposal signatures because they are not part of the block hash so yeah those are the challenges that I'm seeing right now with light clients that should be addressed in the near future to make them more useful most importantly it's this uh slashing and also adding some some way of notion of a block cache or a execution payload header into the black client data so yeah from so is there anyone here who wants to do some zero knowledge stuff like who is working on zero knowledge stuff do you want to talk about it like light finds your knowledge okay yeah the portal network is uh is an important step there as well actually we from status Nimbus we have a ongoing project where we try to make a zero knowledge proof so you can get from any point to any point in constant time and we actually have something that's working using a recursive circuit um but uh yeah not sure about the details I mean the problem is complexity wise you still have to validate something of the complexity of a PLS signature like the pairing is still there so for the embedded use case you don't really save time there but apparently the proofs are quite small in size I don't know the details there maybe or who is next hey thank you Phil and Eton uh I don't think I have too much more to say I think now unless somebody wants to just like relating talk situation uh you can just move to like more like smaller self-organized working groups uh we've kind of did this uh past events like uh Dev connect this year did like you know some talks and then just sort of like self-organized working groups just to get like people in the same place you know tissues uh that's some things I think are probably some great jumping off points and yeah unless there's anything else I think we can uh just move into that phase of things yeah thanks okay maybe I'll just let you self-organize I don't know if it's helpful for me to assign whiteboards but um foreign [Music] foreign foreign foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign foreign foreign [Music] foreign foreign [Music] [Applause] [Music] [Music] just touch [Music] [Music] [Music] [Music] foreign [Music] [Music] I'm on the edge [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] thank you [Music] [Music] [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] thank you [Music] thank you foreign [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] foreign [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] okay okay [Music] [Applause] [Music] foreign [Music] [Applause] [Music] foreign [Music] [Music] thank you [Music] [Music] um so um just a bridge sort of like what happened before as this one is basically just like a continuing session to what happened in Osaka where we thought that we will almost die after typhoon um so basically that was like proof of work to prove of stakes or like transition session uh where we hosted like uh some Community sessions as well uh alongside the E1 and in each One X to E2 in these two sessions it was kind of successful in a huge room where vitalik went on like a huge session and brought like so many research posts of like that inspired the discussions that happen uh during the session um and then um just to give you a quick overview of the um the roadmap of what we've been discussing uh like just a few points uh mainly we've been discussing the IP 1559 the fee Market change for ethereum uh which was a very hot discussion and also the proc pow which we agreed uh uh that sort of a conspiracy driven aipn we should not keep much of a like we should not like pay much of attention to that as that's very conspiracy driven then we also started discussing about State rent um as uh we realized that many people are using blockchain as a storage um so we were discussing uh that there should be uh rent free that there should be some rent fee for the providers um it also will kick out the ideas for stateless claim uh with Alex and Piper I'm not sure if they are there but um they started um pretty much discussing how we make ethereum stateless and then they follow up the session in Paris after ECC in 2020 which was the covet session as well and then we were discussing also Harding uh like how ethereum can like um how like hearts um like The Shard design basically uh and then the ECM transaction history archive which is basically about how we can keep the full state of all transactions and like archive notes and all that um so um you guys may be wondering why everybody's sitting down even I'm like sitting down because I don't want to be like too much like oh I'm like important and you guys are not uh but basically ethereum Edition sessions are always like everybody's sort of like on the same um same like line or something that like there is no such thing as like oh these are the cardiffs or like oh these are the speakers and they're important like no like everybody is equal and everybody is um as important as whether you're a speaker or you are sitting in the audience uh and I highly encourage you to join us anytime in this like how Circle um so if you have like something to say to this session or like to the topic that is being discussed feel free to anytime uh switch like anybody just like take their seat and be like hey like I want to like say something to this uh and even grab a mic um and I also encourage you to take some notes or some like ideas that you have we also have a marker would just like thank you so much who's gonna take a note from this session and then also I forgot your name uh but uh there is a guy who is going to take a note in Spanish so we are going to have a translated notes uh from this session as well uh and make sure to publish them on ethereum magician's Forum um and then I just figured out that this would be a cool quote um to my kick off this session to like make ethereum more Diversified um as I feel like we need more clients um to be run in the ethereum ecosystem and we should not be um just focusing ourselves or on the main two clients and by the way the QR code weird until let the QR code be there and that's where you can find the slides and in the slides there are many links that what I've been just mentioning during my introduction speech and that's pretty much it from my side I'm just going to try to encourage you all to like speak up uh and like don't be scared even the mic it doesn't bite you and it's just like our friend and if you will try to if you want to like speak up about something we want you to like make sure that um everybody will hear you basically and that's pretty much it um I'm excited for this I'm just giving the words to Tim and let's kick off so um yeah thanks Annette for the intro um yeah so I guess on the point of people asking questions and contributing I do not have near three hours of like contents here right like so like you know I have a couple questions to kick things off but like this is very much on you if uh you will have stuff you want to discuss this is the place for it um orobial here in 50 minutes um but I guess yeah there's like three things on the slides I think uh or needs to discuss with the people here um first is just like how the merge went it was a pretty big piece of work um not only technically but just like making it happen from like a human perspective we have all these teams working together so um yeah I'm personally curious to hear just from the different client teams and folks involved like how that whole process went is there things you can do better for like the next one or things we did well that we didn't expect to do so well um after that like we keep talking about like The Surge Verge Purge Splurge all that stuff um I think I just want to make sure we have space you know to hear from different client teams about like how they see priorities there what they think is important where things are generally at and then answer all the questions from you guys because I think it's uh easy to like look at the four acronyms or like meme names and then when you get into the details there's like many more open questions so hopefully we can get into some of those we'll try and keep time at the end um or actually we will keep time no matter what at the end um to just discuss like specific eip's like there's a bunch of VIP Champions Shanghai planning's been a bit different because we canceled the calls after it emerges to give people a break so everyone has like eaps that they want to put in um not a lot of places to discuss them right now so we'll use this as a sort of pressure valve for some of those discussions um yeah so that's roughly it but again yeah please ask questions throughout um otherwise this is going to be over pretty quick um but yeah to kick it off um I just was curious to hear from like different folks on kite teams like their perspective I'm like not like technically how the merge went like I think we've covered that a lot already but just how they felt during the process and like you know now that it's over and and we have a bit more distance like looking back how do you feel it went and other things that could have been better yeah I made the mistake of sitting beside Tim um I it was stressful it was really really stressful and really drawn out for a long time and I think it's called devs we've all felt that real pressure um so that's that's kind of the personal view of it I don't know how you change that I'm not sure we should because we should have been under pressure to ship and we were um but you know I guess I'd say never be in doubt that core devs feel the pressure to ship um we really do uh yeah I think there were a lot of really good things we did the testing that we we managed to do was phenomenal um and it was a kind of another couple of steps up on anything we've done before for hard forks and that kind of coordination and and work that went into that was absolutely fantastic um starting with the client teams testing their releases but then the EF support of spinning up the shadow test Nets were a brilliant idea um and uh Mario and and other people contributing to the test Suites And Hive tests and that kind of thing really really valuable I don't think we started those Hive tests in particular early enough um and I think that probably is a knock-on effect when we didn't necessarily include the execution clients early enough uh which might have been because they were busy with you know 1559 or something giant um so I'm not sure what the timelines there exactly were but it it felt like consensus side almost felt confident and ready and execution was kind of like oh yeah emerge the merge is the next thing we've got time now we can look and it wasn't wasn't a surprise to them don't don't get me wrong but I think that was a big deal that that would have helped to bring that forward and then have more time on things like being able to pay real attention to the hive test we were struggling as client developers to keep up with the sport and the last minute things we knew about and dealing with these Hive test reports that are in a format that particularly for the consensus side we weren't particularly familiar with thank you made a mistake you're sitting too close to age um yeah I I agree it was really stressful very very stressful four years um I think a large part of it was figuring out how to deal with that and how to keep performing and keep delivering good code uh not sure what to do about that um perhaps having gone through it once then these group of clients will be better at doing it again um I think it was interesting to see how they were kind of um like a few little efforts or a few people that kind of poked up and really pushed things along um I think Parry was really helpful um uh I think uh Marius also managed to create a lot of momentum so I think there's a lot to be said about um yeah just I think the right person at the right time relieving the right pressure point so I don't know what we can do to to make that happen more in the future but those yeah those individuals should be enabled when they want to when they want to do something um I think it went really well um I have been quite disappointed with Mev since um since it's launched I know there's been with some of the implementations there there's been some bugs that we that with like Json encoding that really really shouldn't have hit us on mainnap um I think it's probably because Mev um like we all kind of realize that that's a thing is happening quite close to the merge didn't quite get enough time to run with it didn't get enough time for it to be integrated into our setups um I think there's also some catching up that those teams need to do in terms of um announcements and cons so if they have failures um with their software they need to announce it they need to tell people instantly that there is a problem and it's being looked into not sit on it for some time and then and then and then publish it I'm not naming names here on purpose because I think it's just useful for um for everyone to know so I'd really like to see the folks in there same with the builders if you've got a problem you really need to be like hat you need to have channels to tell people that there is a problem and you need to because you're not a critical Service as Mev you just need to get the help see in October and it was like a bit too late to say like oh I was just doctor told me that I was really happy to actually we we almost launched in October so just the next year um so but then I came back to the like after I said that I was like yeah maybe maybe it was not really the common agreement the common sense and I came back today to the Netherlands team asking everyone hey do you think I can do that for October like you think it's possible at least do I have to go and just like tell everyone hey I was wrong and they said no we think it's possible so I thought okay yeah and well there was there was a lot of work but at the same time I remember thinking on the same day I think it was good to say it because suddenly all the infrastructure teams started calling everyone and saying hey like we're launching in October we're totally unprepared we have to start preparing and it was needed like because we needed node operators to start looking at this we needed the community to say okay there's some date for the first time uh so I miss it by a lot but and then in Emporia yes I was still still very confident uh because I I've seen the things progressing I've seen the the Ryan is hitting very fast but it was like all those small things that take so much time really in the testing in a proper testing and the confidence um and I think when I was a bit pessimistic was around April this year when I when we were thinking about Mev boost and how ready the movie boost is and how much everybody was prepared for Mev burst and it was more stressed at the time about well we have a maybe boost on time or will we have to launch without any Revis because at the time we were thinking of June or May as a launch date for Mev a for Ford emerge so everything came together around September uh I definitely didn't want people to does it say to use this difficulty bomb thing as a as something that would drive the dates uh talking to developers to our team I feel like there's really no more motivation you can get or no more pushing or or shouting or asking that you could get for delivery on time so there's absolutely no sense to introduce any other stressor so so the difficulty bombing moved slightly I hoped it won't move the merge date and we didn't so actually the merge it wasn't moved and nobody was talking about the block times but we would talk about the block times if it didn't happens nice um yeah we're focus on another side of the room to share uh I guess a different aspect of the merge um so we are like one of the minor clients uh bezu team and um one thing I would have done differently was um preparing for people uh choosing client diversity uh getting ready for them didn't realize I guess I think it was around like two three weeks before the merge timeline we saw a huge amount of people thanks to a lot of solo stickers coming through Discord asking tons of questions and I think the maintainers were super busy with uh you know trying to get the final release out I didn't get to really respond to a lot of them so I think that's something that if we had to go through the merge not really um but if we had to go through it again that we would do that differently to be ready for Community Support how do we get this uh new users coming through this ecosystem like how can we support that better um and but honestly it's definitely a hard challenge because as maintainers like you're really deep in the weeds of like like really trying to get the code out on top of the trying to support tons of users coming through is uh it's it's really challenging but thanks to a lot of the contributors who also chimed in and asked answering a lot of other people's questions on how to set up their validators and all that um so yeah it's definitely a community effort it's not just the maintainers all cortex so thank you for those uh solo stickers coming in and asking questions and really improving the client diversity and also other contributors supporting each other so yeah thank you for that um I guess one less common is please don't threat threaten us um minority clients by saying oh we're gonna go together I think that's like a stab in your like heart and twisting it uh so yeah thank you hello okay so I want to double down on Paul so I'm from Prism and um so for my team's perspective merge has always just been consensus execution and Engineering API and boom done right and then like I guess like just a month before the merge we learned okay there's this Mev stuff we have to work out now and then there's this Mev booths and there's this relayer this Builder so we just keep adding more and more adders to the picture and then even a few weeks before the merge right there there was this tornado or cash they say all fetch stuff and then flashbots would only relayer so our team had this big internal discussions okay should we drop the support right now like like what should we do if flashbart was the only relayer and they are sensory transaction right even though and maybe boost is a neutral piece of software above flash about is the only layer then as a client team we are indirectly supporting censorship and of course I'm not happy with that this this is now why we're here for right so I think like we've learned a lot I learned a lot it's just like if today relayers are producing 50 of the blocks on the main net are they considered as layer one right and I'm I'm I'm really happy to see this more and more like relayers flash balls they're joining the Oracle Dev code they're starting to participate more on just layer one roadmap like architecture and stuff like that so I do think like we do have to work with them and they have to work with us together to basically push the space forward okay yeah good points by um by Terrence I think it's worth saying as well that I think even though we did have some problems with Mev and I was pretty harsh on them before I think we did come out of it um in a better spot than we were before uh there's no like Mev guests now um and Flash Sports we're really happy to to relinquish that that control which was really cool um an mbb boost um is something that can be implemented by other people um and even as it stands it works with other relays um so I think we really opened up the world to Mev and although we didn't solve it and we're still not in a great spot I think we ended up in a better spot and I think flashbots have done some good stuff um towards that as well yeah the circuit breaker's good as well yeah so now there's um the consensus clients can detect when the chain is unhealthy and then just disable Mev um so that's something that we couldn't really do before in Mev Geth um with the like Mev boot flashbox could have implemented it but um you know it's a lot of work for them um so yeah now there's a lot more control from the consensus clients at least um into what's going on with Mev and I think that's that's a really good place to be in do they still run navigath if you're the theory layers I don't know actually it's a good question I kind of um wasn't thinking about that but yeah because I think like you know for the most part most people who are running validators they're just getting blocks from the either the Builder the relay somebody has to run something that looks like Geth that's taking the Searcher bundles together and putting them into a block format yeah I guess what's up if that that's still the case it's not it's not ideal but I guess what's better now is that every one of those gets produced is now verified by um not that thing before it's published on the network right yeah and I guess like from my perspective the the whole situation with Mev was very interesting because Mev has been around for several years now and you know we've kind of seen through the defy Summer and the nft summer these crazy numbers about how much Mev there is in blocks and you know for my perspective was I was thinking there's so much money in this industry like the solutions are just going to appear and they're going to be really robust and they're going to work and then you know it turns out that it's six seven months before we were really projecting for the merch to happen we're starting to realize that the solutions that we as core developers feel need to exist weren't in the place that we felt that they needed to be and so we started trying to like help help accelerate um that path and the way that it turned out I think like it's better than it could have been I would have been very curious to see like what in you know institutions would have created their own Mev boost had this like open source public thing not existed because I think that there is enough money in this industry that some of these big stickers would have realized you know there's no way for us to extract the movie anymore let's create some centralized endpoint for people to send bundles to and we'll just extract it for our customers and you know Screw all the rest of the stakers they can figure out their own ways so it's not where we want it to be today we want to improve Mev boost and make it better and we eventually want to move to PBS where it's part of the protocol uh and so we're just working towards that but and I think like also you know with respect to thinking that the Mev industry was going to build all this stuff the core development Community is also like very anti-f financialization of a lot of things and so to a degree it felt like we didn't want to think about it it felt wrong for us to think about Mev in a lot of cases and it was only later on where we realized how much interplay there was between censorship and Mev and the ability to produce blocks and it started to just unfold very naturally over the last six to eight months and I think like looking back if we would have understood those things two years ago we probably would have thought a lot harder about how to think about this with respect to the merge and maybe how to integrate some of this stuff a little bit more natively uh into clients yeah and I guess um yes talking about like the design of the whole thing I'm curious to hear like Proto Macau you both spend a bunch of time like early on with like the engine API and and specking out of that but um yeah Miguel do you want to just like walk us through like how do we come up with the merge like how do we how do we get here yeah hey okay so sorry like for me the merge sorry two and a half years ago like roughly at this moment in time and uh yeah by that time it was like an idea of the two-way Bridge of bringing the um Casper F of G as a frame analysis Gadget from the beacon chain to the um the proof work chain um that's was the starting point and there was an idea of like actually um transition in the proof of work chain eventually to the one of the shards that was not yet designed by that time and uh yeah it was like a long journey I can't recall everything that has happened like during this more than two years period of time but one thing that I would just like to mention here like my key takeaway one of the key takeaways from the working on the merge project is that retrospectively um what what I can yeah recall like the first collaboration with other some somebody else working on the same space was uh collaborating with Guillaume Guillaume prototype the basically the first execution layer client was not called that by that time but anyway it like made me much easier to like prototype the whole thing like the consensus interacting with the execution layer um it was not done by engine API by that time but anyway it was like yeah the old idea of like putting it in a chart and make it work somehow so we did this prototype then yeah after some progress on the merge specs and then ideas then prototype it with like renism project so just you know forced us to make the first version of engine API for the do you remember all this enormous amount of hours you've spent the managing this project and driving it through it's like I mean I can talk about that I would want to First reiterate over the first moment in time was that we developed the merge I think it was before coffee before the whole pandemic I think ins like SBC there were still discussions about ewossum but then less than a month later in Paris during iPhone x discussions afterwards eosm or like the ID of execution charts was basically over and done for um these discussions between you and Guillaume Danny me others Brandon also started where hey maybe we can prototype the Catalyst thing with the ECM attached to taku and uh yeah like that's kind of cool but I took that's two thousand twenty it took a year more than a year till we got to to rayonism so it's a huge leap already where things happened and then in realism wherever in this awkward spots where alter was still not shipped and London was still not chapped and we were just like moment in time where there's this stack development where you have to convince people that they need to dedicate resources on something that's not the main hard work to make some kind of progress definitely like there there were moments in time where I was slightly like yeah out of my humor because of the the way the the these calls would go like for a month we were thinking of it like a hackathon where okay we just do the Prototype with more than one client and Martin often it's just meant that some clients would not go and join these cars to even just get a glimpse of what we were trying to do after alter and after London and I think like this disco-free process for future Forks we can improve where that's the time and if we had more people think about the merge we could have realized that hey we need to make these communication Channels with things like flashbots and MFE because the Merit is very soon otherwise it's pretty late then you've got the new situations yeah doing MVP people are going to join us for the last session so we can discuss more that later because Alex he just text me that he's coming for the last session unfortunately he did not make it here can I continue on that yeah it's just a few few sentences left from my end okay so after any this is the first time we had like an Engaged more client developers and client development teams into that we had to written to write us back of the engine API like to make this you know some kind of standardized or whatever it was even before Emperor it was like uh like more than one year ago like yeah yeah April and May when we won yeah it was great so then Emperor then uh yeah all this of chain events uh of chains sorry of sites of of offline events it's a really huge um facility of the progress and all the um developers in one place talking to each other in person I think it's really helps to collaborate on the things even after the event and when we were like on the internet um back to work yeah and as has been mentioned tests and efforts client developers gave a lot of feedback and all this stuff I was just and the key takeaway uh for from from what has been said um yeah the right people at the right moment in time make the things happen so retrospectively this is what has happened with the merge um and I think that uh this may only happen uh this kind of like thing right people at the right moment in time may appear like in a healthy Community which we have currently the healthy community of researchers and developers and uh yeah and this like uh prove the merge is like as for me it's proof of like these um that the this community of research and developers that we have on ethereum ecosystem can like capable of delivering like huge big sophisticated projects and like the merge is like the first one right so next is uh dank charging and other things uh local trees so it's not that I'm not that scared about it after we deliver the merge with this huge success so thank you very much everyone who made it I guess I'm on the note of the community it's been like 35 minutes that's just us talking um Team don't want to give a lot star um to like tell us more because you don't start but after that start asking questions so we can like open this up hey thank you sorry for being late as we are most of the time uh we're we are the fifth the fifth uh consensus client so yeah we we actually produced our first block on Main net in November of last year but it was actually fun to basically Sprint up to uh to with everyone else and basically caught up um so that sort of retrospective was uh was amazing to see the kind of feat that we can get there and of course with the help of all the other client teams um as well we were able to pull through so basically goes to show the what we're capable of when we all work together especially when we're all focused on that one thing and it was to achieve a successful merge so um it was really great to for all you guys to help us basically pull us up to where you guys are um I guess one of the hardest things that we had to deal with with being a more later client uh to be ready for mainnet is really just the fact that client stickiness is really really huge issue and we definitely noticed it as we're going towards merge people tended to not necessarily want to experiment with a new client going up to something as critical as the merge so we found that to be quite difficult even though we you know tried our best to sort of make it easier for people you know like one of the best examples would have been our load star quick scripts that we use which is basically like a one-line command for people to start up load star with any of the El clients um but you know didn't see as much traction with that as we'd like to but um now that we're pretty much caught up going into withdrawals Shanghai dangsharding hoping to see much more adoption with that and um yeah we definitely can't do it without the help of all of our Collective Minds together so really our retrospective is um if you are an up-and-coming client um we're all here to help you nice yeah so we we've just shared kind of mostly core devs and researchers you know our retrospectors on the merge so my question to everyone out in the community is what's it been like like what's your retrospective on how the merge has gone how has it been as a validator how's it been as a adapt developer and those kinds of things someone's got to start just give someone a mic yeah thank you hi I'm not there it was crazy because in Mexico I'm trying to teach people what this vision is making while people like you are building are creating I need scar record all the terminology is very hard to understand and that they don't teach people what you are doing and guys like me that are nerds ready and all the stuff it's very hard to to keep a good Channel so I think it's day to day it's improving life for example when I start to make the threats about all quartets in Spanish a Skylar came to me and said hey guy nice what you are what you are doing then team Baker start to to share my post so thank you to everyone for all this I don't know I feel like very nervous because I think that I am with a very very smart guys people and it's like crazy but that day that night where I was with the East Latin Community speaking about the marriage what he's going to do everybody was very very hyped and we saw the panda and was okay it's done let's go to sleep so great job guys great job and thank you to everyone who's doing those translations and helping with that education we can't keep up with all of it and it is a community effort it's it's really brilliant uh this might be like a random question but can we have more documentation resources for future core developers um because I've looked around in this very hard to find and I myself like to volunteer to take that task and would love to create a documentation tutorials and roadmaps because I really want to work on that but it's very hard to find like a clear path to become a core developer okay uh this is a great uh time to give a shout out because Mario just came and he is actually one of the people who leads the fellowship program for cardiffs um like um thank you um have you heard about a protocol Fellowship uh it's exactly answer to your question are you already I'm sorry about that but you can still participate so this is the thing so like um so uh if you didn't particle Fellowship is a kind of an internship program where um anybody can come and start working on a project and uh in a way that we it provides sort of mentorship from many of folks who are sitting here and other core devs and there are also resources so if with the uh the whole uh later Fellowship is coordinated via repository on GitHub where there is also suggested reading uh there are a few things they put together but I agree it's not easy that's it's a high wall that you have to go through but so we are I believe that this might make it easier so there is uh there is a lot of things to dive into and uh the the reading in the repo can help you to look into what what uh makes you uh what you're interested in and then uh you can just start contributing because so the the calls and everything in the even if you're not accepted it's still open you can join if you are not accepted you just don't get a stipend uh but you can still come and and if you work on a project and over a few weeks we see that you have valuable output uh we can still give you this open as well uh but uh it's fully permissionless you can uh come to the calls propose idea that you want there uh there are calls where you can propose idea in an issue similar to ACD uh that I would like to discuss this and uh we'll invite some of the mentors to help you with that give you some guidance so yeah hopefully it can help and if not that well and uh we have like one of our projects is also car development as well where we we are very happy to guide you over as well um yeah we are always hiring new interns so I don't know yeah so this is like sort of like should be your base layer for where to find more cardos related or like how to be according um although that was kind of controversial where um I think lane or Amine yeah I mean it was like at least a cardos can I be a quarterback um yeah who else ever ever be a hackathon for this sort of thing um maybe like a three-day thing event where people can work on funny stuff we should do that because Chris stuff once you do one in like never mind hackathon uh but it was like it was supposed to be internal but I'm very happy to take this over and um do sort of like a chord of uh hackathon and I mean Mario should definitely collaborate on that if Thomas is okay with that because he's like my boss right now because I'm leading the internship program so just quickly I was gonna say um it's a lot of the open source repos also have tickets that are marked good to start development on so that's another Avenue it does get taken up by contributors and we review and give feedback and help and so there are a few Avenues but it is a big it's a learning curve um so we were working this weekend on the eth Bogota on some hypothesis regarding the new proof of proof of stake system and one question we written couldn't get answered was now with the new proof of stake system is how the Box the blocks get built is it still purely an economical incentive as where they choose the highest fees for the or the highest prices to include in the blocks or are there other incentives include included as there's an increased risk of slashing so they the blog building doesn't really lead to the slashing risk slash increases are on the uh related to the double signing like signing to different blogs or attesting to two different blocks of proposing to conflicting blocks or attesting to conflicting blocks so they yeah it should be economical incentive it should be a line incentive something that is very natural so the blood Builder is looking for the payout and collects the transactions that are paying the most uh plus there might be any not clearly economical not Financial but still quantifiable benefits right so Thomas is right about slashing there are inactivity penalties which are often confused with slashings which you can incur if you miss your proposal and for example if that latency is very high because they're using an external block Builder surface um so that's one incentive for people to use lower latency Services it should just be your or not but I do think that's the part of the whole me discussion and how we make homesteakers as competitive with regular stakers when it comes to publishing blogs so that they get adjusted timely and they're not missed and larger blocks obviously and they they are they cost more expensive they cost more time to propagate so that might play a role thank you I just wanted this I was just going to circle back very quickly to the question on how to be a core developer and I wanted to say that there's I think there's two things to consider the first is just follow all core devs the best of your ability I think orienting oneself around like what's actually happening in the protocol starts to make it a lot clearer like where are places that you can contribute to so doing that and then I haven't talked to a core developer in a long time who doesn't have a long laundry list of things that they would love to happen that they just simply don't have enough hours in the day to do so maybe once you're feeling more oriented with the direction of the protocol to try and just reach out to a core developer and say hey you know this is my skill set this is where I'm coming from you know do you have a project in mind that I might be able to help with and I think like once you start doing a little bit of that and showing that there's like you're creating value then core teams love hiring people so this is a this is what I would recommend all the a tiny note on that about like following all core devs um as the guy who runs it the first like six months if so it was my job to attend all core devs I just like didn't understand anything that was happening and like it was literally like too intimidated to like speak up so like if that's your feeling um that's normal and if you're smarter than me it'll probably not last six months but like it there's a lot of just like implied context that I don't know it's really hard and I felt really good about myself when Danny Ryan told me the same thing when he started I think before he started the EF he would attend the Casper CBC calls and he would just share links in the zoom chat he was telling me like you know when somebody was like oh like this paper came out he would just like Google it to be the first guy to share the link and Zoom chat so like yeah the calls are like hard to there's not like a good way to dive into it I haven't found somebody um so if you're like if it feels weird for a while uh that's that's kind of normal um I have I when people ask me any questions I always throw in these are random ideas I think this is a good idea just be extremely obsessed with like eat Research Forum and to a point where that you read The Forum one two three times and then take notes to like like basically where the notes that you can understand then you can transfer notes into blog posts and then you can share the blog post to people and then but but when you're able to do that you you like you can actually be fairly you can't actually be very knowledgeable at that topic already which yeah and then other things like just just podcasts in general like just like pay attention to like quartet podcast and then take notes on that and then also share your notes because people will actually appreciate that because not just not a lot of people have time to watch like YouTube videos and listen to podcasts and stuff people will prefer to read notes and just make sure to share everything on Twitter that's where that's where the hype is and that's like how you can get involved and go to events and just like speak up or even take notes and then like even DM to people that's like how you can basically get started also great way how to get even closer to the people in the ethereum is just volunteer on a conference that's basically how many people in the ecosystem including me started and I feel like that's the way how you can get even closer and whether that's like volunteering on the conference itself or you can just even volunteer on a calls and you can like find like a client calls and then just like hey like I want to take a note or like this is my skill set and like this is how I want to help you or like I found a Bunty uh like I found a back in your code or something or just like get involved with the GitHub repos yeah I'd 100 agree hi I'm Christine I'm a researcher at Galaxy but on the topic of Youth core Dev calls I too definitely did not understand anything for like the first couple years of my time covering these calls but it takes a while um I think one thing that I thought was really interesting about covering the merge and being on these like watching through live stream over the years that it took to kind of like get the merge going is how over time like the process for figuring out what goes into the merge and who gets to build clients for the merge increasingly become like a smaller and smaller and smaller team um so like in the beginning there was like probably there was like there's this article on coindesk talking about like how there's like eight or nine different client teams like trying to build for the merge but a lot of them dropped off once like I think there was a point in which like the the repo for how ethereum's consensus was going to happen got like totally overturned and that made a lot of client teams mad and so then it like shrank down to four and I think the closer like the client teams got to to the merge it was very it was very clear that like there there wasn't like really much more discussion to be had from like just anybody so like when the Discord channel was like privatized like basically like no one talked because we really have to focus on the merge right now and when they started to become like differences of like client teams Readiness for the merge so like some Clan teams were more ready than others had more features ready than others I think they're the differences between like preparation and resources for each client teams got a lot clearer like the closer it came to the merge um so I think like moving forward and looking ahead to some of the other bigger upgrades um I guess like I wanted to know your guys's thoughts on how to like on basically what the new process is going to be for eips and stuff because like for the eth core Dev calls for the last couple months it's been clear that it's just the merge but now like there's going to be so much to discuss so much about ethereum's roadmap um so I thought that it was like really like part of the the whole topic around like ossification like the more decentralized this process is the harder it is to make changes um but clearly there's like some amount of centralization that's needed and that we've already seen to make the merge possible and so now there's Shanghai coming up so like will it still be like consensus layer execution layer like calls or how are we thinking about governance I guess moving forward to to to start to like um yeah pull off some of the other big big road map items wow there's a lot to unpack in there um I think High short a really short answer is like slowly iterating from what we have like I think just like the merge itself was kind of a feat of governance to some extent to get like nine kind teams to work together um and agree on like uh like agreeing on the big stuff was easy for the merch I think everybody wanted to do it um like all the tiny things I don't know I have the latest valid hash like burnt into my head from typing in the awkward as agenda um like it's just like this thousand tiny things where it's like the coordination was really hard um I think the other thing is like and we're about to get into seconds it's like balancing like large protocol changes so something like the merge something like sharding something like stateless with like smaller things that like bring it out of value as well I I think this is where most of the tension probably arises like different prioritizations there I think it's time curious to hear from then crowd you have like some like more radical views about the process than me so yeah and you have a token so far so I think it's a good it's a good cue for you um I mean that's a big jump but yeah um um I mean my view on all core devs I mean like I mean uh I think the big thing that I I feel is missing and that I want to see is um representation of like ethereum's users and the whole Community rather than it mostly being focused around one side of the of the um of ethereum which is the development um and I think that's uh that's a big thing and we would see very interesting and different proposals if we had more voices like that on the call also the IP process is being like it's being improved over time and right now what like not like we but the ipip group is working on and uh alongside with uh ethereum catalers uh it's pretty much that they are going to like sort of fork or like separate the core IPS from the other year season like our different aips and then eventually [Music] yeah but like I remember like we've been talking with like team and some other even like Hudson I think you were there in the group where we wanted to separate e uh erc's from the eaps um oh Jesus I mean also Hudson we we did not use the stage because we went to like be everybody on the same [Applause] okay thank you um so pretty much we wanted to separate erc's from uh eip's but then it ended up what ended up happening is that we ended up uh sort of separating core IPS from the rest of the eaps that's exactly right and um there's been there's been years and years of debate over whether to separate eips from erc's and a lot of it the biggest problem I'm finding looking back is that there wasn't a steward to be an ERC editor I mean there are also EIP editors already but there are like two or three people at this point and Micah doesn't want to do erc's anymore like he's been open about that I mean yeah there was like many people that wanted to sort of do it I was even like one of the people that wanted to like do it myself on behalf of magicians but then like it kind of fizzled yeah there was there was like miscommunication there was like a lot of different problems but either way I'm glad that it's getting separated now that's gonna make things less confusing when people talk about eips and especially uh because back in like I think we had this discussion and uh like with Mario's right at the amstrom where we started talking about how the EI process will gonna look like and that was like April this year um and only basically pretty much got as answer was that it will be consensus driven um so not not AIP process as we are used to right now which is by the way described in the AP1 which you guys can find on uh eip.etherium.org um yeah guys one more quick thing on the eip's I I guess it's like we're separating like the tools from the process like sure you know like we can split out the RCs and I'm very bullish on my executable specs for for the Executioner like there's a bunch of like mechanical changes we can do to the process and I think they'll be better but that's different from like how do we actually come to consensus on the thing like whether it's an EIP or the yellow paper like a consensus spec PR um you know that's like a marginal difference I think that so one thing I actually really don't like about Cortez is that it's calls um like my like and the reason for that is like um calls are hard because for they optimize for a bunch of weird things so like the optimize for people who are awake at the time who are like uh good English speakers who like are very like eloquent and like can like think on their feet and and like respond live during the call um I think this and also they're like not very scalable we can't have like 90 people on a call um and so like to your point about like having more more of like the community involved it's like that's something where like moving the calls are good because they give like a forcing function and like some rhythms like I wouldn't take them away completely but moving to be like more async is something I've like I try to think a lot about and you know so for Shanghai we have like this uh tag on on East magicians and like you know anyone can tag their eat there Klein devs can review it like I can't force Klein devs to like look at your VIP if they don't want to like you know no one can do that but it's like at least there's a list and they can scroll through it and like if it sounds good to them it might be like more accessible and maybe you wrote it you know at 9am Vietnam time rather than like 4 a.m during Yoko awkward devs so yeah I I think more async is like one actual tick like thing we can do to make the process more open um yeah um I mean so we're going to segue into it you don't have to like okay okay what's the next topic Serge Verge Verge all the big stuff so so so so now that we've talked about how we are going to uh to come to consensus on these new upgrades and new things we could maybe talk about the new upgrades and new things what's your favorite new upgrade that's enough segue for you yeah sure hilarious what's your favorite new upgrade um uh I I like for for four eight four four um that that's that's pretty nice um okay maybe okay a better question on like the new upgrades um I guess I'm curious like Yeah from from client team's perspective like how much have you been thinking about new upgrades because like everyone on Twitter obviously talks about 4844 a bunch of people not in fine teams have been contributing a couple incline teams have as well but like you know mostly like we shipped the merge a month ago um you know like it's not a lot of time um so yeah how are you all thinking about like what's next or are you trying to not think about it as much as possible and you know compressed with emerge yeah so I I kind of think our main responsibility is our users right now and so we have a bunch of upgrades coming up that oh yeah okay hi um we have a we have a so forget at least from my perspective forget it's the most important thing is the current Network and it's not the it's not the future and it's not uh this it wasn't the merge and um we want to make sure that the current network runs and the current network doesn't go down and it's secure and it's usable and it's uh decentralized and you can run your own node um so uh we have a bunch of uh other things coming up to improve uh things about around the database in gath the the states uh the way we store the state and um so yeah I'm I'm really excited about those and because we've been focusing kind of focusing on those we haven't focused too much on the upcoming eaps um okay okay like in general the team uh I I personally also like I I started implementing 4844 in Lighthouse the wrong wrong client uh but uh it was it was a learning experience and I I also worked a bit on the Casey Jesus ceremony because that was just interesting to me um in a fortunate position that we have a lot of outside contributions uh so people actually if they if they propose an EIP they usually implement it in Geth and so when when like the fork time rolls around and we have to uh uh have the implementations for the eaps uh we have something to build on and um we're in a very good position there compared to other client teams and so we can kind of focus a bit more on testing making sure that the spec is correct all of these things I I think I drifted very far from the question yeah cool um how we feel about upgrades um I think we're generally Keen I think uh definitely Keen for withdrawals which uh Mark is working on um I I kind of feel like the merge is not really finished until we have them in we kind of have a bit of an outstanding promise that we're yet to fulfill to the stakers so definitely Keen to get that one in um without even saying like you know what do you want to do after the merge because it's not really after the merge for me until we've done that um yeah so Keen to work on that Keen to get that in soon definitely Keen to get 4844 and I think that looks really cool um personally not super Keen to rush it I think it'd be nice to have a bit of time like Maris was saying to kind of watch the network um breathe um yeah so definitely Keen for upgrades sorry yeah yeah that's right um yeah we we have um like Sean's been working on it um Maris has been working on it for Lighthouse um yeah you're gonna put in your time sheet man um yeah so we're keen I'll uh pass it to otaku yeah I think I really support Paul's point on the the next thing we do is withdrawals bye it's got to be done I I don't think it's reasonable to put in bigger things that are then going to delay getting withdrawals out we made a promise we showed on the promise a bit and you were all very forgiving because we got the merge sooner and now we're going to deliver the promise we've got to come through on that um but I think the thing that really comes after the merge is the use of support for the merge and the optimization and the cleanup and the learning now that we're actually seeing it in the real world for real there's a whole heap of stuff that we can now go oh we should make this better and it's not going to be protocol upgrades it's just client improvements and so on so that's going to take a good chunk of time but I think we can um start looking at a bunch of other things as well and so I think 4844 is probably getting the The Lion's Share of attention of the next big thing after withdrawals yeah I think that withdrawals are absolutely masked and absolutely the most important thing to deliver at the same time 4844 sounds so interesting that it starts to steal the Thunder and and I think it's maybe even it's it's a bit dangerous we should [Laughter] very brave man sitting between photos right yeah exactly uh so so yeah that's that's why it's so interesting because there's people are so convincing in delivering those Visions uh yeah I think that's uh coming back to her Netherlands team thinks about the next delivery I almost would like to to activate Daniel here to tell about his like plan of of delivering them so I think it's uh in our case very similar what I just heard from other teams so you right now I would like to relax a little bit and focus on improving our client yeah so the merge was extremely stressful to be honest for the whole team and now people want to do things that you know you know let's say slower a slower Pace yeah and additionally we would like to clean up a little bit here because there was no time um I know I don't know if you are aware but like never mind a few months ago it was completely different uh a team it was undersized very small at some point there was only one guy Marek working on the merge which was crazy and you know I was really sorry when I joined and I you know uh met my recognized okay we have to do something with that and yeah now it's we are in a completely different State uh we can you know improve our client a lot we have a lot of things that actually we are excited about and they are not you know I would say one-to-one related to to Shanghai these are the things like human yeah like database Improvement our sync Improvement you know our robustness of the clients these are the things that maybe are not so excited exciting for the community but they are exciting for us and that's we are mostly are talking about right now and there are also things like you know documentation uh Community Support user support these are things that didn't work well in the past and I think in general it should be improved not only in nethermine but in the whole community so this is what actually excites us right now and in terms of particular eips we already started you know investigations but you know taking it slowly you know have fun with it like four eight four four it's you know there is one guy working from our team that's I see that he's really excited about it that's cool there is we already started with travels we have some kind of draft implementation and we know that maybe it's gonna change but yeah why not to start playing with it so yeah in general yeah we are in much better State when we used to be and yeah this is what we are doing right now so uh yeah I can give you uh an additional perspective from Minority client like lodestar we're pretty much in the same boat I guess with like nethermind where before like at a point where we haven't even hired the people that you guys have at this point so in terms of time we really haven't had as much time to really think too too far ahead we're getting most of the stuff from you know you guys in in the community and such but uh technical debt is definitely a huge thing for us as well we have a lot of documentation we need to update stuff like this which um you know it would be nice to have a Sprint that's just you know for technical debt and and and getting our client up uh to to par in that sense um but that's you know we're really excited of course to implement all the upcoming stuff um in the roadmap um like this was great for me because I haven't even specked out 4844 as much as I should yet so that's you know where we're at basically so on the Nimbus side there is a delicate balance between implementing a new thing that are still moving targets and doing things that are expecting from client teams like testing and trying not to introduce regressions uh like we had a lot of um Point release in the past three weeks before the merge and that took like all of our Focus but we do like um implementing new things as well for example we took the lead with load star on the light client thing so this is something that we will continue but we didn't start at all on anything related to the search The Verge uh The Purge endless Branch well we did try to look into kcg commitment but they changed a lot in the past two years so everything has to be thrown out and recorded um I I also think it's fine for uh these smaller clients to not be on the like brink of research um because the especially with these uh with these upgrades like like 444 right now the spec will change uh a lot and uh it I think it just doesn't make sense for for smaller client teams um that have that need to focus on like getting their client up to speed um to also uh think about the research and and the iteration on the spec so I think that's something that we did with uh with the merge pretty pretty well is there was like the bigger client teams uh gas Lighthouse prism um uh never mind bisu iterating on the iterating on the spec and the the other client's um kind of uh like like following a bit and um I think that's a good way for the teams that have more funding more people um to take some of the load away from the other teams and also that's also what we're trying with uh with uh testing uh and and ethereum Foundation testing team uh where we're currently looking for new people so if you're interested in testing uh come talk to me or Mario Vega um thank you yes so we we want to completely revamp the way we do state tests um to make it really really easy for for people to to implement State tests and uh to take some load of of the the client teams just wanted to share from a small client perspective so for Basu honestly past month has been as equally hectic as preparing for the merge um with a bunch of major fixes for bug fixes so honestly we haven't even thought of a future eips honestly I think we even haven't gotten a chance to rest and a lot of the maintainers couldn't make it because they're still working really hard working on the fixes some even donated some of their paternity leave to work on the fixes so quite quite intense but I think afterwards we'll um hopefully get some time to rest and get all the maintainers together talk through what's going to be the future um you know works that we want to work on what really resonated me personally with the earlier Talks by iOS the subtraction part so I'm hoping that as maintainers we got to see which part of our co-pays could we actually subtract I think we're still talking about what eips are we going to add and add and add to our code base but I think there's going to be some like future um like you know Tech not future like Tech debt from the past to clear out to modularize um the the product itself and so looking into a lot of um I would say client Improvement is currently where I see maintainers are talking a lot about but yeah definitely it would um would have to come talk about eips yeah um so I'm prism and um I think withdrawal is important but it's also on the easier side and on the eip44 we have been lucky and optimism and encoding base have been contributing to our code base so thank you for that but I do want to throw like a curveball I think like censorship resistance is equally as important as 444 and withdraw I just look at Mev watch info at 47 of the blogs are on the offback compliance right now so 47 of the Bloods or mainnet all have some sort of censorship resistance built into it right so I do think like there's something like before full PBS there's definitely something that we can do to make it better we do have some hybrid PBS we can do we can leverage the Builder API we can iterate very fast we can have some math booths CR this type of thing and we tell this latest research post that's pointing us to some nice Direction and burnout B also has a nice research post as well so very excited for that and I definitely definitely want to prioritize right now I'm very sympathetic to teams who talk about wanting time to uh to work on their client and handle their Technical backload and I thought that that 4844 and withdrawals was a really good example of they both kind of individually look like okay we've got kind of an idea of how these two things are going to work but and then putting them in the same Fork actually does increase complexity uh for you know minor technical reasons I I guess you know we're talking about uh switching the forking based on a block number two based on timestamp and you know the the more uh places that you have to change in the code base uh the the more work that's going to be right so 4844 introduces a new transaction type which means you know more places that this forking based on timestamp has to has to happen for example so just an interesting place of seeing why you might get pushback on hey let's only do one major thing in this next you know Fork instead of two and and leave room for maintenance so so just that as an example for this I actually implemented the forking based on timestamps for Shanghai which was like uh a 20 line change and uh I looked into uh doing the same thing for uh sorry for for withdrawals not for sure um and I looked into doing the same thing for four eight four four and it would be like 70 different files that I would need to touch just for changing the um uh changing the way we we verify the signatures and the sign up so um it's individually it's it's it's okay but together it creates uh even more complexity I feel like we've heard a lot of clients say that they feel that they could spend some time cleaning things up in their code base improving in you know improving things in their code base I'm just curious to hear from Tim and from other clients like how should we think about this in respect to the road map and all the things we want to do because it seems like we're consistently saying we need to ship withdrawals yesterday we need to ship 484 four three months ago vertical trees all these things and it's not clear to me how to do that and balance and you know maintaining clients for the long term so well while we say that we want to slow down at the same time I'm thinking that never mind is ready to to think like a big client like the one that is that is leading the effort on the exploration so so by the fact that we have the large team uh we can both keep cleaning the technical debt but also participate in the research in prototyping uh so so we've we've seen we've heard that from Mikhail talking about is Migos still there no no okay um so she was thinking how much it was helpful to have the Prototype from Guillaume right and and how much Maurice's experimentations were pushing things forward um so we have those excited people like Alexa you know and that in mind and they they want to explore they want to experiment and this should help everyone to to push things forward and and these deliveries are critical and they're still time critical I'll be doing that in parallel even if the field will be we change this style uh that's that we do that a bit more mature way I think that maturity will come also from the learning from the merge and from the team being larger so I think on average the teams grew about like two to three times in the last two years on many of the clients teams which means that we should be able to ship faster than in the past and when we look at the last two years comparing to the 2018-19 we've seen that we've seen that pace and there is much more experience with the teams people delivered so much during the merge and they they are interested in now delivering the things that they wanted to work on the side and many of those things that they wanted to work on the side are the EIP is already are the items that are interesting like 4844 right so this will happen I think this will happen but it's also another factor that we need to consider yeah but like changing let's say 10 lines of code or 17 files two years ago was completely different than changing the same amount of files right now the code base is much bigger the complexity is much bigger and you know sometimes you know Small Change requires you know days or maybe weeks of you know research yeah and testing and testing especially testing here it's something that we need to improve and so yeah the teams grow group but at the same time it takes much I think it's going to take much much time to deliver similar change as the ones in the past um if I can add something else and on clients the right now testing is not much like the experience with testing is not much better than the experience introducing an EIP I think we need a a better platform to discuss like the bottlenecks of ethereum to understand it like the complexity of syncing of disk i o and so on and tools like Hive they are just a stagnant sometimes as the clients themselves when it comes to making changes or improvements um maybe we're talking about awkward off changes maybe we should have like a regular breakout for testing maybe we should have more of this platform to discuss how we get out of this this stacked up into a place where we can be excited about new VIPs so so one thing about Hive is that it's currently maintained by the guest team and we're slowly transitioning this over to the testing team so Mario wrote a lot of Hive tests for the merge basically all of the hive tabs tests for the merge and so as I said we're trying to increase uh we're trying to ramp up the size of the testing team within the ECM foundation and that should take some local eight like that if information is helping but I don't think the right solution is to limit it to one team no I agree so I think as like an open call to anybody if you are a testing team and like you want to work on this stuff ping Danny Ryan and he will answer you any hour or day any minute of any hour of every day about improved testing capacity and I think like I agree it's like a huge ball deck to be fair I feel like the merge we did like an amazing job there like relative to what it was before like when we shipped 1559 I was like not 100 confident I was like and then when the merge went live I you know like I was expecting some operator to mess it up but not like the network to break like it felt like the testing it was like super robust um and and the challenge the challenge I think with shipping all of like this complicated stuff is often not like not just that it's hard but it's like it changes the shape of the network like it's easy to add the new OP code right and write test for it like we have framework for that and now we have and and but like if you want to do something like blobs um we we don't have like blob testing right and so we're going to need to build all that um and then then we'll have it and it'll be easier to like you know grow the size of blobs or something but then when we're gonna we want to do I don't know like data availability sampling we need like you need like always to build the new testing stuff um and it's kind of hard because like you can't build it in advance because you need you need to test something um so there's this weird like chicken and egg but I do think like like we were having the same conversation in 2018 in Osaka and Like Your Capacity was like 25 probably of this so it's like I feel we've we have gotten better I don't know that it's ever going to feel better doing the work which is maybe my thing is like we just do more or better at it but it still feels like we're kind of pushing ourselves because like it's just hard stuff and you need to build it um and yeah like I mean you know the touch on 444 obviously client teams kind of tired you know want to take a break want to focus on withdrawals but like having the optimism had step up having coin race step up like we wouldn't even be talking about 444 if this hadn't happened and I'm not convinced that like four years ago it would have been possible for like there were no L2 teams but like you know for like the Plasma Team to like ship something like that so yeah I think we're making progress I don't know that's ever going to feel better is my rough feeling so I drained from Tokyo I just moved from over there um sorry the context um so I think the other Factor that's slightly interesting is there's this real cycle to Doing Hard Forks in that client teams in particular get swamped it's not really early on the research team probably gets one first then the client teams get swamped and then our work is done and we're kind of waiting on coordination and there's this lull and that lull is where we get all of our client optimizations and all the other stuff kind of done um and uh yeah so that's real opportunity and and a lot of the community coordination happens there there's there's a lot of time it takes to go from the code is ready and done to the code is tested in every possible situation and we've automated all of that in terms of the cross-client stuff and and the the level of detail we want for ethereum and then the code is actually ready from the community's perspective and all the tools of updated and so on and all of those kind of happen to happen in sequence so as long as we don't shoot for a massive hard Fork we can get a you know little hump of we can get you know say withdrawals done because they're relatively simple pick up a lull and then be ready to to pick up something big again um so it's not a case of stop the world it's just managing those cycles and probably the other factor is that teams go through that kind of cycle as well that you'll have a great team and you're going well and then someone will get a better job offer somewhere and you kind of suddenly you've got fewer people on your team and you're rebuilding with some new people again and yeah all the client teams are nodding we've all done it it happens on every team from time to time it just depends where you are in that cycle as to how freaking out you are about the next hard book as well I don't know um I was expecting to come and try and like make a pitch for why it's crazy to do more than just withdrawals is a big thing in in the first half Fork after the merge but actually I've not had a single chord advocate for that so far so oh but maybe Danka oh no I moved within arm reach of the office that's the question anyway um but I did I did want to ask two um points about withdrawals um like mostly to consensus clients um so the first point is um a few people have said things like oh it's a relatively small change and so my my I wanted to just check like is it so um so one of the other sessions recently one of the points that was made uh was that um you know at the at the time that the um withdrawals become enabled also um withdrawal key rotation becomes available so there's going to be enormous queue of people wanting to do key rotation there's going to be massive muv from those validators who are trying to exit before um they get hacked because their key's been compromised over the last two years all kinds of other sort of maybe things you haven't about very much and so do those worry people at all or not at all um and the second point I wanted to raise about withdrawals was around whether uh it kind of links the tech debt point that few people have mentioned is uh is there is there a kind of significant um quantity of tech around the deposit process so so as as withdrawals The Proposal exists is to create a new operation for withdrawals it's not a transaction it's a completely separate type of thing um it seems quite natural you might want to just have a corresponding deposit operation clean out the deposit contract deal with the issues like kind of you know double counting issue it's all this kind of stuff that would make the protocol more understandable for future Generations is anyone interested in that or is that for later I heard some good arguments about cleaning up the deposit contracts to keep the supply in the balances actually matching all the expectations rather than looking up or like burning the the deposits and then minting through withdrawals so that's fair I do think deposits are user initiated and withdrawals are system initiated so they're fundamentally different but yes there's a lot of tech debt there and yes we will fix it in a in a hard book at some point um just to simplify like it's it's ridiculous the beacon chain is still voting on what it sees as the eth1 head after the merge and it's 8 000 blocks behind yeah but hey it works right and we didn't have to touch it and that was good but we will have to fix that I'm curious to hear from the 4844 Maxis so the thing about first performance is it's this complement to proof of stake in the old dream of Serenity so we're very motivated to ship it at the same time I think there are goal posts and like the communication about the state of ethereum are not that clear and so I'd like a better platform for testing I'd like testing to be less stagnant I'd like to contribute to testing and often I see that some of the test tooling are locked into either a research team or the guest team of hives and if we open these things more like up and we communicate it and that goes about these things if we I think we can make a lot more of these quick programs towards the states we're very happy to to work on Forex referral or other complex subgrants um yeah I want to share my perspective on as well I think like we have heard a lot about tech debt this is which is like Fair there's the the devs here like they have the best overview of that and that is like uh a very important point of course um but I think we also have a different kind of debt which is like right now we cannot serve the vast majority of people who might want to use ethereum so like I think like it illustrates my point from earlier that um we are having these discussions among core deaths and they are good and they're important but somehow uh they can't be the only input into the decision-making um so I don't want to Clearly say like I can't say like we have to do 4844 as part of Shanghai but personally I would love to see it and I think there are very good reasons to try to do it and um and I think like only looking at the tech depth and saying like we can't do it because of this and we need to do everything in sequence um is is not enough in my opinion in arguing against it like it's um it's uh we we have to like at some point also start seeing the other side like okay so what what is the consequence of not doing it like my fear like of course like if we have say Shanghai for we managed to do it in February and then we can do like uh four eight four four a few months later okay like everyone's happy with that but like what happens if actually Shanghai does not happen in February it happens in June it happens in September or something and 4844 slips into 2024 I think I mean I would be fairly unhappy with that so like we should consider these scenarios we should also think about what it means to ethereum yes it's like it's a bear market now so maybe we don't have these high fees as pressure pressing an issue anymore but I think they are still actually a pressing issue because right now maybe like we don't feel the pain as much because our the things that we have been doing are fine but still a lot of applications are not being built because the fees are too high and because they can't like the experimentation can't happen because I can't like do some fun stuff if things costs one dollar per transaction which I could do if it's one cent per transaction so I think like we should like be more willing to like think about this part as well and I would love to like understand how we can also like get that in that thinking into our governance Pro process as well so uh currently we're releasing artworks uh basically when it's ready so so more things we put inside uh the longer it takes to be ready because of uh testing for example so if we actually do withdrawals uh which are supposedly simple for uh the talk from yesterday uh was less reassuring regarding that um we can yeah maybe in February we can have withdrawals and then uh for 804 uh for the next one but if we put both together uh maybe it would be only in June um so there is a balance there so I guess one question there is like do you think it's impossible to have a uh one fork in February and then another one like literally three or four months later Berlin in London okay well that's uh I mean one I mean this is like a discussion but just like one other thing I wanted to add just like to what Dan grad said is like thinking about it from the perspective of tech debt of Layer Two protocols and of Roll-Ups right like the yeah one of the kind of philosophical goals of 4844 I think was to kind of be a like the changed the change to end all changes sort of for uh specifically for layer twos in the sense that 4844 introduces stuff like the point evaluation breaking pile and the concept of blobs and if that allows Layer Two is to kind of set their code once and they can literally write their code and launch it and then you know no matter how much we screw around with the yeah sharding design later as long as we kind of screw around within certain parameters roll ups will be able to kind of you know breathe easy and know that you know they don't have to make like that kind of re-architect thing again right so I think there's uh you know there's also a value in trying to like get that phase done earlier because you know the earlier they yeah they can do those uh those kinds of changes that you know the more they can get to the phase where like you know they can start clearing instead of they have to know that like they have the queer eventually let's just it's so worth talking to away or two teams about that too I mean I'm sure they'll have their own perspectives so I guess we various kind of consensus that we want for 844 and withdrawals within the next nine months is let's say so the question becomes both together or uh withdrawal first and then fight over and so planning team I I will say one of the added challenges that the specs for 4844 and capella are not unified so like he's working on 4844 I'm working on capella and that's gonna be a hell of a pull request to try to merge these things and test them so they're kind of already written like they're separate just sure so one thing I would add with regard to hard Forks one versus two is that you generally with the hard Forks implementing something takes usually less time than rolling it out so implementing both withdrawals and 4844 in the same artwork definitely makes it longer but I think overall it would still be shorter than doing two hard works because you have a yeah sorry so so essentially I think rolling out the hard work always takes two to three months of just while every client is yeah I'm not done yet let's test it tests are not done yet okay let's do a hard work this that's not hard Fork that's that that's not so we kind of I don't necessarily want to say we suck at rolling out things but we don't really push very hard so if we want to roll out two hard Forks then you have this boilerplate time that will eat up both up from both of them so that's the extra I I just want to chime in on dunkrat's point from the coinbase perspective that's what I represent and I know that we're still here kind of building trust you know convincing it like I get it I get it like it's the first time you've had me in this room um but you know we've been we've been working with Proto for the last five months on eip4844 implementing an imprisonment spec uh prism in Geth and I think to doncrat's point the difference between like H1 of next year and 2024 for a business like coinbase is massive and I think the way that shows up is we have products that we've built on chain smart contracts that we want to be launching like literally we want to be launching in Q4 in q1 of next year and we can't right now and that is because at the scale that we're operating at where we're talking about bringing millions or tens of millions customers into those contracts in our wallet products fully non-custodially it's just too too expensive most of our customers who are not in the US can't pay for it and we as a business especially in the bear Market can't subsidize the cost for them and so what that means is that in the context of these conversations the people who may be understand the technology benefits of decentralization or security they then go and say hey there's all these other evm chains they have sub one cent fees can we just deploy this thing on that like would that be a faster can get can that get this thing shipped in q1 and it's up to the people like me maybe or others in the company who who understand the whole process and why we're doing Roll-Ups and why this is such an important investment from a decentralization security perspective say no like we need to wait we need to wait for this to have the right solution and so I think waiting until the beginning of next year you know first half of next year I feel like that's like a thing we can hold we can make it happen waiting until 2024 is really hard that's going to be a real challenge for us and so I think I think from from where I sit our feeling is like let's make the list of all of the things that all of the people in this room feel like we need in order to feel comfortable and if that's better monitoring of the network so we can understand bandwidth if that's better testing so we can feel more confident in the change that we're making uh whatever it is like give us us the list give optimism the list and we're ready to throw resources at this and support this and I know that again we have a lot of trust to build that will come through in its work a year from now I hope that there's a lot more trust here but I do want a voice like that's the impact for for a business like us and we're ready to come to the table with you all and work together to figure out how do we make it so the end result is tens of millions or hundreds of millions of more users are using ethereum by the middle of the end of next year [Applause] I I have I have uh two rebuttals to uh to to putting uh for it for four in into Shanghai um one of them is uh I don't I don't I don't see any Roll-Ups that are trustless right now uh most of them don't implement the the fraud proofs uh they're putting the data on chain but the data is not really used for anything so uh like you cannot use it to to prove that the that the that the Roll Up is wrong and so if we Implement uh dank sharding we basically say to the community use these Roll-Ups but the Roll-Ups are not secure um I think that's kind of a it's a minor problem it's it's uh the other thing the bigger problem I I see is that from yesterday's conversation just doesn't seem ready yet so withdrawals the way I see it are basically done that like they are implemented in some of the clients already the spec seems to be pretty stable and with Deng sharding we recently had a new fee Market change that I don't know how much thought has has been put into it from from my point of view it it like the the fee Market change at some some 1559 like uh thing and from my point of view it looks like we have a hammer the 1559 hammer and now every everything looks like a nail and um so I think there has to be more okay you know wanted to do this like this change was like in the pipeline for like months I think it wasn't like something we suddenly came up with it was clear we wanted to do this and we do have the change in when they didn't get in prison in a live devnet okay yeah yeah so but those are the things that those those are things that are uh that were not not quite clear to me and um so from my point of view we could ship with uh withdrawals easily January uh especially if we if we uh like kick out some of them yes now yeah okay it's it's probably a bigger change for the for the consensus layer I think for the execution layer it's it's pretty uh pretty much done and um okay okay okay maybe I'm I'm totally wrong here um but I I agree with the argument that so even if we were to ship uh uh withdrawals in January um I think we could own the ship uh tank sharding September of uh of uh 2023 if we were to do both we can probably ship it somewhere in the middle and so actually would if I if I really have if I really think about it it would probably cut to two months uh to to both of it even even though I don't like it and even though I don't like to admit it um but I think it might be the right thing to do you have is probably that it is a bit more risky to do both at the same time and I think like most people would probably agree with that like that making a bigger change definitely adds to the risk of the hard folk itself um and I think we also should have an honest conversation on in which cases we are willing to accept those risks because what we're doing is just so important that maybe like some risks have to be accepted as part of doing it yeah Paul from techu Team just on that I mean there's a couple of aspects maybe we need to get better at doing hard forks and releasing them honestly it's a long time between code completion and getting to getting to that gate that may be a thing we need to address but the other side of it Playing devil's advocate I understand that 4844 might be important and if it's relatively well defined there's nothing physically stopping us from doing that first and just delivering 4844 and not delivering withdrawals I mean it's a it is it is an actual option that we have no I'm playing Devil's Advocate sure Paul is not representing the views of the turkey team this is no but it's a fair comment this is purely my view but in reality if two is too hard and we want to deliver this early in a timely manner to not stuff over businesses that is an option that we do have yeah we should absolutely focus on doing the most important thing first always um I think the point that's come up a couple of times is you know when we ship withdrawals and it's actually when we finish code completion because running through test Nets doesn't take quite of time we put out a release it should be pretty quick and easy um there is coordination cost and it consumes all core devs for a while so you've got to know what's coming next if you're going to paralyze it but it's code completion that's the big thing I'll give it to them uh in between I'll just pop in um so in my opinion withdrawals are kind of specked out so it I mean there are variations but it's simple it's really really simple whereas with the 4844 that will most definitely take a lot more time to spec out and it has a lot more potential problems with denial of service and everything it affects Network so withdrawals that's just a tiny consensus change you know tweak a bit the data structures and done with uh with 4844 that has a much deeper implication so I think it it will require a lot more work and in my opinion it would be much simpler to say that okay for withdrawals are definitely going in and maybe give a cut off that if we for some reason 4844 gets complicated and we cannot finish it by month X then just say okay we're rolling with the withdrawals first and then whatever whatever the other is ready so quick question from me because I heard the term a few times like about code completion and then rolling the the change and that we are very slow in the second part but you know I joined right yeah I'm I'm a new guy but what I observed with the merge we we shift shift in middle of September but what I observed flag from my the definition I know the code was not completed in August and some teams were like pushing the changes at the very end and for me like when you said that code is completed you know you you've been testing you start testing and you don't introduce new changes here so the code is stable so it looked a bit different from my perspective yeah I agree the merge was not a case where we were called complete three months before um so yeah but but we have done that before right like and and you know again I think burden London was like a good example as soon as we were kind of code complete on Berlin we started working on London before like it shipped and part of the reason for that three-month delay is not like for the client teams it's for like everyone running your node like folks like coinbase because usually like people like not the coinbase here like people just don't care about the hard Forks until they're like announced on a blog post on blog.ethereum.org um and then they're like holy and then they like message me and they're like oh my God we need to like upgrade all our infrastructure and you know and especially so if like it's a complicated hard Fork if it's just like introducing a new app code um they need to upgrade their notes and still sometimes you know they're like oh we need like two months to do that or something so it's it's worth noting like we can set our schedule somewhat independently of like the release schedule um but that doesn't necessarily mean you want to like do code complete and ship two weeks later because there's still value in giving people time to upgrade and I I would also argue the merge was like cutting it close and um I think you know the merger's like a big change the environment bits the like proof of statements so I think there was a bunch of variables in that one um but if you did historically we've been like kind of slow but I think that's that's like healthy for people like peop people who like are not core developers should not have to look at this stuff every day to know like is there a hard fork in 10 days right uh diano basal maintainer um one of the ideas that you you sent me once uh Tim that might help because I'm hearing a lot of just talk about um lots of bandwidth Lots needed in in the layer one um what if we prototype some of our ideas relating to things like evm and transaction formats on a layer two and that way the libraries and the tooling could adopt to that on the layer 2 stuff and then all the downstream stuff is ready and then layer one can implement it when they have the bandwidth um and not having to rush in and get those in so you can get cool things like BLS transaction formats sooner rather than in 2024 or 2025. I really do like the idea at the same time via Slayer 2 optimism especially we're trying not to stray away from layer one where we create conflicts and developments we're literally just trying to if we could we would run GAF without modifications we don't want to be different and often I just like client teams they're one developers to be more receptive towards changes but also testing and like creating confidence in these changes as there to authorize you get this conflict of interest where okay you want to make the change to be special on new and exciting and cheap but you you're not shipping the ethereum dream yes like and especially data fatability like for Furby we can't even ship it on Layer Two every like we created layer 3 situation with a much much smaller validator sound when we go through that big list of uh proposed EIP stuff that's coming up in the next thing like half of them are just evm stuff and that stuff is easy to push through but then you get into the situation like we had with subroutines where it was implemented it was ready and then the week before the first test net all of a sudden it gets ripped out I mean I I like the idea but your voice and exact concern how do we get the commitment that if it's done it will ship on layer one without substantial change because then if you have these changes on Layer Two and they change substantially then it's a burden on the layer 2 Chain if they put it on on one of their Premier chains rather than on testnet it's a one thing to answer here is that I think it's very from the outside it it's a bit strange because it kind of looks like there is commitment to go with an EIP and in reality what at least what I've experienced is that most layer one most execution clients won't even touch any IP until somebody else actually implements it and verifies that it's correct so usually you have the EIP authors which kind of has to beg one of the clients to please implement it and and everybody is just waiting for that client to implement find all the corner cases and when when the code apps say that okay we are releasing this EIP in three months and everybody goes into this holy mode and okay let's quickly implemented and something and then we kind of like pursue this dream of Roll-Ups as like like comparative execution layers on top of a secure data layer that's minimal and like widely decentralized we're noticed we don't have a layer 2 evm standard but I mean what if you wind up in a situation like coven where you had a part of the chain that ran with wasm and then all of a sudden wasm's gone then all of a sudden the one client they can run Watson's gone and you can't run the chain from zero is that a bad thing is it an okay thing I would say it's pretty pretty bad so it's it's basically just shoving teched up to layer twos and then the cost is incurred by users being confused by their smart contract works on 110 not the other 10. right just yeah we can wrap up here we're like yeah we if yeah we can I mean if there's final comments we can do that then maybe we should take a short break but just want to make sure we wanted to keep the last hour to discuss actual eips um because we we don't usually have like forums to do that with all the client teams so um if people want to have less comments or take a short break you can do that but let's take a break for like five minutes and then let's have the Shanghai AP authors line up or switch the spots with the yeah thank you [Applause] changes you know more like while coming and we want to be just like spread around thank you so much awesome they're gonna come and Pitch you their eip's Marius and some kind of game show yes exactly it's a game show where you pitch your EIP to Marius and Peter and if they smile if Peter Smiles a little bit it's in the next hard fork okay yes or no um we'll see um cool thank you so much everyone uh thank you for closing doors um so perfect you guys are I love you so much because you're the best um so I just wanted to say quickly uh today or now we are going to start the Shanghai IP pitch session um and then um there should be all line up the AP authors right if not and if you are a Shanghai AP author please come sit in front uh as we want you to like pitch your AIP and we have like 50 minutes left who here wants to talk about an EIP just like raise your hand so we can like roughly keep track of time wow okay oh come if you're in the back it's not gonna happen you need to move forward yeah yeah okay so sorry I I didn't count because these guys would write again again okay one one and a half two three four five six seven eight nine okay so we get like five minutes each it's hard to like cap that okay I can like keep the timer yeah you can roughly keep track of time um should we start yeah okay yeah I'm okay I'll start I'll start to decide hey guys um I'm Sarah I'm a smart contract engineer at uniswap um and I'm here with I'm Mark and I'm a protocol developer from optimism and um so we're gonna we're in we're done should I drop the mic um cool so we're gonna kind of tag team this EIP today um yeah first of all I want to thank you guys for hosting the session I think it's super important that you know when we're planning for building open source software we're really bringing a lot of diverse perspectives to the table um you know I think it's you know a lot of times client devs and core devs were you know focused on this really long-term vision for ethereum and unfortunately for application developers that means some of the stuff we want to see does not get through um and so hopefully I'm here today to convince you that this is worthwhile and actually this EIP really will complement the kind of Future Vision for ethereum um so let me hand this off to Mark to give a little a little rundown okay so we're here today to talk about EIP 1153 which is transient storage and this EIP adds two new additional op codes to the evm and this concept of transient storage which is basically like a key Value Store per account and anytime that you um you know UT store your t-load which are synonymous to uh s load and s store but you instead of putting in a state you put it into this transient storage map and each one is namespaced by the account and it persists throughout the duration of the single transactions execution yeah and so so actually we have this concept of um you know kind of storage is used sometimes in a transient way right now in the evm you can see this sometimes with like re-entrancy locks right so this is when we clear a slot back to its original value before the end of the transaction and then you know we're allotted some amount of of refunds um and you know to achieve transientness in this way is actually quite messy um from you know the developer point of view um you know it's really not straightforward you know how the accounting will work for this um especially because refunds are now capped um and so you know enshrining this directly in the evm is is a more direct uh kind of use case to get transientness um you know also developers are kind of having to go and do the sort of messy implementation where you'll see a lot of times like you know one zero one instead of clearing to zero we're clearing to some dirtied value because as we actually end up getting you know more refunds in that case and so it's really the sort of patchy way of achieving transientness in the evm and so kind of the way that that we look at this EIP is actually sort of a cleanup it's it's relieving some of this Tech depth here um because this is a real use case and it is wanted so one really cool side effect of this is it actually helps a lot with parallelization because um you know we want we want to scale ethereum we want to increase the throughput of the system and we can do that by paralyzing the evm and right now anytime that there is um a lock taken in a contract it is writing to storage and that prevents parallelization of that transaction with other transactions that are trying to interact with the same contract so if we move all these into transient storage instead then we'll be able to paralyze a lot more transactions and it's important to try to get this change in sooner rather than later so we can start adopting this pattern now and have more of the network using this kind of way of doing locks so we can have more parallel execution in the future another problem is that it's really difficult to know how much gas or how much gas is going to be used when you're allocating memory because there's this like crazy non-linear function so this makes it much more straightforward because every you know t-store uses the same amount of gas so it's easier for a developer to like know how much gas they're going to be using when they're writing their smart contracts cool and kind of on the on the last note here I just want to re-emphasize that this is not necessarily an addition to the evm it's it's really we're thinking about it as a cleanup a cleaner way of of achieving this use case in the evm I also want to point out this is a really siled change it's two op codes it's easily testable um and uh also benefit it's already been implemented across four clients so nethermind Bay Zoo ethereum JM or ethereum JM vs and uh also Geth is implemented we've also written tests for this they all pass um and so the kind of the final ask today is to just really get some more client Dev eyes on these PRS on the tests we've written and to actually kind of seriously open this conversation of having CFI for 1153 in Shanghai um thank you I guess just to make sure we have time like are there like a couple client devs with strong opinions about this we probably can't do everyone with a Demo First I have a male opinion on this I think it's awesome that people from outside the core team are not just writing the spec writing the implementation but also writing the tests I mean big round of applause [Applause] [Music] any other comments Peter so um only know just FYI I do like the EIP however um I kind of have a feeling that the russianizations are not necessarily all equally valid with the parallel execution I think that's kind of a far away dream so I I yeah I mean essentially every time you execute a transaction you will touch some state so if you touch the same contract you will touch the state some State anyway so that I don't think it helps there however I I mean person I think it might be nice one thing it would be yeah let's just keep it at that I I don't really see the point but I'm also not a not a not a contract developer um it seems to me like a nice to have but nothing that's critical for for us right now so I can read about that one of the one of the points that I do like about it is that with the with these mutexes essentially it touches the state and even if it does nothing just flip some bits back and forth it still has to touch disk and this would allow us to do these things without touching this so that's a net benefit for me have you talked to the solidity team yet there is an open draft PR with the assembly op code in the solidity repo right now um yeah I see like it's really paralyzation that has been quite a lot of work in analyzing how much the transaction can be paralyzed and even at flashbots we were running the analysis of the of the clashes the bundle clashes the transaction clashes so I think uh any improvements in this would would be quite nice to see there are some Builders there like I've seen actually the the modifications to get that were introducing parallelization and they were working and I was really surprised like how some developers were able to to do that in their own in their own implementations just for the simulation efficiency uh consider already gains from paralyzing transactions such as the the implementations are so complex and so specific for for searching uh that they are not coming to the journal View uh so it'd be great to see that uh I think the pitch was really great so I I was not considering this one because I was always thinking that anything that was touching the storage was awful amount of testing and huge risk of of something escaping and and some contract breakages so so this is my biggest worry like we were modifying the cost of storage in the past we're modifying the behavior of refunds um and this one feels a bit like that like when you say oh it's uh it's a known cost but do we do we cup the the storage okay I see yeah refunds are capped nowadays they started transition storage because if it's not capped it again it should be exponentially growing and it's it's the same it doesn't exponentially grow but we looked at it to see kind of you know the upper bound and it seemed like it was safe that is the same with memory right yeah so with memory we have the exponential cost and here we have the linear one yeah so I guess just to wrap it up sorry the but like is there something that any like we have tests for this we have we have patience I guess my question is like from the client teams very quickly is there something like you want to see from here like a big open question you have about this Daniel you have the solidity question but like yeah just to wrap it up anything you wish you would see from this that would help you better understand do you have uh tests or benchmarks for like just writing as much uh to the storage as possible writing in uh in like small chunks into it doing uh doing separate calls into different um different contracts that each write their own I would really like to see this yeah there's an open PR in the ethereum test repo I think most of those cases that you just said are covered I haven't looked in a bit but um yes reentrancy I think is covered um I'm happy to share that out with with the wider group but there's extensive tests something that I did want to do is use Felix's mem size library and basically just like look at the gas limit and just like you know fill in as many things as possible that would fit in the gas limit and then like observe the size in the implementation itself um I have that like on my like computer someplace but it's like not really pushed but that is like something that we could add add to make it better the next one one more oh one last question okay I just had a comment on this already depart um adding it to the assembly what the pr there wasn't a PR we just edited it but um it's easy to add the op code and you can use it in an inline assembly um and if the AIP goes live that's going to be added like instantly but adding it into language gonna take quite a bit because it's a lot of changes um I don't expect it to to happen anytime soon um so if you guys want the help in implementing it which may take like 1000 signs of code that would be welcome okay we need to wrap this up I'm sorry but uh Igor you're up next thank you my name is developer and I'm here to Champion EP 5978 entitled guest refund and reverse some motivation for this Eep is that reward of a transaction or any its sub calls drops any state modifications but the user has to pay the full price for the state modifications while with State modifications are not preserved for forever and this has two problems with this is that users overpay and then it limits some solidity patterns where you may may have a call and revert it and it makes it extremely expensive to the point where sometimes instead of reverting a call you may just like transfer easy tokens from one address to another just to like restore the storage instead of uh paying the higher gas price of reverting the call and in my opinion it's an anti-pattern because some side effects can be missed and it may result in critical hacks and lots of funds eventually and vcp suggests to reprise the following op codes as a s store create so so distract through the gas refund mechanism so they're not going to be free you still gonna pay some price for touching uh with addresses or storage variables but it seems unfair to pay a full price vcp is a in a in an early version so I just want to spread more awareness around this problem and hear what kind of comments people have thank you foreign maybe I missed something but um is the suggestion essentially to reprice some of the storage of codes or so because we've got and let's say slope is like sorry for clarity so if let's say if s load operation not sload s store operation inside the reverted transaction uh across 22 000 and then with some uh this call reverts then the cost of the separation should be repriced as just like attaching this load for a different number so you should not pay twenty two thousand for modifying uh storage slot which is not modified at the end of transaction and the pricing should happen through the gas refund mechanism uh does it bring any clarity foreign so I I have comments about like the the general idea so um it mostly means you would need to remember all of the changes you've done and then it's like single point on the reverts goes you would need to go through all of the changes and apply a compute at least the refund you you get from that so like you have like single operation that it's actually like unbounded in that complexity internally and that might be like doable if you have this journal Journal implementation of the state because you do also the like on journal the the changes but if someone has different implementation of that it might be really difficult to implement just one more comment usually with when we reverters you can actually have nested reverters and you that can end up quite nasty when you have sub cores that revert and the outer code doesn't revert then the outer cool again reverts so it's not a very very easy EIP to tackle it might not be too hard but it's I think the reason about it is not necessarily easy yeah I think the if if like that is assuming that we have to keep the general all the time and use it for all all the other apartment operations and uh it kind of did this idea fits into that perfectly I think that might be considered but if there is something that you would need to keep different data structure just for that I think it would be really difficult to have it and I know some of the implementations actually don't use the State Journal light I mean that's not something we manage to to be used so you would need a like in other words it would be forced to keep something like that anyway although it's not required uh in particular right now you have to keep track of a whole lot of information for um for the uh for the refunds anyway for the for the storage refunds anyway so it's the same stuff in all implementations I think whether it's journal or cash you have all the info you need um but I'm just still traumatized by some of the code I had to write for the um for when we were doing the the repricing of the the 2200 um I can't remember exact name of the of it where it was overfitted fitted to get and um our code analysis tool freaked out of the complexity of the code so I'm kind of concerned about that about the maintainability of some of these I mean I was implementing the algorithm as specified and sonar said that's too complex you can't do it obviously no okay actually um it was it was the same EIP transient storage um so without maybe just Reviving you know everything and reiterating everything there um it just just kind of adding on a few points one um you know right now some of the higher level oh sorry uh background uh so I'm actually a contributor to the Hof language which is a low level uh Assembly Language and then uh formerly um I worked with the superfluid protocol and so I think I think both of these could benefit from transient storage so on the Huff side you know higher level languages like solidity and Viper you know they have these re-entrancy locks or modifiers to facilitate re-entrancy locks that you know are very well built and it's very easy to build this in a safe and secure way but when it comes to assembly languages this is actually a problem because if you set a lock in storage and you don't explicitly free it by the end of the transaction those are now bricked which obviously you should catch this and unit tests right but it's just it's one more foot gun on the stack um sorry to interrupt you I think you kind of uh capturing the the governance process right now because we already talked about the CIP and now rehashing the discussionist kind of I think not great uh so I would if the if I would rather move on with the next EIP and uh because we discussed this one already yeah yeah let's move on because I have it done I do think though it is valuable to know that like low level languages and like uh projects like this if you can like write it on the eat magicians post or somewhere yeah yeah we could go we're not going to be done early though sorry sorry so I was going to say yes we probably don't have time to rehash it if but it is valuable I do think like putting it in writing on the East magician's post is probably good to document um yeah sorry uh also the microphone don't be scared for the recording okay cool cool sorry you had any IP as well yes okay um so I'm working on on 5027 um and also another VIP five four seventy eight so I just discussed five zeros 27 first uh so basically it aims to remove the right now the counter limit as 24 kilobytes uh that it was introduced in uh EIP 170 um so uh the reason is like I think the motivation is pretty clear that a lot of people complain regarding 24 kilobytes contract size especially right now the contract is significantly much more complicated when EIP 170 was introduced so the major concern of EIP 170 is basically DDOS attack uh if a large counter maybe fix 100 kilobytes that was deployed on the ethereum then uh if it's just charged in using a fluffy life exam right now is 2600 then you may uh specifically significantly under charged so basically the idea is like right now the solution basically is that split the contract to multiple contrast and then I call kind of like chain of contracts so they can retrieve their data but it basically make the whole logic much more complicated so the current my idea of solving this EIP basically the DDOS Tech the cup two weights one is basically introduced uh basically a contract called hash versus the to the counter size so when we call a contract I mean immediately to know basically the size because size is very small like four bytes number then we are able to pre-charge according to for example what the actual size of the contract is like maybe we can just charge uh charge to 2614 per 24 kilobytes so that we are able to basically have similar um right now gas behavior of calling multiple contracts but just putting it in a single contract so this is one idea another idea is we can if the counter size is greater than 24 kilobytes then we are able to append a size together with the current 24 kilobytes and then tells the what's the actual size it is and when when the first time it costs its first charge 22 600 about the the first 24 kilobytes and then the conscious size and then once we know counter size then we can further charge the corresponding the rest of contracts and then put in the memory and then execute so basically this is the basic idea uh when it takes a explore the ebm code and also I have a simple basic implementation together with some concerns of addressing warm and code storage and also become some of the P2P package size because right now um we have an imitation on the P2P packet size but with for example 50 million gas block gas limit divided by 200 gas price per byte and so the contract limit size actually is essentially limited to 250 50 kilobytes so right now that's still fit into the P2P package so the basically there's a couple of concerns that regarding this if we are able to remove this limit yeah happy to so um there's also EIP 3970 not 3978 um 3860 which is to limit meter and net code which is like we got two conflicting eips um so I'm not opposed to changing the limit but completely unlimiting it I think it has issues um I think it's Martin that has some really glorious code that shows the performance problems with the current jump analysis on on um the Legacy formats but another thing to consider is what if we change the code to required to be done in the eof and it's the code limit it applies to and then we could reconsider how much data you could bring in that is not subject to this because you're not supposed to do the jump analysis on the data the jump analysis is different in eof2 there's not the same risks but then as you mentioned you get into the issues of how does it impact the storage bringing out you know kilobyte not kilobyte megabyte codes code out of the storage um and yeah I I think the unlimiting is going to be a hard sell I think changing the limit I think is going to be an easier sell I guess my question is kind of similar to this that uh saying that 24K is too small I can definitely accept that I think that's a valid concern my question is what is reasonable because if uh if you go towards saying that well it should be arbitrary large and it will get so complicated that it definitely won't ship but for example saying that well let's raise it from from 24 to I know 64. that's that thing can be analyzed there we can put the number on it it doesn't mean that it won't require additional changes but it's relatively simple to understand the implications the moment you introduce these Dynamic changes I think that's that's not really Gonna Fly it's going to be too complicated in my opinion um yeah um so because I do some experiment and and uh using a code so right now I feel like pick some English synchronization and put out some deploying a lot of uh like some 200 kilobytes of country on top there uh it looks like everything is working fine on my test net um so uh has been running for um more than half years um yeah especially like right now in the gas regarding the jump analysis right now the glass metering is charged for 2600 per 24 kilobytes which essentially I think the sensation is equivalent to I call this contract and call another one this Country Corner another one so basically I charge this the chain of this calling to in a single but easy using a single payment so so if we tie this to eof um you can have larger contracts if you do it in an eof container we simultaneously solve the jump test analysis problem and uh reasons to motivate people to use eof so I I think there's a lot of things we can combine and trade some horses to make this work I I just in my opponent review of a why 24 kilobytes was chosen as well it was a convenient representation I think it was two to the six plus two to the seven and at the time of the gas limit it wouldn't have broken any contracts because it was impossible to reach that so I thought it's not possible to do this on uh on mainnet right now and I also not possible to to increase the code size uh if you're a client stuff and you're interested about my reasoning come talk to me afterwards um what is possible is to do it in UF and I think that's what you what you should strive for uh do it any eof when we have the the jump test stuff okay that's commenting just a tiny tiny bit of dot comment you mentioned that you had to test setup and running and it's been running perfectly the with all these changes that catch is it's uh the average case we know that it runs perfectly because the code is written well the problem is how attackable it is and in your private task that nobody is going to attack it oh cool thank you um dang Kratt did you have any IP do you want to talk about self-destruct who's the self-destruct guy Proto okay uh hello everyone I'm Proto I work with Opie lamps um we have this dream of serenity Serenity include us proof of stake and sharding they have achieved proof of Stack I'm on the continuous sharding I'm fully bought into the ethereum for this combination not for one and not the other and I think right now the process that eaps has been like kind of imbalanced with the merge because we have a exclusion layer consensus layer I think an EIP that's does both of these and actually it does touch on the testing infrastructure is the thing we need to repair it right now the merge was I think still shipped in relative hurry but with scaling we have actual incentives outside of just client teams to improve this infrastructure to improve analysis of ethereum to improve integration testing and so we can get the best of both worlds where we can improve ethereum and we can improve the process that we have to accept eips and so that we can be happy to enter future erps without as much concern because we have to write testing in place and then outside of testing and I will process just in case before it prefer if you're not familiar already before it prefer it increases the data for Layer Two layer 2 is meant to be an extension of ethereum you could think of the previous charting dream of ethereum as this execution charting thing where it was all the complexity left on ethereum itself layer 2 enables this to be more competitive and to be split from ethereum where we have exclusive layer as layer 2 and we have the layer bomb just focus on the securing data fatability and this is what this EIP focuses on and achieves and then through this means we can adopt a lot more ethereum users onto Layer Two and projects like like coinbase or other like larger ethereum users won't have to look at these ethereum killers in quotation marks where they can actually host these users at low cost um just because we talked about it a bunch before um the client teams have anything else to add or numbers numbers okay you heard it here um yeah I just I want to make sure we can get to as many people in the next 20 minutes as possible thank God we're gonna add something on I think I've already said the main things I wanted to say but yeah once off destruct I think Mario's has the pitch oh well do you want to do the pitch mares or do you want to listen to more of the other pitches um we should remove self-destruct um yes that's the picture we agree power okay and we need we need to remove self-destruct for vertical and history expiry and uh sorry State expiry and all of these upcoming changes so it needs to be done uh the question is do we do we do it now or do we do it later and I think it's it's a really small change so we should do it now for Legacy and the UF so just to I think this uh if somebody is not really on the page of why we want to remove self-destruct essentially every single op code on the evm is the cost is linear or I mean tries to approximate the actual execution that the resources it consumes and self-destruct is one of those OP codes where deleting the contract store essentially it's a it's a single op code call but it can result in an arbitrarily large execution and currently the only reason why currently it works is because self-destruct assumes that clients represent the state in a specific way in the market Patricia way and it it also assumes that the state does not get deleted from disk it's just a couple of branches of the market Patricia gets updated but the moment you want to do something fancier like what Aragon is doing or what gets new pruning is doing a sensors essentially self-destruct will become Berlin a completely unbounded up code and that's it prevents us from going forward with in the implantations Imagine self-destruct on usdc yeah two sorry yeah and and to like make this uh add to this like if you want to be stateless it would be an unbounded number of State changes and that completely killed statelessness so yeah I have only the comment that um the currency of this track has a quirk that you can destroy Eve with that and the question if if we want to actually like to make the the scent all work the same way or we want to kind of fix it and make it more intuitive I think so I think it's kind of the the choice between like more backwards compatibility between something that is more obvious how it works so the way I implemented it now it um just it doesn't destroy the user so it and and the idea for everyone defined by implementation right yeah okay the first implementation was okay and uh for everyone in the room we are not trying to remove self-destruct but we're changing it uh so that um so that uh the self-destruct will just send all of The Ether um that is in the contract and the but the contract itself will stay and so so it's like it will keep the current way the only the only thing that is kind of iffy about it is there's some pattern where you self-destruct and create uh to uh uh contract but there have been an analysis about it and it doesn't break too much stuff and and we and we talked to the people that that would that we would break with it and uh they seem to be okay with it how did gasto can take it um okay uh beside you Proto uh yeah sorry I don't know your name uh you don't think that's Mike yeah um Ronan uh I basically build uh on chain games uh I'm basically also other application uh many things actually but um and by that I mean application or games that have a zero back-end and where uh the user player uh provide their own node through the wallet the truth and in that context um I am building an indexer that runs in the browser um and so you can fetch the logs and it will all fine and but some application of game rely on time information and most developer assume rightly that the timestamp is available and so they don't need to add the timestamp in the event that they emit unfortunately the logs don't contain the timestamp information and so in my game for example like 20 000 events I can fetch them very quickly like in five seconds it's all all the state is synced but if I have to add the timestamp then I need to make 20 000 more requests and account even batch it because eip1193 which is only interface I have cannot do that so the idea is a very simple proposal is simply to add the Block timestamp in the logs object when you query the logs and actually someone also said you we could also add the timestamp to the transaction received Etc but basically yeah adding the timestamp information so one of my uh questions here is that long term the essentially long-term ethereum attempts to remove access from old chain segments and ideally I would also completely remove access from law accessing logs that are older than I don't know I would remove I would say a month three months something fairly high so uh essentially I mean is it already your consensus because we talked because I feel we are talking now about another thing like so what I was getting at is that uh this is kind of a consensus in ethereum that the past chain segments needs to be pruned otherwise the the network implodes and in from that perspective uh the amount of logs you will have to access is more limited so it might not be that big of an issue not to I mean you could always retrieve the timestamps if if you have a bounded number of logs you can access don't you think that if we go to that stage uh the the wallet interface will also evolve uh with a defined mechanism so that the application can remain decentralized or are you giving up on complete decentralization from the application point of view I'm I don't understand what because like most applications we kind of as a developer we understand that we need to index the data and that's why we use event um but okay so I think events are completely being misused and they are used as a database instead of events and in my opinion ethereum should use it as events and should everybody else should adapt but that's my two cents what what do you mean by using as event so by event I mean that that emits something and anybody interacting with adapt can react to it within a specific time frame but not to look up events that happened 10 years ago because that's that's not an event that's a kind of a database at that point yeah I mean I've also comment to make because I think it's a bigger discussion like a lot bigger than what we have time for now but because I've we have all applications rely on this so the reason why we use event of the database is because I mean the typical example is the NFC if you want to know the list of the token you own uh you can add and a lot do that they have this further called Fetch all you know by providing the starting index and the lens at which and and you can do that but it adds gas codes to the implementation and many decide actually to not do that and use the relevant and I feel it's it's an I mean I feel normal to to do that and I feel we need to have a discussion about how do we will deal with that for applications that really want to remain decentralized have you looked at using the graphql apis because I think you can go into a Blog from a log and then you can get the timestamp and you can do it in one step your graphql is not part of eip1193 which is the only thing I have access as an application uh the graphql is there's a standard for the graphql and it is in the execution apis so um gath and basic both implement it can expose it but you have really I think what is important here is that I don't have access to a node the only interface we have for application is the ip1193 so I I'm trying to have actually had a further AIP to solve this using graphql as a mechanism by which sorry sorry but um just because this is not like a core it does touch on the cardboard we're going to have a whole session about erc's on Friday um it's an infrastructure issue yeah yeah so I I and I agree with you there's like a longer discussion about like how applications use this stuff but I think yeah that's probably a really good one to discuss on on Friday um yeah thank you yeah uh yeah that will be at 1 pm I'm not sure where but in some schedule somewhere yeah it's on there's not a lot of stuff on the schedule Friday yeah yeah um Matt did you have one I have an EIP you get a minute I'll keep it short I'll keep it short um so my name is Matt I am an author of eip374 often off call more to the mic I thought she's going to take it away from me haha you're finished now um yeah so eip374 adds two new OP codes off and off call the motivation of the EIP is to improve the user experience of ethereum I think if you're using dapps today you're realizing you're signing tons and tons of things when you're interacting with a single dap and the flow that we have is not the best and with off and off call we're providing a very generic framework for adapt developers to Define like multi-transaction flows in a way that allows users to sign just a single message and they don't need to use any kind of smart contract wallets they get these types of benefits for free without deploying any smart contract well that's one reason another reason that I think the eip374 is very valuable is it lets all users of eoas sign a message to create some sort of social recovery mechanism and if they happen to lose their metamask or their Ledger or whatever wallet that they're using they can go and recover it with the people that they signed through and the third thing I think is really interesting with 3074 and is a testament for like how powerful it is is a proposal that Alex came up with maybe I don't know maybe last year about replacing the wet erc20 token with a contract that uses the ip3074 to natively move the ether balances around whenever you're interacting with the erc20 token so that's the the um there's some huge user experience risks with it is currently done and the revision took some of the guard rails off so um we don't have enough time to go into some of those those issues with safety and those are I think my number one concern on that right now but if we need meta transactions let's make a meta transaction transaction format and some of the other ones you know account abstraction yeah I think account abstraction can solve that that's something we know we want to do yeah okay so dude yeah yeah various ones to China man they're on the same team we have 10 minutes left sorry no um well well I will give a shout out there is an account abstraction panel I think Matthew on it uh later this week so if you want to go ahead and do a heated debate about the various flavors of account obstruction and fake account abstraction and 3074 right um we'll have a whole hour to debate it yeah so yeah so let's okay cool can you raise your hand if you still had an EIP uh that Alex you don't have one okay just oh no sorry I mean Alex B in front ACC now Okay no Okay cool uh how about it wait wait wait well do you have another one no no uh how about did you have one okay Daniel do you have a fully IP I have three of them but I only want a quick Ena okay I also have a quick one for a Yin a okay Dino first and there we finish with Matt so I just I just want to get a temperature check on the three other eof ones on eof function static relative jumps and stack validation good idea bad idea too complex that's really all I'm looking for okay okay so the first one is eof functions where we'd have the call F and we'd split it up in different functions and had multiple code segments good idea bad idea too complex okay static relative jumps where it's an immediate operation you say jump ahead 10. yes okay and the stack validation which needs the functions where you can say that this function is only going to take five stack items and if it goes and it's not going to overflow so you could remove the Overflow check my thought on that is it's a bit complex to get into Shanghai so that's I want to see if I'm the only one of that opinion I wasn't saying that the other stuff should go into Shanghai I think it's it might be a good idea in the future good idea just Cancun or later so cool so no one actually proposed uof um so I guess the UF is approved for inclusion oh but I just wanted a temperature check so let's not discuss it oh sorry oh yeah these aren't in yet okay okay can I like like one comment is like if you combine functions and relative jumps you can get rid of of uh drop this analysis entirely because they kind of replace that um unless we get rid of the jump code too um the jump board jump and jump I would have to get rid of those to get rid of jump test analysis yeah like to remove all of these we can do that with these two features but uh like the way we didn't Champion it like because I think that's not on us to like actually say it's great because we need to input from people that say they want to use that but thanks for mentioning and yeah we have evm panel on Friday as well I do have another VIP I didn't wanted to like talking to UF because we spent like two hours on like protocol Workshop um but this app is really cool it's called M Copy for memory copying it's not merged yet because of the EIP process but um I'm going to summarize it so basically the only way to copy memory right now there are two ways one way is to do it with the loop amp store mlodemp store and that was really recognized and the identity pre-compile was introduced um I think the first like few months after the launch of ethereum that was used by the solid compiler but then with the Shanghai attacks it was repriced the call was becoming too expensive so nobody used the identity pre compiler anymore it is just there I think Viper use it now but solidity still uses the loop um so then the mem copy op code fixes all of this and I just I'm trying to read the numbers so yeah it takes like 800 gas to to copy 255 256 bytes um with the Shanghai cast with the recent cost is 160 we demo them towards 100 and video IP it would be 25 by 25 gas we did some analysis um I think like 25 of all the memory copying would be improved by M Copy um and there's actually one feature in the solid compiler which is kind of be I mean it's not blocked by this but it's not implemented slicing of memory arrays and a lot of cases people are doing like forcefully using call data stuff um because that can be sliced in the compiler setting mem copy a cheap mem copy would also improve solidity as a language that's it yes the reason I say that half EIP because um it's almost certainly not for Shanghai but I don't think it's been talked about at all and it's nice to like get people's brains Brewing on it so um first of all the ones based on this ERC 457 right which is so much the kind of abstraction that's like a way of getting a kind of abstraction without requiring a hard work to avoid all those like EIP process mess um and um this we found that people quite like this approach right because um they can already start using their smart contract vaults but we found that users actually still complain quite a bit about um smart contract worlds because um like they already have their money on EOS and switching all their balances and all their all their nfts and everything is just too much for them usually so we were thinking quite a bit about okay like how can we develop a kind of abstraction more how can we perhaps enshrine it a bit and so some ideas just floating around again there's no EIP set and there's no like specific roadmap set but an example is um making a new transaction type which converts an eoa to a Smart contract that you specify in a data field right and this basically should be quite a simple new transaction type there's not really that much complexity as far as I can tell but please love to hear some comments some more advanced ones and again just ideation is perhaps making an EIP which converts all current eoa accounts into a sort of default proxy smart contract World which uses the current ecdsa signature scheme that EO has already used right and another sort of a more advanced one is so this ERC 457 it works with a so-called entry point smart contract which is through which you root all your user operations to interact with your wallet and this causes a lot of gas because you do all the signature verification all the um all the stuff on chain using well evm up codes right so what if instead you made this part of the protocol right and that could be validated outside and so we'll save usually lots of gas any last comments oh yeah just real quick this isn't on that but uh it hasn't been suggested for Shanghai but prior to the merge it had a bit of support uh time aware base fee calculation it would essentially just make 1559 quite State friendly 1559 is aware of blocks it's not aware of slots you could have say like an empty block with proof of work but now you can have missed block proposal so here we go I think with a with the amount of missed slots we see right now I don't think it makes sense to to do it now it's it's like I don't know we're seeing like point point one percent point zero one percent of missed slots or something like this it would be a negligible Improvement to ux so yeah just for a cleanup but not necessarily to have so uh with the wallet uh I think there was three proposals uh I just wanted to mention there was one proposal where uh you saw that we could just Auto convert everything uh no that's not going to happen essentially uh that that's already a huge issue for vertical trees where you just want to do an upgrade where the state just gets flipped over and it's a huge linear migration and we have absolutely no idea how we're going to do it for work or trees so let's not do it twice but what if it's not actually touching every account and rather it's there's no code and there's a message signed from that account and it's treated as a default account and so it falls back to some default code um but that would actually break the new semantics that we introduced with right you know which which I mean that no eoa can also have contract code but there's no code in the account and so it's already empty and so it would basically be like me sending a transaction and rather than executing it the same way that we do today it would realize that the recovered address has no code in it and so then it would just start executing it in an evm frame with some default account code that implements the same concept of the ecbsa account okay I couldn't follow so so you wouldn't set the set the contract code you would not set the contract code okay it's a fallback okay I think we're going to wrap up uh it's past six um so first of all thanks everyone for coming um there's more places we can discuss all this this week so we mentioned there's an evm panel where we can get into eof 1153 all that good stuff there's an account abstraction panel uh and we just had some new fresh account abstraction content as well and then finally there's an ERC uh kind of Youth magician session as well um throughout the week I don't know when they are sorry they're all on the agenda is on Friday at Workshop room 4 at 1 pm you heard it um oh product right and so Friday there's a session about tank shorting and Proto tank shirting if you're interested to the helpless of the Erp for Edge referred just please contact us and we are hosting co-work sessions cool yeah thank you so much everyone for coming [Applause] [Music] 