cool so hi I'm Jeremy I work at file Klein is mentioned give us a short presentation on how we're thinking about using BDS and how exploring different ways that we might do that our protocol so there's three main areas of exploration here there's we have two main proofs proof replication and proof of space-time which both have timing assumptions and so currently these are like kind of dependent on VDS and in a way and then we have expected consensus which is our consensus algorithm where we're exploring the use of BDS to improve security but is not necessarily needed so proof of replication for those who are unfamiliar is a slow unique encoding of some input data can be any data it protects against deduplication attacks this is important when you're like giving people some resource based on the number of reps because they have and in the context of the proof of space-time this protects against what's called the generation attack the way it kind of works is you take a file you break it up put it into and you map it onto a graph you encrypt it based on the edges of the graph which is an inherently like sequential operation and then you use the outputs of that to create your replicas it's kind of like a CBC mode encryption except more robust against removing random those in the graph and regenerating things and in order to slow this down you can add more layers to the graph and make it even more sequential and so the main the main thing this helps the timing assumption here helps protect against is what's called a generation attack so imagine you have an honest person and an attacker and you have some verifier the verifier send it sends a challenge to each person the attacker doesn't have the data so they haven't done the the seal they haven't made the replicas but the honest person has so when they receive the challenge the honest person can immediately process the challenge while the attacker has to start doing this slow encoding operation the honest person can respond immediately while the attacker still has to continue doing this slow you know effectively sequential operation they finish they can do the thing they can send it back and it's some very noticeable amount of time later and they get caught so Orbis video yay basically this whole process is slowing forcing you to slow down and encoding such that if you're trying to recompute the encoding on the fly' you get caught and so we'll take some inputs produces an output after prescribed time the same inputs always produce the same output and the verifier can efficiently check that it was done correctly this is done with some like snark Merkel proof magic which is actually the fast part about the whole thing and so that's kind of one way that we're like it is a video and you can keep it you can prove you can slow down the the attacker even more by this graph here is actually each of those edges is a an encryption operation and what you can do is the key derivation function for each of those lines can itself be a PDF and so you can take it as an input run it through a vdf like if we have a nice fast you know exponentiation vdf and then use that as the encryption key for the next node so now in order to encrypt node seven you have to encrypt to five and six in order to equip six you have to encrypt four and then that requires five and three in which required is one and so in order to re-encrypt any random node in the graph it requires a long sequential computation so that's one of the things the next one is a proof of space-time so this is kind of like the other side of this so this proves to the verifier that some known data was correctly stored over a given period of time and then so the main thing we're trying to prove here is that the space that you're claiming to have wasn't reused for some other purpose during the time period that you're proving it to us has a non-interactive succinct output or it's not interactive and has a synced output and it must not be computable too much faster than the expected duration kind of sounds familiar so the way this briefly works is you have some time period you have some input challenge and then the prover takes that challenge does some like merkel proofs on their entire storage then takes the output of that into a PDF slows them down does some more challenges and other BDF another PDF you get a keep doing that and at the end you gather all this together and you have to compute a proof you know we snark it up and then you submit that to the chain and so what what we're doing here is we're trying to make sure that like ideally this entire thing would just be these challenges like entirely consist of proving to me you have ranted pieces of file the problem is you can't prove that in a snark efficiently and so we lower the number of actual challenges that you have to do and replace that time with something that takes a verifiable amount of time and so you can lower lower that a bit because these challenges can be done really arbitrarily fast and are like almost cashable if you have enough space so adding this slowness in here the attack is if somebody can do this much much faster they could take and if they could do the entire thing in this amount of time then they could wait until the very end regenerate all their data and then just do it here and so for the rest of the time they could be reusing this space for saying they have all sorts of other storage which is bad and so what we can also do here instead of having each of those blue lines be a PDF or in addition to those blue ones being with you death who knows we have a random beacon that is made with constructed out of EDF and use the output of the random beacon and each set to reseed these so this whole this means that if you have an advantage you can't you can't complete this early you have to wait at least until the very last random beacon element is known and then you're able to complete it and so combining this with a random beacon basically allows you to prevent an attacker from going arbitrarily fast this whole process does look a lot like a PDF there is some fixed input that produces a Danone output it's verifiable and takes some prescribed amount of time and internally is comprised of many little pieces that are meant to slow things down yeah that's kind of like the main our main like use of ETFs in there yeah yeah so this would be like the the Rando random beacon that this is proposed by a theory on people you know or some other mechanism yeah so here's the interesting thing it's given a sufficiently large advantage let's say that I have really really really fast computation T of all of this all this work I could just wait until here and do it all right here obviously that's really really impractical and the expense of doing that and like the hardware required to do that would be absurd but just keeping this vdf in here makes that much more expensive and makes it much more unlikely that you'll be able to pull that off and it's it's not strictly necessary but it does like this is this is a game of increasing the costs for an attacker that's all this is we're just trying to make an attacker have to spend a lot more money to beat us like the whole vidya thing is that so then yeah then the third the third like piece we're looking at is our consensus mechanism which elects on expectation one leader per round which is why it's called expected consensus includes all blocks in the previous round as parents it's and it's kind of like a proof of stay key consensus protocol and one the thing we're exploring is a lot of these protocols you can look look into the future to see the randomness generated like if you went this block if you wanted this block if you want this block and a way to prevent people from looking into the future in these is just forcing the computer PDF between every block this is only more useful if you have a like a very strong PDF because these is like 30 seconds and if an attacker has a 10x advantage it's it's not super useful oh that's like a low Emacs sorry a low a max yeah so if we can be more confident that the lower bound is a lower number then this becomes more useful you also have to worry about network propagation to like make sure you block times right it goes it it changes a lot the etherion people say they're targeting a hundred x with the ASIC that Supernationals looking to build we're targeting 10x at ten a max so it's it's unclear but that's like more research to figure out what that actual number is yeah yep and so instead of running a vdf between each of these we could also use a random beacon the random beacon means that you can look so depending on how you construct it there and a beacon has the similar problem where you could collect all these things into the past and then you could since although all the randomness is now known in the past you could reconstruct a chain depending on your algorithm like depending on how you you allow people to do this you could reconstruct the chain from the past and so if you force enforce the vdf between each block then you wouldn't be able to easily go back and just recompute an entirely new chain because you knew the randomness and I think we talked about this last night you had a point about if the chain does influence the beacon then that won't work so the beacon if you're using random beacon like this you need to make sure there's some reseeding period yeah yeah we just have one it's just one beacon somewhere you know it's great we could take we could put these vdf hardware on satellites in space and just like throw them out there and they can beam back random numbers it's great just don't worry about it what are we doing here so yeah that's that's what we're thinking and if you have any questions let me know otherwise let's talk later 