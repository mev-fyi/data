[Music] [Music] [Music] so [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Applause] [Music] okay good morning everyone welcome to all core devs uh 135 i posted uh the agenda in the chat already i think everything is working on the live stream um so a couple things today a bunch of updates on the merge so uh some updates mostly on the shadow forking i don't think there's much on killed itself and then mikael had some stuff about the latest flatted hash conversations we've been having uh then on shanghai a couple things as well so alex had some updates uh on the withdrawal eip based on the the the call last time and then there were two other vips uh the s transient storage and the removal of self-destruct that were that were brought up in the comments and and then after all that uh we have afri here to talk about um some of the issues we've seen on gordy and some potential solutions to them um i guess to kick us off uh perry do you want to maybe give like a high level recap of what happened with shadow 14 where we're at right now i saw you you kind of posted another discord this morning as well but just yeah kind of bring everybody up to speed of course um so the last since the last acd our testing efforts have been even focused on shadow um so what we mean by shadow forking is we take the config of a test net in this case gertie and then we just modify it to add a ttd and merge four block so that means up until the ttd we'd be following the canonical quality chain and post ttd all the notes configured with the modified genesis json file would fork off into their shadow form basically um but since we've set the merge fork block to be far in the future we continue importing transactions so the main idea is that we're able to test um syncing we're able to test actual load block production under load and so on um we had two attempts so far the first attempt followed a mainnet distribution and we found issues in i think every client especially related to expectations of how quickly timeouts should happen or or how block production logic has to work and so on um so we had a second uh shadow that happened day before yesterday and that was a lot more stable we didn't do mainnet distribution for that one we did a more equal client distribution um i think we've only found like one or two minor issues um and they have patches being released so we're attempting a third version again with mainnet distribution ttd is supposed to hit on monday and information about the conflict has been shared on the testing channel in the rnd discord and irrespective of how this one goes we want to already try a a mainnet shadow fork sometime next week and the main idea is just to collect as much information as possible right now so that we can make an informed decision about about the merge sooner so one thing one thing where we where we want to push for the main headshot so uh so quickly is to collect data on how the clients behave on mainnet because um we've seen on the goalie shadow fork that like thinking becomes an issue um like bigger blocks become an issue stuff like this and so it would be really nice to see [Music] how clients work post merge on mainnet so one thing for example was that we like we provisioned for the yeah sorry i'm uh i'm uh not at a great place so one one thing we had uh for the first uh shadow fork is that we provisioned um eight gigabyte nodes and uh because of uh finalization non-finalization these uh ran over and uh geth ran out of memory and was killed and and then was killed repeatedly and uh stuff like this is i think really important to see on on mainnet state and on main transactions yep and we don't really have an expectation on client teams to spin up their own nodes to test in i'm spinning notes on your behalf um more or less every client team knows whose ssh keys are on which machine it would be great if you guys can have an eye on your nodes um test weird sync states start syncing switch els midway through sync weird things that people might do there could people provide a bit of details behind those timeout issues um yeah so that them like there are multiple issues there one was apparently no client has a has a pause between creating a new payload and retrieving the payload so just updated with it with payload attributes and then get payload um so all and because gas right now builds blocks uh synchronously in in focus updated that's not an issue um but for nethermind which builds blocks asynchronously every block is empty because they only have 100 milliseconds to to build a block and there was one issue the other one was that clients didn't give us enough time to execute the blocks um so i think some clients did have had a time out of 500 milliseconds for um for new for new payload a new payload can like we can we can and we do execute blocks during new payload and so yeah we we we timed out and uh and so so some clients weren't able to think or able to follow the chain uh what was it http or websocket which communication protocol has been used uh i think i i think http i'm not sure like right now only nimbus uses a websocket as far as i know or maybe they also use http no they're also using web sockets right now yeah and they're the only ones using websocket everyone else is http yes and i'm not sure if we had the same issues for nimbus but uh for the other clients it was yeah i would expect like if it's http i would expect like timeouts set to something some like to speak big number to one minute or whatever even more than the uh slots uh seconds even more than the seconds per slot friendly so i was gonna ask um would there should we provide some sort of recommendations for timeout um duration i guess just for a different method there may be different hammer recommendations and i think it's fairly important that all client kind of uses the same timeout so we don't see some sort of network asymmetry down the line yeah i mean timeouts are fundamentally dangerous because if there's a timeout i can construct a dos block that can sit right on the border of that timeout depending on the machine then i can split the network so uh you know on the order of something far beyond dos blocks uh should be the timeout if there has to be a timeout but yeah maybe there should be explicit recognition do we discuss timeout for uh fortress updated or for new payload to uh because for purchase updates that we should to have relatively uh small timeout i think why because both of them have execution semantics baked in so both of them can have the same uh um right right sorry yeah all right and then in focus updated you might have to run re-orgs so yeah i was just going to say the same so it could be like a rework that is processed synchronously like for if it's like two or three blocks and poke choice updated like what marius said so it should be more the timeout should be should be enough to process like a few blocks so i would expect i would even expect that websocket is used i don't think that websocket is has some timeouts like uh like inside of a session also your case though it might actually be better to have the execution layer signal that it is still working via maybe a call to syncing um a return to syncing rather than hitting the timeout because then the consensus layer has no idea what's going on and i think that since the communication channel was like a trustworthy relationship between cl and dl i would expect that the match should be relatively high so it's not like uh el is just kind of fine it could be really in case of like attacks where every block uh is huge and it takes let's say 20 seconds to execute i don't know it's just a random number it would be useful to have enough time to process such big blocks so the chain will be able to make progress in any case otherwise if if if each of this block will be timed out we could have a blindness failure i think like um so coming from consensus perspective from prism what i can say is that most of the issues or most of the edge cases we're seeing now is always regarding time out i think this is a very tricky area to tactical just before we're shaking time out like the pillow is invalid but that's not true because the pedal is not invalided but it just means that we have to try again later so we discovered a few edges there and it may be important to to basically like i said uh define some time of uh values uh across all the methods so make sure everyone's on the same page yeah um i think yeah we should we should uh defend us uh async and not not to them now one thing one thing that i like maybe i didn't hammer this point home home hard enough because perry wrote it just wrote me about it uh basically the the time that the consent all the consensus layer clients currently wait between they between when they ask us to construct a payload and when they fetch the payload from us is too short and so if we were to switch payload construction to async which is something that we want to do in geth every block would be empty and so that right so essentially they're supposed to be making this call something like four seconds or more before because they know that they're going to be a proposer and instead they're making it like very in rapid succession yes so so they they send us the payload attributes uh we we create like we start creating the payload and then they already send us uh the get payload and this is like 100 milliseconds uh apart and in these 100 milliseconds we cannot create any blocks and i think this is going to be it's an issue yeah i mean the intention here is certainly to have on the order of like a third of a slot if not more of lead time and that's just that's consistency should be doing that if they're trying to have anything profitable coming out of it um yes but we can make a clearer recommendation i mean it should be as soon as you have as soon as you know the four choices of the prior slot and you know you're the you're the composer of the next slide but i think i noticed it uh some time ago and i reported it and i think only tackle uh doing it in correct way uh yeah sorry that was in the correct way or incorrect incorrect way tackle is correct sorry got it got it chris some had that uh fixed we actually just began cashing the pillow id the slop higher when the hedge changes um i can um give you a new image to try yeah danny you said a third of a slot is the minimum why is that shouldn't you always know about one slot or more in advance uh yeah you should i mean the you just you get progressively more information during a slot about attestations um so you you gain confidence but you should know like if your slot is coming up like as soon as the previous slot shows up i feel like you should be able to start building the block like you don't maybe that you're building on the wrong thing but you should be really agreed so as soon as you see that block you should begin the process you might during that slot abort and provide a different head with a different one um so say for example at slot in there's the the proposer slot in actually makes two conflicting blocks and they're going to get slash but there's actually two conflicting blocks out there and maybe uh what i see is the head initially and i begin my build process on actually switches because of the weight of attestations throughout that slot you know so then there are these edge cases where you might switch but you're right as soon as you get the information even if it's low confidence you should probably begin the build process and only if you fork choice update you could change the build process uh if you if you in these edge cases are any of those edge cases non-slashable yes it's deep enough yeah if it was a deep enough fork well yeah a late block potentially or a deep enough fork where the shuffling is diverged um then i might all of a sudden resolve that i want to switch forks halfway through and i have a also another comment related to timeouts or a question uh is this possible to process multiple payloads at the time on the old side just curious how it's currently implemented in the clients yeah like for example one payload takes quite a lot of time to be executed and then another new payload is coming will the first one be processed in parallel with the second one uh yes so that's um no it like it won't because we we have vlogs sorry were they both based on the same state yeah probably both on the same same state like they're just sister blocks and theoretically we could uh do them in paradise i don't know if we do marius probably knows not better no because we have we have locks on the on the blockchain data structure so we cannot like it depends on what you mean with uh with uh like uh execute them so we cannot we cannot like execute the transactions we can for example verify like verify the block header and stuff like this and this is all done in parallel but we cannot insert them into the blockchain in parallel similar in the mind we run validations etc in parallel that's fine but actual execution not because we have a concept of canonical state which is being modified and this is how it works today on the main edge right yes okay thanks i suggest we continue this conversation and discord yeah i think that makes sense um we can use the interrupt channel uh to do that um anything else on on timeouts okay uh next up uh mikhail you had uh basically there was a conversation on discord this week around the latest valid hash um i shared the discord link in the agenda and you put together kind of a summary explaining your thoughts about it you want to maybe just take a minute or two and and share your thoughts and um you can share your screen also if you want with with the document let me share my screen um okay do you see it ah yes okay cool so yeah um we've been a bit touched this late latest valid hash and apparently it's it has some complexity and supporting it so i've just put down some notes on that some initial thoughts on how this could be implemented and when it's important uh so tldrs here we have two cases uh basically um that should be considered during implementing this requirement first one is important and to my opinion it's when the payload realization happens synchronously so it's like when you submit the new payload and it's responded it appears to be invalid uh this is the easy case right so here latest valid hash is also the print hash and it might seem that this latest valid hash parameter is redundant in this case but if we take a look at factories updated which has the same parameters in response we can conclude that this would be important in the case if there is the optimization of short-range rewards done by some clients for example aragon that has been discussed previously on discord that this optimization like when you really work and the new fork is like only two or three blocks it could be the case when like the first block is invalid in this fork and what you do is uh respond with invalid and in this case latest valid hash will point to the first block in this work and this is important because we need cl need to invalidate uh all uh all the blocks uh that uh starting from this uh like first invalid block so that that's this is where it should be like implemented it should be straightforward because uh el client has all the information uh and it re it just sends it back to cl um so the other part is when yell is sinking and it makes an invalid box somewhere in the middle of the chain it's it has been syncing with and the el is syncing with this chain because uh consistently a client said that this chain is canonical and fed all the required information to el to start syncing so in this case implementation would be a bit trickier there is there need to be a cache that will store the tip of the chain that the l nodes that the l observes the tip of invalid chain and this type of invalid change should be accompanied with the latest valid hash printer um so that's the kind of cache required here uh and this this check is pretty simple um so every new payload if if its parent hits this invalid tip hash so it equals to invalid t hash so the uh in this case having this information uh ill client just may respond correctly to this to to this request um yeah we we should not account for i think that we should not account for the case when the some payload is missed and the re and el can't just build the chain uh like to understand that this payload uh p1 for example uh is linked to the invalid chain because uh for some reason cld didn't send the all required information to build this link and that's it so i think that this these two two steps would be enough for implementing it uh in the asynchronous without validation case um also yeah these are this is the option uh it's it's not that complicated but uh it's anyway complexity and the the other question that is important uh what what would happen if we do not support uh latest valid hash uh or we know we do not support like uh sending this information to cl while el is thinking so what may happen is just cl will not know uh that this chain that it follows uh is invalid and will still keep following it and pull from the network if it's available in any place of the network probably in this case a node is under eclipse attack and this being fed with invalid chain uh ndl will just uh like assume that the l just see the invalid block and drops the entire chain and cl doesn't know about it anything and keeps sending uh new blocks from this invalid chain they all started syncing again and this going over and over so in this case what will happen is just either cl will observe the valid chain and rework to it just because all other validators will have a switch to the valid chain and it can be seen from the attestation justifications and final finalization updates either this happens or a user will see that node is not progressing and we'll have to do a manual intervention um so okay so two out out comes from this this lattice valid hash is really valuable thing for this first case and this must be implemented uh this one is like optional i would say uh to my observation so but people may have other opinions on that so sorry i don't like should we only cache one invalid tip hash or should we cash all of the inventive cash questions i would say that you should uh have probably l review cash like for a few entries so and just catch them and drop the most latest one that were was added to this cache it's like uh yeah this this case is only uh actually at at one point in time there will be only one invalid chain that matters because uh el only syncs with the canonical chain and if uh canonical chain is invalid then it matters uh otherwise yeah so there is only one chain yale is synced with this is the assumption and this was also discussed previously so basically yeah probably there could be two chain uh a reward happens that might make sense to store in this cache two three five but propanol should like be on on every chain that new payload is sent for because yeah because in this while sinking uh what only matters is the canonical chain uh while this is synchronous payload validation there is the response is immediate and all the information is available to to make a response according to the spec so um two things so uh i mean you know i wouldn't say that this will most likely happen during an ecliptic attack uh most likely the el has some data corruption in the database or in bad ram or something but i don't really get this first bullet point so we're fed a new payload we should return the parent hash how do we know that the parent has the most recent valid what if we don't have parent uh because if it's yeah right if it's in chronos uh yeah yeah you're right yeah fair question because uh if you don't have a parent you would respond with uh sinking right so it's it falls to the second case not to the first one synchronous like validation happens only when you have all the information available and the current block is known and obviously it's valid in this case like the assumption is that this block is valid okay so so it won't cannot have so if we're that new payload block thousand and we say no this is invalid and then you say okay here's the new block 1001. should we just okay i'll just have to go think then uh yes you will have to think if cly consists on on sending a a child of invalid payload so it must be something bad with cl because it it got rid of a response that the parent is involved so marius we will never restart the sync after we're synced right we haven't changed that i think he means more like reverse header sync like if you got a block and you want to find not not like genesis sync if we don't have a the parent to some block then we will start thinking so it's like as in trying to fill in the gaps of the parents to find its roots in the chain you know about yes all right so martin in your example the if the consensus layer sends the next block in it forgot that you already said something was invalid um so this does you know this could happen theoretically like if the consensus has a failure or the consensus layer turned off and then turned back on or something like that but we do need to handle the case where it does try to insert something that is only switch consensus databases right or just a simple race condition uh not only invalid uh so if the el returns sinking but then while syncing it it discovers an invalid block but uh because the engine api is it has already replied syncing on the engine api the consensus layer doesn't know yet that the block is invalid it only got syncing for that block then it starts building on that new chain and sends new and new blocks to the eo so for every for arigon it's currently a problem that mikhail has described we don't have this cache we have to implement it because you see the problem we reply to syncing and only after that we discovered that some blocks were invalid yeah yeah i think we did something similar like we put them to some future queue complete american and at some point we're going to execute that future queue and might notice that one of them was that so i've heard that nethermine's stores like a few uh the most recent invalid block hashes but this uh this neither the invalid t cash nor the latest valid hash it's like she's the uh the invalid child of the latest valid uh block so yes for never mind so we were thinking about it and our potential potential fix for that is to keep invalid blocks in our block tree and just mark them as invalid uh to have like the whole thing and just when something is finalized we can prune the invalids ones from the tree to like not keep garbage there yeah and so you can just query um the database right so when you insert the new payload into the block you may check this right that the parent is valid oh and yeah we could we could we could go to like the whole ancestors right train right but uh yeah this sounds like uh an attack vector i mean if someone if this chain is you know uh pretty long and you you should always uh hit you should make like multiple database reads to to just traverse this chain to find out the latest valid hash yes that's a potential problem so yes yeah but cache is not enough because if we restart our node it will be in let's state again so we can't just store it in memory cache i think you could recompute the cache essentially in that if you get some block deep into an invalid chain you recursively walk back and then you'll see oh this is invalid and then you can have the cash again yeah but storing it is fine too but it would be recomputed if the consensus player was continuing to pound information in that branch yeah once again i do not think that it's super important it would be really nice to have this supported during sinking danny what do you think on it i um i think it's very difficult to construct realities where these full any number of um are like induced load on any number of large amounts of clients um so you know i want to kind of minimize complexity but also not leave this as a glaring hole i don't know that's not much of an answer yeah i need to think about it yeah we may just probably think more about it and and discuss yeah i mean the problem the problem when when you say anytime you make a claim that this is only a problem this can only become a problem when a node is sinking um then the attack vector becomes can i induce a lot of nodes to sync and thus make this a problem um you know the answer to that hopefully should be no but like you know a bug if i can find a bug that can induce lots of nodes to sync then i can maybe exploit you know an edge case in this that we didn't want to deal with yeah the worst failures are always compounding failures where you have you know one thing goes wrong and then something else powers on top like those are the ones you gotta pray build against marius um i mean they essentially they have these branches that because of the way fortress updated can return an insert payload can return syncing syncing syncing syncing they have these branches that uh once the execution layer does finish syncing uh you know they need to resolve these as invalid or not and if not they just have these branches that they just don't they can never really know if or valid or not um if this cannot be resolved i mean the edge case is really i have i inserted a branch it has n blocks now you tell me the the nth block is invalid what about the the n minus one blocks before i don't know um and then i don't know what to do with them is the that's the problem so it's kind of oh i get it no thanks yep so um need to think more about edge cases for the sinking part of it but for the when the parent block and state are known and it can be and this information can be easily derived i think it's must to be implemented um yeah i like i don't think it's just a must yeah that's bad martin your hand is still up sorry okay always yeah i mean just quick on this at this point we should find a solution that works i think very much i would prefer to not change the structure of this response and just figure out the most correct way and complexity minimizing way to utilize the value that is there yeah and moreover we need this structure to be preserved for this first part so it's it should be it must be in place it will not change yeah yeah this this question for how to implement the second part i'm still kind of open yeah and i mean additionally i think we need to figure out a way to induce and test these edge cases on in some sort of environment because it's hard to actually run into these naturally on some of these edge cases naturally on chain um and this cache i don't think it should be persisted actually so it should it would be enough if yell has restarted and this cache has gone then then yeah then the situation will likely repeat sent the el will be able to respond correctly after restart yeah yeah yeah right yeah yeah yeah right so it will we can't remove uh invite blocks from block 3 i'm not sure how f is working right now but we are uh on mainnet we are removing inviting blocks from block 3. i think you can remove blocks from block 3 if if you do this on on my net currently only information need is basically these two values so in cash so you don't need to store invalid blocks for this purpose to serve this latest valid hash correctly i don't think so okay i will think about it more and i guess yeah we can use the interop channel to discuss this as well and hopefully have something i guess by like the consensus layer call next week which yeah which we're happy with um anything else on this okay um oh andrew sorry go ahead yeah um yeah so i would like to say that um in aragon we probably most definitely will need to implement something like this because it's a problem and i would like to have um tests in in hive or somewhere for these this kind of scenarios um and also on how things with the merch are in aragon i would say currently our implementation is uh uh is alpha quality and uh before um switching um public testness i would like aragon to reach uh beta quality and um we need to fix the tests in hive implement this at kh case and quite a few things to finish and also start um start preparing a release so i would say it will take us roughly a month to reach like to move from alpha to beta [Music] and yeah from from aragon's from my point of view i would like quite a bit of time before before switching public testnets to profile stake right um that was going to be the next thing i bring up uh just want to make sure there's nothing left on the on the latest valid hash but then i can kind of share my my thoughts about that and how we're tracking okay uh yeah so so basically uh i guess you know everyone's aware the difficulty bomb is set to to uh to happen um it's probably it's gonna start being felt around june and and kind of slowly ramp up from there um tj rush and and others uh somebody else has like a dune analytics dashboards and vitalik has a script which estimates it and basically roughly around like the end of july is when you start getting blocks which would um which would exceed like 17 seconds which seems pretty long and it's hard to all these are estimates it's really hard to to estimate how quickly the bomb goes off once it actually starts starts uh having a bigger impact on the overall hash rates it's probably even harder to estimate now because if the merge is the next upgrade people might start selling their miners and so take all these you know they took like a grand assault um but basically i think if if we want to avoid uh pushing back the bomb what you what you'd want is ideally not reach like 17 second block times and people can you know disagree on the number feels like 14 15 might not be too too terrible um and a few calls ago vitalik had this idea where uh if anyways we are gonna um we are gonna put out a release uh with sort of a fake fork block uh in order to like disconnect nodes uh who who haven't upgraded from the merge so just changing the fork id and we could do kind of a mini bomb push back um but that only buys us a couple of weeks because it means that like we need to make sure that um you know by the time the pushback happens on main net the bomb is still manageable so all this to say um if we want to like aim for that sweet spot of like we we we upgrade and run through the merge kind of before we hit like 17 or more second block times um working backward if you want to have you know reasonable time for like a main net announcement and then reasonable time for test nets um we probably need to make oh i guess we we need to make a call about like the test net fork blocks about a month from now so like two not necessarily the next awkward devs but the one after um i think if we're in the spot like like late april we're like we're not ready to say you know the fork is gonna happen on test nets in like a couple weeks um then i i i think we probably want to consider like a longer bomb delay um and how long is something we can you know we can discuss but that's i think that's roughly where we're at right now where if we if if like in a month basically four weeks from today we're comfortable saying we're gonna fork the test nets in another like three to four weeks um i think we're in a good spot to only need probably like a sort of mini bomb delay um which we can include with the merge release if if in a four weeks basically we're not confident about moving to test nets then i think it makes sense to just delay the bomb uh a bit more independently and then and then you know for that's when we're when we're confident and the client releases and obviously you know stuff like shadow 14 main net uh next week will give us a lot of data about you know how how many more issues do we find um and and yeah i guess just general client readiness for to implement uh all the stuff we just we just discussed um yeah so that's roughly my my point of view i'm curious if anybody else have high thoughts that all sounds like a reasonable approach to the timeline your hand is up i think it is goodbye to observe that but uh one comment from my uh site is that i think that we should run the test net public testnet for a longer time the time the first test that should be i don't know one month or something uh not like uh a week after uh let's run the first test and a week after the first test that we are we are running second test and that is my opinion of course not don't know what you are thinking about it is that to see basically what happens exactly exactly so for example park road and observe it for i don't know months or something and then for the next test that is my opinion [Music] that's one month after the ttd or is that one month from like when we launched the new clients you would extinct after the merge yeah after the merge so after td i think that is my personal opinion i i guess the counter argument i can see to that is we get like rapidly diminishing amounts of information like if the fork actually works that's like a lot of de-risking and then if you know it's it's still up for an hour after that's that's great and then if it's still up for like a week after that's really good i i yeah i guess i maybe i'm wrong here i feel like that like from one week to one month we probably get less information than we do from like nothing to a week um but if that's wrong yeah happy to understand why um andrew well one valuable bit of information is after like a week after the merger two weeks couple weeks when you start to think a new note from scratch what is the performance of this thing that's something i i would be curious to test got it uh mirrors uh yeah like this is something we we should already be testing right now uh on the shadow fox um uh i think we don't like we shouldn't wait another month to start testing stuff like thinking from scratch i i tested that on gas yesterday already and it kind of works it's painful of course because thinking a full test net from one of the big test nights probably from scratch is already painful um but yeah so my my my opinion is that we should have two to three weeks for between the first between the first and the second test maybe maybe only a week after polio because i don't think that there's much activity on the polio but for like a bigger test net we should have uh for the first bigger test that we should have like three weeks yeah three weeks i think is a good good thing and after afterwards the other testament should be in more rapid succession maybe every week maybe every two weeks but we should like we in my opinion we shouldn't wait for every big test net test net for a month that would just like delay things yeah just not to release uh all tests in one client's release that is so not in one release uh i set tdd for every test that is my opinion yeah yeah i like i don't think we can we can could even plan that that's that seems like something that would would be very hard to plan because we have so varying uh gtds on the testnets tds on the testnets yeah will it maybe make sense so like we have basically gordy robston and sepolia that are going through the merge um you mentioned you know gordy and robson are probably the ones we want to see live or like we want to have like the longest data for so like maybe there's something where like you can start with gourley and then two weeks after you do sepolia or gordia robson whatever like whichever one of the big ones um and then you do sepolia maybe like two weeks after because you kind of expect that if you know you shouldn't like learn that many new things on sepolia assuming the the previous one went well and then two weeks after that which is like four weeks from the original you have you have the second big test net and then um and then if you if you do that um then by the time we like choose magnet blocks like once we once we've seen that second you know test that be live for for another couple weeks and we're confident that it's stable then you kind of choose the maintenance blocks and and move there um but maybe [Music] yeah maybe like using this this test net uh like the septolia test net which is like lower stakes in the middle allows us to have like more time on under two other one which have more activity basically okay i guess yeah we can continue kind of the test net ordering uh conversation async as well but um i guess people generally seem to agree we want like more than one week between each of the test net like we did for like london and probably more on the order of like two to four weeks and depending on the type of test that does that generally make sense okay um cool and i'll yeah i'll i'll look at like what that would look like from a scheduled perspective and and and we can chat about it on on the next call and uh yeah what uh yeah when we would need basically to make a call if we wanted uh to uh or i guess yeah when when we when would we need to delay the bomb basically uh based on when we choose the test nets anything else on this uh i have a question um especially to the gaff guys but maybe also to other clients did you ten testing around pruning uh how do you avoid do you like uh track would finalize and uh allow pruning only the things that are finalized or how is it working for you like you mean pruning the chain away that is lower than finally for example pruning the block tree or something like that no we don't well we are the ancient so currently at the point 30 000 block back in history we only store the canonical chains and we move it over from our live database and put it in a not a live database and when we do that we only store one of them for each type do you have any plan on switching over to using finalize instead of 30 000 bucks or do you plan on sticking with 30 000 blocks forever i can't say really no no i i think we committed ourselves to not doing that like again back in in the summer because we think if something breaks and if we were to uh break the finalization then we should be able to and we want to be ready for that we hope that it's never going to happen but if it's going to happen then we should at least support it right that's that length of time like if something goes wrong we can fix it between this and that and how long are three thousand blocks about two weeks but it's not really i mean the amount of the we already have 15 million bucks in the old i mean it doesn't really matter if we have if we move 20 000 more there or not um from a data perspective [Music] the big game was moving out 50 million blocks from the live database and moving in sharing up another twenty thousand really and there might be other optimizations that are valuable there you know but they're i agree they're probably marginal compared to the big move you know database versus memory and that kind of stuff has finality uh also we only store so bad blocks are not stored in the block tree and we only store 10 of them so for for like investigation 10 for each bad chain depth or just 10 period 10 period and presumably you only have one bad block per chain right uh yeah presumably um i mean improve the work well i mean in our the way we uh we would only store a bad group of work block if the work is valid we wouldn't just if someone sends us some random junk we wouldn't have done so if the work is great and uh in a post proof mistake well i'm not sure i guess anything this yell delivers yeah well that's been pre-validated validated with respect to the signature so similar but the you you say you keep you keep the stuff not going an archive or not in the freezer whatever for 30 000 blocks that you can repair things that things go wrong but if you're only storing the first block on an invalid chain an invalid chain could in fact be a consensus split chain but how does that actually help you recover [Music] sorry i didn't understand the question uh honestly we should probably move on i'm sorry okay anything else on just the test nets generally or the merge okay uh so next up uh shanghai um first we had uh alex with with some uh updates on eip4895 and then we had some other potential cfi eips so alex you want to go ahead and share the list yeah let me just share my screen to get the eip uh let's see oh wait i'm having some issues uh okay there's i don't think i'm gonna do that right now that's okay um so i can just talk through it yeah i can share your document let me pull it up but go ahead and share my screen yeah yeah i mean so basically um we've talked through the process of withdrawals over you know a couple awkward devs now uh just to remind everyone where we've gotten is that uh essentially you'll have withdrawals uh happening on the consensus layer if you actually just want to go to the render eip that works as well but either way so you have the consensus layer um managing withdrawals they're dequeued in some way piped into the execution layer we decided to have these be represented in the execution layer as these new type of system level operations and essentially it just says here's an address and an amount and you should just credit the address the questions that we had open last time are basically essentially syntax like how do we want to uh you know sort of structure the block and the header and different things and so yeah thanks tim this is just what i want to show now if you scroll down just a little bit further uh there's a block rlp here where essentially all i did was say okay this is going to be appended right after everything else and this is like similarly in the header you have uh a root for all withdrawals that is again opened to the end so i think that was the main open question and i just wanted to get feedback on that point um otherwise everything else should be about the same oh martin you're muted martin if you're talking sorry um i got a phone call in the middle uh i didn't notice that was my parent yes i was wondering if we need a protocol update for this like devp yeah right so we will want to update that as well uh yeah the the ease protocol update rather than b2b yeah right yeah so i was going to make a pr essentially to that repo to reflect those changes but also wanted to get sort of you know consistent on this first so one small comment uh i think uh just just for precision the withdrawal shouldn't be an rop the withdrawal should be specified as a list because then like the rlp rlp should be applied at the end like everything is a list or a buy like a byte array in rlp and um then you apply rop at the end that's it it's it's a technical small comment i think that's how it was written or did you mean something else uh i think in in you you define withdrawals as uh as an rop of a list but then kind of because rop is a string representation but we had a similar somewhat similar problem with type transactions when you additionally wrap rop as a byte array of things like that so [Music] currently like for instance trunk transactions in in block bodies transactions are defined as list and then when you serialize block body as an rlp then you serialize transactions as an rlp but if you define withdrawals already as rop it means that they are a string instead of a lister kind of you you additionally wrap a list into as rop byte array right i want to avoid this additional wrapping sure i see what you mean now and uh yeah i can update the ip for that martin is your hand's still up first oh sorry and uh we already agreed last week that we're not going to use the um ohmer sash we're going to add a new thing on the end is that correct yes that's my understanding yeah because it's simple just to add stuff to the ends and we don't have questions of this is reinterpreted as a new thing a realist work with that work all that any other comments aside from the rlp issue okay so what what are the withdrawals they have an index now there's another amount what is the use oh it was initially put in there actually to disambiguate you know if you're following logs and to make it useful because theoretically with partial draws you could have a collision there where you get the same withdrawal twice um but this might actually have negligible value um and we can consider removing it um especially with the with the push method um very open i think it'll still be helpful for tracking it you know like if you're like a block explorer and want to track them right because you can still have two withdrawals are the same so it does differentiate them but isn't the is there a difference between the explicit index inside the withdrawal itself and the index in the list of withdrawals no i mean if you were tracking all withdrawals ever then there would be a one-to-one mapping from the consensus layer in the execution layer uh but they are dequeued from the consensus layer so they're not tracked in a list forever um but if you were tracking them you can map them one to one with no problem um no i mean like couldn't the execution this is the index of all withdrawals not the index within a single block all right which could potentially be valuable it is not probably critical for it to be in there that seems useful to me if the cost isn't terribly high yeah yeah you should probably specify what type they are there are integer or [Music] [Music] yeah it's like the under the system level operation colon withdrawal subsection of the specification and it has a one two three book uh list that has the types [Music] okay anything else on the withdrawals yeah slight client there is um we can link to it here okay um so next up uh we had moody uh with an update uh on eip 1153 uh and i know there was already like a lot of comments in in the agenda about this um moody do you kind of want to summarize kind of where things are at yeah hey everyone it's ready for muni swap um yeah so uh right now i think my goal ideally for this call which might be a little bit lofty is just to get this eip to cfi for shanghai and and that's not to say that like i want people to commit to like putting it into shanghai i just think we need to get it to a point where we can fund and do the work for potentially a future um hard fork um and i think cfi will help a lot with that uh and just in terms of like signaling that from client developers that like it is actually a good and useful change um and so from the from the last call when i talked about this eip there were a few follow-ups which i can talk about um if if that sounds good to you guys um yeah if you yeah i guess yeah if you can give us kind of the updates in the last time and then we can have the cfi discussion and whatnot yeah yeah so so last time there were a few follow-ups uh first was like the impact that the eip would have the second was a concern about um the linear memory expansion costs which we've elaborated a bit on the eip and the third was uh is this like the best version of the feature the best api for some kind of uh persistent memory across call contacts and so just i'll go i'll start with the first one about impact so uh i mentioned the issues a bunch of developers from different teams that are all interested in seeing this feature implemented um we we estimated in the thread just for v2 you know swap v2 reentrancy a lock alone and just for uh just for the storage load operation being saved uh it's order of billions of gas just for unit swap v2 um and then also uh in terms of like how much load there is on nodes it would we estimate trillions of read operations from disk [Music] and and you can read some additional context of the thread in addition like it it allows us to do new smart contract designs with future versions of protocols we develop that weren't possible before for example this ability to like you know do a lock across multiple pools and do things uh between all the pools without doing any rc20 transfers and in some cases the gas savings are drastic um when you don't have to transfer erc20s or call into erc20s in case in certain cases um and so there's a lot of context to that in the thread um including some gas estimates we did for for different prototypes um so so that's a little bit about the impact the second one is the memory expansion cost um so the amount of memory you can allocate is on issue but martin brought up uh an issue about the journaling costs which isn't really fully included or like not fully uh analyzing the eip i think a simple solution to the journaling cost is just limiting the number of keys that can be stored in the map per address and and i can update the eip to do that i think a thousand keys is is just fine 10 24 keys um or double that or some order magnitude or order of a thousand keys is fine um and then there was a lot of conversation about like is this the best version of the feature i think uh one one alternative was um um was making a persistent memory op code uh which is by address and works more similarly to memory um i think we we've had a lot of conversation i've talked to like charles from viper um he was asking about a different api where a t store and t-load were by address but still backed by a map in the client and so you could write to any uh 32 byte word any offset um but but that one was a little bit uh obfuscating for for the silhouette developers and the compiler developers um so i i think like this this t-store t-load interface is i think it's uh probably the best version uh just because um the language support is the easiest to add it's very similar to s4 and s load so uh so it's like it's easy to think about and there's also already sort of a concept of transient storage in the uvm which works through s store and s load um it's just not very well supported because of the refund cap and the fact that you have to use refunds and then it also has to load from storage um so like there's there's already this concept in the evm that's supported with slo nestor so it kind of makes sense to implement it via t-store and t-load um and then finally mappings and dynamic arrays are really heavily used in the use cases we have and they're not supported today with memory and solidity so it would be extra work to get developers to start using it and for it to be available in the language um and then yeah there was one more comment about uh the fact that this is like um sort of just like a gas optimization doesn't revolutionize anything um i think it i think it opens up new smart contract patterns that will quickly become canonical um and so in my opinion it does have a bit more impact than is clear to see like via you know gas estimates or um i think it has a lot of potential to like improve the developer ux for interacting with you know you know swap contracts for example um but it's hard to express that without sharing too much right thanks thanks a lot for for the update um i know mark martin and you had like a long kind of back and forth on on the agenda comments and and uh andrew also i brought up earlier like the idea um that uh something like say deactivating self-destruct could be higher priority than this for shanghai um so i guess just to kind of take a step back here like with in shanghai we already have uh a handful of small eips i don't have the full list um we already have this withdrawal eip and um oh eof was the other big one so we have eof this withdrawal eip that's cfi a couple of other small changes like the warm coinbase uh the push zero op code and whatnot and um i think on on the previous calls we discussed a bunch of uh potential uh solutions to lower uh the the cost for layer twos and none of those are officially cfi yet but there's like some progress on them um so i guess yeah i'd just be curious to hear from clients in general like do you think we can add anything more scfi for shanghai now uh would you want to wait until we we've we're a bit farther in the implementation to decide that um are there things that you think we should potentially like earmark now um yeah i'm just curious how how people generally think about that and andrew you have your end up um yeah i concur with martin i think uh shanghai is already big enough and uh if 1153 is is is also big uh i personally uh or or the i have checked with the uh aragon's team members and uh the uh our how we see it the priorities the following so if uh 4758 deactivate self-destruct is uh uh a great uh candidate for shanghai inclusion because it's a simplification and it also paves the the way for the ryoko tree so our preference would be to include 4758 into shanghai as to uh 1153 i would say we can see fight for cancun and also i think uh because shanghai is already big i would uh cf 48 44 shot blob transactions for cancun as well got it thanks uh micah you have your hands up i just wanted to comment that i'm ambivalent on shanghai or not shanghai but i do think there's significant value in um core devs giving signals as to whether this is something that we are likely to include in you know some near future hard fork you know i think we all agree this is kind of a neat useful thing but is this something where we can say yes you know uh moody go tell the swap team and other app developers that you should spend money and time um you know developing this further and working with soliloquy team and working with client devs to implement this or should we tell them you know we're probably not gonna get around this anytime soon i wouldn't recommend spending money and time on further developments for now i just want to say that moody's question i think what he started with was uh whether this could be moved to cfi not necessarily whether it should be included in shanghai i think that's ideally what he'd like and the union swap team is interested in it but uh i think he's just talking about cfi and i don't know if it's right to say uh you know this should only be like the what andrew just brought up like this should be removed from shanghai which it already isn't being included um in in favor of uh removing self-destruct i don't know if that's the right way to think about this right i guess the one thing i'm just cautious about is um we haven't implemented like anything that's in the cfi list yet so like if we had already like three quarters of it implemented i think it would be like a different conversation where it's easier to say like okay let's try to potentially add this as well and then if uh you know if if it's working as expected and like it's you know it's it's there's no issues with it then we we can move forward but the fact that it's like there's already like six and yeah at least six things that like are not implemented i yeah i i'm just a bit cautious that like we we we get you know to after the merge and we have a list of then 10 things we need to implement and we're basically back in the same the same spot um yeah so sorry i should give an update on the status like in terms of the development work we've done we've uh we've actually implemented it's incomplete we've implemented it in the ethereum jsvm and it's been merged um and that's how we've been doing the testing is against uh you know hard hat with that ethereum jsvm um and and yeah i agree there's a ton of testing um yeah uh and we've written some of the evm by code tests but like of course we have to write um right okay yeah so yeah sorry sorry i i guess i was more responding to accident but like yeah they're i it's good that like it's obviously implemented in in one client and that it works and and i think that's that's like really valuable but it's there is a difference between that and like it's being implemented across like the four clients and you know we've tested it and it works and and we've had like a devnet running at it like yeah and but we don't have a good a good word to discern between those two things um i guess i'm curious if does any client team like feel strongly that this should be made cfi for shanghai now um yeah it's probably a good place to start okay um can i can i ask what about like for cancun it sounded like one-person signal support but cfi in general i think just means that we can spend spend the time and resources to to basically build on top of it as well as like do the work ourselves um for writing those tests and implementing clients so i i think that a lot of clients i'm not just going to think that i mean are going to agree that it is a useful feature and it's very nice to have and if they ask is like would we be open to at some point in the future you know including this uh i i mean i would say yes but if you're if like cfi is some kind of stamp that this is now slated for inclusion and maybe not in shanghai but definitely cancun or whatever comes next and then i will be hesitant to like say so right now but on the other end i mean yeah sure maybe sometime i don't know exactly what they ask you yeah i think i think the challenge is we you know like shanghai realistically you know is going to be at the earliest end of this year probably you know sometime early next year it's all depends on there's a bunch of things but like so so if you're talking about cancun you're talking about the fork after that it's like you're talking at the earliest more than a year from now like you know say next summer like 2023 and the challenge is the priorities might change between them right so it's like i think even if if we had like strong consensus on this call that like eip 1153 is is like the most kind of important thing to do after shanghai um it's kind of hard to make a commitment that that'll still be true in a year um yeah and i guess we can yeah collect science we can time box this in like maybe two more minutes um so i'm not i'm not sure i understand what like cfi means like just based on the naming it sounds like it's uh like consider for inclusion not necessarily inclusion so in practice yeah and in the past what we've done is like we've tried to implement all the cfi eips and if there were like issues that came up during like the multiple client implementations we would you know remove them as we've had happen would say like the bls eip where we implemented it we still weren't confident with it but there was like this this implication of like if we implement it then it's all good the default path would be for it to go to mainnet um so yeah i kind of see cfi as some kind of almost promise from the client corridors that if you do the work with us and and you help us with the tests and everything then we will put in the work from our side to get the stuff merged and to you know that there's this we have we have approved it and now if everything goes well we're gonna gonna do the work together to actually get it out there yeah yeah i'm i'm sorry to cut the shirt just to leave some time for the gory conversation i do think like marius commented like we probably don't have the consensus to move this see the cfi today um and i think maybe one thing we should discuss on the next call is just like generally when do we want to start like do we want to maybe hold off in accepting new stuff for cfi until we're a bit farther ahead with shanghai implementations and we have a better view um but i i think we can probably discuss just like shanghai cfi at a higher level uh on the next call um and if there is no time to discuss that on the next call then it probably means by default we're not ready to add new stuff because we're still dealing with the stuff that's uh happening right now um but yeah just to make sure we we can cover the gordy thing um afrid you want to give a quick rundown yes thank you very much um i know 90 minute calls are draining so i will keep it short um there is this um kind of issue with some speculation on girly testnet and um i kind of feel responsibility to encounter these speculations and um i personally believe that the girly testnet is still having a lot of significance uh initially with east ii or consensus layer testing uh now was merged testing um pratas depending on gurley and um i believe as opposed to like older test nets such as ring b and robson that might be duplicated soon um gurley should be around for another couple of years at least um after the merge and i would love to drastically inflate the available supply of gurley tesla tokens to first of all uh um avoid further speculation on any non-zero value of early test tokens but also to provide um application developers but also consensus layer testing engineers with necessary resources to conduct testing um my uh i used to have a huge um stock of girl lisa this is coming to an end soon and i know the timing is not very fortunate right now because we are going all in merch this year but i would like to propose uh to inflate the girly test testnet supply i have written an initial proposal i called it a girly eip1 because i don't think testnet specific stuff necessarily needs to go through the eip process or to bind so much more resources so um i proposed to uh pre-fund uh an externally owned account with uh 92 quintillion ether on the girly testnet um to uh to keep it short martin said that we should not do this uh i think peter commented somewhere that we should not do a network specific or tesla specific force um eventually martin proposed uh that we instead of forever hard coding one key i hold uh that we could just um have a more generic approach by pre-funding all active validator balances by a certain amount so i created girly eip2 um it's linked in the agenda under the first comment and again if anyone wants to take a look i i just want to have general notion from the coordinates from the oil collapse if this is something feasible and we should consider it and i would really appreciate if we could do something like this on the girly test net and no we should not um so it was has been proposed to do something with block reward but i believe we should keep it really simple because if we uh modify the click block reward then we will play around with consensus engine and this would might this might drain too much resources from core developers that is not really necessary so it should be like a one-off thing i believe so just yeah because we're we're very close to time and i think that does anyone have like an objection to generally addressing this like and and and you know whether we do it through like a one balance increase or all validator increase like like just at a high level it's anybody opposed to just the idea of like increasing the supply to make sure that the the test nets value the testnet coin's value kind of stays basically zero all right i don't think we should do it for that reason but the other reason i do think is very reasonable like giving money to people that need girly youth to test things issues so to be clear i think the amounts though that like afri proposed are like so high that it basically drops to the valley yeah if that's that if that's if that's a side effect i i have no care i just don't like if the only reason was to stop people from trading early youth for eth i would not be in support of this at all you definitely need a better reason than that that is the the main reason today so micah the the issue with increasing the block reward is that we don't have any block reward on goalie so the the the proposer block the miner is always set to zero and we don't have the private key for the zero address unfortunately and so one thing okay so i guess the other thing i we might be able to get rapid consensus on is um basically is this something we want to do before the merger after um i suspect doing it before the merge would like delay things but i'm just curious from client teams like how it you know assuming we go ahead with some version of this proposal the simplest from like a client developer perspective um in terms of timing how the people feel about this so um the disapproval authority that the the coordination of the network itself is pretty easy it's like four or five notes that need to be updated but then you have the other infrastructure um and the the cl test that's that rely on the gurley and all those i mean getting the test that itself to upgrade with the cedars that's pretty easy [Music] but getting all the other uh infrastructure and cll into integrations upgrade as well might be a bit more tricky and would be the thing that causes a bit of delay as well and operational issues so okay clearly there's and there's some comments in the chats with like different uh different thoughts um what's the best way to continue this conversation async over the next two weeks afree and and then we can we can cover this um a bit earlier on on the next call i personally would appreciate if people could spend some time on the girly testnet repository to just leave a comment i personally believe that this is something because it's only addressing a test that it can be totally uh solved asynchronously also there is this idea of so i turned ideas just to wrap this up are creating a new test and obviously um was the downside of losing all the existing infrastructure um there might be a compromise in in in between these uh ideas is to have a regenesis so basically we said girly genesis um this is also something that could be interesting by retaining the name the the infrastructure but having a new guinesses basically starting from scratch this is also something we can discuss and um i will invite everyone to the early test repository to discuss these ideas and eventually come to the conclusion cool thank you and yeah apologies that we we didn't have a ton of time to dive into this um last up uh live client if you want to give a less than one minute update on 48.44 and this is also something we can queue up for a longer discussion on the next call if if needed sure yeah just a really quick update um we're continuing to work towards getting some sort of stable devnet running we have george working on a tool to submit data blobs that actually commits to the blobs with the kcg commitment vitalik wrote a really nice faq for the eep which i'll i linked above but i'll link again here and then proto is going to start working again on the integrations next week so um you know i think my next acd it's pretty likely we'll have some sort of local sort of devnet available for people interested in using and then maybe after acd after that we'll have more of a like longer running devnet um available i think we're gonna onboard some more people onto this over the next couple weeks as well so we'll just keep posting updates in acd and in the other relevant discord channels cool thank you very much um anyone else have any quick things before we wrap up okay uh thanks everybody i appreciate everyone sticking around a couple extra minutes at the end have a nice weekend bye see you thanks everyone bye bye bye bye [Music] [Music] so [Music] thank you [Music] [Music] [Applause] [Music] [Music] so so [Music] [Music] so [Music] you 