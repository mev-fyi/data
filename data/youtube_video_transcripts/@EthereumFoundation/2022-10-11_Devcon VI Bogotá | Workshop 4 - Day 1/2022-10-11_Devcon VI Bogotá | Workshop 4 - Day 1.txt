foreign the um Twitter I started um foreign interested um foreign trespasses um organizations [Music] okay services consolidity is contracts um [Music] police my name is foreign foreign um local local services um foreign [Music] all right foreign um foreign um automatica into this um foreign [Music] basio foreign [Music] necessary services monetary um application is [Music] local restaurants foreign [Music] local um foreign facility Continental foreign Malay foreign deployed um foreign developmental um um [Music] foreign foreign counter foreign General let me address ES hold up um function um hey Google foreign the killer skin because she knows perfecto ven yarn start ER deployment um foreign foreign contract um foreign foreign foreign the owner uh Massachusetts foreign [Music] [Music] foreign foreign foreign foreign function is foreign Deborah foreign um address is um Network Academy um foreign um Direction foreign contractors Commando verify foreign um um see that is foreign service this is this is foreign I mean that see I didn't even know um foreign proposito okay okay UI we want to know um [Music] foreign for everyone that we interact with if we wanted to there's also an interesting um interesting uh use case for enabling cross chain operations there are many ways to do that um and uh yeah that's just something that becomes a lot easier once once you once you have um this functionality built into the protocol there's also there's also uh some say things that you get from from just being able to batch different transactions together um also from being able to guarantee that the transactions are going to execute with optimicity so for some things it's useful if everything happens together and if everything doesn't happen together then you don't want it to happen um because there you know it wouldn't make sense for the transactions to execute separately [Music] um and you know whether this happens um whether this happens like in a gaming environment or you know there are some you know like Financial scenarios I mean imagine the simplest one would be you have to you have to approve you have to give some authorization to a contract to perform an action on your behalf um and then then you want to perform you want to give them like you know another transaction to actually perform the transaction and those both need to execute together um otherwise like you just wasted gas um and there is there there's um there's a sort of General use case that's interesting where um you can actually uh Implement these um time delay uh flows so for example let's say I want I want to be um I want to be uh uh selling my teeth when it hits uh 5000 but I don't wanna I don't I don't know when that's going to happen I don't want to sit in front of my computer so it would be possible for me to just pre-create the transaction but making conditional on certain certain things happening like okay only if the price of eighth is 5000 or only after this time or whatever the condition is and um so my wallet would agree to execute and pay for the gas for that transaction and also maybe a transaction fee and you would put that into a registry and the registry of uh future time delayed or event driven transactions and Searchers they would be able to monitor this registry and um see oh like these are transactions that their conditions have just been been met so they can compete on executing it for you um and that opens up that opens up a whole range of of uh interesting um uh use cases because now you know it doesn't have to be you that pulls the trigger at the exact moment the conditions are met and if the conditions are met um the transaction will be executed for you by Searchers and it could be time delayed it could be based on essentially like whatever uh whatever the conditions make sense um maybe another example would be you know there's an NFD series and you have to be or I don't like you um an event they have to subscribe to for in a certain time window um okay so a little bit about erc4337 which is so um this is the standard that we have been um working on and uh blow here is is the guy who implemented the the contracts um so yeah please if there are any technical mistakes correct me okay so this is the first step um towards protocol level account abstraction the nice thing about our approach is that it doesn't require any change to the rules of consensus so we can kind of experiment uh for free and we don't have to solve governance um in advance um the way we're doing it is okay we essentially create a mempool a new type of mempool for anyone that wants to participate in this and uh a single you don't you don't need more than a single Network this this mempool it's um uh essentially it uh accepts it accepts uh something that is essentially a transaction we're calling it user operation but a user operation is equivalent to a transaction but it's a it's a transaction that works with with um with these account contracts um so what that does is it makes contract wallets a first-class Citizen and it totally does away with the need for having um anyway you don't need a new way to to control um to control this this account um and I mean the way the way that works uh maybe a little bit about how that works um you have it's kind of similar to how um uh you know flashbots Mev private mempools like the principal behind it is you can have a mempool are bundlers they are provided an incentive to submit your transaction um and essentially we're just we took that that idea originally this was vitalik's idea and we added the gas subtraction part to it to make it a more general purpose and then the bundlers you know they their their job is to just uh take these user operations and eventually they they they bundle them together and they submit them um as when they're creating when they're creating um um uh bundles that are go to the to actual uh blocks okay the the the other advantage of doing things this way is that we're separating validation from execution so you can have you can have [Music] um so so if I'm if I'm a bundler and I am paying for the gas of your transaction there's there's some risks involved for me because um you know what happens if the the you know like you know if you don't pay me what happens if I execute a transaction on chain it turns out that it um and it ends up uh um you know um ends up uh well at the very least you expect to be repaid in gas because otherwise um you know why why would you participate in the scheme um so to make it make it very safe for bundlers to participate what we've done is we've provided um a contract level guarantee that you're always going to be paid back regardless of of what happens with your transaction when it's executed on chain so we've separated validation from execution um and what the bundle needs to do when they accept the transaction they're just verifying that they're doing this off chain initially just okay if I if I accept your transaction um and I'm calling this huge function and uh am I going to be uh paid back for the guess that's that's all they're verifying so it's pretty cheap for them to do that and um later the totally separately when the transaction is submitted it gets executed but by then you don't really care um as a bundler um even if it reverts it's your problem just like with a normal transaction because you're still going to get paid and the the you know without this it wouldn't really be possible to um um the crate uh a permissionless uh pool of bundles that are participating in in this um protocol because the bundlers would have to trust that um they're not going to get cheated we use the term brundler which I know the value debtors just like any node Valley that also supports account obstruction and not one of them is enough to run a network the more they are the notebook is more resilient to a censorship and other things but the bundler eventually and eventually all validators hopefully will be also bundles right so the right now we have so exactly you don't need a consensus change you don't need like 51 um of validators participating in this scheme because ultimately you're generating just uh um Regular legal bucks um and we have we have another mind um as as an implementation that is supporting this um the more the more clients support this the faster your transactions are going to get executed but um yeah Okay so so with erc4337 then you know once we have this scheme um we can also use it to make uh uh Roll-Ups uh cheaper because you can batch transaction you can aggregate signatures so um so yeah that's that's another advantage and like I said it doesn't require any protocol changes so on any ABM chain um you can we can start experimenting with this is there anything you want to add uh yeah okay technically a bundler can run as a separate entity it is much much better for it to be a node to be more resilient so the way to add it right now the way we currently adding it ungirly because it's still in test is elegant as a separate server a the wallets don't care its implementation in order to be highly scalable and to be able to batch more it has to be a node in the network Never mind already have a node that can run a nethermind and there's a work to edit also with a get code so what's what's next well yes we can start experimenting and have um account abstraction and any evm compatible chain without consensus changes but the goal is to do away with the OAS eventually we don't we don't need doas and uh we know we're going to have to move away from them um eventually at some point and there are various ways of of thinking about this um so we want to want to want to have um account abstraction um as a basic feature of the protocol but we want to do it in a way that doesn't enshrine any particular wallet or gives an unfair advantage to any particular wallet um and because the eoas are effect and they're very very common and they probably will be um until we uh you know we move away from them um it will take a few years we need a way to convert EOS seamlessly um to smart smart contracts so there will be there will be some default implementation where yes everything in the future is an abstract account including EOS but if you haven't upgraded your UA if you have an inserted code into your eua if you haven't like activated it in some way then it just you know behind the scenes continues behaving like in your UA but it has it has the um the the functionality requiring allowing you to to upgrade it of course this would require a consensus change and there there's various ways this it can be achieved we're um we're discussing um you know one way would be there's to create a new transaction type and then you can set the code for your eoa this is this is um this is now the code that's running your UA um or you know there's also I've been been um a suggestion um eip-3074 maybe that in combination with another EAP we could also set uh default proxy contract for all addresses um doll you want to add anything Yeah by default that basically two options one of them to let a user decide the exact point of time where he wants to upgrade its eoa into a a contract wallet either using a transaction type or new opcode on such the other way is to decide that at one point of time all Eos start using some default implementation we've deployed and tested thoroughly before which behaves exactly like in a way so all user will not notice a difference except that from now they have a way to modify the replace the actual implementation so they have a smart contract that just behaves exactly like an eoa until they decide otherwise right yes the basic Contour doesn't offer any of the Advanced features we described earlier except the one feature which is replace implementation the user can replace implementation once it is replaced the sky is the limited yeah all which all the use cases that we've yeah use cases and all the users that other people really try to find okay okay I think that is if anyone has question well we still now we're going to talk about well how do you join this okay yeah yeah [Music] um so how do you join this account abstraction Revolution you can start experimenting with erc4337 right away and uh we had eight wonderful submissions um and uh this is something that's um you know already working so you don't have to wait um you can add useful features like the one we did discussed you know batching or key recovery um or any any of the things that that we've been talking about you could build features that were totally not possible with the uas that we haven't thought about and if you do if you if you're building anything cool um then you should definitely apply for an EF Grant because we want to see this we want to see this um uh used and adopted and we want to see the experimentation and want to update this presentation with more interesting use cases um so definitely apply for an EF Grant if you have a cool idea that builds on ERC on the CRC the other thing is if you're building a Dap You have to think about a future where contract wallets are first class citizens I mean contract wallets are already pretty common especially for teams um you know multi-cigs are an example but still many dapps assume that they're going to be interacting with an eoa and that that is just um an obstacle for us to move forward it means your adapt already can't interact with things like Windows safe wallet um if you're assuming you're making assumptions such as um you know the the the like you know the how how signatures are validated so there are easy ways to make your dap compatible both with smart contracts right away and account abstract accounts in the future and that's ERC 1271 with just it checks um it checks if the caller has code and then there's a mechanism where it can just invoke invoke a function and instead of assuming that there's they can rely on this EC this DC DC DSi key the other one is if you can if you can um benefit from batching in your user interface and many many dapps especially games can then you should check if you're connected to a contract wallet that supports it um and that will that will create a better experience for your users it will save uh gas costs the other thing is um with how gas is paid so if you have adapt you should think about different types of uh Gas payment models I mean you know a big an easy example is if you have a token then it makes sense perhaps that your users should be able to pay uh for the transactions in your token when you're using your dap um and if you don't have a token or you want to subsidize your users that's easily accomplished um with with the with account abstraction um you you set up yeah you know you set up a a contract that authorizes to reimburse your users for whatever criteria you feel comfortable with maybe the onboarding process maybe they they have to perform some action but um it's uh it's it's something that's possible now and a lot of the a lot of the improvements that we're gonna get for for that usability is going to also require some you know wallet support so uh wallets are an important part um of uh of usability for depths and as a depth developer you have some influence by collaborating with wallet Dev saying okay this is this is something that you know would would be beneficial for my use case um um and you can you can have a have a bit of an influence by just saying okay this is this is useful for me I need this feature in the wallet for example supporting account abstraction so other than talking with us um we're happy to help anyone that's implementing uh different use cases we do have an SDK up uh later you have will share on his Twitter um the the links but there's there's a there's an SDK there's an SDK you can come up to us and we'll give you yeah or we'll just give you but I can I think I think I just like open this right now and show like where this is pointing to wait this is very small why is it so small yeah yeah so we have our SDK it's on GitHub Ethan finitism account obstruction it's not this decade it's a contract oh what is the SDK is infinity okay oh this is the link is broken then all right we'll fix that is um okay oh this this is the SDK okay so the reference implementation of a bundler simple bundler and the SDK oh cool okay so that's the SDK we will fix the link later um you can also read up on the ERC um and oh maybe that's the right link no no that's that's for the uh yeah that's your seat itself yeah yeah it's the ERC itself so you can read the ERC it's very detailed uh but you can get very precise understanding of how it works there's also a discussion on the ethereum magician's uh forum and and of course um it's nice to be able to talk with people so we have a Discord now we have a Discord server and you're very welcome to join and ask questions and even after this event like if there's something that you don't get to ask us in person um and yeah maybe now we will just take some questions yeah yes over there okay the internal development roadmap what we've developed are the interfaces and the core contract that performs this magic we call it entry point it was audited but then was extensively modified to support the l2s that is not yet audited we still have some work on it the API probably won't change so a wallet will be able to work so it's not deployment deploy the mainnet only and test net currently but you can create and start experimenting with Wallets on top of that the interface of a wallet the change of wallet needs to be it's quite minimal I can go through it adding a single method or two uh yeah we have a sample uh yeah a sample that uses that adds account attraction support to ignosis Safe you add the module and you basically get make a okay a single owner uh you can also safe and account obstruct the compatible um so yes you can create and there are some work on creating wallets today in terms of applications yes it's chicken and egg application it's a wallet in order to work there is a way no it's for a basically a hackathon an application can work without a wallet but it's not something else you want your user want to sign blindly a hash if you're good with that it's also possible to with an application can work with accountability in the day yes the way gas abstraction works is that when you submit a user up I said the contract itself validates itself the signature and its nons and in order to accept the request but there's also a pointer to what we call a paymaster a payment is a contract that before submitting the transaction has a chance to decide whether it agrees to pay or not if it says okay that is it doesn't revert it will use its own balance its own stake and everything to pay for this transaction now what this paymaster does on chain depends on the paymaster so the most obvious example of a paymaster is that the validation will be I will check that this user has a balance of enough die and has approval for me to use this die and I will grab enough die to cover this transaction so and at the end I will refund it with the excess So eventually I pay the user pays with tokens for the transaction this is mostly it's most obvious use case of a paymaster but there are other use cases if you want a voting depth and you don't want the user to pay anything okay so the check is that the user is eligible to vote if a user is eligible to vote and didn't vote yet I agree to pay it's another example of a paymaster other examples are still open you can write whatever you like it's just just to contrast right now if you're using Nosa safe then someone on your someone on your team has to like pay from their account right even if the notice safe has plenty of these um someone still needs to like pay from their own accounts just to have that transaction uh finalized um so with account obstruction that wouldn't be necessary um your account will be able to pay for yourself and this doesn't require paymaster but let's say you're you're safe for um going back to the safe example if hypnosis safe um doesn't hold any eth it doesn't hold a sufficient eth where you don't want to have to care about the balance and continually like exchanging and topping it off and whatever you have a balance of assuming it's enough to pay for the gas um you would essentially include in your transaction a reference to something we're calling a token paymaster and token paint Master can be completely autonomous contract on chain that it just it accepts tokens it and it it would pay for the transaction in eth and then it would settle in some way so well an old reference like an older reference implementation recreated which is unique swap using uniswap and that single transaction which was kind of expensive either cheaper ways of doing it but just it's like very simple and one single Atomic transaction it gets paid it gets in a sort of an allowance in whatever token you have it pays for the gas it charges the transaction fee um and then it gives you back the the remaining uh tokens so with account abstraction if you want to use if you want to use that for example then you would just include oh the um The Entity that is paying for the gas is a token paymaster but there is a way for the token paymaster you know to to receive a commitment from your account to pay it back so it's not just subsidizing the transaction that's also possible any other question yes I think with like this action idea is just like making single wallet more expressive because my intuition is that like well I have like all of that is going to be able to be Consolidated because of the expressivity of smart contract laws of a contraction I'm wondering if you can just like check my answers so I'm assuming so many many people are here have so many laws spread out if we can incorporate logic into a smart contract while all of a sudden we have the ability to kind of concentrate everything into one single log to my intuition yes it it makes it possible right now you have to split it because of a different concern if you need some corporate level security using a you can also safe and if you want a game you will use this metamask and field your private you use trezor now if you if you want them all in a single address yes you will be able to do it with a counter fraction probably you still might have multiple abstracted accounts for different purposes but for different reasons if if the reason to have multiple addresses is only security then yes you will be able to use this abstracted account to find something that can cover all the all bases yeah because you can just limit your risk exposure uh to each device depending on how much you trust it again I'm not saying that there will be one wallet that will give you all these use cases you'll find a wallet that give you all the user's case you need and use that and you always can switch I think the the first use case of a signature is chain signature just think of it you start using metamask and after a few years you collected a lot of nfts and a lot of money but you can't change the security model your browser holds your private key so you have no idea if anyone hacked into your computer and grabbed it through a copy of your computer without changing the address you can't change the security with account obstruction with single operation of change owner now the treasure on the same account even if with the basic simple account I just changed the owner and now I am really secured because the previous private key is no longer relevant yeah even before you get into the really fancy stuff that you can do with the abstract account distraction the basics are actually pretty um pretty useful just by themselves because right now if anything goes wrong then it's really hard to transfer everything from from one eoa to another I mean you would have to create separate transactions for each asset that you hold that could be pretty expensive um and if your computer got compromised you might need to do that in a huge hurry you know so it's it's um it's just not not the best situation to be in even very simple improvements like this will make a big difference yes uh okay so we can come perfect sorry thank you congratulations [Music] so at the end result we will do it yes but the idea is like this create an account today yeah you walk through it a different way to create a chain can be an existing account for this to be a new account yeah and you're using it into 100 minus but you know that this address will be valid in the long run in in the future this is my business okay yeah hopefully so is it possible on the mean s anything happening for example that private providers the service providers they they actually like if there's an important provider like a different account obstruction groups working in a different approach and then maybe the guy so shut down so after we had this three-way consensus split we decided at some point okay it's just too hard I already talked a bit about hive [Music] we use Hive to execute a bunch of tests most of them are spec tests so we for the specification we create test cases [Music] I think and so for example we had a division by zero in the exchange transition configuration call some some rules around the time step didn't really work and so high [Music] a lot of so we have and now we have the testing the merch effort which was really nice foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] to [Music] me [Music] [Music] [Laughter] thank you [Music] thank you [Music] [Music] foreign [Music] foreign [Music] [Music] actually we're also going to test the colors that so many people are interested in this are interested in contributing their time as [Music] if you want to be a part of this I want to give a special thanks to all of these hello everyone hello thank you for joining us today today we'll give an introduction to crypto economics both in a theoretical setting and also in a flight setting so I'm Jillian I'm a research assistant at the robust incentives Group which is a research of parts of the Varian Foundation and here's Barnaby he's the team lead of the Boston census group and so let's get started we'll first start with a section about what a trip to economics theoretically is and it's supposed to be interactive so if you have any questions anywhere please just raise your hand and at the end of each section we'll have um have a moment for questions or discussion if you'd like so crypto economics has a special place in a protocol it's not as heart-based as cryptography in the sense that we verify what people are doing it's not that we 100 sure know what what's happening but we use economic incentives to induce people to follow what they're supposed to do what the protocol wants them to do so if you're a network participant doubting whether you should do the right thing or adhere to another set of rules that you prefer maybe it makes you more money maybe it's easier whatever preference you have crypto economics is here to guard you we have some incentives to for you to follow what the protocol wants to do and we could punish punish you if you do something that's not aligned with proposed tool so then why is crypto economics different from just regular old economics well it's the environment we live in a very different environment with the decentralization and trustlessness that makes economics a lot more difficult we channeled rely on the outside law enforcement that Niche people follow what we do instead it's a very adversarial environment where where we only assume that people are rational meaning that people maximize their own payoffs um and do what's best for them this also what makes it very exciting in my opinion so where crypto economics really started was with the game theory of the Bitcoin protocol uh so game theory is the study of strategic Behavior or how you would respond to situations in which other people also make decisions about how they behave um importantly in this in for the Bitcoin protocol is that you have a decision in which chain to mine you could either remain mind on the longest chain or on another fork and this decision is something that every Miner actually has to make and you're incentivized to mine on the longest chain because you'll get some issuance rewards and fees and if you mine on another chain and it's not included in the technology chain you only waste your energy spending on this mini your incentivized to do the right thing now in Game Theory we usually put these turn of games into tables to make it easier for ourselves to see what's happening what other players are doing and how we can determine our own strategy so as you can see here assume that you're a Bitcoin miner uh doubting to minor the longest chain what you're supposed to do or on another chain from the next plot and all the other miners we've added dated into one group as well and they're simultaneously deciding on their strategy as well so in this table the first Emoji corresponds to the utility or payoff of the person in the First Column so that's you as minor and the second Emoji corresponds to what all the other players are doing and we can see that since all the other players are in this situation uh mining on the same chain they'll always be happy as that chain will be the canonical chain however for you deciding on which chains mine on um it's important to think about whether you're going to on the longest chain or on the other chain and then how you could determine your strategy is to see given that other people are mining on the launch chain what maximize my payoff well in this case mining on the bonus chain as well and given that other player players are mining on another chain but much much more payoff well mine on the other chain this is simple as far um can we do the clicker ah there we go thank you um so since every player in this situation has the same strategy we actually end up in a point where there's a steady state everyone mines on the same chain hopefully and this is the steady state is what we call a Nash equilibrium because no one has a straight incentive to deviate from this from this situation so if you're mining on launch chain if you mind on another chain it means that your payoff will be less meaning that you're um means that you don't have an incentive to deviate so this is the game freeway parts of crypto economics a small introduction and now we'll introduce a bit more Theory called mechanism design so mechanism design is really the study of Designing strategic situations with game theory in mind so how can we make games so that's the payoff or the the outcome is how we want it to be an example could be when we're designing an auction we want people to have have an easy way to bid so for example bit their true evaluation of something um and we want that to be incentive compatible with the protocol so incentive compatibility means that the the designers have a door in mind and the strategy that users are going to deploy reaches that goal um so in that case we could use Game Theory to see what's the strategy and how can we design around this and it's always very important to take into accounts what you're actually designing for so there have been some famous mistakes for example um some game uh some Olympic games where the the pools weren't made correctly and some teams actually try both try to lose which is very weird setting okay so then if we turn back to our example of the miners in Bitcoin Bitcoin enforces that everyone mines on the longest chain which is what the pro skill wants them to do by issuing rewards only for those mined on the longest chain so this was a section about uh Game Theory and mechanism design and bit more theoretical setting days will drive more into apply settings but if anyone has any suggestions questions or anything please just raise your hand if notes we'll just continue to how the dash Market works so many of you will probably have heard of Italian speech this morning he talked to bits about the dash markets and I'll try to elaborate a bit on that so the dash Market is basically for any transaction that you send to ethereum you pay cash and how this gas is constructed um is dependent on the amount of operations and the type of operations that you do in that transaction so each operation or optoad has a fixed amount of Dash units that are associated with it so for example multiplying two numbers just five units of gas and adding two numbers plus three units of cash and this this ratio is relatively defined so five three ratio and this doesn't change but this may be weird because as you may have noticed the amount that you pay for your transactions isn't actually fixed this is because the amount of if that you pay per gas units so these are two separate markets is determined by supply and demand it's important to have this distinction between the amount of Dash units and amount of if that you pay paper Dash unit so we have a dash limit to preserve decentralization which of course a drove of the protocol and this is done because if we could make the trade-off for a higher Dash limit so more Dash units per block and this means lower fees more transactions but it means less decentralization because less people would be able to participate in the protocol less people would be able to validate um meaning that we missed one of the doors of decentralization and how Bloods are in principle made um is that they a minor sees all of the transactions that come into the mempool and they choose the transactions that are paid the highest fee per gas unit and they basically just fill the fill them and fill their blood with the high space transactions at the end of rationality assumption that we talked about before important to note here as well is that this is the pre-erp 1559 gas market I told you bits about this Erp later um but this is the simplest setting how it was before uh some time ago in ethereum um then this actually this auction for blotch space so an auction in which we sell a scarce resource which is blood space which is what the ethereum protocol sells is actually a first price auction so players bid for the transaction to be included and if they win the bid uh win the auction they they just pay their bid and they're included um however this is not an ideal setting because it's very difficult as a user to know what to bid exactly If you bid your true evaluation you you get your transaction in but you also pay everything in dash fee so you're not really better off so you're going to share the bits and going to bit a bit lower lower but you don't really know how much speed and it's also difficult for the protocol because in some cases people that actually have more valuable transactions they were willing to bid more are notes included while people that have lower value transactions are included because it's difficult to determine your bidding strategy so then how could we design an urchin mechanism for ethereum in which block space is sold in an incentive to passable manner so that people can just bid their true evaluation and do not have to worry about shading well we have set in price options in principle these are very simple um we are ordering of the scarce resource again block space and if you win an auction and you pay the second highest bid so for example if I bid 10 10 if you bit 14 even to other people bid 5-1 if you win the auction with your 14 eighth bid but you only pay my bid of 10 if which is the second highest bid and the nice thing about this property is that we can mathematically prove that in this case it's dominant strategy incentive compatible this means that every person has a strategy independence of what other players are doing to Simply bid their true valuation there's maximizers there utility maximize their payoff and because every user is going to do this we end up in the National equilibrium we talked about before and this is actually great because now we have a national equilibrium that we as opposed to want and so why wouldn't we just Implement um the second price origin this is an open question so if anyone has a suggestion why we can't Implement section price option in ethereum unless you know already sorry um yeah so what are you suggesting then um so you mean that you know that what everyone bids um so you can't Implement a second price option because because then you can just pick yourself yeah exactly so because of the adversarial setting miners will maximize their payoff so let's say that you have a block in which there are four transactions one paying 10 fees eight fees seven fees and two fees and we'll assume here that the second price version works in the case where you just pay the lowest transaction that's included in the blood then every user will have to pay two fees if the if the miner uses the real transactions however the miner can in the adversarial setting maximize their payoff by using uh by stuffing the block with no interest action so they switch out the transaction paying two fees insert one of six and now three people pay section fees instead of um four people paying eight meaning that they maximize their payoff and this is something that can be easily done by a minor it's very difficult to detect um therefore we can't Implement these kind of mechanisms um which is unfortunate because as we have seen there are quite a few negative consequences of first-price auctions for example the priority gas origin or PGA for short this means that if there is a very valuable blood space you may want to have your transaction included before other players so for example if there is an NFD minting and there's only one nft you want to be the first one to mintage NFC but other players might may also want to Mint it so let's say this nft is worth 100 if you're willing to pay up to 100 even gas fees but of course you want to shade your bid as much as possible and pay as little as possible and that's what we see here on the y-axis we see gas bids in Gray and in the what oh sorry and on the x-axis we see time and the the orange triangles are bits by one uh bolt who is searching for ethereum seeing if there are valuable transactions they want to bid for and the blue uh is a similar book but just a different one on the green star we see where um the ball side the boat that one and the red bulges the boat that lost and you see that the bids are increasing over time um outbitting each other it's it's relatively by small amounts to maximize their payoff I know why this is exactly bad for the protocol is because um it spans the mempool um it made sure that even transactions that don't win so transactions that aren't the green one are also included in blood because they pay a lot of gas fees so miners are incentivized to include them in the blood meaning that the block is filled up with transactions that revert they do nothing and they're basically only waste blood space and since blood space is now more scarce base fees or Dash fees go up uh which is bad for everyone of course so an elegant solution that was forced for this is erp1559 as I spoke about before and basically what this does is it transforms the fee markets into something that resembles more of a second price option so now up until erp1559 you just pay the dash fees and those go to the the block Builder and they um they can put all of their profits in the pockets but now there's a base sheet that's determined by the protocol and an amount that you give to the block Builder and the space fees Burns so there's no incentive for people to try and make off-chain agreements give new parts of the Bay Street um and this this makes the bidding for bloat space very different because now bloods in general aren't full so miners are just incentivized to include whichever transaction uh pays them enough tip to be included and so bidding is a lot has performed a lot easier you basically just paid a base fee you add a very small amount of trade that's constant over time and this means that your strategy of bidding is basically incentive compatible with how the protocol one should do so it resembles kind of a second price auction um and you can just fit their true value and also um it's a common misconception that's erp1559 decreases uh total fees or gas fees at users pay and this is not the case because it's only a mechanism of how users bid for their transactions to be included the blood space in the long run is not is not increased meaning that that's fees don't decrease because the supply and demand are still the same so that was Parts about desk fees um if anyone has a question Erp 159 is great interesting so um be happy to take anything but otherwise we'll just continue with maximum attractable value which is also a very interesting subject um it's subject which has a lot of applications and there there are many Papers written about this so it's definitely worth checking out but it will give a brief introduction to it so maximum attractable value means that you extract as much value as you can from the ethereum network so how this is done is we'll start by looking at how transactions come into the blood as we talked about before users submit their transactions and they're included in the mempool at first in which all blood Builders Searchers search store people extract Mev can can look and they try to maximize their own payoff um so for example if you submit a transaction to the mempo trading uh token a for token B and it's a very large trade the price of this pair is going to switch so as someone searching through the mempool you could think that um this this is going to happen you know it's going to happen so in that case I'll bid um place in transaction just before this in the blood so that's um I I can buy token B before it increases in price meaning that I have an Arbitrage a risk-free profit and this is what happens a lot and why this is possible is because um the ordering in the block is not fixed it's not the case that if you submit your transaction it's included in the block on a Time basis anyone can um Can shift the order or actually build shift the order or if someone is willing to pay for it they can shift the order as well meaning that they could extract any value from users in ethereum network so interestingly about this and this is very similar in in some sense to high efficiency trading in traditional Finance then blockchain the difference is that you can actually execute these strategies atomically meaning that if you have one transaction in in the mempool that you would like to do some Mev on you can include your transactions only if they're profitable so in this case there's really no way to lose money um so this cell sounds very bad users are being value attracted uh it makes execution worse for users why wouldn't we just forbid Mev well it's not as simple Mev is a quite powerful for us so we'll have a look at why some people think Mev is good and why I mean some people think MVP is bad so on the one hand people argue that Mev is bad because Searchers um searches find almost all transactions in the in the mempool that they can do Mev on and they make sure that your execution is as bad as possible which is of course not something that you like um also interestingly Mev incentivizes centralization this is uh again if we wrote back to the comparison with high frequency trading in traditional Finance you should see the corporations with multi-billion dollar budgets they have very big infrastructure operations and some similar arguments can be made for Mev um you need you need to stand the mental there are multiple strategies that require High Investments meaning that there is Economist skill and which is centralizing for us which of course is not something which we want and it's actually been torted as one of the threats to ethereum and searches-based blood space this is what we saw before uh in the priority gas auctions um where transactions that are reverted or do nothing are included um pushing up Dash prices and Mev searches are generally very smart so they could put their time and effort into building other great projects that contribute to the ecosystem um but some people already that this is very bad on the other hand there's an argument to be made that Mev is good or maybe distributes a nuanced Mev might not be extremely good but it's worth extracting or the way to deal with MEP is not to just ignore it um there's an argument that some searches provide very valuable um very valuable um services to the the network for example if there are two liquidity pools and in the one pool um 13 A and B are trading for um well you can get five token B for one token a and then the other pool you can attend token B for one token a this is of course a mismatch and Searchers um they can do an Arbitrage transaction here making the prices again equal so that's um users in general have better execution if they trade in one of these pools randomly um also liquidations for Lending platforms if there's bad debts Searchers liquidate and the the people that lend out the money uh are protected and these are generally rejected scenarios as quite good um parts of Mev Mev can be redistributed so this is an interesting line of research where um the idea is that you extract all of the Mev but then the Mev is redistributed to users for example as a user if you submit a large transaction that's going to shift prices you could make an agreement with someone that's going to extract from you let's say they make one if profit by attracting from you you could get an agreement saying that okay you have to pay me back at least 90 or something of this extraction and then then it's fine so in this case Mev wouldn't be as tortic as it would be normally and another big argument is that Mev needs to be attracted to ensure political safety there are quite some proposals to ensure postal safety by other means um but the extraction and redistribution seems like an argument which is very holistic meaning that users are don't fall through the trash and there are no incentives to be very quick to your notes okay so it's very difficult to say whether Mev is actually very good or if it's particularly bad it's easy to say that Mev cannot be ignored um why it's not um why it's not really settled is because there are lots of nuances as well for example some backgrounds that we talked about before that made sure that prices in liquidity pools are equal are seen as bad for example if you have a lot of transactions trading uh e for Bitcoin and Bitcoin for Eve and the other way around you can first align all of the transactions trading e for Bitcoin and then back run um by base basically first having all the users pay up the price and then taking free Arbitrage profits which would be seen as a bad background and so noticeable Mev can be detected and also MVP can be easily classified into good or bad meaning that's not that easy to say that we should do particular things with it however there are things to be done so some responsibility lawyers with app developers and you could design your dapps with for example mechanism design and Game Theory in mind such that your users aren't extracted from too much um this is something that's very important and there's an increasing line of research and um we can't say that all of the responsibility lies with app developers because um some Mev cannot be mitigated by only one dab it's a contribution of multiple factors multiple transactions that may be unrelated meaning that there's also a rule for the protocol um to make sure that users aren't extracted too much from okay so and now we'll be going a bit into ongoing research that we do at the EF at the robust and centers group so I would also like to invite for this um and if you have any questions do let us know no otherwise we'll be happy to talk about what we do we do basically crypto economic research on the foundation of the assumptions that we talked about earlier um so are there any questions about what we've talked about up till now thank you yeah um do you mean with with weights before yeah sorry I couldn't hear it um I think any of the it's it's also irrespective of um shutting in some sense so there are for example uh close the main Mev opportunities that don't simply disappear because of sharding um so no I don't think it would disappear um I know if you have anything to add to that hi um I guess it could be mitigated somehow if most of the user transactions move to like Roll-Ups and Roll-Ups are the ones who use the data sharding facilities that we're now building at protocol level uh in that case most of the Mev may be accumulates at Proto at the roll-up level and you might not see so much of it at the base layer of ethereum but yeah as Julian said because Roll-Ups don't just live in their single world for instance you have designs for pulled liquidity where different Roll-Ups could use the same liquidity that resides at the base layer or at some settlement layer you could see that some of the Mev sort of percolates down to wherever liquidity is so many people I think are trying to build models including us so role of Economics is something that we're trying to think about to to see how the value flows from the users to the protocols to protocols which are on top of ethereum and mevs is a part of it yeah yeah so maybe I'll explain what the sandwich is that is in general um so um if there's a is there a transaction moving prices you can put your transaction in front and the transaction at the batch so back running um and profiting at both sides and in this case you have a transaction in between two of your transactions which makes it a sandwich which is seen in general I think as a tortured form of Mev um but yeah you find for safety because it can be seen like that um yeah also in traditional Finance it's a difficult argument I think it's not as Nuance that um or Market making isn't yeah it's it's not um as atomic Arbitrage as it's here like you have a hundred percent chance of making money and if it's not profitable you simply let your transactionals execute um but then there are quite some parallels we made between Mev and high efficiency trading and traditional Finance yeah yeah if I cannot do this I think in traditional Finance when you see high frequency trading a lot of value goes to I don't know put in your computer next to the New York Stock Exchange or billions of dollars to shave of nanoseconds to to your strategies this is economic value that just leaves the market and goes towards people who build all this infrastructure maybe one of the opportunities that we have with a protocol with respect to Mev is if it can be captured and if it can be captured efficiently but this value could serve to strengthen protocol security rather than hamper it so of course it doesn't mean that yeah let's get user sandwich because that gives us more value for protocol safety I think of course like we should design dabs such that these bad outcomes don't happen so for sandwiches specifically there's many different proposals that I would say realize different trade-offs that users might have so just mentioning some of the top of my head one is encryption so your transaction could go encrypted be committed to and then executed so people can send with you because you don't know what happens the trade of here of course is that the execution latency is a bit higher but maybe as a user you're fine with this another ID is receive time ordering consensus so there's this idea that you know with transaction a is seen by most of the network before transaction B then transaction a should be included in the block before transaction B it's in theory I think a property that is really nice but again because we're in a decentralized system there's no one that reports oh I've seen a before B so a must be before B and again you can have these games of collocation so again trade-offs here as well another thing that I would say is a relatively new idea is the idea of offering your order flow so getting paid for your transaction saying well if my transaction is so valuable to you you should pay for it that's what Julian also introduced earlier I hope that we'll see more protocols in that direction because these are the ones in my opinion that make the users whole and also allow the protocol to to capture some of that Mev just trying to make the game more fair I don't want to comment on flashbacks specifically because I'm not working for flashbots but I would say flashbots other people in this ecosystem are trying to understand Mev from first principle where it comes from the view of course is to to use it as a Force for good so trying to ensure that it doesn't disabilize a protocol that it doesn't hurt the users yeah so out of that comes from mitigating it if it's bad part of it comes from containing it and maybe capturing it if it's good uh yeah I would say these are broad Strokes of the ecosystem but yeah sure um so multi-dimensional test phase is very different from Mev it's not placed it means that now we pay cash for any kind of operation that you do whether you store something on the blockchain or whether you do just simple operations like multiplying um we all tram this costs of computation cost of storage into one unit which we call Dash but we could split this up into multiple units and so that you pay for pay more directly for what you use so if you're if you're trying to store things you pay for that you store and you don't congest blockchain with um so in this case the dash limit is set so that's um people aren't their computers aren't overwhelmed but for example if you have lots of transactions only using one particular thing like if they're only transactions using storage there's a lot of operations that could still be executed by people and so in this case multi-dimensional Dash would mean that these computers are used more efficiently basically and the more a transaction could be executed thank you yeah yeah adding to this if you've heard about Roll-Ups so the idea of rollups is where chains that exist outside of the ethereum base layer These Chains to secure themselves with ethereum they have to post data to the ethereum base layer basically the kind of summary of what happened on the Chain so this data is not executed so it doesn't add execution to execution cost to the base layer but it needs to be made available and stores for instance these are two separate types of resources probably if you've heard of eip4844 though the idea of providing a much greater data capacity at the ethereum based layer is separating the market between the ethereum execution and the market for data that Roll-Ups are posting in that case you would have something like two base fees or you would have the way to differentiate between two markets any more questions or okay so what we are personally working on is for example me the multi-dimensional tasks proposable Separation by David thought about and I've already based upon block space derivatives so ensuring that people can hedge against gas fees rising in the future um yeah if you'd like to talk about that please find us yeah and I would say crypto economics is relatively new as a field uh there's a lot of people who don't have traditional economic background or even Computer Science Background who get interested in it so yeah the barrier to entry feels a little lower mostly because there's a lot of resources now that are available if you go into the Devcon video archive there's lots of talks on crypto economics that are interesting and yeah if you find if you think it's fun I think both Julian and me would be also happy to answer questions offline talking about resources we compile the list and so in the table there are some collectives or groups that's published research research on crypto economics and in the bottom there are some links to some personal uh blogs from people during part of bay about some crypto economic research the slide should also be made available later um yeah so that was it that was it uh we ended a bit early so if anyone has any questions please feel free but thank you very much very much for attending and you can always also ask your questions uh later if you'd like [Applause] I want to give special thanks to Julian because he didn't know he was going to do his talk three days ago and I think he did a really wonderful job so if you can upload him again thank you Julian We could decide to draw the boundary a little further so this is the distinction I was making We could decide to say well the protocol should guarantee some [Music] [Applause] [Music] foreign [Music] foreign [Music] foreign [Music] [Applause] [Music] foreign encourage you to to check some of these out these are interesting questions [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] thank you foreign [Music] [Music] [Music] [Music] thank you [Music] how to treat modifying awesome along with 20 different Communications all changing at the same time um communicating and debugging it's great that we have a decentralized environment awesome you have to wait for the Australians to wake up for anything um it does take quite a bit of effort and quite a bit of time on our side we had the kind of schedule around people waking up in Australia around Americans a lot of different things and figuring out how to do all of this in a reliable Manner and on a timeline was crazy um the last one was debug knowledge we were really surprised the type of debugging you need to do for CLS and years are totally different we had to see how to bring all of that confidence in one place foreign [Music] [Applause] [Music] [Applause] [Music] thank you [Music] [Music] a few minutes Mr crack I can't talk too loudly can't talk too loudly exactly yeah like decentralized and fewer God you know it's uh [Laughter] I know yeah it's purposely fit down on it yeah exactly so like Slackers defeated me though can I do this slack has defeated me I'm confident there anymore no it drives me crazy I don't know although did you see this like do you have slack there yeah yeah go go my name call my name I sensitive mic yeah and then click my scroll down what do I see the profile yeah scroll down it is there [Laughter] I don't know [Laughter] one day I know okay guys I'm just going to wear five more minutes and then I'll begin just because I think it starts at half past doesn't it um awesome there's no regressions happening we currently have the Specter setting learning every night the second one is hype tests you might have seen this being reference a couple of times um with the simulator and they essentially startup mechanics and then run the tests against a predefined interface is a couple hundred tests [Music] everything and then brief example of how this is is it sets up a tiny instance this is a lot of awesome work by Mario [Music] um then we have [Music] excited [Music] [Music] [Music] maybe the I mean like confidence like the coding ones I think like breaking smart contracts is [Music] more content [Music] yeah yeah hopefully let me know if there's anything let me know if there's anything that's like right now I want to revamp it to cut down the contents was easier to then yes so if there's anything you find [Music] if you're quite familiar [Music] um extreme [Music] Love Actually [Music] thank you very much [Music] foreign [Music] guys so I'm just going to get started now um so what I'm going to go over today is basically scaling ethereum and of course the pitfalls and the solutions now I was told this was supposed to be an explainer to me like I'm five but who gives that to an academic you know so what we're going to start off with is something very simple like a recap over how blockchain works and by the end of it hopefully it'll be a bit more technical a bit more difficult and we'll challenge everyone in the room but just to get a round of hands so who here has a technical background like who's here as a programmer or a few people then who's not a programmer who's like more business product okay that's awesome this is a good mix so I hopefully I've targeted this correctly then and remember guys this is a workshop it's not really a lecture I don't want to talk with you guys for an hour because I'm pretty boring that way um what I've done is I prepared content that I think would be useful but if you have any questions whatsoever over the next hour just throw up your hand I'm happy to explain the idea a bit further or hopefully even diverge if that's useful for the for people here because if you have a question five other people probably have the same question as well so please don't be shy there's no dumb question so as I mentioned the goals for today maybe I'll leave his laptop here ah it makes it easier for me cool so I guess what we're going to cover is you know what is it we actually need to scale and why like what do we mean by scalability uh the bottlenecks of skill ability today I'm picking some core bottlenecks that we can go over that are pretty simple to grasp and then finally the future roadmap that ethereum is sort of considering I know the whole modular approach I'm sure you've heard off already so what do we actually need to scale so this is a basic recap of you know what is a blockchain so as we all know a block is just an ordered list of transactions a block is produced every 12 seconds and has appended to something called the blockchain you know given the name as a chain of blocks and it represents the canonical history of the entire network now Alice you know Alice is an inspector so she's a cute little heart and her job is to make sure that every new block that is produced is valid and so what she'll do is she'll get a copy of the blockchain so you'll replay and execute every transaction and eventually so compute a copy of ethereum's database okay so the one thing I want to highlight is we have the blockchain which is the canonical history of the network and every single transaction that has ever occurred and we have the database which is your current account balance smart contract byte code you know the actual you know the programs did as well and there are two very different things the blockchain's history the database is up to date on my current balance it's very important to have that distinction and of course the blockchain computes the database so anyone here he gets a copy of the database or copy of the blockchain can compute the CM copy of the database as everyone else so it's widely replicated across the world and I like to call the blockchain a cryptographic data trail because it allows us the audit the database in real time you know there's no other let's say your bank you can't audit your bank can you you ignored the blockchain it's an audit Trail now I'll list the inspector could be any of us you know and who's actually running a node here is anyone running a node one guy over there there you go we've got some inspectors that's creating a small sample size but still a grip so the peer-to-peer Network I mean it changes from time to time but it's normally around like 10 000 computers that are online fully synchronized with the network and they're auditing you know blockchain real time now the peer-to-peer network is responsible for propagating blocks and transactions its only goal is to gossip if you have a transaction it goes bad to peer-to-peer Network and it spreads out the everyone so a single transaction will reach you know 10 000 computers 100 000 computers and the oil has to help them within a few seconds the CM for blocks on the peer-to-peer Network as well are the block block proposers and a theorem we call them the validators and the proof of stake chain in Bitcoin they're the miners the ball proposers are also on the peer-to-peer Network they're listening now for users transactions and of course new blocks now the only thing block the block proposers provide is a transaction ordering service they have absolutely nothing to do with what it means for a transaction to be valid they just take user transactions stick them in a block order them and then you know send that out to the world so as an example user sends their transaction flows across the network and every block proposal will hear about this in about two to three seconds and then a block proposal will produce a block based on the transactions they hear and every single peer in the network will get this block validate it and then update their copy of the database and that's basically what's happening under the hood a block is really like a Bots update you know for a database you're just doing it every 12 seconds and you're updating everyone across the world they're a database and so what we end up with is this public and Global database conceptually it's like a bulletin board like everyone here can see the exact same image under the hood there's thousands of copy of this database everywhere it's widely replicated and that's what helps secure the network because if anyone can get a copy off it then we can also check that it's correct sort of scalability come in you know why do we care about scalability so there's two real people there's two you know parties we care about one are the block proposers it's really important to block proposers can get the most recent block right away so they can you know take the block check is correct and then extend the blockchain the block proposers want to converge let me get back to the blockchain the block proposers want to converge on a single blockchain so when a new block is produced they want to take it and then extend it you know block one block two block three block four so it's really important they can get the blocks very quickly at the same time we have the peer-to-peer Network we have Alice the auditor and some people in this room and your job is to hold the block proposers accountable you want to validate every transaction and if they try to break the rules you know they try to include an invalid transaction then the peer-to-peer network will reject it gets rejected they don't make any money they wasted their time and of course you know blog proposers will then just not extend it so there's two parties we care about block proposers and the verifiers and what's important is that you know want to be mean for scalability what we really care about are the resources you know what compute isn't required What bond with is required on what storage is required and we have to consider these Resources with the goal of decentralization in mind you know what are the minimum requirements for someone to run a node for someone the volatia blocks or even for someone to become a block proposer do you have any stakers here by the way any eve two stickers there you go over there got a couple of e-stickers you know we got to make sure that the resources are good enough so you can run your sticker at home hopefully you're running it at home anyway I don't know how you're doing it but we know we care about compute bandwidth and Storage now there's one takeaway here I hope this is the biggest takeaway you take scalability cares about resources on the delicate balance between verifiers and proposers so proposers commit blocks and verifiers can check it in real time it has absolutely nothing to do with transactions per second that's more like a byproduct you know you could have one transaction that explodes to the database and then none of no one here could be a verifier so if you ever hear blockchain project which of course you will you're like we can do 10 000 TPS then you can just call our  because sorry the character because it's all about resources and that's the most important thing we have to consider and how we measure scalability so the reason just to recap the basics there you know we have block proposers transaction ordering service they propose blocks to the network we have the peer-to-peer Network that has block proposers and verifiers anyone here can be a verifier and your job is to hold the block per block proposers accountable okay and make sure that the consensus rules and the network rules are enforced in real time and finally scalability has nothing to do with TPS it's about resources and how you know what's the likelihood that someone can participate in the network cool so let's have an overview on some of these scalability bottleneck oh yeah exactly yeah it's we'll get to this sort of the balance there but that's a good point Nick what what is the meaning of resources and for the goal of decentralization we need to pick the right you know configuration that maximizes the population so we say you know we want 90 of people in this room to run a verifier what are the requirements that's what we have to have on the network and the other questions by the way please throw your hands up you know there's no dumb question cool okay I'll just continue so let's have an overview of some of the scalability challenges that we face today and we're going to do this again through storage competition and bandwidth and you'll see computation and bandwidth sort of join The Gather when we talk about the fork Grit so let's jump in the storage okay so the storage requirements for a node is you know how big is the database you know there's a database of everyone's account balance how big is that there's something called the mempool and it's more like a cache so you're on the peer-to-peer Network you're running a node and you hear a new pending transaction what you'll do is keep a copy of this and pass it on to your peer if you hear the CM transaction again you'll just reject it so it's really to prevent a denial a service attack on the peer-to-peer Network so people can't spam it with the CM transaction over and over again but again that's something you will have to consider when you run a node and of course the blockchain itself how big is the blockchain and even how long does it take to synchronize it you know how long does it take us to compute a copy of the database so before I begin has anyone ever heard of an archival node or a full node or a print node okay awesome and raise your hand if you think they're really confusing terms oh look at this oh there's one guy thank you the good guy over there the honest guy they're pretty confused I know the Bitcoin Maxes have no idea you know uh they're always misinterpreting the freeze so let's talk about them what's a full node a full node is a piece of software that will take the entire blockchain validated from scratch up to the beginning and then compute a copy of the database importantly they keep a whole copy of the blockchain locally guard blocks they keep all the blocks the reason they do that is if you join peer-to-peer Network well you need a copy of the blockchain so a full node will supply new peers on the network with a copy of the blockchain so they keep it all around and uh most important was actually quirky about ethereum is that so if you look at the blocks you know block 7 e at 910 nodes typically keep around copies of the database I think it's like 120 ear blocks worth I forget the exact number and the reason for that is to handle reorgs so has anyone ever heard of a block reorg before okay we're going to jump into the block reorgs very soon that's part of the fork rate but the idea is that if your transaction got confirmed in Block seven and naira blocked hand you have some guarantee that is probably going to get finalized but an alternative block for maybe from block five could emerge that removes your transaction but if that's the case well you need to have this database so you can quickly jump back and then deal with the new fork and so we have to keep around you know but 100 copies of the CM database just to deal with reworks but all the other databases can be deleted you don't care about your very historical databases just the most recent copies an archival node is very different an archival node is something that ether scar mode run or a block Explorer archival node is where you want to quickly look up historical data so maybe you have a request what was my balance a block to that could have been a year ago there's no reason to run that on the peer-to-peer Network because that you know the probability of a one-year reorg is very small in fact impossible in proof of stick ethereum but an arc Avenue will run this and that's why when you hear quotes you know an archival node is like two terabytes in storage and ethereum isn't scalable but that's because they run an archival node in you know you don't need all these databases you just need 100 of the most recent databases then a print node is where a prune node will discard the historical blocks a prune will just keep around the most recent blocks and the most recent copy of the databases they prune as much as they can and they have minimal resources they still validate everything you know you still go from the start to the end you validate it all but you just keep around the most recent data so my question to you guys is you know uh it's my next slide if you're going to run a node like a block proposer or a verifier which one would you run let's do your readers of hands would you run a full node for either verify or block proposal yeah okay read your hand guys if you think you want to run yep that's fine that's a good answer what about archival node would you run an archival node for the network no maybe when you come but there's no need you know it's spit a waste of resources and what about a print node yep exactly that's probably what most people run today you know most people do want to keep around the entire blockchain they just discard most of it and as we're going to see that's quite a lot of gigabytes so a copy of the oh yeah what one e what's an e on oh events um yeah so an event that's a great question so basically when you execute a trump so in solidity and a smart contract you can define an event so let's say it's the vote function if I cast my vote it will emit an event that will tell the world and notify the world that Patrick has caused the vote the way you get an event is when you execute a transaction it produces a transaction receipt and in the receipt is there is the event importantly most I don't even think archival nodes receipts aren't typically stored they're normally discarded right away but yeah so you don't really store them but you can still get the events in real time because you're validating blocks in real time any other questions by the way yep yep I think I mean I will touch on this that's a great question so just to summarize ethereum now has two layers has the beacon layer that deals with proof of stick and you have the execution layer which deals with obviously the execution of smart contracts for now I'm assuming they're the same thing you know if you're joining the beacon you know for the beacon chain again you just care about the last finalized block after finalize you've done that's about like 15 minutes you could technically just delete the rest of it you need to keep around recent data but for now we'll just assume they're both the same thing but we will hopefully touch on that soon so let's oh yeah yeah so the real difference is that in a full node you keep around the entire blockchain and the reason you do that is to serve at the peers on the network a prune node deletes most of the blockchain they just keep around did I say and this gets four blocks and that's the deal with forks and reorgs it's called reorg cfd so you're assuming that if I keep around oh yeah I think there's default settings um I think there's a default setting but really you know the more you store the better you can handle reorgs so one issue we had was I previously worked on a transaction relayer where you'd send transactions and try to guarantee the delivery so we wrote Our Own blockchain machine the deal reorgs um we would keep around three to four hundred blocks and you know obviously copies of the database but if you run this in robsten Robson was a very adversarial Network and you'd wake up one day and there's a 20 000 block reorg and the real Arrow will just tip over because it just can't deal with 20 000 block reorgs so it really comes down to what network are you running and you know uh what so another example is a theorem classic ethereum classic has had 10 000 Block reorgs in the past which would also make a lot of notes just collapse over because they they're not expecting that huge reorg cool any other questions by the way yeah oh for Forks sorry what was the question oh for reorgs um I think okay so let me I'll make it to the rework section first we have a picture of it but yeah reorgs are less likely to happen and proof of stick you know because part of it was part of the puzzle for proof of work but I will touch on that because there is still prop you know a good chance on proof of stake any other questions hmm yep exactly so improve of stake is above optimistically it should be about 15 minutes worst case scenario could be like three weeks um well maybe we'll chat about that afterwards it's a great topic okay cool any other questions or are we all satisfied cool I guess your goal is to make sure I don't finish my slides as well very likely to happen Okay so that's a good what a node restore so on bitcoin this is maybe four months ago when I made this uh the database was roughly about five gigabyte but if it was an archival node version it was about 35 gigabyte on ethereum the you know a normal pruned a normal database was about 700 gigabyte so the database is quite big on ethereum you know according to the stock that I have here an archival node was 10 terabytes and that's normally the number you hear throwing around Twitter by all the Bitcoin Maxis but as you know that's for Block explorers an Aragon got this down to about 1.9 terabytes uh I actually forget how about but we'll first we'll figure it out and then what about the the blockchain itself I could just look here the blockchain so for Bitcoin you can see that the blockchain's about 422 gigabytes which is huge by the way but on a print node they keep around seven gigabytes perfect blocks they discard most of it on ethereum next slide the blockchain's about 200 gigabytes you know so it's actually smaller than Bitcoin which is surprising actually given you know there's blocks every 12 seconds that's generally because blocks are smaller in ethereum than they are in Bitcoin because we worry more about Goss than we do bite size you know bitcoin's all about one megabyte two megabit blocks and it's really about the size of the transactions but here you know it's about 200 gigabytes for the blockchain and overall including Watson memory and what's on disk you're probably going to store around 560 gigabytes give or take it's a roundabout uh like client got me this by the way he's really really thankful from getting me that that picture of course flood protection you know how do we deal with you know now the service attacks in the network on ethereum it's about I ought to rough estimate about 100 megabytes you might store in the worst case so the memory pool has nothing to do with scalability really so far we're not really hitting any storage problems for dealing with uh pending transactions on the network such storage you know we covered you know blockchain the database and the mempool and the different types of software that you could run repetition and so there's this really great blog post that I'm going to run through by Jimson Lop he runs this every year how long does it take to synchronize a node and it has this pretty beefy machine has one terabyte stories 32 gigabyte let's see how let's see how well it works so on bitcoin back in 2011 but November it took about 400 minutes to synchronize the entire blockchain that's pretty damn fast don't even know how long that is like five six hours and you're fully caught up on every transaction that's ever occurred in Bitcoin yep that's a great question I'm not too sure if this includes bandwidth I don't know if they already have a copy of the blockchain or of the request not in real time because because our latency will cause an issue awesome well I lots of seems for computation for now and we're not worried about latency because I'm not actually too sure I don't think he defines it in the in the blog but anyway it took about you know 500 minutes or 400 minutes for Bitcoin what about ethereum so his issue was that uh goofy amount of memory when it was synchronized and it stopped but after five days but that's because he has one gig you know one terabyte storage and he could have more stories to do with synchronizing uh but he would estimate it would take about 10 days but the important bit here is what's the bottleneck why has it taken 10 days to synchronize ethereum and you would think as execution but actually is input and output it's just reading and writing to the database here according to the the stats that he had you would read 15 terabytes from disk and 12 terabytes back the disk for the first five days of Trends off you know the blocks and there's another five days to do by the way so that's probably 20 30 terabytes worth of reading and writing and why is this you know why are we doing like 15 terabytes of reading from desk for a blockchain there's about 500 gigabytes are the database so the reason is that oh actually just before I get into the reason but I deleted the reason oh I haven't okay it's over there obviously I've messed up my slides the reason is that in ethereum in the block header there's something called the state route and this is good for the snapshots you know you want to download a copy of the database you want to make sure this database was correct for this block and so in the block header you have a hash of the entire database you know you get the entire database you build your Merkle tree then you have a host that represents the entire database but if you're hush in the database for every single block that's a lot of reading and writing from the database and that's why you have those dots um it's also this is very expensive so Aragon stop doing that so they're reading and writing the disk is still about a terabyte after 10 days but actually no they heard of us that they synchronize they synchronized in two days so just removing that one part of the validation you see if it days worth of time to synchronize the question is do you need to do this should you have to Hash the entire database and store it in a block header so in Aragon when you get to the most recent block you'll download the database from the peer and peer Network and you'll check if it's correct and if it's not correct then you'll start rolling back until you find the Mystique so it's not as an essential check but it's useful if you want to download snapshots from the network of the actual database itself but the surprising takeaway here is that execution isn't really the bottleneck it is expensive Extrusion is expensive it's just reading and writing from the database is the current bottleneck for ethereum so is the bottleneck also related to that is the disconnect I think it's the first point that you made it's just the the latency from Freedom writing from disk that's why it doesn't work on his new hard drives Works in ssds that's also a great find very good technical Point um yeah they should I mean so uh there is work on that called an access list so in your transaction if you define an access list I think I touch upon it later you can Define what storage slots in the database you're accessing and so if you have two transactions that don't access the CM part of the database you could run that in parallel but right now access lists aren't heavily used but they they should be because they help a parallel execution awesome so anyway just a final joke I mean I don't mind Salon acts I'm not a hitter on Solana but I just made a joke that you know some projects gave up on the fact that people can synchronize the blockchain uh I think actually this is the internet computer you have to get special Hardware from their own suppliers they run a note on their Network very permissionless isn't it um but anyway that's synchronizing is a fun topic to talk about so what about the fork Grant so let's assume more block all block proposers are honestly following the protocol they get a block they extend it and they propose a new block no malicious behavior whatsoever by the block proposers so the fork grid is the following so let's say you have block one and block two then a magical wild Fork appears you can have block 3A on block 3B proposed by two different block proposers the question is which one do you extend eventually you know you'll get Block four block five and everyone will Converge on the longest chain or the heaviest chin block 3B becomes a steel block you know the content is ignored and it eventually ends up as an uncle block so if you ever hear the word Uncle block as a fork that didn't make it into the canonical chain but it did exist and in a way this is wasted resources and we have these two competing blocks you've wasted some resources because this never actually gets used you really want to maximize a single canonical fork with no with no Forks okay any questions on this part before I continue because this is quite important fairly straightforward awesome so the inability so why do we why do we consider the the the the the fork rate so one is about this you know how reliable is the network you know if you get your transaction confirmed in a block but there's a 16 chance that it gets dropped and reconfirmed later well that sucks from user experience perspective you know if I only have to wait two or three confirmations that's way better than winning 20 confirmations and the fork grid is really about how reliable is a confirmation and a block at the same time there's a bond with on a computer overhead if I send everyone here a block I've used your bun with you then validate the block I visited your compute but in the end the block never gets in the blockchain so I've just wasted your resource for a block that wasn't actually useful do you really want to minimize that fork grip and there's two aspects that we have to consider is one you know what's the length of time between blocks is it 12 seconds is it 10 minutes and how fast does a block reach another block proposer okay and also of course what's the Frequency so this is sort of the big block first it's a small block we're back in the 2015 World with the block size Wars you know this is pre actually I guess the theorem was born around this time so if you have a one megabyte block every 10 minutes if you imagine this being the peer-to-peer Network and reaching all the peers in the network then it won't megabyte block should fly across you know everyone gets this within a second not much issue we have a one gigabit block you know every 30 seconds well maybe you know one gigabyte takes a long time to get across the network then you may have a competing block at the same time then you have another competing block and you have lots of forks and then you know you've wasted time because there's more there's three competitive blocks and of course this is a bomb within compute and so these are two extremes we have a two megabyte block every oh yeah I'm right now you know if you have a you know a block that's greater than two megabytes but less than one minute you increase the fork rate smaller blocks longer interval smaller Fork grid so is there a good way to get you know a good feel of the numbers here so there's a study back in 2016 for Bitcoin and I would love it to be repeated for ethereum because it's very useful for proof of stake is oh and this one point is uh on bitcoin we only consider megabytes the size of the block on ethereum we consider gas because gas takes into account bond with stories and compute you know 30 million gas is the maximum size of a block and it tries to take into account all the resources that are required and actually there's a z cash article there because right now Z cost is being spammed is costing ten dollars a day and they're going the database by like a gigabyte per day or something you know it's very cheap that you know attack the network so on ethereum you know blocked you around 120 kilobytes there are 30 million gas and they occur every 12 seconds roughly even previously a gun proof of work that's that was for the proof of work chain on Bitcoins you know one to two megabyte every 10 minutes so on ethereum and proof of work ethereum the fork root was around five to six percent at any time so that means you know five percent of all blocks were wasted resources and that's just because of the nature of proof of work where today with proof of stick is less than one percent and one thing to highlight is that on proof of stake ethereum the block proposal has to send the block within four seconds then the validators not committee will vote on that block if it takes longer for a block does it take if it takes longer than four seconds for a block proposal to get their block across the room you'll end up with a fork because the committee will vote on the parent block and not the current block and so you can see right now there's very little Forks so clearly you know there's a good block size for the proof of a stick chain if the fourth rate goes up then we know stickers are no longer keeping up of the network and on bitcoin I got this picture from February 2022. it happened every one of the two months because it's small blocks every 10 minutes very rare to have a fork in Bitcoin so what's the ideal block size and interval again from 2016 for Bitcoin and they were trying to work out you know given the current peer-to-peer Network like this room if I want to make sure ninety percent of people in this room can get blocked in real time what's the ideal block size and so actually what do you guys think that I do a block request I'm just about to get on that that is a big part of it yep the Chinese firewall specifically um so I'll leave that for a second any other questions before we continue awesome Okay cool so just look like just told some numbers out there back in 2016 we want to keep 90 appears on the network what do you think the ideal block size would have been without increasing the fork Grit does any wild megabyte number out there one point five megabyte there we go on the other megabytes two megabytes one more guess one more guess oh Bitcoin this is 2016 Bitcoin 20 megabytes oh wow so we found the Bitcoin Maxis and the Bitcoin Unlimited if you know your history no that's great though that's a great quick yes so um so the ideal block if I deleted the slide of course I have so the ideal block size was actually around four megabyte from what I remember it was by 4.2 megabytes or something uh to keep 90 appears on the network um and just for that table so what we're saying there is that that table is really saying you know how long like how fast did the top 10 of nodes get the recent block then how long does it take for 90 90 of nodes do you also get the CM block so in the first example for for the second one for one megabyte block ten percent of nodes will get this within 1.5 seconds then 90 of nodes will get this in 2.4 minutes so that's a 2.4 minute difference between the top nodes or the Foster nodes and the well-connected nodes on the slowest on the network so what impact does this have so that's my little China logo so as I mentioned back in 2016 2017 uh some there were some Forks on the network because the chart like 70 of miners were in China 30 percent were in the rest of the world which implies the Chinese miners got the blocks faster they get the blocks faster well then they can you know start working on it before the rest of the world so they may get like a 30 second or a minute head start on just solving the proof of work and so there was a biased towards Chinese Miners and what actually happened was that you had this private relay Network between all the miners so they just bypass the peer-to-peer Network altogether because of this issue but basically you know uh ball proposers will fall behind if they're you know the 90 nodes in the network on stick for proof of stick if it takes longer for four seconds for you to get the new block and you vote for the wrong block or even 12 seconds you may incur some penalties you know if you won't get sloshed and it's not like you want to lose all your money but you might not get like little rewards and your your yield will go down a bit so your yields directly impacted by how well you're connected to other peers and obviously as a verifier well you know the if there's blocks every 12 seconds but I'm getting the block after two minutes well I just fall behind eventually and I just can't keep up with the network I can't get up to the a cup of the database I just fall behind and I'm not useful anymore so typically when we think about the size of blocks we normally assume that the block proposals are very powerful you know they should be able to quickly get blocks execute them and send them out within two to three seconds we assume verifiers are weak so verifiers maybe it takes them you know six or seven seconds to get the block but that's okay because 12 seconds is the deadline so you normally assume you know different specs for different parties and I do have it there there you go four megabytes was what would the report recommended on bitcoin and you still have 90 of nodes participate on the network it's probably much higher now but that's like six years ago why is this all important you know why do we care about this aspect of scalability and it really comes down to you know what does the mean to be decentralized and everyone has different takes and what it means to be decentralized my take is really you know what percentage of the world's population can validate and protect the database in real time so regardless if you're in an Olivia Australia China the US you should have the right to run the software get a copy of the database rather be a blocked in real time or participate as a proof of stake you know sticker validator it's the CM for both because that's what it means to be decentralized it's a bit like Captain Planet you know we put our rings together and we protect the network so there are the bottlenecks you know I've just gone over some bottlenecks that impact the network there's obviously a lot more but there's always not the overwhelmed people so let's summarize this storage you know the storage bottleneck is really how big is the database uh how big is the blockchain I'm realistically who can you know cut my Hardware deal with that you know the size of that database as we saw his computer GM's lob's computer he couldn't synchronize Google ethereum because he ran out of space so clearly his computer could not participate on the peer-to-peer Network so you have to consider storage and you know how big this database gets two is compute how long does it take me to get a copy of the database and be convinced it is indeed you know the the one true database that we all have and right now our proof of work at least you're supposed to objectively follow you from the beginning to the very end and then you know how long does it take for blocks to get across the network and can we fall behind because we just can't get the blocks in time you know what's the latency issues around that and the most important bit is and this is why I don't like transaction throughput as a metric you know if you just blow up the TPS you know the the tip of the chin can become unstable because there's too many forks and then it's also difficult for us to keep up so I think for I remember hearing a stop for polygon so the proof of stake polygon an archival node was growing two megabytes every second and that's pretty damn big isn't it like two megabytes every seconds because of 15 terabytes then AWS can no longer handle that in a you know straightforward monitor so anyway that's actually why again the whole point of scalability is that fine balance between block proposers and verifiers and you know how big that database gets so how can we scale by still adhering to what it means to be decentralized and before I get into this was there any questions for the previous section I remember there's no stupid question yep so the uncle blocks um so you have the so the entire block is the block header and the block content the block content or the transactions that gets thrown away all we keep around is the block header and it'll be included in a future block so we'll block one's the block here if block one was the uncle block then maybe the header gets included in Block five so we're still aware that it exists yeah that'll be an uncle block so an uncle block has no impact on the database we just acknowledge it to say that it existed and then you reward the block proposer for doing their job and the other yep a very easy oh definitely I hope I allude a point I think I've got that my slide but what he's saying is that you know one of the ways we're going to solve these issues is your knowledge proofs so their knowledge proof is really useful it allows me to do a lot of the hard work to say you know let's just say I want to approve a transactions valid I do all the hard work then I send you the result of the transaction on a small proof that will convince you that it was correct so that way you don't have to natively replay the transactions yourself I give you the result plus the proof and you're convinced is fine but that proving part is very expensive I think a TX uh one or two seconds per transaction on a CPU and you know if you're having to do that for uh when you're proposing a block you know I create a block I make a proof for every transaction that's pretty slow so uh that's still very much a work in progress so that'll be more for the Roll-Ups so the rule apps you would assume there's a very powerful executor you can run gpus paralyze them and do the proofs in real time for proof of stake ethereum it's probably a little while off because proven is still very expensive I mean it's not too it's much cheaper than it was four years ago anyway I think a z cash transaction because that uses your knowledge proofs back in 2015 I think it was 60 seconds on a CPU to prove your Z cash transaction and now it's like a second or something I don't know it's pretty fast over the BLS signatures yeah I think it's like so he's also in bed the bite size so I think that's more of a problem for Stark so Starks will grow based on how much you're proving where a snark is constant sizes they're funny I forget the bite size with a Freddy Spa but star has a stalker issue yeah they're I think it proves to cost like five million gas and ethereum to verify just for the proof because they're very big but anyway any other questions guys before I continue awesome cool okay so how are we going to solve these skill ability issues so just a reminder when we consider scalability for the blog proposer we want to reduce the forcreant we want to make sure no one is witnessing their resources when they propose a block and on the verifier side we want to maximize the population of who can validate blocks in real time so reduce the resource requirements to run a node that's basically what we're trying to achieve now over the years since 2015 up to about 2020 and today there's been lots of crazy Wizardry tricks from basic engineering principles on the you know the make it easy to run a node so one is you know you can compress Data before you send it across the wire so on bitcoin we call that a compact block you know I give you a block but actually I don't give you the transactions I give you the block you know like the the transaction hash and then you've already got the transaction in your mempool so you can quickly reconstruct a block yourself so you reduce the data you're setting Across The Wire very simple engineering you have private relay Network so in Bitcoin all the miners had a private Network that only they could connect to and probably get blocked so you bypass the peer-to-peer Network completely you know is that you know ideal for a censorship resistant currency it's a different question you know you could do parallel execution we had a question down there before maybe a viewed access lists you know I know two transactions don't conflict execute them in parallel we speed up our ability the validator transactions in real time oh no there's also like you know set reconciliation that's also compact blocks but the issue is that in all of these engineering approaches we're making it easier to do the job but they still have to do it so now a lot of the scalability research is thinking do they need to do it could we take that responsibility away from the peer-to-peer Network and you know give it to a external provider who could do it in their behalf so the peer-to-peer network does the absolute minimum so what's the goal so it should look like this the protect decentralization we work out what is the absolute minimum the peer-to-peer network has to do and what can we offload to Services providers businesses always make the joke and fewer could do this you know why could you pass off to inferior to protect the peer-to-peer Network and so who's heard of this idea like the monolithic blockchain and the I guess modular blockchain but this says macro Services who's heard of that idea before the monolithic blockchain okay but the last people that I expected actually that's great so I stole this actually from a normal web 2 company because it isn't a new idea you know you build this big monolithic cobius difficult to maintain difficulty upgrade and what you really want to do is take out the little components and maintain them individually and hopefully delete them as well so that's what a theorem has been struggling with for the past six years we had this monolithic blockchain where it's trying to do everything at once I know what we're trying to do is Define each of the macro services or the modular components and then of course solve each problem individually so let's go through how we're doing this so first we have compute compute was one of the resources that we cared about you know how long does it take the execute a block what if you could uh have a dedicated execution layer so there's an execution layer that's doing most of the work and ethereum doesn't really care about that all a theorem cares about is the result of that execution if ethereum doesn't have to do the execution well it's way easier to run a node if you don't have to execute anything you know you pass it off to someone else the other one was bond with so right now we have to propagate all the transactions and blocks across the network you know what if you could have a dedicated data availability layer where you don't even care about the transaction content it's like a blob of data as long as there's a blob of data you know you get the blob of data then you can throw it away eventually you know could we have a dedicated layer just for data and ethereum doesn't necessarily care about that either and finally storage one of the node you know didn't have to have a database could you run a node and just delete the entire database and not care about it but the database is stored somewhere else so if you want to transact you talk to the provider you get the database content and then you send it off to the peer-to-peer Network you know can we build a settlement layer in a sense where all it does is minimal competition maybe stores account balances but otherwise it doesn't minimizes what has to store because you push that problem off somewhere else that's the idea behind the modular blockchain it looks like a simple Rini I mean you know I could be a marketing person being like well we're going to solve compute with an execution layer you know I just rename it but actually if you just find like you know if you make this dedicated layer in action then you can think how do I solve this problem so we're just renaming the resource in a way but in a way where it makes more sense on how to solve it so what this actually leads to is the rule up Centric or the roll-up Centric roadmap for ethereum has anyone heard of this the rule up Centric roadmap okay great about five or six people so that's good though because in 2016 ethereum they thought they would solve the world of execution sharding we all realized that was like a moonshot that was too hard to do and then roll up started to emerge and Rule UPS look like sharding in a way and so we've all pivoted towards this rule out world where we do all the execution and roll ups and what we're actually what's really solving the day are bridges you know uh this is how we've scaled cryptocurrencies for the past 10 years so raise your hand if you've ever used a coinbase binance or bit stomp or whatever there you go don't worry I'm not the SEC I'm not here to dox you you know uh but you know realistically speaking in a way cryptocurrency exchanges are like sharding you know you deposit your funds in the coinbius you go in the Columbia's execution layer you transact as much as you want there and then you bring the funds back and you're using the theorem as a settlement layer you know the get your phones on and off coinbase but otherwise coinbase is where the execution happens the issue is that I already got the question but the issue is that if you move all your computation the Columbia is cracking and binance well it sucks a bit doesn't it it's pretty custodial you have to deal with their customer support if you get locked out there's a private database we have no idea of the art you know of the rsats cover the liabilities we have no proof of reserves we have to blindly trust this execution layer we can't audit it in any way and we can do better than this and that's the goal of this rule up Centric roadmap the goal is to build a bridge that connects to another blockchain system this off chain system that you can check in real time and the bridge will hold your assets you can mint it on this other system transact there as much as you want and then bring your funds back to ethereum you know you burn it on the on this chain and bring it back to ethereum so really bridging is at the heart of how we scale ethereum we can build good Bridges and move the computation elsewhere we solve a big Pro a big part of the scalability problem an ethereum then becomes a settlement layer that does minimal competition for the bridging and of course recording everyone's account balances so it really is how it's going to deal with bridging in the future yep so much more expensive to do that with gas or you mean the actual bridging part remember drawing it back into like okay so just to summarize what he means is that when you bridge the you put the phones in the bridge you go to this other network there's gas fees here as well and there's also bridging it back that causes funds or you know causes yeah so I think um I mean a theorem should be the most expensive chain so that'll always be expensive so ideally most users oh sorry I'll just finish this one most users should not have the interact with ethereum they just live in these other layers and quickly transfer their funds and because they're the execution layer we can assume that they have like you know like starknet for example they can aggregate lots of transactions and you know aggregate the cost for everyone so there's still a cost but hopefully you'll be a lot less than ethereum it's the goal good yes that's a grip point so what he's saying is that when you bridge your asset to another layer you're taking on the risk of the bridge and we've all seen the binance bridge The Nomad Bridge the Ronan Bridge the Wormhole bridge and other Bridge there's obviously a collective War they'll all keep getting hacked and there's clearly a smart contract risk of using a bridge and then depending on how you've designed your Bridge you may also have risk on the off-chan system as well so that's why the roll ups you know what they're trying to build is just jump to that now oh actually let me finish this bit so the rule ups are trying to build a bridge where you don't have to trust the off chain system at all so really if the bridge is you know bug free then you should not have to trust the off chain system at all so that's the long-term goal but right now a lot of the bridges are a bit there's a lot of trust a lot of bridges but anyway so let's go to the settlement layer a theorem was a set of funds and the execution layers are on these off-chean systems that offer a seamless user experience and the point here is that the off chain database and the execution layer recourse the liabilities and the bridge records the assets and the bridges here just to make sure the outside is cover the abilities armor taxi user on the off chain system and how does the bridge protect the assets you know this is sort of the rule up talk where you talk about a validating bridge and how the bridge is designed to protect the funds for now we can just talk about you know how do we guarantee all the updates that the database are valid you know the bridge you get an update from the off-chin system then the bridge have to be convinced that this update is valid and correct and if it's correct then it will accept that that's the new state of the database you know the bridge will always check is every opt-in to this often system valid yes it is the funds are CF and the other one is where the fraud proves versus the optimistic roll-ups the next one is the availability problem that's really what I want to talk about so if you're on an off-chan system okay I'm all my funds are locked on I don't know arbitrim what that assumes is that there's one honest party in this room you can come online get a copy of the database and guarantee that all our transactions are eventually executed we can withdraw our funds from the system if that system is malicious and this comes down the data and there's three things to consider you know why does the data need to be publicly available what data needs to be publicly available and how do we guarantee it is publicly available so for these Bridges what you're assuming is that there's one honest party who can get the data recompute the offchain database execute the transactions propose an update to the bridge and let you get your funds out of the bridge now this is very different to ethereum as we've just said for about you know the past 40 minutes they're just trade-off between block proposers and verifiers and the resource constraints here we just have to assume there's one on this party there's one honest party out there with enough resources to get a copy of the database and execute the transactions and anyone in this room ideally could be that honest party so it doesn't allow you to go beyond the restrictions of the layer one you create all this big beefy machine this I don't want to say super computer but you have a beefy machine on this network that can you know reduce the fees for everyone because now the main fee on a rule app is not execution the main fee is the data that you push to ethereum that's the biggest cost now for using roll ups um the type of data that you propose to be you know the transaction history to ascend the bridge all the transactions or maybe just an update to the database and what is the state def between the two databases um how do we make this available you know just to skip over this a bit those are challenged proof sets for plasma and sort of field any trust like arbitrary Nitro or starknet um their data availability committee they have a committee that guarantees the data is available and that one honest party can get the database or a roll up where you take all the data and you post it to ethereum and that's the next part so now we have this settlement layer which is ethereum we have the execution there that's doing all the hard work the guaranteed one honest party can get that database all the data is being posted to ethereum and so ethereum also becomes this data availability layer they guarantee that one honest party can get the data base for the off-chain system so the long-term scalability goal for ethereum is to make this data as cheap as possible if you give me a theater or a bond with cheap then you could have real Ops that are humongous in dealing with you know crazy amount of transactions this is the Dank sharding this is eip4844 and this is sort of what they're going towards in the next few releases of ethereum their goal is to make data cheap so roll ups are viable and so this is the case then we push all the hard work off to the execution layer and the protect decentralization we just care about data and we just care about settlement okay that is what we mean by protecting decentralization can you get the data across the network as fast as possible I know there's also you know different networks emerging because now that we've separated our concerns maybe you have a dedicated availability layer like Celestia polygon novel or a theorem itself of dank charting I have a minimum ethereum actually I don't know we have the settlement layer here which is ethereum because they're doing the Roll-Ups then all these different execution layers are emerging they're all solving different parts of that puzzle of how we scale ethereum and so just to summarize oh sorry I'll put that back up I see someone's taking a photo cheers it says to summarize because I know there's a lot to take in by the way as I mentioned we start off easy and then we get very very hard um this is summarized you know there's there's uh we allocate a resource of purpose no data settlement execution now we're solving each of these puzzles individually scalability is really by bridging bridging is how we'll scale ethereum because we'll move the assets but we'll move the assets to another Network and transact there bridges are a give or tick very insecure but we're working towards building a a secure version of bridges and of course data availability is really the big bottleneck now for how we're struggling with ethereum and just to finish up because I've got one minute left is one of the oddest last part oh God so the bridge should be able to independently check everything itself so when you give me an update about the bridge you give me the update and I used to be able to check this is a valid update so either you give me a zero knowledge proof so there's a mathematical guarantee I ran a fraud proof right get the update and there's like a one-week window for anyone to convince me that is incorrect so the bridge has to be convinced cool awesome so maybe I'll just finish here um I think this is a great talk we nearly I didn't finish the slides but that's awesome because we had a lot of great content so I'll leave it here and I'll let the next speaker come up because I think he's hanging down there somewhere so thank you guys GG awesome oh wait here good times good times oh I noticed near the end it was but I was running out of times I was like I was going getting very intense um okay hello everyone uh let's start about today's uh explain like I'm in five session of the zero knowledge probe um yeah so I'm very happy to have this session for the beginners here by the way uh actually this session is a for a kind of a five years old kid so if you are already familiar with the journal knowledge proof that I strongly recommend you to uh listen here this session the designing public goods using zkps which is uh running by the Rachel uh who is a super designer of the intern foundation's PSE team she have designed a lot uh she has designed uh in three years at the team and designed a lot some zkp related user experiences and user interfaces so it might be very very inspiring oh yeah so let's start with uh about thinking about uh where the ckp is mostly used so we use zero knowledge proof uh when you want to prove a fact but you don't want to share the information so I want to give you an example about uh more detail so uh let's think about some example at immigration at the airport um so let's assume that I'm a immigration officer here and just an entrant here so uh when when you go to the airport the immigration Officer says where are you from then uh I say like I'm from Korea I'm from Korea then America's Officer says like uh North or South I definitely say I'm from South but unfortunately uh this immigration officer doesn't trust me that much so he's just like um I really think you look like from North so give me your password and and oh this poor Korean guy wants to keep the freedom to keep my personal information so I said like oh that's my personal information I don't want to keep my passport then okay what happens they just kick him up kick him out there and this situation uh what should I have to do here what should I do what can I do here yeah actually we can I can show some passport number I I can show my nationality or passport card um they cover but if we give a zero knowledge proof that I can prove that I'm a part of this South Korean yeah this is just an example which can be really happened in the future um yeah so let's think about what happens here so we can call the immigration officer here as the verifier and the Antron the poor South Korean as the prover and then the proverb prepares the witness using my passport which is which means like witness means I have some passport number here and I have my um I'm a mail and my my number in Korea something like that the birthday Etc so and then generates a ZK proof and give it to the verifier which means the immigration officer here than the verifier the officer says okay I'm gonna verify this proof using the existing database we have actually this is pretty possible and we're going to take a look at the details uh later about the how uh can how we can prove the membership of that I'm a part of this South Korean people then by the way uh let's see what is witness and what is informations we have to share here so there are some informations you can see here and maybe there's a passport number that I want to uh that I don't want to share with the immigration officer actually uh totally doesn't make sense but um so I recall these values uh as private inputs but I'm gonna just uh say I'm a part of the South Koreans so I just opened my nationality this is the public input uh and uh to prove that I'm a part of the people I just culturally some mathematical values using the inputs there so uh we call this intermediate values including all the private inputs and the public inputs as witness here uh and this is just a basic model of how zero knowledge proving system works so we have private inputs and we have public inputs and also we have the circuit here the circuit is uh about the relations uh what was the relations here in the immigration situation it's like I don't reveal my passport number but my passport number is definitely a part of the registered existing database of the Korean people's passport number so it's kind of a membership oh there's a database and I don't want to share my information but this is definitely exists in the database so this relation is the what is is the circuit here that uh do we have to soundness there yes we have the soundness there if the proof is too small we cannot have the soundness like the false positive is to probably drop up is the probability of the false positive too high so the proof that is pretty important and here okay but uh imagine if we do this thousand times at immigration so in regular Services okay just tell me your message something okay I actually it totally doesn't make sense so we need the non-interactive system here so any idea how to make the non-interactive system okay let's go back to the Rhino case in this case uh this is verifiable uh try to make the random question every time but what if we generate some random set of questions uh before these proven uh proving happens like uh just like this left left right left right left what is the problem here yeah right just using the dolly so we need to homomorphically encrypt those values actually uh today we're gonna We're Not Gonna uh deal with the homomorphic uh concept here but um we should encrypt this uh in a very viable way so um by the before uh we go to the next slide I just want to say here this is called common reference string because verifier uh already make made this uh for the proofer so this string is shared between the proverb and the verifier so commonly shared string which is can be the reference for The Proven system so this is called common reference string uh by the way because we need to encrypt those values I'm gonna get this in Korea maybe there might be a little people who can read this word but actually this is saying about go left and go right and if you go left and right go right twice something else so let's assume there is something that can interpret uh this encrypted command reference string so actually interpret is not a good explanation here uh in more detail let me assume that the five years old key understand hash function here we are adding assault and hashing the value and make a modular there so it's kind of a deterministic random stuff so uh the Rhino never can cheat on that if this rhino doesn't know the original reference string um yeah by the way um these encrypted common reference string is not enough to be used publicly because this is okay when only we want to verify this rhino is smart what we want to build is some protocol that can be used publicly widely for everyone so for example we have a system that we can uh we can transfer some if using ZK proof then the ZK proof will include some information that I have enough balance and I can generate some signature and every information will be in the JK probe but if if you can sit on that I just can move a lot easier to my account without the correct information without the signature so uh this is really important to make anyone no one knows the original reference string so uh anyone who wants to guess what it's called yes this is called the trusses setup to achieve this we have to do the trusses setup um so let me explain this one by one um actually this is how a zkp system works with I mean the zika snark works with the common reference string so there is a common reference string made by the trusses setup you share with the proverb and the verifier and the program verifier both does not know the seed original reference string here and then proverb picks a random salt and share it with the verifier and then also once the previous picks the soul the privilege can drive a set of questions because there is a reference string actually the verifier at first in the interactive system very final just gave the question to the writer but in this case Rhino can generate the questions by by himself and also the verifier can be can derived the answers write answers without knowing the exact questions uh if the verifier using the homomorphic feature homover characteristic of the reference string it can be a little bit um tricky but this is also a pretty important thing so I want to explain uh some multi-party computation thing here yeah actually um yeah this pretty technical but uh not that um difficult actually so um yeah please uh by the way uh let's uh let me say why we why I'm explaining this multi-party computation here it's like do you guys know how the trust itself works and why you need to join the ceremony that is share today's opening ceremony so I just want to share the how it works and why you need to join the uh kg transistor of ceremony here so let's go through actually this pretty for 15 years old kid but let's go so yeah um there is a homomorphic hiding G to the A is kind of a we can make some encrypted value using number a so actually there is a signature of the homorific hiding age but just think about that just as a hash function so to think about G to the a is something related to the hash of a actually it's kind of a homorphic hiding over a but um then there is a um characteristic that we can compute the G to the a using number a pretty easily but in contrast is extremely difficult to compute a from G to the A actually this is called the logarithmic discrete assumption here but Let's Escape there skip here and also if we have a g to the A and B we can also compute the G to the a b pretty easily okay this is a this is a um some key features of the homography hiding using the electric curve cryptography and uh let's see how the traces setup works so trust setup is creating a command reference string and without anyone knows the original reference string so at first Alice joined the ceremony and picks the set of the questions the set of questions here is a b c d e here all the A and B and C the n d n e are numbers in a finite field so some numbers there are numbers and then uh we compute actually we did some encryption thing here you remember so the left left right left right left is a b c d e and this something already encrypted value G to the a g to the b g to C something so we can compute G to the H to b c and G3 there and Alice shares this G to the A and G to the E values publicly peep to the people then Bob joins this ceremony in this case uh Alice May uh Alice May discard the ABCDE value if Alice is innocent but if Alice is Not Innocent maybe she's just uh store the ABCDE value in her computer by the way uh Alice uh didn't share the ABCD value with Bob yet okay then Bob also picks a set of questions again here that is f g h i j and then we're gonna create a new reference string using G to the a to the G to the E and the F G H I J only because we can compute G to the AF using G to the A and F together so Bob can generate G to the AF G to the BG G to the CH without knowing the a b c d e value here and call does the same thing here okay then um if any one of these three participants discarded and destroyed the randomly picked question values then uh maybe anyone might know the original reference string here AFK bgl chm and Etc because to know AFK you need all those values a and F and K so it means to just recover its reasonable value you need all the secrets from the older participants of the ceremony so it makes if any one of the participants that is called it and destroy the value then oh we are safe okay so the best way to use this common reference string is actually not to trust anyone it means just go to the transistor of ceremony and this car you'll see it there then you will be safe at least by yourself if you just keep it discarding this destroyed right don't trust anyone just trust actually don't trust yourself too yeah just go to the triceps ceremony and this is the uh page uh that share today's opening ceremony so you can just go to ceremony.etherium.org then you can join the ceremony and actually uh you just saw that these should be conducted in a sequential manner because always should do something and share and then Bob do something about something so there might be some cue but please don't lose your faith you you don't need please don't trust anyone so uh go into the queue and let's join the ceremony together um okay great uh then now I think I've explained almost every important concept of the historic then let's rebuild why this is called ZK snark using the concepts we just Explorer today okay so ZK snark is zero knowledge succinct non-interactive argument of knowledge first journals means just hiding some information you remember that I just uh one I just wanted to hide my passport number here and but doesn't want it to prove something so this is called as a general knowledge if you want to hide some value that can be called zero knowledge uh and uh to talk about succinct actually we have to talk about the soundness you remember if we have if we repeat only 10 times the proof will be pretty small but if we repeat the uh answering like 10 000 times the proof size will be larger so it's pretty important to keep the proof succinct while we keep the soundness so we need to find the gray balance there so succinct is used here for because of the soundness thing and uh the non-interactive thing we can do that at the thousand times the immigration office right so we have we should have a non-interactive system and for the non-interactive system we should do some common reference string and because of that we need to do the trusted setup stuff and all we all we we did all these things to prove argument of Knowledge from this rhino right so this is called ZK snork journalistoxin non-interactive argument of knowledge so uh does everyone understand now great I am pretty happy now yeah okay so we're gonna I'll go through the apply uh zkp stuff so where can I use zkp mostly people think like um I can hide something then uh it can be used for the Privacy definitely so uh the usages are mainly the privacy and scaling and there are a lot of undiscovered usages so let's go to the Privacy thing first um yeah and actually we already go through some difficult Concepts like multiple computation homomorphic hiding and logarithm um discrete assumption stuff so Let Me Assume again that our kids already knows this function and Mercury proof uh and please let's remind how the ZK proving system works here again we have a circuit that represents the relations between witness including the public inputs and private inputs and the proverb creates a zkp and the verifier will verify the proof using the circuit together and in the miracle tree what we want to do here is proving that there is this a leaf in the Merkel tree uh and without revealing any information about the leaf and The Sibling information which can reveal the path of the leaf which can be kind of a reference uh and here I'm gonna share the Mercury information between the both both the verifier and the program so this is where where do we use this thing at the immigration office yeah so just to prove I'm a member of this group but I don't want to reveal my identity here so this is the reason why we do this is marker proof using zkp so actually we can compute the Merkle root using sibling values and The Sibling values also should be private input because if they are revealed then they can be some hint for about the leaf and then uh to generate the multiple proof we need to compute the intermediate nodes here right you need to compute the branch node of the marker tree when you compute the Mercury proof and these intermediate values are already witnessed actually um witness also include the private and public all the values but uh I'm gonna say this is an witness and also this is the relation of the witness so the first let's see the first relation node one is Hash of sibling one and leaf one and node two is Hash of the Note 1 and sibling two node three node three is the hash of node two and sibling three and finally the computed node 3 should equal to the root value here this is a relation that we want to prove using the witness while we are not revealing the private information here Okay so yeah we just put these values like this so there is a circle circuit uh the logic is a blue color and the private input or the green color and public inputs are the red color here then we can generate ZK proof and the verifier can prove that okay you don't need to reveal the private inputs but I have the information about about this group which is the root value the public input red thing and also there is a relation logic between the witness here that is the marker proof yeah uh and this members proofing also can be used for various usages actually first of all the Privacy protocol uh definitely and for the Privacy protocol uh we have the very good example for the identity we are having the semaphore protocol which is just the name of a um membership proof protocol that keeps your identity private but lets you vote on some agenda uh in an anonymous manner and also we can't have some private transaction stuff uh Z cash and also Aztecs as xdk money and pulling a nice ball and PS team 0 Pro and for linear cash there are all the uh implementing the same logic with this members proof system and also this members proof system can be implemented in various ways definitely the first way is the method that I shared here the marker proof thing and actually recently uh people are exploring another methodology using the vector commitment which can let us express a set of values using a polynomial yeah so if you are interested in you can just uh Google this Cork and uh take a deep look at that and the um by the way we have 10 more minutes so I'm gonna use 10 more minutes all over 10 10 minutes so the next example is a scaling um yeah I think you guys are pretty um familiar with the word roll up right actually the roll-up started um I guess it started from 2018 by Barry uh the our PSA teams leader and roll up started from the ZK roll up and I'm gonna explain uh what is the basic form of zika roll up here Okay so oh my God uh yeah let's use this diagram first uh the first block is a just a normal some ethereum block let's assume that is this is a normal ethernet then there should be some transactions transaction one changes two and for each transaction every account external owned account should generate the ecdsa signature right so every transaction has its matching transaction and signature there and we finally we compute the block hash using some another values there but how if we uh make this signatures as private input what happens here we can just remove the private inputs and we can replace that using its zkp make sense this is just what the hair Roll Up is and here are two advantages what are the two advantages here yeah definitely the data used and the another one [Music] yeah computation because if we just compress all the signatures then we don't need to verify all the electric curve signatures so let's assume that if we have ten thousands of signatures then the general launch proof can be much less than the 10 000 of the signatures so we can reduce the data size a lot and also we can skip the computation just using the cryptographical uh verifying system so we have a two advantages here the scaling of the computation and the scaling of the data uses so yeah so this is the reason why we are we are using ZK for scaling Solutions uh and actually there is a tutorial that you can implement the simple ZK roller by yourself so if you want to just uh Deep dive into how it really works then you can just go there and go to the tutorial okay it'll be very helpful for you to understand how it works okay and for the next uh I'm gonna share another fun examples like Macy and rate limiting nullifier the Macy is a stat Macy stands for minimal anti-collusion infrastructure which means [Music] um actually have you tried the CLR fund before using Bitcoin phone quadratic fund it's a quarterly Fund in quadratic fund it is very useful to buy the vote because um the number of the participants is much important than the amount of diverts right in quadratic body so buying the vote is pretty useful then how can we prevent both buying attack it's like collecting all the votes first and then the coordinator makes it and generate a new state tree then the water cannot prove that I put it to this one then we can just prevent the vote buying attack so we can also use zkp here that the coordinator mixed the result without just modifying them correctly using zero knowledge probe so we are also doing the uh cr1 phone uh CLR fund for Defcon so you can go to the at columbia.co.n to participate in the new round contract funding round and the another really fun example is the rate limiting nullifier this is pretty novel and maybe it is pretty hard to think about this concept from zkp because we can just we are just thinking about like only the privacy or scaling right then let's uh see what it is so there is a polynomial a one degree polynomial so it is y equals ax plus b and actually this polynomial is a polynomial that I just chose that I just chose and actually the value p is my secret key of ethereum then okay then I can show you some point on this line but if I share you more than two points what happens you can just compute this polynomial right because you have two points and this is one degree so you can just know the A and A and B then you can know my secret key so my secrets gets revealed right so rate limiting lonely fire is using this actually this product is charmiral's secret sharing protocol rate limiting logifier is uh using this to prevent spam attack so if I want to communicate with you I should revolve my points on on the polynomial to you so if I uh if I just send you too many messages that actually you can just recover my polynomial and just get all my it from the account right but here we have to use zkp that all these shared points are on the polynomial this is the only the relation pretty simple right then we can use this for spam protection protocol we are doing a lot of experiments using this right limiting modifier concept uh for the consensus layer and also the peer-to-peer networking so okay so this is the last so from five years old key to a student uh I want to recommend this curriculum the first one is just write a ckp application using the tutorial thing I shared for the ZK roll up then it will let you it will help you understand how zkp works and how the proving system works there and then you need to study and learn about the at first abstract algebra because in the proving system we are using a specific set of numbers and we need to understand how these numbers works and how the homomorphic hiding Works to understand this actually you need to understand the abstract algebra and the group Theory thing after that please study and learn about the electric curve cryptography first and then after that please study about the pairing based cryptography then maybe some of you guys are heard about a plonk and K to 3 and inner product argument stop and they are all the uh kind of a things after you you have to you study about this pairing with cryptography and then astrology and extra so after you study these three things then please start able to punk which is arithmetization which means the converting your program into a polynomial then after then just go through the polynomial commitment schemes which is like how to make the questions and how to make the answers what we have done using the alibaba's case with the rightness so actually the arithmetizations is pretty related to the polynomial commitment scheme so you're going to study with some rncs arithmetization with growth 16 and you're going to study Planck arithmetization with kg or inner product argument Okay so uh thank you everyone so I'm once up uh from intern Foundation PSC team and I hope this session helped you a lot thank you so much I think it yep [Applause] [Music] thank you [Music] [Music] [Music] [Music] there is [Music] foreign [Music] [Music] [Music] [Music] [Music] [Laughter] [Music] [Music] thank you [Music] foreign [Applause] [Music] foreign [Music] foreign [Music] foreign [Music] [Applause] [Music] [Music] thank you [Music] foreign foreign [Music] foreign [Music] foreign [Music] come on [Music] [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] I am [Music] [Music] [Music] oh [Music] my God [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Applause] [Music] [Music] up up [Music] foreign [Music] [Music] [Music] 