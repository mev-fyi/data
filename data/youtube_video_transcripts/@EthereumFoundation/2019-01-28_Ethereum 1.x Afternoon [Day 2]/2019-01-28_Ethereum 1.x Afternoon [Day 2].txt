you you link which means taking storage of chain right now this seems infeasible because of the cost of core data so let's take an example layer to contract so the cost of core data is 68 gasp per byte which means 2,176 gasp per word of 32 bytes now let's take a side chain that can accommodate 2 million users each with a balance in two different token types these are the rough numbers that we consider a minimum viable product at 0 X this won't even take us like to a the whole world is tokenized scenario in which case we need a million tokens and a billion users anyway this will already bring you to a balanced treat it has 32 layers in it so a miracle proof in this balance tree is 30 true words which means 70,000 gasp of call data verifying it is super cheap like it's a couple of showers you do a couple of cold data loads it's only 2k gasps to verify this proof but compare this to doing a storage update of a same single balance that's only 5k gasps so we're talking about a more than tenfold more expensive solution to bring it off chain this makes this entirely unfeasible right now this makes us right now use storage for balances instead of taking this off chain this problem actually gets worse once you look at more sophisticated solutions starts right now are limited by call data you can verify a stark and EVM fairly easily you don't even need a precompile but for that the problem is that Starks contain a lot of merkel proofs and these merkel proves right now cost millions in gas it's hard to stay under the gas limit purely with the call data so if you want Stark based layer two solutions called ADA needs to become significantly cheaper another thing is fraud proofs I know that the true bit people they have this mechanism where you can submit a fraud proof of incorrect execution and they had trouble keeping this fraud proof which contains a lot of miracle roots to the memory that was used in webassembly they had trouble guaranteeing that this fraud proof would actually stay under the gas limit and if you have a system that depends on fraud proofs and you cannot guarantee that the proof actually fits in a block your system is up for disaster like someone can do something fraudulent and you would not be able to prove this on the main net so it's it's essentially uses that way another thing is take for example the mini-me token this is a token that uses a tremendous amount of storage because it keeps all the old balances around and the main purpose of doing that is that you can do things like forking it's open at all balances or implementing folk and voting mechanisms that uses balances at a certain point there's a much more efficient solution for this where you just use a miracle proof of balance to an old block hash or to a particular block hash or statehood snapshot that you store in the contract but right now this is also infeasible simply because of the cost of merkel proofs so where does this number come from the 2176 gas per word and photonic please interrupt me if they get this wrong but the way I understand it is that it started out with a particular maximum called data block size of around 120 kilobytes then divide this by the gas limit and this will give you the minimum amount of gas required to ensure that the coal data is always under a particular threshold so basically it's starting from the what is the most adversarial scenario of call data given the gas limit and the cash price of it and how do we need to set the gas price to prevent this from happening I'm not exactly sure where this 120 gal kilobytes itself comes from this could be either like chain growth ask stuff where we don't want the chain to grow with more than a maximum of a gigabyte per day or something like that it could be a bandwidth issue where a transaction propagation grinds to a halt if we go over this limit or it could be a block propagation issue where blocks themselves don't move fast enough as soon as we increase it I'm hoping that snack can come up with a decent answer here soon of what actually breaks if we if we up this limit and that being said in practice we don't even get close to this call datablock side according to eater scan the average block size is about 20 kilobytes I assumed that this contains more than just the call data and also maybe transaction we sees but I don't know how eaters can now compute defines block size at any rate it's 20 kilobytes and it Peaks under 40 kilobytes so we're nowhere near the limit so I also want to quickly reflect on what other uses this call date I currently have so the main use of court call data is to argue for state chains it is basically proof like some signatures so meta information that the state should move from this state to the next state there's some auxiliary use cases that people have sporadically used such as proof of knowledge you can submit a hash of a document to the blockchain it gets stored in the call data and then I can always claim in court that I had this document in my possession before a certain time same is proof of publication some algorithms require data availability things where you need to prove that you actually publish certain things you can store that in call data and the other way is just simply use the etherium blockchain as your own like distributed file system I would argue that only the first is an appropriate use of coal data and the rest should refer to logs and storage instead so I would argue that we can ignore all the other care all the other use cases make it only for state transitions and therefore make it secure device safe to prune as soon as a decent amount of consensus and finality exists on a state so given all that I propose the following first of all remove the 0 nonzero distinction I didn't really get into this during this slide but basically this is a remnant of using a length encoding in the wire format which to me seems like an overemphasis in the gas model on a particular implementation and as far as I understand this run length encoding isn't even used anymore and right now people are mining aetherium addresses that have the maximum number of zeros in them scattered throughout the address which doesn't even help for run length encoding so yeah I think this can go I would also argue that we should lower it to a hundred gas per word in order to make Merkel proofs economically competitive with change storage now this is the immediate consequence that the will that the maximum call data size of a block can increase so I would I would say we can add an additional limit for valid block so block has only fell it if the combined call data size is less than X where I suggest as a starting point we make X twice what the current maximum is 256 kilobyte and then in order to make sure that chain growth and things are kept in check we aggressively prune call data under the assumption that it's only an argument for state chains that then doesn't have any further usage and that's my presentation already give some more context about the zero nine zero thing and basically the issue is that before we introduced that what happened is that people started feeling the needs to kind of really optimize the hell out of their solidity functions and instead of packing variables into 32 byte values they would spend a lot of time deciding whether it's an institution in for and into a door getting into whatever and the goal of this is to basically just make writing contracts much simpler and so you can make everything be a 32 byte field which is really much more convenient for contract programming you don't have to worry about it thank you that's a good point that's also part of the reason why my proposal moves to a gas price per word instead of per byte there's a problem is that you could even still get efficiency is by packing multiple arguments into four bite things in one word gotcha do you think that's still feasible if the gas price is a hundred gas per word because then the overhead of packing becomes more significant right yeah you'll definitely become like better but a hundred gas per word is basically three gas per byte which is in 20 X 20 X decreases significant and I'm also kind of scared of this kind of hybrid rate and limit problem issue because it changes the miners optimization problem to this weird month multi-dimensional thing that requires more complex math to figure out but I guess like the core of the proposal is probably just reducing the limit and I and aggressively pruning called data which seems like fine so if you've got 256 kilobytes per block that's about like half a terabyte of blocks a year so I think I I wanted to ask if you go back to previous slide when you were describing the users for call data so interestingly here what you said that the other usage is like for example distributed storage number four is this we were actually going on about the not using storage for like for as a contract storage is basically like a really valuable resource it's probably more valuable than the coal data because it could be accessed from the smart contracts so I would rather say if you want to distribute the storage you would should really use coal date instead of the storage exactly exactly so I would say this is actually very interesting in the context of a chain pruning so and in the state rent as well so it sort of encompasses all these things so what are the I think what we could get out of it as a recommendation for like how the storage should be used and what in which cases coal data should be used instead on an issued case in which cases a log should be used so yeah the main takeaway from this is that as much as we would love to use a state root in storage and basically run zero x at a fixed storage size of only a couple of bytes right now it's simply infeasible because of the cost of Newark approves yeah I'm a little confused about this slide too because in order for something to in order to you know a minute a minute log or go into storage it has to come in through the call data in the first place right now there is people who are just send transactions that shove stuff in to call data just for the point of having the data be there right so you know just like one brand of use case for this to give people context is let's say I wants to have a gap that is just accessible from East like in one line of lobstery code which would basically say load up the the get raw transaction of this then DHEC safai it and then execute that as JavaScript and like that would let you have gaps without having to have centralized UI or whatever it gets it's obviously a stupid workaround but there's reasons to do it so if you wants to do things like that post this change then you would have to have the transaction but then you would also have to log the data the and if you don't log the data then the transaction data just becomes like not accessible easily after a couple of weeks that might happen anyway with logs and well the point is that the guarantees for a law basic think of it as kind of like a hierarchy where transactions are very ephemeral logs are kind of medium ephemeral and or adjust forever but costs forever I think we can also have a discussion to which extent we want aetherium to be used for bulk storage as opposed to solutions like file coin I think there's a hierarchy here because like there's a difference between storing a few hundred bytes and storing a you know like a gigabyte of CAD picture files so I mean I can actually just give one motivating use case of extremely high value store use of the blockchain to store even like 50 bytes so let's say you wants to have a some kind so some a brainwallet or something hidden behind a basically a file encrypted with some password or something like that and instead of the pad instead of one password that you might forget you might want to have it say be a three a five of passwords or secure other things that you remember now you can make a three a five with secret sharing but the problem with that is that with that big with secret sharing you can only choose your first three pieces of data and your other two basically randomly generated stuff based on the first three so what you do instead is you choose all five pieces of data then you use a razor coating to extend that to five out of seven and then the sixth and the seventh are random junk when you publish those to the blockchain and now you can recover data with any three of five human memorable things plus two random things that are publicly viewable so there's a bunch of like really cool tiny things like that where like the value of storing data is extremely hyper single byte and so using the block should arguably is really optimal for it but I would also say one more comment although it might have been designed that logs should be sort of cheaper to tour the Nicolle data but the print practice the the growth of the log is already faster and growth of the Cole data so it's already like if you look at the archive note that I'm running the size of the older receipts is already larger than the size of all the blocks which answering your question it pretty much contains the code data plus some addresses so yeah I think it's like ever the intention was the current log usage tells us that it's already like so I would start pruning the logs more aggressively than the blocks already because they actually grown faster so then my guess is that actually use cases two three and four are really sporadic yes right now the logs don't really differentiate between the sporadic uses first is signaling that some token did some transfer and also that one of the big differences between sort of narrative of the cerium kind of sinking and Bitcoin is in a Bitcoin essentially there's only one way to sync the chains to do the from Genesis and because they don't have the state routes and the blocks so essentially the old it blockchain is one giant miracle tree in a serum however you have a ability to sync from the snapshot which means that it allows you to prune everything and still sort of have some kind of version of every so basically it's theoretically even if the whole network removes the first few blocks and it becomes unavailable we can kind of still go on right we can still go on producing this thing we just don't know what was in the beginning that and I think that means that tells me that the storage in so the space in the state is more precious than the state in the blocks so and it has to be and what you're saying is that it's actually looking at the core gas gas cost it's the other way around so it's actually easier to put stuff in the state rather than to put it in a code called data which should be reversed yeah that's the main point when it comes to your point on log messages so if we were to do security critical information in log messages they will get mixed in with a lot of not security critical information that we would love to prune so then the argument would be to not use not use log messages for points to entry but instead use storage directly because we do have security guarantees that storage will be maintained forward and then we can aggressively proven both call data and lock messages yeah um secondly I wanted to yeah thank you for running these numbers this is super relevant but about the like a a block size so I'd hesitate to introduce block size limit unless we want to have not just a guess limit debate but also a block size debate isn't gonna be fun right now we're nowhere near the current limit but the assumption is that if we build off chain storage we will get a significant increase in Markov all proofs that are submitted to the chain so you can expect a significant uptick in the block size and then right but if you then do this 10 times gasp increase you also have a 10 times block size increase so it already becomes 1.2 megabytes which is something worth discussing it in its own right like that bad in practice to not have the limit cuz without it like the block size without the lab it goes up to 68 thousand words which is a bit more than 2 megabytes and from there like I mean first of all that's only a factor of 8 more than this more than this limit and I think you can totally make reductions that are more modest than to 100 yes award and like second this is one like big blocks or one of those areas where if they're too big or your uncle rate goes super goes super high so there's also a natural disincentive to not make very big ones will have to discover what the uncle rate like what the I'm looking forward to number two seeing regressions on what the uncle rate contribution for block cipher block size is as opposed to other things and those can be you know ameliorated with with optimizations on block propagation just minor comment that we've observed that uncle's are smaller than regular blocks so um box size is not the impact that we think it is justifies that some reduction from the status quo is like good but the question is how big of a reduction because he still wants to kind of keep it balanced relative to other stuff and it seems like like if we wants to if our goal is to make more goal proofs be the cheaper way to build these kinds of applications then like we're actually attacking that problem from two angles one of them is making the call data each data cheaper and the other one is making storage slots more expensive I mean that is the goal it's the only option doing both yep yeah this is just to clarify this this limit on the block size if we can do without that would be better for me of course thank you [Applause] [Music] okay so I'm here about here to talk about uncle right so the previous conversation kind of flows nicely into this we've been looking at the data historically of uncle rates and then just to give some context uncle rates are still blocks that get mine and included into the chain to get some reward which is about 7/8 depending on when it gets referenced in the main chain and you also get a reward for including an uncle and so why is this important it's it can help you see what the state of the network is higher uncle weights are theoretically related to lower block times and also to a larger size but it's not necessarily the case as we've noticed in the last six months so we look at the uncle rates historically and then we find different points in time where you have interesting changes so you had some hikes in August 2016 which is the dowel or like another now another fork in November 2016 and so on and so forth but for us what's been most interesting is this drop in the last six months of the uncle rates it just seems to have been going down which is interesting because the number of transactions per blog doesn't seem to be going down so there's not necessarily a relation between the transactions the size of the block and the uncle rates and so the idea of this is to make it a little bit more interactive for you to give us also some ideas of why this might be happening and some of the implications of this so essentially this is just like a closer look in terms of the last six months and as you can see the transactions doesn't really match this slope that we see in the crease of Uncle rates and so we just thought about a number of different possible causes for why this could be happening so maybe the data size is more important than the transaction count maybe it's the number of minors or the number of nodes protocol changes or the parity and get optimizations or changes in the minors strategies maybe an increased connectivity between the minors or since there's a migration of a lot of nodes and minors into Amazon maybe that's also causing better connectivity and thus resulting in lower uncle rates and so we started testing all of these hypotheses we have collected data from eligio eats eat their skin and ether chain and we're gonna share all this data so that people can also start doing their own analysis and essentially these are the variables that we've used so they're all number there and so we started checking the number of miners which doesn't seem to have changed much in the last couple of months so if anything there's a little bit more miners so it doesn't seem to be related to the own quarries we can also see that there's a lot of centralization so if we look at the top 20 miners historically it has decreased a lot now you have like only five main pools that have most of the hashing power so that could explain also the increase in connectivity and the drop and onco rates but it's not conclusive and this are that mining pools that have been doing the most mining of uncles in the last six months so we started theorizing what could cause this maybe some collusion or selfish mining but it seems very unlikely in terms of the results that we found we did find this curious increase on number of empty blocks in the network there's been an increase of about three percent there was some articles they came out about a couple of months ago in October that was claiming there was a spy mining happening in the network but we talked about this this morning with italic and it seems that that wouldn't be possible but it does seem that there's an increase in terms of empty blocks that have been mining I've been getting mined in the network so again we tested this idea that miners would stop working on blocks as they receive the header from other miners and they and since they don't have complete information all they can do is post empty blocks but it doesn't seem to be the case we're also look we were also looking at these empty blocks and find out of the data a bit inconclusive as you might have higher block size than the uncle size or the inverse so we're a bit puzzled by why this is happening and so like you can see here you can see the size of the uncles on the first column and then the size of the blocks on the second column and you just see variation sometimes dunkels are bigger sometimes the blocks are bigger and so we started doing some analysis in terms of this data and trying to see what could be causing this behavior so yeah we also check the number of notes but it doesn't seem it might have an effect in terms of less participants in their notes or they're more connected between themselves and then decreasing the Encore rates again which seems to have been happening there's well a lot let's nodes in the network actually I heard that it was about 3000 the real number of notes that's actually writing in the network at this point so very low we did conclude that some of the protocol changes the optimizations have definitely helped in terms of the on call rates and then we don't have anything on Amazon so it's really hard to tell but this is some of the regression analysis that we started running so this is data from last year and looking at what impacts the most Yonker rate so definitely the number of transactions the hash rate the block size the number of accounts but correlation doesn't necessarily mean causation so it doesn't really matter and also the rewards and the price of ether but if you look at it in the last six months the number of transactions seems completely irrelevant it doesn't affect the yong-chol rate anymore the hash rate is even more highly related and the block size has also decreased in terms of correlation between that so we just started doing some regression analysis where all of the variables this is all the results for all the regression analysis for simple linear regression with all the different variables and these are the ones that seem to be the most related so you have hash rate average block size difficulty mining return uncle mourning reward ether price gots limit and gas usage so essentially the idea is that with all this data that we've collected we can actually build a mathematical model and start playing around with a lakh gas limit and see how it impacts the uncle rates because essentially if you have an increasing size of blogs but the uncle rates are still dropping then you could potentially increase the size of the blocks to a certain degree and so this is like building a model in the idea is that we're gonna build a model with this with multivariate regression and trying to find what's most impactful and so from all of these hypotheses we concluded that maybe the data size has something to do it's not the number of miners maybe the notes as well definitely the protocol changes and the cat and parrot the optimizations have had an impact and as well as some mining strategies that have been employed there's no selfish mining or no SPV mining as far as we can tell and there's definitely an increase of miner conductivity which yeah it definitely impacts the uncle rates but that's it I don't know if you all have any ideas or anything that comes to mind so can we 5x the gas limit or 10x we'll see depends on what we run with the model and simulations one of the things that I was expecting for after this so you mentioned the guest and parity optimizations so one of the effects of those optimizations is that it actually breaks the relationship between the block gas limit and our core rate so it actually weakens it in this is because the block propagation now happens so the block propagation between the nodes the speed is basically doesn't doesn't depend on the like how hard is to execute the block it only does the proof of work at the moment so the only points where it the the bigger blocks or more complex blocks slow it down is when the miners themself tried to ingest the block so I would say that it kind of makes sense to me and also it makes sense to me that it will become harder and harder to use the unco rate as the as a criteria for for network health so we need to either if we still want to to monitor a network help we need to find out the criterias or which may be stronger correlated to what we want to see thank you that's very interesting analysis hey good who did we want to have next Zach yeah you ready [Applause] okay yeah hey guys I want to talk about some testing criteria right a lot of this is some of this is gonna be a little redundant but I just want to make sure that we're all the same page and we have some clarification as to like what it is we're trying to do here and how we can most efficiently go about achieving these these goals that we're setting for ourselves so I'm a proponent of obviously like doing this you know you know an accurate and like deterministic manner so what we need to do is we need to like have a structure to the the way that we're we're testing and validating all of these hypotheses so obviously first we want to formulate our hypothesis and we want to figure out which problems are valid like what's worth spending our time on right and this isn't any one about any one particular project or issue that's at hand it's just in general obviously time is of the essence for us and that's the whole point of this so we need to identify potential solutions and optimizations for these problems after we establish their validity and these these the certain optimizations may not be practical like we're presenting a lot of stuff and it's it's a great idea but we're kind of trying to come up with short-term solutions so I think that we should target like you know low-hanging fruit obviously like which optimizations are the best and which are the easiest to achieve and implement in a short period of time and then just you know repeat this process going down the list right so we're gonna have these three criteria for evaluation essentially and it's nice to segment these and like break them up into group so we have like environmental factors and that's going to be you know more protocol related stuff like the state size and the hashing rate and another important thing is what can be controlled and what's uncontrollable in relation to these protocol related variables and then we have computational factors which are going to account for like the resources of a single node like the IO the NIC capabilities all that stuff because we can't control the wide area network necessarily and we can't really control a lot of aspects of the protocol peering there's a lot of things that are gonna be difficult and then there are also Network factors which would be like bandwidth and latency and packet loss the actual conditions that we experience when these nodes are communicating with one another you know within those within the network so this is kind of like a brain dump and I'm really just presenting this so we can start initiating some sort of dialogue amongst ourselves so we can formulate a more structured way of going about testing and validating this thing's so like which environmental factors we consider like how the nodes available computational or network resources how do they affect these performance like what's the relationship between pruning threshold and bandwidth like how often does a sync fail with X bandwidth and Y pruning threshold if we count for state size within protocol to account for this and what parameters would result in the most effective optimization like which tweaking and tuning which parameters and so in regard to cache size what's the minimum viable cache size all of these are things that we should go about researching so we have some assumptions these are just like my basic assumptions I came up with I wrote this last night just based on some conversations and dialogue that from yesterday so like processing the block I mean this is redundant because everybody's been talking about it like okay so I'm not really gonna go over that too much I just want to like guys know share with you what I'm thinking so first we should establish our testing objectives like identify this assumptions and formulate the hypothesis like what's the bottom line and which criteria should we absolutely evaluate and which factors have the most dramatic effects on the outputs so by identifying those factors we can understand a few key points like which optimizations should be prioritized which are the most practical which are the most immediately achievable and I think that's one of the most important and formulating a test plan I think that III created I wrote an example test plan for what we're doing right here so we can identify like the client like we want to test so I just kind of put this in and I haven't changed it me and Alexi we had a conversation and we need to identify like what is the criteria for a state size you know how would you guys to find that because we can't really do it by the actual size that it occupies on a disk right you have any thoughts on that Alexi yes because it is dependent on how the actual client store it so I suggested to just number of accounts or and a number of storage items and things like this yeah total amount of code okay so I mean this isn't something that we're gonna decide right now between the two of us but before we move on and actually start testing these things we need to have those those variables need to be defined so we know what to look for right so within our test cases that we're running we could write we write out a test series like this and we we do that on white block which I will run a demo of let me just let me just wrap up though so using our emulation platform we can acquire large data sets that are going to be highly accurate or reflective of real-world data and then using those data sets we can perform additional simulations kind of similar to what Vanessa was talking about and the output of one test series can influence our hypothesis and like bring you issues like that's just the process so but I think we should it's it's easier to just be practical and I think we should just demo white block right now so I'm gonna have Daniel come up and help me do it so I'm not like fumbling around and with keys and talking to everybody but let's see okay hello all right yes so we log on to the platform I already have a chain built out so we should just run the account info let's rebuild it while you're working on that I had a question anyway yeah so well what are your priorities and where does the state size emulation fit in among those priorities I'm gonna work with Andre yeah so we just started talking initially initiating that dialogue today so I mean that's something that will work start working on when we get back to our lab I have a few ideas but I think it'd be more relevant to discuss in terms of like after we show what we're doing here so we're building out this blockchain with using the etherium on a theorem with the goth client we're just going to indicate five nodes you can indicate the computational resources that are going to be assigned to each node and then you can identify the define the parameters of the box chain like gasps limit or difficulty or whatever you would in a genesis file and then it's all dynamically created and then our platform like provisions all of those notes sets them up with the appropriate client funds the accounts because we so when we automate transactions and all that and assigns them each to a signs of virtus inés a VLAN to each one of them so they all have their own IP now we can dynamically configure any of the links between those nodes with packet loss or bandwidth constraints latency whatever we can also add accounts or nodes after it's already been built so right here you can see like this was the initial balance there's a transaction count and all these nodes are up and running so but right now there's if you run get get info get or get stats all yeah so it's all zero right now because there's no blockchain that exists so when we do minor start it's gonna just start all of the the mining process so if we open a new tab we can show the process of generating the dag so so Joseph to answer your question I think maybe Andre can do that so can you tell us what do you think you're gonna be the plan yeah so it's actually generating the dag work well we have to figure out how what time it takes to get to a reasonable storage size and I guess they're interested in testing two modes and we have a number of like a limited number of smart contracts which we inflate or the other scenario is that we have actually just a number of dust accounts but huge number of them and then we approach the size of the actual blockchain so depending on how fast we can get to that point and then keep inflating it we have we either have to or not have to make a number of of modeling assumptions or like simplifications because on one one hand we could use the real death and parity client and so on and be as close to the real thing as possible but of course we'll have to reduce the difficulty but but still if that proves to be computationally prohibitive expand expensive then we'll have more and more modeling assumptions because at the end of the day we are not interested in any security aspect of it or like even that it's a theory or whatnot they are interested I given a certain synchro tikal then what is the breaking point of that sync protocol and then yeah so one of the issues is like how do we test effectively starting out because the state needs to be generated and before we start actually testing so what my idea was is that we create a series of images with the state at size X and then we after those who have been pre generated we can just import those states and our framework and then start testing with those so we can automate transactional activity if you want to show that and deploying smart contracts as well so for this we do what's a transactions yeah so let's show transactions so we're gonna start a stream of transactions between accounts and you indicate the transactions per second and it's not like transactions per second that are going to be processed in terms it's just how many transactions we're going to be sending in total within the network regardless of how many that's not the real TPS that everyone thinks so we're sending a hundred transactions per second with a value of 1/8 it just started so we can do get get stats passed get stats that's 10 so we can look at the past 10 blocks it's going to take a second so we're watching that so you can just dynamically observe how these how this how these are those TPS is can change or whatever values or data points you want so one thing to note is we haven't applied any sort of latency or delay or bandwidth constraints or packet loss between those nodes yet so we and in order to do that we also have a tool set for just standard network utilities like ping her iperf so we can run like ping between two nodes in the network like yeah so it's gonna ping between node 0 and node 1 so you can see at 0.05 milliseconds of latency and now we can apply a hundred milliseconds of latency to the links between those nodes and then we can observe the effects on TPS or whatever I'm just using TPS as a basic metric that we're all familiar with so yeah so we're applying a 100 milliseconds of delay and then if we run ping again should be 100 milliseconds of delay between those notes we can also apply other network configurations as well and then if you want to like run hyper for whatever we couldn't show you I think yeah I want to continue back to the question yeah thanks for the demo okay do you guys understand the does it make sense so far yeah okay it's helpful yes we want to test essentially the syncing Oh first thing and using this framework to test the like failure of the where does the failure of syncing happens and kind of get more objective picture about where you know what are the criteria that where is the breaking point yes yeah I'm clear about that my question is about alignment because I'm I think I'm pretty clear that um yeah Andre and Alexis priorities you know the yeah getting getting the data for the state size yeah increase impacts if as it keeps increasing but I want to get alignment on like where does that fit among your or white blocks priorities you know it's like number one for them I think that's pretty clear but if it's like number five on your priority you know that's something that we need to align on so that's where I'm trying to get at I think that's like are you asking me what are my personal priorities in research yeah I guess whether you or white block or again if it's like your if it's number five on your interest list yeah number one and there's ten right it'll be hard for them to get well we're gonna be working together so we're gonna be we're gonna be collaborating on on these initiatives so in terms of my priorities that's something that's very interested interesting to me but I think my priority is finding out how we can be useful and contributing so if if Alexi sees value in this then it's my highest priority that I'm that we're able to provide value and help solve problems and create solutions that's my priority is like yes I think we kind of I think what we're hoping to do is that we we are get so basically Zack would provide access to this to this platform we have to decide about like the the cloud cloud compute bills like how we're gonna pay this bills and stuff like that but I think we can figure this out and you know essentially to improve the documentation as we go because there might be some some some lobsters and documentation and eventually like the reasons why I want to dedicate Andre for some time for that so that you know I know that Zack also works and other things but this is why we're bringing Andre in in in this yes yeah so okay to do this platform after because I was working on etherium and I didn't have the tools that I needed to do a lot of the low-level research that I wanted to do so this was made primarily for aetherium and I've been working and focusing on aetherium for some time but I think we should talk about that like offline because I don't want to like I wanted to add a comment on testing sinking so you didn't include this but something you might want to think about an important factor in sinking is at least in warp sink is the snapshot is now so large that just downloading it takes hours right and you can't expect a single clients be online for that long so in the vast majority like in 60 to 80 percent of cases the person you're downloading from will actually disconnect you halfway through and so you need to then go back into pairing and find out like we now have it in place where you can actually restart a snapshot way they need to find a snapshot from this same block height this will be fixed in the next version of faster but that's how it works right now so if you want to test how it works right now pair just connecting you is an important part of it yeah so those are all features and functions that we have implemented yeah I think we're probably gonna start with the guest or at the beginning because it's probably and then move on to parody maybe by that time you've got already fast warp implemented so we can just do that right so just some things we need I think like the most like the priority is just identifying how we can effectively introduce the state within the environment so it's not occupying a an exorbitant amount of time and generating that state initially from like Genesis so but I don't think that's very very difficult problem I think that's probably something we can solve in a couple days of real I guarantee which we'll get started on immediately so but uh does anybody else have any questions I had one just just a quick one like how do you handle different clients parity versus get with versus suppose we have precompiled images that we store and then we just deploy them based on that client by indicating which client you want to deploy so what we're showing you here is like oh just like high-level like obvious Atlantic when we're testing we're not using these like build Wizards that are prompting and doing all that stuff we're actually running just scripts that we define with based off of like a Yama file or something like that and then we automate all of the tests so after you define them in like a test file like I showed we could just like run all of those tests and one of them would be like client we want a hundred notes total we want 50 of them to have guests and 50 of them to have parody after you deploy those clients we have flags like we want to run these as many transactions or we want to do this or do that and that's what we want to do for this one test case so we've done a lot of work with III search on exploring sharding and lid p2p using this framework and using those testing methodologies and it's been pretty cool experience so I would encourage you guys like check out what we've worked on before cuz we don't really do any marketing like this is okay it's just us like hanging out and talking to people so you know but I think that these are valuable tools they were useful to me so I think they would be of immense value to you guys as well and we want to work with to build these tools to make your lives easier and I want to open source all of this and that's that's a plan but right now I'm beholding to a board of directors but I don't want to say anything on the livestream but we could talk about my personal feelings and plans like afterwards well you're not as just livestream right is it live streaming okay yeah yes I love corporations and I very thankful for everything that the Board of Directors has done for me okay I'm their humble servant thank you very much like I think we're really glad to have you on board there's any other questions okay thank you [Applause] then I think we have another are you guys ready to do one and more yeah let's do another awesome presentation testing come on okay so I was going to talk about metering today but instead I wanted to show you guys what webassembly looks like zoom in somehow yes yes metering tomorrow so that people know that there there is some beauty and some sophistication to have assembly it's not just sort of you know some bytes of an EVM that hopefully there's no bugs in it this is some engineering you know project and they have some proofs of some things like most languages they have a syntax okay so this is a paper that they put out when they were first announcing the language and they have this I guess syntax theirs I'm sure people in this room know about these kinds of things there's standard ways to define languages and usually there's a syntax like we have a syntax in the English language there are grammar rules likewise they have some sort of grammar rules or syntax to build these programs with and on top of this definition so this is a definition of a syntax I don't know if they have all the rules or things slightly changed since they publish this on top of this definition they define you don't have to know what this is by the way each of these is a rule and you have to you know that if you want to implement the spec or you have to follow these all these rules so these rules are for validity everything you know that can be checked statically they wanted to write down and they hopefully they wrote most of them down I think there's some more later on about for some other things but anyway we're there's some beauty that we're defining something called the grammar or define another thing on top of this grammar for each of these grammar rules were defining these validity rules and once we define these validity rules we can prove things like that the absence of undefined behavior actually they have some undefined behavior that they identified but we we know about it and source the point is we're standing on solid ground we're not on an EVM we're on a sort of solid foundation that we can you know build on of course there are some things like floating point that we have to be careful about and some things like growing memory that we have to be careful about and anyway after they define these validity rules and they designed it in a way that you can parse through the I guess he was in contract or the web assembly module and in one pass you can you can parse it based on the grammar rules then you can check the validity while you're parsing it and you can also compile it at the same time in one pass and also you can do it in parallel for each function so it's it's an engineered sort of it's engineered it's a great engineering project I think and they have a compact representation like 80% of native code size on average so portable any you know you can compile it to different architectures and the ecosystem is growing there are some growing pains but it's growing so and there's some beauty in this these sort of definitions and there's there's execution semantics these rules you don't have to know what these are this is too much I know but each one of these is one step in in computation so there are computation steps it's sort of sophisticated they're the computation is actually rewriting rules so you have some web assembly code and then it goes through these computation steps where you rewrite where you copy and paste stuff and eventually you get to your final result and your guarantee they have some guarantees that you'll get to this final results and I think there's some beauty in and sort of defining things and having some sort of foundation and having some sort of solid proofs you know these proofs are gonna you know just like Euclid proved things like 2,500 years ago about geometry you know and those proofs are still good today there's proofs about this stuff and they're gonna be true 2500 years from now and you know so there's no obsolescence to these proofs and I guess I just wanted that's all I wanted to say questions one second Jay we'll give you the microphone just you know a layman's question basically you are using web assemblies so where assembly gives you some mechanisms and facilities for virtual machines but also LLVM can give you certain facilities as well and one kind of thing that doesn't run in browsers directly but like I guess it can be compiled into web assembly at that gives you a means to run a lot of am stuff in a browser so like have you compared using web assembly as your primary kind of tool set versus using LLVM as your primary tool set yeah this was a question I was not working on II was and when this question was answered but yes there's a there's an effort with Cardno that they want to use LLVM instead of instead of web something or they may be using both and yeah it might be a reasonable alternative I don't think they design there are you talking about the LLVM intermediate representation yeah sure it's an alternative I don't know if they have the same properties I don't know if it's designed to be you know passed you know for transportation doesn't it might not have the same properties that web assembly but certainly card ana was justified in in exploring it they change their intermediate representation every version of LVM so LVM eight or seven you know they're starting to work on eight so the their IRS is constantly changing so you have to keep up or you have to either you know move with them and they might they might introduce something strange yeah you're right that this is an alternative and yeah there are other you know the the.net CLI there's there's a few other alternatives that people might suggest and I think it's an interesting discussion to have but certainly webassembly has some properties that might be hard to beat so I don't know if I answered your question I'm just ma'am I'm just mumbling yep I might be able to yeah I might be able to answer part of that as well that's it's not an either/or distinction you can compile down to LOV Mir and then write a back-end for webassembly webassembly is the actual virtual machine as opposed to this abstract intermediary right so it's actually I mean really any modern system is gonna go through several of these layers before you get all the way down so we can absolutely do both we can also use LLVM I are down to EVM as well there's nothing stopping us from doing that and we could actually get like you know obviously wasum was designed from the ground up in a particular way which is nice but we could get a lot of these properties moving forward at the EVM prior to you Watson being implemented as well and plug it that then into the LLVM ir as well write it back in for that so we could get a lot of these benefits in the meantime and I agree I think using LLVM is a good strategy so I had a subjective question a little bit they're about to clean up the food and drink go get it if you want it so basically ignoring some cost and all that stuff that you've already invested into would you choose it again today and B do you like it the like wasn't is it nice yes guilty on all counts yes I wanted to add more to the decision for using webassembly so LCM is designed around two assumptions the first one is that this intermediate representation is every mirror or they only exist during the duration of compilation you control it away and redo it the second is that you trust it web assembly is designed on the principles that you don't trust the byte code that goes in it and this needs to be standardized and the reason is that web assembly is meant for the web so you don't trust the code it goes in it so the whole security design of rep assembly is way more geared towards adversarial behavior than L liam is LVM will have a number of exploited aren't discovered because no one is testing it in this field and I think this is actually the most compelling reason to go for webassembly [Applause] check-check last chance for food anybody else and drinks as well are we done with programs you 