foreign [Music] [Music] ha ha iled [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] thank you foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] laughs [Music] foreign [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] [Music] [Music] foreign foreign [Music] [Music] [Music] [Music] foreign [Music] foreign thank you [Music] foreign [Music] foreign foreign foreign foreign foreign foreign [Music] foreign [Music] foreign foreign [Music] thank you [Music] foreign foreign foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] thank you foreign thank you [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] DM hope you've been having a great Defcon conference until now our first speaker is a little bit delayed so we're really sorry about this please stay we're going to continue with the regular schedule so please be patient and we're going to start in a couple of more minutes and nothing welcome enjoy foreign [Music] [Music] foreign thank you [Music] foreign foreign [Music] [Music] thank you [Music] thank you foreign well we are ready to go please welcome our first Speaker here in the flowers stage Odysseus with lessons from The Nomad hack hello um I'm super happy to be here or not so happy I would prefer to talk about something else but you know we do what we can right so um first we have to start with the basics I'm not here on behalf of illusory which is the company behind Nomad so the views Express are my own and do not reflect those of the company as it's an active uh investigation right now an active incident we will be not doing any q a unfortunately um my name is Odysseus I'm a protocol engineer in Nomad I've worked in web free as a consultant and also been working as a devops in iot um so today we'll be talking about Nomad what is Nomad how the protocol works we will need that to be able to talk about bridges how they work on the incident finally what are the learnings we have what did we learn from losing about having a hug has resulted in a 190 million dollars in tokens being evaporated Nomad is not a bridge Nomad has a bridge right but if no one is not a bridge and has a bridge Nomad is an optimistic protocol first of all which means that it has an optimistic security model it's a it's a protocol for interpretability which means that it allows applications to be able to meaningfully react to an event that happens in another blockchain right meaningfully that's super important we don't Define how uh your application will react to some event basically the normal protocol will just send arbitrary bytes from one domain to another so it's you the developer to interpret those bytes so Nomad is an optimistic protocol for interpretability that supports arbitrary messages between domains how Nomad works the first thing you have to know about Nomad and probably the last um it's that it's super simple on the sending chain all the messages that are being sent they're added to a metal tree right so and why we do that is because it's very easy with a medical tree to prove whether a methods belongs to the tree or not the information of inclusion in theory and in practice is included he's compressed into the root so the protocol only it only has to do is to send that route from the sending chain the receiving chain if we do that securely then anyone can go to the receiving chain and prove that the methods indeed belongs to the tree that has that root so all that the protocol does really is that's passing this route from the sending name domain to the receiving domain domain is a chain right let's see the life cycle of a massaging nomad first we go to the home contract uh if you see here the home contract on the sending chain and we send the message right then a new route is generated then that route is relayed to the receiving chain and you will find itself in a contract called replica then we must wait for them to be optimistic window to pass afterwards we can go to replica to the receiving domain and say hey here's the other proof of inclusion here is the message I want to prove that this message was indeed sent and after we prove a message we can process it and we will process a message basically uh the replica contract will take the message metadata hold the contract uh designation contract and pass the message payload super simple now that we have in a very basic sense how understand how Nomad Works let's talk about bridges what's a bridge a bridge is a super simple application built on top of any interoperability protocol basically you go to the sending chain right let's say ethereum you go to the contract and say hey bridge I want to send my native tokens with for example it's a native token to ethereum it has intrinsic value and I want to send it to Paul to evmos right then the bridge will hold that with and it will send a message to the receiving chain to the uh Nomad Bridge or the receiving end and say hey you should mint you should create a new mod with a representation token which doesn't have any any value in itself the value is derived from the fact that we can do the opposite but the user that holds the mod with can go to the bridge and say Hey I want to burn that token and I want to transfer my mod with back into ethereum so the bridge will send a message back and say hey unlock and your wave a hug is when the all this locked collateral in the bridge is stolen right so now all these representations that are flying around they're essentially worthless because they can't be redeemed for the original asset for the asset has intrinsic value and I think that's why we have we're seeing so many Hudson Bridges not only because they are indeed complex systems and they are but because also they have so much collateral locked inside them they're very juicy targets they make good targets for hikers to pry and test for the vulnerabilities let's talk about the incident what how was the name of the bridge possible to be hacked we'll talk about two mappings in the replica contract two pumpings was all they took the first mapping connects a route to the timestamp that says that after the time stop that route is indeed valid you can start proving messages against the root the root of the Merkel tree right so a new route comes and the new timestamp is generated so when now I can go to the replica and if the time time has passed I can say hey here's the message here's the proof I want to prove it against a valid root and if that happens then the message is proven now the message and uh when proving a message you connect the uh the message which about 32 the route which was proven under solidity and the default values of my opinions all the default value for byte72 is zero and for a number is zero as well so in a way if you look at the second mapping here in a way all messages are proven under the zero root proven but of course in the code we uh you know we we set an authentication flow let's say that of course this is not a valid uh of course this is zero and if the uh the time step is zero then it's not valid because also the number zero is the default value the problem is that when we deploy the contract we set that to one non-zero value so what this results is that the root zero is active After Time step one and all messages are proven under the root zero so all messages are proven against an active truth so what users did they forged messages that were meant to the bridge but all the bridge hay and all that collateral and see any arbitrary methods that has not already been proven is proven under the root zero it could prove and process whatever they wanted and that's right there 190 million bug so why did it come up now right the normal protocol has been active for months we had an upgrade and during the upgrade we changed the semantics of the second mapping so we used to store an enum here at numbers numbers so if the number is one it's proven if number one is two is processed so we didn't connect messages to the roots under the under which they are proven right so even if the route was active uh we you know through the authentication flow we made sure that we didn't authenticate that but we changed the semantics and that is why the bug was so hard to find our testing didn't find it or how the doors didn't find it because it needed the old state with a new code the old state with old code secure new state with new code secure all state with new code not secure Nomad now the protocol is posed will restart the bridge that's the hard part how do we restart an uncollateralized Bridge uh we'll be sharing more information soon the tldr is that users will be able to access to collect some of the recovered funds as the recovered funds and continuously flow into the bridge and they will do so fairly uh you'll be able to read more about it in our coming weeks in our blog posts and Twitter accounts so what did we learn what did you learn from this um I like history Bismarck says that stupid people learn from their own mistakes wise people learn from the mistakes of others so be wise we'll talk about not improvising that and overcome but we'll talk about test observe engage and communicate you can think about as different layers in the defenses of a castle right hopefully these layers of defense will stop the attackers from reaching the Citadel the king or queen it's like the treats his analogy uh security I'm sure most of your you researchers will know about that security um analogy but I think castles are way cooler than cheese that's why I prefer to do castles yes the bread and butter of every developer right um there are although there weren't any best practices I think the industry is now uh slowly aligning on these um on some you know best practice on this so we're having the unit tests property-based tests integration tests working tests and Environ tests and I want to go a quick rundown through them first of all concrete test super simple I want to make sure that the function if I give it five I will get 25 we also name it concrete function concrete tests property-based tests are more advanced they force you to think about the properties of your code so basically to verify that this function will give the input multiplied by five always then we have the integration test where we want to test bigger picture uh features user flows um yeah working test which is like a web free specialty all the other tests you can find them in other paradigms a forking test basically could be an integration test or a unit test but it would test against the on-chain state and this is very important because as you saw a bug can Surface itself only uh using the Unchained state and finally warranties my personal favorite are these um equations phrases that should always hold for your protocol right if that phrase doesn't hold at some point the protocol is should be post the protocol has broken in some way so this is a big project uh you'll do it in two phases first phase you will Define the invariance so it's a very theoretical phase not easy definitely and then it's testing the variance using any tools there are a lot of tools out there for example in Nomad the environment that broke was that all messages that are processed right received must have been sent people were processing messages that were never sent this environment broke and finally I would like to mention static analysis tools this could be very useful especially for neural developers as they can find some simple vulnerabilities and storage layout analysis basically you can use tools to verify that the storage layout of your upgradable contracts will not change uh Without You noticing and this is a very common source of vulnerabilities in my view you should prioritize unit test obviously a property based test um on forking tests this should be your primary focus with tools like Foundry boundary uh it's very I think it's easier than it used to be of course you should always audit you know use a do an audit although it's not a silver bullet so don't just do audits and of course always take the source layout always uh observe the second pillar now we have tested alerting if you receive a hero passage and you aren't already up it's too late that your alerting has failed you shouldn't wait from a certain Twitter account to tell you that there's a problem with your protocol this is a solved problem in web 2 web 3 would like to invent things again and again but it's a salt problem in wealth two so go and read the Google series handbook you'll get a ton of input also talk with your devops engineers if they have worked in web 2 before they will have a lot of insights for you uh usually start with an object uh you know the business objective and then Define actionable alerts actionable that's the key word actionable that means that even alert a is activated then you should do B should be a very simple if that if then uh you know close you should have a playbook for every alert so if alert a happens you should do that and that's the way you should do it and here's the script you should run super simple if you don't do that you will get a lot fatigue because you'll start not paying attention to alerts because you know it's not that important and by not paying attention you will miss that alert that will erect your protocol um a nice mental model for alerts in web free I think is uh heuristics and environment-based alerts characteristics are rules uh they regard human intervention to make to understand whether that's a false positive or not uh environment-based alerts where you have an off-chain agent running and continuously taking the variance of the protocol more complex but can be automated because invariants shouldn't produce false positive like if the environment alert is on that means that either you didn't Define the environment properly or be protocol is um program another way probably it's a good thing to pause the protocol now engage testing has failed alerting maybe so now we're engaging we're in the first minutes of the of the incident what do we do uh hello who's told us that would not rise to the level of our expectations we fall to the level for training so if you don't have don't have an incident um Playbook if you haven't gone through that you will not be able to uh you know act appropriately and you will get wrecked even if your alerting was good so a good uh uh Incident Management means explicit ownership very specific persons should have very specific ownership of the management of the incident outcomes every person should know what should be the outcome of their work during an incident Game Day game day game day do that again and again go through simulations the entire organization should create game days for this incident because they will happen they will happen and you shouldn't be uh you know during the incident you shouldn't read the incident playbook for the first time uh yearn internally has a very nice blog post about it I highly suggest you look at it and you adapt it to your organization within an engaging proper in proper time we lost money right so it's now time to face the music what do we do I'm sure everyone would like yeah let's talk to the users you know we have to be transpired we have to tell them what happened we have to be no uh tell them before they read it on wrecked no nope you shouldn't talk to anyone to talk to your legal team you don't do a commit you don't do a tweet you don't to your mother if you're done with your lawyers you should think about what the users want but you should talk with your lawyers first when you do communicate you should be honest no sugar coating you know you just insult them you should have pre-approved messaging because that's easier like during a crisis after the first hours you're shaking you haven't slept you're high on caffeine you can't properly create a communication or PR output you should tell them what you do what are you doing right now and what you plan to do let's see a quick timeline of the first days after an incident first of all we talk with our lawyers and we form them of the situation so they can start talking with uh law enforcement agencies for asset recovery we do the first batch of Public Communication we tell them what happened um what do we want to do what are we planning to do we do we talk with our partners ideally through more privileged communication group like telegram maybe we can share it more than we share with the public because of ndas and all that uh because as you are losing money they are losing as well then you should talk with a chain analytics firm that is very important I will tell you why because apparently people they suddenly have a change of heart when legal enforcement agencies are zeroing in on your the real identities right suddenly they just want to return their funds back and the only reason uh and the only way for uh to recover assets is using a chain analytics firm as we go on and analyze all these tornado transactions and find correlations and they would be able to point uh the law enforcement agencies to centralizing changes so they can freeze funds and requests data personal data so if you want to recover your assets you will need a chain analytics firm then you will set up a recovery address where you can you will start collect the funds and align with your team on the Bounty and of course the bank has many legal externalities so again your lawyers will be your best friends um we will not do any q a so we have time for one question so we will not do it in Q a no we don't unfortunately we don't have time for the Q a but if someone has one question we have time we will not do any q a with what we will not do okay okay so no q a sorry excuse me thank you so much this is for your time for your presentation [Applause] thank you all right [Music] [Music] foreign foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] all right [Music] it's time for our next talk decentralizing inferior with eg Galliano and Tim Myers please give give them a big Applause [Applause] all right here we go hi everybody um um my name is egalano I'm one of the co-founders and the general manager for infuri and I'm joined today with my colleague Tim Myers one of our lead engineers at inferior so you've seen the title of the talk you might have caught our announcement at East Berlin a month ago we're here to talk about decentralizing inferior it's something that's been on the Twitter waves and crypto Twitter for years something that people ask us about all the time like is inferior centralized how are you going to decentralize are you going to decentralize this is this is the talk where we answer quite a few of those questions for you we gave you a preview at East Berlin if you're able to see the recording from that talk this is going to go more into the details of the why behind it the why now what now and especially the technical details that go into decentralizing inferior foreign was launched in 2016 with the sole purpose of providing utility for ethereum and ipfs Developers it's something I say in all of my talks because I I want to make it very clear that inferior was always about the developers and making sure that developers that are building on web3 which for us was initially ethereum and ipfs could quickly get started and we did that at Devcon 2. uh it was four of us from the inferior team we flew to Devcon at the time we were a small team within the larger consensus organization focused on trying to build something that would be useful and so we all went to this conference and we were saying that we were going to get up on that stage and say there's a lot of innovation that's happening in this space we've created this new service called inferior and it's a traditional SAS service that you can use to connect to blockchains and we had a lot of apprehension about that like what would the reception to that be because at the time we were all running our own infrastructure we were all running our own nodes there was very much a lot of interest in like clients verifiability of data and we were saying don't run your own infrastructure trust us to run your infrastructure and so the the reception at Devcon was a little mixed some people were like okay great I'm going to use that other people said but why would I want to do that I can just run my own no then it's going to sink in an hour and then I don't need you anymore and uh so we said well that might be the case but we're going to try it like we're just here to try to provide something useful and maybe it will be useful and maybe it won't be and uh so we went up on stage and we we announced that we were going to be doing this and then during Defcon 2 the ethereum network was uh subjected to the denial of service attacks that almost brought down the network and some of the many of the heroes that helped save the network from that are either in this room or definitely at this conference the people that helped mitigate the attacks on the network and inferior's role in that at the time was trying to keep our endpoint up that was already being used to serve metamask metamask had launched uh two three months beforehand and uh thousands of users were already using metamask every day to send ether do some basic interactions with some of like the early maker contracts that were out there some of the early other experiments that were out there around that 2016 time and we said well we can't do anything actively to fix that client we're not the protocol core developers the core developers are solving that problem but there was already this issue where we were in Shanghai the Wi-Fi there was awful even worse than at most conferences and nobody was able to like get access to their infrastructure to update their clients once the actual client development teams had put out these updates and so our inferior team was sitting there in the lobby of the hotel updating our endpoints so that the thousands of users that were using metamask that might have been pointing metamask at their own infrastructure could use infuria until their infrastructure came back online and so that started to highlight one of the value props that we saw which was by having this centralized point we could simplify um or try to eliminate a lot of the pain for these users by um yes we'll take on this like aspect of centralization but for the most part it'll be beneficial and we said we're going to keep doing that until it gets to the point where we feel like it's crossed a Tipping Point and we need to try to do things differently and when you look around the space there's other points of centralization that have paths to decentralization things like Fiat on-ramps and centralized exchanges and how there's a transition from those to Alternatives or decentralized truly decentralized ways of doing the same operation my favorite example is when you talk about Roll-Ups and how initially Roll-Ups start as having a single entity that's creating the Roll-Ups and the roll-up teams have a road map of how they will eventually get to being more decentralized so that's how we thought of ourselves as inferior started centralized and we'll see when we get more decentralized and we're going to have a a solution for how we do that and the reason is because we had to create quite a bit of uh proprietary infrastructure to serve the traffic that we've done over the last six seven years what we run today as inferior is far more complex than what we ran with in 2016. if uh if you had tried to replicate what we had in 2016 um maybe a month or two after we launched you could do it within a week or two now it would take you probably years to try to replicate the systems that we've had to build because between 2016 and now we went through the 2017-2018 Ico bubble the 2019 crypto winter 2020 D5 summer all of the explosion of nft growth and with every single one of those things you might have run into an inferior production incident something uh that showed Oh this didn't scale exactly right and every time it didn't scale exactly right we improved our system right and so it forced us to innovate every single time take data like back in the day we used to send requests directly to ethereum nodes and there was really no virtualization as we call it um custom services that would handle the traffic because we didn't ever want to be in a position of providing the wrong data to you because that would be really bad that'd be the end of our service if we ever provided you the wrong day that like that and especially in 2021 we ran into this issue where we tried to be very uh conservative with how often we updated our clients because uh early on the clients were a little bit uh bleeding edge and a little bit unstable the closer you tracked to the latest release and because of that users were complaining like maybe give us a little more stability less of the absolutely latest client that ended up biting Us in 2021 when we were running a client that was about six months old and a bug was triggered by by a team that I won't mention because there was an accident and they apologized to us but um it took down inferior for six hours and that was our first multi-hour outage that we have ever had and it's still our worst outage that we've ever had and so that really made us think maybe that was the Tipping Point maybe that's the point where we need to start considering Now's the Time to do things differently because for us decentralization was always the plan we said we're going to get there it's just not the first step in that plan we said that we are going to set out to prove that web 3 could be served by SAS we're going to say that we're going to run a SAS service and that's going to be beneficial for the ecosystem and it's going to bring more developers in it's going to bring in more investor interest a lot of activities going to come into this space because people will see oh I can actually make a living and support myself running a business in web 3 when at the time it wasn't very clear Which models would work and so now inferior is not the only game in town There's dozens like they can't even fit on the screen anymore and we feel like we've accomplished that goal we've proved that SAS Works in web 3 and so now we want to prove that web 3 models can be used to serve SAS and that's what we took as our approach to decentralizing inferior and that's where we are today and the first step to that is open collaboration and transparency we don't have the entire solution in the white paper form that we're going to upload to ipfs and you're all going to read it right now like we're still in that design phase but we've already started collaborating with other people in the ecosystem on that design we've been working on it for a year and a half this is where we want to start showing you more of the technical details of what goes into that what does decentralizing inferior look like it could people have said oh are you just going to create a Dao so that we can like have some governments over what inferior does is that good enough or is it some sort of uh utility function that we're going to try to have as part of this network what are the incentives of participating how do we incentivize the right behaviors and what people are especially asking was does participating in this decentralized infrastructure Network mean like inferior still owns and controls everything and that's the biggest question that we got in the last month since we announced it East Berlin and the answer to that no it that's that defeats the purpose of what we're seeing with a decentralized infrastructure Network that's supposed to be a permissionless network so inferior is going to just be a partner in a decentralized permissionless network one of an equal amount of or one of equals in operators so now I'm going to pass it over to my colleague Tim and we're going to start sharing some of the technical details with you thanks sorry uh yeah so as EG mentioned we don't have all of the details of this figured out yet we're still pretty deep in the design phase uh but the next 10 minutes I'm going to try and walk you through as much as we have figured out right now as I can and then hopefully save about five minutes for any questions you all might have uh so first I want to start with just a really high level overview of what we see as the network participants in this decentralized inferior Network so first of course you have your end user who wants to connect to a web3 network maybe they just need to send a transaction to a blockchain maybe they need to read a bunch of data from it either way they need some way to access the API that a blockchain node typically exposes and so the first participant to support that are the infrastructure providers of course and they're the ones that are going to actually be writing the blockchain nodes off the supporting infrastructure that makes that possible to allow for a high throughput of requests to send all the transactions to read historical data all of those things that you typically do with the centralized provider and they'll provide this service in return for a reward um which is what's going to motivate them to want to do this uh the next participant we're calling the network status Watchers uh this party will provide performance and capability reports of the infrastructure providers so they'll be there to kind of check and keep them honest you know they'll they'll see that they're keeping up with the head of the chain they'll check that they're you know responding quickly that their uh answers are accurate all the things that you would hope that an API does and finally we have this concept of an Ingress node they kind of act as an intermediary they will sell the actual network resources to users after purchasing them from the infrastructure providers and later I'll also show how they're not entirely necessary but they can provide a ux option that I think is useful uh so more detail on the infrastructure providers the first thing of course to note is that instead of now with just a single centralized provider uh we'll have many of them and they won't be run by a single party so here we just have a little simple diagram we have infero we'll probably run one of course uh you'll also see other named providers that you're used to seeing that we hope will collaborate with us and you might also have Anonymous providers so you never really know who they are but through the mechanisms of the network you can come to trust that they are going to provide a good service so these infrastructure providers they commit to providing capacity on the network they'll specify the protocols and capabilities that they support so we envision this network working for ethereum but we also Envision Envision it supporting all of the sort of new networks that inferior has added over the last couple of years the layer twos new layer ones but not every infrastructure provider is going to want to serve all of those so they can pick and choose and specify what they want to support they'll also say the capabilities so you might be used to having you know archive nodes for ethereum and then sometimes you need that sometimes you don't and running archive traffic is a lot harder so some providers might do that some might not they'll also give an idea of the amount of throughput that they're capable of specifying and this may take a couple different shapes but you know you can think of it as like a thousand requests per minute when someone registers as a provider they're saying I can support this much and so that people can then buy that back and of course they'll be compensated for for doing this I want to talk a little bit about what it actually looks to be a node provider and so this is a diagram of what inferior does and what other big centralized providers do it's complicated as EG said it's become much more complicated since 2016. to start of course you have your blockchain nodes these are the ones that are produced by the client teams that interact with the peer-to-peer Network that sync blocks to verify it but um there's a lot of supporting infrastructure around that so first we have a snapshot system uh if you've ever tried to sync a node you'll know that it takes a very long time so we typically take the disk of a blockchain node save it to our own private Secure Storage and then when we need to horizontally scale out those nodes we use that as a seed for new blockchain nodes so it's downloaded onto a server and then sync from there which makes it much faster you also have various indexers and accelerators so if you've ever tried to do like a big wide git logs query for events from Apache node you'll know it's really slow so we have special indexers to speed that up also for things like nfts in front of that you have a load balancer that makes the decision of what accelerators to send your requests to maybe for just sending a transaction it goes directly to the nodes and then front of all of that we often have a consistency system that helps make sure that users see a canonical view of the chain so this helps with reorgs to make those little less painful for the user and also make sure that if they like query a block it doesn't change in between requests and this is something that is altogether quite complicated there's been several uh projects out in open source land that have attempted to help with this I think most of them are you know more on just running the blockchain nodes running several of those but to really provide good service you need a lot more than that and we will be releasing our own open source infrastructure kit that node providers can use to help them participate in this network of course there's also going to be parties that already know what they're doing and we'll probably just run their own flavor of infrastructure to provide the same service in the end uh next we have the network status Watchers again there's multiple of these run by different parties these will periodically test the infrastructure providers by sending them a small volume of requests of measure the performance and check for correctness of those responses they'll report all this to a Federated status page so if you're thinking about using this network you'll be able to go there look through the different providers see how they perform what their you know capabilities are of those things and also in cases of provably incorrect responses so there are certain requests on the ethereum Json RPC API that you could prove with Merkle proofs rather easily so in those cases those might also be used to inform penalties with the node providers and we hope over time as stateless clients start to improve that there will be more and more of that API surface that is verifiable a picture of kind of what it looks like to run a network status Watcher they will have to run some small amount of blockchain infrastructure themselves not nearly on the scale that an infrastructure provider would because they're not serving a high volume of requests but they need sort of a baseline to compare to they want to check that the infrastructure providers are near what they think is the head of the chain um and then of course they'll have all these periodic tests that they're running to see if the providers are operating well so they'll interact with the infrastructure providers they'll potentially send things to a Smart contract on chain for those provably incorrect responses and then they will also send their results to a Federated status page where you can view more detailed information and then last we have the Ingress node so as I mentioned this kind of serves as an intermediary between the end user and the infrastructure providers so an end user will send their Json RPC request to an Ingress node and then that Ingress node May then forward that to multiple different infrastructure providers maybe they generally send it to one and if that goes down they have a fallback maybe they round robin maybe they choose to actually send it to all of them and do a quorum of that and return it to the user this will be something that the Ingress node can actually choose how they want to do it uh and one reason for having this Ingress node is that you can use it to give the same web to ux of the centralized providers today so I think there's a reason that inferior and centralized providers have been so successful it's provided a an easy entry point for people to come and swipe their credit card and get access to the network and that doesn't necessarily align with the decentralized ethos but it's also brought a lot more people to our our world and so an Ingress node might allow users to actually pay them in Fiat and then give them API access but this will still have better results than the current model because they can then be sending the traffic to multiple infrastructure providers and get some of those benefits uh in addition to that though an Ingress node might purchase resources from several node providers and then register themselves as a node provider uh with the sort of the you know the sum of all of that capacity and then an end user can go and pay for that provider in crypto um and then you have a fully crypto native experience so there's options here the node provider lets you do a lot of interesting things with uh combining other providers so the Unchained registry is what we're typically calling our smart contract that kind of coordinates all of this and the main way to think about it is it's basically the data structure of your node providers so we have a very simple example of what that kind of looks like this is uh just one detail but you have your name providers and the protocols that they're supporting so uh as inferior maybe we're supporting the five that are listed here but maybe operator a is only supporting Ethan polygon this is where you would also be able to see things like maybe what region they exist in um whether they support archive nodes things like that uh going to look sort of of the timeline of how this all operates so first of course the smart contract is deployed uh some amount of time later your infrastructure providers come in and they register they specify the protocols the capabilities and the throughput that they'll support uh some amount of time later you'll have Network watches that will register they may only want to participate watching for certain protocols maybe they're only capable of checking certain capabilities maybe you have a watcher that can't check that a node or that an infrastructure provider can support archive access but they want to help with you know the the near head requests and see that those are good for providers and then finally you have your consumers who come in and purchase throughput so that might be an end user who wants to go directly to a node provider or it could be an Ingress node that wants to combine them and once you put all of that together you get sort of what we view as our decentralized architecture uh so in the center you have the on-chained registry you have your network Watchers your infrastructure providers your blockchain API consumers and your Ingress nodes that all will register with that on-chain registry and then once that's all set up your blockchain API consumers can send their requests to the various infrastructure providers that they've chosen or to an Ingress node which is sort of helping them do that and yeah uh we hope once we have that all figured out that it'll give us a self-sustaining reliable and robust network of infrastructure providers uh that's built to really serve the high throughput that inferior can today or other centralized providers can today uh for all of the blockchain API requests that you you need um but instead of what we have now it'll remove a single point of failure uh we help to hope to do this out in the open as much as possible and really follow that collaborative web3 Spirit uh this is we hope really going to improve reliability for these for the users and get a lot closer to that kind of 100 uptime dream um but also which I think is important still provide the ability to have that really easy web to ux if you want it uh we could also have the crypto native uh and yeah that's it uh we're seeking strong web 3 infrastructure providers to work together with us if you're interested in joining and participating in this network I encourage you to snap this QR code it takes you to a form that you can fill out provide a little information on your experience and your interest and we'd love to work with you um that's it I've got about four and a half minutes for questions if anyone has any thank you so much um please raise your hand if you have questions so our volunteers can lend you the microphone foreign so do you plan this whole decentralization to be more created or censorship resistant sorry what was the first you said censorship yes so either you have for example a list of created providers for quality or you have a censorship persistently so providers that will treat any request from anyone regardless of definitely improved censorship censorship resistance because it's permissionless anybody can participate in the network by meeting the criteria to join the network which is technical um technical criteria of meeting a level of performance right if you can meet a level of performance you can participate in the network we're not going to be selecting who can participate and then as a consequence what if a provider constantly performed badly they would be penalized by the network right there's there would be a financial incentive to provide the minimum level of performance and uh they would get penalized by the network if they're not meeting that level of performance additionally that Network Watcher that we're in status monitor that we're talking about is continuously checking and Reporting on both the capabilities of the network as well as the performance so you can see here's a new provider that just joined they've only been part of the network for seven days they support seven networks of those seven networks this is the performance criteria of that right and all of that's going to be published in a stream of information that people can consume from to make routing decisions so a user can like a very technical user that wants to be in control of that can say this provider is coming from this region or this providers coming from that region and this is their performance and make selection criteria of how their traffic gets routed or they can pick their level of decentralization and ease of use by going through an Ingress operator that can make those routing decisions on their behalf can you um give um an example of what the centralized infrastructure currently looks like in terms of like a number of servers and bandwidth and so on and what you expect will be when it's decentralized will there be overhead will it will it need more servers to do the same performance that kind of thing you're by number of servers are you talking about in fear is current are currently yeah um dozens of subsystems hundreds of different instances or servers providing different types of functionality so like we have a separate subsystem subsystem that all it does is index event logs and when you query events from uh like erc20 token events and things like that it's pulling from that subsystem and then we have this consistency system that tries to keep all of that uh in sync when you're querying one subsystem versus another um so that's the challenge that we've had is we built that as quickly as possible to solve problems and it's very proprietary Cloud specific and we've been working on how do we retool that so that it's something that can be not just open source and something that somebody else can run but can run with like not a team of dozens of operations people and so it's not what we're going to release in the node kit is what we think is an minimum level of functionality of performance where this is better than running a bunch of nodes behind a load balancer because it now has some of these indexing and performance optimizing systems but it's something that can be taken on by hopefully an individual but um at the very least a small team much smaller team than the current inferior team do you think it's likely that this on chain registrations of the providers will be a new yet to be launched dedicated chain or is it more likely that it will be on an existing chain like this you remain at uh that's one of the things that we've been considering as we're going through the design we're leaning towards don't don't start a new chain for that like try to use something that exists and that goes back to the why are we doing it now rather than in 2016 when people are talking about like micro payments between providers and like do you use State channels for that and all these other things like we would have had to develop a significant amount of our research just on that problem whereas now a lot of these Primitives are out there and we can try to use some of the things that already exist to build the network uh yeah can you tell me who is the entity or people who decide what the requirements are to become an operator and um who decides which operators or providers would meet those requirements and then how they would change in the future okay so multi-part so who decides who can be an operator so this form to sign up is to participate in like a permission data um similarly to like how uh when other people are uh developing new networks we're going to have to start with a small group to try to test and iterate iterate with and for that we're looking for people that already have experience with the infrastructure and that we can reach out to and get feedback from um pretty quickly on the technical front but once we actually launch this network it's fully permissionless the criteria of that is what we're defining as a protocol in the design of this would be you know the amount of resources that you would have to provide or this would be the minimum level of performance so you have to at least support one network you have to at least support this level of performance and then as long as you can meet that it's um anybody hey Lefteris here from rotia right in front of you I just wanted to ask about the Ingress nodes will it be a requirement or will they be optional did you say like the indexing notes no no Ingrid of the Ingress yeah it'll definitely be optional um so I can't quite get to the slides it'll take too long uh but you can still have a consumer who will go directly to the contract to purchase resources and go directly to one or multiple node providers that they choose the Ingress node is kind of an option where if you don't want to go to the trouble of doing that maybe you want to use one that allows you to pay in Fiat that you can but it's an option is this was our last question but if you have more questions please reach out to our speakers are going to be here thank you so much [Applause] [Music] [Music] right right now [Music] please everyone grab your seats it's time for our next Talk technical details of the opt code compatible zkevm with Jordy balina and please receive him with the bigger Plus hello I'm Julie bailina I'm the technical lead at polygon Hermes and today I'm gonna present mainly the public test net that we publish it last last Monday first of all I'm going to explain a little bit how the ckvm works and then we are going to proceed to a demo how are roll up who is the KVM works from the user perspective well a user should not see any much any any difference like working with any other network what we will do is first we will send transactions to a sequencer a sequencer will already give a state that means that the state would be final as far as you try as a sequencer sequencer the first version is going to be a centralized sequencer we would decentralize later on but if you trust a sequencer this transaction is final and you have the warranty by the sequencer that this transaction is going to be mined it's going to be processed the sequencer will collect transactions and at some point we'll send these transactions to the blockchain okay and and then at this point the state is final and safe here we don't have any proof yet it's just the transactions are set and you have the warranty that those transactions are going to be processed uh in that order and because they are on chain you can they cannot they are not going to be changed so they you know that they are final and you don't need to trust a sequencer anymore you know that these transactions are final and in background in parallel it's going to be the proverb actually that's going to take all these transactions and it's going to compute it's going to prove that this implicit state so this is a state that's everybody can compute but it's not it's not Unchained because non-chain is are only the transactions the data availability if you want the transactions are going to be processed but it's going to be converted to a real to real estate and this is proved by the approver this is the big difference with the optimistic relapse in optimistic relapse you need to write for somebody to to challenge this state in the in you know in in a zigiro app this state it's become the prover just set this state and you know for sure that this that that this is valid is at this point where the user can withdraw with router funds so here the most important piece or the differential pipe is of a CK Roll Up is the Brewer the proverb is a zero knowledge proof If you want a validity proof that uh validates the transactions which transactions ethereum transactions it's taking a state is taking a set of infinity transactions and is Computing in your state and validating that this state is valid so how the proverb is built so what's what's inside the proverb or the proverb is a set of Technologies but the way we built this we have a circuit uh it's a traditional circuit written in in pill specific language we built for the zkbm that's mainly a processor it's a generic processor it's generic it's with some specifies that but it's a processor that's built with this zero knowledge uh technology and on top of this processor we are running a program we call it a ROM okay that actually is an ethereum it emulates ethereum it actually this program just is this problem is the one that is actually taking the transactions uh analyzing the transactions checking that the signatory is valid discounting the balances checking the fees deploying the smart contracts executing the smart contracts and doing exactly the same that does gaffer that doesn't interior note just processing this processing these transactions all this goes to approver and it's this provery is the one that's verified on chain if you zoom in in the processor well we have a mainly a processor is this ZK processor it's a all these processor is very tailor made for uh the ckbn so it's not like a very generic processor it's this part that's generic but there are specific pieces that are made explicitly to be optimal for running the ebm program the ckbm program okay this processor has a ram has a ROM which contains this program that's executed contains the storage because the ABM you should be able to store values and and get the values it have also uh kind of a suit processor that handles all the binaries operations here includes addition subtraction and torque xores and so on has a model that's for idiomatic this is you know in the in the in the IBM it works in 256 bits so this arithmetic circuit actually does all these operations in the 256 56 bits okay and then has uh the hashing for ketchups and other hashes that are also inside the inside the process inside this processor on top of this processor there is this ROM okay this ROM is written in assembly it's a specific assembly for this processor and here is what contains that this contains all the logic actually what contains all the all the ethereum logic that when we are processing here here I want to show you just a snippet of code of how this looks like this is just for example the the cops two dupe one and two but here we have all the opcodes we have implemented all the ethereum of codes uh at this point and then once we run this then we need like the cryptographic prover the cryptographic proverb uh what we do is mainly we're using Starks with a a very optimal way to compute proving systems we are using Goldilocks it's a technology for from people from for our colleagues in uh polygon 0 that makes to build these proofs really fast and it's again for recursion so it's a proverb can aggregate many proofs and at the end this is a stark and at the end what's doing is we are converting verifying is a stark with a snark so at the end we just in ethereum we are just verifying a normal growth 16 or block uh proof it's just a circle it's a circum circuit in the last in the bottom of that piece so this is the stack the cryptographic stack for verifying that okay so let's cross fingers and let's try to see the demo let's see if it works if it doesn't work you can try it okay you can go to public.zkvm.test.net and test it you will see that's very simple the demo that I'm gonna do is first of all I'm gonna Bridge it let me just uh switch okay so the first thing I don't want to do is I'm gonna I'm gonna Bridge uh so I mean go early uh I'm going to early Network I have an account here that's uh has three go early already and it's just a strike new account that I just created before this so the first thing that I'm going to do is I'm going to transfer I'm going to bridge three ethers uh three uh well 0.25 ethers this is the maximum that we allow here in this test net just to protect some Daniel of service attacks here uh and we are just to transfer to the uh to the layer to the layered tool so I mean go early right now so I'm just uh like let's see if ethereum works okay here we are let's Bridge it film okay so let's sign let me just uh let me just let me just modify the gas fee so that it takes uh it goes faster let's go early so you never know save okay and then just send this transaction okay foreign so we just deposit the transaction this transaction is mining early we need to White also a little bit so that this transaction is kind of a final so that this transaction is included okay right now it's this is already so it's this is already done what we have done is we just put this transaction in America tree and then the root of this Mercury of all deposits is passed uh is passed as the as the state of the of the Roll Up the sequencer actually there has not been any trans any special transaction on chain because the sequencer already takes in account that okay so now let's uh finalize this okay and this finalize what is doing is collecting is doing an L2 transaction to collect these uh for these phones this ethereum in the in the layer two so here when I push here the first thing that asked me is ask me is just to switch the network so I'm just switch the network okay I just know better matches disconnected I think I just now I'm just signing the transaction in in and then just just signing the transaction in Layer Two okay because I just signed into transaction let's see if it works there we go foreign so now let's now let's go to do something with this account okay let's create a smart contract and deploy it in this layer tool so let's go to remix just gonna use uh an example very simple smart contract [Music] oh God this looks like remix is gone so let's delete this little world yes it's just a very example smart contract so let me compile let me compile this smart contract there is and now I'm going to deploy it to the layer two so I'm going to connect this to um metamask okay I'm just using this account and then I'm just uh well this is I don't know if I'm connected to the last one well just connect here this is the one that I just did I have the 25 feature here so and let's deploy this okay so just deploy this a smart contract it asked me to sign I just confirm this transaction see if it goes its internet is not like the fastest thing here is okay so now here we have the smart contract if we do a get a it's we get a zero but if we for example we set a 22 so this will generate a transaction to set this state variable to 22 we sign it confirm save case and then just get the value and I see here 22 right here okay so I just deployed that in Layer Two like any other network here okay so um what's going on in here so let's let's see let's see the let's go to the let's go to the uh to the uh this is the roll up smart contract in the layer one okay and here we see two kind of transactions one are the sequence uh batches like here where here you can see in the data that this is here all the all the transactions that we are setting okay but in parallel but in parallel to this we have the the batch the batches is the proverb that's generating batches that's actually validating these transactions and here is where all the magic happens okay here in the blockchain you cannot see much but here is the proof actually this is the row 16 proof in this case that validates all the transactions that we have been processing processing ok so until here is well I can maybe I can give you a bonus bonus track here so we can go for example to UNI swap okay you need this y PC is already deployed assist without compelling and recompiling anything it's just a normal unit swap it's deployed in Layer Two and here we can do for example a transfer let me just just change the account to the first one so that I have some some tokens to to change okay and here I want to compare from Full who are just a normal swap okay it's fetching the price I'm just doing the swap firm swap why should sign the transaction so here we have deploy it so we here we have already deployed the full uni swap but version three in the layer two and all this is verified in the approver just confirm that okay now this is confirmed no well here the the unit swap interface has some uh it's iterating takes maybe some 10 seconds or so to to realize that the transaction has been moved but that's mainly the thing okay so let me go well let me see if it finished this is the here is okay so I already do the swap so I can just checking in swap just like normal like any other uh ethereum transaction so let me switch back to the presentation so nothing fancy right that's the cool thing and that's like strange things even for me explaining for me here I'm just feeling that I did a demo of ethereum but this is the interesting thing it's that the all this is running all this is validated in the program all these transactions that are really complex are all the unit swap transactions and everything this is validated uh inside the broker and this is what this zkbm and the main importance of this design you don't have to recompile anything you just take the code exactly the same code you don't have to reality anything you don't have to learn anything you can use exactly the same tooling you can use the same the same language the same gas model it's no difference for developers they should not notice any difference in deploying in working with ethereum or in uh or in zkbm the only difference should be the gas price and the quantity of transactions that you should be able to deploy foreign please test it we have been running for this week already we have more than 1 000 accounts we have most of them are just deploying uh transactions some of the projects already tested uh already tested without uh any big issue we have some reports of some bugs that we haven't fixed also and we will continue work it's this testnet is a little bit like a baby it was worn uh last Monday but it's going to get stronger and this is the the previous stage just to the to the to the main net okay here is uh okay and what's the limit of the scaling this we don't know yet what square is not gonna it's not gonna be in the proverb it's going to be maybe in the data availability it's going to be in the maybe in the sequencer in the other pieces but because why because the prover can be paralyzed and here the important part actually for example we are running uh seven provers at this point because this week you know there is main transactions some of the Brewers have been stopped so we are just just trying to catch up with the trend with some of these transactions but this is the cool thing we can run as many progress as we want so if there is a lot of demand we're just running more Proverbs and the only thing you need to take in account is what's a cost per transaction because running these servers are not free so what's what's this cost well the costs right now is less than one cent per transaction and there is this is in AWS cost which is probably the the most expensive cloud service in the in the in this in in the world and there is also a lot of optimizations that are coming they are in GPU we can we believe that we can improve one order of magnitude and there is other improvements that we are working on that but Brewer is not the is not the the bottleneck anymore what's missing well not much so we are fully compatible we are running all the all the all the off codes everything works as as ethereum is there are some things that we are already implementing they are not implemented yet and I'm just listening here but everything is there what is missing is that AIP is the original ethereum transactions this is mainly to deploy mainly nursesafe but it's uh just for for distance these smart contracts that have the same address in many in in in in many chains they use they're using these primitive transactions that they don't include the chain AV we are implementing those and then the we are not supporting yet the shadow 56 the Blake and the paintings pre-compile to Smart contracts but this is a work in progress all of them are doable and we will we will work in the upcoming on those on in the upcoming in the coming months there is the Audi Title One to enter in detail in the audit but this is probably the the most challenging part that we're facing right now there is a full plan for running this audits if anyone interested in that please contact me what else are we working on we are working also in aggregating of the roof right now the aggregators right now we are running one one proof per batch okay but we have to uh run one so we need to aggregate all these proofs in a single transaction and and with this in a single proof actually we are working in this proof this is not that much because we have all the recursion I've already done and it's just putting them together that's a piece that we need to put there and we're also working with AAP 4844 for line shelving this is uh clearly the future for the scalability and we already uh working uh on that we are very excited on that I I have to recognize that when I read this EIP I was very acceptable at the beginning but uh it's really the way to go we we can Implement insight and it really go will go even faster and it's even better that what I would what I would do it's just that you the only thing is that you need to go a little bit deep to understand this VIP that's really interesting and very excited for the scaling testing we're running the the ethereum test Suites right now we are at 97.7 97 of uh passing all these tests there are edge cases that we are still uh working on that but I'm sure that uh very soon we will be covering the will be in 100 of the of the ethereum of ethereum test broad map of course we just launched the public test net uh we need to audit and we will launch when it's ready here is uh we want we want to be enough safe we want enough sure that so 100 safe is going to be impossible but we want we want to be responsible and we want to update this very well and when we feel comfortable then we will we will we will launch a reminder everything is open source you can take a look you can see you can you can review everything is uh in the GitHub repositories and yeah gkvm is no longer a myth care about this again thank you so much uh we have a couple of minutes for uh questions so if you have a question please raise your hand foreign [Music] is not a frozen object it's leaving and it's evolving with new eips and we are going to see possibly big change like the eof or other things like that and they will be way easier to implement for core devs and for you so are you worried that some possible living change will be hard to translate into circuits in the future uh we need to see which are those changes and once I see the changes I will tell you but there is one thing that because this is this is some of the the questions that I received now it is sometimes it's more about the upgradability of uh this what happened if the ckvm upgrade and then what happened with the roll up it will upgrade and if you want a decentralized system and but here is uh we need to understand as a community that uh the ABM at this point is evolving because it's work in progress but at some point this will this this will need to be frozen uh I don't know when and how but and I'm not talking about the zkbm I'm talking about the AVM so the ABM if you want uh I believe I I I I believe I hope and he's talking with people with the effort they already think very much on that time is that at some point maybe in two years three years five years once the ethereum is finished then the ebm will be Frozen and then it will be also the time to throw to Fritz the the roll-ups thank you for the presentation great talk I would like I mean could you detail a little bit what are the edge cases that you are meeting regarding the failing tests uh here we have uh Carlos in the room that's responsible for testing you can ask them but they're very very they are very very um complex one it's a Carl of a static call and then do a surface truck and and do whatever you know this is our very very edge cases uh uh very very edge cases testing but you have to take you have to look at it because you never know one and it's important that's why those tests are are there but are really really edge cases at this point and complex ones hi thank you for the presentation uh when you were showing the uh on either scan uh the contents of the badge you showed like that there was like you know uh the code like encoded a set of transactions is there any way to decode them uh to to see what exactly is in the batch yeah it's mainly mainly other transactions uh one after the other uh um we have some internal tooling just to to take a look on that well actually they are open source you can check the repositories but we are building it and in any case it would not be difficult to interpret again it's just a format with with the array of transactions I think we have time for one more question so if you can lend the microphone to our speaker ah cool go ahead hey thank you can you hear me yep okay um first of all congratulations this is amazing um when you launch let's suppose it's early next year obviously it's a very complicated system if you find some bug or something that needs to be addressed what can you do in that situation with the strapping decentralized systems is not uh is not an easy topic uh here we have the experience of Hermes 1.0 in our team and here you do some I would say nasty tricks uh here is maybe you do an upgradability things that you we can you can do is an upgrade that so you can upgrade the smart contracts with maybe with a time lock and you can not do for example is limit the the withdrawal the the withdrawal flow that's leaving the smart contract so that it's like if you are working with less as a certain amount you are freely decentralized but if you want to run fast and run bigger numbers then you can you have the option to go Central lights or just or or white and you you there are some tricks for Buddha strapping but this is our just temporary uh Solutions until we feel comfortable at some point the roll app should go along we are building in the centralized system and and it should be safe enough here is so we are managing that in the in the in the in the beginning thank you so much Jersey valina please give him any thoughts [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] guys [Music] everyone welcome to the flower stage it's time for our next speaker Harry kaliner with on the path to a role of centric future please receive him with the bigger Plus thank you all right thank you it is very exciting to be here Defcon is just such amazing event and you know the energy here it's incredible and just excited to be able to talk to you guys today um so yeah I'm Harry I am the CTO and one of the co-founders of octane Labs the company that built Arboretum and I'm here today I'm going to kind of walk you through a bit of the history of arbitrum and how we got here um where we are today and then talk some about sort of where we're heading and what the future looks like so arbitrim has actually originally came up with really cool history in 2014 actually before ethereum had even launched one of our co-founders Ed had the idea of hey this thing is really cool but it doesn't seem like it's going to scale um now at the time it was very early and the idea ended up sort of you know forgotten to history for a little while until 2017 we started working on it um at the time we were at Princeton University academics thought hey this is really cool and interesting and it seems like there might be something that'll that'll really grow here and we started out we wrote a research paper back in 2018 it's crazy because it feels like yesterday to me but it's almost ancient history um in uh in crypto time um and then uh managed to somehow uh and I still not completely sure how um to found a company around it um and share it with the community so at that point we had this idea we were building um back in February 2020 um just around East Denver we actually had the first Arbors from testnet um it was a lot different than it is now um actually we started with um technology that looks a lot like it looks more like an included features um like our Nova chain has today which I'll mention a little bit but it was not a roll-up it had a committee um it had sort of off-chain agreement uh but it had a lot of features like it has now and doing application specific change so interestingly sort of a lot of where we are now it was too early then um but uh there was sort of a lot of ideas that have been bouncing around that I think are now sort of coming out as uh really popular um and then we went from there we launched test Nets we figured out arbitrary contract deployment we figured out arbitrary messaging um we uh actually got our main net out and this was back in May of last year we we launched our main net to developers only we had a few months there and then in August so over a year ago now we launched on mainnet and that is how we we ended up you know building arbitrum one getting it out there it's been an incredible time since then but the building never stops um you know if you if you stop in crypto you might as well you know you're done um nothing is ever sort of you know research always continues no technology today will be sort of it'll all look ancient in five years um and kind of arbitrary when we launched already looks ancient compared to arbitrum where it is today um because this last year we worked on our Nitra upgrade which I'm going to be talking about some today we launched our Nova chain which as I said sort of actually takes a lot of ideas from our original paper um and sort of the world is now I think in a position where they make a lot more sense than they did then um we went through a really sort of challenging and interesting thing which was the live upgrade of our arbitrim chain to an entirely new technology stack um and it was really interesting to us we did it kind of a couple weeks before the merge um and it was essentially our merch um and that certainly not not you know the same amount of coordination necessary but the idea of kind of taking a technology stack and somehow well with a running system actually kind of replace the technology underneath it with a newer version with a more powerful version that allowed it to be cheaper faster all sorts of good stuff um which has made it for a a really exciting uh year for us um so Arboretum one um just to kind of jump into like well what is this thing I've been talking about I walk you through the history but you know it's nice to sort of dig in a bit more talk high level um what does arbitrim provide why should you care hopefully most people here already know um but it's certainly nice to say um low-cost transactions um security rooted in ethereum so inheriting security rather than trying to have independent security which is really powerful which is what Roll-Ups give us and really really really full compatibility with ethereum which is the thing that only optimistic roll-ups at least today can provide and means that sort of tooling languages everything just works and it's been really huge for developer adoption it's meant that the learning curve and the difficulty of playing on arbitrim has been incredibly low and it's how we've been able to get so many people involved and have so many people benefit from the technology that we're building um so just a couple stats and this is actually slightly out of date because the market is down and this slide was made before the um so I think it's more 2 billion plus now although the amount of value in East I think has increased um so you know from it depends how you look at it but arbitrim is going strong we have a huge amount of adoption huge amount of projects huge amount of users um and it's just been sort of incredibly thrilling to watch it feels as though it's been like forever now but it was only a little over a year ago that this all became possible um we have a huge ecosystem we have kind of native apps we have ethereum daps that are kind of have been on L1 and started out there but then it migrated um we have tons of infrastructure support from all sorts of different companies um shout out to uh to tenderly that just launched uh our full arbitrum support recently which was very exciting um and arbitrim is becoming a major part of ethereum um two kind of interesting charts here one being oh and it's not rendering very well but you can look on etherscan yourself um one being that arbitrum the arbitrary sequencer is one of the biggest gas vendors on ethereum um that we are now sort of using a significant amount of L1 resources in order to power the roll-up which is really exciting um and also very exciting that that will hopefully go way down uh with 4844 uh which I won't talk about too much in this talk but uh is extremely exciting that and the other thing being the amount of eth is just in arbitrum that our Bridge escrows funds that are deposited in the system um and the amount of funds in that bridge is a very kind of very significant chunk of of eth which is crazy exciting um and then the other thing to look at and this is sort of just you know very exciting and also has been kind of vastly improved with Nitro is how cheap it is to use um I think we're on average coming in at somewhere between kind of 10x and 50x price reduction um compared to ethereum um this has gotten better with Nitro as we added compression um L2 costs are quite low because of all of the efficiencies of the system and the fact that kind of the gas limits can be quite High um and so you can see we have uh we have up here just L2 fees which is a very nice site to look at you can see that we are coming in sort of just a couple cents to uh to transact in the simplest case we're doing heat transfer um and even smart contract execution is very cheap and easy um so I mean that's one thing always to look at is essentially if transfer it's easy to be cheap but also to be cheap with uh with contract execution where users can use a lot of gas and it still comes in cheap is is really uh really important and I think one of the areas that I'd say is sort of especially of course this presentation and the last one obviously for people who saw both are covered covering kind of you know similar ideas and I think this is one place to distinguish optimistic rollups is that we can do a lot of execute a lot of computation a lot of execution at very low cost um and just sort of for Roll-Ups in general and this is sort of General across uh across DK Roll-Ups across optimistic roles but I think it's really important to talk about um is just looking at sort of what sort of what Roll-Ups can give you um versus side chains um so with Roll-Ups for daily we were using ethereum and we're depending on ethereum for data availability all transactions get posted um unlike side chains where kind of it's the data is separate maybe they're posting headers back but it's an independent system we have a bike cut for a second but I'm back um we have um L1 to L2 bridging that's enforced through the security of the roll up and that's sort of one of the most important features of Roll-Ups to me is the fact that the brick is part of the system there's no sort of independent uh multi-sig no independent Bridge mechanism the bridge is the roll-up which is really fundamental as opposed to sort of needing some sort of messaging layer that's independent from the security system um Roll-Ups use fraud proofs uh well or fraud proofs for optimistic Roll-Ups fraud proofs of course um which means that anybody can the correctness of the chain um and it means that basically you don't need two-thirds honest you don't even need half honest um you would only need one honest validator in order to secure the system which is an incredibly powerful property um as opposed to needing kind of um and then just to mention a little bit I'm mostly talking about the roll up today but I want to also mention Nova um which is sort of our our kind of version of Arbor tribe but with the data availability committee there's some really interesting trade-offs here um and it's a kind of a very active design space that I think the whole roll-up Community is exploring um which is ways to make data cheaper posting to ethereum is the expensive part and so being able to move that off Chain by having a committee where you're not worried about a majority you're only worried about a couple on that committee being honest is a really powerful thing and it's what allowed us to be able to offer a platform that could really be repetitive with sort of other non-roll-up systems which don't have the cost of Hosting to ethereum um it's an interesting trade-off it does make some sacrifices in security maybe we think about it is that we're not competing with roles we think if you can afford a roll up you should use it what we're competing with is other solutions that don't have that level of security and try to provide something more secure than they are um now just a little bit about sort of how Nitro works and sort of how to imagine the system um a lot of times with Roll-Ups because it's so compatible it's kind of a black box um you have the RPC you point at the C you use it just like ethereum and that's it but it's really I think you know valuable to really understand what's going on um and really important so we've split it up into a number of steps and this is we've done this a lot how do you explain this thing it's complicated there are a lot of different pieces I think the latest iteration is one that we're pretty happy with um so we start off by talking about sequencing and this is probably you know the role of the sequencer is one that's sort of most well known in how Roll-Ups work sequencer is a is a is a um node that orders transactions it receives transactions in it puts them in an order um and then it runs them it evaluates take transition function and it produces blocks well that basically sounds like how ethereum works if you replace sequencer with miners although for the sequencer there's one entity rather than a lot of them the interesting part is in parallel or slightly trailing The Ordering of transactions the sequencer is also batching and compressing those transactions and posting it to the L1 chain now one key thing to understand here is that the sequencer is not attesting to State routes the sequencer is not making claims about what the result of executing those transactions are like what the result is all it's doing is saying this is this are these are the transactions so there's no such thing as sort of claiming something valid you into that sorry claiming something invalid um you could post an invalid transaction certainly but it would just be ignored by the state transition function and rejected and the sequencer would be out some money but it wouldn't have any other consequence and so that gets posted to the L1 chain and then picked up by the actual roll-up security mechanism which I'm not going to be able to get too much into in this talk but there's some great material about online so what does this mean in terms of finality which is one of those important questions because you have these systems and you have like you have when metamask says okay the transaction's in a block but that's not enough finality is this really important question which is when can your transaction be reversed um it's one of the things that with with the uh with the merge has changed a lot for ethereum in very interesting ways um and sort of arbitrum has its own notion of basically how you can tell when a transaction is final and so we split it up into three phases we have soft finality where the sequencer said it's the order if the sequencer is honest that's what the order is but you're trusting the sequencer and so for many applications this is this is actually pretty good um currently the sequencer is being run by us long term the sequencer will be decentralized over a number of parties um no need to trust it but you can and a lot of people do after that is kind of the really important Mark though which is when can you actually not trust the sequencer because trust if you just trust it the whole time it's a centralized system and for particularly for kind of exchanges for anybody doing cross-chain stuff you really don't want to introduce any trust assumptions and so for that basically the idea is that after the sequencer has posted a batch on chain the order is set once the transaction posting that batch itself has L1 finality the system is completely deterministic so any node off chain can get a guarantee of what the current state of the chain is based on batches posted and so if you're familiar with optimistic Roll-Ups we have and this is the last phase the certification process which takes seven days the only thing that's for is to prove back to ethereum what the result is because after 10 minutes after a batch is posted and finalized on L1 anybody in the world looking at the chain other than ethereum can know and the reason for this is really simple ethereum can't actually run all the transactions because then it wouldn't be a roll up and then it would be be expensive ethereum's not running them we're using fraud proofs but off chain anybody can just run the transactions themselves and calculate the result so now that we have that down what is the state transition function I mentioned it quickly um but it's sort of a very core part and that is basically with Nitro we now have essentially a wrapper around the kind of core gath implementation of ethereum State transition function which means our functionality can be essentially identical with Geth we don't need to worry about Corner cases we don't need to worry about weirdness and about matching matching implementation specs because the guest team doesn't imagine it does an amazing job of that and we can just lean on their work and on gets correctness for our correctness which is really valuable because it's incredibly hard to get all of those Corridor cases correct and we don't need to and then I think the last part that I'm going to talk about today is how execution and proving are different um which is really interesting and really where where we get all of our where we can get most of our performance from and this is something actually that changed with Nitro before Nitro there was a VM it ran transactions a result was produced and that also contained and that was also a proof that's very inefficient because proving tends to be something that's very slow with Nitro instead we split up these processes and so we have basically One Core code base compiled in two different ways one to run at Native speed on your computer at exactly the same speed any any evm chain can and one compile to wasm and used for proving both from the exact same code so there's not there's no need for kind of multiple implementations there's no need to worry about is the client in sync with the prover and all sorts of weird edge cases you can get into there it's using the same code and this is and you know I like to show the slide just because then it's it's when we first started showing it I think it was sort of very recent now it's it's uh two years later which is kind of insane which is really kind of how we ended up here and how I think the ethereum community came to this path um which is the idea that Roll-Ups are the way that ethereum is going to scale and I think that's sort of really important to us and really we the way we think about it is that kind of yes there are multiple different Technologies yes we're all building but kind of really what we're doing is we're empowering ethereum to actually kind of be competitive against other alternative blockchains um since those two systems in combination can do much more than other blockchains can do alone um and then just at the end I wanted to mention so what is Nova I mentioned it once it's got this other system all Nova is and this is really cool is adding this data availability committee um so otherwise the diagram is exactly the same it's just what I explained but rather than batching and compressing and posting to L1 we instead batch and compress hand it to a committee have them generate signatures and post those signatures to ethereum um which is how you get so much cost savings um when using when using the Nova chain but why it has an additional security assumption compared to ethereum um and then I just wanted to wrap up a little bit by talking about sort of I talked a lot about where we are and sort of what's great about the technology I want to really talk about sort of where the technology is not yet and what still needs to be done um and this is something that's been sort of a huge effort for us to try to figure out um there's so much going on there's so much complexity that really sort of keeping people aware of kind of the status of this technology of where we are is really important um l2b um for anybody familiar has done an amazing job with this and I would highly recommend anybody who hasn't to read through their security analysis which they've done on all the major Roll-Ups um but just to talk about kind of where arbitrum is in this regard so arbitrim is I think right now fair to say the only optimistic roll up in production with fraud proofs which is incredibly exciting and which was we have been since launch and is really core to us to actually like lead tech first we come from an academic background the tech is important but it's not a full roll up yet and I wouldn't want to consider it that because for arbitrum validation is currently permissioned which is one of our big priorities for this coming year is to drop permissioning now it's not just us there's a great set of validators I think actually in the coming weeks we're going to do an announcement where we list all the different entities that are currently validating the arbitrum chain um but having it be fully permissionless so you can truly make good on the promise if anybody can force it to be correctness is critically important next is the sequencer's fast finality guarantees they're very useful and in practice we are running the sequence now which is fine as a short-term solution but these guarantees only hold if you trust that sequencer and so getting a more distributed system where you can kind of have a much stronger guarantee that the ordering you get quickly will definitely be the ordering you get when those batches are posted on chain is a really important step forward to again reaching sort of this this real promised future and the last one and I think this is sort of the hardest kind of com the hardest issue and I think the biggest the biggest sort of discussion and one that's sort of really important has been having happening a lot here is how do you think about handling critical bugs in these systems that fundamentally this is sort of a really scary process if ethereum hits a bug or a Bitcoin hits a bug they will fork and fix that dog because it's in the core protocol and those protocols can Fork but what do Roll-Ups do Roll-Ups are smart contracts and so figuring out sort of if ethereum would fork and fix an issue in arbitrum that would be amazing but I think that's a long way to come and that's sort of what you get into when you imagine enshrined Roll-Ups which is a whole other conversation um but how to do sort of without that um in a way that maximizes decentralization while also protecting ourselves from the risk of bugs because anybody who's sort of extremely confident they don't have bugs today um I think is is overconfident um and sort of figuring out what the balance here between sort of necessary critical emergency paths and security for users is a really important question um and I know just sort of shout out to one one idea that vitalik's been been running with a lot recently there's some really cool work being done around the ideas of having multiple provers and having a majority of those provers need to all agree because then if one of them has a bug as long as the other one doesn't have the same bug then they can be checks for each other and if you have multiple Implement multiple independent implementations then you get a lot of the same Security benefits that client diversity has for ethereum um as for roll-up security which would be a really big thing So yeah thank you all for uh for for bearing with me through this and I hope it was interesting um been a total blast being here and and uh yeah I think we should probably have a few minutes for questions thank you so much Harry um we have time for three questions so could you please raise your hands foreign this was great um do you see optimism's bad joke and arbitrans Nitro converge in the specifications so there's absolutely been a lot of kind of very interesting convergence between arbitrum and optimism over the years um both projects obviously that have been building for quite a long time um I think that sort of and this is you know I don't think this is happenstance that like we've learned from each other and kind of designed models have shifted I'd like to say and then you know take this with a grain of salt because I'm obviously biased um that sort of our initial design was much more influential um particularly interactive fraud proofs which were a mechanism that we have been kind of arguing for for years um and finally one out on versus the alternative which was on fraud proof um which kind of was the original optimism design so I think there's a lot of alignment there and a lot of coming together and I think there's a lot of room for standardization um yeah [Music] hi Harry thanks very much um just on the last topic you were talking about about critical bugs and how you could fix them and the sort of issues that raises I was thinking about what Danny was talking about in his talk in the opening day uh you know minimizing governance uh do you see any Prospect of breaking out the arbitrim design into multiple components where a bunch of them are immutable and you're able to restrict the governance to just a small part no it's a really great question and I think it's it's something that sort of as we are now kind of in and I'll say kind of for context Nitro was our biggest priority for a very long time because it really kind of was solving key performance issues um that users were having I think our next priorities are all around really deep diving on these questions of exactly how much can you minimize upgradability is a really tricky thing because a lot of the time if anything is upgradable then that component could be captured um and so really figuring out sort of like if there's if there are ways to sort of modularize your security in a way to create protection is a great is is sort of a really interesting open-ended question so for instance a modular stack which is just a set of layers on top of each other doesn't really help there if some of them aren't upgradable because if you control an entire layer chances are you can do whatever you want um so figuring out sort of what Arrangements there are um to minimize that I don't have a great answer but it's a really interesting uh area for work hey thanks for the great talk um so asking kind of like a spicy question what is like the core reason why you are still maintaining a white list on the fraud proofs and then you know if that's for like you know gath reasons or like dos reasons or whatever is there a reason you wouldn't deploy you know a version of like arbitrum on something like Gorly that has you know non-white listed you know fraud prover so that you can increase kind of the confidence of people by able to generate fraud proof or at least run that mechanism themselves no it's a great question no you know I really appreciate when people ask critical questions I think a lot of times in this space there's a lot there's too much trust that's put into teams and it's really important to be be skeptical um so there's another there's a few different reasons I think kind of both the sort of performance of the roll-up protocol as many parties come in and sort of confidence in sort of the underlying fraud proof mechanisms which have been growing over time um are sort of kind of core reasons I think that in this coming year and I fully expect um within the six within the next six months um we will have permission fraud proofs on mainnet as to why not gorilla it's an interesting question um we totally could I think the main argument against it is that having the code on testnet and the code on mainnet be exactly the same is the best way to sort of minimize the chances of bugs um we have if you're interested um a large amount of fuzzing um that sort of has been done on the fraud proof mechanism itself um which is obviously sort of not very visible um to to users but there there is quite a bit of it would be definitely happy to point at that thank you so much Harry kaliner please give him a big Applause [Applause] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] this is our next uh talk the history and future of the centralized operation with water campaign please give him a bigger Plus thank you thank you foreign [Music] [Music] foreign [Music] foreign [Music] [Music] thank you [Music] [Music] foreign okay seems that the Wi-Fi issues have been resolved so let's get started uh welcome everyone I'm embarger I will talk to you uh today about the history and future of decentralized operations at makerdale um first a little bit of background so um for those that are not familiar uh Maker Now is the foundational D5 project behind the die stable coin in 2014 this started with Rooney Christensen one of the founders posting an e-money post on Reddit and then in the years after that we launched several products such as a single collateral die which is maker but only with ethereum has collateral in 2019 there was a launch of Multicultural dye which also added other collateral types and then after 2019 when maker was still functioning as a classic Foundation we started to decentralize and that simply means that we dissolved that foundation and we rebuilt everything in the Dao so um we have had almost two years of experience with that now and yeah we are wondering what is coming for a 2023 will it be make it RV next so very quickly myself I have a background in software engineering I joined makerdown in November 2017 I had to make a foundation I was the head of engineering and after that when I also joined the I became the co-founders of one of the 20 core units as we call them the contributor teams at makerdale my core unit is called maker down SCS this is short for sustainable ecosystem scaling we've removed barriers between decentralized Workforce capital and work lately we have been thinking more and more about decentralized operations and while you may ask why don't I just use the term governance as any proper Dao member well decentralized operations um I Define it very simply for me it's the art of getting things done in an open transparent and decentralized organization and by things we like to have work that produces actual value not just activity but actually creating value so why the term decentralized operations well Dow governance involves mostly discussion and decision making it is not enough to get things done this decentralized decision making needs to be supported by effective execution and decentralized operations includes governance but it emphasizes the execution part so we will be talking a lot about voting we'll be talking a lot about governance stuff but we say decentralized operations because we realize we need to execute and deliver so let's return to maker.in um what does operations and governance look like today at maker Dev there are a number of well-known Concepts that have been implemented um basically when when the Dow first started for real so uh one year and 10 months ago and of course the foundation of it all is token token voting so maker has token voting by MTR holders and it also works with delegates so mqr holders they can improve changes to the maker protocol um they have to approve these changes because the maker protocol is permissionless so without a majority of the vote you will not be able to make any changes in any way these changes they can range from simple parameters such as stability fees debt ceilings and so on but also bigger things such as collateral onboarding and off-boarding um today about 15 of the total mkr Supply is delegated and this is delegated by 222 addresses we have to say addresses because we don't know how many people are behind those addresses there are 24 recognized delegates 92 Shadow delegates and these recognized delegates they do receive compensation proportional to the voting rate although there is a limit so for example the current payout of the delegates is around 120 000 in cost for the protocol per month if you're interested in voting or delegates then definitely have a look at our voting portal vote.makeitout.com the second piece of the puzzle today are the maker Improvement proposals the mips so maker governance follows a monthly governance Cadence the mips they go through three phases first there is the request for comments um this lasts three weeks and uh one week where the proposal is frozen so that the governance participants can properly um form an opinion about what is in there if the RFC proceeds then there will be a governance poll typically this is about three weeks and yeah this poll will then be up on the voting portal that you just saw MPR holders and delegates will then be voting on the individual proposals could be that one proposal makes it another doesn't all the proposals that make it then are then bundled in the executive vote so this is uh one uh one smart contract that bundles all the changes that have been voted in by governance that month and then um yeah if mkr holders approve this executive vote then these changes are applied to the blockchain protocol the MIP system is quite Advanced for example we use templates which we call sub proposal mips there are hundreds of proposals that have been voted on since early 2022. if you want to know about all the mips that went to maker governance go to mipsommakerdial.com and the last piece of the puzzle are the contributors the core units um as you may know a maker is a pretty big Dam so we have 20 contributor teams 100 full-time equivalents of contributors around and these coordinates the way that they voted in it's with three proposal types so they Define or they propose a mandate they propose a budget and they propose their facilitator the person who will be responsible for interfacing between governance and the core unit so maker Dow grew quite spectacularly in 2021 and um as a result the total expense has also grown uh to a total of about 30 million a year the scalability of the elements that we saw is is reached now in 2022 with the bear Market that has kicked in maker has long been known as the protocol or one of the few protocols in D5 that is actually profitable um in the the longer run if you average it out it's still the case but um if you look at the latest months that is no longer the case because the bear Market has taken its toll and you can see that the dye expenses remain around 30 million so what are the challenges that we have faced in maker governance and what are some ideas that we're thinking about to improve on the current design the first challenge that I want to talk about is stakeholder alignment and political Deadlock so in a truly open and decentralized organization it is very difficult to agree on things even the smallest things let alone long-term vision and strategy maker has been experiencing that firsthand throughout the last two years there has been a lot of governance drama at maker you probably read about us even if you're not following the dial up close and at times core units the contributors they really got sucked into this drama and they broke down entirely in certain cases so there is an incredible noise Factor there is a lot of distraction and um this culminated a while ago when there was a standoff between two factions over a set of mips that um were about Real World Finance so there was a new core unit that was up for vote to be voted in and this was the landing oversight core unit and then there was a maker Hathaway fund that was up for vote I won't go into the details of what these proposals were about but they came to an ultimate standoff between the two factions and as you can see in the screenshot there was a record MTR voter turnout and um yeah this attracted quite a bit of attention and it caused a lot of distraction in the Dao at that time definitely political rhetoric reigned Supreme Over the rational debate and one of the reasons is that makerdale has very few consensus building tools other than just a forum to have a discussion and then majority voting which really isn't a consensus building Tool uh you do that when you can't reach a consensus so what might be an idea to improve on this um recently there has been a lot of talk about the idea of decentralized voter committees in fact this is a main element of The Proposal that was put forward called The End Game Plan by the uh one of the original Founders roon Christensen without a commit these they recognize on the one hand the political nature of decentralized organizations but at the same time they try to apply some structure they try to offer some mechanisms to form consensus and to reduce the conflicts that are going on all the time what it does is a voter committee is quite simple it is just a group of mqr holders and delegates and the important thing is they are self-selecting so you have people in a group that are of the same mind they think the same thing about what makers should do next what the long-term strategy should be Etc and of course not everyone might agree with that group so there might be multiple voter committees the idea is that these voter committees will put forward strategies and that the mkr holders can then vote in a liquid election system on the combination of a strategy and the delegate that they believe in if you think that this is quite similar to political parties or elections then you're quite right I don't think it's um it's a coincidence that we are re uh discovering the same mechanisms that have been tried and tested another part of the voter committee setup is that we want to um we want to restrict the flexibility and the possibilities that these voter committees have because if you leave all possibilities open then there is a lot larger a lot larger space for a disagreement so another part of the design would be a number of fixed scopes a number of business activities that makers should focus on and not go outside of that maker is not an ice cream factory for example um the Scopes I have been proposed are so far protocol engineering real world collateral permissionless collateral and growth this will probably not surprise the the strength of these is that they Define the scope and and make it closed and then as a last element um these voter committees MTR holders and delegates they need to deal with something else that is challenging about maker which is it's a very multi-disciplinary organization and you can't be at the same time an experts in engineering and finance and all the other areas of expertise that are needed in maker so when these voter committees they put forward their strategy they will Define for each one of those Scopes how they see the future for maker but by doing so there will be um there will be supported by a number of expert councils that have been put forward by the workforce by the core units and this is one idea to deal with the um yeah the the difficulties within makers to reach agreement about the long-term vision and strategy what is the next challenge the next challenge is broken transparency and a breakdown of trust maker is one of the most radically transparent organizations not just within the crypto space but I think globally um an important lesson that we've learned is that transparency in itself is not enough the right information really needs to be served to the right audiences at the right abstraction level people don't have time not everyone has the time to read through long documents and just distill the information that they may need so standard size structured data needs to be made available via apis if it is to be analyzed summarized and leveraged successfully today this is the case for some types of data in maker but not all so the clearest example of this was the core units and their budgets all information was always available it was on the forums but it was very difficult to find our user research has shown that stakeholders were unable to find even the simplest relevant information which core units exist what are they doing what is their budget and what are they spending their money on a result was a breakdown of trust between MTR holders and core units and even MPR holders and delegates because people had this impression that despite all the so-called transparency there was a lot of obfuscation going on so what is an idea how to deal with this there's the ideal the idea of decentralized operations platform we want to Leverage The Power of software to serve the right information to the right audiences and automate processes and embed best practices this isn't exactly a very um original idea but um it is one that will work we think will work properly for maker um in fact we have already been doing this for the core units and the budgets this um uh piece of information that was so difficult to um to understand for stakeholders we created a limited prototype that um focuses specifically on that information so if you go your expenses at make a diode.network you will see a clear overview of the core units right now this platform is being used for core units to submit their budgets and Koreans can use it to verify that they've properly reported on their budget this information is available on the Forum but if you would want to collect the same information that you have in five minutes here you would probably need five days or longer so what is the next challenge coordination failure between core units there is a construction error at the heart of maker data that we were not really aware of when we put forward the first proposals for the Dow the mechanism goes like this so core units are voted in individually by mkr holders and delegates for example you may have the growth core unit that is voted in separately then from the smart contracts or at the protocol engineering core unit is working separately from the Oracle score units for example so one coordination is needed between these core units and if you want to be successful and if you want to deliver value a lot of coordination is needed these core units I can only propose work to other core units but they can't enforce a collaboration so there is a constant negotiation that goes on between the core units how things should be done what are the priorities and if Koreans don't agree there is no one stepping in and saying we're doing it this way or that way in fact the core units then just walk away and say okay you do you I'll do me and we'll do each just do our job the result is because these core units they have a defined mandate they have a long-term work stream but there are no end-to-end deliverables that are defined so as a result core units don't feel responsible if coordination fails so you might have a core unit that say builds a front end and does a tremendous job and then you might have a core unit who needs to promote the front end but they haven't been aware that it it was built or that where it is available and the marketing might fail or you might have um disagreements between for example technical core units which architectures to use and if they don't agree on that well the end result will simply not work so a lot of work is done it's just that sometimes not a lot of value is created because the pieces aren't integrated or working together well so which idea can help with that we have been talking a lot about project-based budgeting at make it down and I believe that this will be a much better structure to deal with this the real value is really only produced when delivering an integrated solution delegates and MPR holders they should approve these Integrated Solutions projects more so than mandates so they wouldn't be paying for work like for coming to to work every time every day but they would be paying for the actual results now the partly proposing the solution so proposing the project they should be responsible and this is critical for the end-to-end delivery not just a technical part not just a marketing not just the front-end not just the back end the integrated solution because that's how you create value and then responsibility comes with authority over the full project budget so if core units disagree how to implement a certain piece of work then the party proposing the project should have the authority to say well we'll go with your solution and not yours the current system that we have with mid-40 which defines the budget of core units and their MIP 39 with their mandate it will probably not go away but we will gradually try and transition it towards a system that is more balanced as you can see in the diagram you can think of today's budgets as 100 retention budgets we're just paying you to do work for the Dow instead we would be transitioning to a budget that is split up between still a partially retention budget but separate from that clear project budgets where the cost items are directly related to the integrated and valuable work that is delivered then for the last challenge Talent acquisition onboarding and the compensation question maker has been struggling a lot consistently with hiring onboarding and comp challenges maker dial SCS micro unit we have run an incubation program for some time but now we are winding that down why well it's very difficult and time intensive to coach teams throughout the long and um costly onboarding period I already mentioned when you do when you make a governance proposal it takes you all in all two months to go from the RFC to the final vote but if you really want to propose a core unit you need a lot more work than that at least two months of preparation adds up to total of four it's difficult for people to just do that out of either their own effort without compensation however for a core unit it's also very difficult to make these calls so we have been doing that for a while but we felt that the model really was it wasn't suited for the new situation that we're in and of course a large element of that is simply the bear Market this has ended the onboarding and of new startup teams and it also has created a lot more discussion about where the money should go um it's easy to get a budget approved if there is um there is about enough budget to do everything comp questions in general they're extremely difficult in a global open and decentralized organization with such diverse areas of expertise even if you know how much an engineer is worth or how much a designer a web designer is worth you may not know how much you should be paying for a risk analyst or someone working in banking for example so we have attempted to ameliorate that by hiring or incubating rather a people core unit but this proves to be extremely difficult and has not succeeded so far so what is the idea here the idea is to make maker Academy instead of an incubation program um a maker Academy would be a platform that is open to everyone and that is also permissionless meaning that it is out there in the open uh in the Dao and if you want to create a core unit or you have an idea for a project that you want to see funded um you need to know how can I you know how how do I fulfill the role of being a good coordinate facilitator what are my responsibilities who how do I need to um uh interact with governance and all these things that can be made available in open platforms open education platforms with specific courses but also General topics especially today when maker is going through a lot of transition um there is a lot of complexity on the technical side there is a lot of complexity on the Real World Finance side there is a lot of complexity on the organizational side and there is a lot of complexity in the new proposals that are up for vote so um yeah an open and available platform for Education and Training may help people to to acquire the skills they need and then if you combine it with the possibility of trans then you get a funding mechanism that doesn't need to be inside a single core unit but that can be done out in the open and so maker can continue to onboard the talent that it needs so these were my four challenges so what does the future of decentralized operations that make it look like no one knows there are a lot of ideas that are floating around we do believe that the solutions we're thinking about the solutions we're building they're all open source software and they could be interesting for other projects as well so yeah thank you for coming to my talk and let's continue discussion [Applause] thank you thank you so much Hunter camman from maker Dao we don't have time for questions so feel free to reach out to water outside and we're having a launch stop so join again at 1 30. thank you [Music] foreign [Music] [Music] foreign foreign foreign foreign foreign foreign [Music] thank you foreign foreign foreign foreign foreign foreign foreign see the ground [Music] [Music] foreign [Music] [Music] foreign [Music] you can change yes [Music] [Music] [Music] [Music] thank you foreign [Music] thank you foreign foreign foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] thank you [Music] thank you [Music] thank you foreign [Music] foreign foreign foreign foreign [Music] foreign [Music] foreign [Music] [Music] foreign foreign [Music] [Music] foreign foreign well today we have uh will be yellow he is a former eth2 co-researcher will also is currently the CEO of and co-founder of Elemental finance will welcome [Applause] hello okay cool hey what's up everyone um give me one second to get set up here get my water all right I don't think this is working oh there we go cool all right so today I'm going to be talking about LP uh volatility harvesting across yield rates so um element Finance we had an event last night if any of you are there thank you for coming um so it was super cool super enjoyable I'll go into what we do in a second but one of the major things or one of the things that I want to talk about is what is volatility harvesting so volatility harvesting at its Essence if you think of amms in the current market our sort of volatility harvesting engines and what does this mean is as spot prices drop whereas spot prices rise in the space this gives opportunity for market makers to then go ahead and ARB the market make profit bring in those spot prices to a certain level and allows LPS to capture fees and get value from those volatility changes so this is essentially you know what I think of when I think of a volatility harvesting tool so let's let's apply this to the yield markets before we do that let me give you a quick intro into what does element Finance do so element Finance at its core is a yield splitting protocol so what we do is we take different yield sources in the space let's say you have an st position let's say it's a urine position a lot of our things are built on urine and what we do is if you put a million dollars in we take that position we split it into two parts so there's the principle and the interest so you have a one-year lockup if you have 10 interest on this position you have a million dollars in principle and I guess this shows 20 so 20 on a million dollars would be 200k in interest so at the end of the term at the end of the year you can collect both of those uh we use a curve this was developed mainly by actually yield space we did an alteration on this curve it's called a constant power sum and essentially what we do is during the market we let people essentially stake the principle that they have that's locked up and what the curve does is it's a time-based curve and it sort of follows this concept you know where I give you a dollar for 99 cents or 90 cents so uh this is the concept of the opportunity cost of money so if I say hey I'm gonna offer you you know this million dollars but you can't touch it for a year you can't do anything with it what is it worth for you to have that million dollars you know I can't put in a savings account I can't stick it I can't farm with it I'm losing some type of interest on that right so I say hey I think I could probably get like 15 apy I'll I'll give you 90 of that so here's a dollar you can't use it for the next year I'll give you 90 cents for that dollar and so what this curve essentially does is it acts with respect to this time period um between the constant product and the constant sum formula so early on it has price Discovery you can have the apy change on the principle this million dollars it's locked up um and later on it sort of converges one-on-one value if I say hey here's a million dollars tomorrow you can't use it until tomorrow you'll probably buy it for a million dollars right um versus for a year from now then you get that opportunity cost so this is essentially how how that works so what we saw early on in our platform is these markets actually worked really well um as people locked up their principal in the amms uh people were actually trading these with opportunity cost of money um based on the variable rates that you could see in the market so for example when we had first launched we saw a curved try crypto term where the yield was the fixed apy was at 15 percent and the variable rate was at nine percent in this term so what ended up happening is the fixed rate people actually brought the fixed apy to be higher than the variable apy how does that make sense um it's hard to grab that it's hard to understand that what happened is we were in a market low so the variable rates had all dropped in the space and people were speculating that they would rise um and sure enough they did they ended up making money off of it and so why why is that related to the number value here so we have this system called the old token compounding which essentially lets you go through and uh leverage your exposure to variable interest to go long on variable interest and it's this concept where you meant these principles these yield tokens you sell your principle for a fixed apy percentage and you do this repeatedly until you own a stronger exposure to the yield so this is like I don't want to go into too much depth here but if the variable apy is at 20 and let's say the fixed apy is at 10 I have a 10 spread if I basically re-hypothecate and do this I deposit I sell I deposit I sell I deposit I sell I do this six or seven times you know I can 7x that to around 70 so this causes an upward price action in the fixed rates in the market and so if I believe the variable rates are going to go significantly higher I might be willing to let the fixed rate go higher than what the current variable rates are so we saw some other interesting volatility behaviors um at one point there was I actually did a talk about this at another conference there was issues with MIM I think a lot of you remember that there's a whole Scandal there we have a lot of those sometimes in our industry and the fix APR like you know popped up to 130 percent and then we had another case where we had a wbtc vault where it dropped completely to zero percent actually um and during this time we saw a lot of volatility movement the price action basically the the fixed apis went up and down they went to 50 to 100 down to 20 back up to 70. it moved all around all around the place um and so this is sort of you know an example of the volatility in action eventually normalized to around 10 percent so uh what do we notice in the fixed rate Market is it generally sometimes except for some of these exceptions it tends to follow the variable rate ends up being somewhat similar um the apy to what the variable rate is in the market sometimes a little less sometimes a little bit higher they tend to track each other they're correlated this is what we've seen from the data that we've gotten in our current V1 and this is where you know I've done a talk on this before is um essentially these fixed rate markets that we do they're not really just fixed rate markets they're they're yield markets um we're essentially creating this value capture mechanism for yield markets and so um what is like element Finance like and and these systems and fixed rate systems as a whole you can sort of see them as it's more than a fixed rate system it's a highly liquid Arbitrage Market that lets you sort of play and create profit as yields shift within the space so as you'll Spike to 10 they dropped two I can create profit off of this in a similar way that if eth spikes up and then drops I can create profit mechanisms off of this it also allows for really cool things like Leverage um going long variable being you know negative on the you know future variable interest like I believe this one's going to go to zero percent this this Farm shot up in value so sort of the the dream for me and it's not just element it's sort of the fixed rate space it's just akin to you know some some similarities to zero coupon bonds other things like that is that it ends up becoming this intermediate layer for yields across lending and borrowing platforms and also like not just blending and borrowing platforms but yield platforms as a whole so real quick uh I'm gonna go into like one example this is gonna be I I can't go into a lot of the depth but I want to sort of paint a picture so I did a Twitter thread you can check check it out on Twitter it's pinned um where I actually coded I went for a week in my room and coded a bunch of simulations and this simulation particularly was on something called Fiat Dao and what they did it was the kin to maker where they had a one percent stability set fee and that one percent stability fee is basically your borrow rate right and what they did is they took principal tokens as collateral so I could take a principal or a fixed rate let's these principal tokens are these fixed rate tokens right so I could buy a fixed rate uses collateral borrow Fiat swap that to die buy more fixed rate die I could basically leverage into the fixed rate side and so if the borrow is at one percent apy or let's say makers stability if you want to think in relation to maker or other systems is that one percent apy um and uh essentially what what we're able to do is if the fixed rate's at three percent you can leverage into that fixed rate until it basically converges to that one percent value um which is really cool and what does this also allow for so there's um this concept of fixed rate borrowing mechanism or adapter someone there's a few people who are working on this on the element platform actually currently and essentially what this is is you could take these uh these instruments that we have and you can plug them into compound and Ave and existing systems and what you do is you essentially create a hedge Market you transform them into fixed rate borrow markets so let's say I have a compound borrow position um let's say I'm borrowing to I from compound let's say we hit like you know very low uh utility rate and in this case the apy to borrow goes low let's say it goes to half a percent this is great I want to lock that in you know so what you can do with sort of the system that you know we have is you take out that borrow and you hedge by going directly into the yield so these are the yield tokens I talked about earlier and what you do is let's say in this case I'm saying okay uh lending is at three percent whatever borrow is that one percent apy if suddenly the borrow goes up to 10 apy The Lending side also is going to spike up to around 10 or 12 and so because I hedged my borrow position with this yield exposure on the lending position that means my borrow position gets pretty stable I'm hedged against Stark drops or rises in the interest rate this is really cool because we can basically turn it in any platform lending platform doesn't matter what chain into a fixed rate borrow system this can be built on top this can be an adapter so what does this mean is if you sort of take a step back you can see that these mechanisms these fixed rate and variable rate in these yield markets that we have what they do is they create this convergence layer for defy rates for Lending Market rates you use it to take out borrow positions you can create essentially fixed rate and variable positions on the lending side the side where you get apy and when you go cross-platform if I basically do a fixed rate borrow on platform a and then I essentially use that to sort of go into and purchase the principal tokens on platform B that are certain value you end up having this sort of liquid layer in between that brings all those rates to convergence and brings them together it's really cool I wish I had a better diagram for this but I'm sort of starting to introduce this topic and and playing with this and this works across different l2s l1s um it's it's really really powerful actually um and so we've already started batching um basically activities on Aztec for ethereum and so what I sort of see is this world where we can sort of batch a lot of the activities from different um different layers different chains all on one chain and you can sort of interact and Arbitrage basically the borrow the lending rates the yield rates within that one chain that sort of works as a centerpiece for everything foreign I want to talk for a second about you know amen so amms I think are there's a lot of downsides to them um so you know we had unique V3 with concentrated liquidity model um you can kind of see amms also are sort of a free straddle option for market makers a lot of the value that's captured off amm's uh goes to market makers not to the people providing the liquidity not to the lpers an organic way this is why a lot of people are doing research on cool ways how do you bring Mev into the actual um the actual amm how do you bring transaction ordering and validator activities into the actual amm because sort of the most altruistic position in the markets is being an LP especially in an organic market where you don't have emissions things like that and they take on a lot of risk so spot prices can you know go uh up down through the roof um they a lot of times can see less gains than if they had held one position or um you know worse gains on the other Spectrum and so there there's definitely work that needs to be done on amms but they're really really good for yield prices and yield tokens and and principal tokens and everything that's involved there and the reason why is because if I have uh you know fixed rate usdc and we call these principal tokens in our platform and that's at 10 apy over a year that means it's going for 90 cents day by day that 90 cent value converges to one dollar you don't really experience any impermanent loss it's sort of more like a stable swap um the fees are are you know pretty cool um as a percentage of that yield that people uh secure and swap with and you know that that's really interesting and so one of one of the issues we've had in V1 and what we've been doing um is we have this liquidity fragmentation it's like I have this six month term this fixed rate term that I'm interested in but it's three months through and now I need to switch to another six month term or um you know as an LP I need to it makes sense for me to pull out my position and go into another one these are some of the weaknesses so we did a bunch of simulations analysis in our current fee markets uh capturing volatility in the space um which is super interesting um this is fairly complex the main thing that I'll share uh the biggest learnings is as you see uh yields rise um that is the highest one of the highest value capture mechanisms for LPS as you see a yield rate drop this is actually also if you're active extremely profitable so what happens if a yield rate drops so in this case we have a situation where we're going from 10 to 2.5 apy and in 2.5 months it drops to 2.5 apy um and this is a six month term so that drop essentially brings that principal token right if 90 cents on a dollar would be 10 if it drops to five percent then that's 95 cents on a dollar right five five cent discount um so essentially it's worth more and what you can do is it drops is you just sell the principal token so someone who locked in a 10 fixed rate API in 2.5 months when it dropped to 2.5 percent they sell the principal tokens they got 4.2 percent return in 2.5 months because they sold it which equates to 20 APR so if you're going into position and you see you know what I believe this like rate's going to drop it makes sense to buy the fixed rate side because once the rate drops then you can flip that you can sell that um and then you can basically get an early Redemption on your apy you get fulfilled quicker on the other case if the API drop or Rises um you get a higher exposure to the fixed rate apy so you get more more yield exposure it's also profitable Endeavor and mechanism and so what what does this lead to this sort of leads to products that can be built on top active strategies vaults really interesting things um even Bots that market makers can do to really capture a lot of this value the value is absurd that can be caught especially once you're doing borrowing lending markets once you're hedging on those once you have printable tokens as collateral once you have these markets like running truly smoothly there's a lot of ways to sort of make profit off of changing yield rate so it's it's fascinating uh this is just for fun so another thing we saw is it's sort of unideal for an LP because with these like terms we used to have um we're essentially uh you in element we have like usually six month terms and we do a new six-month term but what happens is like here's a simulation the fees drop off as this term ends it doesn't really make sense for me as the lp to stay the full term uh it makes more sense for me to pull out why do the fees drop out because as time goes on the value converges so you have less of a differential so the fees are less and also people are less likely to trade on like one month left of apy so um Johnny Ray my co-founder we're both uh E2 researchers um but he actually recently came up with a new model we're calling this hyperdrive and it introduces no more terms um LPS is Perpetual positions um and essentially this new amm that we've been researching and we'll be releasing a lot of data on this simulations on it here soon lets you basically underwrite someone to take out a fixed rate term um on whatever time they want they can say six months three months and it's a brand new term you don't have to worry about these terms going halfway into a term I have one month left you underwrite that position immediately and this is really good for LPS too it's really good for users if I want to take out a loan from Ave I want a one month three month six month I don't want this weird halfway position thing for an LP it's a Perpetual position I get I get average exposure to um basically all these different term lengths um it's it's a really good situation for them they're able to capture and Garner more fees and this also allows for just better systems to be built on top Simplicity um and better better Vault better value capture mechanisms better ways to go long and variable yield on the market better ways to be negative on variable yield and sort of play with these these markets and and have fun so this is LP volatility harvesting cross yield rates I'll uh try and release a lot more uh simulations and data here soon on Twitter follow us this is really cool we are doing some like new and groundbreaking research here and there's a lot of profit mechanisms so like pay attention is all I have to say thank you thank you thank you we have a little minute for song Korean a and if I have a question any questions yes Matthias soon DM me no just DM me yeah any other questions okay so earlier this year there was a project called defrost Finance on Avalanche and they did leveraged yield farming um but they had a lot of trouble with keeping the liquidity in order to do that so in your example where you had that three to seven leverage on the dies how do you maintain that liquidity to keep that so people actually want to trade the other side on you yeah so great question um so we actually saw like I think it was through the bull market something like 400 million dollars of trades on our platform um so we actually saw a really active uh activity on yield token compounding or increasing your exposure on the variable side um I think maybe some of your question is how do you match that if you have leverage to the other side because that's cell but you match that on the purchase side that's why things like uh like having these fixed rates is collateral right being able to leverage into buying the fixed rates is important liquidity is another thing like liquidities down significantly in the market including our platform uh I sort of think we build sustainable strong products things like hyperdrive that makes sense that are a significant leap on what exists in existing tradify like this stuff doesn't exist it's it's crazy cool um and I think it's just a matter of time of sort of building and garnering um garnering that that space I've been looking at a lot of things too looking at um real world assets like um some some other things that can also play in as yield sources I think staking derivative is a really good yield sources people are going to do those regardless Mev is an interesting one etc etc so yeah any other questions no more questions so thank you will cool thank you [Applause] remember closing ceremony will be at three and a half PM uh top of the mountain thank you [Music] all right [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] thank you foreign [Music] all right [Music] well now we have Arya Clark's Mon and Daniel Perez to talk about security risks in defy please welcome all right let's see um hi everyone and thanks a lot for coming to our talk so I'm Daniel and with me here is Ari and this is a price that has been uh done by a few people so there's Sam and Lewis who are in the audience but won't be presenting uh here today and so today we're going to talk about uh security risk in defy and in particular we're gonna try to sort of give some definitions and explain how technical security and economic security are different or how they differ so a quick outline of um what we'll be presenting today so this talk is meant to be fairly accessible so we'll start by presenting a bit the different Primitives using D5 and then we'll present a couple of protocols that are that can be built or and once we've done this we'll enter a bit more into the main part of the talk which will be explaining what technical security and economic security is and we will finally present a few open challenges for research with the focus of on these different types of security uh so I will start on a very high level but what is D5 so we have a couple definitions properties for D5 so definition of give is a peer-to-peer powered Financial system and for a typical D5 we're saying that it should have a few idealized properties uh first one being non-custodial which means that participants should have control on over their funds at any point in time um and next one is that it should be permissionless anyone should be able to participate in this financial activities without restrictions or without being able to be censored by a third party um should be openly auditable which means that anyone can look at the blood state of the blockchain or whatever the defy and be able to see the transactions and what is going on and finally it should be composable which means that different protocols should be able to communicate with each other and um to interact uh to form sort of new Financial systems on top of this um so well with um this D5 coming there has been a lot of controversy and all and we can see this a bit as like two sort of very point of views and optimistic and a pessimistic point of view where for the defy Optimist defies a huge technological Advance this is new Financial system that's openly editable and that has all the properties um listed before and that's obviously very promising for the future and there has been already a lot of good things with D5 for example um stable coins like die has been used in countries like Argentina to fight inflation any sort of things and also we have seen that more custodial system has tend to fail in some places where um decentralized Finance could could have allowed people to have more visibility on what was going on um on the other hand there's also this pessimistic view that is that well defied and regulated it's hack prone um there it can allow people for its certain most nature to to commit many sorts of crime like scamming money laundering uh and so on and uh well there had also been like many hacks as probably you have all seen um and this North Korean hackers hacking protocols and also um recent a bit more recently uh the crypto mixer uh being sanctioned um and and so on uh so well in this talk we'll we'll focus mostly so on security which is uh something that we think is absolutely uh it's a complete must for um the defy the vision of the defy Optimist to be fulfilled and really what we'll be trying to do is to to differentiate between what is technical security problem and what is an economical security problem uh before this we'll give um a bit of background around like different Primitives that are needed for all this and we'll start with some very basic assumptions here is well all Z by protocols rely on an underlying blockchain and it assumes some security properties which are consistency integrity and availability and they're going to be any D5 without disease to begin with um then it uses a few other properties of the blockchain and here one that I want to highlight because it's very there's a lot of sort of security issues because this are like um many potential let's say it's atomicity which means that if a transaction starts it will either succeed completely or it will revert but there cannot be a half transactions that just cannot be and and obviously if I relies on Smart contracts which are um programs that run on the blockchain um and using these Primitives there are a few really um essential piece of software um and of other Primitives that are required for defy first one being oracles so because blockchain cannot have access to off-chain information somebody needs to take this option information and put it on chain and um and these are called oracles and are used for example to to get price of say USD because this is not an information you could possibly have then there is governance which is used typically to upgrade D5 protocols with time change parameters and this sort of things um then we have Keepers which are off chain sort of bots and that will submit transactions to update States this is because um in most blockchain systems you need a transaction to be able to perform any sort of State transition and therefore somebody has to take care of this and finally there are many Market mechanisms that are used in D5 there are collateralization where people will put some money at stake um to make sure that you can hit the default on the position for example then there's arbitragers there's also liquidations equations which are used if somebody does not have enough collateral for whatever position uh so that covers roughly um the main primitive that we'll need um in D5 protocols and now I'll present just a couple of the five protocols probably some most of you are already familiar with these but just to highlight a few properties um so that we can kind of all be on the same page to start talking a bit more about security aspects so there are many types of protocols but we don't really have time to go through all of them so we'll first start with automated market makers which are a decentralized Unchained electronic changes because on chain it's way too expensive to have some order book based um uh Texas amms or somehow has become extremely popular and they have a lot of good properties a few properties are less good but the main idea is that people will come and provide liquidity to a pool that consists of typically two or more assets and by providing these liquidity they in some way commit to a portfolio of these underlying Assets in a portfolio that will be rebalanced by arbitragers that will try to keep the prices consistent with some other off-chain for example prices or prices on some other exchanges once that is done people can trade for this pool and that generates fee for the fees for the pool and typically this is profitable in the case where they are volatility harvesting when it was talked just before was a lot more advanced than this but basically it's if the price is around some line and going up and down it's typically profitable as opposed to if the price is consistently diverging then maybe not so um and it there's still some risk and especially strategy risk and adverse selection risk involved with these amms um and another um very important uh type of protocol for D5 or protocol for low limit funds also called lending protocols which are Unchained markets where people can borrow and lend assets so typically people will come and deposit some assets that are pulled in smart contracts other people can come and borrow these assets and to do so they will need to be overcollateralized so they cannot default on their position um an interesting thing is that um there are algorithmic interest rates um and which means that typically with this Market there's no duration risk um and if a borrower would default on his position which means his collateral ratio is not high enough anymore he can get liquidated um based on rules are imposed by the protocol and a final Point that's also very sort of typical to defy um our flash loans that's quite an interesting primitive because it allowed people to um um borrow money without having any under equilateral at any collateral and um the condition for this is that they repay uh this loan in a single transaction and this works mostly because of the Primitive I described before which is atomicity um so with all this then this protocols can communicate together as I mentioned earlier and for example one person could deposit some money in an emm and get some LP shares and use this MP shares for example in lending protocol as collateral to be able to borrow some other type of asset and that's a very interesting thing with D5 that all the protocol can really very easily communicate um so now that um I'm done with this sort of intro background about D5 itself we'll dive a bit more into uh the security and we'll really try now to to um delineate Technical and economical security and first we'll start with some informal definition and um so we say here that for um protocol or smart contract to be technically secure it needs to be secure from an attacker who is limited to Atomic actions and we're like here being secure has been going to get exploited we have a more formal definitely exploit in a paper that we'll try at the end but uh for for example it could be not to be able to sell assets and so here Atomic actions means that the action would be either a single transaction or either a bundle of transactions but the property needs to be that all these actions will be executed atomically um and because of this so technical um so attacks on technical security are risk-free because basically the attacker can just perform the attack and at the end of the transaction or of this Atomic operation he can see if yes or no he made money and if he did not make any money but if he made money he profits if he didn't he'll only pay the gas fees and can revert the transactions so by definition um or by kind of extending the definition a technical attack will always be risk-free otherwise it it will fit our other type of attack and so there are some examples of um tactical attacks are Atomic Mev sandwich attacks and for example like reference C or also attacks that exploit logical bugs and that's all now fairly well studied we know more or less how to protect against these so of course like testing smart contracts very well in program analysis or formal methods and these are in general uh the better studied one so smart vulnerabilities we have re-entrancy interior manipulation logical bugs all of which are by now quite well studied there are single transaction sandwich attacks which is where um if a protocol say would use the spot price of an amm to to use as a price in their protocol an attacker could come and and balance is amm so that when the protocol would try to look up the price it would get the wrong price and under attacker could fairly easily exploit this to make money or governance attack if it's possible in one transaction to do some governance action like we could come probably borrow enough enough governance token to do so and execute some malicious proposal uh lastly there are transaction ordering attacks so framework displayments attacks where an attacker could front run some particular transaction to make profit instead of the person who initially initiated this transaction and also multis transaction sandwich attacks which are an attack in where an attacker could come and see that somebody is trying to swap but have for example a very high slippage tolerance and he could invalid the pool before to give the the victim a bad price and then rebalance the people after and would get the profit that the victim lost because of the of the price he got so now I'll give it to Ari so that he can talk about Economic Security so the other type of security we Define a protocol is economically secure if it's not profitable for an attacker who can perform non-toxic non-atomic actions to manipulate the protocol into unintended States uh that where they can essentially like extract assets from the protocol or cause other sort of Mayhem in the protocol and so Economic Security is about where you have an exploiting agent who's trying to manipulate some sort of incentive structure of the protocol to profit like by stealing assets and since these are non-atomic they have upfront tangible costs and are not risk-free basically you have to like set up set it set up the attack and then actually perform the attack later on um and something could happen in between that those two uh those two times and basically something the attack could fail if something happens in in between those uh those two actions and such as a market responding or other agents responding and to address this we really need to have economic models of what's happening in between these transactions uh and the attacker would need to to understand this and basically manipulate what's happening uh in between these transactions so let's hammer down a little bit uh further what the difference is between Technical and Economic Security so in a technical exploit we have an attacker who's effectively finding a sequence of contract calls that leads to a profit and these are either in a single transaction or a bundle of transactions but it's being done uh all at once or not at all and for these uh formal models of contracts are basically enough so to say although it can still be quite a hard computer science problem to work out sort of optimal uh the optimal ways for attacks to be performed in comparison an economic and an economic exploit an attacker is performing multiple actions kind of at different times or really different points in the sequence and they don't necessarily control what happens between those actions and so there's no guarantee that the final action is profitable so there's kind of a setup there's uh actually performing the attack later but in between some sort of Market can respond or other agents can respond and so the attacker doesn't really know if uh if it's profitable uh at the end and for this we need models of what's going on in between which is a bit different than just formally verifying contracts and so this is kind of an open area of research especially around kind of understanding liquidity of markets so let's hammer down even a little bit further with like a very simple example of something that'll be a technical exploit and then something we can change a little bit about it that turns it into instead an economic exploit so in the example of a technical exploit let's say a protocol uses an instantaneous amm price as an Oracle and that then an attacker performs an atomic sandwich attack to steal assets from that protocol because this can be done atomically this is a technical exploit but we could change it we could use a smarter choice of Oracle so that this isn't possible and that leads to instead it being an economic exploit so here consider that the protocol instead uses a little bit smarter choice of uh of Oracle a Time weighted average amm price but these can still be manipulated over time but it involves risk for the attacker but they still may be able to steal assets and actually something like that just happened very recently in in mango I believe so we can see this also like in in data about what's been happening in different protocols so one example here to kind of illustrate this a little further is something that happened in compound in November 2020. now this wasn't really clearly an exploit but it kind of illustrates uh what could have been an exploit and what can be exploits in other protocols so basically the price of dye was trading on on coinbase and for a very short period of time uh the price pumped to a dollar Thirty and because compound was using uh coinbase as an oracle um this allowed a lot of liquidations to be possible on compound and that cost a lot of uh a lot of LP's money and compound and opened up a lot of opportunity for uh profit from liquidating those positions now this wasn't clearly an exploit but you could imagine that somebody might set this up intentionally manipulate this Market uh that manipulates the Oracle price and then profiting from the uh from the resulting liquidations and that's essentially uh what we've seen later as well so in a clear exploit something similar happened in Venus in uh in May 2021 where the the Venus Market was was manipulated and essentially the attacker was able to leave the protocol with a lot of bad debt and again just recently something similar happened also in mango so how do we what are the tools available to like help to fix Economic Security and and make protocols more secure one of the first ones uh the biggest is really over collateralization uh and here um it just doesn't come without risks though and so it's very important to include uh an analysis of the the actual economics in designing and calibrating your your protocol so for instance you could have persistent negative shocks that affect collateral prices and you could also have kind of illiquid markets around those uh those assets and this can lead to loans being undercoateralized and the system being left with bad debt uh it can also lead to situations where it's unprofitable for Liquidators to actually initiate the liquidations uh which then also can lead to another protocol having bad debt because the liquidations don't happen in time and there's also sort of issues that can happen with stable coins and deleveraging um of these stable coins like we saw on on in die on black Thursday where you had this like short squeeze effect and you also had this sort of like uh collapse of the of the liquidation engine some other things that you need to be aware of when you're designing protocols with respect to Economic Security is the minor extractable value that you can be can be setting up I won't go too in depth here because there have been a lot of great talks already about Minor extractable value I'll just point out that D5 applications tend to give many new sources of Mev and you need to be considering these and this is essentially coming from Arbitrage opportunities so for instance in uh indexes you can have sort of like stale order quotes and whoever fulfill those is able to do an Arbitrage Loop and profit and in lending protocols there's usually a a liquidation incentive and if you're the person who can come in and perform the liquidation when it's uh when it's allowed then you can profit from from being the person who does that and this can lead to consensus layer risks um if this Mev is greater than the block reward another important area is in the design of governance and the risks that can come up from your governance layer um so the governance is basically introducing a way to upgrade protocols and these need sort of careful guardrails and careful design so that your Governors aren't going to have Mis incentives to do things that are actually bad for protocol users and so commonly governance may not really be incentive compatible with the actual users of the protocol and this uh this can be an issue they may not act in the interest of these protocol users and to illustrate a little bit in some sense Governors have some honest uh cash flows but these cash flows may not always be very high sometimes they can crash and then if they do crash the region of incentive compatibility might shrink and it may be more profitable for these Governors to instead of doing uh sort of honest actions and upgrading the protocol in good ways to instead decide to attack the protocol and basically steal assets from the protocol or do other things that put protocol assets at risk and the costs to do this can sometimes be very low in D5 and should be part of the design of governance systems so for instance tokens Can Be borrowed and agents can be sort of anonymous and this can lead to low costs to uh to actually do these governance attacks the last one I want to hammer down a little bit on is this what we were talking about in the examples of where you have markets or Oracle manipulations that can directly affect your protocol so here we need to distinguish between uh one a market price that is being manipulated but correctly supplied by an Oracle and two in Oracle that is itself being manipulated so in Market manipulation you have an adversary who is manipulating the market price either on or off chain depending where that market is is occurring over some period of time and they can profit if if they can if if the manipulation they can exploit it in in a protocol that uses that market as an oracle and these problems persist even if the Oracle is not an instantaneous amm because it's just there is some liquidity in the market depending on that liquidity uh there's some cost to affect the market price and you can instigate uh changes to market price that are then reported through the Oracle system and importantly though this is risky because you have to do it over time it can't be Atomic which is again the main point about Economic Security and this compares to Oracle manipulation where uh it it depends on the design of your Oracle but even if the market price is not being affected uh the Oracle might be reporting incorrect prices so centralized oracles have uh potentially a single point of failure and you might want to control for that in designing your protocol and on-chain amm based oracles as we've seen can be manipulated and so the costs of manipulating that depending on the liquidity in those markets is something you should be carefully considering and other decentralized Oracle Solutions are really imperfect for the issue that you can't really verify the correctness of prices on chain and so it's quite an open problem how to do this very well so that sort of concludes our discussion of Technical and economic security but it leads to a host of new uh research challenges that are really going to be important for securing D5 protocols into the future so I'll give you just like a quick a quick flavor of these one is around composability risks mostly these are not very very well Quantified but a lot of program analysis can be done to uh to understand these risks a bit better and then to design your protocols in ways where uh where how you're composing with other protocols is as safe as possible another is what we were just talking about this governance sort of risk and modeling uh the incentive compatibility of of Governors and sort of modeling out what we call governance extractable value and trying to understand when uh our Governors and your system incentivize to do things that are good for the protocol and how do you stop them from doing bad things and these need economic models about how these government systems work over time and how the agents make decisions another is around oracles uh so um basically a similar sort of role as governance incentive compatibility to report uh correct prices and then um in a fair amount of work to be done in in Mev and there's just one illustration of sort of like what makes Mev very hard is that um if you're if you're looking at just intrablock Mev Atomic Mev um this this becomes an optimization problem that resembles uh knapsack but where the items in the knapsack can change depending on the current selection so it should be even harder than knapsack and so it should be an NP hard problem and this becomes even harder than if you're looking at interblock Mev and this includes cross chain Mev because now you have to look at an inter-temporal version of that of that same optimization problem and there's also a lot of work to be done in sort of making uh Anonymous D5 protocols and preserving privacy so that brings us to the end of the talk uh just as a quick recap we've covered how D5 has uh several Innovations but it also has several risks and to fulfill The Verge the the vision of the defy Optimist we really need to make sure that D5 is secure and to do that we've delineated uh two types of security risk between Technical and Economic Security and the key distinctions that uh that allow this to be useful are that it's based on atomicity and it really tells you a lot about the models you need to understand Security in your deep in your D5 protocol and the types of models that we as researchers uh need to build out so thank you and let's open it up for questions if we have time we have time just for one question foreign just while we're waiting for the mic um so this is based on the paper that we wrote and this is a QR code in the link if you want to have a look and there are more formal definition in there so please feel free to take a look um so how does this overlap with a lot of what gyroscope is doing and sort of your mission and vision um the super super curious about that yeah that's a great question so how we've designed uh so it's a gyroscope we're working on a new stablecoin project where we're building a bunch of different uh Primitives that allow what we think is a more resilient stable coin design and it's really coming out of all the research we've been doing we've set up some of the initial models that like helped understand for instance Economic Security and the mechanism design that went into gyroscope takes all of that knowledge into consideration and tries to do the best mechanism design that we can considering uh how we understand Economic Security today foreign thank you so much remember the menu will be open this weekend so come and talk with the people with everyone close deals Etc [Music] me please [Music] foreign [Music] foreign [Music] protection welcome Felix thank you hi everyone my name is Felix I'm working with an awesome team of people at cow Swap and I want to talk to you today about how we can design fera trading mechanisms for ethereum and I want to frame this Vision based upon an idea of just having a single price per token per block and we'll explore a little bit of why we think this is the most Fair Way of trading on ethereum the reason why we're talking about this is if we look at Main adoption of decentralized trading we can see we've made some progress in the last couple of years but we're just seeing the tip of the iceberg here even digital asset trading as a whole including centralized exchanges is many times larger than Dex trading today and when we want to get into more interesting markets like the U.S Securities Market or maybe the Holy Grail of trading Global foreign exchange trading then we really need to step up our game and I think personally that ethereum has as this credible neutral settlement layer the potential of enabled trading of all different parties in Cross Nations supernational actors so I really think this Global foreign exchange trading is a goal that we can strive for so to talk about how we get there let's first revisit a little bit the brief history honestly that we had um in the in the context of decentralized exchange Trading and I want to frame this history based upon an article from Alvin Roth who is an expert on Market design and won a Nobel prize in economics a while ago who wrote this essay on necessary requirements for a market to function in Harvard Business Review a while ago and he mentions three properties that a market needs to at least fulfill in order to function properly the first one is a good Market needs to be thick Ness just meaning that there is enough liquidity that when a buyer and a seller come or even just one side of them they can actually interact the second point that I will put a lot of focus on is what Alvin calls safety and by safety he means the market has to incentivize its participants to truthfully reveal their preferences they shouldn't be holding behind the bush and haggling and kind of hiding their their true relate their true preferences but instead be incentivized to commit them kind of openly and then the last Point touches more on the scalability aspect which I will not go into much detail here because we know as ethereum as a whole is focused on getting scalability done and we have all these great talks at this conference about ZK VMS sharding and so the hope is that with ethereum figuring out scaling we can basically piggyback on that scaling for Dex trading as well and so if we if we look at how everything started a couple of years ago the first taxes on ethereum where etherdale to idex Oasis basically on chain limit order book taxes and those worked fine for some token pairs like etherus DC maybe worked quite quite well but they had a fundamental flaw which was that they required active Market making participants to constantly update the quotes and the bids and basically keep the market flowing keep the market in sync and with the explosions of tokens in the space it made it very difficult to bootstrap markets for new token participants or for new pairs and so what the problem with unchain limit order books was that it was just eventually very hard to create a thickness create thick markets and this is where amms came along amms really solved the problem of liquidity provision and allowed every one of us to become a liquidity provider just by staking two assets into a smart contract that would then automatically sell them on our behalf based on some um based on some curve based on some preference curve and all of this started in the context of prediction markets way before blockchains were born Robin Hansen had a paper on on logarithmic basically amms and then in the context of ethereum we had many different teams pioneering more or less complex functions everyone of course Knows X times y equals K but really the the key thing that amm's brought to the space was overcoming the liquid liquidity bootstrapping problem and creating thick markets even for long tail tokens so what's the problem with amms the problem with amms and again we have heard many talks about this at this conference is Mev or how I will call it in this presentation pay as bit pricing and pay as bit pricing comes from the fact that what you're seeing on for example uni swap if you're trading some tokens is not actually what you are sending into the mempool to be executed this is a trade that actually happened a couple of weeks ago somebody was selling one million dollars on uni Swap and probably saw on the UI that they would get 750e but the thing that you need to add on uniswap is a discount to the current fair market Price which is referred to as slippage tolerance and you can think about slippage tolerance as basically how much volatility are you willing to accept in order for your trade to still go through blockchains are asynchronous by Design so there's race conditions and so the moment you click swap on uni swap somebody else might also be clicking swap somewhere else and therefore you need to price that volatility tolerance or slippage tolerance in now the problem with it is if your trade is large enough if the slippage tolerance is large enough the block Builder proposer validator Miner column what you will has an incentive to actually manipulate the prices and execute your trade exactly at your bid at the bid that includes your slippage tolerance and so this is what happened here um the this person got sandwiched and lost ten thousand five hundred dollar to the in this case it was after the merge to to the to the block producer or validator and so this is really what makes amm's fundamentally unsafe by Design the fact that you cannot be honest about your slippage tolerance you basically have to lie to the amm and say hey um you know just set the small slip Insurance because I know if I really tell you what I'm willing to accept somebody is going to sandwich me and and run over me and so maybe to go a step back and and revisit why is it important to save a safe mechanism design here a safe Market the first reason is that safe markets are just fundamentally simpler to reason about and make conclusions that basically the system is secure and that the allocations is is optimal and and you find the right answer the second reason why safe markets are better or important is that they're fundamentally more efficient than unsafe markets because if users are incentivized to Rel to reveal their true preferences you can just look at the preference and find the globally socially optimal application in in one round trip rather than doing multiple bids and asks and basically haggle around the haggle around the table and then the last point is maybe a more altruistic argument that safe markets are just fundamentally fairer to the least sophisticated participants Tyrone was talking about stealing from grandmothers I personally think more about people that visit Reddit and find a new coin and then go on uni swap trying to trade it and basically Get Wrecked um and yeah the least sophisticated participants are automatically protected in safe markets and so I agree with Maureen O'Hara who's a professor at Cornell who says especially on this problem of Mev that blockchain is not going to succeed if it's not viewed viewed as fared and so let's you know revisit the site that we saw in the very beginning if we want to get larger market adoption we need to design fairer and safer Market mechanisms so how do we build safety on top of amms this is what the second part of this talk will focus on the first thing that I want to kind of revisit is what we think is probably the fundamental root cause of most of the Meb that's out there maybe maybe even all Meb and that is that a single asset today on ethereum can have many different prices within the same block here's an example block from I think a week and a half ago where the most liquid pair that exists on ethereum eat USD was traded 11 times within the same block and at eight different prices the difference between the lowest and the highest price in this block was more than one percent and imagine the block really just happens at a single moment in time that's what blockchains do they they freeze kind of the the state of the world in a specific instant of time and yet that instant of time told people well there's eight different prices for for the most liquid pair and for example latency Arbitrage when you see a price change on binance and you try to Arbitrage it away against an amm that is an example of Mev that comes from have one asset having multiple prices because the first person that gets the trade in the amm still has the outdated price available and then everyone else in the block uh trades at the fair market Price we have one asset many prices the reason that liquidations cause Mev is because that whoever triggers the liquidation gets to trade at a discount whereas the rest of the market trades at a different price rather than using the liquidation volume and put it into the fair price finding and just settle everything at a single price and then the last point the sandwich attacks we saw on the last slides very obvious there was three different prices the opening the victim and the sandwich again one asset many prices and that's why we have Meb so I argue that any Market structure where prices depend on arbitrary the the block producer can choose it at no um basically at their own will that arbitrary interblock ordering just makes the mechanisms unsafe by Design and so what is the solution well you might have guessed it from how I've started this this this chapter but the solution is to just have one price per token per block and so would this look like we basically associated with every ethereum block have a price Vector for every asset that is traded in the block and that asset just has a single price at which it can be accessed within well that block and this price here is nominated in dollars it could theoretically be nominated in every in any currency you'd like um but the idea would then be that the participants that are trading in this block would be trading according to this single price clearing so if you're trading eth against Bitcoin in this block you can basically get your exchange rate by just looking up the two values here and that defines what is your exchange rate in this in this block and so this idea of uniform price clearing is very tightly coupled to the idea of um not executing trades sequentially one after the other but batching them together and executing them in one single batch so now I want to talk about how we at cow swap have built a batch auction system on ethereum we've been live for a year and a half now and yeah you can actually achieve this with everything that ethereum offers today and just six simple steps the first thing that we need to do is we need to stop people from sending raw ethereum transactions because that's just a fundamental limitation of the protocol layer today you cannot when a user signs a raw transaction you cannot open up this transaction batch it together do anything with it the user signed it so it has to be executed as but trading can also be done on an intent based really what you do when you when you sign a transaction on uni swap you're just saying I want to sell token a for token B at a specific limit price and so what we do is we collect users user trades as trade intents rather than as raw transactions and we collect them over many different token pairs so we basically have this multi-dimensional order book of signed trade intents which we then combine with the entirety of on-chain liquidity that exists on ethereum today so all the amms you know about all the RFQ systems you know about and that together creates a thick Market we saw earlier that amms are really responsible for the thickness and so by adding user orders to that to that liquidity pool we have the first criteria which is a fake Market we then go ahead and now trying to find a settlement in which ordering doesn't make any difference in which all at least all user trades are executed according to one single price clearing Vector where each asset just has a single price and so basically we want to find a uniform price clearing and we also need to find a concrete execution path so basically meaning where do the tokens flow so that demanded Supply equal and basically that we reach an equilibrium in this case for example the maker token that the first user wanted to sell would flow directly to another user that user would sell their usdc maybe on curve to die and then sell it to give it to the third user and so we can in so-called ring trades mix and match amms together with direct peer-to-peer trading with the only basically the only credit the only constraint that everything has to happen at least everything between those hands has to happen at this uniform price screen now the problem is that this poses a pretty hard optimization problem it's basically an NPR problem because we're acting over multiple Dimensions not just on a two-dimensional order book but this problem can be quite well approximated or the optimal solution can be quite well approximated if we just maximize the total user Surplus user Surplus is basically the price Improvement you get on the user's limit price so the user was willing to buy ether at thirteen hundred dollars and you were delivering it at 12.50 you would have given fifty dollars of surplus to that user and if we sum all the surpluses up together we get one value which we can optimize for and by having this optimization Criterion we can now dispatch this hard problem to a network of what we originally called solvers now we're starting to call it batch Builders we're basically tasked to try to find the most optimal solution to this problem and they could employ it very different strategies or heuristics here we saw the the settlement from the from the previous slide but you could have a solver that just goes and tries to settle every single user trade with the most liquid amm on that pair but by virtue of having this approximate approximation criteria and optimization Criterion we can now rank the different solutions and find the one that settles the user trade at the best possible price and then that solution will get chosen and the proposal for that solution gets a protocol reward and so we kind of moved into a consensus scheme where the leader selection is not based on work not based on stake but based on optimality and so how does this work in practice so we said Council has been live for a year and a half this is the UI looks very much similar to what you might be used from your favorite amm front end but here's an example of a batch that we saw I think around 20 days ago where two users were trading in the same so we can see there's one high-level ethereum transaction but in that single transaction we have two people Trading um two trade intents so we have one person that is selling the manifold token and another person that is buying the manifold token and the first thing to notice here is that both people were executed at the exact same clearing price whereas if they had gone through a more traditional Dex mechanism one of them would have to go first and get a different price than the second one in this case it was actually better to be the second because the prices the the trades were against one another so you want to First have the uh the other person move the price in your favor and then trade but basically by matching those at a single uniform clearing price we removed all the games that the people in the batch could have played against one another and made the system more simple fair and safe to reason about the other thing that is cool is that because these tokens were traded exactly in the opposite direction which is what we call a coincidence of once you want the exact opposite of what I have we actually saved about forty thousand dollars in trading volume that we didn't have to send to UNI swap in sushi swap because it could be settled directly peer-to-peer so here we have one person buying 40 000 UCC and one person selling uh 48 000 usdc and so in the specific example we actually not only got fairer pricing we also got a structural price Improvement that only batching people together and trading people peer-to-peer can accomplish because we saved so much money um that didn't have to go to the amm we saved about 800 in reduced LP fees and we also saved about 1 500 in reduced price impact which stems from the fact when you trade against an amm you're moving the price up and then when you sell against it you will move the price down again and so this is how the the solver competition I talked about is looking at the moment we have um as of last week we had 12 servers in production as of this week we have 13 solvers in production that are competing for finding the best solution so we actually have quite a um yeah quite a quite a um heterogeneous landscape of different different solver entities and then there's a an entire presentation from our data engineer Ghent um that was given at depcoin which I highly recommend watching which compares the performance of cow swap in terms of how much slippage can we actually or yeah how much of the slippage can we actually give back to the user compared to other Dex aggregators in the space and we can show that we have substantially better performance compared to some of the other deck segregators foreign so for the last couple of minutes um I want to talk a little bit about proposer Builder separation or kind of the general direction I think that the ecosystem is taking with regards to Mev and why that could be or why I think this is dangerous and I want to start this by quoting one of my favorite columnists the money stuff BBC writer Matt Levine who is from time to time repeating that well cryptos in the business of constantly rediscovering the basic ideas of financial history and why this I think this is true specifically for PBS is well I'll talk about this in the next few slides but um you might have seen this graph before and this is basically trying to resemble the Mev supply chain going from a user that wants to make a trade and the trade being something like I want to sell UCC for Eve opening their favorite wallet or opening their favorite dab which converts that trade intent into a transaction and sends that transaction and PBS to a network of block builders the transaction will be something concrete like use uni swap to sell one thousand dollars for example and then those block Builders will try to collect transactions from a bunch of different places and propose a block to the actual leader of the consensus protocol the validator for the next block and so the validator is posed with a bunch of choices of logs that they can pick from and basically the block Builders are bribing the block proposer to accept their block by doing a Max bidding kind of War this meme is from from John I have to give credits um so how does a block Builder actually get the money to to perform this bidding uh basically block Builders try to extract as much Mev from the transaction order flow they they get and then pay back some or most of that uh in this Max bidding or to the block Builder so what might a block Builder do in order to get an edge in this game well they might try to find deals with wallets to get exclusive access to order flow if they are the only one who can extract value or even just you know pose this transaction for its transaction fee into the block that they propose they have an advantage over other block builders and so it could be that the buck the the flow just stops here wallets are starting to receive an extra income that's nice they might take that that would be very terrible for the user maybe in a slightly better world the wallets will pay some of that returns to the user and maybe give the user something like free transactions or commission free trading at least but if we're in this world then we've literally just reinvented traditional Finance because the Brokers Brokers such as Robin Hood receive payment from for order flow from market makers or wholesalers just as such as Citadel Securities jump Jane Street but it's just really a handful and then offer commission free trading to their end users and I'm not saying that pfof is bad actually in traditional Finance you know there there could be there's arguments on on each side but what I try to stress here is that we've learned from traditional finance that this mechanism doesn't work in a decentralized Fashions Market maker there's just a handful of them they're highly regulated without the SEC users would be run over there's basically this law that requires Market maker to give people a price Improvement on top of the official bid and ads that they see on NASDAQ and New York Stock Exchange so you require regulation in order to make this Market safe for the user and it is just a centralized system so if we really want to get ethereum to be this neutral credible neutral settlement layer we cannot reinvent the system and and work with traditional Finance the other thing that I hear a lot is you know this narrative around Mev maximization is great we all have to focus on Mev maximization sure we should minimize but we need to maximize the the  of what's out there and I just want to pose the question of these two philosophies Mev maximization and minimization can coexist at the same time because at least in my mind Mev maximization leads to very dangerous incentives for the participants and creates an extremely hard coordination problem that Mev minimizing protocols such as cow swap need to overcome right now we are in a uh you know you could you could think of a Mev reducing protocol or an Mev reducing Builder to having to play a repetitive prisoner's dilemma where in the current status quo everyone is extracting Meb so everyone is making a little profit if you now wear a block Builder that wanted to operate without the rent from Mev you would deflect from the extraction but you would basically not get any rewards from Mev and everyone else would get slightly more and it would require the entire space all Builders to cooperate in order to achieve a new equilibrium in which we'll actually hopefully are all socially better off definitely the the people that are trading because they're not getting racked but even the people that are that are validating I'd argue that in this system can make much more money from transaction fees and just adoption but really this requires all Builders to cooperate otherwise we'll not get into this new equilibrium and what we see today is that basically everyone is focusing on the top left corner and the status quo is to fight over the existing Meb that's in there and potentially even um fight against new entrants that are trying to propose Mev reducing protocols because basically that's eating the searcher's lunch and so my call to action here at the end is let's not focus on Mev maximization and split the pie that's out there because it's tiny compared to what we want to unlock let's work together and grow the pie and try to get ethereum to the next order of magnitude of adoption and with that thank you very much thank you Felix does anyone have some questions there we go all right hey great talk by the way um so I have a kind of a quick question right like what you've done is you've taken a heart problem you have made it an NP hard problem you still you have an NP hard problem which is like this very complicated auction um which brings a bit of a you know difficult situation because NP hard problems kind of by definition are very sensitive to like people kind of manipulating things and adding Solutions and I think fire call correctly bancor had some interesting issues with this um so why not for example you know one simple condition to have one price for many assets or sorry one price for many dexes with or for one for specific assets is just to do optimal routing and optimal routing is a convex problem you can do it efficiently it's very simple to solve why not just do that instead yeah so I think just having one price per token pair per block is I think what you're suggesting that you know would already be a huge Improvement to the status quo and it would be very computationally feasible to solve I totally agree with that I think the reason why we're aiming for a multi-dimensional batch auctions is to make the life of our solver team harder of course but also to because we we know that or we've seen that the token space is just absolutely fragmented right there's no cost for somebody to launch a token and so even if we just look at US Dollars there's 10 or 15 different stable coins that are all trying to resemble the US dollar and so sure we might have then one token per one price per token pair on USD eth and die each and LUSD but we would still have this implicit fragmentation and this implicit Arbitrage and so we think the most efficient way of like you know of arbitraging the if usdc to die is one one and then eth die is 1300 and ethucc is 1200 we still have like some some imbalance on that right and so I mean I agree with you it's it's a good first step maybe but we kind of already aimed for the for the second you know the Holy Grail is re-fragmenting the um re-aggregating the fragmented liquidity space that we have on ethereum and therefore we went multi-dimensional um great talk I think like one question on kind of like seeing liquidity effect went between l1s and l2s how do you see it kind of like um like is there a way on how color swap could settle like cross layer of one like cross layer twos because I think like yeah it's probably like the next problem in like every view yeah I mean yeah we are definitely I think one one problem that we're trying to solve rather sooner than later is to just access liquidity on another chain through the solver abstraction so right now if you're for example trading on polygon a large amount you have to potentially bridge to mainnet create their Bridge back and that's kind of annoying from a user perspective because we already have this abstraction of solvers that can just happen under the hood right the solver can do the bridging swapping bridging back for you and just you know needs to let you know that you might have to wait a little bit longer but you as a user don't have to worry about it but then of course the the true Holy Grail is to just have one price per token per block across many chains and I think it's an it's a very interesting research problem which we've where we've touched the surface on but not gone super deep yet um synchronicity between chains maybe you have to do some locking of funds like there's actually um Mohammed is sitting in the crowd there's actually a hackathon project at Amsterdam that looked a little bit into this but yeah it's very early days for that so so nothing that you could build on right right now thank you for leaks thank you [Applause] remember there will be an after party after the closing ceremony so Jay knows please [Music] [Music] thank you foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] and the last start we have Theo De amandis his researcher a bained capital crypto and wow MIT PhD is done welcome to you thank you um all right hi everyone my name is Theo um I think I'm in steel proof of optimality in this talk from Felix's talk because I really loved that but uh today we are not going to talk about D5 we're going to talk about multi-dimensional fee markets or kind of the fancy term is how to do Dynamic pricing for non-fungible resources um and this is Joint work with Alex Evans True and chitra and guillermar and jaros all right so the first thing I hope to convince you of is that fee markets with the joint unit of account like gas are actually pretty inefficient and what we're going to slowly work towards in this talk is a framework to optimally set multi-dimensional fees so first part of this is like why are transactions so expensive why is having one market not necessarily something that you want to do and first a little bit of an aside one of the things that we actually do sometimes see with one-dimensional fee markets are these types of denial of service attacks and this is because all op codes have fixed relative prices to each other and whenever you have a potential mismatch between a relative price and the resources that that op code consumes you can get something that takes down a network so these have been termed resource exhaustion exhaustion attacks in the literature and there is a famous one back in 2016 that took down the ethereum network or essentially made it unusable did not take it down but made it very hard to use for quite some time and this was essentially due to a disk read mispricing and of course this was patched in a subsequent EIP but if we had a multi-dimensional rather than a single dimensional Market we might have been able to adjust prices such that there was no need to actually reprice the op codes after the fact however what we're going to concentrate a little bit more on today is um throughput is why having a single dimensional Market is actually bad from kind of a network designer's perspective and so this is a very very stylized example that is not at all like close to in practice but I hope it illustrates the idea let's assume that we have a bunch of users that are submitting transactions some only consume CPU some only consume bandwidth the CPU ones have some utility of four so that's kind of how much utility they give to the user that submits it and the bandwidth ones have utility of two and let's imagine we have a block each of these transactions cost one gas the gas price is three and the block can fit four CPU transactions and four bandwidth transactions well in a single dimensional Market what happens we fill this up with CPU transactions but actually we have a lot of block space that's not used because these bandwidth transactions aren't high enough utility and something like this can happen say with like an nft mint however if we have a 2d market and say CPU has that cost of three but bandwidth has a cost of one we would actually end up filling up all the CPU transactions in this block and filling up all the bandwidth transactions as well like I said this is a very stylized example but it does illustrate when you price things separately or in other words if resources are orthogonal they should be priced separately however that of course uh needs we need a mechanism for Price Discovery to do this so how do we decide that you know CPU is three and bandwidth is one or what are those prices uh what do those prices even mean how do we get to those prices and I'm going to do a little bit of an aside in that uh I've been throwing around this term resource a lot and there's a question of like what exactly do I mean by that um the working definition we're going to use for this talk is anything that can be metered so a resource is anything that I can say how much of uh this thing a transaction uses so for example um one thing right now roll up data however we could also talk about like kind of big resources like compute memory and storage we could go down to the op code level and think of each individual opcode as a resource however we could also say you know sequences of op codes are resources for instance if you're calling like a hot storage slot versus a cold storage slot that's going to be cheaper so maybe if we have several store job codes um all in a row that's actually a different resource than calling these one by one furthermore if we're running uh full nodes on multi-core machines maybe compute unlike node or on core one is a different resource than compute on core 2. and and so on you can imagine this is a very general construction resources can be very dependent on each other um and so as long as they can be metered or we can say how much of a trend of a resource a transaction uses uh that fits into our framework so to formalize this and this is kind of where we get a little bit into the math we're going to say a transaction J consumes some Vector of resources and there's so M resources and the Specter is 8j so essentially the ith element of that Vector is going to be the amount of resource I consumed by this transaction J and now that we're starting to build blocks we're going to denote this Vector X that essentially is this 0 1 vector and we have n transactions XJ is going to be one if that transaction is included in a block and 0 otherwise so this allows us to very easily write kind of the quantity of resources that's consumed by a given block and we're going to denote that why and all this is is summing up the vector of resources that's consumed by a particular transaction times XJ and XJ you know if it's zero then we're not going to include this in the sum if it's one then it's going to be included in the sum and this can be written in kind of this very convenient Matrix Vector notation as well where The Columns of a are going to be the transaction vector or the resource vectors for each transaction all right so now that we kind of have a notion of a resource um and what each resource is we can talk about things about constraining resources targets for resources and uh charging for each individual resource so we're going to first Define a resource Target um and that's going to be this B Star and then the deviation of the target based on what I uh introduced earlier is just ax minus B remember ax is the quantity of um or the resource utilization of a particular block and in ethereum this is one dimensional so this is going to be a scalar and B Star is just 15 million gas we also want sometimes a resource limit that says how much or after a certain point a block is invalid and then we can have uh transactions satisfy something like this where ax has to be less than or equal to B um in ethereum this is 30 million gas so again in ethereum this is all one-dimensional however we're extending this to a multi-dimensional case finally this allows us to talk about prices for each resource so we're going to have some Vector P this is going to be an M vector and Pi is essentially going to be the resource price of uh or the price of resource I so that allows us to very easily write how much a transaction costs which is just the dot product of its resource vector and P and then this is split up into the sum here one thing here is when I talk about prices this is going to be the amount burned by the network or essentially the price that the network charges for a given resource so think like EIP 1559 it's not actually going to be the price that uh users pay say validators for inclusion in the block so nothing about tips here this is just going to be purely the amount that's burned all right so we set up all the math which is great but we still have to go back and say well how do we actually determine how to charge for each resource now there's a number of very reasonable things that we want if the utilization that we have is equal to our Target utilization we probably don't want a price to update that seems to be kind of like a good price however if we're over the target utilization we would want the price of that resource to increase because we want to make it more expensive so people decrease their usage and if we're under kind of vice versa so a number of things have been proposed kind of to this end one proposal from ethereum research forums back in January was this price update rule here you can kind of go through and basically see that it does satisfy these properties that we want however I could write down a bunch of other price update rules that also satisfy these properties so this kind of begs the question is this a good update rule or like what is this update rule actually doing are there other update rules that are better or have different Behavior how do we go about analyzing this and kind of the punch line of this talk is that um one the all these update rules are actually implicitly solving an optimization problem and a specific choice of the objective which you can think of is how the network designer wants the network to perform of that optimization problem is going to then give you a price update rule so this means essentially what I kind of want to convey is the a good way to think about price update rules is not like oh how do I design the best price update rule it's what do I actually want the network Behavior to be so kind of what is my objective and then from there we'll show how to get to the price update rule all right so this brings us to what we call the resource allocation problem and the setting for now is we're going to pretend the network designer is omniscient and gets to choose all the transactions in each block I know this is entirely unrealistic or not even unrealistic it's just absolutely false however this is going to allow us to build up a very useful mathematical problem that's going to get us to the price update rule so there's a few things that we need for this problem first we want a loss function that's defined by the network designer and this loss function essentially is going to be the unhappiness with the current resource utilization so there's a few very reasonable or or potentially kind of silly loss functions that we could choose one is this so the loss function of Y remember Y is going to be the resource utilization of a given block maybe it's zero if we're exactly at our Target and it's Infinity otherwise another thing we could do is we could say that actually we don't care if we're under the target we only care if we're over the target so we can say okay the loss is zero if we're under the Target and it's Infinity otherwise again these might not be what you actually want to do in practice potentially you want to have something where if you're a little bit off the target you're not that unhappy and then it grows say quadratically as you as your deviation increases but the whole point is we only need something that tells us kind of the unhappiness with the current resource utilization then we need some way to say what is the set of allowable transactions and in this we're going to encode all constraints in the set s so this is going to be this binary set that encodes things like Network constraints so like earlier a block in ethereum is invalid if it's over 30 million gas however there's a lot of complex interactions among transactions as well so for instance if a lot of Searchers are all trying to get a specific liquidation only one of them can get that liquidation and this can also be encoded in this set so this is a very general kind of object that just says what transactions are okay we're going to do this is kind of the first mathematical trick that we play here um and this isn't that important but it's uh essentially instead of considering s we consider What's called the convex Hall of us this just means that instead of forcing X to be zero or one we allow X to be a fractional value between 0 and 1. so the way to think about this and the way this kind of makes sense is from the network designer's perspective uh you care more about the average case or the average kind of usage of the network not one particular block so say if XJ is a fraction that would just say that we include that transaction after roughly 1 over XJ blocks and we'll see that we can actually remove this constraint in a little bit but again this is just to set up kind of the mathematical formalism so this doesn't really matter this won't really matter in a bit but I just want to be complete all right the final thing that we need is we want to know how much utility a given transaction gives to the Joint user and validator set regroup these two um these two parties together into what we call the transaction producers and the reason we do this is because we don't want to deal with kind of the game theoretic analysis of uh looking at bids and auctions and that type of thing um so we assume that kind of these this group of people is together they're submitting transactions and those have a specific type of utility you'll see that our mechanism actually doesn't matter it doesn't matter that we group these things into the transaction producers but this does present an area for future work and I'd like to point out that we almost never know Q in practice it's more or less impossible to know that however we will see that this actually doesn't matter once we write out this problem okay so a lot of setup I'm sorry um but this is kind of where we get to so what is the resource allocation problem it is to maximize the utility of transactions minus the loss that the network um that the network has subject to kind of the resource utilization being defined by the included transactions and the transactions being allowable so this is the ideal kind of best case scenario of what we would actually like to solve for all the reasons I mentioned earlier this is not something that we can actually solve in practice but we'll see that it turns out to be a very useful starting point um and again this is because you know the network designer doesn't include say which transactions are included Q is unknowable you can't partially include transactions all of these issues however we'll see that we can actually pull from a branch of math called convex analysis and specifically Duality Theory to take this problem and turn it into a way to set prices so that the validators and users or the transaction producers implicitly solve that optimization problem without the network designer needing to really do anything just update prices in a very simple way all right so the the 30-second version of Duality theory is essentially it allows us a way to relax constraints into penalties so I can say that you actually don't have to satisfy this constraint you just have to pay for every unit of violation and this allows us to take Y which is what the network designer cares about that's the throughput and decouple it from the transactions that uh are actually included in the block there's just going to be a penalty for these two things not matching exactly however what strong Duality tells us is that if we correctly set the penalty this penalty being the prices then the Dual problem is going to be equivalent to their original the original being this problem which is what we actually want to solve and these two utilizations are going to be equal and they're going to have the same optimal value so again this tells us we correctly set the prices and we solve this problem without you know having to know q without caring about the fractional transactions without all the things that I mentioned are issues all right so kind of turn the crank of the math a little bit and you can decompose this uh dual problem so the Dual problem is to maximize this thing or sorry to minimize this thing into a network problem and a block building problem and P is going to be the Dual variable that's going to connect uh these two problems together so again those are the prices and the prices are essentially a penalty that we pay per unit violation of this constraint um this first term here is actually easy to evaluate you'll probably just have to trust me on this it's this object called the central conjugate it's something that we have in closed form and that means that essentially this can be run on chain however the second term is a little bit more interesting so let's look at it in a little bit more detail what is the second term actually saying well it's saying maximize the utility minus the cost so this is the net utility subject to the transactions that we can actually include this actually has the same optimal value if we just use S instead of the convex Hall of s and this is exactly the problem that is solved by the block producers so what does this mean the network never has to solve this problem it could just observe from the previous block which transactions were actually included and then it kind of gets the solution of this for free from kind of you know the decentralized block builders so what do we get at optimality well if we assume the prices are set correctly so that's going to be P star and then the block Builders use those prices to include essentially the transactions that are optimal then what do we get well we get the resource utilization of the network is exactly equal to that of the block again that's back to what I was saying earlier is that we essentially get that this constraint does hold at optimality and Y satisfies this which we can look in a little bit more detail um what this means is essentially the prices that minimize G so this is the Dual problem charge the transaction producers exactly the marginal cost faced by the network so if you set the prices optimally for whatever loss function that you define the marginal cost of like using more of that resource is exactly what the price is that you charge furthermore these prices are going to be the ones that incentivize the transaction producers to include transactions that maximize welfare generated minus the loss incurred by the network so that's back to uh that original optimization problem that we saw use correctly set prices you solve that problem the network designer doesn't need to know the utilities or anything like that all right so okay that's great I still haven't told you how to choose prices I've just kind of talked around this for a while so how do you actually do this well we can compute the gradient exactly and um what is this well essentially the network can determine this y star and I said earlier that this is computationally easy then this x-star at some current price is found by observing the transactions in the previous block so then all we do is we apply our favorite optimization method like gradient descent and we update the prices using this gradient up here there's a lot of other optimization methods that you could choose here they're going to have different convergence behavior and different trades offs between say convergence and complexity this is all stuff that we leave for future work just to go through simple examples of what I saw uh showed earlier so let's say you had this loss function looks kind of silly you actually do get somewhat of a reasonable update so this is looks like the residual so you essentially just update your prices by some you know fraction of the residual if you use this one where you're only unhappy if you're over the utilization you have the same update except you essentially make it so these are non-negative so if any of these are negative you zero them out and so this you can actually see that this makes sense here in this first uh loss function we're unhappy if we're under the utilization so this means that we might actually want negative prices to incentivize people to use more of a particular resource here though we actually don't care if we're under utilizing based on our Target so we're never going to have negative prices and again this is to kind of get to the point that the network designer chooses the loss function and the loss function encodes exactly what your unhappiness is with a particular resource utilization and then once you do that update rules that will maximum or sorry minimize that loss function will fall out of it so it comes down to instead of choosing what the update rule is you're choosing what the loss function is all right um so this is a lot of math uh we did some numerical work to kind of see how this would work and very simple um very simple examples so here we have kind of a steady state behavior of a network um with only one type of transaction that's being submitted you can think of this as pretty analogous to that example that I uh showed at the beginning of the talk but you can see that one dimensional prices uh are doing about 10 transactions per block and multi-dimensional prices are able to eke out maybe like two to three more transactions per block but this is even when you you know it's the same type of transaction that's going through so even this kind of like the simplest case you get some improvement from using these multi-dimensional prices um and I'm not going to go through all the details of kind of how we set this up but I would encourage you to look at the paper for that however um where this really shines is when we have a distribution shift so in this example what we did is we have this type one of transactions which you saw earlier but we add this type 2 transaction which has a much different resource profile so you can think of this like an nft Mint or something and they come at about block 10. so the multi-dimensional prices are the purple and blue and you can kind of see that there's this nice Spike here where we do less of type 1 transaction more of type 2 and then kind of once we go through all of these we return to zero and we clear some backlog after we've gone through these you could also see on the right here these are the multi-dimensional prices so once we hit block 10 and the distribution shifts a lot this uh light blue price goes down the other price goes up quite a bit and then they return to steady state a little bit long a little bit afterwards um again the uniform prices are still able to adjust to a certain extent but it's going to be less throughput overall and back to what I was talking about earlier where you have some Target utilization that you want to use you can see here in the second example the dashed lines are going to be the targets so the uh multi-dimensional prices which is the top deviate from the target for a short period of time to handle kind of the big spike in transactions but then the network returns to steady state afterwards in the uniform prices where you can think of you have less dimensions for this controller you essentially get this oscillating behavior and you eventually return to somewhat of a steady state but you kind of get a lot of a mess right here as your trans as the distribution shifts cool um so there's a lot of future work to be done here um one thing that we didn't do is super extensive numerical examples and you can imagine that using real data here might lead to valuable insights that allow you to tweak the framework in specific ways in addition like I mentioned earlier we group the transaction producers or we group The users and validators into that transaction producer kind of set and there is some work on the dynamical behavior of like essentially how do we make the strategy proof how do we um kind of you know we just talked about essentially the amount that was burned by the network in our prices so how do we kind of you know make this into an entire system um also I mentioned earlier that the update rule while I chose gradient descent here there's a lot of other things that you could do you could actually choose the update rule in a way that gets you something that looks very very similar to what was proposed on the ethereum research forums back in January and there's a question of okay well which update rules are good which update rules are the most useful and how do you trade off between say convergence Behavior um and complexity so how quickly kind of your prices can adjust and how much work that you're doing on chain then of course on the system designer side so if you're actually trying to use this in practice there's a lot of questions that this General framework doesn't totally answer so for example like you know what should the actual resources be in a given system and how do you trade off kind of the complexity pricing every op code every sequence of op codes and so on and the ease of use of these things and then of course how do you determine a loss function for the desired performance characteristics again kind of the very important Point here is that system designers should be thinking about these questions but should not necessarily be thinking about okay well like how do I do the exact update rule for prices um because in this framework if you think about these questions then the update rule falls out quite naturally all right and I encourage you to check out the paper which has a lot more and is kind of like 38 pages that goes through this entire thing and excruciating detail um and happy to take any questions thank you [Applause] yeah seeing as your your models you're willing to give a different cost to different op code and resources and everything uh I think something that could be interesting to see is um the order of a transaction itself if I have different custom items specific of storage like for example November semi visa thing and generally speaking if you eat a storage slot earlier it does much more potential value because this menu being settled first what would be the impact of costing differently uh storage uh of code depending on where you are in the block and generally speaking in terms of resourcedurization to anything that happened earlier in the chain is more costly for the network as a whole because you need to store it for longer and uh I don't think your framework is straight up compatible with this costing because it's missing one dimension on the vectoral cost Maybe I'm Wrong yeah that's actually a great question um I think it's compatible with the first but not necessarily the second or the second one you could probably put into it but it would be a little bit harder this kind of multi-block one however for a single block you could actually view you know that set s can saying like you know this transaction goes before this one or this transaction goes after this one and then perhaps if you're the second uh transaction in the block you're actually using a different resource so you're using like the second read so that would be extremely beneficial on field from a network if you can convince more people of going this direction because this means lower cost for everyone and better reputation first one so very good direction I'm very happy to see you I'm not saying it's easy to implement though it's very difficult to implement but if even if here it breaks like implementation is not going to be possible thank you foreign so do you see this research being applied directly for example in ethereum in like I don't know 50 years or maybe sooner uh I think there's probably some people in this room that could answer that better than I can but um I I think we we've gotten a lot of interest from different protocols that are you know Roll-Ups um Etc that are interested in us and from the ethereum research team as well um I can't speak to development timelines though and when this stuff would be like I said there's definitely quite a bit of future work that has to go into making this production ready um and I imagine that uh you know newer newer chains that maybe aren't as uh don't have to pressure test their changes quite as much we'll probably adopt something like this before ethereum would thank you um so in the interest of keeping this a convex optimization problem are there any limitations this puts on how we can construct different parts of the problems such as the Lost landscape or have I missed something and is there kind of a chance we can land in a local Optimum rather than a global Optimum here um also a good question well so the loss function has to be convex uh so there's you know one immediate thing that you have from con and you can imagine that maybe I don't know if there's two states that you want to run in and maybe sometimes you want to go hit for one target sometimes you want to hit another that wouldn't be convex if you kind of your landscape looks like something like this um so there definitely are limitations the other thing here is um we kind of have this uh the resource part is very general and to the question earlier that you can kind of have these resources that are dependent so like one transaction can be dependent on another however we do encode this all in like an additive linear way um and there's it's probably not the most efficient thing to do um for the reasons that I talked about earlier is you kind of get this exploding complexity as you do that however if you don't do that you get to kind of the non-convex world so it might be a more succinct or lower like uh complexity way of describing what it is that you want to do but you won't actually be able to solve it and this entire framework does rely on strong Duality which you only really get in convex on you mostly only get in convex optimization problems or it's very rare to get it in non-convex problems and that that allows us to look at the prices which is kind of the Dual of that instead of looking at like what transactions do I include I look at how do I set prices but that's a great question thank you thank you thank you and now people let's go to the top of the mountain to find where is going to be defcom7 see you there [Music] happy birthday [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] 