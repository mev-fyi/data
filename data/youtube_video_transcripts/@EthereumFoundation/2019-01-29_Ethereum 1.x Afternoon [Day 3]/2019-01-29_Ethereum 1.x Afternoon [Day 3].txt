hello I will be talking about you as a metering so the gas model is similar to EVM we have web assembly app codes just like we have EVM app codes each of these app codes has a cost and the contract gets executed and the cost of the execution is the sum of all the app codes that were executed so that's our how we charge gas for a web assembly you azam execution so we have options I'm going to go over five options and then that's the whole presentation the first option is the naive option do exactly what we're doing with EVM and at execution time we just charge gas for a chap code can we do better than that yes option two is metering injection this is what we're doing right now so this is an optimization that we know webassembly has a specific structure that we can jump to specific locations and we know that certain chunks of app codes sequences of app codes must complete so must execute in sequence so we can for each of these basic I guess they call them basic blocks we inject metering so we have a we already have a few implementations of this injection and so in this example we injected these two webassembly this is web assembly code by the way and what it means is the I 32 counts 3 call use gas so we're using gas for this whole the the next three app codes under those yellow ones and then this becomes the e azam that we execute and so now we're just using gas one time instead of three times so and then perhaps the block might be longer so this is an optimization can we do better it's the next question may be I hope so okay so I so that says at execution time we just meter it the three op codes one time instead of individually can we do batter so this idea of upper bound metering came up so at the ploy time we would like to be able to statically analyze the contract and find an upper bound to the any for any execution path for all execution paths we have an upper bound of how much gas is used for this contract and at call time we just charged this gas now I realize that sometimes that this might be much lower you know there might be some worst-case runtime and we're just we're overcharging people but this is a simple model and it I think it would I think it's cool I'm I'm sorry I'm being Thanks okay so this upper bound may be larger than the average case for the for the average context accusing because we might exit early like right away because you know our input sizes is too small or too big and so we might overcharge people sometimes but it's simple so for a given contract we just charge gas once and it's more efficient because we're not constantly charging yes so this would be sort of elegant to have this so the question is how can we do this sort of computation the static analysis it turns out no unfortunately we can't for a given contract there's no effective procedure to do this sort of upper bound metering because we don't even know if we can bound the runtime of an arbitrary contract so this is the I guess the decision the halting problem so we can't decide whether a given arbitrary code will halt its first specific examples we can decide but for arbitrary examples we do not have an effective procedure for it so that brings up the question a controversial question do we need touring completeness it's nice to have but once we have touring complete lists we can't do this so there's a trade-off a Viper model if we used it for awasum where we have no recursion bounded every loop has a loop variable that just acts as a loop variable and nothing else and gets incremented we can put an upper bound on things and maybe there's some other things we would have to do but this is just an idea just to think about but there are some elegance to it I think because for each contract we just have an upper bound and that's what that's what we charge is Evie I'm currently touring completes the yellow paper says it's quasi touring completes I guess maybe the the language itself is touring complete but with the guest limits you can't enumerates an infinite set so in some sense it's not touring completes so this is just an idea that's I'm that's been around for a long time but this is just an idea here's an example where we can't bound things catch act 256 it's going to be on it's going to grow exponentially things sorry I feel a little lightheaded right now give me just one second okay so this formula will grow exponentially fast in some cases so there might be some pathological code that this formula of the input parameters and state will be just arbitrary large we're better off just computing the the contract and not the and not the guess cost but for some cases for some pre code Powell's this might be very reasonable like this catch a qug sample we might use a formula thank you and by the way we might have to re meter stuff so we have to keep that in mind and that's it [Applause] just a note on the last option option for another name for it is symbolic metering yes we have some ideas of how we're gonna do this maybe some symbolic execution maybe some what I call interval arithmetic so we execute explore execution paths came for a framework does this and I might build something that does this too on pi web assembly which I wrote or cpp web web assembly which I'm writing and yeah yeah the K framework team is a fan of that option so a question about this the ket chick plot did you so let's say for input link 3000 um did you try all possible white sequences of three thousand bytes no we understand the structure of cat check and we're saying that it's arbitrary it doesn't matter they're just doing X doors and bit shifts and things like that and they don't really care so we have this sort of pre previous knowledge of how this cut how this function works but if someone's deploying something then we can't we don't have this sort of domain knowledge so we can't really we would have to check each input each possible inputs those things will grow exponentially obviously all possible three thousand bytes inputs or we won't be able to enumerate them anytime soon so the static analysis tool to determine that upper bound it would it would know whether it's only the input size and not like the specific bytes and they in the inputs yeah so when we write our static analysis tools for the pre-compile we can make that assumption so I I think we could actually sort of maybe combine serves these ideas the ideas of the injection metering turn to in non completeness in this sort of function you basically can try to you stride try to do the like the static analysis first if it sort of fails there's some criteria for failure so if it succeeds then you basically get the best situation and maybe you even kind of reward they because people can try to if they know how the static analysis work they probably tried to make their contracts whatever they write an amenable to the static analysis to get some sort of benefits if however they don't care or they they don't know how to do that they just read read they just fall back to the to the easiest option which is the injection or whatever / metering what do you think about them yes I've had this idea that we might have two classes of contracts one that the user might request people to pre-compile or to compile or to do this sort of I'm sorry the class that they request this upper bound metering and then they save on gas because they do this ahead of time metering and and they'll have like when they're deploying their contract they'll put some sort of flag to you know do this and then we would have to bound the algorithm that does this sort of static analysis you know if there are only a few execution paths that it can actually go through then it can do it in a reasonable amount of time and then they can save they can amortize that cost over the all the executions in the future so we would have some flag at deploy time that says yes do this is it worth it I don't know is this metering so expensive maybe just the injection is fine my sort of dream I didn't finish my talk there was actually one more point the option 5 is metering coprocessor I imagine that eventually this will be done in hardware and that's why I don't want to back ourselves in the corner with metering injection where we must do metering injection I'd rather it be sort of user users decide how they want a meter and if they want to invest sort of resources in this ahead of ahead of time stuff or if we want flags and things like that so because eventually it might be done in hardware anyway so that's it any more questions one second as an alternative the hardware could it be done in the execution engine that's what's happening now I think that there's some extra compute time to do the metering and that's that might be too expensive I don't know is it too expensive that's a good question but yeah that's the interpreter the VM is doing it now and we're doing the basic black one now for our test net then how do they four I guess option three and four if you wanted to change the gas costs that would require I guess three and now analyzing every contract was deployed yes so rimi Turing on the top right four options two three and four we would have to you know go through and really Dean's then we have to re-inject new metering and we would have to redo our static analysis best based on these because maybe the the worst case path might be different based based on the new gas costs so we have we have to redo everything if we change gas costs so there's an overhead there too yeah yeah so all of these are obviously optimizations performance optimizations so we shouldn't have these baked in some part of the spec I agree especially you're going to be doing it in hardware at some point right another option that you have is to get optimized so because this is all going to be pushed down to the client I assume rather than the spec you can let the clients decide how they're going to optimize and one option is every time that you hit the code you try an optimization pass while running the you know less optimized version and you either succeed or fail and then set a flag with the last thing you attempted based on some heuristic and you know you can get better performance over time which a lot of the ends do it just ends up being more work for the client yep so we would give them the ability to do that if we don't require this upper bound metering or this metering injection or something like that we give them more freedom to optimize and and and do heuristics and things like this yeah I guess it follows up on this one thing you could do is not try to do the static analysis over the whole program but do it over let's say a particular function and then you can see oh this loop always repeats this many times I don't need to compute it very gasp I can do it per block instead yeah so the hash function function the update one where it just does the X ORS and bit shifts and these kinds of things yes we can bounce individual functions we can have some sort of flags in the functions or we can yes we can do it at the granularity of functions also yes very good yeah and then you get basically the advantages of one to entry combined sort of regarding for I have a question and would it help if I as an input not only give you an even contract but also what I claimed to be the upper bound function and you would only have to verify it yes so if you have a proof and the verification of the proof is faster than the computation of this one function yes then that would be great so then yes that that's another idea I think Gregory were also mentioned that as well yeah very good we'd like GT optimization and optimization general how do you predict or how do you predict what the gas castle is gonna be at the end of the execution what's the what's the up what's the gas cost at the end of the execution or the total gas used at the end of the execution how do you determine that with all the optimizations it's based on the original deployed wasum code and that execution path if you do it in a naive way you can just do it like the EVM where you go to each app code but you have you need consensus on that okay so it's gonna be right now it's gonna be the worst execution or the naive execution path that's going to be the total gas code that would be the upper bound for the option three that would be the upper bound yes right so and optimizations are not going to affect that cost there's no you can do it once you have that upper bound it's fixed it's consensus then you can do whatever optimizations you want or you can even take you know not even execute the wasum you can execute a hand written you know hand optimized FPGA implementation of about 256 if you learn whatever as long as you're charging that guess [Applause] okay so we're back I think so this is a I just forgot to forgot to mention this before before I go into the snapshot think the discussion so first I have made addition to the framework so this is the framework document like state management state size management so I added another item number 3 here this is the ways we can possibly reduce the state size or state growth which is mentioned by Remco yesterday I just included it for completeness so there is a fourth way to try to reduce the size although it's more indirect and it depends on behavioral things is to make it more attractive to use the of gene storage and so what we have learned yesterday this ok so what we have learned yesterday from Remco and also I actually found a tiny mistake in his calculations so we so he gives us an example of a off chain contract which has two million users and each user has two mm token balances in in total it's about 4 billion 4 billion entries and it's a 2 to the power 32 right 4 billion which means that if you're represented as a binary miracle tree it's 32 layer tree that means that if you have a miracle root which has a double it's a double number of words of the length of the path which is going to be 64 hashes in the miracle proof I think you had you said it was 32 right Oh sir yeah that's that's that's - take anyway so let's say that it wasn't okay so yeah my bad I think it he's right anyway so you can divide everything by two so in my naive understanding that was a you need 64 hashes in more refined understanding it's 32 hashes which is a you know 1,000 or 2,000 bytes and it takes in or 70 K or 140k to pay for transaction data so I wanted to clarify this is in situation where you both read the data you basically want to prove that you're reading the day's reading the correct data or and in the case where you actually write it right so you want to prove the old value yeah in in the simplest case where you have one value you would need 32 values to hash it back to the root and then in order to update it you can reuse the same merkel path because all of the side branches didn't change in the process so I say I understand so but more complex scenarios make the situation much worse fast okay so basically the conclusion is that it only takes 200 gasps to read the value from the state and 200 K gasp to write the value in to state in worst case but in a not worst case is 5 K gasps but it if you by comparison it's much more expensive to try to do it in with the merkel ice tree and obviously that means that we're not incentivize in this behavior at the moment and that's probably why it's not happening so that's therefore the one of the possible designs the forest path is or combination of paths should be to reduce the gust of transaction data or increase bogus limit by let's say a factor of 20 as you suggested 20 factor of 20 Remco a or you do the combination of the two your slashing the cost and an increase in the block size limit at the same time so that's what I was going to say about the framework so added it in well let's see I woke keep iterating through this so let me go to the snapshot so this is not like a professional presentation or anything like that so I had to cobble it up little bit today because I thought it would be important to people to know what we've discussed and having a little rough or review of the sinc protocols and what we're thinking of doing not not only we saw what parity is I think is thinking of doing as we discussed that the sync is one of the is probably one of the main pain points at the moment with the with the cerium and so let's look at various sync protocols so and we're going to only talk about snapshot sync which is different from the like full sync where you start from a Genesis and apply all the block so snapshot sync is when you just you just sync the current state and so I just put this slide together from my various present present previous presentations so in guess the fast think works so it realized in the fact that all the state is stored as the has this giant this is probably better illustration like this is the top of this giant Patricia Merkle tree which is heck sorry heck sorry means that each node could have up to 16 children and this everything is routed into the state route so when the new node starts it has a state route which is retrieved from the block header and what it for the first thing it does is in the first thing it uses one of the operatives so you can see here there's operatives of the east 63 death PGP protocol so one of them is called a get node data and the one it takes one parameter which is the hash of the node in in a miracle tree in this patrician miracle tree and the the result of that if you if the another peer responds it responds with the corresponding no data operative which carries the ROP representation of that note so obviously when you receive that response you can immediately verify that that this is exactly what you asked for because you can hash therapy and compare it with the hash that you asked for so it's immediately verifiable so then if you look at the picture imagine that you first reconstructed the the node and a left and what it gives you is it gives you array of another up to 16 hashes like this is probably the better picture imagine that you had a root and then after the first response you've got this first little house with 16 hashes so each of the hashes is 32 bytes and then your issue another 16 get no data operatives - maybe - potentially multiple peers and then what you getting back is the thing on a second layer so for each of these requests you're expecting to to get those other nodes and so forth and eventually you know you can use the parallelism here by requesting multiple things from multiple nodes and you can do pipelining what what what would you what you want to do so eventually you will sync the whole tree um let's go to the yet to the points so Merkle trees reconstructed from the root to the leaves but what you're really interested in is the leaves you're interested in each accounts because if you only had the leaves but not the tree you could always recalculate the tree because the tree is just deterministically calculate abel so and as i said before each result of the node no data is instantly verifiable to belong to the tree so you're if you appear start sending you bad data you can figure it out pretty quickly and there's maybe ban them and then disconnect from them and things like this so but the biggest problem with this mechanism is that the weight like the the traffic that you require to transmit the tree is much several times bigger than the lease so because there's lots of hashing it there and but the good thing about is is there's no presentation pre generation is required because if the gas or your other if the goalie theorem is storing the data in exact exactly the same way it simply takes the data from the data structure and which we sorry returns them as the no data okay let's now look at the warp sink in parody I don't have a picture for this because it I didn't have time but let's highlight the differences so in order to save on that the the bandwidth usage the parity warp saying only transmits the leaves so it takes all the leaves and then packages them into the chunks of four megabytes and then just you can sort of receive all the chunks there unfortunately bit about it is that the verification of the chunks can only happens after you've received all the chunks so you received all the chunks you have the leaves then you rebuild the tree arrive at the root and only then you know whether the chunks are actually good or not so so far nothing bad happened about it as far as I know but what I'm worried about is that if we somehow lose this lose the fast sink and there's only parity warp sink in the network and this is gonna be the golden time to to cause mischief in there because there's no fallback anymore so this is the weak point of the warp sink another slightly weak point is that because it's not natural for for current version of parity to to just keep the leaves in like a turbo gas in a flat format it's a bit of expensive to try to fetch all the leaves and put them together so you you have to traverse the the miracle tree down to the leaves to be able to to prepackaged them as a chunks and at the moment it's I oh intensive operation and it takes about four hours to j2 progenitor chunks for the current main net so that is due to the sort of discrepancy between the how the data is stored and how data needs to be presented to the to the peers so their current the party theory was currently researching the new mechanism which is called fast warp sink which so slightly different problems so as we noticed yesterday how would you be describes okay so let's look at this little thing in the corner in the right hand side corner right or top right hand corner so each block has its like each block header has its own route state route and obviously that corresponds to differ different Merkel tree but the difference between like there's the the difference between the adjacent trees and is not that large but it's still like if you're started from the certain block and you started the far sink or warp saying let's say warp sink then by the time you finish because it could be nine gigabytes and gigabytes by the time you finish the network has already moved on let's say five hundred blocks and it could be that those peers who were serving you the chunks or don't have those chunks anymore for some reason and you basically left with your partially synced it snapshot no what would you do next so so they the as far as I understand the first word sink solve this problem by enabling you to fill the holes so essentially you said okay fine if you don't have the previous chunk give me the whatever you've got the latest chunk you've got and then what will happen is that you will construct the miracle tree of the as much of the miracle tree of the past and the miracle tree of the present and you sort of compare them compare these two Merkle trees interactively with your peer and try to figure out which paths in the tree have changed since you've received the data and then you're interactively kind of who using the same mechanism because if I think you're you're you feel the the holes in the in your in your snapshot I don't exactly know how this is gonna work but I understand the general idea of this so that would allow you to potentially catch up even you if you had an old saw slightly older information that you can still like try to catch up with the network again I don't know the details I think the the implementation has been started but we will see so but again the same problem remains here that verification of the chunks can only happen when the chunks are received as far as I understand I asked Frederik about it yesterday so you confirm that so this is this is still an issue so what I suggest that some time ago is the what I call the hybrid sync which is another variation of fast sync plus warp sync and the the the ideas that the chunks are generated on the fly from the sub trees of the state chunks can be verified on their own so you don't have to download everything and but the feeling up the holes I haven't researched it very well yet so we start with this picture so some of you are familiar with that but essentially if you look at this picture that so the so the note the first basically that the first block is the the values of the first nibble the nimble as a first it is a four bits of the address of the catch arc of the address so then so the the second level which contained 256 roots and each root corresponds to the sub tree then you go to the level 3 which has the 404 2096 routes which correspond to 400 4096 subtrees in the miracle tree so now let's stop at that point so let's stop at number the level 3 and look at one of these routes so these routes corresponds to another to this subtree and this subtree can be constructed from all the accounts that have the specific 3 nibbles in their hash of the address so let's say that we have 1 1 D so if we query all the accounts that have that starts with 1 1 D in their hash then this is going to be the con this has been the gonna correspond to our chunk so our chunk is essentially everything which will go into that subtree in a miracle in in the whole Merkle tree so this is 1 chunks as we know on the third level we have 4096 roots which means that we can have 4096 chunks ok and now let's do some math so in order to satisfy the property that each chunks is verifiable on its own there are two ways to solve this so first of all because this chug is the distinct subtree then we can use the miracle tree for what it is good for is that to provide with with each chunk we provide the proof which which convinces us this chunk actually belongs to the tree if we know the root and then if you have a three levels so each proof of each level is about is 480 bytes so in total we have one kilobyte something per chunk so each chunk can be can have one kilobyte extra data for a proof alternatively instead of having chunk having a root the proofs we can download all the proofs ahead of time which means we simply download the first three levels of the hash tree so that we we already know the proofs for all Chung's ahead of time so don't they don't need to tell us that the proofs we can verify them already so this is actually so the daily requires that you upfront receive around 128 kilobytes of data from your peer but then it saves you about four megabytes of data that you need to receive in the proofs so which could be a good trade-off right so to 120 24 28 kilobytes upfront rather than 4 megabytes later if you are not happy so with a cut with this mechanism or for 4096 chunks the each chunks will be around one to two megabytes if it's too big let's say that if it's too big then we can add a level 4 we do the same logic but with the level 4 other than level 3 so that we have instead of 4000 chunks we're going to have 64,000 chunks and each chunk will be about 200 kilobytes and then again we can do the proofs or we can download some data upfront it's up to up to us and when it comes to the storage of large contracts so this is this whole scheme allows you to to download the the main account tree and so what we left to do is to download the storage of all the contracts because some of you may know that for the contracts the the actual leaf contains the storage route for this for this contract so that allows us to to use the similar procedure but with maybe some smaller number of levels for large contracts so let's say for the things like Ida X or a either Delta we have millions of millions of storage items we could do a similar procedure might be with the two levels or something like that but if you have a very small contract which is just bunch them up in the in one big chunks in big chunks and send them all together because then in this case if you know that your chunk contains like data 4000 a contract you can just verify them straight away and or throw them away if it's a bad junk they could be more sophisticated approaches to apportion the chunks but I am NOT gonna talk about it right now and obviously I haven't thought a lot about how fill the fill up the holes but this should give you some ideas about what will what it what what exists what would that what will will exist and would what could exist possibly so yeah that's it any questions okay when you say send in the proofs yeah what do you mean specifically okay let me let me explain so let's look at this picture imagine that we are not doing three levels but only two levels right and in this case we only have 256 chunks okay and let's say that I want to send you the chunk which with the prefix one want one a right so what the proof will be essentially the entire array of hashes on the top and then the entire array of hashes on the one of them on the bottom which is the corresponds to the the first so essentially that okay just to really much send in the two levels just yeah so basically each for each level of the tree the proof is basically the block of SiC 15 hashes right well you could do 16 or 15 doesn't matter right in a binary miracle proof is obviously one each but in hex reamer Co 3 is 15 or whatever 16 right okay and okay so just basically sending this this disproves and then sending beliefs distracting from there yes so the chunk if we decide to send the chunks with the proof so that you send the chunks which contains all the Leafs of the subtree and then if the energy and the proofs then what what the receiver does is that it takes the leaves reconstructs the tree hashes the tree verifies the proof if the proof belongs to the state it keeps it if it doesn't that it throws it away and and disconnects from the peer okay okay any more questions all right then thank you very much Oh for Aundre have the question is it better now okay can you say that it's a modification of the fast sync algorithm but instead of asking just one level further you ask some number of levels but you don't need the entire subtree and not only need the if you ask for the four levels that you only need the fourth level instead but you don't need the intermediate levels because you can't calculate those yourselves no yes so potential improvement of first thing could be that instead of asking one note you're essentially asking for let's say 16 nodes or 256 no but you don't need exactly so you're saying that okay yet no data will or we they get no data whatever X which will given hash please return me let's say 256 exactly you specify the depth and then it will return you multiple multiple nodes yeah could could be a good idea yes so it will save you some of the some of the traffic no well I'm suggesting that basically where the chunk is just go back down to the Leafs the only hashes you would only you would ever send are the ones that that are used for the proofs that to prove that the chunk is inside the is in the root it belongs to the state yeah but of course your what you're saying that it could be it could be brought to what I'm suggesting with setting the the depth to whatever yeah now to sixty-four or something okay yes but it's a good it's an interesting point of view thank you see somewhere do we have a USBC cable there was the somebody had the connector see what's comes out all right okay so decent put me up really quickly for a quick talk on deaf p2p discovery v5 currently is before and just for rationale or why this description is coming about and what we could possibly do that would actually match very well was a previous discussion with Alex II so today you come up discovery connect to all the nodes start with your foot nodes then you expand the number of nodes you talk to you you feel academia buckets and then you connect you a number of them of our lpx so you exchange your public keys you could handshake you have a hello message when you talk about what subpolar calls are being supported on both sides eventually you find out that some of them had Samsa protocols that you wanted to talk about and then you do for example for if a status message and you find out that actually they are ATC nodes right so you can't actually do anything with them so there's quite a bit of churn and you are doing quite a few manipulations to get to a stable number of nodes that you talked to today that's a problem for support calls which are not very common like Elias or whisper and you know it's kind of resolved by having both nodes for those particular subnets for those particular protocols so it's easier for you to get started if you just try to do any s with random good nodes then you don't get that many nodes so it's hard for you to get there but it's not a big deal because most clients supposed to be speaking if you don't really care that IDs in any whispers match in production and so it's alright but then we introduced the chain pruning and all of a sudden not anybody is created equal anymore right so some Eve clients will have some range of blocks but some others won't so you'll need to find a way to connect pretty quickly to the right node with the right number of the right the right bugs right and this goes well with what you're saying but fasting how do I find a network of a bunch of peers which I want to talk to at this point I don't want to connect you everybody on network just to find out it's five of them on this I'll have my minute so how do we add in ideas to add capabilities when we do discovery so we just add more information the discovery level we say which support calls we support and maybe even more metadata that would be specific to your protocol like what's the range of blocks we support today there's a few considerations we don't want to create Islands this is taken from the mystical I project you can see for example islands on the right here right so it's easy for you if you specialize too much the the protocol to create a bunch of nodes don't talk to anybody else because they just are curious for example so we're gonna be between ourselves and the another thing that's kind of neat was discovery that it's very generic and it's not related to a particular supplier code so you don't want to kill on capsulation of having those sub protocols being neatly arranged inside discovery so a simple possible fix would being we create two new types of messages on this particular discovery be number five and six all we can add to the existing pings and banks so that we can add additional capacity declarations when you exchange with other peers that would be a you know after we add them to academia buckets we can we can list that for each sub protocol so an if example the chain ID the range of blocks stored and you would want to call that on a regular basis as part of pings and poems as you do today so that you would be able to see what changes over time because possible that you start storing more blocks you start dropping more blocks depending what what you're doing right so in an example of a v5 ping ping packet that I made up two months ago and we could do more so the second you add metadata to a ping package you can do a lot more isn't gonna add like your hashtag about your custom note you could create even metadata that would be useful that you'd like to share so you'd like to say well I only support this range but I know that don't support that other range it's kind of useful for me so just go to those other nodes in a little helps you get your fast sink to the next step for example so these are just ideas that would be possible what's kind of neat about this is that it leaves this to every sub protocol to define what they want to do if I go back to the d5 you can see Elias has kind of its own little things which are now in the hello status message that you have when you do Ellie yes so whether you're reading transactions something like that so each sub protocols will have a way to encode additional metadata any questions about that stuff hello how big would this v5 ping packet be oh that's a good question so you would want to keep it as small as possible to be dropped fixed size is great and small as possible is great yeah so we're just talking about that maybe four kilobytes tops that's already stretching it a bit so right now it's very small being packets are really really small right now okay so thank you very first of all thank you very much for preparing it such short notice I realized that he just made it in like ten minutes so thank you so much can you tell us more about the UM I guess the cadet Academy layer is a different use of Kedah million v4 and v5 No okay so cadonia is just about making sure that you have some level of fairness on the network right so you won't you have discovery be as plain in as possible that's why I came up with this slide about avoiding islands so the way you do it is you create 16 buckets of 16 peers and you create a there's an algorithm of distance there is comparing your node IDs with the clear IDs to decide which bucket you would go into so this allows you to create a hashing function that you know puts a random number of peers in your bucket so that there's no islands being created based on capacities or if you like that so we want to absolutely have a way to keep this as Imagineers as possible so that you can have as you know you don't want to create a situation where you have a network partition because everybody decided that fasting is much better so we're only going to talk to this peer v5 nodes that have this particular set of range right and I might happen if if we don't if we're not careful for not careful by the way we implement it nodes will select which nodes are really interesting and they'll start their studies connecting from those that don't have the right attributes so what is the way to prevent the islands I mean you say that it needs to be careful but what what are the practical things you can do to to stop the onions from forming so right now that's why discovery is very agnostic and does not actually give you that much it's because it's just giving you things and bones and ideas and all that it doesn't actually tell you anything about the capacities of the of the node if we add it in the pings it might actually be too aggressive and people may be able to start you know filtering which not start to talk to you it might be worth it making those different messages for that reason so that you have just trying to understand so we're how should we look at the reef discovery v5 is it something specified and implemented or is it something that your things should be still improved before it's implemented what is your opinion I came up with this ago those people over there put me up to it so they can answer you a bit more but I think it's not nowhere near ready the problem is that you are with chain pruning creating those issues of having a thorough generis network of nodes would have any sort in the case today anyway but you have different nodes with different functions and they have to start having a lot more different behaviors because they won't have the whole chain only some of them will have some portion of the chain and you'll need to be able to retrieve that and somewhat short order and then I'm not sure if you you know it would be great if you can answer this question but if not then it's fine so there was this paper about I guess we discussed it yesterday the paper about eclipsed attacks on aetherium you might have seen that one No so essentially like so yeah the the idea was that because you use I mean the premise was that if you if you use Kadhim Leah then your network becomes too structured and that poses the risk of eclipsed attacks so where you can basically surround a node with your own puppet nodes and then you just let the node believe whatever you want it to believe I mean truthfully that makes sense right as you discover more and more nodes so that's why we are using this Academy running you're trying to make it so that you're not going to connect every single node Xena in the network economy allows some of them but it has the issue that I mean discovery itself is completely unrelated to how you can actually use those nodes in a peer-to-peer this is just trying to create an element of a homogeneous environment for everybody to participate and so I'm not answering questions sorry but yeah regarding eclipse attacks all Kadeem lea is known to be vulnerable to that but it's described in ESCA d'emilia white paper the range of attacks that are possible and how to mitigate them so they're relatively well understood and and I haven't read the paper that you mentioned but I imagine that the attacks that were presented there were just general attacks on code Emily and Emily so they might be relatively well understood and mitigated and and the second is that yeah that possibly will require discoveries for each purpose sokka demo a source one one one purpose and finding specific information in the network but then there might be a different level of discovery that just serves a purpose of gossiping Adri information propagate in transactions or blocks or whatnot so right so do a first pool of connections to everybody and then on top of that do another pool of connections to different set of buckets that would allow you to do a bit more filtering and and see what's available don't make sense okay because I remember there was something about poisoning the poisoning condemned Lee or something like that say you can pretend to have certain capabilities and then you just lure the other nose into you to your to your nose and then they just kind of and you feed them whatever you want this is sort of the attack yeah there are various attacks on Kadima but in general they're relatively well understood and and there's mitigation techniques yes baby basically just I'm just imagining now that if we were basing our fast or whatever sync capabilities on to we put them onto discovery then obviously there's no way to verify that this node actually it advertises yes I have whatever work saying or or I have this thing or that thing and then but there's no proof that he's actually gonna give you something good but anyway so I guess it's I think Nikolai wants to say something it better yeah so attack and kill amelia is basically as you know that there's a distance function you can use with information to fill the buckets especially because we're Burkett's are quite small 16 so for example you know that the last bucket you won't be able to fill it because it's all run notes but now it's 50% of an origin whereas buckets with the first bucket it's very easy to tune your distance because you can actually calculate it from the ash of your node to maybe put your I functions we will feel all the first buckets and this way we can equip the node for some of functions that you want to use that's with generic attack and it was a quite serious attack actually so it had to be fixed in one of a Willys effect home whatever I can't remember  number but it was kind of workarounds with for example taking it work on the IP addresses in the bucket so in the back edge we'll try to have a lot of different domains to be sure that you don't have a single guy on a single Amazon datacenter created 100 nodes that will fill your bucket it's kind of thing so it's small Walker once when the natural fixes I will try anything I think you also that attack goes against its a certain note ID so you have to already be online from the start attacking you and it seems like you would be able to like you already have a few peers in your bucket like not having any peers when you find a new one and then when you start if you get and you do an ADEA every time they can't just like have already attacked you before hands maybe we can talk offline yeah if there's no more question thank you very much Antoine thank you so I assume there's no more presentation right okay so we probably should stop the livestream then for for maybe for now or for today forever we all do some internal work now to figure out the weather we achieved objectives and stuff like that 