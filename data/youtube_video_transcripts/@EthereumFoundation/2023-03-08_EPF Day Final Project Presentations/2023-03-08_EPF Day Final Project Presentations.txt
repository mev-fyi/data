um and today I'm going to talk about what I've been working on the past four months or so this project actually started um way before the EPF I've been working on ethereum of different research Branch Branch for nearly a year and a half and as a result of that research I got an idea of further um implementation for the work that could be done as a result from from that the first place of the research so really today what I'm going to be talking about is this new feature that allows the validator to to update their their signing key and I'll explain why it's as important um and there's a lot of security questions and still unanswered and I'll go through that as well and um and it was also what it means generally on the security for um for ethereum blockchain um so really the goal of the the project was to understand the feasibility of of this new pubg feature and see whether how easy it is and and understand um whether there's any implications uh to having such feature um deployed in the near future because because there's so many moving parts and the the keys are being used so many different places obviously it's quite crucial to to understand um that you know what changing one area does not necessarily break another movie part essentially um so so that was the second objective of the uh of the project is to try to do like a simple proof of concept and for this I used a price back um written in um in in Python to just do a very quick implementation to to you to understand um for me to understand what's happening and also um to to to test whether it's achievable um and really the purpose is to um um to reduce any opportunities for for an attacker to basically um basically Leverage The the slashing mechanism as a method to extra extort the money from victims essentially um so in our attacker model obviously um we kind of assume that um so in our Top Model obviously firstly that they actually have compromise the key because if I was just come approaching hey by the way give me 200 because I've somehow able to compromise a sign okay no one would believe it so the the model sort of relies on the the proof of ownership of that compromise signing key and then once that one star is is compromised at this this sorry I'm gonna update it um so once that's been compromised the attackers basically go like three possible outcomes so that's when the the explosion games really begins and that's when you got this negotiation back from what happened between the the the attacker and the adventure and really based on you know once once that proved once ownership is proven then there's only these three things that can happen really is firstly is the target could simply just assigned to exit which we wouldn't really because there's no any game for them to do that um and and secondly is um sorry the first is the it's from from the victims we're talking about from the values perspective sorry this is why this perspective it's called three possible outcomes so firstly is as soon as he realizes that um the compromise just basically exit you know um or or it would be refused and get slashed and as soon as it gets slashed then obviously you're on an exit game before 36 Days approximately but I'll be and within those three six days you still you still you still get penalties right and you'll still be slashed so if I put your key and I keep on signing you'll still be slashed so you still keep on losing money essentially or the last one is paid once and the last part is is also very tricky because um which I'll discuss a bit later um because um these um um these um there's no way of of stopping valuable the attacker to come back and basically want more money all right so that's a big problem and these needs to be aware well if the victim has paid they want them then then then they're allowed to successfully exit without anything for the penalties so that's very crucial uh in the in the really the key benefit here of having the the key updated feature is that um the the compromise validator does not uh you know need to exit the building chain essentially because um you know one thing I'm from um from uh from um from a business perspective because validating his own business right the longer you are um on an exit queue and you're not performing duties with money at which which is obviously not great um so having this feature will just basically allow um it provided us to just change their key without having to exit um and also it increases the availability providers because now uh if there's a key is compromise they don't need to exit right which means it increases the um uh the Integrity thing and availability and also the um the overall security of ethereum uh blockchain essentially because now you always have validators who are going to be there and doing their job rather than having to exit because and lastly any period in activity we're looking at Opportunity loss so any you know loss of potential reward whilst performing the device so this there there must be many of the um summary of some of the important main key benefits so really how it works is as soon as the evaluator writers called sign key has been compromised uh then uh really there's going to be um two conditions um so the the two conditions here is basically is that the so what are the conditions of for this for this to work firstly it should be something unique the key is to be unique you can't use the the current signing key because I believe um correctly from for voluntary exit you could basically use a public key to to design a validator exit right but you can't really use here because we're assuming attack has already got access to access to that key right so we need something unique that has not been previously been used and the only thing is the value to withdrawal assuming that's that's been still safe offline and it hasn't been accessed right so so those are those are the conditions of of how this this queue would work is we're going to have to use something unique uh that's that has to be compromised um so um really the goal as a person mentioned was to try to get inside of the development process try to understand the consensus um then try to do some simple implementation of um of this feature now this is very very um a basic proof of concept but obviously there's a lot of other security concerns that needs to be addressed which are going to latest lines uh but really I had to um I had to sort of modify the current price back and add new functions this is just a few of the important ones I had to add that would allow this this update to happen there's a lot of other definitions that were included in various parts to basically facilitate this change and I've got to get a link here where I've been pushing some updates and it's to be tidy up a bit because it's messy but hopefully in the next week or so there should be a nice update with the device working feature with the test case as well so the last part is the test case so you just basically check that the key has been changed so you're basically checking the old key with the new key and I'm just making sure they're all the same a very simple test case but again the purpose is to just try something quick and fail a lot from it um so challenges were um just there's more there's a lot more challenges that were put here but some of it is obviously um the ability to update the smart contract so I was speaking to uh you know we've got the VLS keys and we got the unit one address keys and uh and and those those the withdrawal keys are embedded in the smart contracts so instead of a validator it's a smart contract and it's all captured so uh is the KO is the case of then how do we make sure that the new key is linked to this new is next to the original withdrawal essentially so so that's an interesting challenge that we need to look at um and explore explore that a little bit more detail and then the last part is even a bigger challenge because the key the the public Keys used in a lot of different areas so we need to make sure that uh you know the the enrollment of the um so the real mental the effective validators um it doesn't break the other moving parts um and so just to summarize some of the unanset questions this is just some of them um there's a lot of more um which you know just having discussions yeah I've kind of found out and about a lot to this list so it's great to get you know ideas and and make this list a bit longer because it just means that there's a lot of other things that to do um for for this project and in the final report I'm hoping to serve uh sort of answer uh this uh some of these questions so firstly what happens um in a particular Epoch for example our key has been changed um then then then then at the same ebook or say for example an exit is initiated as well then then what happens what are the implications of that the other one is penalties you know I I mean a particularly hip-hop and I said I want to update my my key and then later actually slashing is happening so there needs to be a way of of of having that history to say okay this has changed well this is your old key was found to do something something miscellaneous so you will be you will be um charged with that penalty so you need to have that so is it that that's going to be a lot more Challenge and also then we have the other security implications as it's taken for now um second Paul I think I've kind of speaking to manually and I'm kind of happy with that element in the sense that lost taking polls there was sort of their own violators so you basically kind of smart contract you give them to eat and then they would manage the keys for you so in that sense it's actually better because the issue is that if it's a delegated then what if the client changes the key um so yeah I need to carry for the tests to obviously test the um the the this new feature um and the the other one is the the validator cache um on the client side um which I need to look into a little bit more detail and implication of the sync Community now this was just something that was raised last week to me I didn't even think about this and it's a very interesting um an interesting problem because sync committees are 27 hours long so then um what do we do do we just basically inactivate um a typo resisting committee to not be able to change the key for 27 hours or um or yeah Water Solutions can basically employ here to to me to get that so yeah very very interesting questions that is Beyonce answered with each other would even come to close to being applied um and um and yeah it's a it's very um uh exciting um so that kind of concludes my my presentation and I'm obviously happy to take questions and if you feel like you're going the other issues I could think about please let me know that is basically um how to take any questions representation I think I just want me they might make your life easier is that um they're we're moving into what's more like in political depositing matter so therefore it's kind of similar to withdraw we don't need the deposit contract anymore so they might interrupt a little bit yeah yeah no I think the withdrawal I think it was just I'm in a chat earlier and if the withdrawal window could be reduced to do like a couple of hours then you don't really need to have this implementation but the problem is withdrawal kit also it also takes longer which means that the longer your key is being compromised then then you'll still be you still this is still going to be an issue basically so until that window is completely narrowed down to a couple of hours then then yes then then you can just exit basically basically such a new validator but the fact that because it's a queue and there's other prolonged weight which means for for um for a validator for any business who is running voltage is important basically yeah any other comments questions online yeah exactly yeah even if the withdrawals are are implemented with the Shanghai earthquake the problem the thing is because in order to have the stability they obviously have to have this key system um and and it's it's it became system it basically mean that you could be in the before you get to funds it could be like a long waiting list basically and that's the window where the attacker will play that game to extract Ransom from you um and uh um and and unless the window is shortened whereas a couple of hours then it reduces the the attack very very significantly so a couple couple of hours very difficult for you might get slashed ones but that's it really because especially just launched you an exit key why but the problem is the attack could slash in it could be executed but you could still be penalized whilst being on the current bus station so um so yeah thank you so much all right everybody um I'm Echo and I'm here to talk to you about data availability sampling and creating simulations uh within local environments to try to figure out some possible uh solutions to passing around this new set of data that is going to be uh needed whenever Bank shorty comes around um so yeah what is data availability sampling uh data availability sampling is basically this process of an individual being able to ask another individual uh whether whether data is put on chain or not without the person asking the question having to download all of the data in order for them to know that all the data is available um and so why do we need it we need it for uh blob data or blobs um that layer 2 scaling Solutions are able to prove to prove to individuals that they've made some off-chain computation and that they're not lying and this blob data allows people to with fraud proofs verify that people aren't lying and with uh ZK snork where two things zero knowledge proofs they're able to reconstruct uh information they need to read true okay so how do we make this happen that is the question that is the Dos networking property um uh yeah and we need to have uh peer-to-peer networks that are fast that are privacy preserving and are civil resistant um and I basically just kind of stumbled upon the Dos networking problem and heard from daygrad about impossible solution and just kind of picked it up and ran with it but it's the possible solution is to uh create a secure academically a DHT uh on top of disc B5 overlay Network um so yeah what that does that mean it took me a while to figure that out um yeah so basically a secure cademia DHT overlay network uh it's kind of like this this these two uh sub protocols built on top of disc B5 uh using discipline Vibes talk requests talk response um like functionality uh and you're able to in the best case scenario have a sub Network the main dos network uh pass around information any individual can really just kind of go in and pass around samples and other information needed and if that Network were to come under attack because distributed hash tables are really easy to attack um there's a secondary Network that uh only validators are allowed to participate in sharing information and everyone else just kind of has to uh rely on the validators passing around information to get data availability sampling stuff and the cool thing about the secure overlay network is that validators are just incentivized to make sure things are running smoothly or at least a lot more incentivized than uh a random Anonymous person uh and a few things to note um yeah each each of these uh the overlay networks and disk V5 they all have different routing tables with a secure and overlay protocols will have similar stuff but um but yeah they're they're all their own uh dhts and uh yeah so what I've been doing uh over the past few months is kind of trying to create a simulation to to see what it is or see how the the overlay networks uh how they kind of like dance with each other um and I originally tried to of create this secondary overlay routing table within uh Eric and Timothy's uh dos prototype repository it's like a a pretty complex uh rust simulation uh that kind of just Tinkers with data availability sampling uh things mostly related to querying samples um but yeah so I've tried to implement the secure overlay Network within their repository and realized that I was new to networking in general and new to rust and needed to chill out and take like an intermediate step so I built dos playground and it's just this uh a simple simulation that instantiates these protocol struts between nodes and so yeah I've gotten the discovery protocol and both overlay sub Network protocols um instantiated and message processing and just like simple communication between these local peers uh and the next thing to do is to uh put my big boy pants on and and make this uh make this secondary overlay Network happen in uh Eric and Timothy's or forked version of Eric and toffee's uh so yeah and so yeah with the idea being to measure uh performance on how these uh two networks communicate and so yeah uh that's that's it for now um but I want to thank Mario and Josh first of all for letting me like letting this happen and making this happen for me um and I wanted to thank Clara who was like my first friend in crypto whenever I was trying to build like a flight client bridge in Python she was the first person that was like Hey I can help you out came in came along uh and he became my first in real life friend uh through ethereum world uh and then Eric kind of uh was a bridge between Cayman and Timothy and he's been very kind and Timothy was just like leading this uh this dos prototype simulation and was huge in being able to answer all sorts of questions like dumb questions that I had turning into better questions uh as time on but yeah just a lot of patience and love and appreciate them all and thanks for everything guys are any questions any plans to apply this on top of eip44 ah I haven't thought about that yeah yeah thank you yeah I will yeah we'll talk um anyone else yeah thanks everybody [Applause] for for making this but he's so much better yeah yeah thanks everyone for showing up my name is eknot and or Daniel and my project was to reduce the trust assumptions in the PBS relay so that's my agenda so first of all what is the PBS even it's like it's proposal separation proposal Builder separation sorry so you have the validator and you split them now into two entities the Builder and the proposal and why is that it was kind of a Market Force because the proposal has an incentive to to Outsource their blog construction because of Meb so these Builders are often like um highly sophisticated objective of good algorithms to extract NAD and so they split against one another to get included in the proposals slot and also uh there's it's needed for Diamond sharding because in bankrolling we have these roughly 16 megabyte blobs and so the the Builder has to have like high bandwidth requirements and the rest of the network just needs to verify their data available to something which you just heard of um that the data is available available and that doesn't require a lot of bandwidth and computational power so we need these um like highly sophisticated builders for that yeah and like most people think that's like great um it's like a future thing but it's already there and there's I out of protocol PBS solution uh which is displayed here so we have these um different kind of Builders which construct a block and they send this full block to the relay with a build um and this is just how much it's worth for them to be included in the proposal slot and now the relay picks the highest bit from the Builder and sends it off to Mev boost and Mev boosters just a program that's operated by the composer and I mean you boost selects um the most profitable block from all the relays so there are multiple relays it's just not one but they're all like centralized and then oh sorry and relay sends the header not the full block to the uh to the proposal or whatever you lose and then the proposal signs this block header and sends it back to the relay and the relay then releases the old block to the network so this has some trust assumptions and the first one is that of course the Relay can see the whole block content so the whole transactions and as I said there's there could be MPV included and really it could just um tamper with the block content to kind of generate this Mev for themselves so the builder needs to trust the relay that they are not doing that and as I said the relay is releasing the block not the Builder not the composer but the relay so the proposal needs to trust really to actually release the block on time and so the proposal is not missing out on any uh proposing Rewards also they really can can send through different blocks so for example with the tornado cache if there's like a tornado with cash transaction inside the inside the block they could just say no I'm not I'm not forwarding that block I don't want to and also the the last one is that it's unbiased in a bit selection so for example there are with the operators that are also operating Builders and they could be biased to just always select their block that their own buildup Constructor so so what's the proposed approach for PBS so first off the both the Builder and the proposer need to register before the proposal slot on an on-time contract and the Builder also needs to take some amount of if oh okay oh that's not good leader of is nice I'll just try to show it like I like this and yeah I'm just talking this um so so the first step in this step the Builder is sending the design and the block header and the build to the intermediary so it's important that this approach does not dictate what intermediary is used so it could be a roller it could be a modified version of the nav boost relay or it could be a data availability there and yeah so all of those could be used and now the yeah the Builder since the block header so not the whole block that's important so in the other approach before the whole block was sent to the relay and now we only sent the header with the bid and the proposal then can select the best bit from the intermediary and receive the header so and the real approach the degree that kind of shows what uh which a builder to choose and now the proposal doesn't and the proposal then signs the block header and sends it back to the intermediary and the builder then retrieves the sign header and releases the block to the network yeah so as you can see here the post to the approach that's currently used the intermediate never knows about the blog content and also the Builder here is releasing the block on their own so I'm not the relay anymore yeah okay try to go back to the phone yeah and never mind actually um so as you can see here what what guarantees are there that the Builder actually releases the block on time so the Builder could just brief the proposal by uh just not releasing the block and the proposal that missed out on the Block reward uh or just not pay the bills so for that you need like these extra slashing conditions which are that the um yeah if the Builder didn't release the block on time or the Builder submitted an invalid block could also happen and or the Builder didn't pay any promised fees and the first one could be proven to be through and we could assume that the Builder always doesn't somebody block on time or an invalid block and then kind of say hey you need to include a transaction on your block that it's a call and it calls the function the selection contract kind of say hey this block is valid because if it wasn't then you wouldn't be able to call this function uh uh I know it's kind of maybe kind of hard to get but uh yeah if you uh you need me to repeat it just say it at the end and also there's the that the block added the Builder didn't take from speeds that's just done by uh the builders and the feed the session contract and there's like a challenge period uh where if people didn't pay in that amount of time then the proposal Construction okay and so they are now like these are to reduce trust assumptions now as I told you already the intermediary never knows about the full block hand content so the intermediate cannot censor the blog based on the content that's inside and also it cannot tamper with the content because it doesn't have the content and so for like the relay and the roll up as the intermediary we're left with two two trust assumptions which are the intramural does not glucose The Proposal so um yeah for example the proposing the intermediate conclude to slash to build around fairly in that case and the interviewer does not does not censor transactions so for example the intermediate business not forward the people's uh based on some arbitrarily uh reasons are there some different there's some other trade-offs than just trust assumptions so um for example spam protection for example the roller have some spam projects built in where the builders need to pay bills and pay fees to the intermediary to the to the person or the organization that runs the intermediary and yeah that could be a suspend protection and there's also latency to consider so um yeah the latency kind of depends on the intermediarity you choose because for example really it might be faster than a data availability layer and they and also the ability to stay that the state is like a herd into entry so it's kind of difficult to say how much you need to stay because it can't be too much because then it's important to entry for new Builders but it also can be to less because um it should be at least the amount of of each that the Builder is paying us a bit to the proposal oh uh oh yeah and we can uh there's an additional da layer for the roll up and relay just to reduce trust assumptions even further of course if you use the da as the intermediary you don't need the extra it yet yeah and that would result in removing that heavy trust assumption that the that the proposal of the intermediary concludes the DDA could attest to that that the data was available at the time and there are some future work includes like a search or other markets risk-taking they're dividend there um Dynamics taking amount so the Builder could just we wouldn't dictate what the stake is how much they wouldn't stay the The Proposal can just choose how much risk it's worth for them to take um with the builder of how much they have staked and there could be some extra slashing conditions like including this yeah and that's it [Applause] any questions foreign [Music] currently also the proposals only signing the header second blinded block right now The Proposal at this time and his life is they give the full block to the network right now okay yep oh yeah sure um so what they can do is um so all of these intermediaries are basically just there for data availability so and let's now say I have a I have the roll up for example and so the proposer kind of sends the ancestry roll up to where we can conclude to the with the book that we can include to slash to build our field and what they would do is just not provide design header to the Builder but kind of attest to it that the sign head was available at that time so these are like all the contrast entities to attest to the availability of uh of the sign header and if they just say yeah it was available but if it wasn't at the time they could gluten slashed the building for not releasing the block on time for example talk about a Serial pickup chain it's quite important to have some kind of in-depth operability between client implementations for example um can connect to vitals we can learn and write us a reduction but and so the purpose of this uh topic is to rewrite the prison but it doesn't play on occur to be compatible with instead of questions API we can API is an official API designer with a specification which is a HTTP API and we try to use it right now so philosopher that's the winner improved which is what's here at me and mentors where verix Reddit be able to talk two weeks ago on Tuesday and James from president and so leadership so we had into a case so he has a prisoner of the friends which can communicate with President Lincoln but only by using a grpc request which is an internal JPC implementation in prison and for example if you want a non-prince non-prisonment like Deku to communicate with President Lincoln so it's possible the non-present the leader claimed can send a HTTP request to the present Defender that's the present legal nerd has to convert its into JPC requests to be correctly handled so this this was the situation before scholarship which was not possible it was a prism related Appliance to query a non-prisonment and also it was not considered for for example client to query prism because API this case uh so we had a few steps in order to remediate that so the question of the design document by that um also because it is kind of a breaking challenge we need a kind of future flag to be sure that you use like an API only when when we want so when you launch Ed APK and and by default it is still a Jefferson eventually I hope it will be by default we can API president IAL jftc cars in prisoners to convert more or less easily into bigger than any clear course sometimes it's easily there is one more mapping and sometimes it's far more complicated and then after right all the code to query so we can not using Beacon API it was basically what we did during this four months okay um [Music] but for the grpc API it's only the same one um you have to do some like translation between a grpc as drugs and diverse API structs you'll also want to basically mix and match you can see for example their purpose they conduct users to make an API endpoints internally to be able to engage your PC functionality so what we decided to do was basically create an interface that takes geodesic drugs and the reason we we adapt is to be able to let's change the rest of my code as we work instead like of course you added a feature flag because we don't want to break prison when we're working on it but also uh like to minimize the risk in the surface area code that we change we didn't want to change any of the the bilateral code except for this very thin layer between the beginning API between the beacon node and developer so as long as you pass the same geodesic drugs to this interface uh developers they need to know which security system it uses out there you have the job to see implementation and the rest API application that just implements this interface basically and which one will be used is just decided by the feature flag that might be explained um some some functions here you can see uses streams for your PC so those are other basically added to emulate the grpc string because obviously the the rest API doesn't have any stream functionalities um so to do that which is called the beacon API every uh for every like one second So eventually those streams appear won't be needed when we get rid of the Azure this is similar Edition entirely here yet um so I'm at the current moment all of these endpoints have been implemented so it's basically feature complete um so so now we're basically in theory we were compatible with other prevention so you can use the present value of your client with not presumably bigger notes like the human the other uh consensus teams and now we don't need to go to the uh well the applicant note still has this conversation but this conversion approach but the theories that in the future will be able to remove all this grpc Legacy code and just have a lot less maintenance Alberta and a lot less bugs surface um so the next step even though we're future computers a lot of uh thing to do obviously we didn't test this uh about the test Nets yet we did not even like they really like short-term tests so we need some more resolving some more end-to-end tests and we need to enable internet before the product was that worse but we didn't want to slow down the prison teams so right now the tests are just run after the personal version you don't like the present team at all but to create professions in the future we should enable them before the request gets emerging and we also want to enable discuss functionality in higher security Hive as the special compared just for prism and since prison doesn't have doesn't well developer didn't have any functionality to communicate with other people we can load clients uh the best special data just for presume and you want to remove it and start testing with the rest API we also have we also want to have more test passes that mix and match by letters and new constraints basically just to test whether it's fully confirmed or if there are still bugs are around and at the end obviously the only reason we're doing this project is to eventually be able to remove this maintenance burden and remove all these translation layers into the GRTC API um so that would be the final goal to remove all this Legacy script um that's it thank you I wanted to ask what is the Woodstock see from removing the grpc like is it the current compatibility like for upgrading the validators or why just because like the converter is like so much overhead right so yeah um so it's not uh we didn't test it enough yet so we want to be able to just have more testing done and okay onboard all those trespasses I talked about so that we're sure that we don't encourages any progressions that we're not there before yeah thank you um any other questions I just want to say on we have a present thank you sorry before and it kind of sucks we were kind of the odd one right there we heard them like interrupt but I think you guys and now that we're able to work without us thank you good afternoon everyone uh so I'm very pleased to talk about our work on defending gas memory pool against the Dos attack so my name is Juan ending and I'm now a phds candidate at Syracuse University and here is my thank you and here is my research group I work with my lab mess you should and boy and also my PhD advisor Dr Tom and uh the outline of my presentations first I will introduce some uh the basic problem uh we are working on and then we will go to the memory pool 200 tag uh which we will mitigate and then I will introduce the design and implementation of our designs and at last about evaluation of our defense so first we all know that uh in the blockchain and there is a transaction pool between client and minor and if the transaction pool uh got some problem uh we can't uh guess on uh confirmed transaction into it and the minor con mined some valid connection from it the blockchain will crush so uh here as the miner cannot read on confirmed transaction problem and it will cause some empty blocks generation and also will cause the revenue be lower for the miner and in the long term to say there will be less minor to this blockchain and it's very easy to attack for the attackers and for on the other hand uh the clock if the clients cannot get their transaction included in the block blockchain uh also this blockchain will lost the clients and uh so uh here uh we should we discovered uh the deter acts uh attack which is evicting pending transaction by Future transaction here we can see there is a victim convention pool uh we we have the uh two transaction fulfilled the connection tool and it all sent by the bulb and then we have an attacker node and here uh the attackers and the transaction which have the higher gas price in it and then it will evit the connection one from that victim connection tool before and it will turn collection to become a future connection because uh here uh the announce one will disappear and the announce two connection will be in future and here uh so so we already uh discovered uh two did her attack virus one is the data X which I uh just uh introduced and the second is teacher Z which is explored latent overdraft connections and uh to sum up these uh two attacks us all by sending the high price but on profitable transaction to evict some medium price but profitable transaction all right so how we can defense those attacks uh in the memory pool we just declined those uh transactions sending by the attackers uh and to the implementation where uh first to check if the transaction rate is future or overlating or latent overdraft transaction and then if it evicts some uh connection in pending pool then we will intercept this process of the eviction and decline those incoming connections uh and for the evaluation part uh the first evaluation is about the transaction full security under uh our fixed connection sequences we just set the initial state of the transaction pool with uh four thousand and uh 144 Penny connection which is uh now the uh the the food full state of our uh gas memory pool and then we attack the pyramid pool by sending uh 200 future connection to see if the pending connection will get less and the future connection will be more and uh the second attack is to send those future and latent overgrads connection uh combined and we see if there is some over uh latent overdrafts transaction appeared in the memory pool after the attacks so uh the result will well we we obtain is that uh there is no attack connection after we set the uh said the defense and the second evaluation is the performance evaluation uh we just use the test case in the uh uh on the GitHub with go to ethereum and uh it will send some transaction workloads and uh it will test the time of the workloads and the local uh the I think it's the yeah there is a time between difference between the old time and the new time here's the result uh we can see some of the time will increase but some of the credit time will decrease the middle and uh it will not increase a lot so the benchmarking is significant is not Civic significant worse so it's foreign ongoing work so we just put out this open problem uh so is our current defense in security enough or secure enough or uh are there any unknown crafted transaction sequences can deny those guest memory Pool Service uh and we think the father will uh solve this problem in the future and we are now trying to use the puzzle to get more uh concrete workers and very grateful for the support of ethereum foundation and my mentor Maris he helped me to benchmarking a lot and put our code on GitHub already but it's not merged yet and that's all I think it's a time to have any questions thank you [Applause] well hello everyone thank you for being here first call um I know we just ate so try for you to not to fall asleep um yeah so I'm Ricky my name is Alex I'm from Buenos Aires um happy to share with you what I've been working on uh for the last couple months yes so uh yeah my project uh is uh the implementation of the consensus layer peer-to-peer network interface for the heliosi client so uh first of all for those who want to check out this is the GitHub repo that I've been working on um yeah I won't display any code but if you kind of follow if you want to follow uh the code uh just here's the QR all right uh yeah just talking about my first steps on the EPF um so we have these project ideas um file so I just checked okay what can I do using rust uh Alex submitted some mbb stuff uh and and mbb repo for some utils and then also some consensus layers so I started I started checking both of them um did my first update on mbb but then the users like kind of lost so yeah what is Seal is anyway right so Helios is a rest space ethereum like client uh made by the existing team led by uh Noah Citron and so yeah I mean it showed up uh just explore the issues and started working on something for those that are not familiar with what a Lifeline is uh it's basically uh like basic implementation of a team node that will allow us to connect in a trustless manner with the tier Network in local resource environments such as a phone yeah hyper I know that maybe a few years ago but maybe a browser okay sorry okay asking uh first what can be improved so Helios right now is fetching consensus layer data using Alchemy or inferior you can set that up uh that's uh not cool as you may know so it's basically holding a trust Assumption of what the provider is sending to us um yeah so definitely something that can be improved so how do we do it we just have to implement ethereum because Slayer P2P network interface uh yeah not so much right um and adapt the architecture of the Helia sir uh my client to work with the maybe like the already working uh centralized provider pitching data and the new uh per experimentary interface maybe like maybe synchronized using the provider but then that's to the particular Network by the way like clients use a consensus layer data to verify this statement it uses the state route that it's called it's held in the consistent layer data blocks so we can verify this state on config section side yeah easy stuff yeah so my initial research was on ethereum POS architecture uh yeah spends a couple weeks doing that then the lifetime protocol and the consensus layer peer-to-peer interface in the last two of these uh I made like a a short post about yeah both of these topics um that really helped me to grasp these Concepts uh also like the main driver for for me writing this stuff is was like I had to submit an update for the APF so why not just make a small pause about it right yeah so that was really helpful for me and yeah also like to help more people understand what I'm trying to do uh following that initial research I started doing a deep dive on the periods per sec so we have uh the no Discovery protocol basically we need a okay no way I'll go into that uh on each of these topics uh we have a Discovery V5 uh we have the P2P which has a gossip self protocol and uh the request response protocol starting with the uh with display file so we basically need a way to find a repairs in the network this if I Works uh using only the UDP um transport so it's it's inspired by kadimlia the distributed hash table but we're not using that because um we have a few email Breakers which are not like Monty addresses uh these are flexible records and so you have to make this uh adapter to work with lipid V also yeah so this is uh the first thing I started implementing uh and working on on my minimal setup we didn't have live PDP and why are we recently PDP well basically it's declare in the specs that speed transport should be must be supported for every client uh it's a modular networking stack you can basically use that as a base layer but then Implement your own protocol something for that we have because so gossip Services general purpose uh published subscribe protocol so I just made this graphic as an example you can be subscribe to multiple topics and then receive messages on each of these topics that you're subscribed to you can also like publish to a certain topic um yeah basically this is this will be the way to that uh message um yeah data is Gossip between notes and then we have the request response protocol or break rest protocol this is uh mainly used to fetch a specific data from your already connected peers so you have a you have establish a connection and you maybe you want to just ping the node let's see and see if it's up or just fetch some specific blocks um if it's not an archive node uh you're you're not gonna be lucky but maybe it has to develops that you're asking for so that was the research part now let's get to the Hands-On um I made an initial implementation to get this network setup working this was basically for me to abstract from The Continuous code base um getting me to like a test environment closer to uh yeah something where I could iterates fast and find bucks faster yeah made like make make like the small examples from the elliptic appear um code examples or the DSP file code examples too yeah so I already mentioned this my between my initial challenges uh Discovery B5 needs to be adapted to lipdp since the regular stack uses mdns which has multi addresses uh addresses but uh since we have enrs we have to make an adapter between uh regular like period IDs and if you know records I tried to implement like this primitive non-generic data structures so what this is because like I a lot of the code that I'm doing I've been looking forward into the lighthouse client and so life is fine Lighthouse client has these generics to deal with every Network's Network inspect so uh it handles like not only manual but testnet and also like uh I think noses so I tried to abstract from everything so this code setup can be just as simple as possible you can handle on the manual data and yeah this is possibly a mistake I tried a lot to keep code simple I think I spent quite some time on that that's why I'm saying like classical premature optimization uh I think I should have focus more on get things working for having that yeah a couple more challenges yeah definitely peers are not that cool so for a bunch of reasons you will get this kind of um yeah at the beginning it was pre-scoring so me not supporting correctly some certain protocols from the request response protocol uh maybe also like just the uh if if you cannot be pinged uh you will get shut down instantly but also this is different on every client so another challenge was that I had to test with multiple client implementations because right now Lighthouse works but maybe other clients are not working correctly with with this setup the current capabilities of this project are I can find peer assisting discovering file for a good note I can subscribe and receive the existence of topics and uh I can communicate with uh peers with the request response protocol thank you all right so next steps so this is something I'm currently working on still have to adapt the current code base to meet healer Center so there are a bunch of uh reposites Or List modules from rust that I've been using that aren't the ones being used in uh Hilliards mostly I would say SSC encoding crates but also I will try to use some ethereum consensus traits which has some more standardized types so I can integrate them easily easily easier into Helios I have to implement a peer manager so right now I only like run a initial Discovery setup and then connect to every peer but once you get disconnected then there's no another Discovery step so definitely a peer manager has to deal uh has to do something to uh keep this amount of stable appears so you can rely on keep receiving data and uh yeah of course work on the Hilliard side to adapt the architecture yeah all right so uh takeaways from all of this work luckily for me internet communications are no longer magic so this I'm super happy about this like I got to understand how nodes actually communicate between themselves but debugging networking stuff is super hard I think I should uh get some better tone for this yeah I think uh I will do some research on networking debugging tools uh special thanks to Alex Stokes grateful from the uh he helped me a lot through this no Citron from the ICC he basically handed me a lot of stuff from billis and the lighthouse team for a great job that they do in their clients yeah thank you all for allowing me to explore this Garden uh with you all so thank you [Applause] all right um hi my name is Kevin and I want to talk a little bit about the past four months I worked on consensus client reward apis and as the name already says the motivation behind it was to develop apis that provide detailed reward data for ethereum validators and and this is the current state of the art before we implemented a project and it still is so you have on Beacon chain a blockchain Explorer different tabs where you can watch the data you're getting so for proposing clocks for making attestation or for participating in a sync committee and on the right side you can see these attestations are even split down to the source Target and head votes so our goal for our project was to provide the data on rewards paid to the validators and broken down for each of their duties which are as I already said and many making illustrations proposing blogs and participating in the sync committee and the project was or could be divided or can be divided into two big Milestones first deciding the weekend API endpoints weaken API endpoints is like a standardized set of apis for the interoperability between Beacon implementations of weekend implementations and the second big thing or big milestone is to implement those endpoints into one consensus client and we which means me and another fellow called NC decided to go for Lighthouse okay so what are the challenges and there are mainly four challenges the first thing was to understand basic questions because regarding the rewards so questions like what are adaptations what is a sync committee and what would the validators get paid for the second Big Challenge was to have an alignment between the quarters and so everybody is happy with the situation of the beacon APS eyes and how they should look like and the third challenge was rust so we we all knew to so NC and I are new to rust and we haven't had any prior experience and the fourth and last challenge was understanding the huge Lighthouse code base so coming to the conversation between the core developed developers of the consensus clients um it was really time consuming but there was also some really great feedback for example from SAR we um he's working on the numbers code base we received the feedback that he wants to um to show for the attestation rewards the ideal rewards paid which means if the validator voted correctly for head Target and source and correctly means they voted in time and they they wrote it right so how does our apis look like so this is the attestation rewards API you are able to provide a validator index you can see it here and you are also able to provide a public key if you want to which is here and you can insert the ins um desired Epoch at the end and the output looks like the following so this is the ideal rewards I already mentioned and it is array containing 33 objects from effective balance zero to effective balance 33 32 sorry um in one Eve increments and you can maybe you can have a look at what's the ideal rewards page if you have effective balance of 32 or higher and you could compare to the other reports you're receiving and you can consider the public key got converted to a validator index and also um the ideal rewards and the actual output of the rewards are the same so there are no opportunity costs um the second API for Block rewards look like the following and you can provide a slot and for this slot you get the proposal index and the rewards split down to each of their parts and for the third and last um endpoint similar to the attestation endpoint you provide a validator index or and a public key and provide a desired slot at the end and you get these kind of rewards paid for participating in the sync committee okay what is the future of the project look like so we implemented it into into Lighthouse and it should be released in about one month um but I also spotted an issue at at Brazil they also plan to implement it so I think it's a huge success factor for for our project and there is the this one guy Patrick working in Beacon chain and he's coming to the lighthouse Discord and mentioned that they want to implement our API which is really nice because um we use Beacon shell to verify that our Rewards or our data is correct and now they are the third tables are turning and they are using our API which is really nice so another honorable mention and there's this guy called lx88 um he's some random guy in the lighthouse Discord and I think this really describes the spirit um I experienced the past four months so he just took our open PRS and implemented them into his lighthouse climbed locally and tested him and helped us with debugging without any benefit benefits he received for it he was just interested to to have this API is working and here I'm I'm talking about a bug that I fixed for multiple entries for one validator which really shouldn't be the case here's some closing words thanks to my mentor Michael from Lighthouse thanks to NC for insightful discussions and thanks to Josh and Mario for running the EPF client [Music] um but you're gonna realize like uh most of the slides are not related to this because there's some challenges on the spec update uh there's been changes to the spec and and as a result I have to also change my implementation as well um and it's been a great time in respect so I kind of jump between different tasks so I'm gonna just talk to you A lot of them and so I don't really have a standalone project like like most people do it's just a bunch of smaller tasks subjecting explain to you we have enough time yeah so I've mentioned that so just about a smaller task uh mainly focusing on which is um foreign [Music] [Music] Lexia I'm gonna show some diagrams but if they're useful um so that's that's kind of like the main task that I was working on um upping through the API for 444 um which is um so Gabby and I initially picked this one because we thought it's uh something that we haven't started to expect on so there's an opportunity for us to start from the very very beginning I'm looking at the spec um drop them as big and getting feedback just starting to send it into them record the process because that was cool and initially we thought um this might be like a smaller task because based on the discussion between like terrorists and a few other guys it's going to be like one field change to the API so we've got up this might be too small so we need to add a few more tasks to our project and it turned out completely wrong this is still work in progress It's I don't know if it's like 50 done or less [Music] because um there's some recent changes the content suspect that required me to do a lot of update to it I'm going to show a prefer design um but it's mostly could be wrong because there's a risen change I'm not sure I think for people that following political form it's a quite a big change on decoupling the block on the Block on Gossip so they kind of change a little things on how we how we can do the uh the Builder floor because then we also require to sign the blobs [Music] so that's quite a bit of change and I still haven't figured out what exactly how we can do this so this is definitely working for us um quickly show a diagram [Music] thank you um yeah I was slightly tempted to steal Danielle's diagram because that's a lot better um this is slightly different um so this is me trying to understand what the end-to-end flow looks like on the Builder site for yeah before I fall um I don't know if it's too small [Music] so yeah so this is an attempt to try to understand end-to-end flow of where the blob is coming from and going to um and this is like now like quite our data now so I'm just going to spend a bit of time just to get through it because I've actually done a drop implementation in Lighthouse as well which is not going to be changed um but um this bit is like the the proposals um if you have notes so there's a bigger node there's a local yell and there's map boost and there's a validated client as well um and that's kind of from the user side where users send a block transaction it goes to the execution node and then it gets broadcast or actually announced to data execution notes in the network um and that's how they get the block into the the mempool which has to suit like a magic to me I don't know what's the um the deployment was going to look like um still um so um once they get the blob onto the mempool and then there's also Searchers and block builders that will try to receive transaction look at transactions and blocks in the mentor and then somebody that blocking the blob to the relays and the proposal side there's a the flow start from the Violator where they will try to get a binder block and then this display is a customer to what then you just explained basically that we can know where to go to I think you can boost to get a get a header back uh but the difference here is that we also now need to get the block on the Block code component back so that the latest could sign and there's been discussions on whether we want to get the food block full block or applying a Blog blog um so uh right now this is based on having like a blinded blob um flow so what happens is the radiator will start from retrieving the block and then the relay will return the blinded block which is the header and then plus a blinded blob back to the budget clienture sign and then um the client then be able to sign the header and sign the um the binder blob as well and there's something back to relay um and then the relay would then um broadcast the blocking blobs and they're also we return the review payload in the blobs back to the beacon know which that we can then broadcast again so this is my final but just for people to show like the kind of work that I've been working on um yeah so the next one so yeah there's been quite a few tasks related to this I've put in a bit of a bunch of links point out that's super useful but um there's been a quite a bit of time with Gabby on the spec updates so initially we'll start with that because we thought it would be good to have the specs out early to start some discussions so we'll drop this back and then we realized oh we also need to update a couple of stuff as well so it's kind of like a dependency so we extend our scope to also update Capilla um spec as well um and that PSB merged and then now we also have the internet um we are separated now because he boarded before it's been pushed back to the next walk so that's a deal working process and progress and which is mostly updated now um I'm not going to go through it too much um so yeah so that's the time that I spent on the computer update so actually I ended up spending a lot of time under stuff so I'm going to go through that foreign version of the previous one except that um it's become more complicated and potentially outdated now as well so I need to another version go through again but the main thing is that uh we experimented um separating the um the public of walking blogs from the beginning side which turned out people to be a bit of a problem and because of it more complicated because we needed a way to put a bigger note to to to review what I'm buying the blogs so um that means the value that client will have to always talk to the same Beacon mode which is not always the case if you use something like that or if you have like [Music] there'll be problems with um blinded laws so this could be a slightly more complicated we haven't found a solution yet um I think right now I think it might be easier to people just combine block and blogging in the uh in the summer you know this is also not fun and then the other component in my project is the um the testing video yeah before um initially I at the beginning of the project I looked at the erp4 4.0 Philippine created by Murphy optimism I think um and um I think the house was missing in that report and then I thought it'd be cool to have it because that contains um the docker compose file for with other other clients like client as well so you can test the interval um and also tested a few main features yeah I think I think working there some some tests that I really supported for foreign V3 uh we should probably know that news for now because we've got the D4 B5 and then I also did a um did some work on the download API in Lighthouse um it's kind of just the help of testing because before we had that had that API we were mainly using the request response interface to create the blobs and that turned out to be a little bit of challenge that's similar to what we're actually explained about um appears are not that cool so you have to implement like before I get to implement all the the the topics in the gossip so that was slightly painful but I think we've got that as well but now Bitcoin API kind of make it a bit easier to test because you can just like create your own book and then get it back um quickly show the response from that API but now that's that's all going to be changed as well [Music] I'm pretty sure this is next slide so basically uh this endpoint allows you to like provide a block ID or like a slot number and then get back the the blobs this is like the opener clubs um so like the block arrays empty and then you've also got an empty I create a good proof If that doesn't exist anymore in Spec so this is this needs to be updated um and there's also an example output blog as well that's also on my list update and then that's testing and then the next bit I looked at was um understanding Lighthouse and the ability to work on Lighthouse which is which are like less related to 4044 um I've got some notes Here on like my initial attempt to understand try to understand my house [Music] this is too small yeah according to too much detail but um [Music] uh this is kind of like the the like what happens when you start a new Lighthouse mode um you have like a main like main component where it creates that you can know um object and then it's got a bunch of stuff in it it's got an API server metric server and then a networking service where you'll get like messages from from your peers and you have to get them passed on to other component process and in this diagram I was focused on understanding the beacon processor which uh maybe look after like um blocks that come that comes in um and then um so what happens when you get a message is that it um the network what do you servers passed on the message to a router which then pass on to pass on the message to a beacon processor that that that's once um I've worked interest in parallel and processity um the tasks so um this initial exploration turned out to be helpful because um it helped me in doing the next things I'm going to mention oops um which is the uh another issue on my house that um about that fuel sync so Battlefield sync is is um is a mechanism that that happens when you use checkpoint sync um to another Beacon note um if you download the latest finalized date and then um and then there's four or six that happens so then it goes into the latest book but then there's also a black fuel sync which allows which which will allow test to sync all the way back to Genesis um I think there's also Open PI and Lighthouse to to just limit that to like the wizard of Defense we would subject to the point which is like five months so that will decrease the storage requirement for CEOs but this one's slightly different this is about red limiting the backfield processing um and the reason for that is because there are some some people for the community that that's that's raising the issue about the CPU consumption when they initially sync um it looks like to see if you just get hammered from like all the backing processing because it doesn't get very limited so just keep processing it until it goes back to the Genesis so sometimes it requires a lot of CPU power to do that um and the point of doing this is to to slow down probably not super important for most people you're only about a data you don't need like all these historical data it might be useful for people like that once archive note that we want historical data um quickly so um I actually had a feature to also allow you to top with that on and off so if you run archive node then you don't want to resume it you won't just want to do it as far as as you want I guess you'll do that but for most people we're going to rate limit by default so unless you want to write that um could we show a design but it's slightly to the level I'm not sure if it's going to be useful for everyone but the existing how I work within lighthouse we're going to Too Much low level I mentioned before that there's a networking stack that pass on the message to a beacon processor and when that fuel happens we get the um the backup batch in the blocks from a Event Channel and that goes the bigger processor and then it then processes it immediately for the people it kills it so this is what happens currently it just keep looping through and then see if there's any work coming in and which doesn't get very limited so it's constantly Loop end and the solution that was proposed by Lighthouse team was to add a very limited in between so rather than processing this immediately um you have any intermediate cue we should probably get a bit too small I'm not going to go to too much detail but I'll share this slides as well but basically what happens is now there's an intermediate um really limited queue so whenever we receive this it's going to send to that Cube and we don't process it straight away we process it at the latest timing schedule right now we've um did a bit of performance bench versus benchmarking just to see the CPU impact um right now we're doing like three times within a slot so we we processed it three times um so every mesh is like two also 60 64 blocks so we process three batches in the slot so it's not like previously we'll just do as fast as we can usually go to that four or five bashes per stop but now we're just like slow it down to three so it happens at six seven and I think nine or ten seconds after it stops starts so it doesn't impact the violate as much because that happened in the critical window which is like the first few seconds of the stop so this is now like a scheduled processing you need to um so that's one of them and then the most recent piece of workout is I've been working with the blob signing bit so that's also like some new work that came up from the freezer blobs um via decoupling of blocking flops so now uh we have to sign the gloves that gets propagated to the other nodes so I've been working on this now and this is still uh um uh not found last year because there's been discussions on on the blob signing as well whether this happens as a single compliant request or whether it happens separately to the block so there's an open piano specs on the consensus specs so I have a PR here but uh still also needs to be updated depending on what outcome is which is like happened to most of the world I've been from I've been doing so it must be durations but I don't think it's way Sports it's just good learning like I mentioned yeah like I love the worker working to do work in progress um there's lots of spec change along the way and sometimes change can take a while because um you also don't want to get this back emoji too quickly and then modify later it's good to have some interest from other client teams and also the people that also been doing it for experimentation they might have some takeaways from it and that contributes to the spec so it's generally not a bad thing and it's good to have like draft specs out there these are starting discussions and I've also explored that it's quite useful to also Implement a POC before the spec is finalized because I just I I can find sometimes I find it useful to discover potential issues which happens to most of my PRS anyway um I'm so happy that someone can't emerge something I still [Music] very helpful everyone that has helped me as well thank you yeah so hi everybody so I will present so part of the fellowship I was working on the bundler implementation for the a new account obstruction proposal in Rust so the motivation was uh this quote from metallic he said account obstruction has for a long time been a dream of the ethereum developer Community uh there are two things this one is that account abstraction has been a dream so it is something that is very good is very beneficial and it also made in the resources It also says that it has been for a long time uh so there were many attempts to do a con abstraction there were three Improvement proposals so I think that first the Improvement proposal is like many years old already uh but there was like problems with first two proposals is that they wanted to make some changes to the evm to add some new OP codes and things like that which is like was a problem because it was never the best time to do like these exchanges to the evm because there were like other priorities like the EIP one 559 or like the merge and now there is like proto-dunk shortening so the last proposal that was introduced 457 is trying to do a count of abstraction a little bit different way uh so basically the idea here is that avoids consensus liar protocol changes and instead relies on a higher level infrastructure so basically the consensus and execution clients don't have to make like don't have to make a single change you just introduce the new entities that work with the execution clients and basically you can run account abstraction without without making changes to the poor clients um so basically you have you introduce uh several entities in this model so one are the bundlers which I was working on then you have like these entry point smart contract this is like the Singleton smart contract so there is only like one on the blockchain and basically all user operations like garbage already mentioned today uh goes through through this smart contract uh so basically bonders submit the user operations to this smart contract and then this smart contract verify the signature and also calls the paymaster so play monster is the entity that can sponsor the gospel the user operations and basically execute these user operations which contain this the smart contract calls uh for that user operation and user operation is basically very similar to the transaction it can also contain other like signature schemes um so back to bundler a binder is some kind of node equivalent component for the count of structure so it can be run by anybody um it has like several jobs it receives user operations for smart contract wallets it then validates these user operations it bundles user operations into bundle which is like a normal transaction that is in the end submitted by the bundler to the execution client um and it also has like this peer-to-peer protocol to exchange user operations with other bunders um so the goal for this Fellowship was to like implement the bundler in Rust uh like to do a full implementation from the ground up uh and to pass the boundary spec test that was that were released by The ethereum Infinities and this team in the December and then to try this bundle with one of the smart contract wallets uh so basically these these goals were set in October um so now we are in February we are in March already and some goals were a ship but not all so basically I will go back to that so uh this is the architecture for the bundler so I wanted to like split some responsibilities into different components this is like similar to the what is Aragon is doing with their execution client so basically you have I have like three main components which is like the Json RPC API then we have like user operation pool which is similar to the transaction pool and we have like this bundler core logic that is actually bonding user operations and so we have like outside of the bond that we have like ethereum execution client and also ethereum execution client but the difference is that here is like the peer-to-peer protocol to exchange user operations with different bundles and here we have like when the bundler submits the bundles to the execution client um so this architecture was interesting for me because let's say that you have like specific logic in how you want to bond the user operations into bundles so you can do you can just sort the user operations and bond like the ones that pay you the most or you can like support one time master or you can like have different Logics and with this architecture the idea was to like if you like run components separately you can just pin on all the bundler instance with the same user operation pool and with this new bundle instance instance you can just have different Logic on how to bundle the user operations into the bond uh there is also one thing that when the binder submit the bundle to the execution client they have to communicate with both producers or use flashbots uh protect to prevent front running because like when funders let's say they find some smart logic how to combine the user operations bundle and when they submit the bondage the execution client they submitted public transaction tool other bundles could see what did they do and just steal basically the bundle and pay more feed and basically front run the other binder so there are like some solutions to that um yeah so here are like the details what had to be developed uh basically sanity checks simulation mempool verification interview protocol and X and communication with execution client node execution layer nodes maybe I didn't select the best color for that but this thing is darker this thing is darker this thing is darker so basically the dark things are already implemented the notes of dark things or not um so sanity checks these are the base these are like simple checks when the bonders submit user operations so basically they just check if all the fields are shut correctly every bundle also has the simulate user operation this is basically calling the handle Ops function of the entry point entry point smart contract and then the debug API for The Gap is used to basically get like information which op codes are used for this user operation because some op codes are like forbidden because there is a time when binders receive the user operation and then there is a time when they submit actually user operation to the as a transaction to the node and between this time like some things can change like the block numbers and things like that and basically if the user operation contains this block number after sometimes this user operation could be invalid but at the at the submission time from the smart controller wallet it could be valid so basically some of codes are forbidden and there is also like a replication model this is basically a each binder keeps reputation for pay Master factories because it's like some pay monsters wants to cheat it will get banned by the binder after some time if like it wants to submit to the invalid user operations and this is basically to prevent denial of service attacks um so basically the mempool is already implemented there are like two implementations one is just simple in memory so and the other one is with database so it's first it is persistent if the bundlers shuts down um and also support for multiple mempools is already implemented because like I said there is a Singleton entry point smart contract but it can turn out that after some time it will be like one block in the entry point so the entry point can be upgraded and there is like this time when the entry point is upgraded uh basically both entry points will be used for some time before like all smart contract wallets uh go to the use the new entry point so the binder should support multiple manifolds and that and also the idea is that everyone can Define his their own mempool which has like different rules which user operations are allowed um but at the moment there is like only canonical mempool and so peer-to-peer protocol was defined I think it was three weeks ago um the idea was to use leap peer-to-peer and sscz um because the idea was I mean the leap peer-to-peer is like I think that the execution clients next to also replace that peer-to-peer like in the future so the the idea was to use like for the boundaries peer-to-peer so after sometimes the peer-to-peer protocol won't have to change um and yeah that's it uh so here are the specters that are like a lot of fails um like some some tests are passing like this for beaten up codes uh also some tests are passing but the the basically it took a long long time to set everything up um but now it will be much easier to continue because like for some tests there is like you have to just Implement one function and check like one rule for the user operation and it's like it can be like 15 lines of code to just pass some additional test but for this just to run this test you have to like set up like Docker files you have to run the get node in development mode you have to uh like deploy entry point contract on each uh run and things like that so it took quite a lot long time um so in the future the plan is to finish this implementation paste the remaining tests uh there is also the idea to develop some clients library in Ross for the user operations um or extend eaters library for that um because the ethers plus library is very good but if you want to do some specific things or maybe things that are more new and not so much used by many projects it turns out that this Library probably won't support that functionality so you have to do it yourself and basically the idea is also to use ref in some way so one way would be to add bundler component to the ref or because graph is developed like in like in a very modular way with many crates it is also possible to reuse like one of the traits of the ref maybe for the deaf peer-to-peer protocol because uh basically the bundlers need to listen to the execution layer nodes to see which user operations were already included in the blocks so they can remove these user operations from the mempool uh so there was like kind of idea to show the demo um but now that I'm the Mario computer is what happened um but if someone is interested we can talk later um yeah so some links so I was working on the binder with will he was also the fellow that was working with that he's actually very good in Ross so it was interesting to work with him so the mentors were how often the roar I don't know I didn't put his name uh and here are like some links to check so this is like boundary implementation and like anyone wants to ask some questions after the if number and that's it thanks percent okay hi everyone um so my name is Andre uh from my uh project I did a kind of a self-sustained work on something called Pavilion monastery they're expanding this thing called that's the first like a little bit about me I think I did it so I'm Andre I graduated a few years ago I spent a year and a half working at this database company on the distributed database on the Kernel team um and over the last year I've been doing uh application Level research at this place called CRX Park which is um has some interesting experiments that I'm happy to chat with so there um so a little bit of background of why I'm interested so so how this came about so I got interested in core development and uh heard about EPF along the merge it's just an amazing technical achievement um and then after the merge I got interested in Mev so um one thing that was kind of yeah caught my attention was the adoption of boost um like and these things really is popping up and uh had a pretty significant yeah very significant adoption a lot of uh just plots being delivered through essentially like uh one relay and one Preposterous like one single source code um and so yeah I got interested in this interaction between Med and the protocol um then I got interested in short-term uh mitigations because we all kind of know now that there's in protocol PBS ideally soon but um like what can be done in the short term repeat my interest and I stumbled upon the relay monitor repo by by Alex Stokes um and kind of this was also on on one of the suggested projects but but first like um why do they monitor so I'm gonna try to keep this very brief and we kind of talked about I think there was a mentioned enough boost relays before but like this is kind of the reality right now that there's like barely double digits of relays right and a pretty high percentage of loss being delivered through a relay so um metal boost really is an auto product called PPS implementation where um blocks are constructed not by food evaluators they're constructed by Builders and then forwarded through this relay that is required for multiple reasons but the fundamental reason is that Builders and validators don't trust each other um and uh yeah like at a high level like others opt-in so it's the opt-in they don't have to run Netflix but they a lot of validators do um and the relay kind of signs up the map boost relay uh signs up to like signs up what it promises to do is it delivers uh headers to um validators to proposers when it's their time to propose and it kind of signs up to deliver payload so it promises hey like if you sign over this I'm gonna give you the payload so then you have a block and then you like got proposed into nothing um and a proposer in turn like they also sign up to do certain things they they enter in this kind of contract but they say well um yeah give me like when it's time to propose a block um a validator says okay what kind of header do you have for me and like how much value can you give to me how much Mev did some block of their track and then they kind of wait for the payload and hope it happens I hope hope it hope it comes um there's some things here that I already mentioned like dominance of my Boost another thing is that is just like another places to go to go binary so um there's there were already issues where like because of improper validation uh by the relay uh people could submit blocks that had enveloped Builders submitted blocks within valid timestamps and they got uh forwarded by the relay to the uh proposers asking for a bit but then proposers checked themselves they were like well this timestamp is incorrect and they had to fall back on uh local building or like yeah so so not that bad but still like you could just see that the relay is in the middle if it fails to do something there's direct consequences to the proposers who like often communicate via this free line um so yeah basically the relays are in like a privileged position so what a really monitor does um kind of fundamentally is it connects to relays so you specify like in the config like which Real Estate which relays you want to uh Monitor and it checks the headers so I'll mention how it doesn't it can check the payloads by the relay and it computes scores for its relay based on some 50 years so this course I just I kind of came up with them but like uh yeah you can do multiple things there um and uh after doing all that it exposes an API for records and stats so like if somebody the point is that to kind of surface this information out for anyone who cares for instance a validator who wants to get some influent to what the relay is doing um and yeah this this little point is um not that important it's basically like it'll it's another way to kind of allow validator input into the behavior of a relay so so a validator can essentially make a post request and say look um here's a block that I received from this relay I signed over it here's the payload like something went wrong and you can kind of um yeah you can you you can kind of prove that something um there's just a simplified diagram again like um without a protocol PBS does and what this map boost really does is that like it separates um this like proposer and block videos right um there's different apis here and so what the relay monitor does is over this thing over here right it talks to the relay API The Proposal API and like there's an opt-in way for portfolio system into Data so it's like it's it's like it's a watch Network right um yeah um I mentioned this already but um one of the like there are two pieces of like opbs first is like headers and then block bodies because uh you can't just have a builder submit the entire blood body because otherwise they can get stolen uh so by the proposer so um for the demo purposes I'm gonna show like header validation so a really monitor can validate like header Theory receives through uh from the um from the uh really uh basically it's like a header a bit I use them computer change um so what I did I mentioned started with Alex's repos the foundation I added a bunch of stuff this is just a subset but uh bit storage and Analysis so like every bit that uh it just stores everything that it can find from every relay it also stores the analysis which uh it performs validation of the headers which I'm trying to show um it allows for time-based queries you can ask for like give me like old bits that this how many bits the specific really deliver between these slots you can actually get like records of faults um they're scoring that are implemented like for reputation and bit delivery rate uh just pretty simple uh like approval Concepts um and uh there's also like these operational metrics that are I suppose um basically like how what's uh the monitor doing um yeah I wrote up a spec um it's kind of just a little progress and then I created this like build a bit faucer because I wanted to simulate uh map boost relays that are faulty because super super like even the one in sepolia the one that flash was runs it's like it works but it doesn't really fail that much so I wanted to simulate a bunch uh so I just like wrote up this thing that can seem the default um uh yeah so I have a few quick demos um I recorded them just for the sake of like simplicity so so I don't keep like switching back and forth but uh yeah uh uh let's see I don't know is this big enough cool um so yeah first like pretty simple uh yeah um you can see like endpoint monitor fault stats also I'll mention that like I put up this back on like API that really monitor that plays XYZ I just like typed this up so you can check it out if you want like the demo is just shows uh like requests the the you know in Postman but it just implements this spec um I just like wrote up a bunch of stuff here uh yeah so um monitor if you want fault like stats endpoint you send the request and you get back data that looks like this so it's like a report uh for uh all the relays that are monitored there's like a that gets indexed by public key so um this is like you know some sort of relay identify like that you can see hostname this is the flashback Builder on sepolia um and you know over the last like whatever slot you can specify by slot bands it delivered like 914 um so pretty simple so far um sends the request um you can do individual relays so nothing to you can specify a public key in the route and you get stats just for a single relay uh so pretty simple here um yeah like for zero faults for this volume um the other uh route is like the actual records so you can like request a record report first it was the stats report this is a record report So like um it actually gives back a list of consensus and database so it's like everyday this behaves and they're and they really matter the text like look there's a bit that violates um you know yeah but it's invaluable respected because in this case you can see like the parent hash which is like zero isn't so it like recorded a bunch of these bits dumped them into the uh like a database and you can like query four times here essentially it gives you some info like like the proposal Pub Key here or like slot and what exactly was wrong with it and it's grouped under the thing that I didn't mentioned is grouped under construction valve bits so there's different faults and they're really modern I can like track all of them um yeah so there's like a list of all these faults that are just tracks see what else is here yeah same thing like per relay record um other metrics I just I'm showing validator account so these are like relay operation metrics it's like how many like bids that did uh monitor analyze how many of them were valid so pretty simple endpoints and API um okay so this is kind of stuff but explanatory um the next thing I wanted to show is like some of the score so here um there's an endpoint uh Slash score slash reputation and um this um uh what's Happening Here is basically like the relay monitor refuses the information that's dumped that it has like all the records of all the faults and it computes um yeah like a trust board in this board so right now this score is pretty simple it's just some like exponential like based on recency like if default is very recent the score is plummet um and if you know the more time passes and if the relay doesn't uh misbehave the score increases so um in this little demo like I show so I send a request like this score is 100 this is like score 32. uh for this like fake relay that I'm running now it's jump to 39 so every slot there's no Faults so the score keeps increasing a little bit by a little bit um let's see what uh yeah okay now it went up to 45 so you get like really doesn't misbehave um score increases so uh what I'm doing next is I'm spinning up like a faulty relay I'm just like using this like fuzzer little tool that I made um and now I'm enabling some sort of fault so I'm injecting fake fault in this case there's going to be some sort of like this oh yeah I'm using the wrong public key so the the bids that this fake like local relay that I'm running uh the bits that is going to be delivering are going to have some mistakes in it so they're going to be signed by the the public key is going to be wrong so the relay monitor is supposed to detect it and so the score is 45 and now scored jumped down to zero so like penalized that it detected a bit it figured out that it's very recent and it like dumped it all the way to zero um yeah because it's using it that public key uh similar and and so um what happened there is so the score went back back down to zero because it's like a critical fault right but then um you can kind of imagine that maybe it's like a bug in flashbacks relay or whatever so it's got penalized I shut down the relay so imagine like people the box has been fixed and so um now if there's no fault the score is going to keep going pretty soon um and yeah I think um the other demo I had was just like uh there's another bit delivery score I'm not going to show it just because it's it basically just computes the ratio of in the last and Slots how many bits were delivered um so like for instance uh you can maybe imagine a validator might use this to say well this is a relay it just stopped delivering a bit it's like what happened I actually saw this like it was funny on Tuesday um since separia got upgraded the flashbots uh um relay just stopped producing bits like it just like for for a few hours and so like you know this ratio of the delivery score which is completely since like the relay is not delivering it's um yeah uh okay so yeah there's a few links that I wanted to it's just like some work in progress um repos like the whole father my implementation like my Fork of Alex's repo that's uh yeah this work in progress It's Gonna I hope it's gonna be emerged back into the original repo so it's all in the same like soon TM uh uh back into uh like yeah just aggregate mine and Alex's repos I just forked it in order to commit like working progress code without waiting for code reviews um and uh lastly I wanted to to finish with this little meme but like uh I think MVP is very interesting it's to do like a pretty big problem or a challenge and it's it's very difficult to design like things that quote unquote solve uh Med so this is not really about that it's more like illumination of activity and like kind of seeing like what's going on um and potentially this like Watchtower just can act as a source a source of information about Theirs to see like what's going on with them what's going on with the specific relay it's not much but it sounds work yeah it's like finally thank you uh Josh Mario for the opportunity to be here and uh extra Alex for starting the original version yes [Music] I think it should be both but but I imagine that it's probably useful to have like a kill switch kind of thing like I remember coming across some some designs that suggested that if something goes terribly wrong the consensus climb or something just just shut off you know access to uh yeah just shut off that enough boost um obviously there's like challenges right that people can grief and cause a lot of value disconnect from Blues but um I think it should be both I do think that like for things like the delivery score I think is less important because it's like you know it may be like a Valor who wants to maximize their like profit wants to you know automatically switch to the relay that delivers the most bits maybe they can like themselves or maybe do it manually or for something like faults uh I would say that I can see it being useful to be in math books so members can like disconnect as soon as possible from a rebate that's faulty until the score goes back up based on you know no Faults and like end fault in the last end slots whatever yeah uh it's a project um no it's um so I have a thing running on sepolia like uh monitoring like the the building there and the stuff that I showed so as I mentioned there's just not a lot of interesting activity because like that built it doesn't get that much that activity and it doesn't fail so much that's why I came up with this like closer to just make artificially failing realize uh but no yeah I've been I've been monitoring the one on uh because they kind of force more remote operator to use eBay or no operated doesn't know as well so I think this would be very interesting yeah for sure I think so um I think just some things like custom remains and then ideally like it's emerges Alex's repo we just figure out how to combine This and like yeah for sure thank you so much [Applause] talk about some of my work that I've done open Fellowship these four months I had my hands in a few different cookie jars here but what I started off with was eofv1 and I was learning quite a bit about the implementation of eof uh EIP 5450 which was stack validation and the whole motivation behind eof is that if we increase how we if we standardize data in our headers across the execution layer and how we handle a contracts specifically within the ebm we can actually reduce the number of checks we have to use at runtime so the evm can run a little bit leaner and meaner um there's a bit of a Fallout between operated wreath and so while aloe Phila got deprecated I wasn't able to translate directly into wreath so I transitioned my project into uh ethers RS so what I did with ethers RS was actually combining event handling with write operations to the blockchain so the first thing that would happen is you would instantiate your contracts in your provider and then you have a trade so traits and roster shared behaviors across different types and say you have the approval function from the rc20 right you would create a structure that is that effectively mirrors it there are some declarative macros you'll throw in and then you'll be able to throw in your contract instance your function selector the arguments and it will run you through so the first thing I do I want to get into the function body is ask hey what block are we at and so then I'll be able to tell how many blocks have passed since I have made that original call in the middle I make the call to the function on chain using Json RPC and lastly what I'll then do is make the separate call to the events try to then get the event from The Trial and lastly I'll wait the six blocks to ensure that there are no short-term reords in that we have been successfully included into a block the next steps for this will be using wasn't mindgym to be able to Port this over to JavaScript and python as well I've been talking to some of the maintainers of web3py for this on the other end of things I also wrote one of the first fuel Improvement proposals in support of our bustling layer 2 ecosystem what I have proposed is that you so there's a the big difference between fuels execution layer and EDM is the utxr set versus a non-spaced account model and so uh coins are added to the Ledger in such a way that it's in the utxosa and not in contract State storage and so what I want to try and do is enforce consistency um between how we handle all sorts of assets on that ledger so my suggestion was we have a contract ID which we can use to derive asset IDs in the utxo set and there was no way to select specific utxos I didn't want to have to use sat selectors kind of like they're doing in ordinals might have been a little bit sloppy so I decided to take some inspiration from how cardano took an approach to including nfts in the utxosa and given Fields unique architecture what I ended up doing is we can apply a bitwise operation to the contract ID and then we can use that to derive a unique ID to interact with the individual utxo being produced from that set and so that is the specification for that and my my main reasoning behind even implementing this in the first place is that it's very nice to have standard Behavior so that both the developers working on the application layer and the developers working on the protocol layer have high levels of consistency and that behavior is not broken between similar types of actions you could take on chain um the last thing I've really done around here was some research on vertical trees and the implementation of vector commitments and so my idea in the research on continuing with is separating out the individual Vector commitment from the different polynomial commitments so the motivation behind this is if we need to use a vector commitment that's binding if we separate out the polynomial commitments we can actually batch them together and as a property of the type of polynomial commitment we use if it's hiding we actually don't need to enforce that property at the level of the vector dependent so our individual polynomial permitments are just commitments about data but the vector adds the additional layer in terms of position-wise commitment about that individual data so I think we can knock off some individual terms for the vector Commitment if we are able to successfully batch together all these individual polynomial commitments all right I think [Applause] foreign uh what happened uh technology and place their ideas out and don't happen very important uh foreign together is everything um resources foreign that's it again so control um if you want to participating important perfect the ability from each other um you have to send out you don't need this function actually you just need one more function which is will uh inherit this is very relaxing inside so here for example so here for example if you see I get a context from the content for those who don't know 337 we don't have transactions anymore we have some good condition options scheme but in user operation you can have a custom signatures [Music] the idea is again to promote account obstruction in hackathons so you like your smart contracts ready and working and this is I am struggling with the fingerprint one if someone is good with solidity please help me I don't think I have enough time but I'll be happy to Showcase it yet thank you thank you it is uh imaginary questions feel free to ask here or online um yeah I hope you can hear and I'm sorry about the fan [Music] yeah okay if you can hear it right [Music] this is the first time hopefully people will use it there is to release it uh so drawer will be showing showcasing this um thank you any other questions thank you thanks so much 