okay we are live hello everyone and welcome to the young shopping uh seminar today um we are going to have two workshops um the first workshop will be lead by uh dankra himself for the dog shouting and it will be like about 14 minutes and the second part would be uh vitalik will show us the his pre eip of the shouting format transactions that would be like about 20 minutes but the whole workshop should would be very interactive so if you are in the zoom and you have some questions or um you want to discuss something that you and you can just raise your hand and we can have a quick communication in time and then after these two workshops and we will have um like a discussion time for the engineer question and challenges and any other question qa so yep i think it is oh and thanks uh ech and project help us to do the live streaming today so yep then let's go oh by the way can anyone try to the um try to see youtube qualities okay anyone yes it works fine oh thank you and also we have we are recording the call so thank you okay then start cool hello everyone thanks for joining our workshop yeah the workshop is uh yeah called dude what's the dank sharding situation um when you meet our lovely new researcher george you will understand where the title comes comes from sorry that this is probably the joke is probably not working for most right now but uh i promise you will understand it at some point um cool um so um i'll start my talk by talking about what's old so basically um how we uh what we already um designed with charting and what we've already talked about i will give a quick recap about what data charts are for how data availability using kcg commitments work as a reminder i gave a presentation about that about a year ago so i will link to that later so for the details about that please have a look at that um and then then i will talk about the sort of original old sharding design where we had separate chart proposals for every chart and then i will talk about what's new and basically the two things i want to talk about is proposal builder separation and cr lists which are kind of the basis of why why what enabled this new design then talk about the 2d data availabilities game using kct commitments and then and then show the architecture that i'm suggesting okay so what's old so data shards they provide data availability for rollups and other scaling solutions and the meaning of the data for data charts is defined purely by the application layer so the ethereum based consensus will have no responsibility other than ensuring that the data is available and every everything that is about the interpretation of the data as in executing transaction and so on is defined by the roll-ups or whatever else you're running on it and and the consensus notes don't need to care about it why don't we do execution charting anymore because at least we believe now that roll-ups are or can be about 100 times more efficient than native execution on charts and it's hard to see in this context that actual execution charts would ever be in demand um so the goal is to provide a data availability layer that will provide about 1.3 megabytes per second uh with the full charting uh rollout and that's about 10 times the max capacity that we have now and about 200 times the typical capacity and this is the ethereum target since about late 2019 um okay quick recap on data availability sampling so what we want to do is we want to know that o of n data is available but we only want to do a constant amount of work so we we wanted to scale and the idea is what you do is you distribute the data into n chunks and each node downloads k randomly selected chunks so data available sampling you have the data and the client samples these blocks of data and um okay so what if one is missing well in this naive way where you just um do this on on the data that you want to prove available itself this is a problem because if one is missing the client is very unlikely to detect that using their constant number of samples um but even a single piece of missing data can be very important um it can be the one transaction that prints a trillion ether so so this naive solution is uh is unacceptable in in the blockchain context at least um and so uh what we do instead is so-called erasure coding um we have uh data that is extended um using a read solomon code which means it's uh the data is interpolated as a polynomial and we evaluate it at a number of additional places um so in this example we we extend it by a factor of two so we have four chunks of original data um we add another four chunks uh by interpreting the state interpreting this data as a polynomial of degree three and um and now now sampling becomes efficient because um if uh if i have any fifty percent of these blocks due to that being this polynomial of degree three uh so if i have four of these blocks i can always interpolate the polynomial um and so um for example uh now if you imagine a lot more of these blocks if you create 30 random blocks if all of them are available then the probability that more than 50 is unavailable is less than one billion two to the minus 30 and and so this becomes extremely unlikely um but there's one caveat we somehow need to ensure that this encoding is correct um [Music] and this is where we use kcg commitments a very quick reminder on this as i said like um it's best for a detailed introduction to watch my presentation from last year um no yeah i'll share the slides um afterwards sure um so if we have a if we have a polynomial um then a kcig commitment gives you a uh a commitment to this polynomial um and the nice property of this is that um you for any evaluation of this polynomial so any uh y equals f of z um the prover can compute a proof pi um and using this commitment pi as well as the position and and the evaluation and the verifier can verify that this is the correct evaluation so this is a very nice property it basically gives you a commitment where where you can prove that um [Music] that each evaluation that you that you reveal is indeed on a polynomial and um and this commitment and the proof are both one elliptic curve element on a pairing friendly curve so using bls 12 381 that will be 48 bytes and the way we use these as data availability roots is that we instead of publishing a merkle root we will be working with kcg roots for the data availability and what it does what we do essentially is that that we commit to a polynomial and then we say say the data is the polynomial at the positions f of zero to f of three and then we extend it by evaluating the same polynomial further at f of four f of 5 f 6 f of 7. um [Music] yeah and and um yeah as a further like we can easily prove with kcg roots that's another additional feature that the polynomials of degree 3. um so it's very nice because we get this guarantee that all the evasions are indeed on the same polynomial and we don't need any fraud proofs or anything like that for it cool um so let's have a quick look at um how sharding worked um uh like as it was specked out say be before december of this year um what what we what we had was we had we had the beacon chain which runs here on the left um and then we had shard um shards and each chart had its own proposal like he had a separate proposal um selected from the validator set and then there was a committee that that verified that this this proposal was available so no this is not a data availability sampling this is simply the committee itself downloading the full shot blob and confirming that it's that it's there and voting for it only only if it's available and one of the problems with that is that um it's uh we we cannot like if we don't want to introduce very tight synchronicity conditions we cannot guarantee that this voting will be completed within one slot um because that would depend on many things including whether the next big block proposal is actually able to include the votes from all these committees so basically this this introduces uh a bit of complexity because each committee needs to be managed each committee needs to be tracked uh on when when the chart block is confirmed and um you also like don't get guarantees when any particular shop hello mike okay maybe just okay all right okay good okay no question then um yes so this is the um original charting design that you're having design okay now i'll come to uh what's new with this new design and so i'll summarize first um what are the what are the new developments that we've had over or that have been developed further over the last year that that we'll need to construct this um the first one is so-called proposal builder separation it's a it's a construction that was invented to find fight the centralization tendencies due to mev um so mev basically means that more sophisticated actors can extract more value than for example at home validators and that leads to an advantage for uh big mining pools because they they can say like uh invest in having their own i don't know like any researcher teams or having special relations with any researchers um and home validators can't can't do that it doesn't sk um well it does scale yeah um that's a problem in this case and um the way pbs fights this is by um by containing this sophistication or centralization in a separate rule role for which we only need an honest minority assumption so the way to think about this is that the process are the validators and um we need an only an on well we need an honest majority assumption um in order to to build a viable system in this case and that means we have high decentralization requirements whenever we have an honest majority assumption we want we want to have like the highest standard and decentralization um yeah builders the ones who actually create the blocks or at least the parts of the blocks that have any any mev in them they are a separate role and we only need an honest minority assumption as in we only need at least one of them or i guess two of them to have have some sort of market to be [Music] to be efficient and this means that we have less lesser requirements on destitution decentralization of this role um because because uh yeah like we we don't need a majority of them to be honest and um to to illustrate how this works i um show i'm showing here a graphic of that metallic made on the so-called on the truth slot pbs construction um so the way it works um we have these headers that um that contain bits that uh builders send and um and the big block proposer um select one of these headers and say like this is this is the builder i select to build this block and then then once this this block has been confirmed so there's like a committee that confirms this block um the the builder will now release their actual block the the execution um block body um and everything else that's that's required and um yeah that that's all yeah then there's a separate um committee that confirms this and we repeat this process um so uh it's basically like we we have this chain of alternating blocks that alternate between the beacon blocks and what vitaliy called the intermediate block here which we're now going to rename to builder blocks um so that's pbs um next i would come to um uh one uh very nice new construction that um uh one researcher uh francesco has thought about here um which is so one of the dangers with the with pbs is that um if you are a very good builder uh that that um say consistently is able to to win these um uh win these auctions or that is maybe uh willing to sacrifice some money to like win them despite not being the most efficient and then you actually gain the ability to permanently censor some transactions um and so the course to do that is basically for every block you uh you have to kind of take the hit of like um paying as much extra as it would cost to include this transaction um this is actually the the case today if you go like if you accept a full on bribery model then that this is the same actually um but but usually we assume that there is um always some small non-sensory minority and so then um the cause of this is much higher and so cr lists basically restore the um the old equilibrium where um uh where proposers um uh yeah are able to include transactions at will and we don't rely on on only builders to for censorship resistance um and they do this by allowing proposers to to specify a list and of transaction that the builder must include and he he has like an illustration to understand so this this is still a fairly open design space i would say um i'm illustrating here one that that we called hybrid pbs um where the where the proposer creates the cr list um to to see how this roughly works so uh whenever the proposer has a slot what they do is they um create a cr list um and they that that is just a list of all transactions that um that they see in the mempool um right now that are eligible to be included um and they um they add another summary that only contains some basic facts about these transactions basically i think as a minimum it just has to be the center of the transaction and the builder is that they are supposed to to see these two things and when they make their bid they include a hash of this cls summary to confirm that they have seen it then they the proposer can accept this bit and finally when the builder publishes their block they have to prove that they they either included all the transactions from the cllist or that they um or that the block is full so um i mean that that's the same today again like a full block obviously is not uh cannot promise to include all possible transactions so so there's always the way of um yeah just producing a full block and in that case you can obviously can't include all the transactions from the cls um yeah so this uh very nice uh design um has basically uh allows us to be much more comfortable with uh going full on on the proposal builder separation um because it's a question yeah is there a way to run with our builder i think we should come to that later in this presentation i don't think this is the best time yeah but we can we can talk about that i want to introduce the full design first um another question from alex yeah repeat the question i that's that's not a question um okay um all right um so this allows us to be much more comfortable with uh proposal builder separation design and trust that this will not make censorship properties worse cool okay so now i will come to one further ingredient which is the 2d kcg scheme so i've already introduced the kzt for data availability encoding and obviously like we can see that um if we encode all our data in akz commitment then we can have efficient data available sampling so why do we not just encode everything like all the data we want to be available in a single commitment okay so i mean we need a super node to build um the blocks i mean we are kind of okay with that um that would be okay but um the problem is if there's a failure like if not all the samples are available um which is actually probably a fairly normal case because we're talking about um yeah tens of thousands of samples so um it's uh it's very likely that's potentially like um a few of them are missing so we need a way for the network to reconstruct this so that all validators come in the case that this is an available block obviously come to the same conclusion eventually um and so this is kind of uncomfortable that we have this requirement that even to just for this convergence property we still need a super node and so we want to avoid this and um so what we uh what we do instead is we we add many kcg commitments um so like m different chart blocks blobs um and uh oh this should be an m by the way so encode m chart blobs in mktg commitments um but if we do this naively then we need m times k samples so that would be um a lot of samples um so instead what we can do is we can use use the read solomon code again to extend the m commitments to 2m commitments and here's an illustration how to do that so basically we have our original data and that's committed to by this these commitments zero to three um and um and like when you when you extend them these rows then that's already committed uh to buy the same commitment right so the kcd commitment zero um is the interpolation of the zeroth data row um but it encodes the extension as well like that's the property of the polynomial um and what we do now is we add extra commitments so the commitments four to seven um and um we do this in a way that these commitments lie on uh on the same polynomials polynomial as zero to three um and and thus basically this thus makes the whole thing a two-dimensional polynomial and and uh at the same time it's very easy to verify that this property is true like it's easy to verify that these commitments zero to seven are on the same polynomial and uh using the scheme um which yeah we can now verify that uh all the all the data samples directly against the commitments so there are no no thought proofs at all necessary um but we only the constant number of samples to ensure this probabilistic data availability property so um what we need um in this case is that 75 plus one of the samples are available and if this is true this if you can ensure this using data available sampling then all the data is available and it can be reconstructed from validators who only observe row rows and columns so you don't need any validator who observes all all the data in order to get the reconstruction property and um yeah and that's very nice so we don't have any supernode requirements in order to ensure the convergence of all the data availability sampling okay now let's put it all together so the the idea is that instead of this previous what we had like where every shard was proposed by by its own separate proposal and we have we have one block that's together the beacon block as well as all the chart proposal and the committee has no other rule than confirming that this is um available um all together and with adding the proposal builder separation um so i uh uh i simplified the graphic a bit then there will be a proposal block and then then the builder will build this this big block that is will be an execution block as well as all the chart blocks and uh and then then this same process will repeat okay um so uh what does this mean so basically why why did we need these this separate committees previously um that's because uh if we have many different actors who propose these blocks we cannot uh we cannot guarantee that all of them are available or all of them are unavailable like basically you would um otherwise have the problem that if you want to want to assure availability and aggregate then any of those chart proposals could stop the whole process by simply making an unavailable block and like and all the other uh proposers who did there made their blocks honestly um yeah would would not get their blocks confirmed because of that one unavailable block um so that would be um pretty terrible um but but since we had now have a single actor creating this whole block um this changes um uh this property because now we can simply say well i mean they they are responsible for distributing all their data properly and so we can just do the whole thing in aggregate um okay so and how does the honest majority validation of this work and so what we what we do is that each validator will pick a number s of random rows and columns huh as probably like i suggest two but this is still a decision to be made in detail um and they are only as i only supposed to attest if the assigned row and columns are available for the entire epoch so basically i indicated like four rows like two rows and two columns and um and they they will they will watch the subnets for these and um and like from one attestation to the next they will ensure that it was always available and um if that's true only then will they vote um for on the next attestation and we can we can further reinforce this behavior by adding a proof custody um and this means that any unavailable block an unavailable block is one that has less than 75 percent of data available uh cannot get more than 1 16 of the attestation and uh this is even true like for future committees so like any chain that has an unavailable block in it would only ever get 1 16 of the boat for the whole epoch um so that's very unlikely to be to be confirmed um which is uh yeah a nice property that we can scale this to the to the full valley data set very quickly um and reconstruction works like this so basically we have that we still have these um rows and columns that each individual validator samples and um and they because they sample full rows and full columns they can they can always like availability means that 50 of like the line was available and um and in that case they can just reconstruct it and that's a relatively small amount of work because it's only like it's only a single row or column so it's um any individual validator can do it and then what they can do is because they're also on these orthogonal subnets so let's say like they have reconstructed their top row and say the one of these rows one of these columns was missing one sample in that location then they they can resend that sample on that channel and and will be made available to all validators um and uh i think yeah my rough estimate is that um with 55 000 online validators um in this two rows and two columns um you would basically guarantee reconstruction like a one in a billion times or so that's um a block that is available can't be fully reconstructed um yeah in practice this is probably going to be a lot better because most nodes run a lot more than just one valley data and yeah as a future upgrade that's probably not going to be in the first version of the spec and we'll add the data availability sampling and that basically means that um each node will check 75 random samples so why has this become 75 we previous docked about 30 it's 75 because this is a a two-dimensional polynomial and in this case we need um [Music] uh three-quarters of the data to be available um and uh if you do the math then uh this increases the number of samples to it uh to have this one in a billion chance that um a non-available block can pass um to 75. and this requires only about 2.5 kilobytes per second of bandwidth so that's a very nice improvement on the data available for sampling cool um so uh let's go to the through the summary of what this construction means in terms of uh implementation and as well ux um or devx um so i think the first uh very major advantage is that um the design is a lot simpler so like it's um it will uh remove uh probably while i i don't think the the previous charting design was fully specced out but i think it will this will be simpler by many hundreds or even a thousand lines of code uh in terms of the specification uh because we don't need any shard committee infrastructure it's all um it's all only the main chain that that committees need to um vote on um there's no tracking of the shard blob confirmation so like before like there's all this like different states that child blops can be in and you don't need to do that anymore they just get confirmed with the main chain or not um there's also no additional builder infrastructure so previously like for for all these chart proposals um in reality we would have to build build our pbs for these as well and and that means a lot of complications because in since this conformation is not synchronous it means that you basically need to have builder accounts on the beacon chain and stuff like that um but the nice thing is that with this construction all the um all the interactions between beacon and chart chains become synchronous um so we can also make these payments synchronous we can simply deduct them from an eth1 balance so that makes it a lot simpler and the last point on the design simplification is that we don't need a separate execution layer so we don't need a separate free market infrastructure for these chart blocks we can simply use the existing execution layer free market um [Music] okay so that's that's the advantages from the design perspective so i expect that this this can hopefully uh lead us to deliver sharding sooner than um then on a more complex design um now to uh the basically the advantages um for for roll-ups so the nice thing of this construction is that we have a tight coupling between the execution chain and charting so basically a transaction can immediately operate on um on a chart blob and know that it's available in the same beacon block this wasn't possible before because we don't know when a shard blob actually gets confirmed um so this i think will make roll-up design a lot easier um in practice and um it will also mean one a very interesting property that um actually zk rollups will be able to make synchronous calls between the execution chain and and the roll-ups so that's that's pretty nice i think that's an exciting property that people might want to experiment with yeah we will not need separate pbs for shots um as another advantage i think already mentioned that one um it has increased bribery resistance so previously like one of the kind of problems that was pointed out was with the separate committees per chart um in a bribery model it might be relatively easy to bribe one single committee and so that's um that kind of made some people a bit iffy um i think in this case uh that's much nicer because each uh each block block gets immediately confirmed by one thirty second of the validator set and over the course of one epoch it actually increases to the full value data set and then finally like thanks to the full the 2d scheme that we can now implement immediately that where actually plants do that on the previous charting proposal as well but what's much more complex because you need to decide like who at what point interpolates the polynomials and extend extends those samples but now now we can do this immediately for every block and that means like it's very easy to for full nodes to follow the full chain with um guarantees against even malicious majorities um only by downloading those 75 samples per block so that's about 2.5 kilobytes per second right so what are the new challenges um that come with this design um one is that we now need these builders to construct the kcg proofs for 32 megabytes of data this is actually something that i'm currently still working on so like um this is currently in order to do this in a realistic time like i think your sticky has to be done in about one second or so um that needs hundreds of cores to do that um uh and this should not be cpu but gpu um so i'm currently investing in gpu implementation and we're quite confident that this can probably be done in about one second on a reasonably not super high-end like mid-end gpu and and it needs a fairly good but not completely unrealistic internet connection of 2.5 gigabits per second um [Music] and uh yeah the second problem is that we're giving more power to the builder because they now well i mean was kind of already clear that we needed this for the execution but it will be a single builder who does serves both the execution and data layers so we need a mitigation with the cls that i um quickly introduced previously cool and now uh yes this is this is just a list of some resource courses that that will be useful so um there's the summary document uh that i wrote up back in my original tweet uh thread um there is um kind of an initial consensus pr um that shows how to implement these changes to the spec if you need some yeah if you need to revise the kcg commitments then here's a link to my presentation and the workshop from last year um a link to vitalik's two slot pbs and then a summary on the list so that you will be able to get these when you when i share my slides after this talk cool um okay so we have a little bit of time still i think right yes i think we can take crushing for 10 minutes before the next presentation starts okay but we have a question time afterwards yes yes also so if that's i think there are some good questions on the chat and okay if anyone wants to speak out and just unmute yourself and ask the question right now anyone so uh maybe i missed it so this all describes how we verified that available at block construction time how do we validate the data remains available indefinitely for the future like in five years how do we still know that the data is somewhere so well i mean so the okay complex question i don't know if i should answer all that right now but i mean i think like um so the the the consensus layer i think should not be responsible for ensuring data availability forever um but it should uh ensure it for a limited time yes so um this is this is a good question and i think like so um there's definitely a um a digital hashtable construction that will um that will store this data for some time but obviously that's purely based on the peer-to-peer network um we will also add the proof of custody that basically requires validators to store the data and make it on-chain available for some limited amount of time so that will be like a few months but i don't think the consensus layer itself should be responsible for uh for storing data forever that that's not the the right way to do this the containers layers is supposed to ensure that data was published and was made available to all participants who were interested in it gotcha okay so the idea here is that um you're trying to solve the the problem of making sure the data is available when the block is proposed and then you know for pretty quickly after that then we shrunk it off to some other system for long-term storage like bittorrent or whatever that solution looks like in the future right right now like the timeline for that is like i mean in the on the order of months right so that's at least my idea that that there's like a consensus layer data availability guarantees that is on the order of months and that should give any actor who is interested in the data enough time to be able to download it and and after that um yes we we uh use some some yet to be defined layer like bittorrent or something to uh to download that data yes okay and um just one thing one thing i would add there is that it is like the long-term data availability problem like if you do the math it just is a much easier problem right because let's say yeah it's a one of n uh honestly assumption and if you actually figure out like how many terabytes it is per year like it's vastly less than the amounts that the bit that like bittorrent network is able to handle already right um so i think that's the reason why we're just not super worried uh we're worried about that problem at the uh at the moment like it just it definitely is possible to uh stick stick it up on tour and files and have way more cedars on them than like what what almost every movie gets um it's definitely possible to um just like come up with 50 different uh centralized actors that are rolling the store or even one yeah even as long as even one of them is actually storing it you're fine there's like five like five different ways of doing it um so i'm like i think all of our view is that as long as the data is actually available for a few months the risk that any of that data will like somehow stop being available 10 years in the future is like just extremely low gotcha and we're assuming here if i understand correctly we're assuming that the there is some portion of the validators that not only attests that they have the data but they're also willing to share it with anyone who asks right like because i mean you could imagine a and a tester says yes i have the data and they can prove they have the data but then when someone asks for it they're just like no like we can't force them together well okay are you talking about the um so uh no if you do data available sampling you know that like like um that the data is actually available to download right so we know someone has a data huh we know someone has the data but that doesn't necessarily mean they're willing to give it to like you know some article availability when you're data availability sampling then you check that someone's also willing to give you the data at least the samples that you're asking for so the the idea is that if you if you have many nodes doing this then uh if if like there is a malicious actor who only gives out the samples that acquired they would still have to at some point give out enough samples that you can reconstruct the data right that's what happens and the other thing that i would add also is that like if you as a client get data by data availability sampling you should also rebroadcast it right like i think that's part of the security assumption right okay so we are assuming that um so there are enough people out there who are asking for the data and then propagating it out i think this is already an assumption we basically make right um today like when when a miner minds a block you know you assume that they're also willing to give the data out the difference i think from today versus this future is that today the miner needs to give the data to other miners in order for those other miners to build on top of them and since the miner doesn't know who the other miners are they're forced to give the data to everybody because they want their block to propagate into the future with this new system i don't think we have as strong of a guarantee but it's probably still good enough if i understand it correctly um i mean i think so i i think it's fairly similar if you assume that everyone does this data available sampling they still have no other choice if they want people to accept their block they need to distribute um those samples and so anyone who asks okay oh the the the only difference is that like what what can be done with data will be sampling you can trick a small number of samplers so like if say there are only if there's like some some super high targets say like an exchange you want to track and uh and you know exactly which node they are running you can and you i don't know yeah you um that then then you can potentially trick them into believing something is available that is not um so i guess like what i would say if if you're literally like uh so like data will be something you kind of hide in the mass so if your node is like so important that it literally secures billions in value then maybe you're better off just downloading the full data and you get slightly better guarantees but for almost everyone the data available sampling is good enough cool did we have either i think um uh yatsec had a question you saw the oh justin yeah quick question so um basically i'm wondering what happens if we don't have enough uh online validators for reconstruction you had this this calculation 55k validators online and honest or just online well online and honest in this case yes or like online and recon to be precise um in practice right this is this this is a very very pessimistic calculation where every um every node runs exactly one valley data which is very unrealistic like in reality we know the average is much higher than that and then like when you look at the reconstruction so basically what happens if you run more than one validator you will get more more lines more rows and columns in this picture and um and just sending the extra samples you can reconstruct it on those other lines and columns uh that's basically free right that's very very very minimal extra work so you might as well just do it um so like this the scales a lot better once you have like uh 10 or 100 validators on one node it will um it will make the situation like uh yeah quadratically better um i haven't redone the computation but i would expect that if you run 10 validators so 100 then it will be um it will be in the hundreds or thousands of nodes okay but i guess the point is that there is some sort of number like what happens if we go under that number let's say there's there's only 200 active validations like do we do you have some sort of full shutdown like finality or do we have some sort of graceful shutdown of the data well i mean clearly you cannot get finality in this situation right um well we can if the the raw number of elevators is kind of lower than this today let's say we only have let's say 30 000 validators which is less let's say the numbers we need fifty thousand online and on those ten but we only have thirty thousand active if there are fewer validators then uh wouldn't i guess shouldn't we just like automatically start assigning the more rows and columns to make sure that we get full coverage no matter what the number of values is or reduce the number of rows right well right this well this was the solution in like free gank sharding right you the the number of shards goes down of revelators just below we can still do that right right right right stopping yeah from doing that um yeah it seems like a really reasonable thing to do yeah so i think my question was basically that um given that builders and proposals are separate and builders require more specs what happens when all of them go missing like um yeah so it's an interesting i mean i think like we're actually thinking about uh how to uh so i mean personally i think this this is a very very very unlikely situation uh but let's let's entertain it anyway so i think like um we have thought about this a bit and um so like one thing you could you could always do is i guess like um build build blocks with much much less data in it in them like anyone can can do that but there's probably also a way to enable more distributed builders where um so if you assume that the transactions themselves are already available in the sample form then it may actually be relatively easy to um to add some uh structures to the peer-to-peer network so that you can actually have like a distributed builder where like the builder actually only contract constructs the block with the commitments and then ask the peer-to-peer network to just do their reconstruction on the data and um and add all this um this is something that we're still working on um so if you want you could like maybe join that discussion but i think it's actually possible to also build a more distributed version of this and the rewards for building the blocks is basically your what your mev or what how how do we split the rewards oh i mean yes it's mev but it's in the sense like i mean it's also just fees so even if you're not good at extracting maybe you just get you still get like paid all the tips from the transactions and so on for it um so like even if you do it as like as a proposal you can always like build your own block as well be your own builder um and um in that case you can uh yeah you can just uh take all the transaction fees which uh which should be enough to pay for this so it's not like it's not an altruistic work that someone has to do this may be um too forward-looking so feel free to decline to answer have we thought about what mechanisms we're going to use to decide how many shards to have because presumably the the builders you know however many charge we have determines how powerful the builders are gonna need to be right and so we could make that you know you need a supercomputer and there's two people in the world who can do it but that seems too high like if we thought about what our metrics are going to be for determining you know what the right size hardware for builder requirements are and network capacity all that um right i mean i that that's i guess i i think like our limitations i i would say are less on the builder side and more on the validator side like what we're willing to for validators to um to be a because like in in this construction where we want the validators to be able to lead the reconstruction of the full block um yeah we have some restrictions on on how big we can make the data availability set for that i say so your belief is that our bottleneck will be on the validators before we get into any reasonable bottleneck on the builders yeah i wrote about this in my uh in the article and the one that's the blockchain scalability a while back um basically like the two limits are one one limit is at what point uh do we get to the the point where we are it take it's really hard but there's so much data that there are actual risks that will lose some uh some amount of it five or ten years down the line and i think we're very far from it um given like these 32 megabyte numbers but like if we add like you know even one order of magnitude or two or two orders of magnitude then it quickly does start getting into uncomfortable territory and then the other one is the data availability sampling security assumption which is which would be that like there just is this new security assumption that there is some minimum number of nodes that have to be doing the sampling uh for the uh sampling attack to actually provide guarantees because if there are too few nodes then an attacker could just like sit around wait until they hear queries and then when we respond to those queries and if there's very few validators then those queries add up to less than half of the data um and the minimum number of validators is like very manageable given these numbers but either that minimum number or the yes or the data bandwidth requirements of the data availability sampling would have to go up um as capacity goes up um so and that once again like it gets it it does get into uncomfortable territory one or two orders of magnitude up but building um on the other hand i guess the theory of the whole like mev sophisticated builder separation uh kind of mindset right is that a top quality builder is going to have to like basically pay human salaries already um and a computer with that costs the same amount or a similar amount as of those kinds of human salaries is already going to be very powerful like you could you could even have lots of computers you could have multi gigabit connections in multiple places and so forth so and the other thing is that there is also this concept of a distributed validators right where validators can uh delegate the load of their checking data availability of some data transactions to other participants and this is one of the things that we're trying to keep in the um in this design um so you know like the builder's side is like less of a risk as the scalability numbers go up i think like the bottleneck to uh kind of like cranking the um the numbers way up is mainly on the validator and the client and user side okay i think that's the last question of the before the the next uh talk by talik so um that's and let me give people some time to think about the proposal that dan carter gave um this was a very very brilliant presentation and then so uh let's vitalik to join the zoonko and set the presentation now stop sharing okay [Music] so that's us wait is it are you still doing audio yes yes so you might have to okay give us one minute to prepare this and then everyone's seen now oh okay great uh so this is a pre uh pre eip that i wrote um over the last couple of weeks um and the goal of this is basically to try to give some of the scalability benefits of dank charting and provide like some uh transaction fee to roll ups um uh like on a much faster schedule than full dang sharding with data availability sampling with pbs and with um all of those uh other kind of fancy construct constructions would be able to be implemented right so the core idea here is basically um that the cip um implements the transaction format that would be used in charting um so it implements this new transaction format where you have these transactions um and each of these transactions has um or is connected to what we call a blob uh this piece of uh 128 kilobytes of data uh and the but uh the long term idea is that we want these blobs to be sharded right so you you you're not we could have hundreds of these uh transactions while carrying transactions in every block but the actual blob contents would go to would be distributed across the again some different peer-to-peer subnets or possibly some dht construction some kind of system that we do data availability sampling on and every node verifying the chain would only have to actually download and verify the headers of these transactions but in this eip we don't do that yet right so we still require everyone to download the contents of the uh of every blog and instead of having a cap of 32 megabytes we have a cap of uh two megabytes and a target of one megabyte um and the is that so the transactions would basically yeah be just regular they would look from the execution client point of view very similar to how existing transaction types like except they would also have these extra kcg commitments attached and then there would be this structure um um that gets added beside the beacon block and that gets passed around the peer-to-peer network with the beacon block that contains all of these actual blobs and part of verifying of the beacon block would be verifying the blobs against the commitments that are in the transactions um so skipping past parameters and skipping past helpers um uh first um so we add a new um eib 2718 transaction type um and now one notable thing that i did here is that this is a transaction type um so there's a couple of reasons for this uh one of those reasons is that i think we do wants to migrate uh execution layer transactions into being like part of the same system as a beacon chain layer layer logic eventually um and this is as good a place to start a setting and the other reason is that the the eip actually does require the beacon chain to have uh visibility into this uh list of version hashes that get that's a part of the blob transaction um so ssd structure uh basically you have your your assigned blog transaction um which has kind of the header um which we call the header because it contains the hashes of the blobs but it doesn't actually contain the full blobs the blobs get broadcasted in another layer um and it has a signature the blob contains like very standard stuff that we all know and love i mean a whale of non says we all love gas we all love max based fees and priority fees um who would get sent to value data so it is a transaction that makes a call to some address with some piece of data just like anything else but it also has this list of blob version hashes embedded on um now you might wonder well if the transaction type as described in this is that only has these wild version hashes well where are the blocks and so the answer to this um is if you scroll down what there is this object called the blob transaction network wrapper and this would be the objects that actually gets passed around the network and passed around the memphis right so it would contain the transaction and it would also contain the actual blobs um and there were there's this network level validation function that uh basically checks that like the the routes that are included in the transaction um actually match up with the blob that gets uh that gets passed along with them um so this um yeah basically this part of the eip talks about what happens on the execution client side right so like in some ways there's uh like three sides to what's ha well what's happening to the cip there is the execution client side logic um there is the consent the consensus client side logic and then there is the network and memphis logic so from the execution client point of view the execute we actually don't have to care about the blobs at all because there is this magic other structure that's ensuring that the blobs of um are available and and the way that this time ap is constructed is that the execution and client layer logic doesn't actually have visibility into what the blobs are all it can do is they can see the hashes right so it's uh this kind of very pure design where we're basically adding this extra big data field where that data field cannot actually be accessed sort of synchronously inside of execution and this is critical because in the long term these blobs are actually going to be broadcasted um across like different sub networks and most clients are never going to see like more than a small portion of the entire blob data um so otherwise you have a transaction signature standard ecdsa signature for now um so and uh the way to verify the signature and give you the origin is like fairly self-explanatory um and we also um check a couple of restrictions um so this will be one point where i'll talk about like what these aversion hashes are um so like if you've been watching this carefully you might notice that we have a uh concept of uh casey blob tcgs and kcg is our the kcg commitments that duncan talks about um then uh we have this concept of version hashes and we have a function that converts to kcg to a version hash which basically just hashes it and it adds a version byte at the front um the main reason why we want to do this is a forward compatibility um so the the one way to think about this is like suppose in um you know the dark year of uh of uh 2000 like x y um quantum computers get invented um and uh you know oh no we can't use kcg anymore um the we we already know what we're going to upgrade to right like we basically upgrade uh to um merkle trees with uh arithmetic functions where we have we add on a stark uh to prove that the miracle treat is uh uh committing to real data and it's committing to data that is uh correctly erasure coded um but yeah but that would look like a very different structure from the kzg right so what we want to do is it wants to kind of establish today this kind of very permanent and long lasting idea that version hashes are 32 bytes um and you know they always will be 32 bytes and then if you want to prove things then the thing that you're proving um based on is that you're approving you're checking against the version hash and like part of the proof would have to contain the kcg commitments today and in this stark future the proof would always would have to uh contain the stark that proves that it actually the the data actually is correctly erasure coded um one way to look at this is that there's a down here we have some uh pre-compiles so it for example there's a pre-compile to evaluate the polynomial represented by a blob um at a particular coordinate and it the thing that it checks against is it checks against the version hash right so we have version x y and then it's assumed that everything after this point is the proof um but then today the proof consists of the data key the the data kcg is so the ksdg that hashes to the version hash and then after that you have the quotients where the quotient is like the standard kcg proof right um so the standard way to verify keys uh kcg proofs like well it's uh somewhere in the um in the headers right it's the usual like q is your proof and you check the q paired with um the x minus x minus x and g two form actually equals to p minus y where p is the commitment to the polynomial but here like p would be the case of g and so we would just have to provide the kgs part of the proof check it against the version hash right so we try to basically in the execution layer deal with version hashes as this kind of object representing the blobs that will survive for the long term um and [Music] the um today um the uh pre-compiles that deal with these um they try to deal with commitments they deal with the version hash with this fight but then in the future they might deal with deal with something else um so here so the three restrictions so first we check that the version hashes have the correct version then we check that any single transaction has to have at most like some maximum number of blocks per transaction and then there is also a cap on the number of blobs in a block um so if we scroll up to the here right maximum number of logs per block is 16 the targeted number of blobs per block is eight now once again if you're watching closely if you see the word max max and you see the word target that implies that there's like some eap 1559 mechanism that adjusts the price of a block so that um on average it hits the target and that is going to exist as well um just haven't gotten to it yet um so this this does introduce this like kind of some multi-dimensional um eap1559 mechanic where you separately have the same eip that we always do for the gas price um and we also ha uh ours are for the base fee and we also have this separate um eip159 mechanic that like adjusts how expensive blobs are so that blobs um so that we have like roughly this um a constant level of blob sizes um so new transaction type um so so far this is just a new transaction type and um there's an op code to get version hashes right so from the execution client's point of view so far this is just a new transaction type and we're going to have an op code to gather these blob version hashes so it just looks like a transaction with a bit more data on the beacon chain side um so we talked about the structure right there what i call the beacon block and blobs um and the beacon black and blobs has to begin to block and it has the blobs and so the i um there's a function to do this kind of like pure beacon chain side verification um where the beacon block body is required to have a list of all the blob kcgs that were included in the transactions um and the long-term goal of this is to actually exp like erasure code extends this list as well and there and that like that's what um allows us to do 2d sampling um as uh digrad mentioned um but for now it's just a list of a list of all of the kcgs for all of the blogs that were included in all of the blob carrying transactions in that block um so we have the beacon block and then we have the list of blobs and we basically verify that for each blob the course the the corresponding kcg is actually correct and then we also have this cross validation requirement which basically validates the relationship between the beacon block and the execution payload um so basically the idea is that we actually walk through all of the blob transactions that are in the execution payload um and we make a combined list of all the version hashes and then we check that the version hashes match the list of kcgs in the block body and then over here we checked that the keys the list of kcgs and the block body matches up with the list of blobs right so beacon chain sign logic also actually pretty simple and pretty and uh pretty minimal um probably the main weird thing is that we have to add this extra object called the beacon block and blobs and that has to be uh broadcasted um around the peer-to-peer network but like so far right it's uh like what it what it is uh the mechanic that it's producing is uh fairly complex um but the actual kind of number of lines of code in either the execution chain psi client side is not very high and then the beacon chain side is not also not very high okay um let me take a break and uh just to go through a bunch of uh uh questions um okay and give people time to come up with other questions because i'm sure that was fast um how costly is blob to kzg uh what was the number of decades you remember how costly is blocked to kcg commitment yeah um i remember like what was it like either 60 or something uh so [Music] we have 2 to the 12th right was 128k which is 2 to 17 divided by 32 to the 12 uh points yeah [Music] it's the one one curve multiplication was it 100 microseconds that you were saying yes so 100 microseconds multiplied by 2 to the 12 is 400 milliseconds but then because let's say a linear combination and we can do the the the fast linear combination temperature stuff divide that by a log and so we get like somewhere around 50 milliseconds um great now okay um yes yes we can do multi-scaling well um and okay so another thing another new ones to keep in mind here is that in order to verify this if there are multiple blocks there's actually a further optimizations that optimization that we can do on top and the optimization there is to basically take a random linear combination of the kcgs and the random linear combination of the blobs and that's just field math and so the field math is very fast and so we only ever have to actually do the 50 millisecond thing once right um so the the per kcg math here like how much um how much is it to take a random linear combination of i guess that's like two 81.92 field ops per um like 4096 times two field ops per blob and a field up is like what 15 nanoseconds so 50 nanoseconds time cd 192. so that's about like 400 microseconds like less than one millisecond per blob so the incremental cost is high another thing i would add is that if we decide that even the 50 milliseconds here is too high um then there is an option um to basically add a yeah like to add to the blob a proof of uh evaluation at a random point um and that's something like it would add some protocol complexity but but it could decrease the amount of work that we have to do over here even further um so like i think right now it's good but like right now it's there's um it's quite good already right because it's just 50 milliseconds plus um maybe like one millisecond a blood but you know if we uh decide that we that we want to get like even further efficiency gains at the cost of some complexity then there's there there is a path to do that doing that as well um okay yes ding ding ding um right um so at some point um we should probably talk about like how rollups will actually use these blobs i'm actually yeah just to get um right okay so maybe you just uh well to kind of give people context um we are going to kind of skip past and do the stuff and uh um like talk about rollups a little bit now um so the way that optimistic roll-ups would work right is that optimistic roll-ups today um they just stick their transaction data in call data and they make like a data submission transaction where that data submission transaction just contains all of their call data and often what even happens is that the way rollups already work is they basically just stick the transaction on chain and they do they walk go through that process once to verify the transactions available and then from that point in the future like basically if you ever want to do a fraud proof you would basically have to resubmit the transaction and then it would get checked against the hash that got saved right so here you would actually do the exact same thing um so you basically when the when the data is being submitted uh the role of block data would just be part of a blob um and then you would save the version hash and optimistic rollups um basically whenever there actually is a fraud proof um then the uh the full contents of the blob would have to be submitted in call data um and that is why we add the blob verification pre-compiled here right so bob verification pre-compile basically like checks an entire blob again it just checks the provided blob against the version hash um so that ends up the yeah the cost of the breaking pile is like around 2 million gas but the cost of like actually providing this amount of in of just providing the call data itself is also 2 million gas um there are no once again if we are willing to incur more complexity there are ways to do this um a little more efficiently um but like there isn't so there isn't a huge room for optimization though the other the one way in which we could optimize actually is that we could basically make the point evaluation precompile cheaper if like the the the input has a lot of zeros right and so if someone if some rollup uses something that has like uses like a less than full blob it would cost still the full amount to submit but it would cost the lower amount to actually do the evaluation of right so that actually is an option um so that is what optimistic role also do um what zk roll ups would do is uh zk roll ups would uh basically they would generally provide two commitments to whatever data like either transaction data or state delta data whatever they do um in the their uh transactions so there would be the kcg or i guess the version well i guess the version hash of the blob and the and kind of implied is the the kcg um and they would have some other commitments using whatever proof system the zika rollup uses internally right um so you would commit to your data um is uh like in the kcg and then you would also have a polynomial commitment using whatever your own scheme is uh to um where that's the polynomial commitment that would actually kind of talk to whatever or be part of whatever zika start scheme that you're using um then there is this protocol that we call like commitments proof of equivalence and that proves that the two commitments point to the same data right um and the commitment proof of equivalence protocol is actually very simple um it's uh this uh here it's the the there's a lovely research post that i can't um quite access but it's uh oh there we go it's it's literally you have a bunch of commitments under different commitment schemes and then you would take one z as the hash of all of them and then you would just open them all at a random at the same random point and you would have different proofs uh that proved that all of the different commitments uh evaluate to the same value at the same random point uh so if you do this then like statistically it's guaranteed that the commitments point to the same data right so this um right so as a zk roll up like basically you would just like you would have to have this like you would still be able to mostly just like design your zika roll up using like whatever scheme you wanted as long as it can talk to any polynomial commitment scheme that you're using and then you would use this proof of equivalence to connect your polynomial to the point to the polynomial of the data um and doing that random evaluation is what the uh um what this uh point evaluation pre-compile was for right um and so as as a zq roll up you'd be able to do that um okay the um issue here is that right so expensive verification um needs to be done before gus uh before gossip relay right um so there are ways to optimize this um so one of the ways to optimize this for example is that you would like like have kind of discreet periods and you would really many at a time right because of like how we because of what we talked about it's uh much cheaper to um a real verify many of these um at the same time than it is than it is to verify a single one of them um so that's one technique um and then if we decide um that it's uh um that we want like extra uh fast verification then like i mentioned once again like like there is this other uh technique that we could add we could add it at beacon block level or we could add it just a memoir level or both um and uh this is a the the technique basically the way this technique would work is you would take a hash of the full blob contents and of the commitment and then you would use that as a random evaluation point and then you would provide a proof that the polynomial um opens to that evaluation point and that the actual blob if you treat it as a polynomial and evaluate it at that point which you can do in o event time and also in a couple of milliseconds and do evaluate to the same value right so like if that turns out to be a bottleneck we do have this path that like does add some complexity we could add it at one place or the other place or both places that uh does also improve on things so [Music] right so okay we talked about beacon chain side validation um can you verify a fry start without providing the full data to build the verbal tree i mean yes right like you just like starks are starks are snarks like they can include unlimited amount of private data um okay so sure maybe i missed something uh this sounds like we're just moving the problem from execution layer to the beacon layer did i miss how we're making this so the beacon client doesn't have to propagate and store the full blobs to everybody right good good question um so the beacon client does have to propagate and store full blobs in this version in this form um but um and the the reason for why this is worth doing at all is being similar to the to the rationale for eip4488 which is basically that like the uh we're at the limits of the burst load that we can handle but we're very far from the limits of the sustained load that we can handle especially post four four eight uh four four four four um and like these blobs clients should be able to delete fairly quickly like we could even delete them in like in less than whatever the 444 period is because the blobs are like very separate data um but basically it's the look after the like today with the chain the reason why we don't just have like very cheap uh date um call data is because of the the the burst uh size or the maximum number size would just go way up and like way beyond what clients are willing to handle which is why eip4488 added the reduced cost with the separate limits so we do multi-dimensional pricing this eip also does multi-dimensional pricing for the exact same reasons um except this eip basically instead of opening up that extra data as a call data it opens up that extra data as this kind of inaccessible extra data that is in the same format as it would be when we yeah and when we end up actually fully sharding it right so once the cip is included like it is extra work um to um to accomplish a very similar benefit to what before 488 accomplishes but the benefit is that once this eip is um is added then basically all of the work needed to get to full sharding can be done in the background rollups don't have to care about it execution clients don't have to care about it even most of the people working on beacon clients don't have to care about it like the cip already kind of turns the the blob verification uh process into this kind of this one extra function that just verifies that the block and blobs are available and you can kind of quietly uh replace that with data availability sampling uh over time without even needing to touch the rest of the consensus or the execution layer okay so if i understand correctly the this creates a new class of data which we can then price separately from all of the other things so we don't have to price it if gas like everything else and it sets us up for a smoother transition to sharding sometime in the future right um okay how difficult at minimum would it be to discard a spam tx okay yes uh good question so i did talk about this in the uh the security question um mempool issues so basically the issue so i think the reason like this is actually i think a problem that the uh that the ethereum mempool has to some extent already right like transactions that are very big in terms of data the reason why it's a pro like the reason why the big data transactions are a bigger problem than execution transactions is that if a transaction has like 10 million guests of execution then in the mempool you would only still have to do the 21000 gas verification of the signature and that's all you need to do but if a transaction has the full data then you would still have to every node would have to download and rebroadcast all of that data and so you you like you don't get those same kinds of savings um so the risk of this is that there is this kind of attack where the attacker makes and publishes a series of big blob transactions where they just kind of like escalate the fees up a ladder um and then they finish it off with um a transaction that only has 21 000 gas right so you have like a whole an entire row of transactions here with increasing fees where these transactions have like big amounts of data hundreds of kilobytes and even without the cip right like you could do this today um but but then what you do at the end is you just stick this transaction at the end and you could stick it into the mempool you could even do it with flashbots um you could like just be a miner and do this attack yourself um and then you would finish it off with this 21 000 guest transaction and so you've basically imposed like nine times millions of gas of load on the network but you only pay this tiny amount um so the simple mitigation that uh that i think um i recommend i'm actually i think when when i brought this up in the yeah um in the guest chat if they were given some of the guest team suggested this um basically that four transactions that carry a large amount of data which could be blob transactions or it could even be transactions today they have a lot of call data uh we increased the minimum increments from what i was told the current amount is which is 1.1 x to 2x or if we want we could even be harsher we could even literally say you know maximum three fee replacements for a transaction um and the idea would be that at least like this part of the attack would be yeah completely yeah removed um sorry um eap4488 maybe is what i was talking about so four four four four four four four four or i guess four fours um is the uh history removal four four eight eight was the uh call data increase uh or or gas price reduction that i suggested last year that adds some multi-dimensional pricing um and right so the memphis attack basically like if we just increase the minimum increments then the mempool attack would be able to go down and um the like basically yeah the transactions would be um similar to or the total load that you could impose on the network would be similar to the total load that you can impose on the network by just doing like this kind of fee ladder attack with a regular transaction um so that's kind of one answer another possible um answer of course is that like if let's say in the future we want to increase the size of the blobs um then one of the things that we could do um is like you could have a separate mempool for this um in the longer term you could even have uh like some uh some kind of sharded mempool for this um or you know obviously there are these like uh separate uh proposer builder markets that are starting to exist already um so that is like there are longer term solutions like that but i guess kind of like the simple tweak that seems like it compensates for for a lot of uh for a lot is just kind of like minimum increments of uh increase so i think this gets us to um i feel like what have we talked about we talked about the execution of client layer stuff we talked about the beacon chain stuff we talked about there's an upgrade to get version hashes there are these two pre-compiles that are they are pure functions um they will require the beacon or execution chain clients uh to also have um the uh bls logic in them um or bls12381 um so one way of doing this is to just say okay fine we're also going to add the bls 12381 freaking file to shanghai because that's just something that's been sitting around and ready for inclusion since forever and if we do that then the libraries would be in and just calling the same libraries would be pretty trivial um but you know there's other like just kind of highlighting that like it does mean that bls 1231 would become a dependency of um execution clients in addition to consensus clients um but otherwise um it's uh it's a pure function blob verification is a pure function point evaluation is a pure function um and like as i mentioned before a lot of the the ways and like there are some fine details in how this is organized like the concept of version hashes that really strongly have to uh um have to do with the forward compatibility and like one of my personal kind of visions is that i hope that we can do more like explicit forward compatibility in the future um so one good example of this right now is that today there's a bunch of ethereum applications uh that use uh like use um merkel patricia tree verification implemented in solidity um in order to verify things about history i think even some of the optimistic roll-ups do this but when we switch to verbal treatise every application that does this is going to be completely broken right unless they upgrade um and i think this is unavoidable um and like force in the case of roll-ups i think like ropes are gonna wants to upgrade to take advantage of these point of blobs and other everyone else will just have to upgrade but in case we wants to upgrade again right so like for example in case we want to upgrade from um you know shot 256 yes has that to some arithmetic friendly magic polynomial thing for like the zika stark ethereum of the future or whatever we want to do uh then we we do not want applications to have to like upgrade everything like upgrade their contracts again right um and so the way that we accomplish this is uh we basically say we we could make a precompiled that is a pre-compile for verifying access proofs of history and today it would be a miracle branch recompile but then in the future if we let's say switch to i don't know vertical trees um for the beacon chain state then there would be some block cut off and if the proof comes from before that block the pre-compile would automatically do the like continue doing the miracle verification and if it comes from after that block then the breakout would automatically do the vertical verification or whatever other verification we add right so off chain like client logic would have to update but on chain like beacon logic would not have to update um and it's the same thing for these point evaluation like for optimistic roll ups and zk roll-ups right like if the underlying ethereum later layer eventually switches to something else um then they should be able to like basic the the roll-up contracts should not have to change at all the only thing that would have to change that that would change is um in which like the kinds of proofs that you provide from the outside right like instead of the proof being the the kcg and the quotient the proof would just be a stark um and that would that would be off-chain client logic only um and um unchained stuff would uh continue to be exactly the same as before um finally the gas price update rule um so i i i will preface this by saying so far this has been probably the most internally contentious parts of the eip and there are a couple of uh like there are a couple of different approaches that can be taken and like i would be happy to like to switch over one of the other parts um but basically so we this eip does do multi-dimensional pricing right and as we mentioned it has to do multi-dimensional pricing so that we can actually have like a sustained load of one meg of like one megabyte of a chain data without having burst loads that are higher than uh clients are willing to accept today um and the way that we do this um is um actually if i yeah i think you go into the research and we just search multi-dimensional eip1559 um there it is um basically like the idea would be that instead of having what one of the ways of doing this would be that you would have instead of having one base fiat you would have a vector of base b is and you would have a different base b for each unit of resources um and then there would be um or there was a couple of uh ways of applying this right um so one uh way of um applying this would be okay so one way of applying it would be to say like actually would be to actually have like different base fees and you would actually have to like pay each for each for each different resource um and then but and this is the option too or option two is well option i guess there's three ways of doing this option this one this is kind of hardcore right basically the gas based fee becomes fixed in one way i mean and then all of these uh like instead of this being base fees like these would be gas costs that would float around and that would end that would become very high um option one easier and less pure as we keep gas cost of execution fixed um but then all of this other resources that we want to price um they would have like their fees would be expressed in gas and the amount of gas that they have to pay would be volatile um so that's the scheme that the cip currently uh takes right so basically we have this get intrinsic gas function and that we have i put 20 000 instead of 21 000 the idea of this is to just like have this tiny nudge so that if anyone wants to use um this new transaction type for things that are not blob related they can and we can kind of nudge people to doing things with ssh transactions over time but you know we don't have to do this and then this is the pricing for the call data and then we have get blob gas get blob gas is going to be um the number of blobs multiplied by uh by i think that the number of um right number the number of blobs um contained in the transaction i think this is uh actually something that i forgot here like there should be like le len uh tx.blob a version hashes should be in here somewhere times get blob guess and get blob gas is the current price of a blob and the way that it gets priced is basically through this um eip1559 mechanism where basically you take the targeted total um so this is like the target multiplied by the number of blobs uh since the start um since the whole mechanism started and then you take the average total which is just a counter in storage um and then the gas price that you have to pay is basically an exponential in the um in the delta um right so this is um this is also a new mechanism it's uh here um this if people want to understand how this kind of like different version of the ap 1559 works um it's a ethereal research burst 902 make the ipo and 559 more like an amm curve um so i talked about like kind of like the benefits of uh doing this like exponential exponential pricing it has very similar effects um so basically the idea is that there is a blob gas in that blob gas is a volatile number um and uh if there are the uh their a block has more blobs than the total the or more blobs than the target then actual total minus targeted total increases and so the blob gas e is going to go up exponentially um and then if a blob has or if a block has fewer blobs than the target then actual total minus get targeted total goes down and so the uh the gas that you have to pay for a blob also goes down um and then here's the update rule so it's like fairly yes a fairly straightforward like you just take the new total um and you then write that it's a storage and you would have a storage variable that that just is like what's the total number of blobs that have happened um and the gas price is the exponential of that minus targeted total um in case anyone is wondering what fake exponential is it's basically just this really lovely um integer approximation to an exponential function so if anyone wants to mirror doubt on the math i'll let them do that on their um on their own and like plug in values in python and verify that that you know it does interesting things and and it works fine um so that's the gas price so that's so the float floating gas price there are the one natural alternative to this is basically that instead of having a uh an intrinsic gas that's floating and you would have an an extra base fee that's floating right so basically the gas would just be these two terms uh but then there would be a separate base speed that you have to pay for a blob um and that base fee would just be directly subtracted from the uh from the transaction um or from the transaction origin uh balance the um there are like important ways in which that is cleaner um the uh i guess the main um risks that like one of the things to think about is that if we wants to do like more multi-dimensional pricing in the future then like we might wants to be forward compatible to doing more multi-dimensional pricing um i think the two natural dimensions other than blobs that we would want to pre or maybe three natural dimensions that we would want to price multi-dimensionally one is called data but call data we might want to just like stick into the same system if we want to another is uh witness size so vertical proof witness size so the amount basically the amount that we charge per um s store and the fifth would be um the increase in total storage size so like the s stores that convert empty values into not empty values so there are other things that we might want to price multi-dimensionally um but then the problem is that a lot of those other things like in particular the storage reads and storage rights they are done inside of sub calls right and today we have like we have this system where a call can trust a sub call with a limited amount of uh a limited amount of gas and that's supposed to reflect the facts that like you can limit how much resources a sub call can consume do like me turning that into a multi-dimensional like thing that you have to pass into a child call would just be incredibly complicated we don't want to do it um and so like basically keeping that side of things simple is probably the best rationale for basically saying we're still going to shoehorn everything in terms of gas and if we want to shoehorn everything in terms of the gas then like we might as well start shoehorning everything in terms of gas so that's one argument like i think there are a lot of like different issues here and we are undecided on this like i think i i definitely don't want um you know like which version of eap1559 we add to be like a bike shetty thing that ends up delaying this whole thing like i'm personally totally fine with lots of approaches the things that i do think are important right one is that i do think that we need multi-dimensional pricing of some kind like we need a floating uh price per blob uh because if we don't have a floating price for blob then like either the the price is set too high um and these uh transaction like people don't use this nearly as m as much as they could or the price is set too low and that like basically ends up kind of breaking the market and it turns into a priority gas auction which is not very nice um so like i think multi-dimensional pricing is important how we do it is what's important um i definitely think that like it would be really lovely if we can start actually using ethereum state um to store things instead of just um like continuing to shove this kind of like imaginary implied state into vlog headers i'm so trying to push things in that direction um but otherwise i mean no strong opinions um but i guess let's see what what other kind of rationale things are there um so we have this there's this nice section that basically talks about like what parts of uh the full and you know pbs dag sharding vision this eap already implements and uh what parts it doesn't implement yet uh so the new transaction type already there all of the execution while the client logic already there all of the execution consensus client cross verification logic already there the layer separation between beacon block verification um and the way that blobs will be verified already there most of the beacon block logic already there the one thing that the cip does not include in its current form is the low degree extension of the kcgs which as we mentioned we needed for 2d sampling um but like that's not very hard um self-adjusting independent gas brace already there um things that are not there low degree extension actually doing data availability sampling pbs which we actually need to require to avoid requiring individual validators to process like 32 megabytes of data and like some form of proof of custody like rows and columns the stuff that dean greg talks about is also not there um i'm i'm also trying to kind of set the stage for some longer term protocol cleanup so i'm adding an ssd transaction type and basically hoping that we can start the migration the transactions being as i said um a cleaner gas price update rule um so um there was this uh post that was um i think mentioned it was um i'm trying to remember what this was it's like okay i forget what keywords to search for unfortunately but if people remember i mean i made this post back in august when ap 1559 first happened um and one of the things i basically talked about how oh like why did um average block sizes increase by nine percent um after uh london despite nobody intending it and it turns out that there are like weird eq one 559 properties that made it happen um and one of them is that like it doesn't actually target the target because of like weird amgm inequality things uh because of like if you have an empty block followed by full block seven eighths times nine eight sixty three or four sixty four so the gas price still drops or uh base b still drops a little bit and like these exponential pricing rolls it's just it's very um it's very easy to understand why this pricing rule actually forces the long-term average usage to equal the target um which i think is like a really nice property um and it also uses some storage instead of like header fields so you know if we like it then it's something that we could end up kind of like um bringing into base the adjustment as well um or we know we could end up um like coming up with other um other formulas in the future too and so they're basically like there are some like the eip is on the path both to dank sharding and both to some other things that like i think we might eventually um eventually wants to do already um and i do think that like there aren't like there aren't a lazier ways to do a lot of these things right because uh there aren't um lazier way like right there like we had like if we don't do something like this then like basically we would have to like go and like abuse uh some existing header field and abusing header fields also has costs and so we might as well use storage as i guess my view here um okay so i feel like we've covered the the eap and the rationale right so i guess the what cl what clients have to implement for verification is in my opinion like once again surprisingly easy because all you would have to do is create a new earpiece 2017 transaction type um you would have to um on the execution layer you'd have to add a couple of freaking files those wikipowers are pure functions we've done those before and then on the beacon chain side we don't just add one field um add one struct and a couple of checks um so on the verification side it's actually not like not that hard at all um oh thank you yeah kev andre yeah i had that my the reddit post that i mentioned um but the the part of the cip that is harder is like the block construction right because when you construct a block you now have to like when you're when you process a a transaction um when you get a transaction from the network you get one of these um and then you have to do this your validation function and then you have to stick this into the execution side and then you have to stick this into the beacon side and so we would need to do like some extra execution and beacon chain communication for that i mean my honest opinion is that like i think if even the like the block creation stuff is ready and only one pair of clients like by the time the eap happens like that's not even all that bad right like i think like it as long as there exists some channel by which users can or like roll up teams can submit these transactions that get included then we then we still get the benefits um but like it does have this property that it's a higher complex basically the complexity ends up being concentrated on the vlog creation side which doesn't have like you know consensus failure issues a lot of these other issues i mean the black verification is uh like simpler than expected um on questions um yes okay how much time do i have oh no i thought it was okay fine um okay sorry oh no okay um am i correct and understanding that dank sharding isn't really sharding there's one monolithic shard that has this data spread out ah there's one monolith um there's one monolithic domain um there's one monolithic thing that has its data shorted out so it's still sharding um so it does preserve like i think what what to me is the core definition of sharding which is this idea that you have a blockchain where you don't need any single node to process all of the data um like like that's been the definition i use for sure for like what charting is almost since the beginning um but like i know that the term does have like different um connotations in different people's heads um so you know whether or not it is i guess i i guess you decide um [Music] the um okay oh um proto said that there's an argument that you can use to call it shorting on the network layer but the execution consensus looks like a single large sequence of blobs one other points to reiterate it's not a large sequence of blobs it's a large sequence of blob hashes right like the blob contents are never accessible to the execution clients which is really important because in the future they're not even going to be on the same sub networks um so that's uh right so the execution kids so i mean it definitely is like if you're it is a very hierarchical form of sharding in that like you have headers and the headers all go into the chain and like the the data on the edges is spread out um so you know like you know i guess it's another among those cases where it just like depends on how you define the terms but i feel like i've blabbed for enough so i'm happy to go on so whatever the next step is okay yeah i don't know unless people have like last questions so now it's the qa and engineering discussions okay like so we had like two topics that i mean all this are still part of the downshining and chatting in the future and things and so this is the time that um if anyone have any question or feedback to this two proposals then please thrash your hands anyone or just unmute yourself [Music] too much content as in the past two hours well one question i had is is is pbs a requirement for sharding or is it just a nice bundle um feature my view is that pbs is i guess our view is that pbs is a requirement for dance charting um because because of how dang sharding does kind of like create this more specialized builder role um and there are a lot of simplifications that we get by yeah um by making that sacrifice um so it is a requirement for like the the full eventual sharding but pbs is not a requirement for this pre eip so like we do so we do have like basically still years to figure out pbs no no not many years true i mean maybe also like i mean we're pretty sure that we'll need some form of pbs um like whether we do use it for sharding or not i mean we're also talking about different roll-ups this kind of reignites the idea of different execution engines how what's happening on that front yeah i mean i feel like in some like in some ways the whole like roll up and uh the acting shorting thing is that is sort of bringing back the execution engine vision in a lot of ways right like because like the original execution engine vision did basically say okay we have these kind of like big transactions where the big transactions contain blobs of data that go to a particular execution engine like that and that basically is like what this design is moving towards so you know it's cool it's kind of going full circle um um oh the the trip uh yes up when you asked about the uh interested set up ceremony um yeah okay so we this does require a trusted setup ceremony the good news is that it requires a trusted ceremony with a much lower degree um than the trusted ceremonies that are done for um zk snarks um because we just needed to be big enough to handle like the re the the maximum size of a blob that we would eventually wants to go uh gets to um and i do think that there are that there are good rationales for like not having for not having the blobs big uh basically because like the way that this design works right like you have like you pretty much have to either use a full blob or you or not have a blob because if you use only part of a blob you're mostly you're you're basically still paying for the full blob um and i think one of the reasons why we were okay with making that trade-off um is because the yeah like there are fixed costs um associated with uh sending a transaction uh uh sending a batch in a roll up already right like in the zk roll up so like it's all it's super high it's like in the hundreds of uh in the hundreds of thousands and then in uh even in optimistic roll-ups it's like 50 to 100 000 and like we would especially in sharding i mean we would roughly expect the the size of a blob to be somewhere in that range anyway um but you know if we just decide to say that oh eventually rollups go up to or the blobs go up to two megabytes then suddenly we might have to like decide that oh now you know roll up the the per blob gas would have to be hundreds of thousands and that does make life more inconvenient for uh roll-ups submitters so like that's a reason why i expect future scaling improvements to do more of uh like increasing blob counts than uh increasing blob uh increasing blob size um the but what that means so we do have one i think we do want to add some space to increase blob size just in case um but like so today the uh these blobs like their degree 4096 polynomials um so degree 2 to the 12. if you want to give us like 8x of room to be future proof then that's degree 2 and the 15. and like degree 2 to the 15 with the g1 points and like like ng2 points so much smaller than like what the existing ceremonies are and which is nice because that would allow us to have a huge amount of participants like my personal hope is that it should be possible to participate in the browser um which is like not viable for a size 2-22 ceremony but is viable for a size 2-15 ceremony but this is definitely something that will needs to happen over over the course of this year and it's going to happen is this one of those ceremonies that can be reused for other things there's a ceremony that has single use um it can be reused well actually i guess okay so one argument for what it's more like we can build on other people's ceremonies i think that's probably what we will do um but it'll be harder to reuse after uh because like if we do like if we do make this choice to make the size of the ceremony small then anything that we add will not be useful for uh zika starks because the gestures do need bigger ceremonies yeah it's universal so it's it's just powers of tower but it's um it's not really practical for for snacks because the circus would have to do yeah would be extremely tiny i mean i guess like there might be some like weird halo pairing thing that could do with small circuit sizes but like i know if you do that you might as well just use ips um but it can be indicated it can be built on top of like aztec ceremony for example yeah right well no aztec ceremony is uh is over the bn 128 curve right i don't know yeah well like because like you can only use bn 128 within the edm right now but like i believe this there is some serum like z cache there is several there is this file coin one i think um alio maybe yeah yeah so we can we definitely have a choice of things to build on that will yeah strictly increase our security okay um trying to understand this question when the roll-up chain makes synchronous calls to the main chain for other roll-ups this will be published separately like is this like when you try to do like some sync with some kind of like synchronous cross roll of communication i mean well i mean so what i said in my presentation was that with this you could have synchronous calls between zk roll ups and um and the execution chain and the reason for that is that all the data to make a like everything you need to make a um and a zk roll up block valid can be in the same transaction right and so you could have like uh something in the in this ek rollup call uh block saying now you need to make this call and then like the whoever publishes this transaction chain would actually make this call and it would just be part of the transaction so um so basically what would happen is all the all the zk roller part would only be published on uh on the data charts um but the synchronous transaction clearly that would would be a call on the l1 so that would be a call that this zk roll up um management contract um yeah is forced to make directly on the l1 that's how it would work i don't know if any of the roll up teams have thought about these kind of transactions but i thought it was very interesting that this would become possible questions any more anymore there was a question about what application does random data availability from alex there's a new message too much information need time to process borrow story [Music] what what implications does random data sampling have for b2p network throughput um yeah i think like the way that we do data availability sampling is still like under discussion like there is a yeah there is a subnet based approach and then there is a dhd based approach that feels cleaner but relies on the assumption that we can make phds that support extremely fast uh uh publishing inquiries um so like i think das should uh you know should not be rushed and it's okay if it takes a while um and uh before like well the nice thing is that we do have this kind of like fairly nice uh roadmap for like slowly increasing i'm scaling over time and like even doing a data having some nodes that start start data sampling before other nodes doing all of those things um so yeah it's still early days for diys hmm how do you think that these things fit in with the other major items on the road map like uh four four four four and four four eight eight like um do you envision both of these coming after or around first one coming around the same time and thanks charting after everything else we've got on the roadmap i mean i'm i'm definitely personally hoping that 4444 is done asap um because like it is like four four eight eight do you mean well no four four four four four four four well okay i guess both four four four four yeah can we compromise to do four four six six [Music] yeah um i guess like 444 is uh just easier to it makes a lot of other things and easier um i guess one thing you could argue is that if somehow for four fours uh like stalls for two years which i really hope it doesn't um then for uh what like we could just say we're only doing like expiry for these extra uh for these blobs right because like blobs today don't exist and there's no assumption that blobs are or nobody's doing anything based around the assumption that that blobs are going to be um accessible for more than some period of time uh so like in that like in that extreme case i guess uh you can say dank short or pre dank sharding is uh not not even dependent on for on four fours but like i mean i strongly support four fours being done faster because like i do think that it is a very pragmatic thing that can open the door but like to whatever combination of higher gas limits and making it easier to run a node that we choose um and then four four eight eight um i guess like my perspective is that it depends on how quickly we can do this like i like i personally you know would be very sad if uh if this uh eip gets like we converge on the cap but it takes a long time and the fact that this eip it takes um a long time becomes used as an excuse to not do 448 and roll ups end up being expensive for longer than they need to be and so i think like either the cip gets does get done quickly um or if the cip gets done more slowly then i mean i personally do support uh getting four four eight and eight eight in um even before it i'm just at to give shorter term relief to uh to roll up projects i think i think we should see the high fees as an emergency situation that's happening on ethereum much more than we do now so like we should prioritize all the data availability like get get something in by shanghai at the very least like i ideally i would see like vitalik's um eap implemented by then and and then i think the next hard fork after that should already be shouting that would be the ideal in my opinion yeah there's some fighting words oh i like this proposal from thomas can we change the eir and start counting from zero um hmm good question actually what's our request all right yeah well i guess now i want like are there ways to uh make github repos that like well i don't know i do like this idea that anyone can make any ipa and it gets it does have the nice democratic property but it does also mean that they know they go up to four digits if most of the numbers aren't used we could just redo redo our numbering system and start going back and using all the unused ones well that's true yeah like to confuse everyone go go in and look at some of the forgotten eips that are just clearly not viable anymore and say oh this totally thing is like just actually like what it's account abstraction it is eip86 it's just it is the original vision of the ip86 that's right um anya in 2935 which we should also do is going to be renamed eip96 someone showed an eip to discuss this [Music] each eip must have it on chain signature this each it's a disability idea you have to either have 32 eth or a proof of humanity profile i shall we brought some tea and that's so lovely nice yeah i think yeah we are out of time [Music] two hours and seven minutes okay no there's uh no no need to um extend things uh beyond their um their natural lifetime so i mean if if people have questions there are plenty of forums on which the the questions can be asked including the including the ether researches that i linked to um including the uh this oh it's uh is there should we just make a magician's thread for this yeah yeah okay sure i'll i'll make a magician i'll make a majority thread for this separate ipm um and uh yeah they're called reached so is there a discord channel where we can uh just chat about this topic that's appropriate i guess like the the the shorted data discord room like that might be as good a place as any um yeah i mean if people want to ask questions they can reach out to me at chat.blogscan.com as well i guess after today everyone knows better about the situation and if you have any questions ask joe oh yeah you never explained the joke by the end i was promised that by the end of this presentation i would understand the joke about the title of the talk what's the joke situation [Music] [Laughter] a series of streaming that's george what's the george situation so but i think it was a misunderstanding you need to meet george that's how you understand the difference yes no george george likes to refer to everything up to and including restaurant orders as the situations yeah see it's uh it's more fun when you have george explain it [Laughter] thank you everyone for joining the workshop online in the zoom and the youtube viewers and you know tweet live tweeting okay then thanks pooja for the live streaming i guess that's that's all for today thank you thank you 