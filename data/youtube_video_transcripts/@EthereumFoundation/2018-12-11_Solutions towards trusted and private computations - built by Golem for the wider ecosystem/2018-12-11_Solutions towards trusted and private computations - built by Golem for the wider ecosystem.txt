so in the decentralized environment and taking of the consideration that malicious software can get somehow to your computer transfer to computation Roosevelt sank it's a need it's a must but different parties may need different aspects of trustworthiness in terms of computation and this is an important question of why we really need these computations so for example we may need to deploy a peer-to-peer network of decentralized computers with a non consensus out ribs or we may need to deploy a smaller consensus between two computers to compute something and the second use case requires other aspects of transport units than the first one in the first one you need bounce on the malicious actors the number of them and some availability in the second case you probably need confidentiality if you would like to decentralize the centralized service that hosts as an all-purpose computation then it is quite a different deal and there are more so we had to ask this ourself this question what is important from our perspective at Colin so just a short recap in context of this presentation Joanne is a network of catalyst resources ranging from individual machines to sub networks exposing the compute power to the network and two kinds of participants that interact with both the resources and with each other the first one is requester the participants who would like to do some computations using the resources and the second was one is the provider provider rents out the resources and perhaps would like to do it in a secure manner so this is the basics and this is the fundamental layer on top of which of course we need to build another layers for example economy but in context of this presentation I'll focus only on this infrastructural part so the problem statement is pretty easy the requester wants to carry out computationally trustworthy manner on the resources provided by the provider and the provider should be safe in this set should be protected from the malicious of software malicious binaries and this simple problem stamp and most pretty well to high level requirements so yeah requester wants to be able to run any any binary in this network perhaps efficiently and this binary shouldn't need any additional changes just to be run within the network tasks input used by the requester should be exactly this way by the provider should be exactly the same as request who provided the binary that is run on the providers machine should be exactly the same but requester wanted and the invar environment should be exactly the same as requester wanted this is different from the first requirement which states that requesters should be able to choose any binary any absolute to any binary right here we need to make sure that no one can interfere with the binary before it is run the execution should be carried out the valid way no tampering no interference which means that the requesters can expect the valid results provided that task person called at the right way the output data shouldn't well it cannot be in fact altered by anyone in a way that's undetected to the requester and only the requester should be able to show the data to the external world meaning that perhaps the application that's taking the Constituent providers machine can look up the data but other than that no one should be able to leak this data to the outside world only the requester if who is do so and let's not forget about the provider he has to be protected from the cold as well so how can we meet these requirements there are a few approaches quite a few the point is that there is no single bullet some silver bullet for this requirements and different approaches result in me some of them or all of them in some way but not all of them with all the required features so for example we can constrain ourself to the terminus covers for each proof-of-work exist with the trade-off is that we only can do deterministic work we make sacrifice confidentiality and get into integrity but yeah we lose confidentiality on the other hand we may for example use third-party sources of trust this is either a third party service that is going to provide trust to the actors or we might simply use some check pointing to make the cheating less feasible at the same time we can use infrastructure such as trusted execution environments or mix of thereof so as I said there's trade off for every approach and is either inherent to the method such as using proof of work or task specific for example in rendering you can be interested in confidentiality if you're under a movie but if you render open source animation you it may not be so important you care about the integrity it's only also runtime dependent for example for trust at execution environments memory access patterns may have profound impact on the efficiency so this has be taken into consideration from my perspective you wanted a solution that is generic easy to use has well specified security considerations known to us and to others well allows for remote computation because it's what going with so as you know our trusted execution of choices as the eggs or more importantly it's not only SDXC the technology stack that can be built on top of the HDX and we achieve it by means of graphene graphene energy and this is quite interesting stuck and quite fine because it allows us to provide generic computations meeting most of the requirements with more or less known if you chassis considerations and non-security considerations we know that the communities around building those well evolving in terms of for example security and efficiency and this is that's why we move toward this question so just a sure recap what axis as we axis and Intel technology it's an architecture enhancement to the process source allowing protection of application and the data from the processes on the same machine even previous ones this happens in so-called anklet compute model and additionally there is a way of making sure that the computation takes place in the end life so it's called remoted station so this is quite powerful in quite good the problem is that well not a problem the issue that from developers perspective it's kinda limited so we can only run application if you call them from scratch preferably using intellect ak' you cannot run arbitrary binaries so they have to be modified and by default you have to specify static interface of interaction between the application in the end of life and the untrusted part of application of site of the Enclave the host and this is well something kinda limiting so we get this powerful feature of running insecure ant life but at the same time we cannot run arbitrary binaries which is important to us but this is a good starting point good building block for something more generic and the next step from our perspective is graphene graphene is libros based framework which allows you to run arbitrary Linux binaries in enclaves using all the features of SGX and from the application point of view this is just as interacting to the regular eyes so this important this is completely unmodified Linux binary and why we preferred graphing over other approaches well there are a few nice properties that we are interested in the first one is that the libras approach clearly states the distinction between the trusted computerize and the attack surface so as you see less is bigger than the alternative approach it contains more code it increases the size of the TCB but this is the part we and invisibility Labs can work on mostly invisible things lapse so they can put lots of resources to making it better not only then the community so we can improve on that but we have absolutely no control over how the user is going to abuse the interface so the smaller and the better stuff content it is but it's simply more secure places before sandbox we still want to have our provider security even if you add this additional layer and it's pretty cool because blue bars can be used in a way where it's easy to replace foster or gas device so what's the difference between the regular way you run the app computation and the graphene well in vanilla SGX well using the integrals decay you have to get the application source code Tyler it to the nth life compute model specify the interface compile it and run okay so this is quite different to what you have to do here you get the arbitrary binary right now it's well it's binary Ubuntu and Debian but other than that it's an arbitrary binary you pack it with the graphene and good to go almost because this process still requires some money well work to configure deploy and run the application that's why we flip the next step this is called graphene energy which is graphene it was a bunch of features that are interesting and should result in better UX regarding both NCLEX lifecycle and deploying the application so the protected files the protective Isis that library which allows secure or encrypted communication between the owner of the nth life or any party that initiated the computation and they aren't life itself and host which hosts the graphing on the end life cannot interview the files in impact that way the peer support is important because as I said we want to make sure that the computing side the provider side is security at the same time it makes it easier to configure the environments tools and scripts are there for the UX and bug fixes it's very small point but it's very important making graphene stable took a lot of work so it's a lot of work put by invisible things lab and all doubt there we're now or almost no important security for vulnerabilities there were quite a lot of bug fixes we related to the stability of the solution so for us it is important that those arbitrary binaries can really be run and this requires stable graphing so the features whether I think there is this way to showcase how it works is just to show an example and boy accidentally has one such example it's going to be Asian with brass its graphene energy with blender it's proof of concept for now due to the stability issues for example but other than that it is a working example so let's take a look at a few points of views of how to use the graphing the first one is the providers point of view so the providers just have to prepare an image with arbitrary application preparing this in which is mostly automatic the manual part is single action when SGX have to be enabled this may require some BIOS tweaking not tweaking just switching something in BIOS and configuring protected files which require central if manifest and specifying the run docker parameters during the run because simplify has nothing to do with docker by default so those protected files have to come on both sides in the end life and with doctor but other than that is almost automatic so the user prepares the container deriving it from the provided one the one with one image the exten plate is the one that's ready the user app is whatever the user chooses to to be the key is something that user can either generate using a script or it can be used by well from other resource and the key can be the eastern used to sign the consecrated container it happens with script as well so it is mostly automatic exactly the same process was used to prepare go an integration and blender integration so it was nothing different only thing is that we have a specified application this arbitrary up and the arbitrary miss bender here okay so the point of view of the handshake process I'm not going to describe in detail but the point here is that this process allows the requester to connect with the end life provider machine and the hosted binary to entrusted one trust encrypted channel and what's important here is that it's mostly automatic and if it's not then there are scripts to assist here so it was important for volume integration because the user interact interacts with the go and buy UI and it's not really interested in seeing all this so this happens automatically and there's script verifying quote for example and yeah the more interesting point of view of the application so the application point of view is what you see on the left simply noting application sees it as a regular regular IO and that's it no changes if you take a step back then you see that the framework is there the logic is there and it's doing its job under the hood for example they I always encrypted and decrypted and fly so it's it's there to be transparent to the application and in fact to users okay one more important and interesting point of view is requester point of view so requester still has to interact with the nodes in the network and connect to them should choose the notes we should compute something to him but it can be envisioned as if the requester was using more locally available resources of course this resources are not available all the time but other than that it can be treated well requester can treat them just as maybe not all the time available local resources making his computer more powerful so yeah as I said we have a working example work integration you can get more information about it at our booth and see the demo what we achieved with this integration is well we proved that the graphing and G approach is the valid one and that the features that it offers can be used to make such an integration it offered what we wanted confidential remote computation with arbitrary binary well the blender is binary of choice but it could be any binary well Linux binary at all and what's more important is that it's a clear decoupling of the binary and in the application that is hosted on the infrastructure in the infrastructure itself so the infrastructure is SGX and the application is an application which can be seen as a SGX as an infrastructure from one perspective which is important because we can rent out we can provide us can rent out this platform ok so it is pretty cool but there are still there is still some more work to do both on our side and in the general SGX intro side you name it so what would be fine to have is liberated in the interrelation service so that we don't need this central point at Estoril and place flexible launch control meaning that for now inter controls the Whalers include so lunch and can potentially disallow so many lives to be lunch so this should be you should be able to write your own lunch and life and lunch any ant with you you will you wish another thing is that there are some known attacks to enclaves and mitigation steps are required for these attacks and we need to increase the efficiency of the solution so this is from the as the Excite we want stable and efficient graphene and this requires back fixing proof as computation for economy and windows support because right now we have only Linux binaries so this is proof of concept this requires additional work but we can talk about use cases that potentially can benefit from this stack so the first one is one specific is local brief occasion EPA you know you have about two minutes okay so we look at verification you just make sure that the verification takes place on provider machine usually takes place on request or machine and you don't lose any trustworthiness this way other one is gamma limited anomaly that you have a run like setting we've trusted computers you can equip them with ESRI X me making a more powerful SGX node and expose it for example to encode transcode movies Identity Management inside and golem Network gamma meter network this makes sense because it's easier to seal the identities well keys inside the enclaves and provider knows and work with them automatically or other use cases that people can come up with should be able to benefit from this technology wider audience for example decentralizing server services we can do it using the STX of course it is not an easy task because decentralizing may require additional algorithms but the building block is there and it may be potentially used by sector multi-party computation it may be used to decentralized constant service atomic swap well we can do it well we can do it it can be done it has some problems because in atomic swap you need to make sure that the application has not only not forged but they reach the blockchains but if you use multiple and place then it's easy to reach the consensus and push it up push the transactions this can be used for example as a building block in distributed exchanges we can also seal keys alright either locally or remotely it's just a question of whether you trust the technology enough to see look silly or kiss remotely and this way you can for example store your identity in a distributed manner both geographically and by making K of n signatures required you can express the notion of uncertainty towards the how do you trust the TE if you trust them there is one if you don't trust it that much then there is more and in existing projects it can be used as well so for example we have a manual variable plasma with the central point being the plasma chain operator we would like it to be reliable and not to cheat so let's just enclose into n place and at the same time we would like to make it reliable in terms of block withholding which means that we can be centralize it and make sure that it's harder to withhold blocks by an operator another one is hard hard is a platform for governing assets in games and binding it to players so that players have the true energy both items and this requires keeping the notion of state on the server side so games against everything the internet player plays the game and the game server keeps track of state and make sure that player doesn't well in fact this most of the logic can be run locally in the enclaves and we only care about the integrity not to confirm confidentiality under step is we centralizing the whole system and making this local game server connect with others and build a bigger server and the last one is data streaming so we have this intermediate layer of SGX enabled machines and content creator creator uploads the content and by virtue of what svx offers only outdoors clients can download content the video streaming is the most obvious example here but we can also envision processing the data aggregated and data and hosting some custom logic for this data management so users can benefit from it not only but down long-delayed up and getting some results resulting from the data and this may be a building block for wider a bigger network of data processing okay so I know it was dense I know quite a lot information I hope that you got a glimpse of why the technology stack might be interested it might be interesting to you does definitely listen to us we are going to work on it well and thank you [Applause] you 