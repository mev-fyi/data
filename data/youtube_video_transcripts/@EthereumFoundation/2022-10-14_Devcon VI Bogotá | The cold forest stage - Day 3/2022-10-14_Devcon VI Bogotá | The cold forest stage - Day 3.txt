foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] thank you [Music] thank you foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] challenge [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] thank you [Music] [Music] [Music] foreign [Music] [Music] thank you [Music] United States [Music] [Music] [Music] [Music] [Music] thank you foreign [Music] thank you [Music] foreign [Music] [Music] [Music] [Music] [Music] [Music] thank you [Music] thank you foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign foreign [Music] foreign it's about the sun it's about the world it's about the sun it's about the sky without the road it's about you spinning around and it's about good morning everyone so happy to have you all of you here we are going to start their third day of defconing Bogota and bogotarian I'm local I'm so honored to have you all of you all of you here and we are going to start with math that is going to present us and thank you very much for being here it's a pleasure become an ethereum mistaker and a setup now note thank you very much Matt thank you let's go for it all right cool thank you everyone all right good morning to you all so myself Matt I'm from blockswap we're presenting the steakhouse so let's begin 60 second journey of ethereum staking right so what's the Stakeout Steakhouse is a meta delegation protocol built for ethereum staking for inertia onboard mainstream users ethereum doesn't have a delegation built in so we build that protocol in on Smart contracts so we can enable the node Runners to get more users at the same time the users can get you know safely stake with ethereum and it's completely 100 smart contracts and all formally verified right so let's take a step back and see what we can do in two minutes and alive you know you can have a coffee or you could do a speed run of ethereum staking So currently it takes about two days to do that staking so that we're going to do that in two minutes so 60 seconds taking and 60 seconds to set up run the node so you guys are ready for that all right cool let's go for it then [Music] counted two minutes foreign [Music] [Music] foreign thank you that's it two minutes you become a Staker you got a node up and running and you're good to go so what we've seen here is like a complete smart contracts tooling wrapped around ethereum deposit contract and you know you got a note you have it you have your coffee what's next yeah two minutes is a very short time so we got to do something else let's go get some LSD you want that I want to get a yes then only I'm going next slide all right cool let's go all right so now we want to get this node Runners to get more eat so stay can get eight validators for 32 eats so you know that you need 32e to get a validator but can we get that to eight so that's how we go from 1 to 8x so how does the LSD Works LST is a contract suit that is fully automated that will allow anyone to to send their um any tokens erc20 or Eid and it will automatically get feed into your node Network so there's a two marketplaces node operators can simply register with the four eight and they will be get you know anyone can run this LLC Network Let It Be Dao or you know you're taking service or anyone and this pooling will automatically get the de back into the user's hands remember the de this slash free so the users who are protected and the slashing is going into the node if they are leaking or do something bad so this is like an inclusive fair and open it's fully permissionless anyone can set it up under 30 minutes so here we're talking about who can use it it's completely compatible with dap node or steering packages or your staking services or liquid staking protocols think about you know we have Lido steak wise rocket tool and all these guys are doing amazing service they're providing not running but you also have solar stakers and you have institutions like coinbase and other people but it's all the fragmentation of liquidity we bring in this heat back the meta delegation protocol will allow you do the LSD networks underlying staked ease is completely fungible so no matter which node Runners are there they can all enjoy this and it's it's completely redeemable it's fully smart contract driven right we don't stop there so Steakhouse will also give you a complete visibility to validator level for stake earning and threat deductions so you know that if something goes bad you don't really have to go and find out we'll automatically get it everything is on chain it's completely coming from consensus layer and you can build web hooks top of that so more things are coming there right so if you go to joinsteakhouse.com today if your Builder it's gone 100 friendly all things are included you have templates ready to go if you want if you're D5 protocols you have that if you are you know if you want to do gatekeeping if your institutions you get there if you're a node operators you will have all the sdks that they're in you know npm packages everything is there so if you want to do something if you want to learn more about it come and find us you know you can join out this code or you can go to a website you know let's make your mistaking accessible for millions of users and make it safer too that's it thank you I speed run the time actually I get 20 seconds more it's a good thing to start all right yeah bigger plows thank you very much Matt no problem we have some seconds a quick question everyone anyone no okay thank you very much again a big Applause for for Matt all right cool thank you [Applause] at uh in three minutes we are going to start the the next uh the next talk we are waiting for the people that want to to attend and uh yeah let's wait two minutes please uh and we are going to have the the next uh talk is gonna be challenges of butter Lily's ability under ethereum execution model what a word it's gonna be very interesting from a Peter garam bulgi and try to do it my I'm doing my best Peter uh so yeah let's wait two minutes and we start with this amazing complex and talk [Music] foreign [Music] [Music] foreign challenges of parallelism ability under ethereum execution model welcome Peter hey everyone good morning how is everyone doing okay I know I know it's pretty early now so really appreciate that you get up early and come to this talk so my name is Peter and I'm going to talk about uh our work that we did at the Shanghai blockchain Research Institute and what we did was we investigated how hard it would be to execute transactions in Peril on the evm as you might know the evm is designed in a sequential manner usually you execute all transactions you know block one by one so the question is can be paralyzed that what we found what I want to talk about today is two challenges that we identified and the first of those is transaction dependencies so you're probably all familiar with uh how ethereum Works basically we have a global share state which is the state tree and then theoretically a transaction could access any point any entry in this state tree that is where it can access a slot or not that's defined by the contract code so for instance if we have two transactions from two different accounts and both of them call the same contract and that's contract reads or writes the same entry then these transactions will have a conflict or a dependency now on ethereum this is not a problem now because we execute transactions sequentially but if you paralyze them then this will become a problem because you cannot concurrently execute such transactions so we did some evaluations basically what we did is we collected some storage traces for a few hundred thousands of blocks from you know historical data on ethereum and we did some simulations with some OCC scheduler and what we found is that the theoretical maximum speed up is as little as four times so even if you have eight threads or 16 threads uh at least on the VM level if you don't consider storage you cannot do much much better than four times and that's kind of an underwhelming result so some examples some simple examples of these dependencies for instance if you have two transactions to the same erc20 token uh if they send from the same senders to the same receiver then they will conflict on the balances mapping or if you have two transactions swapping tokens and unit swap swapping the same token pair then both of them will modify the reserve variables so that's also another kind of conflict or if two transactions are minting and nft then for instance they can conflict on total Supply if the contract is keeping track of that so we identify this and this is an issue I think this severely limits the paralyzability of evm transactions as they are today and we have some suggestions how we could improve this this is kind of early just some ideas to to throw in there the first idea is to use a sharded counter so from the examples before you saw that many of these conflicts are on complex on a single storage slot which is like a an integer or some counter and yeah two transactions read and write the same slot so the idea would be to Shard it into multiple slots so one int is represented by three five seven slots and then we could route transactions based on some heuristic to different slots let's say we take the last byte of the hash of the sender address so two transactions from different senders would probably with a higher probability they would modify different slots in the contract and the second idea that we came up with is a lazy ad op codes so for in this example we have a very simple function that's just incrementing a counter but when you compile it to evm bytecode what you have is a storage load operation then you do something with that value and you store it back so to concurrent transactions uh we'll have a conflict on this like concurrent storage load storage store operations and uh you know we cannot parallelize them the idea here is to maybe introduce a new kind of semantics for addition lazy ad or commutative ad and that would be evaluated lazily so these transactions can execute in parallel and then we basically batch those two updates together into a single update at the end and that could be executed at the end of the block for instance and the second challenge that we identified is that of determinism so as you might know in parallel execution there's a you know there's a lot of non-determinism and we can make sure that two transactions have the same result or two executions but there are other types of non-determinism for instance here we have two executions of three three different transactions and let's say transaction two depends on transaction one so in this case on node a one and two are executed concurrently so two has to abort and be re-executed because there's an update that it could not see and on node B they execute is sequentially so no transaction two can commit directly so our idea is that if we have parallel execution at some point we have to introduce some incentives because if you can trigger some aborts that kind of opens up the door for some denial of service attacks and if we do associate some incentives with aborts then to these two executions in the example they will yield the difference final state so that is going to be an issue two nodes will diverge so for this we kind of came up with a new scheduling framework called optimistic concurrency control with deterministic aborts and the idea is very simple instead of deciding these abort comment decisions during runtime you made this decision prior to running the transactions in a deterministic way so the uh these decisions will be deterministically you know the same on all nodes and this way even if you attach incentives to these decisions the final states that different nodes will arrive on will be the same so this is the OCC da framework just to summarize kind of the takeaway from the stock so it's pretty hard to get parallelization done on the evm because it was not designed with this in mind but I think it's going to be worth it for instance if you manage to scale ethereum on layer 1 or layer 2 with a higher transaction transaction load per execution will have a very big impact on the overall uh yeah efficiency of the system so if you're interested in this uh you know research Direction I would encourage you to you know read up when I think about it and we actually have a publication about this topic so if you're interested in a much detailed discussion about the ideas that I talked about here you can go ahead and scan the QR code and read the pre-print and that kind of concludes my talk today thank you very much for coming thank you very much Peter we have time for one quick question if you want to ask in the audience uh I was wondering uh what your thoughts are on parallelizing just balance transfers which are much simpler um instead of contract stuff just like ethereum bounds transfers like certain transaction types are radically simpler than contract execution and I wonder if you had any thoughts on that yeah that's a good question so here we only considered con contract storage conflicts but actually there are different kind of conflicts you know another part of the state trees the account with the nuns and the balance so yeah simple transfers are expected to be much easier to paralyze and even with contracts with access lists you get much more information that you can use to paralyze but I'm not sure the majority of transactions I think are contract calls so I'm not sure about impact of this but I think that's certainly something to to look into thanks again okay thank you very much Peter and Applause for Peter please we're going to continue with uh the next talk that is how to use executable consensus Pi spect from Xiao everything to restart you know the the computer science strategy to restart everything so everything works three minutes to start the next stock price foreign [Music] [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] comes from Taiwan and she's show away one and she's going to talk to talk to us about how to use executable consensus spice back you're very welcome oh thank you hi everyone I'm shower I work at Eastern Foundation consensus or IND team so this is a lightening Talk of the consensus Pi speak so you can found the code days with this URL or the UI QR code and this is a cc0 open source python project now we have like more than 100 contributors so far and most of them are actually Eastern core deaths thank you for your contributions okay so this project has three main purposes so first this is um the Center of Eastern core consensus space and second it is also executable and verifiable Python program that you can use it to run the consensus logic with and the third um it's also our test Factor generators for the clients the consensus layer clients to test their implementations with and so um if there's a new feature that we are thinking about to apply it in the consensus side like the big engine side so you might try to prototype it with the markdown files that I will introduce later and and we will implement it in markdown files and we'll do the test and the test vectors would be generated with the python programs test case like the pi test if you have used python before and for clientings they can download the test vectors to test their implementations so um if you understand this process you can see that oh sorry um the space is now important and unique component in the whole clrd process and it also helps us to found some very basic bugs before cleansing started to program to actually prototype and test it um also it helps tie things to sync with the Core specs more efficiently the reason why we chose python instead of other language was that because it's probably I think python is probably the most readable programming language for most Developers like you can say like python are just pseudo codes and okay now let's see how to read it you know so I shouldn't stand it um so this is the specs folders and here you can see some um the blue ones are the main hot folks that already be implied and the yellow green parts are the work in progress features and also inside one of the directory you can find that the specs are actually written in the markdown files and like the most important entry might be the big engine spec and us is also like for choice or p2b interface in it and we can Define the constants and configurations just like you are writing markdown and in the tables so looks like this and we also Define the classes uh the SSC containers so listed objects are for the consensus objects because we use it to do the serialization in the consensus object okay then uh so this is the example of the state transition functions we can just write down the uh speak write down the python functions in the code block of in the markdown files and um and also you can see there are some assertions so if the input is uh if the assertion throws and exceptions then we could say that okay the input is invalid because it doesn't pass the test okay that's some more useful links for you to understand more about the spec itself and also um so this is a elf I still call it the elf in the past uh in the past week in the setup dot Pi file so it is the program that helps us to pass the markdown files and convert them into a Python program and so the readers they don't have to reading the the pi file they can just read the markdown files which are more readable no Excel example of um so if the phase 0 is the base layer of all consensus specs and out here if you want to understand art here you have to merge merge extend extend the phase zero space and new features in the out here so and as well the Bell tricks they have to merge three of them so I have one minute I have to do how to use Pi speak and so installation is easy just install from Pipi or from source code with this command let's see hope you can see it okay this is a very basic hello world example and this is and simple to you can just write hello world into the gravity of the beacon block and in the first line is you can import the whole many battery expects us we need it to Specs like that so you can use it as the playground and see the example code in pi spec to try to play with it okay this is how to write your first Pi specter's case okay you can there are already some existing um helper functions to help you to prepare your the beacon block or to manipulate the beacon state with and there are some assertions here these are for us to set our test scores of this test case like an important part is here like you can see there are some Guild uh python yell comments and it is us to for us to Output the test vectors for example this one we output the pre-state and the side becomes blocks and the post state okay so output file will be later and the list is the pure functions for the contents to test with okay documents if you want to see more documents you see the urls and okay so this is the most important page actually so how to contribute with pi speak if you are interested in helping with consensus layer and the I think Pi spec is a really good start to learn and contribute with and there's a different labels that you can participate in sorry and you can do it from just fixing a little typo or to submitting a bug report and if it's got accepted and I think the F will be happy to give you the rewards okay and we also have looking for some in-house resource for pi spec implementations so if you are interesting and feel free to reach out to me or if you want to participate or have any questions okay thank you thank you sure if you want to ask some question to show I invite you to do it so they they the the this talk because we have to continue with our next speaker very welcome Peter Davis you're going to find your microphone oh yeah the same microphone could be and Peter is gonna present e e l s the future of execution layer specifications whenever you want leader but you're very welcome thank you for coming to Bogota hello thank you and hello I'm Peter I'm from the eels team um eels is an acronym for ethereum execution layer specifications and we're working on the future of how do you specify the execution layer so we've just heard about how the consensus layer do their specifications in Python if you think what I'm talking about is similar it's because that's where we got the idea from before I begin I want to talk about what I mean by the execution layer we're only here interested in the state's transition function so you have like a long chain of blocks you have some State you add in some new states and the question is firstly is that block valid are you allowed to add it to the state and secondly what is the new state afterwards we don't care about anything else we don't care about networking we don't care how we implement reorgs we only care about what is this state transition function So currently let's suppose you're like someone you want to know about the state transition function the Executioner how it works where would you go for information well the first place you could go is the yellow paper where I have the yellow paper here this is a lovely extract that explains what the hell of a validity conditions are and some people think this is readable um they are in the minority if you're curious about the like weird mountains um they're not mountains that's how mathematicians write down um the other problem with the yellow paper is it's actually really out of date um this is the Berlin version the reason it's the Berlin version is because no one's implemented London yet secondly you can look at eips eips are individual slices of like changes that specific change proposals and they're like you can look at them and see the history but they're they're not you don't they don't tell you about the relationship they only tell you about individual changes in isolation so if you like for example if you re you can read each where it's like well this thing says this and this thing says this but how do those two things interact and you just can't get that from the IVs um at that point you can then go here when I look at the test suite and the test Suite is great it is full of loads and loads of tests but ultimately it's just a pile of tests it's not particularly well organized and if you want to find some Behavior like maybe you can find somewhere in the test Suite where that behavior is tested but it's not specifying anything and then you can just give up and look at the gas source code fundamentally if you've like got to the point where you're reading the having to read the client source code to work out how the execution layer works we have failed at specifying it these clients they are big like complicated high performance pieces of code they're not like designed for the reader um the other thing about specifications is they need to be part of a standards process you can't just like fail we're going to implement all this stuff and then we're going to update the specifications later this is the big problem with the yellow paper is that it's not part of it no one like proposes A Change by saying this is how I would change the yellow paper and so the yellow paper just gets updated later on as an after foot um the other problem is that the other paper is just like a document it's not executable you can't test whether it works at least close to eops as well so this here is a sample from the EIP that changes how the cost of the mod x pre-compile works it was proposed it was accepted and you're like this is nice there's a nice piece of easily readable python that says exactly what it does problem this is this python is wrong now no one has ever executed this python because if they had they would discover that it doesn't really make any sense um and I mean as Donald crew said Beware bugs in the above code I have only proved it correct I haven't executed it um so you need uh so this is our approach we want to start with like a code first approach to specifications um we take like Python and we're not just writing any old python we're like trying to create the python that if you open like some programming book and someone had an example algorithm that's python so we don't use classes it's just methods and effectively structs this is like the common language that all programmers speak this um you know anyone who's done any programming will immediately know what this is you don't need to like understand pure mathematics like you do with the yellow paper and crucially it can be executed we can test it if you want to know what the python python does you can run it if you want to compare it to a client you can run this you can run the client do they do the same thing we're purely here interested in readability we don't care about performance it's extremely slow it can think mainnet I think it takes about probably getting about six to nine months if you want to start from Genesis and get to the head of the chain it's not a viable client we also make some very weird like design choices we look at hard Forks in isolation so if you have there is if you go you'll see it's ethereum slash theorem slash Frontier and slash Homestead and each of those is an independent implementation of those hard Forks so this is like terrible for like code duplication like literally every time we include a new hard Fork there's like this massive coughing of like thousands of lines of code but it's great if you're like a reader you want to read it you know you can see like how Homestead works completely in isolation you don't have to have to think about how it relates to other things whereas if you read a real client it's just a pie of like if we're later than spiritious Dragon do this thing otherwise do this thing um if you do want to see look at comparison so we're developing specialist diff tools and those diff tools allow you to compare like this is exactly what happened this is the specification for Frontier this is the specification for Homestead this is what's changed exactly line by line in code that we have tested um so here's a sample this is the S load op code to give you like a feel um a number of things here first you can immediately see with really trying to make it as readable as possible we've also divided that up into these like individual sections which we've given headers um this actually like matters there are a number of quite subtle semantics that the evm has that relates from the fact that gas um calculations are done before any computation is done and you can get very confused about some of the subtleties of particularly the cool lock codes if you don't realize this um now I want to move on to like now we have these things how can that like affect the development process how does having these specifications make improving and building on the VM better and I think it's helpful here to think about two signs of development process on one hand you have like your r d people think of italic and then on the other hand you have implementers um Peter who's the gef team lead the r d people um they're doing research they want to throw together some simple python they don't really care about performance they just want to write it test it and think is this a way we should do this thing um on the other hand you have the implementers who are trying to write her robust production-ready system they want to know every single subtle detail of every single thing that has to happen because if they get it wrong that's a major security incident and also they have to like spend all their time thinking about the subtleties of like dvdiff performance which is a endless bug there for anyone who has worked on the execution there um and Eels provides a common framework it allows these two sides to talk to each other it's a Common Language your r d people they can write that Python and this is what they already when they're doing their research they write little python models well now you have these specs if you've written that little python model you couldn't like stick it in eels you can implement it eels it's not much more work and already you have like you have something that you can then show to the implementers um this leads to this sort of development process you have your r d people they develop their idea they prototype it in eels um you then give it to us at the eels team and we as part of like the governance process integrate all of those things together we make a hard Fork exactly what goes in obviously is like a theme for all quartet it's not like we're taking all the power here I mean quickly once you've done that there's a whole bunch of things you can do because you have an execute full spec we are executable spec will be able to fill all the test sectors it might even be able to like run its own very miniature like two million gas a block test Nets um that means that you can have tests and for size specs before you've even written a line of code in your production class this like frees up um production plans to not have to be also the r d playground that they cut that they sometimes are it is not like a perfect thing like often you have to go and Implement models in production clients to deal with performance issues there are networking issues we don't deal with but currently like anyone who wants to propose any like significant change they have they like implemented in geaf or wherever um whereas now you have this playground to do it in and critically this is like a sort of paralyzation stage because like currently you you currently you have this like situation where like the implementers and the r d people like get have to like interact much more tightly if you have this like eels process you can like separate that out and do these in parallel so that you can like have the implementers finishing up one-hearted Fork while eels are like getting ready to prepare the next hard Fork whereas if you're like busy modeling things in geaf and the implementers are also tied into doing the r d you just don't get the same parallel efficiency um I'd now talk a bit about where we're at we have implemented all the hard Forks um uh the merge is still appear but we're nearly there um we're going to need to do a bunch of like um refactoring um and then we need to freeze the code because we're asking people to like build it we're saying if you want to take an EIP and you want to build on top of that change it you have to take up our latest Fork copy it Implement your own changes and we can't ask people to do that sort of thing until we've freeze the code and said we're not going to like refactor it under you and we're not quite there yet once we've done that we're playing to like Shadow the current governance process for Shanghai so Shanghai is going to go through in exactly the way that all previous like hard Forks have gone through but we're going to also be working with a whole bunch of people to try and Implement those changes changes as eels proposals and then we're going to like see how the government's process works and then hopefully we can like improve on that and talk about moving away from some of these Legacy approaches to specifying executioner uh finally um there's the question of how you can help first if you actually need your help yet we're still um uh code we're still coding we need to finish that once we've done that we ask you to like Implement your favorite EIP um and like give us feedback like how was it because we don't want this to be a world where we as like the eels team can like develop what we want develop things and we can propose changes but no one else understands it I don't want to be in the situation that you're currently with the yellow paper but actually only a really quite small minority of people know how to write Tech um and how to change the UI paper so I want this to be like an open process where anyone who's like oh I want to propose something to just clone that repository run the new fork tool and Implement their change and like produce a proposal without being burdened by having to think about like how the gefdb works or whatever or whatever else they're going to do they're modeling um yeah um that's the end of my talk um we are have plenty of time so do people have any questions bigger plows from Peter I see some questions there okay uh has there been any thought or consideration because the yellow paper talks about things like State growth and those sort of things that'd be nice oh can you hear me by the way not quite can you speak up a little bit okay why I can like can you hear me now later I think okay can you hear me now yeah okay State growth so if I make a commit to the execution spec and I want to see how that influences State growth over time and like the capability to run a node and things like that have those sort of things those mutations been thought about at from a uh you know metrics Gathering level um I mean this is a thing that like people who implement this like think about this all the time um like I mean yes because we hold the whole state you can like model that sort of thing I think a lot of that is mostly about gas pricing and gas accounting and like there are advantages here so for example there was a proposal about how withdrawals worked because I have the execution specs I could like look up the table of gas prices in the execution so I can say well this is how much this sort of thing should cost and like yeah but I don't I don't think our tool is like particularly useful for like modeling State growth particularly you just need to understand like What patients can cause State growth and how much gas those could those cost uh yeah all right next question how is this going to interact with the reference tests that the eels there's a lot of crazy edge cases in reference tests um such as integer boundaries and other strange things oh yeah I mean I I've had a lot of fun with crazy edge cases I should do a talk on Crazy Edge cases once they at some point they are amazing I mean firstly we run all of the reference tests so if you go in our test Suite you type talks the automatic um like test Suite will run all of the reference tests and So currently we pass every single one of them and I have like learned an awful lot about the crazy educators of the VM we actually hope to confirm that you know we want to be the mechanical builder of the um execution test So currently for those who are not familiar we have test fillers um which are like they're not like complete test they say here is what the free State's like here are some assertions about what it should be like per state then you run you take a client um it's usually deaf and you run a filling process that gives you a completed um reference test um that reference reference test and it tells you what the state bridge is it tells you basically exactly what the block should be after you can perform its execution um we will think that we should be like the chemical filler of those tests we can fill them before the execution clients had even implemented those tests um and then we can if you want to and that means that clients can like use the test to build their clients whereas currently until you've implemented something you can't fill the reference tests in you have to invent a production plan um there is also and I haven't thought about it in great detail the question of whether we should use the whether we can like analyze the structure of the avails to like generate new reference tests and basically do some sort of path analysis and say well these are the possible parts of this sort of code base can we follow every possible path and make sure there's a reference test that follows every possible path but that's like r d that we like haven't done yet and next hello uh hello yeah what's your question uh you met you mentioned you implemented um no you care only about State transition function and that means you implemented your own evm and you can fill the test and and you provide the post State hash yeah cool yeah like if you have a chain and you want if you have a chain and you want and you have a block and your proposal is to add that block to the end of that chain and update the state we can tell you we are tests specify exact same thing specifies exactly um whether that block is valid are you allowed to do that because blocks something particularly vanderbox tend to be invalid um in that case you're just not allowed to do it and if it is valid it tells you what the state changes are and then you can do the next thing we don't do anything else that's the only question we answer we must meet Helen um so this is obviously a inspired by the consensus specs or the pi spec are there any particular architecture changes or learnings you've gotten from the pi specs oh yeah that's a really good question okay so there's a couple of difference here's here the first one is that the um Pi spec is price spec is a document that can be it is a bunch of documentation that can be compiled to code so if you look at the pi spec it's a bunch of MD files with some code in then there's a compilation process that spits out code we are a bunch of code um that is that we can then you can then compile using standard documentation tools to documentation the second big change if the pi spec does this like incremental building stage where um in the post-spec you have phase zero and then you have out air and the outpair documentation just adds a few changes to the phase hero whereas what we did was we copy we copy the entire thing um and create a new version that contains everything which means because otherwise what would happen is if you wanted to know how um the merge worked for Paris Works you'd have to read Paris which would say these are the changes versus London uh and then London would say well these are the changes versus Berlin then Berlin would say well these are the changes versus just keep going on and on and so we don't think that's a particularly good way especially because a lot of the historical details are really quite Legacy and not things that people need to be concerned with nowadays because they've been changed out and so we just like do isolate they're completely isolated files this calls like a whole bunch of code duplication issues um and complexity but it does mean it's like really simple you can read a particular Fork completely in isolation from any other Fork whereas if you've got you can't do that with the consensus space uh cool with anyone else um thank you here for the presentation I was wondering for the executable specs how do you think it might change and improve like the EIP process of ethereum there was lots of conversation in terms of governance of how when you're proposing an EIP maybe this should be like a requirement to kind of speed up the testing process but would love to hear your thoughts on how it could be integrated and what parts of it it could improve to change that that EIP flow oh there's a lot of opinions on this um so I I will say um so on the one so there are some people um who think that we should get rid of the IP process for the um execution layer we should abolish erps the execution layer and instead you should have whatever eels proposals are called and it should just be if you want to change the execution layer you write an eel's proposal you might write a small amount of ancillary documentation but mostly just quite a PR there are other people who think that we should do a like hybrid process where there is a EIP and the EIP comes with as like a mandatory component a yields change um oh you actually don't have particularly strong opinion of this I can see the argument for both sides um yeah but there's lots of thoughts on this I generally take the attitude that I'm gonna wait until like we've started using this and people have like seen it in production and then they can start talking about what they actually want to do we have time for uh last question if someone is interested can I also if not thank you very much Peter amazing presentation [Applause] I love your share amazing one thank you it looks so good in this stage uh our net next talk will go will start in at 11 o'clock in four minutes and it's going to be hybrid PBS From cl's perspective of tyrion's Sao so be prepared jeans please four minutes we're going to we're going to be back here foreign [Music] foreign [Music] [Music] [Music] foreign foreign [Music] foreign [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] [Music] foreign okay we're going to continue with this amazing agenda in the third day of Defcon 6 in Bogota and our next speaker is Tyrion Sao and he's going to talk us to hybrid PVS From cl's perspective please and Applause for ethereens very welcome hello thank you for the warm intro all right let's get started with my slide that's my slide hybrid PBS interest layer I'm Terence I'm a core Dev and as as of earlier this week we are proud of option Labs so I had to change my name real quick before I get in trouble so let's get started so this talk is about consensus layer interfacing with hybrid PBS it's not so much your life Searcher Builder your typical mevitars so we want to understand more like what does it mean when consensus layer interface with hybrid PBS from like from the it's it's tonality such as latency file and censorship then we'll also talk about mitigations so just to reiterate what I said everything in the purple we will go over it we'll have three layers matte boost which is a relayer aggregator we have consensus layer client a solution layer client availability the client and this is what we have in me and my team and other teams have been working on over the last few months so background why are we here right so there's options and options are nice so the first options which most people use these days is normal blood processing so what's the consensus of their client you know which validators you're serving you know when validators are proposing a Blog so you will build the block for the validator and post merge we utilize execution layer client to prepare payload so consensus of that client uses attitude on their client to prepare payload they put a payload in the blog that passes to the validator validator signs it return the blog and consensus their client broadcast the blog so here the separation concern is nice here because like institutional client and consistency their client they're both like very complicated piece of software but they only need to know each other through the engine API and so the separation so the separation of concern is very nice the second option which people are starting to use more and more now is that I can Outsource blood production right as consistently client if I want to participate in the Mev game I can utilize the relay Network I can hey hey relay there what can you propose a block for me and a Blog are usually more profitable and also in the background consistently client can also talk to the attitudinal kind of like hey can you make me a backup blog in the event that the relay network doesn't work I can still use it and this is kind of the Paradigm that we're heading towards and it's important to understand like what does this mean so this is today's number and hopefully you guys can see it I capture it this morning there are 55 of the network participation is using Mev blog production it is outsourcing their blog and then out of the 55 81 percent are dominated by flash boss relayers and the Seven active three layers so let's talk about the first risk latency right so when when you propose a block normally under your local setup you have your consensus layer client execution client without their client so how does this work you ask educational client to prepare payload and then you pass the payload to the other client to sign it well their client sign the payload and then you broadcast the payload simple easy very easy to reasonable right but with Mev blood processing it's a commit and reveal approach right why is it coming in reveal because like um as a relay as a builder you don't want real data client to um steal your um transaction if it's if it's if it's in the clear text so you made them sign it first then return the signature and then then you give the full transaction so given it's a commit and revealed approach there's a more like Steps in the middle right you first you get a header the relay Network return the header then you sign the header and then you submit the sign header and then relay Network return the full payload which is transaction in clear text so as you can see there's like two more steps here right and besides that it's just like it's also on a different network right it's not local anymore so ideally like you want to relay Network to broadcast the block as fast as possible instead of giving it back to you and then you broadcast it so hopefully like most of the relayers have been like publishing blogs like by themselves instead of just passing back so for that you do save some latency which is nice so so let's ask yourself right do the additional run through and latency matter because like I said before when you have your El and your CL they are both in the local setup it's lighting fast it's it is reliable because they just go through Hardware they just go through electron circuits right but if you really relay Network it's slightly different you are actually talking to some like um infra provider gcp or AWS on some regions and um so this is I was able to capture some numbers unfortunately I haven't been home for like the last two which so these numbers are slightly old but on growthy with a thousand validators they're they're not many validators but yeah just a thousand thousand validators but given it's worthy the network topology may be slightly different but I mean I hope we can get a picture here right here I'm not even using me boost I'm just talking to the relay directly it takes about three times slower to propose a block given the additional latencies right so what does this mean when you when there's additional latency because when you propose a Blog you shoot how it works it's like you run the four chairs to get ahead then you build a block based on the head then you get a payload from the execution engine then you broadcast the blog right and then and then at the four second mod which is one third of the seconds per slot a testers will vote right what is the head of the chain and if the testers did not see your block then your block may get orphaned and that's not ideal you do not want to lose the block right so this is what we don't want we don't want something that's taking up so much in the Middle with the gear header second header and suddenly blind block right so this is this is this to me is worrying and then let's look at some more numbers some block arrival latency differences this is actually captured on mainnet this is my home setup at home with 300 Mega big bandwidth so this is a home setup as possible and we capture over 15 000 samples and map and then the MVB block took about 500 milliseconds longer right and um and what does this mean from the some beat anticipation timeline it just means that if the block takes longer to reach to to arrive then a testers will unfortunately miss it like that right you don't if you're waiting like in the front like for that alone then you will essentially eat up the time that you have so we don't want to be the lazy block we don't want to be the late block because you would get orphaned right at the top example Block C was supposed to build on plot B but you build on block a and then because the blood B was laid on the bottom example of block e instead of building on block D because of proposed boost or something it was supposed to be the head but it's not the head so block e built on Block B therefore C and D got orphaned right so another set of numbers 50 of the orphan block actually came from relayer from September 17 to 27 and there's our the orphan plus slot the relays they're using validator ID the entities right and that's unfortunate I mean I mean you could ask that okay well maybe it was going to be often one or another we don't know but still like 50 off they come from the relayers so it's not just like sunshine rainbow and Butterfly right this is like like this is I mean there is risk to this right I mean like we often tell people hey we should use the relayer because it's more profitable but it also comes with risk right if you're using a real layer makes your blood two to three times more profitable but you get off in 10 of the time is it worth it that's something that you have to ask yourself about and another um risk I think with latency is your centralization right the whole point of Mev Booth is like to make validated decentralized because now we can extract Mev at home so it gives everyone equal access to Mev you don't want to look like this right you don't want people to start realizing oh well if if I have a better latency then I won't get orphaned so I'm gonna move by a home station node close to the relayer so it it becomes a negative externality this is what we don't want so what's the takeaway with this right the takeaway is latency matters both for hybrid or even maybe for little PBS right and latency can lead to centralization risk which we have known a long time already and it's actually really hard to optimize towards enable latency right at the client level like I don't think there's much we can do I think on the map boost side there's talks about like instead of Marshall or martial Json with martial module as the Z there's some input from there from the real perspective I hope like they have really good Network config they have a lot more peers I feel like they have a robust like infrastructure there I think that's as much as we can do because unfortunately if someone just have a slower internet connection they want to use a real layer I mean this is going to be offended right it's important we educate these risks so precise latency there's also false too right so what does fault mean fault here means that sorry that's like a little mess up there point so when you ask for the header the relay Network failed to return the header so that's the convey phase or the happens when you submit the header the relay failed to replay failed to reply the payload so that's the second fault so we'll focus on these two type of faults here right so the first so the first category of faults is just get header false when you get header and then the relay net will fail to reply and they can be categorized as like you have a mail form header you have a consensus invalid header you have a payment evaluate header or you have a non-conforming header so I will go over them one by one so what does it mean when like a head is male form right it just means that it is syntactically invalid it has an invalid structure it has to email the signature can the consensus can can the CL client detect it yes it can right because when you're show it if it's not the right structure then well you know it's run you also can verify the signature so this type of so this type of like false we can detect and we can mitigate so this is fine another type of fault is just consensus value of header so that just means that the block hash is reality the transaction similarity but for this we unfortunately cannot validate because we cannot see the full transactions right we are we cannot calculate the block hash ourselves so that's something that we just have to trust blindly that's unfortunate but it is what it is then you have payment in value header so at this type of fault it just means that well um the Builder was promised to pay the validated proposal some like some Eid but it failed to pay so consistency consistently their client cannot detail that type of file this which we like relay we we try to relay to simulated that's why relayer is trusted for us and then there's also the non-conforming head that just means that when the validator register you basically say hey this is the guest limit I want to use but I guess they may is incorrect the time stamp is incorrect the block hatch is incorrect right can consider client detect that yes he can so those are like what we say the commit faults so now we're jumping to reveal false so this is the set second type of false right so what are some like reveal thoughts so the payload could be invalided or or the payload is available and keep in mind there's no following back for this like if you just you sign the signature ready right so at this point you can complain on Twitter you can probably like complain something else but there's not much you can do here to basically make the block as a whole header it just means that it is invalid like the blood the flip Halo does not match the header can the consensus layer client validate yes it can but hey it's too late right you at that point you lost the block already there's not much you can do and it's the same with consensus in value payload you can validate the transactions now because we see everything but hey it doesn't work too late sorry they're unavailable this this means that the relay Network just went to sleep when malicious decided to turn off it did not reply back the header to you so it did not fulfill its commitment right and then you know when it does that because he never replied he never received the payload and there's someone and there's still there's nothing you can do there because you already signed something using interest out there so we have this concept of falling back to a situation layer client just if gear header goes wrong you can produce real local execution client that's that's that's probably fine but if the game payload fails then you cannot produce with your local execution client just don't want to double sign so the return header the first commit can fail two ways it can either fall or timeout right we do prefer far better the skills that you get a response right away you can start proposing the blog right away if the timeout it kind of sucks because you have to wait for them to time out you wait for them for a second then you lost a second of like your precious time there so let's go through some many incidents I don't want to sound like I'm like like pulling out their fault incidents but I do think it's important to go over this type of incidents so that we can learn as a community right the first type of incident so the the first incident is um September 16th flash boss relay they fail to our Marshall deposit for the payload reply the damage with mystery blocks the second one blocks rewards relay this one is when the this one's going to relay did not validate the block and then and then they rep and then they reply it the consensus it smelled it we miss Ada blocks here the third one is philosophers again this is consistency value payload and the damage of that is 15 blocks was missed so this type of things do happen right false happened and and then it seems like it's mostly happened on the on the commit and then also the reveal phase so we need some mitigations like the circuit breaker and for example as a beacon client I can detect when there's a likeness failure and then when the life has fitter is determined by clients say if you miss three sauce in a row or if if or the chair misses a slots over 32 slots if that type of things trigger then we just default to local HD engine right so this is to prevent like a dominant relay Builder go offline and it doesn't solve like the cases I mentioned before just because like those are happen maybe like 0.1 percent of the time for this this happens all that but but for this like this is a stronger defense then you have your relay monitor for example such that you can monitor relay space on performance such as behavior and such as Behavior which is like safety and liveness and performance which is latency and people can see how the relays are performing and people can figure out okay based on that do I want to connect with every day so did it just makes the information more available then we have features like beef filtering solos so as a proposal you can you can say hey I only want to use the relayer if they give me something that's over this value right so there's some like so there's some nice things you can do there so what so what is takeaways right we're still early but we need more robust relay and also we need a way to hold relays more accountable right for example a simple idea I just you monitor all the missing slots and the orphan slots live and then you pull the API the relay API and then if you see hey the missing slots is coming from the relay okay I'm gonna just like shout out loud Tweety or something so that people know hey there like there is an incident right there is happening live you need to turn off your relay or you need to switch local processing in terms of just like faults time out I think I prefer get header thoughts then that property has the timeout then I prefer get payable false just because like with gate header files you can propose the block just like still and then it's it's most likely fine and I do believe that relay quality will improve over time just because like it's still relatively early-ish so I mean they're still learning and we're still learning and something that waterproof over time so okay last section censorship that's something that we have been talking a lot about so as of today um let's see what number okay set 49 of the many blogs have some sort of a compliance building and that's unfortunate just that's already over half right so we have to understand like who is sensor right like who can censor Builder can sensor right if you're a builder you don't want to build blocks they can send like offer compliance transaction you can censor all the relation sensor as well right so so therefore it's really hard it's really hard what's the problem here problem here is the enemy boost is a neutral piece of software it doesn't care about censorship right and then the ux of just figuring out like how do we defend and censorship is still early right now it just essentially everyone chooses the relay that's non-censoring that's it right but then it's hard to figure out who is censoring at the given time there's it's like you don't know who is censoring I mean we just look at the news with the Google search and that's it we need more like information there so potential Solutions right these are very instrumental ideas that I just have been thinking top of my head right you can have some active encryption such as matte Blue C or this you can have some solar essential issue or censorship Oracle so for active inclusion right high level how it works it's just like a proposal you express the intent to like force transactions into the payload right and the relayer has to present those slight transactions to the Builder and then proposal will only accept those transactions if if they are included or the block has been full right so for this you do require some sort of multi-proof to make sure that hey the transactions are actually included and then consistently a client can do the validation so what's the downside with that right that's not that we just like there's more timing because now as a proposer at the previous slot you want you do need to send the transactions you want to send a transition transitions during the previous slot so we need to figure out what's the timing for that and now the proposals also need access to the mempool to the institutional area State there's also some latency complexity here just because like now it may take longer to propose a Blog and then yeah and then you can also have like this is like a poor man version of just like censorship like um filtering basically like like proposal you just walk into the mempool for like the top and transactions that's that's that's based on ksv at the given time and then when the reply the relayer is so happy short approve that hey this top and guest transactions are included unless that the guest is at its price at a given time right so we don't really trying to force transactions inside but we just want to make sure that the top end transactions are actually getting in and and then which and here we're assuming that okay if someone's in the sensor someone will probably use a higher dsv and stuff so it's not ideal because it kind of loses like this inclusion control but it's probably easier to implement then there's also like ideas like censorship Oracle you can introduce like a new actor to produce censorship kind of like relay Monitor and now as a proposer I mean boost when I receive a header I can add this Oracle this new attribute hey is this a sensor one if they reply yes then I won't use it right but then again you're putting Trust on like a new actor and that's obviously not ideal because like because who is going to monitor the censorship Oracle and there's always that problem so there's has been a lot more research going on which I'm really happy to see right Vitality posted a research a couple weeks ago on just like how do we constrain Builders without bringing blood proposal burdens to to the proposal there's ideas like using blood prefix and stuff and I highly recommend you to take a look at that and then there's like from Barnaby I think he came like um last week which I haven't had time to read it it basically enforces that proposal commitments on chain and then I think that's very neat I definitely want to take a look at that very soon so what are the take care of ways right I think they take away with this just like when it's important to figure out like who can censor and who can feel the censorship because there are so many actors in the future there's proposals there's there is some Meb boosts and then there's also the relayer right it's important to figure out who does what and then I do think like we need to leverage the Builder API a lot more because the Builder API is probably the best thing we have today it's the best defense we use Builder API a lot more we can like provide more waste to like to basically organize defenses against censorship such as like inclusion this block prefix and then there is like a spectrum of solutions and then out there which we're thinking but the simplest solution typically has like more trusty assumption so I'm not sure if that's the way that we want to go so some final thoughts I think like for me I think censorship resistance should be the highest priority aside from scaling and withdrawal like what is ethereum if 50 of the transactions are censored right that's something that we have to ask ourselves about right and I do think the hybrid PBS is basically our best two bars to defend against that because it allows fast iterations that's and then before we enchant into full protocol PBS then we kind of lose this that because everything will be hard for based right for high BPS you have to build an API then you can play around with that and then you can we can figure out like what works best and what doesn't work best and then I know like people that have been working on high bpps such as Med boost relays Builders they have been getting a lot of live bashing and stuff I I don't think it should be that way I think we should be like working together as one team and to like to basically Advance this um Advanced this um censorship thing forward so yeah definitely um shout out to all the teams that have been working on that I mean they are the real heroes so yeah that's all I have today thank you so much for having me and yes thank you clearance we have some questions for you after your very interesting presentation hypothesis hi uh so regarding latency the numbers you showed would more or less the same as Dennis on the first day so if you have 50 of the network proposing through relayers and 50 of the orphan blocks are coming from relayers I would have taken that as a sign that latency did not matter that would be the expected yes but I do think that we can do better from the orphan point of view right because now if you look at like 20 plus years often a week if like if let's say just have 10 plus they get orphan then it's probably better than 20 plus year orphan right it just I'm coming from more like the orphans perspective because when something orphaned then it's obviously not ideal because that transactions could be included and there may be some U.S concerns there right hey Terence thanks for a super informative talk um can you say a little bit more about the three relay faults that happened recently like how were the malformed headers or or payloads generated and like how was that mitigated how do we prevent that in the future right I would say like the relay landscape there's still a lot of work to do there because I need more testing and these bad tests then more end-to-end tests and in terms of the fault right the first fault is Flash ball and then they fail to Marshall the deposit and that point is too late because someone already submitted the header and stuff there you have a signature so there's not much you can do there and then but then they filter on more shows because they did not test um the payload with the full signature embedded and the second fall and the third floor are basically this is the second incident and the third incident are basically the same and I think the blood relay just so the blocks Rock relate did not validate the payload which is what they're supposed to do they're supposed to make sure the payload is welded before they pass it to the proposal but they just did not validate yeah hi turns thanks for the talk uh I was curious about the out of protocol CR lists and the proofs of transaction inclusion how have you done any Research into it is it practical like would the builders or the relayers be actually able to calculate the proofs in time yeah there's something still under research I think like Chris from flashbots but I just open the pr so I need to look at it but high level I think like how it should work is just that say you're proposing a slot n at slot A minus one you have access to the mempool and then you see some different sessions they get filtered you basically I started minus one you send those you basically present those transactions to the relayer and the relay will also present those transitions to the Builder and then the Builder will essentially build a block just include those transactions real quick and then send it back to the relayer and then like so at the end after the signature is done when you get a payload right there so they they can include like they can include like the multi-proof to basically prove that the signatures sorry to make sure that the transitions are indeed like are are basically indeed included in the payload yeah I I don't know yet definitely still under research yeah hi uh Alex made coinmetrics thank you so much for your talk really loved it just wondering if you would comment on your opinion about maybe non-public relays do they exist is this a possibility just wondering as for now I don't know any non-public relay I only know the seven relay that was pre-standing MVB boost.org and I also think that if well if there is non-public relay It's probably hard for us to know just because if coinbase is using a relay there is directing Mev they're not open to public I mean that's not something that we can easily find out um why do we think that proofs help in that like what he was talking about before um Provo inclusion of transactions why can't you just if the things that are not included once you see the actual payload disconnect from that relay because I mean the relay even if there is a proof could always just not release the block and or the block could be invalid or there could be all kinds of faults that make the proof kind of meaningless and what you end up doing is anyway disconnected from the relay in either case so just yeah can we just make it simpler on ourselves and just you know you say please include these transactions if they don't want to disconnect from them and next time that won't happen no yeah I think yeah I mean I fully agree I I think that's possible and I think there may be a better solution yeah I think yeah hey yeah um I apologies I've already missed this before but you said that uh 50 of the network is making use of Mev boost do you have um more like fine-grained uh numbers in terms of like whether or not uh like what the numbers of institutional stickers versus like homesteakers um I just know like an institution that I work at they're security considerations Mev boost is sort of like a centralized yeah yeah actor at the moment yeah there is a site for that I'm happy to share with you after but yeah there's a side that actually I typically track like example Lido coinbase Finance like the top three like like just like what relatives are they using and stuff like that and what percentage for each layer yeah we there like there is active tracking on that but I don't have the data in this slide sorry yeah but they're yeah but there is a site for that just one in the front the last one that we are out of time please thank you Terence um given all of this uh a validator that's deciding whether they should run Mev boost or use Mev or not what would be your advice to them I will say this is a hard question for me personally and then me and my team have different debates I think for now it's too early to be using this type of Technology unless you know what you're doing just because we don't have any like public infra out there for I would give you an example just like for example for the second incident right when the block relay has support it took them six hours to turn off the relay which they could turn off right away but as the public validators like we don't know right I don't have the access information be like hey there's something wrong I need to I I basically need to shut up MVP boost people the best thing can do people can go to Twitter but I'm not sure that's like the best like media for these type of things right so I would say wait yeah that's that's my advice okay okay the last one last one sorry um so you mentioned at some point that if you can't reach the relayer your execution client you'll the sorry the consensus client will reach out to your execution client and build your own block it'll default to that so that makes sense if you can't get the header right but you mentioned there are two situations one where you sign the header and return it and then you can't get you know a receipt of that is that not a slashing risk no you cannot get the right so once you sign the header you you basically pass the header to the relayer you you don't want to like you don't want to use your local block anymore because now it's a slashing risk because now you're assigning to blogs so so you don't you don't default to building your own Block in that situation no okay I misunderstood yeah thank you very much there is amazing talk and thank you all of you for the so interesting questions we are ready for our last talk of uh this morning and the next talk it's about reducing bacon chain bandwidth for institutional and home stakers and please and Applause for Adrian Manning from Australia and Diva that I'm surprised to say that is Colombian [Applause] cheers thanks I'll just give a quick intro yeah so my name's Adrian this I'm joined with Diva we're both from Sigma Prime we work on the lighthouse client uh so we are we're going to be talking about reducing Beacon chain bandwidth for institutional and home stakers and I'm going to leave it up to Diva who's a Bogota local and our peer-to-peer Network expert to kick us off hello is this on yes hello everyone uh first sorry if I'm very nervous this is my first talk um so yeah [Applause] let's get started okay so first thing that we need to check in order to analyze how to reduce bandwidth for institutional and homesteakers is how does the what does the bandwidth look like right now so we are going to be looking at data from Lighthouse like ash said we work on Lighthouse is one of the production Beacon notes there are many others techu prism Nimbus low star we are the second most popular one and while the data we're going to be looking at is Lighthouse um we are certain that this applies to any other client across the network so have that in mind first thing to notice is that bandwidth Associated to a bigger node is proportional to the number of validators is how it has attached um you can see that it increases when the number of validators goes up but then 64 validators it stays more or less the same 64 is going to be a really important number in this talk H is going to explain uh further why and why it's important so yeah we need to remember simply that we are going to stop at 64 and this is where bandwidth is going to be more or less stable now if we look at how many validators does every bigger node on the network has we're going to have like main three groups we're going to have those that don't have any validator attached only simple Beacon notes uh notice taking big notes now we're going to have those that we are going to come homesteakers which have less than 10 validators in this graph you're going to see a very big bar that's the one with two evaluators I hope you can see it those are a very big chunk of the network almost 60 percent of the network then we have uh those that we call homesteakers most of them have just one validator and then we have those that we call institutional stakers those are the ones that have more than 64 validators which is uh the big war at the end now why do we care about bandwidth to begin with so there are a couple of downsides of high bandwidth the first one is of course an increased cost uh if this course comes from running for example in the cloud where your chart charged for uh data data transfer and it's also a matter of diversity because this means that you're only able to run a full node in devices that support processing all the data that you're receiving from the network and for people that are able to pay for the services in which these notes have been running um I see a few devs here more my colleges for example um we know that there is going to be an upgrade to the network that is going to bring a lot of a big big increase in bandwidth like a lot so we need to start tackling this problem right now before this happens that's another big reason why this is important um also another really really important reason is when we say for example A diversity problem the truth is if you are for example a homesteaker and you're running your beacon note with your single body data attached and then your brother or your sister starts playing video games then you're going to have a bandwidth Spike and you might lose at the stations you might miss other stations blocks so basically you're losing money simply because very very short spikes in bandwidth so yeah we don't want that now let's check where is the bandwidth coming from for the sake of this talk we are going to separate protocols between discovery and non-discovery so why is that because Discovery has its own transport its own encryption it's over UDP whereas all the leave P2P we're going to call them lip P2P protocols uh they're over TCP we have gossip sub and the request response the one we are going to be focusing mainly in the stock is Gossip so so now that we have this separation between let's say Discovery and not Discovery let's check where is the bandwidth coming from this is what one with uh total bandwidth looks like for one of our neurals for the last couple of days so is it Discovery is it live PDP where is it so now we have two graphs the purple one is going to be Discovery as all the discovery bandwidth and the other one in the bottom is wptp bandwidth now forget about the scales this is just like a non node very Sciences analysis of the shape of the shape of the bandwidth so I guess we can more or less agree that bandwidth has the shape of the leaf P2P right whereas Discovery is well it's very magical as we will see so the total bandwidth is around 250 kilobytes per second is it better like this yeah okay about 250 kilobytes per sec per second then we have lip PDP which is around 200 kilobytes and then discovery which is between 40 and 60 kilobytes per second so for the sake of doing analysis over pan with Discovery is really not important so we're going to focus only with P2P yeah so I was saying we have gossips up and request response we're going to focus on Gossip stuff so in Gossip so gossip soap is a published subscribe system protocol we publish messages to the network those get disseminated to other peers and we receive messages to subnets or two topics to which we are subscribed we have large amounts of data and those will split between subnets again here we have 64 attestation subnets which again H is going to be talking further in a moment and then we need to think how many subnets are we subscribed to since this is since this is the reason why we get so many so much bandwidth and the reason is that right now HB can only subscribing to one subnet for each validator that it has attached and yeah well subscribing to a subnet means a lot more of bandwidth so a very simple example of how gossip stuff works gossip stuff has a lot of parameters but one that we care a lot is the match degree so this example is the three three match degree so we are going to start with our guy in the middle that one I hope you are seeing the red circle just right and reel so this guy has a validator attached it's going to publish a block so it's going to beg to pick three Piers it's going to disseminate the message and then each period is going to do the same with some other three peers and so aren't and so on so if you think about this for a moment you're going to realize that is very likely that I know this is going to receive one single one unique message a lot of times from very different uh notes across very different paths in the network from the source to themselves Okay so why do we like gossip stuff or why do we like the way we do this uh this message propagation so it makes for a very robust Network in the sense that we are sure that if we publish a message and we have this kind of topology the message is going to reach all other nodes in the network in a timely manner we also know that we have low message latency does mean that means the time that hap the is spent between the creation of the message in the source node to when you receive it is not very long and this is as I was saying because of lower paths smaller paths lower hoops but then again we have a lot of duplicates and these redundancy results in higher boundaries this is a graph that we have for nodes so this is like real data for the gossip sub duplicates now this is going to be weird but I'm going to ask you to focus on this number the beacon block so we know we need to publish one block each slot so we know for this topic we are supposed to receive a single unique message however we are getting six or seven messages across the network that means an application an amplification factor of about seven so imagine what happens if we were able to reduce that kind of duplicates that was like a huge huge gain there okay so a summary of the current state so bandwidth is proportional to the number of validators I know it has H is going to speak about how we intend to tackle this and the other one is the high amplification in Gossip so how are we going to do to reduce duplicates without harming the network so I'm going to leave Ash to continue this talk foreign yeah thanks Diva so I'm gonna try and have a crack at explaining how we can potentially reduce some of these so I'm Australian and I can see a few Australians in the audience I think they'll agree with me that we have terrible internet like ridiculous internet I run uh a validator at home and someone that watches Netflix in another room causes me to lose attestations so I as a disclaimer I have a personal Vendetta to try and actually reduce this bandwidth so that I stopped losing out of stations yeah so there's probably two solutions we have to these kinds of problems which I just want to kind of briefly touch on and give you a feel for what we're trying to work on one of them is called minimizing topic subscriptions so let me try and explain that if you haven't already covered some of this kind of stuff before so a validator in an Epoch needs to do some kind needs to publish the messages on these things called subnets in order to do that you need to have peers to be able to publish those messages on those subnets now the problem with this is that peer Discovery is kind of is a slow process it takes a while to actually find some peers and it's even harder to find a peer on a specific subnet that you need to publish an attestation so the problem we've got essentially is that we need a stable set of subnet a stable set of pairs that exist on each subnet that we can find them easily when we need to publish a message so it's not really an uh an easy problem to solve the the way that we currently have this solution I guess is that every time a validator is attached or sends a message to your beacon note that Beaker node is required to subscribe to one subnet for a very long period of time it's about 27 hours or 256 epochs so what that means is the more validate is you have attached to your Beaker node the more subnets you to subscribe to and subscribing to a subnet means that you have to get all of the messages on that subnet you need to verify those messages and then you need to send them on to other peers so you're doing this there's a lot of bandwidth and a lot of processing so essentially you're supporting the network at the cost of bandwidth on your node so you're kind of being like a good actor so there's a number of downsides to this approach one is that it's not enforceable so what that means is you you as a beacon node you can essentially lie to us and you say I don't have any validators even though you've got like 2 000 attached to you and you just don't subscribe to any Long Live subnet so you don't consume any bandwidth so you kind of incentivize to do this and there's no way that any other node on the network can tell whether you're lying to us or not the next thing is potentially our subnets are over subscribed we actually have quite an a large number of Beaker nodes on mainnet today and when we originally designed this kind of process we we didn't really realize how many nodes would be participating in the network so potentially we have more nodes than we need on each of these subnets so that would lead us to think that we have excess bandwidth on the network that potentially we can remove so the idea that's being proposed is why don't just every single Beaker node on the network subscribe to one or many subnets the there's some benefits to this but the general discussion is in is in this um specs repo as an issue so what would the bandwidth graph look like if we did this if one Beaker node was subscribed to one subnet rather than having it being proportional to the number of validators attached to you you would then get this green line on this graph which hopefully everyone can see and that green line sits at around 500 kilobytes a second at the moment from what we've what we've measured so essentially everybody on the network that has a validator wins in this in this snow except for those that have one so sorry for the people that only have one validator in this room you guys then have to your bandwidth will increase by about 500 to 100 kilobytes the other benefit that we get from this so from a quick scan that we did of the of the DHT or or the current nodes on the network we found that 57 percent of nodes apparently don't have any validators attached to them so they're either lying to us or they actually don't have validators attached to them and they're not they're not participating in any of the subnets so if we do Transition over into this state they actually have to start participating in helping out and it lowers the bandwidth for the institutional stakers or the ones that have high number of validators um yeah so essentially all the beaker nodes will now contribute to to doing um to to helping with this subnet stability so it kind of sounds pretty good that we get a massive reduction in bandwidth but there is a cost and the cost is how many what is the density of of nodes that you can find that exist on these subnets so if no one was subscribe to there we wouldn't be able to find those peers in order to publish publish our messages on these subnets so we still need a decent density so that if you just randomly look through the PSA you can actually find nodes on those subnets and kind of hold on to them and so this graph here is a distribution of the current density and what the density would look like if we switch to having uh one one Beaker node per one topic so at the moment it's it looks roughly like eight percent so you randomly pick a node there's an eight percent chance that it's going to exist on any given subnet if we switch to one Beacon node per one subnet it drops to about one and a half percent which is one on 64 as you would expect um the benefit of that is that it's configurable we don't have to say one bigger node one subnet we can say one bigger no two subnets one bigger no three so now so we can adjust that that kind of differential and whether that's feasible is dependent on the number of nodes on the network so that's something we need to measure but the benefits that we get from this is essentially for the institutional stakers which represent roughly like 10 of all the validating nodes on the network their bandwidth will drop by about over 90 which is kind of Handy and as I mentioned before everyone that has more than one validator will have a reduced bandwidth so fundamentally we're talking about do we need all of these nodes on the network to support these subnets at the cost of the huge amount of bandwidth they're currently using and so potentially not and it's customizable the second thing is it's also enforceable because we would tag a beaker node's node ID to a subnet and so when we connect to a bigger node and it's not subscribed to that subnet that we know that it's being naughty or lying to us and we can kick it off so we actually have the enforceability property yeah so that's one solution and that's kind of a low hanging fruit it's kind of easy to do we get substantial gains the other thing that Diva was talking about is message amplification in gossips app so if there's anything we can do about that the idea that we want to try and push forward is a concept called appisub so gossip sub is a protocol that exists in lib P2P as Diva was mentioning and the lip pdb guys which is kind of run by protocol Labs have had a Evolution stage of Gossip sub they've talked about Epi sub for quite a period of time they've done a bunch of research in particular viso or Dimitri who works for protocol Labs has has had this kind of vision for episode for a while but has never had the push or the drive to do it but now that we're seeing quite substantial amounts of bandwidth it's it's probably time we try and realize this thing so I just want to briefly go over the concept and and what it's what it's going to do and how it could help us with bandwidth okay so app is up you one the biggest problem that we have is that if you have a high mesh degree as Diva was saying that that means you have a high connectivity of nodes in your network so every node could be connected to another 10 or eight other nodes and then you get huge message amplification so every time you send one message most of the network will probably have to download it eight times so if you increase that message by one megabyte you're actually increasing it by eight times eight megabytes because that's the amount of boundaries that has to be downloaded and then propagated so naively you would first think okay why don't we just lower the mesh degree I did try to do that at one point but as everyone points out that's not very it's not a safe thing to do we don't really know how the network is going to behave we can try it on test Nets but the topology or the structure of a test net doesn't look like mainnet if we just lower the mesh degree to two for for example a mainnet you might just not restart receiving blocks so we have to do that with either great care or with a lot of testing so it's it's an interesting idea but we could probably do better so another idea is to dynamically adjust adjust the connectivity we also run into a kind of a similar problem and typically a lot of uh the people that use our client they they suggest lowering the peer count which is um to Falls it doesn't quite help you so that's also not the best solution so the idea with Epi sub is to try and minimize duplication and latency by making the mesh it keeps the same connectivity but making it a little bit more efficient and by efficient I mean we're trying to remove the number of of duplicates and at the same time either maintain or lower the latency inside the mesh and the mesh is just a subset of your peers that you're connected to that's where you receive your messages from so that kind of sounds like we're trying to win on two fronts we get less bandwidth and higher latency which kind of doesn't sound like we should be able to do it but let me explain the the general principle and and maybe it makes sense so the general principle is you're a node on the network you're receiving all of these messages a lot of them are duplicates you want to just start collecting Statistics over which nodes are you sending you these duplicates and which ones are sending you late duplicates and you'll end up having some distribution of the the number of duplicates you get and the latency so it could be that Paul over here in the front constantly sends me some late duplicate three seconds late whereas Sean's sending it to me straight away instantly so I come up with essentially a choking strategy is what we call it where we look at all of the distribution of people sending us duplicates and the ones that are late or the ones that have lower latency and we send a message called a choke so in this example we say I'm going to choke Paul because he is sending me late messages all the time what the choke message does is it indicates to Paul to no longer send me these messages over the mesh instead he does a process called gossip where he just sends me like the message ID which is a much smaller thing so over time if eventually based on your choking strategy you should have a more efficient Mass where you're only receiving the messages with a lower number of duplicates maybe from just one or two peers where the rest of them are choked and you still receive gossip from them so if the two peers that you have in your mesh are slow and the other ones that you have choked are started sending you message IDs before you send them in the mesh we have an unchoked message so we can unchoke them and put them back into the mesh so ultimately the idea would be is that you're you're dynamically changing your mesh and how many what peers what peers are sending you messages and how fast they're doing it in order to try and make it essentially more efficient hopefully that makes sense I take questions afterwards so does this work I guess is the question there's been some preliminary simulations on this uh as I mentioned vizo from lip P2P in protocol Labs has done a work done some work on this he's built a generic simulation uh from for the Go version of this with 250 500 and a thousand nodes and so this is this is with a mesh degree of six so there's roughly about a six times amplification if you look at the messages in these simulations but pretty much in in all of the simulations what this graph is showing you is that you get roughly a 50 reduction in the duplicates from episub now as I said there's a choking and an unchoking strategy which are left somewhat generic and I think we can tune this to be significant especially if we target it for the ethereum network I think we can get better versions of this but at face value it looks like you can reduce the gyps by 50 just by adding these kinds of messages the next thing we probably should talk about is latency uh I kind of suggested that we could get a reduction in duplicates and reduction in latency uh the initial results that uh visor has uh completed in a simulation seems that the latency increases so on the left is uh Gossip sub latency distribution where the buckets are like uh milliseconds since you received the message and on the right is uh episode so you we receive more messages with higher latency in these simulations but as I was saying uh these are somewhat generic the the topology of these does not look like the ethereum network that we have and these can be highly tuned to what we need there's a lot of like research work going in there there's a few people in the audience where I think that I've promised results for a mainnet version of this so uh asset Sigma Prime we've built uh essentially a production version of episode inside gossip sub the advantages are that it's backwards compatible so we can release it in in our clients it'll work existingly with every other peer and every other gossip sub um node on the network but if there happens to be another episode note on the network we can start getting this bandwidth minimization so the fact that it's backwards compatible is super handy we can kind of just release it whenever we want we're also working on a mainnet simulation so that we can actually simulate the bandwidth that exists on mainnet and then apply Fe sub and then get essentially more specific data that's more robust and how we can actually apply this to uh what it would look like on an ethereum two mainnet these will publish results from this very soon and if you're interested just let us know so the title of the talk was reducing if I can remember the title of the talk was reducing bandwidth for uh institutional and home stakers so if you're an Institutional or a homesteaker and you came here being like how do I get all these bandwidth gains what do you need to do in order to get a 90 reduction the answer is nothing you just have to wait we'll release it maybe run a lighthouse node and hopefully yeah we'll we'll be able to reduce the bandwidth but that's the end of our talk thank you thank you very much Adriana and Diva we have uh some minutes for questions if you want to ask some we have from um great talk guys is this something that's backwards compatible meaning that clients can gradually roll this out as time goes or is it something that we all have to upgrade at once yeah yeah so here we're handling basically protocol versions so we can run against nodes that are episodes compatible and if they're not then we're just going to run episode version 1.1 so yeah that's what we mean with that yeah we add a protocol ID into the gossip sub and so when you connect to a pair you can identify which protocols I support if they don't support episode you don't choke or uncheck them if they do support it you can choke and uncheck so it works perfectly with 1.1 notes um yeah I was trying to ask why do we want to reduce bandwidth for institutional stakers like isn't it kind of a nice property that there is no economies of scale at least if the um if it's enforceable that they have to subscribe to things um it just seems like I don't know it's weird to me as a concept to shift uh the load from institutional stakers to like normal Beacon nodes or like um I don't know homesteakers even if it's like a little bit but just it generally seems like we want those the lack of economies of scale good good part of this is the part that edge mentioned about being enforceable so having 60 almost 60 percent of the network being knows that don't have any validators we're not so sure that's true those might as well be institutional stickers so truth is that this is more like about being fair across the network so it's more about that than targeting something uh that is going to be better for institutional stakers it's just that they happen to get more gains because of this so I know uh with four eight four four we're going to be exploding our bandwidth costs I was curious if you guys knew if that scales with the number of stakers you have or number of validators you have uh so I just the question is that we're going to increase the the block size in 4i44 and how that applies with here like if that scales the way that under current gossip or bandwidth um increases with the number of validators you have I was curious if that was the case for the increase in 4844 as well yeah it's not so the the bandwidth increase is due to the subnet subscription um when you increase the block size everybody feels that because everyone's subscribed to the block topic so that's that's felt uniformly here the part that matters for what Mark asked is mainly the amplification factor and not that much um how it behaves regarding the number of validators so since we have an amplification factor of about six so now with this uh this Improvement we're going to have huge block sizes which if we continue doing things the way we are are going to be sent across the network six times each time each block so that's that's insane but there is yeah so it's related more about duplicates that part than the number of validators for each node is that answering the question sure yeah just a quick one so you showed that bimodal distribution where there's a lot of one validator nodes and then there's that sixty percent that were just regular full nodes like not validators and then you know uh the institutional notes with like more than 64. so that um I understand how the Epi sub like helps everyone and it like helps the homesteaker it helps the Netflix problem for the institutional stakers their all running like data center nodes anyway so I I don't understand as much how the like subscribing to fewer uh gossip subtopics helps yeah so I think I think this is a very similar question uh to over here so I think fundamentally we were looking at as a whole the the network may be consuming more bandwidth than we need to be um you depending on how how you build your client You Can Be Clever about which peers that you connect to um when you have so at the moment you have these institutional notes all right they've got they're not just institutional uh some people there's a usually paramet I think in most clients which is called subscribe or subnets so you can even if you have one validity subscribe to all the subnets and the reason you do that is because you get some benefit from seeing all the attestations you get a slight increase in performance so it's not necessarily just institutional people but the the the institutional I forgot what for my train of thought um the I forgot what I was saying and I did have a point maybe part of what age was saying is that well subscribing to a subnet we also advertise that we are inside the subnet inside Discovery so that means that we need to find peers using discovery that are useful for us so one strategy that people who have the bandwidth for that use to be sure that they have bullishing all in time and everything is timely is subscribing to all the softness regardless of how many validators they have this is what age was saying about maybe it's not exactly true that all nodes that have more than 64 subscribed softness actually have more than 64 validators attached to them yeah sorry I remember my point my point was that we have these institutional stakers or people that are subscribed to all the subnets and they become more valuable than every other node on the network if you're from a client perspective if you have peers that are connecting to you and one is subscribe to just one subnet and you have another pair that subscribed to all 64 you're more inclined to be like I want to stick keep a peer connection to that guy because in case I need to I need to send a message on one of the subnets he's he's uh he or she is connected to that subnet so you're left with like uh 10 of these nodes a super valuable nodes across the network whereas everyone else you kind of just throw them away they're not they're not all that valuable whereas if we transition over to this thing one it's enforceable which is something that we want two we're not entirely sure whether the amount of banter for using across the entire network is necessary so we reduce that as a I guess maybe a side effect and the the third example is that all be all the nodes are equally kind of valuable to you and because we tie it to the node ID it makes the node ID specifies which subnet they're supposed to be subscribed to so when we're doing Discovery queries we can actually search uh more efficiently for nodes on a specific subnet because it's tied to that node ID so there's a number of benefits it's not just uh we're helping institutional stakers that's just kind of like a byproduct the last question please oh yeah hey uh Cole could talk to you guys thanks um so you mentioned uh reducing bandwidth um and uh reducing latency to like avoid missing expectations something that we've looked at as well is like affecting their effectiveness ratings um with validators um can you guys speak which can result in like penalties or loss reduction and rewards um can you guys speak to how this would help with effective Effectiveness ratings um if it would um which I yeah yeah so I guess the first one is kind of what I introde with my my personal uh attestation Effectiveness drops when someone watches Netflix so uh I imagine for a lot more homesteakers that are kind of on the bandwidth limit or their upload speed is quite low like in like in Australia and we reduce the bandwidth requirements a lot uh Diva said that when you have these peaks in bandwidth you you can miss out our stations you don't you don't publish them in time and it's not just missed but you also get penalized if they're late uh so even if you even if they still get included in a block but later you get you get less so that that lowers the attestation Effectiveness um from a lot of the people that use lighthouse we find that uh that there's a number of main reasons that that impact the attestation efficiency and that's one is bandwidth and the other one is CPU limitation flight processing so if you're running a a node that's overburdened and and this the topic subscription is another thing that can overburden a note because if you're subscribed to a lot of notes let's say you have five validators attached to your you have five Longleaf subnets you have to get all the messages and you have to process them so the processing also kills you and lowers your average Effectiveness so if we get that in there that'll should in principle improve the effectiveness because of the bandwidth and and because it lowers the CPU usage of your node I have to make this quick um so would it be possible for a node to detect that it's been choked if so um could it somehow combine that uh with being dishonest about its subnet count and other grief or otherwise stall its local node graph sorry I missed the last part could it if it could combine that with uh being dishonest about its subnet account connection could it just stall or otherwise grief its local node graph ah so the question is um can I know to lie about it subnet subscription essentially right uh or fake it sorry oh wow is explicit so I know this is actually going to no it's going to be showed we're asking him to stop sending messages to us it's not yeah I know it's I know it sounds similar to what is happening for example in in strategies for example in I don't know follow sharing but it's different in the sense that we are the ones asking the peer to just stop sending the messages so it's kind of a benefit for them so when uh when we connect to appear let's say let's say in this new regime where we have every node has to subscribe to a topic based on its node ID we can look at its its peer ID but let's say it's node ID we know that it's supposed to be subscribed to let's say topic three um the way that we when we connect via gossip sub uh they should send us subscriptions about what it's subscribed to and and if it's not if it's not subscribed to that we know that it's being malicious or faulty straight away and we can just disconnect it uh there's technicalities where I can say that it's subscribed but then won't let us uh so the next the next phase of the thing is we try and create a mesh mesh Network so if we if we're connected to let's say 100 pairs we only really form a mesh with with the mesh degree with like three of them so in principle they could always just say no no you can't go on my mesh you can't call my match I'm full I'm full I'm full so that's one way to kind of grieve Us in that scenario they still have to forward us gossip sub messages there's a there's a small mechanism inside gossip sub that is called gossip subscribing it's introduced in 1.1 which somehow made Gates uh attempts to mitigate censoring so where where you just say that you're subscribe you do nothing and and the whole network essentially tries to kick you out of the mesh in terms of while being choked we only choke people that are in our mesh so only people that are subscribed and then once they've grafted with us so we form this mesh connection which means they have to be subscribed they have to be sending us messages if they're not sending us messages 1.1 scoring we'll kick them out and then we then we choke around choke them in order for them to be malicious and try and cheat that system they're choking abstract the choking strategy is is abstract and can be implemented independently on each node so you don't know on each of your peers what what is specifically their choking strategy they could just pick random nodes on their mash and choke people so I think there is Avenue of security to to look in there that hasn't been done yet but I imagine we can probably solve that with some of the scoring parameters in gossips up thank you very much Diva and thank you very much Adrian thanks thanks guys amazing talk if you want to ask more questions they can contact you outside the room right hang around as well yeah uh this is Vietnam my pleasure this is the end of the talks of the morning we're going back to to this stage at 1pm uh so let's have lunch guys enjoy your your food see you at 1pm thank you [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign foreign [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] [Music] foreign foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign foreign [Music] [Music] [Music] foreign [Music] foreign [Music] foreign foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign foreign [Music] foreign [Music] [Music] thank you [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] foreign foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] foreign foreign [Music] [Music] [Music] [Music] foreign [Music] I didn't hear anything how are you that's right that's right okay I I am crypto realmd I'm a medical director actually I'm working with bankless now in Spanish also I work for uh espacio crypto podcast and I'm actually working with the New Mexico in the core development of the the community I'm very excited about the three themes the three talks that we are going to have and it's a pleasure to me to present a caiman that is going to talk about Lobster metric driven development he is a ethereum core Dev he's working actually in something that is very passionate for me because I think that ethereum needs uh in the in the role of this uh censorship resistance blockchain to be like a lot of notes a lot of validation validators so they they are creating like this JavaScript drive on full project so sorry I'm very excited just give him a worm our recentment [Applause] hello hello okay so today I'm going to be talking about lodestar and metrics and really just our story and how we've how we've developed lodestar over the past three years so about us my name is Cayman I work at chainsafe it is a great company we're hiring uh we we basically build blockchains we have a whole bunch of layer one projects lodestar is one of them but if you are interested in getting into core development we're a really great place great culture so lodestar is it's one of these projects at chain safe we we're um we're basically an eth2 or ethereum consensus ecosystem written entirely in typescript um we really take the open source ethos to heart we have all of our meetings they're public everything we do is public and we really encourage anyone who wants to contribute we we try to help people out and get get people involved a lot of our team members actually started started their work on the project through their open source contributions and it was kind of like a open source to contract to hire kind of a situation so we really we really like that that kind of ethos but what we're talking about today is specifically our consensus client and how we got from like prototyping typical kind of code to something that's like production ready something that you can really rely on something that doesn't just blow up all the time so before I kind of step into our story I just want to kind of do a very high level of what metrics are uh this is I'm not going to go too deep if if you really want to go no more you can like read the docs about these things so um metrics in our case we're using Prometheus Prometheus basically gives you the the tools to um build time series data for specific elements that you want to track in your code the kind of three main things that you're tracking is either counter a gauge or a histogram uh we can go we'll go into kind of some examples of all of these things but basically what you want to know about metrics are you have some value something in your code that you're wanting to track and you want and Prometheus will kind of query the query your software over time and you can get a sense over time of of how things are changing how things are are moving um and then you use grafana to visualize these metrics so um a picture is like a thousand words so it's basically the difference between like looking at a bunch of logs and then looking at like pretty graphs that can show you give you a little bit more insight give you something at a higher level than just squinting so our story um basically we we started um with really bad uh with not very much infrastructure not very much a discipline and how we tackled um metrics are tackled like the data that we we track and eventually we kind of like over time and like through a lot of pain and and suffering we kind of realized that metrics are the way to go metrics give you a um insights that you'll you'll never get just by looking at logs so really metrics aggregate a bunch of information and give you um the tools you need to make better decisions about how you release your product uh what things to focus on like what are priorities um and they're also just really pretty so they're cool so this is a picture from the early days of lodestar from the interop walk-in in Muskoka in 2019 and here's our message yeah we're stuck syncing and some P2P issues uh we didn't really know what what was going on like why things are breaking we're looking at logs actually on the other side of the screen here you we're like you know we look happy but it's like uh oh things are breaking hahaha um here's some uh um some funny issues that we had um you can see priority low adding Prometheus monitoring we didn't even know we were doing uh um yeah out of memory om out of memory it was just like plaguing a lot of our development um throughout 2019 into 2020 uh lodestar was like very Proto very much like a prototype and we ran into the issues time and time again like we would have the same thing happen multiple times and we weren't like like regressions that were happening and we weren't really like learning our lesson um and kind of the turning point was you know starting to take some time to to Really uh to take the time to like build out these metrics and build out these dashboards so we had a contributor I think I can save a new world maybe with peace great um and so throughout 2021 we started going really hardcore into adding a bunch of um just add a bunch of metrics um and taking time to think through what questions that we want to answer what pieces of code are dark where is our you know where are we spending time in the in in our software where are we where is a bunch of memory being spent we really kind of uncovering all of the complexity of this running software I mean blockchains are really complicated software there's a whole bunch going on and um unless you're really tracking tracking key pieces you're you're kind of be going blind and now we're finally kind of at a point where um we for any new feature that we're adding we ask for metrics so it'd be very important to track the retry Behavior meaning add metrics for blank link so that brings me to metrics driven development um really the the kind of the key the key thing that we've learned is that every large feature should be documented with metrics so if you are adding in case of a retry mechanism you're adding some new feature you need to make sure that it's actually working and the only way that you can um or maybe not the only way but a really the really great way of doing that is being able to show that visually being able to show okay our cash is now bounded let's see let's see that in the metrics let's see how it's How that cache is growing over time or let's see how many times the retry mechanism is actually being activated um those are the sorts of things that you really only look confined by through through these graphical methods through these metrics um you're going to have a really hard time looking at the logs for that um the other great the other thing about metrics driven development is that any deployed software that you use you need to be monitoring it very carefully for regressions so look here this is the process Heap increasing over time this is a comment from Lion for the last two days the leak is clearly visible an impact of 75 megabytes a day yeah so that's going to be a problem if your blockchain is if it's just increasing at 75 megabytes a day continuously you're going to run out of memory and these are this we were able to catch this this issue before we actually before we actually cut a release so um it makes your release process a lot at least release process is a lot better here's another here's another example of a regression we deployed deployed a version of our software and we saw the cache size grow it's like uh oh is that is that is that a bug is that a is is it okay well we only actually caught it because we're tracking these things you probably aren't even going to have a log that's going to show you your cash sizes so unless you you actually measure this you're going to be blind the other great thing about the about dashboards and about metrics is that it lets you correlate different different pieces of the code to find out where the problem is so here we have different you can you can see a bunch of different graphs kind of all at the same time and see um okay something is affecting the event the vent Loop lag I can see at the same time the active number of handles is growing the number of requests is growing so you can kind of um you know kind of solve the mystery in a in a in a way that you're not going to be able to do in the logs another great strategy uh is um is using different versions if you're running software so running um like a staging version of your software against a production version or against several recent versions and being able to overlay data on top of on top of one another and being able to see okay well it looks like our beta version is is using a whole bunch more memory than our unstable version so you know being able to kind of compare and contrast um it gives you the tools to do this and so just just kind of like drilling down to a few a few few tips um these are very very practical kind of tips don't abuse the histograms so I said there's three different types of metrics um yeah just when in doubt you use the simplest tool possible histograms are really only used if you're wanting to see the kind of the distribution of of uh of something so like maybe like looking at like request timing maybe you'll want to see like all right certain requests are happening very very quickly but then others are happening kind of longer um so the classic mistake with histograms is um is uh using a label that's unbounded so if you had a metric that was tracking something per peer ID on the network well you might have thousands of peers with and if you each new each new peer is going to be using a new peer ID it's going to blow the metrics up and it's going to affect the performance of Prometheus and you're not going to be able to run the queries that you want to run those lips oh so how do you know which metrics to add so you know I showed some of these pretty graphs and everything um but like in your in your in your software how do you know what you want what do you want to track um so really what you want to do is um think about questions that you couldn't ask otherwise um so a really good example is the size of your internal caches another thing you can think about is like if this feature or if this part of the code would degrade or explode in the in the kind of bad case that that's a that's a prime candidate for for adding metrics around it um and then really just like keep asking questions and then it's like if you have the data to be able to answer the question you're good but otherwise keep adding metrics until you can until you can answer those questions so some examples that we we've kind of come across how often are our streams being reset how many peers do we have these are kinds of questions that that the metrics can help and so I'm going to show you just a live demo of or a live uh example of of our dashboards and we can kind of run through some things and if anyone has any questions about it while I'm going through it we can we can do that too okay so this is this is uh our dashboard for our Fleet of of uh Beacon nodes and validators um this is kind of the the high level dashboard right now we're looking at an unstable node we can go to like stable a stable version of lodestar so this is running version 1.1 or on the girly Network we're synced it looks like it got restarted about a day ago some interesting things I guess you know peer peer count is looking good all right you know there's nothing everything looks stable there we track different types of peers inbound outbound and uh for us we really care about the number of peers that we're going to be receiving messages from kind of continuously those are our gossip uh peers so the number of mesh piers on our core topics staying about six or seven which is good it looks like we're connected to a lot of Lighthouse nodes and a lot of prism nodes and there's another load star out there let's drill into something a little a little more detailed here so one thing one one area that we really struggled with and where we added a lot of metrics is um in our gossip sub-implementation we found through adding a few metrics that we weren't getting blocks on time and it turned out that we we didn't have enough peers who were sending us these um who are sending us blocks and then we started asking questions all right well why are they not sending us blocks it turned out it was because our peer scoring was too low or why why was the score too low uh well it was it turned out it was because there was a specific part of the scoring that was that was too low and so we kind of you know you start asking questions and you can build up um you built you start building the answers to that foreign okay so what you're seeing here is kind of some of what I was talking about um we have breaking out the score into different components um the different parameters that are going into your kind of aggregated score for peers um what we were seeing before is you know you'd see a graph where the values are like going down and down and down over time or if they're like very negative numbers another really useful metric that we were that we check a lot is um things related to the VM so memory CPU um and disk storage so looks like our memory is stable that's great um GC is only taking roughly seven and a half percent this is things are looking good yeah I think that's enough of a enough of a demo here so um at this point um open to questions and we're also hiring so if any of this looks cool or interesting and you know typescript Come On Come On In [Applause] questions hello and thank you for the talk um so I see that you have a ton of metrics and I believe that most of them are useful but um do you have some kind of an alerting system for you to be able to know when a metric has gone wrong because I think it's just unfeasible to go through each of them like daily or something like that you I think you need some automated way to to alert you that something is going wrong yeah absolutely so that was something I didn't touch on but um it definitely feeds in um so we we use uh pagerduty you can you can basically set up um thresholds for certain metrics that trigger um either slack notifications or Discord notifications or in the case of a critical kind of issue it'll call you or text you yeah we do have that set up I would definitely recommend that if you are in running something in production okay uh thank you um again there are lots of metrics do you did you guys um develop over time um a way to or you know kind of a mental model or framework to decide how to where to put them whenever you have new um you know graph for metric instead of or do you just like decide to put it somewhere and then go back like you know to make it easy to to read or to uh browse the dashboard and if that makes sense there's a question about organizing the yeah exactly yeah so um I think you probably have to grow it organically what we did is we we started with One dashboard That Grew and grew and grew and eventually it became kind of unsustainable to have everything just piled together so we we started breaking it apart um the other thing that the other kind of issue that we had initially is we would create a bunch of metrics but we wouldn't add them to the dashboard so there was no actual like visual cue for for a lot of it and so it kind of just went unnoticed and so one thing we would we would definitely recommend would be like if you add a metric also add it to the dashboard at the same time thank you uh the question about storage so again too much metrics parameters or what does it affect the storage um well so some of it that kind of depends on your architecture um of how you're deploying this it let's see so um what we do is we actually have one Prometheus and grafana server servicing all our entire fleet of of Beacon nodes so you know we have a bunch of different machines and then we have one one machine for these uh right to to uh things so it it doesn't cut into the to the uh storage we retain for a month uh quick question uh is are the metrics relevant for like other evm chains besides ethereum mainnet um or is it just like specific to ethereum mainnet so yeah a good question um so I'd say like maybe I don't know if it's a half and half but like half the metrics are like more like for like monitoring uh the chain something like a like a block Explorer kind of kind of thing um but I would say like a lot of the metrics are specific to the implementation so and those are the ones that are I think probably more helpful um just the internal caches and like timing timings of things that we're really really wanting to get right hi uh I wanted to know if you think these metrics are useful for other clients and not just the typescript one and if so would you maybe consider like you know putting them in the public domain so like anybody can see them and apply that to new clients they might be considering building right I know that um other clients do have metrics and I think like the ones that are more user focused uh those are the ones that look more like a block Explorer and maybe with some like validator like showing your balance ticking up over time uh and I think those are those are public and I and there is an effort to standardize metrics across the consensus clients so I think some of that's happening um but again to the to the previous question uh a lot of the metrics will always be implementation specific um in terms of in terms of roles and you know in team members who generally looks at which metrics and which graphs um just kind of to get an idea of uh you know which metrics and graphs are useful to which team members or depend on like the role or the the type of work that they do you're asking if if only certain members contribute to you know more like the uh looking at the the graphs and the browsing the the metrics themselves like you know for example there are lots of metrics so not everybody looks at everything so you know it's more um what you know what team members look at look at what kinds of graphs or right so so for any of these larger feature pull requests we have the the person who is authoring or kind of owning that that feature they're responsible for um making the case that that that it's correct and then it works as intended and so they're responsible for looking at the metrics and building building that case and then as far as like um our release process we have um a checklist of basic things that we look at um before we before we actually release we have a testing period so we cut a cut a Beta release we deploy it to our Fleet um and we watch it for a few days and then after a few days we um gather you know check do the checklist and any other kind of ad hoc things that were were noticing then yeah hello so following on the question of um making metrics between client standard did you take a look at open telemetry sorry I couldn't couldn't hear that can you say it again okay um as the about the question following up on making metric standards between multiple clients did you take a look at open telemetry do you know what's that no okay so in web tutors a standard of how to consume logs metrics and traces and it's done by the cloud native Foundation to make the same pipeline that you have here more standard in a way that's consumed by other tools as well did you take a look at that I recommend it if not right I I think that the I think that's the the standardization effort is to have to build out some kind of Baseline of functionality like you can have one one Prometheus and grafana running and then whichever client you want plugged in here you kind of have some guarantees like uh it's not breaking and like it's it's kind of working so yeah that's exactly what open Telemetry does so take a look at it wow why we're not clapping please bro I think I need like two brains one to understand everything that you explain and in order to be explode right actually as I said I'm a medical doctor and I think that all the protocol that we are being is like uh making all the code like being alive like the like setting a heart that needs to be like everything in telemedicine on telemetrics just to be reviewed and to be like understood I'm really happy I'm really happy to be here and to hear about all the depths all the the chords that are here and I I don't know if I I don't know if they're smart enough to understand everything that you already said but I think that we can make a lot of changes just to to make uh to like other people and other uh like not so smart people like me just to understand all these all these new stuff so as I said before I think that Lannister is one of the opportunities for all the newbies that we don't know like all the codes uh that that needs to be like to deploy another type of clients so maybe it's a very great opportunity so we are going to block everything for the other conferences so uh please be welcome aggressive kumarvis okay sorry a little problem here okay I'm very pleased to present kumarvis have you ever heard about betamask I think you but maybe as as I said before for the new is like a like I am uh matamus is like something that we like and remember every every time that we think about blockchain and about with web tree but I'm very pleased that kumabis is going to like uh to to make this uh uh like a very high presentation very highly representation about not only just to keep your kids in your like uh paper just to also get like a client or something like that just to be in like an evolution of the wallets so thank you thank you all right cool [Applause] okay hi everyone I'm kumarvis uh I founded metamask with Dan Finley and now I run the security research team at metamask uh today's talk is the attacker is inside JavaScript supply chain security and lava mode uh so yeah love mode this is what I've been spending the past couple years working on um first and foremost it is a set of security tools for any JavaScript application to mitigate software supply chain risks so what is the software supply chain it's everything that touches an application or plays a role in its development uh sonotype has this cool website it's a timeline of supply chain attacks you can scroll through it and see how many they are and a lot of them are from Python and rust but a good 50 are from npm they're from the JavaScript ecosystem so why is Javascript supply chain targeted so so frequently primarily because it's really popular this is from the stack Overflow user survey and uh for self-described professional developers uh javascript's way at the top and there's typescript in node.js not far behind so when we're talking about JavaScript supply chain we're primarily talking about npm dependencies so this is your apps dependencies your dependencies dependencies your build systems dependencies any other tools you might be using you might think that these are your dependencies just the dependencies listed and packaged Json in your app but that's just like an inner ring around your application in part of a constellation of your whole dependency graph so that's why there's so many node modules in the node modules directory okay so to understand what it looks like when it goes wrong let's look at the copay wallet hack copay wallet was a Bitcoin wallet by bitpay and this is my primary case study for Designing lava mode uh so at the time end of 2018 uh the copay wallet hack also known as the event stream incident made a lot of news uh an attacker got control of a dependency that was used in a Bitcoin wallet uh deployed a targeted attack against that wallet and stole private keys and digital assets from the users of that wallet and it was I think three versions of the wallet that had the attack in it were published so they didn't even know it was in there for a while okay so let's look at the the build pipeline to understand how it got in there now so this is a non-custodial wallet so the keys are in the user's application only they're not on a server or something like this so the attack happened here at runtime that's where the secrets were that's where the private keys were but the evil dependency wasn't even part of the front-end application it was part of their build system so it actually entered in here and then it went on the disk during build time and modified a dependency that ended up in the web app but it sorry it could have also happened here at the dependency install time npm has life cycle scripts so after you install a package it gets a chance to just run any command on your computer to you know finish up its install process or whatever so Jackson Palmer of Dogecoin said bitpay is essentially trusted all the Upstream developers to never inject malicious code into their wallet Upstream developers being the dependency developers in this case and uh yeah he's right and it sounds pretty scary when you say it in these terms and was bitpay being particularly unsafe well this is how we make all JavaScript applications today and most wallets not metamask so so when this attack happened there was all this discussion how do we fix this should we never use dependencies are we giving up on an open source and collaboration what about don't roll your own crypto like how does that fit in audit all your dependencies always yeah that's a great idea but how many dependencies do you have hundreds thousands tens of thousands and when you need to make a change are you going to be able to wait and re-audit all those things so is there nothing else what else do we have here so around this time I I met agorik and learned of their indo.gs hardened JavaScript tool set so these are tools for working with untrusted JavaScript and in the core of it they had this package called SAS and it gives you a couple things for dealing with untrusted JavaScript um so there's a couple things about JavaScript that make it easy to attack one is everything is mutable by default all the built-in functionality and everything so you can just go in and modify array prototype and map and and change the way JavaScript works this can also give you access to objects that are being used like on the complete other side of the application of where you got your attack in so you want to lock this down and conveniently cess provides a function called lockdown that does exactly this by calling lockdown you're doing what's shown here at the bottom of the screen basically calling object freeze on all the prototypes all the built-in parts of JavaScript okay that was simple enough all right there's this other problem called ambient Authority this means simply by being able to run some JavaScript you get access to all the powers of that platform network access or disk access so say I have some little package I'm an attacker I've gotten a hold of sub-package that's deployed in my victims code now this package doesn't do very much it's just formatting strings but because of ambient Authority I have access to fetched network access I have access to environment variables and I can send them home to my evil layer and maybe there's some API keys or something in there so just because I get to provide a little bit of code I get to have all powers of the platform and that's quite dangerous so cess provides this notion of a compartment it puts a little container around the JavaScript and limits access to what that code can can touch or modify so for example if I have some code that that needs to make Network requests and it needs to know the current location of the web app in the web browser or something I can give it just those things and not give it disk access or other powerful features of the web browser and then I can run my code in here and it will only get access to those things as well as the basic JavaScript functionality and you don't need to know the internals but I think it's really fascinating and I've been working with JavaScript for a long time before I saw how this works and I didn't think it was possible so let's take a look it relies on two weird parts of JavaScript that you may not be familiar with one is the with statement with the with statement first you have an object it has some properties on it and you can you have the with keyword and then you put the object as the target of the with keyword then you have this block inside the block all the properties on the object can be accessed as if they were variables in scope well there it is it's not particularly interesting but that's what the width statement does the next piece is the proxy if you're familiar with Getters and Setter of properties it's like that for the whole object and any operation on that object you get various handlers to intercept what those things do Now by combining these two things with the with statement and then a proxy in here we control all the scope access inside this block so you have your untrusted code here it's trying to like get network access with fetch it'll look it up here it'll go to the proxy it'll say do you have fetch and your proxy can throw a reference error return undefined whatever you want but it can block that look up another way to write this which is easier to audit it may be easier to think about is with two statements one here the inner one you put the untrusted code here it's trying to look up fetch it'll look up inside this object first and that's the object we gave to the compartment when we said you should have access to these things and these things only if it's in there it'll it'll just get it it'll get whatever value you gave it if it's not in there it'll go up to the scope Terminator and that thing just always says no you don't get it there's a couple more things you need to make it safe but you don't really need to think about the implementation you can just use this API okay so cess gives us lockdown and compartment and I compose these in lava mode lava mode wraps packages inside of compartments and only gives them what they need to run so that little string formatting package we had before it won't have access to network and it won't have access to disk and other powerful features how you use it sorry it's a little small it's kind of like this previously you might run node index for your little server you replace that with love mode index But first you put this right auto policy API and that will generate a policy file of what the packages are allowed to use and then you just run you just replace node with lava mode and run normally and it will at runtime enforce those packages only get what they need the policy looks something like this this is a policy for one package you'll have a bunch of them in your policy file this is all automatically generated and works 99 percent of the time without any additional changes uh here in the built-ins these are built-in packages provided from node we're giving them just exactly what they need for example we're giving it just read access from the file system and not write access or anything else also giving some Global variable access and they're only allowed to import these packages so that's just an example but in order to help you review your policy we have this visualization dashboard um the per so there's the list of your dependencies on the left and a graph of your dependencies on the right the purple node is our application here our example application and then all these uh this graph coming out of it are all the dependencies and the transitive dependencies and you can you can dig in and take a look at what these packages are and what their policy is and the point of this is to help you prioritize the auditing of your dependencies and to review your policy because remember with without lava mode this is what your app looks like every single package has is maximally dangerous whereas with lava mode we know that some of these are are safe all these green ones for example are not importing any globals or any built-ins they're only composing other packages uh this is a static version in case the internet didn't work okay so here's the build pipeline again install time build time and then run time on your users machine um so we have in lava mode tools for each of these steps for depth install we have lava mode allow scripts this me this will prevent all these dependencies from being able to run a random command on your computer when you install them and then you can opt into just the ones you want at build time uh you want to run your build process in lava mode node and you want to add to your build process a plug-in for your bundler so we have one for browserify we're working on one for webpack for react native and swc switch um so this is in production in metamask we're protecting tens of millions of users now so we know this can scale you can use it too but we do need your help it's open source it's free it's out there find it on GitHub try it out let us know if it works if you run into a book open an issue if we don't have a plug-in for your bundler let us know and maybe you can help us with that okay thank you very much this has been lava mode thank you does anyone has any questions I see your order a lot of questions they're gonna bring you a mic okay thanks uh so I actually worked at bitpay on a different team oh cool uh that's overlapping the time of the copay hack and uh yeah like you said it's it's not necessarily that that company had bad processes it's sort of the whole ecosystem doesn't really have a good solution um since that time not only at that company uh did no one really talk about lava I mean I brought it up because I was aware of it at the time um but externally any side projects I've been on I've really never had anyone propose using it so do you think there's something not at the technical level but like at the social level that we should be doing to make this a more common part of stacks either across the board or when we're working on applications that deal with private Keys specifically yeah great question so what you know what processes should we put in place of what tools are out there um I think a lot of people uh let's see there's a lot of projects that are working on preventative measures like Ci flows that warn you if there's known vulnerabilities or some code scanning to see if there's something funny looking like eval and those are great and fantastic and you can use them in conjunction here like one example is socket.dev that's a friend's company and they're doing great scanning work but it's all preventative at the beginning that's not the way the security of your operating system works right it's it's making sure that all your applications are are safe um so we I mean I was very surprised that that no one else was working on runtime protections um besides the algoric folks which we partner on on building lava mode um so yeah I think more people need to just know that there's a solution that exists and you can actually do something about this uh hi so I had a question um about about the auto policy generation can you explain the logic behind that a little bit yeah so it um we are using static analysis here for convenience and not for the security layer static analysis is can be faulty it'll it'll miss things and it's generally imperfect but here we use it to generate the policy so you don't have to write it all by hand and then we enforce it with the love mode kernel which is not relying on static analysis but yeah basically what it's doing is you give it your entry point it's walking through all the requires and imports and walking through your graph it's you know converts the code to an AST it looks and analyzes it for references sees when there's Global references sees when there's Imports and it'll actually follow the Imports to where they're used so it can give you just the minimal things that you need like uh only using the read power of the file system instead of the right power for example hello a little bit about like limitations of lava mode specifically like if uh even if you grant access to like a specific function or feature to a package like they can still I guess use that feature in a malicious way um yeah I was just wondering like what else we should watch out for even if we use love remote yeah absolutely um so it will look at your modules love mode we'll look at your modules and generate a policy and if it sees that you know network access is used there it'll put that in the policy but if it's something that's not supposed to have network access then it we rely on the user the security engineer to catch that now it's most likely you don't have a supply chain attack in your app right now but you might have one in the future so every time you change your dependencies you can just run that thing generate a new policy and you'll see the diff of your policy and that's a lot less to review um some other things there is a slight performance impact for compute but if you're relying on network or disk which is usually what you're bound when you're Computing then it doesn't make a difference for you I think we have a lot of time for more questions it's me it's me come on it's Kumaris we need to ask everything hello um it is very good how you solve the the problem of executing its insecure code with the width and the proxy okay sorry um it is very cool how you solve the problem with um running in secure code with the width and the proxy and I was wondering if there is any difference with doing it with a web worker with a what a web worker a web worker yeah you can use uh iframes and web workers and you can use in on node.js you can use the VM package but what these uh what those create are what we call Realms so you actually have a slide for it um it's it does work but it's not very good for backwards compatibility um so yes when you use oh I'm not up on the screen how do I get up there um so in uh we we have a lot of jargon when we're talking about this JavaScript language security stuff but we use this term realm to refer to like a window frame and so an iframe is another realm and they're different in a couple of subtle ways one is inside of the VM or the iframe or the webworker capital A array is not the same as the capital array on the outside we call this problem the identity discontinuity problem and it breaks things like instance of so an array from the inside is not an instance of array on the outside and so this is fine you can deal with this problem but it is really bad for backwards compatibility and that was a primary requirement for lava mode because this was not built into metabouts from the beginning I built it afterwards and rewriting the app from scratch was not an option uh hello congratulations for your product um currently there is a proposal to in ecmat tc32 39 sorry to introduce compartments natively in JavaScript I would like to know if you guys are behind that proposal and what do you think about the current state yeah great um so yeah we're trying to take some of the things that we built here like the the car compartment API and actually get it standardized into the JavaScript language at the tc39 JavaScript standards body um yeah I I'm not up to date on what the status is um but it seems to be finding its way in via the loader API so that when you load modules you load them in a special compartment all right thanks for your talk I I would like to ask if you can share um some supply chain attacks that might have actually been detected had made Ms since you start using it ah great question we have not detected any supply chain attacks the the day-to-day battles at metamask are primarily phishing but the problem with the supply chain attack is even if it's uh more rare a little less common it could take down the whole wallet and that's why this protect this uh this protective system is so important I think we have time for another question yeah um my name is my question for you is why not use quick quick yes with a wasam what some engines instead of uh success were you asking about Wasa modules yeah why not use quickjs with any wrestlemanian compatibility I didn't catch the whole question but wasn't modules have been designed to be um sort of self-contained and not have ambient Authority not have access by default to Network and and disk so that is fantastic I'm so glad they designed those things that way um blockchain companies use some some what some engines that I have some uh special add-ons that you can enable networking and file system access sorry I couldn't understand the question sorry uh so there were some engines in the market that supports add-ons that give you the connection to tensorflow file system or networking I heard some cool things like tensorflow in there but I still can't make out the whole question maybe you could ask someone next to you to repeat it for whatever reason I I'm not able to hear it hello uh I I don't know if I've got about your question but I'm a bit closer so uh he was asking about um inside wasm or the wasm connections are there any contain connections to tensorflow or what's the last one file system or networking all three oh yes okay so how do wasm modules connect to the powers they need like disk access and network access great question so much like with the compartments you have to pass them in and make them accessible on the like foreign function list that you exposed to the awesome I'm not a wasm expert so I can't answer perfectly but you you do have to you know go in and put them there they don't get it by default uh I have a follow-up question um well not directly related but we were hacking on snaps over the weekend and um I was just wondering like the interface in which lava mode interacts with ses and um and that JavaScript engine because uh no I mean just on the high level I was having a hard time differentiating where lava mode ended and SES started yeah uh great question um so uh we have these compartments from SAS SAS is giving us primarily lockdown and compartments there's a few other features I didn't cover today and then I in lava mode composed those by putting the compartments around the packages you also mentioned metamath snaps this is a plug-in system we're building for metamask to extend the functionality and create new user experiences for minimas but because we're putting uh outside code inside the wallet we have to make sure it's safe and so we're also using Seth compartments to limit their powers like limiting network access Etc yeah that that was the last question I you're going to be around here for example I think that some people needs to ask you some other questions yes I know you're going to be around though because okay thank you so much [Applause] okay okay I'm very excited to know all these people that are like my superheroes I don't know we are you also your superheroes uh also when I was reading about like like CVS and all the work that they are doing I I was like wow this is awesome we are going to have a panel we have a six panelists that we are going to that are going to talk about all these uh audits that we need to know maybe uh the people that this new like I am doesn't know anything about coding about contracts or about something like that maybe we just look that if the contract is like verified and it's out it's all that that I know but for other people that that has more and more knowledge about crypto and about all these smart contracts we need to be sure that all the uh like our ecosystem is already out it uh so I I'm sure that we already know uh like all another systems like Paladin like hacken so uh are going to be wrecked or or we are going to make it so the panel is over here so please give them an Applause [Applause] foreign can you guys keep please coming I hope that [Music] [Music] thank you okay so welcome to the future of smart contract security audits correct or they're going to make it uh Mike uh first I just we get into it I want to think Rajiv uh who is the founder of securium who uh which is training the upcoming class of Auditors he proposed this panel put could not make because of issues am I yeah so and so uh I just want to give him the the recognition for that um so just really quickly I work at optimism I focus on security in our protocol I previously was an auditor and co-founded diligence with Gonzalo and have had the Good Fortune of working as a client with these three gentlemen and not yet sir Torah but have had an opportunity to get to know them really well so I really feel in my element and I'm excited to have this conversation so I'll let them introduce themselves quickly if you could you know give us your names and tell us about your firms and what makes you special I can go first hey guys um and thank you Rajiv yeah if you're watching um yeah we co-founded consensus diligence almost six years ago now or going to that um me and John who's now at optimism unfortunately and the consensus diligence is a crypto native security firm that has for the entirety of its life done Security Services and has also a product side which you might have seen so mythex and more recently diligence fuzzing and a bunch of other smaller tools that are all open source that you can check on our website hi everyone my name is Chandra and I am from sertora sartora is a automated auditing company you can think of it as that way what we do is we use formal verification techniques to prove certain properties about your smart contracts and I'm personally more on the r d side and one of the things that we are focusing on these days is how do you have more Trust on the specifications that you write because at the end of the day that's really what we're using as the ground Truth for verifying your smart contracts so if you have any questions about formal verification I would love to hear about those I think they're good hey everyone uh my name is mehdi co-founder and director of Sigma Prime um we're in information security consultancy focused on blockchain technology doing a lot of smart contract auditing as you can imagine but very comfortable diving deeper into other layers of the stack um as John mentioned layer two systems new l1s and whatnot we're also the founders and maintainers of Lighthouse the rust implementation of the ethereum consensus protocol some of you here I'm sure are running validators some of you may be running Lighthouse as well we had the merge a few weeks ago my Sigma has done for everyone massive milestone for Sigma Prime and yeah Maddie hi everybody I'm Jonathan Alexander I am CTO at open Zeppelin open Zeppelin we are a security advisor security auditor we also work on open Zeppelin contracts open Zeppelin Defender and we are core development contributors to Florida and hi everybody I'm Nick Selby I'm with trail of bits we are an audit and research company based in New York but people are all around the world in addition to audits we have a research group that that mainly works on just sort of pushing the edges of of a bunch of different Technologies including blockchain and where things going next and we make some pretty popular open source tools like Slither and Echidna which probably several of you are you know familiar with it thanks I'm glad to be here awesome thanks everyone uh so first question I think is like maybe a bit of a softball just to get us going uh and I'll I'll pose this to Medi first and then just let the conversation get going so it's been three years since the last Devcon what do you see as the biggest changes in that time maybe it's something you're excited about something you're worried about we'd love to hear it yeah interesting um let's see I think what I've been noticing is the quality of the projects that we've been reviewing has somewhat improved over the past three years uh this caveats obviously we still get the occasional you know D5 projects with you know lack of access control and the typical unsafe and safe external calls and re-entrances but I'd like to think that thanks to some of the tooling that's been available trailer bit's been doing great work on that front developers have a lot of abilities these days to pick up the low-hanging fruits contracts are getting more and more complex in the defy space particularly some of the lending protocols that we've been reviewing are pretty hectic um but yeah I think you know the amount of money getting stolen on a weekly basis these days is probably greater than what it was three years years ago I'd say but it's probably due to the fact that there's just so much more so many more people getting into the field a lot of new projects being deployed and obviously you know our time is you know constrained we can't unfortunately service everyone all the time um but yeah so goods and bads I guess yep anyone else we've seen a lot of projects come and go too and a lot of money come and go so yeah I think that like even I fully agree with Maddie the quality has definitely improved in I think the bear Market kind of helped at that okay anybody have uh something different to say than better quality I have a mixed bag all right so like three years ago it was it was a simpler time uh things were a lot easier to look at things were a lot more self-contained uh and as you just said right the the complexity the the Maze of dependencies the uh the the complexity has kind of exploded at the same time that there's more and more value so the stakes are much much higher I think on the other hand you're right the tooling has gotten a lot better but it's also I think all of us here working with with firms that are we're getting used to the we're used to the risks we're used to the threats we're we're inured to the patterns we we can see them a lot more clearly everybody here has a lot more experience in in finding things uh and on the other other hand um three years ago I remember being describing to board members of a of a pretty large crypto company uh about nation-state threats specifically talking about North Korea and they kind of looked at me with blank stairs and they were like well you know so what and I think that over the past three years that that nation-state involvement has gone from a kind of a risk to a definite and definable threat yeah uh Chandra you so well this is my first Devcon so I wasn't there in the the first the one three years ago uh but what I've noticed is that um there seems to this is a very fast moving field and there's like uh new terms new buzzwords new Concepts that keep getting thrown around a lot uh but I think you know the main thing that I would uh you know say here is I think it's important to remember that at the end of the day uh like companies like surtora or like you know formal verification companies are like any company that is building tools for uh verifying smart contracts they actually rely on a very fundamental uh concept and I think that even though this field is very fast moving uh the fundamentals are still the same and you know those ideas uh go really far and I'm actually excited I think that we should be able to scale some of these techniques pretty well to some of the more recent Concepts that are coming around in this field nice okay I'm gonna I'll give you the next question but let's we'll keep it moving uh so yeah so we're done with softballs um Jonathan what responsibility do Auditors bear when code they've reviewed has been exploited thanks for is it hey I don't know if that's targeted um you take a sip of water first yeah well I I think um first of all I think all of us because we uh I think everybody up here we take our work so seriously that the first order of responsibility of course is dealing with the issue that's found and I think that I know we at open Zeppelin make every effort if we've had situations I'm not going to say we've had situations where things we've missed have been found but customers we work with maybe have a situation arise and you are a trusted security advisor to them you're going to Rally to help them in the situation and that may be in discussing what are the potential fixes in reviewing fixes and even talking about public Communications and that is a uh I think if you are with a trusted you know we view these relationships as Partnerships then that's what what we're going to try to do I think that in the case in a specific case where something is missed in an audit which sometimes um like we've had some situations where uh we'll we'll do an audit and then before thankfully pre-release an issue is discovered that was missed in the audit maybe because there was a another you know set of eyes on it or maybe there was internal testing and in that case uh we always view our responsibility is to do a very thorough postmortem on what happened exactly what was found have our team analyze itself as well as get other people in our team and other get the customer involved so that we can all learn from that and figure out and explain to ourselves and improve our processes going forward so I think that's the very common uh approach so so we've heard from Jonathan that it's I I think this is just like late to the premise on the same topic but Jonathan has been open that a top-tier auditor can miss something and so you're lucky right like it got caught before it hit production that can happen to anyone so let's like put that out there is anybody on the stage like actually had something they voted be exploited in a significant way I'm curious not really but we've had stuff being disclosed after it's in production yeah like I I think that like we establishing and we've said this many many times we're not insurance companies right so that's the Baseline but there's an ethical and moral code that like that we uphold to and so if something happens obviously this is also Case by case on a case-by-case basis it should be analyzed differently because if it's if it's our the if the problem was created by us specifically then we might react one way whether if if the if there it was just like a Miss we might react some other way but at the end of the day we'll try our best but we're not obligated to anything right nor can we be because that actually destroys the entire relationship well I'll go because I'm just gonna jump uh we we actually don't I don't think that we have a different way I liked what you said about the moral responsibility I think it's really it's really important um I don't think that we have a different um process we have a we have a pretty stringent process to to determine um whether whether something was missed but but much more important is what happened and how can we help and how can we reach out proactively they're going to be an incident response mode they're going to be their hair's on fire there's going to be media involvement they're they're in a world of hurt a lot of us at trailer bits have worked in the incident response field so we're we're very accustomed to understanding what they need to do right now uh to stop the burning to stop the leaking and and get things done so our first reaction is to to refresh ourselves as to what we know about this project and then uh reach out to try to offer as much assistance as we can I'm very very pleased to say that the entire Rec leaderboard does not have our name on it today that that will change but you know as of right now but but this is a process that we take very seriously because uh it it can happen there are no guarantees in audits we we always do our very best but I think to stick with the customer when they're in a bad time as well as when they're in a good time is the most important thing sorry Jenna just give us like an insight into what that process looks like like just one or two actually the the process of choosing whether to help the customer or not in in one of those situations so the the helping the customer I mean I let me be clear because I made this same mistake in Stanford I don't mean free uh necessarily right like we we will help obviously for nothing there are certain things that we can do as Auditors and and as experts in in the field where if there's more services required if it's fixed reviews if there's other things but the first thing we want to do is reach out and say hey we saw that this happened do you need any do you know do you want to talk to us about stuff that you're that you're doing right now you said that pretty well if you don't mind go ahead um can we agree that audits is probably the the one of the worst terms to describe what we actually do I feel so bad using the word auditing or audit every time I do but these are really time boxed security assessments let's just all agree on this as John was saying these things are not bulletproof right um bugs will be missed and we allocate a certain amount of time to do our best to find these uh vulnerabilities in your code but guess what if you give us double the amount of time chances are we'll probably find more bugs um so let's just you know make sure that we're all clear on what we do up here in terms of Security reviews and security assessments okay and by the way that's that's not just a cya it's not you you said the word partnership where you said the word partnership um people wait a long time for audits it's a very important part of the process of going live with a product especially a product where a lot of money you know if it goes bad a lot of money can go out the door uh customers have their own priorities customers have budgets customers have realities and usually it ends up that we're not negotiating actually about money but about time we're not negotiating about money but about the number of Engineers who can be looking at it so while we can't guarantee that a three-week two engineer audit will find all the bugs we can guarantee that a three week two engineer audit will find all the bugs that can be found by three Engineers by two Engineers working for three weeks and that that's a different sort of guarantee yeah okay um so I think I think Maddie set this up well in that uh by saying like like just the semantics of what this is what we're doing how is it defined um so we're saying time box Security reviews now do we need do we need a set of Standards to make it clear whatever word we choose do we need to have a set of standards for this and I'll give it to Chandra to to take a crack at it so standards for Auditors you mean you mean are for yeah I mean one that's a good place to take it as well if you like I was thinking more for what an audit is for what what needs to happen during the course of this Security review well um so you know I I can perhaps speak from a more automated auditing point of view but then I think some of the ideas probably also apply to uh human-based auditing uh I think the hardest thing is to come up with what with the specification right like even when you're auditing like what are you actually looking for I think that's uh one of the main questions that you should very carefully think about right uh and you know I think one of the things that's kind of uh difficult for auditing is uh you it's hard to be very precise and if you're not precise then you kind of think that you did a proof or like you have some kind of guarantee that this bug isn't there but I think you know unless it's very easy to uh feel like you you know audited really well if you don't have the specification correctly in your head and um perhaps a related thing is uh in formal verification there's this notion of trusted Computing base and I think that idea also uh you know maps to auditing like what part of the code did you audit what parts of the code did you not audit right and you know you can only say confidently well with some confidence that the code that you did audit maybe doesn't have some vulnerabilities but the parts that you didn't you can't say anything about them right so it's very important to be very explicit about what the TCB is and what you verified and what you checked or whatever you audited anyone else I would say I'm a little frightened of the word standard in what we do because I think sometimes it comes from the place of people hoping that we're going to simplify down to some very simple set of rules what needs to be covered and then anyone could cover it and so I do think we need common language we need in many just helped us with some of our common language and understanding um so that we're working together but I think what's important is um if you're if you're a team and you're engaging auditors um it as opposed to you want to have common language about what will be done what what won't be done but this is people also and people with experience so I think um getting to know who are the actual Auditors and not just I think we were talking about this before not just the team or the you know the label but who are the specific people what is their experience because those are the people you're going to be working with and and what's more important probably than standards is agreement with who who's going to provide you the security review and who are those people and what are their skills and what are their ex what's their experience yeah so I'm gonna like that I'm gonna go to the other side of that the interpretation of the question of standards and well I'm re I'm going to just steal the phrasing uh from of Rickard who I was talking to the other day um he said who odds the Auditors and and so you know as a project and I experience this now as somebody who's audited before and understands the industry far better than most still scary like talking to like trying to evaluate the auditor like for the code that I have that I want looked at does this odd is this the right audit firm and do they have the right people available when I it's there's so many moving Parts it's crazy um but back to that who are as the Auditors like is there some way that we can that we can or should be holding I don't want to go back to like holding otters accountable and the skin in the game thing that we can go around round about but yeah like how can how can we help non-honors evaluate the Otters that they're working with should there be some kind of third-party Watchdog that's ranking you all or what like there's a I think it's a place like this a good plug here would be like to talk about each trust and I think we all participated in the standard in one form or another and I think it's like a dangerous proposition because what the the proposition of each trust is that there's levels of how secure your code base is there's like kind of a risk assessment of how your code base will look like after you have done certain things right so so this this uh this is part of the eea the Enterprise ethereum Alliance and I think it was uh Tomlin who started it I think it is a dangerous proposition but it might actually be helpful if you are not super embedded into the security industry right again it has it is useful but it is it should be taken with a grain of salt because like if you take level three or like formal verification is it gonna say it that will probably be a disservice to you so I'm mixed bags on getting standards for what a good security practice looks like foreign this is a scary one so oh but I'll go ahead I actually think in terms of security work there and and I think about other niches of Technology the work that let's say these teams here do is very public it's it's very public so in some sense we're watched yeah very closely and so this idea that so our reputations are on the line all the time all the time and honestly I know all of us up here we lose sleep over it right like and and I will say too that like it opens up when we work and opens up on contracts and we've had issues and you know so we're our reputations are very public um and we're all glad we're not on the rec leaderboard and um so we can talk about I think it's perfectly healthy to have conversations about how could we provide more transparency and more info but I think we are actually very well watched right now okay yeah now now there still is the thing about what's the right match for you as a project and I think that's an important consideration yeah yeah and I like how you said like you lose sleep over it I think you know everybody in this room loses sleep about either the code that they're running or the code that they're auditing so should all go easier on ourselves um so I just want like I think it'd be fun to get some questions from the audience uh so if you have a question raise your hand uh and meanwhile while the mics are getting handed out we're gonna try something overrated underrated lightning round no people no firms these are Concepts uh okay so where should we start uh Nick uh plain English specs overrated or underrated uh underrated twice it it I was actually Jocelyn Feist is here is the head of our blockchain and he was saying you know you can pick the tool you can you can pick how it is that you want if if you're not able to put down in English what what are the invariants what are the things that you're trying to accomplish here you're you know once you do that you're 75 or 80 of the way through it also it just speaks to a number of things not just for Security review but also just for your own understanding yeah to be able to declare this is what this thing does you can't put a value on that yeah um let me know Gonzalo overrated underrated contract upgrade ability so overrated [Laughter] spending upgradability is a bug not always we went through this so there's a there's a longer conversation about this that happened at defy security Summit I think that is available on YouTube so I could rent on and on about this there's also a bunch of articles on the on the internet about this but generally speaking it you are just injecting another um attack Factory in your code base okay all right uh Chandra how do you feel about sorry overrated or underrated reading the code very carefully truly underrated I think we should read code extreme so we can't just write rules and you can't just write notes no no you definitely need to understand the code before you start writing your rules formal verification is not a silver bullet it's definitely not a silver bullet but really but really like how you know like it's uh how do you know what kind of guarantees you're getting from formal verification uh I mean yeah I think this is a very good question right like you I this kind of goes back to what I was saying before you know it's important to think carefully about what you're approving right and your spec is really all you have and uh you know when you say that okay we have a formal guarantee you should be very careful when you make these kinds of claims and I think it's very dangerous to make these claims because you know um the spec could itself be wrong and even if the spec is reasonable I mean there could be a bug in the formal verification tool and like then that's you know that's also bad and uh you know just because there is a specific bug that you verify verify your code against does not mean that there's uh you know it's just bulletproof and you can just trust it with your life right so um yeah okay um many yeah Europe um have I asked everybody something all right so you're after many you'll get something don't worry um married alternate virtual machine designs so not the evm overrated under Aid overrated I don't know do you wish to qualify that or yeah I don't know I've been spending a lot of time in on the evm so yeah getting familiar and you know it's got its quirks.com syndrome probably yeah probably yeah um look no we've we've we've definitely uh done a bunch of security assessments targeting different execution environments don't get me wrong um but obviously most developers are operating with solidity Viper at the evm level it's got its quirks but it it's kind of working I'd be excited to see the upcoming upgrades uh particularly eof AVM object files that might change a lot of things with regards to security um guarantees or assumptions that we make with that with the evm but yeah I don't know I quite like it by now all right uh Jonathan um how do you feel about like insurance against smart contract vulnerabilities over 800 that's where to start I guess I'd I mean if I probably say um I'm gonna say underrated because I I don't really think it's widely adopted and used at this point and I think it's a it's a it's based on where we're at as as a you know our risk tolerance is really high in our you know our skin in the game is really high and um and and our risk levels are not well understood so it's hard to get insurance yeah if not impossible for a lot of us so I think that'll change that's gonna change for sure and so that's why I say underrated I think it'll change and then yeah there'll be Hedges and insurance and I mean it is happening it's not like it isn't happening but I think it'll it will happen more um and hopefully you know some of the work that some of us do up here will help everybody understand the risk better so that insurance will be obtainable and things like that for those who want it yeah I do I do like I'll I will give one I haven't given many opinions I don't think but I think that having somebody try to price risk uh who is not an auditor Auditors don't want to price risk they like look for bugs right but and it's a totally different thing to be in uh what is an actuary but there should be actuaries and maybe they are the ones maybe they audit the Auditors they have an incentive to use and we're working what I'm what I'm hoping and and with the insurance companies to make sure that insurance in this industry does not go the route of insurance in cyber security where I mean I thought it was quite hilarious about a week ago when Lloyds of London got hit with with a cyber and like they can't set Actuarial terms if they can't even secure their own systems yeah what we're really hoping for is that that people doing Security reviews can set those standards so that that the insurance companies are actually as passionate about what they do that they're getting into it not just because oh look at that it's a new adjacent Market that we can get into and much more around wow this is really Innovative stuff we should get in there to make it safer and we're working with several people like that and I'm very hopeful so I'm with you about underrated interesting okay uh is the audience ready does anybody have a mic and a question yeah I do all right let's just uh Mike if you I can hear you so I'll repeat it come first oh somebody's somebody's set up okay go ahead please hey uh so I was wondering because of all the monitoring tools that are coming out like photo for example and there is also private mempools is it really overrated oh so you keep I like you're using the format uh who should I give that to you later what's the question uh is is is about monitoring um is monitoring like mempool monitoring overrated or underrated specifically mempool monitoring I think or just is that is your question monitoring in general is monitoring over right yeah yeah so uh for example like uh there are products out there that claim that they could like maybe detect hacks and all that uh or maybe secure your smart contracts that kind of a thing but a hacker could use like a private mempool to like really send these transactions across and to even to react right you your transaction maybe I'm gonna say let's talk after not sure it's real but what I'll say is that um I don't think there's anybody claiming that we've solved monitoring in the ecosystem nobody's claiming that but that we can stop threats but I I what I will say is that I think we're all I think there's a lot of agreement that security is not just a time box Security review if you if that's all you do good luck and and there are things you should be doing Upstream and considering like formal verification and fuzzing and various techniques time box Security review part is part of it and there's things you should do so pre-release pre-deployment post deployment there's things you should do in monitoring is part of it and so there's value to an end-to-end security and I think that's where we're at okay we're starting to appreciate that and we want to sneak in quickly while we move the mic over uh uh go ahead Alex you're up to explain don't worry hey uh do your clients request public reports and when you write reports who do you write the report for yes um yes so I was telling one of my team members the other day that uh what we sell effectively is PDFs obviously joking a bit but yeah um yes it's critical the output of the work that we do is a security assessment report which you know contains your standard executive summary detailed description of all the vulnerabilities identified and recommendations um who do we write that for that's an excellent question we write that for our clients now the problem that we may see quite often in our space is uh you know readers um sort of going through all users of the protocols that we review going through our reports and not necessarily understanding again the concept of a timebox security assessment in a lot of cases the scope that we actually go through is a limited part of the entire protocol this happens quite often particularly with very very complex systems um and unfortunately some users may use our security assessment reports and say look this protocol is absolutely safe because this auditing firm has only found a couple of you know mediums and lows and informationals and they've all been resolved because you're at it right exactly that's right they passed the audit big check mark none of us here provide you know any big green check mark in our auditing reports there's plenty of other firms that do that unfortunately um so yeah I think I think you're absolutely right the audience of these these security assessment report is um is a tricky one let me sneak another question and if a client asks you to remove something so or rephrase something are you writing it for a client yeah this happens quite often I'm sure I'm sure that like it happens quite often yeah I'll take it uh no no no no I mean I mean a client can argue on the severity of a bug right and that's absolutely fine you know we're more than happy to have that conversation and be like okay why do you think this is not a critical like look I've got systems of chain in place to prevent this from happening that I would actually drop down the likelihood of exploitation I'm like okay fair I'm happy to drop down the likelihood which would effectively impact the severity of the bug but removing a bug from a report if it's valid hell no now this goes against the trusted advisor role which we play and and how to read our reports or I think all of our reports is that it isn't that check mark and what I want to see from a customer is I want to see them come out with our original report which we will not change and so you as as the public can look at the report and see the problems that they faced and then you can look at the fix review report which is separate or an appendix it's just it's it's different and see okay you can actually watch the progression as they're taking the advice they're understanding not just how to fix that bug but how to fix bugs like that from happening at all and and some companies are afraid to do that because people will think that they're not good at security I look at it as the exact opposite a company that is proud to say look these are the challenges we face and here are the innovative solutions that we came up with to address them that's where I want to look because now they're they're actually taking our advice and they're and they're doing what they should do with an office is there I want to like is Chandra is there are a difference in the way that like a report needs to be presented when it's like heavy on formal verification similar reports and like Maddie said right like we have the severity of the bugs and we explain exactly what property we wrote and why that corresponds to the bug and I think the only additional thing that we very much emphasize is that you know uh like this like be careful that don't uh just take this as like the you know uh this is not a silver bullet and like the yeah I think there's no difference to be honest in terms of the reports we write um okay Mike thank you um so uh disclosure I work with Jonathan at open Zeppelin but I didn't tell about the question ahead of time so there's no front running here um if you could wave your hand and every single developer in the space but most especially your clients just automatically fall to best practice without you having to tell them ever again what would it be yeah yeah it was a bit echoey but uh if you could magically like if you could be sure that every client if there's one best practice that you wish every client would would adhere to what would it be um who's feel yeah all right I've got one do not change commit meet the reviews please thank you here uh should I should I should I start formal verification at the very beginning and continuously throughout the project or only at the very end well it's such a good question so the question is you start from a verification towards the end or towards the beginning of your development so this is a very interesting question you should start as early as you can that's based on you know our experience I think that's the right place to start thinking about formal verification I think the most important thing there is you know you uh it really helps you sort of write your code in the most simple modular way because that's really what is best suited for formal verification and you know it just it automatically just prevents you from making certain mistakes and I think it's extremely important even if you are you know no matter what kind of verification technique you're using the earlier you start the better and along the same lines you know writing your specs out in English and also like you know gradually making them somewhat more formal I think the earlier you do it the better like once you've already implemented your code trying to come up with an invariant or like trying to write down the pre and post conditions and stuff it's it gets really hard so I think there's like I I like the shift left security meme which is like it basically just the idea if you think of your time and your process is moving from left to right writing writing your code then writing your specs and tests is doing it on the right hand side so the more that you can do any of these things your specs formal verification test writing earlier in the process generally the better your outcome will be absolutely yeah yeah um okay anybody else have question yeah so I have a question for you I think Jonathan and uh Nick were touching upon it but crypto is a very narrative driven industry in general and if we zoom out a little bit and get outside of the security Eco Chambers what are some narratives that you want to see and what are some narratives you want to correct that are floating out right now in terms of security I'll try to so you're saying you said it was this is a very we're in an early stage right now we're in an echo chamber of security conversation and we need like I'm struggling yeah I was going to say if we make an assumption that security Community within crypto uh is is is on the on the table and there is a broader developer Community outside as well uh who are all developing building the tools who who are susceptible to certain narratives what are some narratives that are floating around around security around how people see security audits around how people see Security in general that you want to take the chance to like correct if you see anything wrong what does some ways yeah all right um so I'll just repeat it for for everyone from what I heard and correct me if I misheard um bad narratives in our industry that should be corrected hopefully you know quickly and early on um is anybody got done what are you angry about I know how to like um [Music] I'm I'm I'm angry about a a lot of the raises that happen in the security industry recently I think the the the the money and capitalism is creating legitimacy for for people that have not demonstrated proficiency so this is a narrative that really grinds my gears for example um wow and uh yeah yeah so I'll give someone else I'll sneak in a smart contract over availability well all right since Gonzalo started with things that make us angry I'll do one too that um well I think that the our desire for cost efficiency and time efficiency is not in our best interests and I mean gas savings and speed of transfer from a layer two to a layer 1 through the bridge that was just developed yesterday like and our lack of acceptance on latency on anything so that we could do more security verification so I think these things work against us and I understand why we're doing them and how they're helping initially but I hope the narrative changes so that we um aren't as worried about gas savings and we're more and we aren't as worried about speed and we're a little more worried about security and safety for the users okay next question hello everyone my question is is uh if auditing is good enough or would you consider like uh Alternatives like open arena or Bounty bolts like hats finders for example or maybe deploying to medium tier uh Auditors or just one top tier auditor per contract consider a timeline thank you oh yeah you guys don't even so I'm not sure I fully understood the question but I think it's please correct me if I'm wrong but I think you asked if auditing is good enough then why do we need uh the bounties let's make it more General I like we sort of I I think I think I'll get we will get this is like like just generally we have some new alternatives to audits that have emerged giant bounties uh and and like code Arenas and Sherlock doing these uh like auditing competitions that are a little bit like a Time boxed uh Bounty program so yeah um how does that uh how does that like substitute or complement auditing he wasn't enough I don't know if anyone said like we might yeah yeah but my answer is is back to what I said a little bit before which is a bounty can give you that this bug which is interesting but it's not really as interesting as as how you are working with this is a this is a holistic view of of all of The Code by by Engineers who are deeply and intimately involved with the code to the same extent that that your internal people are but they're seeing it from through a different lens they're seeing it through the lens of people who have literally watched everything that could go wrong go wrong and and being able to see mistakes in everything from documentation to to just your overall maturity it's a different set of things if you want if you want to find specific bugs and fix those that's great there's other things that are that are outside the the general scope of what we do in a Security review uh I'm not suggesting that that audit or Security review is the the be-all and end-all but it's also uh different from a lot of things that I'm seeing out there and and as far as automation goes we use automation to find sort of low-hanging fruit or to point us in the right direction so that humans can look at it and make educated decisions about what we think and there's a lot more than just run this tool see what you get and hand over the report anyone else quickly we're running low on time no no that's I meant on the stage like if anybody wanted to um okay and all right I'm gonna be selfish too there's something in the time we have I really want to plant this idea as much as possible is that it's not the auditor's job they're not selling you Security in my opinion they're selling you like insight into your ability to write secure code so you need to take responsibility for writing secure code uh and not act as if like you just get to write code that does cool stuff and then give it to the auditor and they will make it safe for you so you need to focus on being able to write secure code whatever process you think that might need whatever tool you think that requires and then you give the other your code so that you can understand how close you are to being able to write secure code amen it took me a long time to get there so it's a bit of an epiphany and I'm trying to but unfortunately we see what John was describing way too often and yeah it'd be great if we can all collectively shift towards a different um yeah definition of what our work here means um yeah thank you thank you okay there was another question so let's have it yeah so um Foundry when Foundry first came out I was surprised by the fact that it included fuzzing so my question is when does fuzzing make sense on these code bases which are generally open source and small and how do you approach fuzzing in your use for auditing fuzzing makes a lot of sense all the time it's we we're going to like the shift left the shift security left narrative and all that like you should use all the tools literally uh you you should write specs if you have specs fuzzing will work better right um so I I don't yeah we should just do it all the time use Foundry and use all the other tools use a kidney use Harvey by the way forgot to say I've been announcing this the entire week but this is the panel we at diligence will be open sourcing all our tools just like trailer bits we are a bit behind schedule but at least very late than never so even Harvey yeah yeah yeah all the code bases so yeah foreign did you uh I just well yeah I guess I to add to what Gonzalo said I think uh using tools is actually good so fuzzing makes sense to me I mean you know again it's not a guarantee but you know I think uh it's just like good software engineering right like you write code and you want to write tests and you want to do fuzzing and you want to write specs and these are all good software engineering practices and I think they also just are they carry to this domain as well so it makes sense um the one the one comment about that you just made well both of you like uh that the Declaration of things the ability to set forth the spec uh and then and then fuzz around that that's really important that first part is what Jocelyn and and Gustavo are going to be doing tomorrow there's going to be a two-hour workshop on how do you declare these things how do you actually set out and and describe invariants and how how can you use those to make your fuzzing more effective so I would really recommend that that you go to that so just to be fair I know like we're at a time by this timer let's give every speaker like 30 seconds to sorry no okay all right so we started the questions way early we got tons of time um I should have read the manual um did uh Brian I know Brian had his hand up at one point um and okay you please just yeah just like I'll repeat it through the mic so go ahead okay you guys are excited just go ahead repeat the question and answer it please that's that's cool is there a way to make like the the output of of audits machine readable um and that's interesting because we we do actually that's either right yeah yeah no I wasn't gonna go there I was gonna go that that's really interesting and I think we're gonna take that up and talk to us to hit us up afterwards um I I think it's gonna really depend on the writing style of of the security firm that you engage with um but it is something I'd like to think about a bit more yeah um perhaps we could parse some of those reports into a readable format or a machine readable format that shouldn't be too hard to do we write a report in reports in latex so should be pretty trivial for us to do cool so uh so try actually we do have we have been working on things like Dr oxygen I don't know how to pronounce it and like there's also uh Nat spec is another similar Library so uh the nice thing about you know if you were if you have a formal spec it's actually a little bit easier to like sort of convert that to a format because you can annotate your code and then you can automatically generate some kind of a formatted report from from that so I think it's a good idea I got one back here uh Hey guys Kevin over here uh Hey friends um so having come from sort of the security side and now being protocol engineer and founder um I'm wondering about this this line you tread as an author of being very slow methodical you know heavy testing core protocol must be super secure to now okay now I'm wanting to move a bit faster maybe their integration contracts or something that's not in my core protocol um you know how much time am I spending and testing and auditing how much money am I spending there versus move fast and see if things work I wonder if there's I have my own opinion but interested to hear what's on the stage I'll just quickly I'll say um the most secure code is the code that never hits mainnet so just keep testing I also think it depends on how much money you raised or how much money you have and now like it's very it's a very practical thing you know like you can spend a lot of time and resources doing doing writing tests but at the end of the day if you run out of money and you only have the tests you're probably anyone else okay um hey y'all question back here can we get a mic for Brian I want to hear what he has to say but go ahead please okay cool it might come off as somewhat pedantic but I'm curious looking at the different way that auditing firms do severity or impact and then difficulty slash likelihood I'll be honest I like when I prefer likelihood so that it's like difficulty is low when likelihood is high so I was just wondering you know High likelihood and high impact would equal High severity have you seen clients get confused about this different and without being overly like strict as the industry is still developing I'm also curious about like how we might classify things trail of bits has a nice classification swc seems like outdated and not updated as well as or maybe it could be going forward are there things that we can do as an industry to improve and start to I hate to say solidify but structure our information architecture a little bit more so that clients can understand when they're getting multiple audit reports and reduce confusion thanks I think I think it's uh yeah it's a great Point as you said uh well for us for us at CP a high likelihood high impact critical um and I think we all use perhaps similar types of risk rating matrices um it it's probably beneficial if you could standardize on on how we would write severity so as you said it's typically likelihood of exploitation impact of exploitation but there's different levels so we only have we got four of them at CP um so informational low medium high and critical that's actually five um but yeah I think I think it's a great idea there's been a lot of initiatives over the past few years on the it security channel to try to standardize some of this stuff it's never happened I don't I don't really know why um but I think it would be it would be quite beneficial okay great I think that like so Brian started that channel so yeah what do you got I'm gonna give you a bit of a left field question so there's been a few regulatory proposals that includes uh software security best practices in The Proposal uh what sorts of standards that are driven by regulations could be do you think could be damaging to the space and what ones do you think could be good lately so so almost like uh yeah the typical like like a regular comes out of nowhere and there may be a bit naive what kind of damage could they perhaps do by uh misunderstanding well I think I I don't know if it goes to like the intent of your question having to do with structure of code I think we all know we're concerned about regulations that would impose requirements on protocols to be able to block certain parties as an example so I think the biggest a lot of our concerns are around regulations that would have a requirement so I don't know if that goes to your thought about the question having to do with specific Developers your contracts must be upgradable for whatever reason those things are always damaging if you're talking about regulations at the code level I think there's no way they can be good you know like I I think that um as an example having worked in uh security before and in the in the cloud space the sock 2 standard and actually I believe it has a lot of really good aspects to the software development the secure security focused processes of your software development life cycle and I think those are good practices and if teams were transparent about the processes they follow I think that's healthy and if held to a certain standard all those things always have downsides in terms of the effort that goes into auditing them and proving them but yeah the life cycle of Regulation is really tough um getting getting regulators and government officials to understand the key areas they they have a goal of what they want to accomplish but usually they're they're rather far behind the technology I think it's up to us everybody here we certainly do this I bet everybody else up here does this we spend a lot of time talking to Regulators about what is possible and what isn't we spend a lot of time talking about the intent and trying to come up with sort of more more open suggestions to Industry as opposed to specific because if if you say thou shalt do the following it's always going to be out of date by the time it passes and and it just becomes very very difficult so I think that the best thing that we can do the most practical thing we can do is educate those those people who would regulate us thanks for clarifying Brian uh yeah that makes a lot more sense um just to give you guys maybe a an exotic uh perspective I was catching up with some of the Regulators in Australia a few weeks ago and this is exactly what they were asking for they said what should we be you know requesting from projects to do in terms of security uh practices in terms of security development secure development um and you know what should we be regulating in the space so I try to sort of convince them to go down the educational part and um and yeah this is this is spot on I think it's certainly something that is happening at least in Australia these people are starting these conversations and are starting to realize the need of you you can't enforce these things on projects but at least you can have these set of guidelines and point your users your you know citizens to these guidelines and get them to demand these standards this set of minimum requirements from the projects they interact with uh we must be a time now sorry yes we are we've run out of time thank you everyone I just want if you want to yeah we had we had a little words we had Nick from trail of beds we had Jonathan from open Zeppelin Maddie was from Sigma Prime chandruff's from Sir tour and Gonzalo was from consensus diligence maybe I could have just two words or three or four words I think that uh this presentation could now be called like we are gonna be audited I don't know but maybe uh if the social layer maybe I'm thinking that we need like more more knowledge for us that we are like the beginners so maybe if we can like uh get more uh information about all the Auditors that are actually working in this space maybe it will be like a better and more important uh like a social major thank you [Applause] foreign [Music] [Music] foreign [Music] to already introduce myself we had a little MC Handover so I will be emceeing the last half of the day today here on the cold Forest stage happy to see all of you um in case you didn't see me on Tuesday my name is Francisca I work for the ethereum foundation and in the foundation mainly in the solidity team but I also do some other stuff and yeah in this afternoon we really have a quite cool lineup on um developer infrastructure mostly about solidity evm assembly yeah we will go pretty pretty deep and I'm excited to announce the next speaker it's Alex he's the team lead of the Epsilon team at the ethereum foundation Epsilon is a team that is focusing on improving the evm but not only that Alex was also the core team lead of the solidity team and also a very early contributor to the solidity language and compiler and that's also the topic that he will be talking about today he will uh yeah talk about the magical version 1.0 in solidity uh when we reach it how might it look like and even further going Beyond how my solidity 2.0 look like so please give it up for Alex [Applause] I'm not sure can you yeah you can hear me right and I hope this works as well okay it does um yes Francie said uh thank you for the intro and I wanted to also make sure that um a lot of this stuff is my opinion it's not like the opinion of the team um I have been working on solitary since 2015. and um in the past year I've been kind of like taking a break and uh did write a lot of code in solidity itself and I think some of this is like a culmination of of you know my experience using the language but I hope some of these things are mentioning may come to infusion Okay so zero eight Seventeen is the latest release it's a hundred second release we already had more than 100 releases that is just insane and we probably have more than like a thousand night leads which is just crazy basically we had eight breaking releases a bunch of feature releases and sometimes we do have like bug fix releases as well um in some of the years we had releases every two weeks I think lately it's more like monthly um and I hope that's going to come to you maybe you're going to get back to like bi-weekly releases but at least want to release this and I'm saying this just to show that how active the development of solidity is I'm not gonna go through all of these um you can find this really nice chart on the solidlank.org website and it shows you all the interesting milestones we have accomplished since the beginning however it doesn't show anything past 0.6 because he made this two years ago but as you can see it's it's a lot of stuff going on and based on like having over 100 releases you may ask a question when do we actually get to 1.0 and you may not be alone asking that question we do get that question super frequently last year there was like this big debate um this particular issue I'm looking here the issue is much longer the description is maybe 10 times as long and the author of the issue provides a bunch of different reasons why he thinks solidity should be at 1.0 I summarize like three different discussion points we have been debating on this issue the other set or had the opinion that the current scheme doesn't actually allow breaking releases the separation of breaking release and non-breaking releases in his opinion on every single solid release is a breaking release he also argued that the languages widely used it is it has been for a couple of years and so it should should be a 1.0 because that signals that it is ready to be used um in fact you know when 1.0 was released we never signaled a 0.1 was released we never signaled that it's ready for usage but the nature of blockchain it just started people just started to use it and it's there it's always going to be there um but also the team had some concerns about this 1.0 stuff basically we have the impression that 1.0 kinda implies that we have to have a long-term maintenance of that version we have to add new features non-breaking features um and all kind of changes and if we still want to do braking changes that's going to be like 2.0 and then suddenly we have to like maintain two versions or we're not going to make breaking releases so that that is the reason we kind of wanted to avoid going to 1.0 I'm not sure how many of you actually know how the versioning system works um do you want to like give hands on are you a developer first of all so many of you are using solidity are using anything but solidity not many any other language besides solidity nice so I think you should be a very hot this versioning system works um we were under the impression that it is semantic versioning turned out it is not um because anything below um the major one so major zero is a breaking change however we did assume that um you know the three three numbers in the version is Major mine or patch and we assume that if we bump the the minor that is a breaking release if we bump the patch that is a non-breaking release so it's not actually in conformance with semantic versioning but it kind of works for us um here's just an example you know how this would look like and now suddenly you know once we know what 1.0 could mean maybe we should talk about how could we get there here's an example contract um with the latest release this actually the solidity stocks contract which is on mainnet you can use it to make stocks both land and the right hand side if you're lucky you can you can win both and uh I think the stocks are gone at this point but you could have gotten them a physical version of the stock at the solidity stand um I think on the floor above I'm not sure now this is 0 8 17. it is 1.0 you see any change yeah I kind of think that actually for the users you're not going to see too many changes at 1.0 we may actually have some changes okay so there was the the non-changed version okay this is an option change version and this has a few changes they're not important yeah where is it like this stuff this stuff all of none of this is agreed on but there may be a tiny changes but I don't think they're going to be big changes um so no major difference is if you're a user but giant difference is if you are writing a library I listed a few things on what you would be able to expect in libraries uh we should have like operator operators and letters to use the defined types standard Library generics you know data types I just gave you a few examples now but actually Daniel from this already team had an extremely good talk which you should watch and I'm going to share the QR code for the video as well so here's an example of user defined operators I cannot see that I actually like the syntax but the team kind of agreed on it so without tests you have to use the usual function chaining but once you have user-defined operators you know what you can expect you can just use them fairly easily so I think the team kind of agreed that this is going to be in uh this is going to be a focused project and I believe this is needed for like 1.0 another major thing is a standard Library which we have been talking about for years and the main goal of the standard library is to move out the majority of the compiler code into solidity itself but it is also really good exercise doing it because it shows us what kind of language features are missing so here's one example this actually doesn't require anything major the other thing that requires is this fragment standard lib and what it does it just disables the built-in functions because if you don't have this primary standard lib this function definition is going to fail because there's already a shot 256 implicitly in the language if you have this it's not going to be there so you can define it this is the simplest example in the standard library and this works doing anything more complicated going to require a lot of language changes here's one example what we will require for the language changes it is just generics just a just an extremely old example and in fact I copied this example from my talk from two years ago from this already Summit you're already seeing the same things back then so now can I go any further if you want to take a um a photo of the QR code this is the talk from Danielle it is a full 30 minute talk and it did a really good job at explaining the reasoning Behind These features and how they're going to work okay now taking a tiny break these were the changes the team wants and the team agrees on but I kind of want to ask you all of you to look at the Repository this these are all the it's kind of hard to see but these are all the issues Tech language design and we have 237 issues open those are the issues people want want as a feature and I think many of them actually are kind of obsolete at this point um but I ask you to look at this repository look at all of these issues or you know a tiny subset of them find something what you like and leave a comment if you want to have it or leave a comment if you think it is a bad idea you know besides these features that the team wants I think it would be nice to know what you guys actually want so I think that's what 1.0 gonna be like um not many like visible changes for the user side if you're just writing like a token contract NFC contract Etc but if you're writing libraries it's going to be entirely different now what 2.0 that is a crazy topic and uh we haven't actually talked about this for a couple of months at this point on and off and actually 2.0 is two different things and it has been like super confusing and the team even talk about it because you know some people meant the one or the other and I just want to highlight it again you know nothing is decided here a lot of this is just uh you know my idea what I want to have um but I hope some of this is going to happen so first I'm going to talk about like this compiler rewrite where it sounds crazy right um how many of you actually know the compiler how does the compiler work or a compiler in general not many of you um basically a compiler has multiple stages you know we take like the source code we process the source code into some internal representation we do a lot of different analysis on that representation once we are happy with the code that it is sound that it works we are generating some kind of a next stage in the first version of the compiler this generation was directly into evm in the current stage we already have we we still have this but we also have a second pipeline where instead of directly generating to evm we generated to you which is our Intermediate Language um but you must be familiar with you because it is a line assembly basically and then we take fuel and generate evm byte code so that is the the pipeline and then these are the different libraries we have in a compiler itself so this one is just really tiny helpers I'm going to move to the other side so these are just helpers used by the others the assembler is a kind of separate it's it's Standalone um it takes in a data structure like the representation of the the assembler source code you want to assemble um it generates even bytecode but it also has optimization steps then we have this separate helper for it's called language utilities but it's really just helpers for the parsers because we have two different parsers we have the solidity parser and you'll um so Lang duties is used by both of these the libyo has a parser has a code gen has another kind of optimizer and then lips solidity is a big one uh it's the it's a monster it is a solid departure um does all the analysis I mentioned and does these two different pipelines for code generation and now we have two smaller libraries the smt library is just for the smt subsystem but I think that that is also split up so some of it is here some of it is in this lip solidity directory and lastly the smallest one is lips on C which is a tiny binding that is the only Library which is C has a c API everything else is C plus plus so this basically is a wrapper between C and C plus plus and that is uh what is compiled to M script and into JavaScript if you happen to use any kind of a web app that has related it would be used I listed a few issues and maybe some benefits as well but you know one of the bigger problems we are getting into is these libraries were physically separated into their different directories um they may not be like super well separated conceptually or even like interface wise um one example I can give which which is I kind of hate it's not like a you know a logical like um issue in there it's more like just a source code issue but we have a a loop between libuild evm ASM um and the front end they're interdependent on each other um which is kind of bad if you want to separate these things nicely the major components they do have some kind of a clear boundary but it's everything is in C plus plus the the clearest external boundary happens in two places the assembler has a Json Import and Export feature the export feature has been there forever the import feature is still not merged but it kind of works and then solidity itself has this Json AST import export I think some tools like Scribble may be using it um basically you can skip the parser no scribble uses a typescript parser but yeah some people we wanted people to use it some people use it but they ended up not using it at the end um but the main issue I kind of see here all of this is C plus plus which is kind of hard to integrate with other kind of languages it is hard to integrate with with uh like JavaScript we have this m script and layer and we have the C wrapper it is hard to integrate with rust you need to see layer as well although rust does have like a C plus plus binding generator it is not really stable if you want to integrate this into go it's the same case all of the all of the languages they only work with C they don't work with C plus plus so ifsu you know ideas how could we resolve this and first of all we want to just improve the the separation of these libraries we want to have like clean interfaces between them and maybe reduce the number of like C plus features that the interface they're using to make them more compatible with c then parallel to this because this this can happen on the existing code base prior to this we could start working on components in Rust and we in fact already have components in Rust we have sources rust which is used it's used by Faye actually and I think at some point fan reused it but they were annoyed by by the size of the binary and compilation pipeline so that they're not using it anymore but we do have a bunch of like libraries new libraries in Rust which is also used by Faye and some people in the team now are working on some crazy features on top of it the first useful step could be rewriting the assembler and the reason behind this is the assembler is one of the oldest components we actually never changed um the parser itself has me mostly Rewritten or at least significantly improved um I think the type system that is also kind of old but it had more maintenance but the assembler that is the oldest component which has never been significantly improved on it would be a really good task to rewrite it and by rewriting it it may or may not even need to be Rewritten by us but we could use an existing assembler and in fact etk the evm toolkit evm assembly toolkit I don't know the what the acronym is for but it is an assembler toolkit basically for evm it's written in Rust we could just use that in any case once we have some of these components we could think about creating a compiler skeleton so basically just a driver which drives the completion process and this doesn't mean that we would need to rewrite the compiler all we would need to do is provide trust bindings to some of those components and then have this driver which just uses those components and once this work works we could swap out parts of those components for example the assembler because they are just components and once this this will be working the biggest change would be actually doing a major rewrite of the front end in Rust and the front end by the front end I mean mostly the parser the analysis the type system Etc and while the code generation as well that is an insane project and why would you want to do that well you likely don't want to do it for 1.0 but you want to do it for something else so what is the reasoning behind all of this um the main reason is that we kind of or at least me when I turn solidity all this code into usable compiler framework I want to make sure that all the you know the optimization steps we have all of these features are not just wasted they're not just there for solidity but they can be used by other languages imagine if we would have had like this compiler framework earlier on you know it came to like llvm4 evm you know how how long would it have taken to Forfeit to to come to fruition or like have for any of these other languages if you could just use any of these components I listed a bunch of uh different projects which are already in the evm space working on Rust compiler components so of course we have fail we have the two other solidity compiler slash parsers so solang solang is a full featured compiler from solidity to webassembly targets well actually llvm targets they started with webassembly but they also support like BPF like Solana and a bunch of other targets the one thing they don't support is evm and in fact solang I believe is used by Foundry for parsing solidity because it's in Rust so they're not using solidity solidity s-lang is a project by hardhead they're trying to write a compiler as well the motivation is slightly different they want to have a person which is flexible and supports every single version of solidity because they want they don't want to like survive the compiler um you know mid compilation um and that's the problem with the current compiler it only supports a single version so why would we want to do this because I want to attract more people to solely compiler development and it seems like C plus plus is not like a language people like or people are interested in it has been kind of hard to attract people to to write C plus plus code but it seems like rust is extremely thriving um every other project is in Rust I've been using rust for a long time so I would be happy for this to happen and if we do some kind of a re-architecting like this that would mean that we have an opportunity to actually improve the architecture of the compiler and maybe improve the language itself so that is the next one I'm going to talk about is what kind of language could we have here you're really surprised or not but the fact is we have been talking it like this um you know rust inspired changes to the language since 2019. um way before some of these are arrest inspired evm languages came about oh it's actually another slide um I think the main motivations the main reasons those discussions were started because there are a few issues in solidity itself or at least a few issues I think shoot the address um one of them is really the storage and implementation itself is not really separated so the only thing we have right now is this contract construct it can have storage defined anywhere it can functions defined anywhere it can inherit other contracts which also Define storage and functions anywhere and you never know where storage is it could be handled by any of these imported libraries it would be nice to actually make this more clear the other thing would be nice to kind of make clear when State changes or state access can happen we do have some of this in the language today um you know we are payable we have View and pure functions but that's about it it would be nice to you to have like more clarity even within the functions when is a state change taking place functions can be quite long um I think good code bases started to create small helpers and limit the scope of like State changes to those smaller helpers but it's still people can write no matter what you do people can write really bad code but in solidity especially people can write like giant pieces of coding would be nice to have like Clear View whereas State access and state modification taking place and lastly if we you know take such a big step we could even consider removing like inheritance or looking at like a different way of composing um the source code and in fact if we have like more clear control about the storage maybe we also have more clear control of a storage layout which has been like a really annoying question and I know many of you have open issues how can we like stats like slot numbers Etc um some older contracts have like these padding storage items it's insane okay now finally what would it what could it look like so here's an example I'm not sure if any of you recognize this it looks like rust but more specifically no it's not sway it's Faye this is actually say and uh they started to basically do all these steps um we were discussing um which means having a context which is like a clear separation of State access okay this is what's on the website but um yeah this is another one which is nice um you know not having like special contracts rather have it tied into State access and I think they also like changing it from having a contract and all these things to having uh separate pieces now here's a real example of what is more like what we discussed under like trust solidity um do you have like a separation of this could be contract could be struck um people leaning towards struct so this would be really just the data um and then that would be the implementation I mean it's not too different to what the Faye is trying to do now and this is the distal contract I had as an example before yeah I mean that's literally I only have 10 seconds left [Applause] thank you I guess we can take one question if somebody has a question please raise your hand so that the mic can find you hey Alex so just a simple question you got you're talking a lot about rest but have you guys give your talk to Cabo that's like the new language that they are going to upgrade C plus language guys would be able to use everything that's written so far I'm not sure what benefit that would bring um it's worth a thought yeah yeah I think we I don't think we had a look but we heard about it unfortunately time's up but thank you so much Alex and I guess uh find him for questions somewhere here around if you have more okay um moving a little bit from the potential future scenarios of solidity to the present of solidity and the present to evm I'd like to introduce the next speaker Harry he's a contributor to the solidity language um also an auditor and decoder and today he will walk us through how symbolic computation Works end to end in the evm and in solidity he will give insights on problems that can be solved efficiently how to build custom servers from scratch and produce computer proofs and I'm sharing some Alpha I think we will be saving some gas foreign so I want to start my talk with a question uh Harry say here's a solidity code um how do we like optimize this code there is one trick here what is a trick okay perfect um yeah so what he's saying is that this branch is not reachable for whatever reasons we're going to figure out how we will symbolically figure this out um and how we can use symbolic computation to find that this branch is unreachable so what is symbolic computation symbolic computation is about representing properties using mathematical equations and we use the solutions of these equations to reason about the properties we started with um the property we wanted to know in the previous example is that the reward is unreachable and we have to figure out a way to convert this solidity core into a mathematical equation and I want to give some hint about the climax which is usually these systems having the system of equation having a solution means that a property can be violated and on the other on the contrary um if the system of equations don't have any solution then it usually means that the property is always true I mean this is not generally true but most of the time this is what you're trying to do um and now we want to convert code into mathematical equations how do we do that so um so the question is what do we encode so we could encode solidity directly uh we could encode Yule which is an intermediate uh representation of solidity it's a one level down from solidity we could also encode evm which is at the lowest level so solidity is a complex language and there is let's maybe like skip encoding that because there's like too many rules we have to handle um idiom on the other hand is like too simple and we have to extract a lot of information about the control flow and then code it so maybe Let's ignore solidity and evm for encoding and just deal with dual it's it's in the right middle ground it's simple enough and it has enough information about the control flow so the most fundamental thing to encode would be a variable and a variable in AVM is 256 256 bit integer um most of the time you represent variables as an element of integers um Z as the notation if possible we add the constraints to roll less than or equal to X less than or equal to 2 raised to 256 minus 1. um now that we know how to encode a very simple variable we need to figure out how to assign um values to the variables it's a very simple dual block with three variables x y and z x is assigned the value 1 Y is a sign called that a lot 0 and Z is an expression in uh U less than or less than x y so we want to represent each of this assignment by constraints um for X and Y it's pretty simple for X you just have the equation x equal to one for y we have to just say y That's it it's a simple AVM variable we can't we can't we can't really like produce any extra constraints from call that a lot zero because it can be anything so we just have to treat y as a regular even variable um and with Z we have to figure out a way to encode a less than of X Y uh we will deal with that later uh but the big question is that can we handle every assignment so here is a different Yule block where you assign the value 1 to X and then there's a switch of statement which has a three different control flow uh branches so depending on the value of call that a lot zero which we don't know what it is we can assign um x 2 3 or 4. so the question is can we actually encode the switch um this brings us to the notion of a single static assignment um so these are variables that are only assigned once and working with SSA variables simplifies our analysis quite a bit so here is an example of a different you will block you have two variables X and Y gets a sign called that load 32 at the beginning and then reassigned something else by definition Y is not an SSA variable it's signed twice but it's actually possible to transform the same transform this block into another Yule block where we introduce a new variable Z um and all these variables are actually SSA variables so generally speaking we only want to work with SS variables because they really simplify our analysis uh however it's not always possible to do a yule to Jewel transformation such that all the variables are SSA there was the example of like two slides ago uh the switch the switch uh sorry uh the switch example uh you cannot encode uh X as an SSA variable purely in you but we can still get a lot done with just uh taking whatever is SSA and maybe we have you know we also have this step in the Dual Optimizer called the SSA transform that lets us transform Yule into what we call as a pseudo SSA format so a lot of variables are SSA but it's uh uh it's not not it's not necessarily that every variable is as I say so whenever we encounter a known ssf variable during the analysis we would replace it by a free variable so a free variable is what we mentioned like uh two slides ago sorry three slides ago which is just uh the basic constraints that you can give to any variable in in the VM but there is this important caveat here that okay whenever we encounter a non-assessa variable we have to replace it with a fresh variable because the value may have been assigned something else during the two reads but we can of course optimize this further but this is what we'll do now so now let's think about how do we encode some evm instructions um so perhaps the most fundamental evm instruction is addition uh you take two numbers and you know you add them and leave it at the stack so how do we symbolically represent uh the addition you know X X Plus y um what do you think it just is it just X Plus y um unfortunately it's not that simple so if you look at the evm semantics of addition addition is defined by X Plus Y modulo 2 raised to 256. and if you look at high level solidity code um since 080 we have detector automatic so X Plus y would revert If X Plus Y is greater than um 2 raised to greater than or equal to 2 raised to 256. so we are already seeing that it's not as simple it's not that simple to encode add it's doable but it's not the easiest but perhaps here is like an easier set of instructions um less than greater than and e0 here's a formula formal like representation of these op codes we Define when when these values take one or zero there are like if for less than or a B if a is less than b the value takes less than the off code gives one in the other case it gives zero uh almost the opposite for greater than um for s0 if the value is zero then you get one and zero otherwise so let's uh decrease for a bit and talk about difference logic um so let's start with an example so x y and z are integer variables and let there be constraints two of them x minus y less than or equal to four and x minus Z less than or equal to three the question is does the system have a solution what do you think it does have a solution you can just assign x equal to 4 y equal to 0 is that equal to one and these two constraints are satisfied so to go back to go back a bit you can generalize this um a difference logic by saying okay you can have n number of variables X1 to xn that are integers and constraints of the form x i minus x j less than or equal to a constant but let's look at a different example you add one more constraint here which is z minus X less than or equal to minus eight and the question is does this have a solution now any any takers it actually doesn't have a solution and how do we prove this so assume that there is a solution let's just add all the variables so let's out let's add all the equations so you add x minus y plus one minus Z Plus Z minus X and the RSS is going to be 4 plus 3 minus 8. and the ls is going to be zero so we arrive at something less than or equal to minus one which is a contradiction so there is no solution but how do we use uh some graph Theory trick to do the same thing so we try to encode each of this constraint using a graph um every variable is a naught in this graph so you can see that x y and z are three nodes and we assign some weights these are the weights that come from the equation so x minus y is less than or equal to four that would be the weight of the X similarly for the others what's important is that it's a direct graph and um the direction follows you know so in case of a minus B less than or equal to K uh The Edge is from B to a and has a weight of K so the important takeaway here is that if there is a negative cycle uh in our direct graph then there are no Solutions uh to our problem um you can see here you can see here there is a negative cycle so if you add up four plus three plus minus a that is negative one so that that's what we are looking for so how do we find uh negative Cycles in a graph so there is this very classical algorithm called the development Ford which can tell you given a direct graph is there a um is there a negative cycle you can also use it to find the shortest path between two um two nodes that's the classical use case but it can also tell you if there's a negative cycle and it's a pro it's surprisingly easy to implement you can even implement this in solidity Leo has a repo where he implements the Bellman forward and much more in completely in solidity and he's going to have a talk tomorrow at 11 PM 11 am you can come for the talk for more details and here is like some insight about unsatisfiability like unsatisfiability is when uh the set of constraints have no Solutions um and a lot of times we only encode like a very small set of like uh what we can actually encode and we are very generous about like ignoring the constraints we can we can't solve like I said already uh we ignore like non-necessary variables so as long as we only care about unsatisfiability we can do this um and we can optimize and we can usually optimize when the constraints are unsatisfiable otherwise we just leave the code unchanged um so we talked about difference logic but what does it have to do with um all this like evm um opcodes so it turns out that we can represent these three evm of codes using uh Expressions that would match uh difference logic so in case of less than of a b so when the value is 1 it's only when a minus B is less than or equal to minus 1 and 0 when B minus a less than or equal to zero so similarly you can uh build these constraints uh for greater than and is zero so in the last example um zero is just a variable that we used to indicate at zero there is some like Nuance here but you could just treat zero as a variable here so how do we encode yule so a lot of times we want to know if the value of an expression is always zero or always non-zero so if you take this example of if of condition and then uh something going on in the if statement uh we cannot replay so the question we want to know is if we can replace condition by zero or one um inside the branch we can actually replace we can add additional constraint that condition equal to true so to take a in particular if you look at less than F of less than x y we start by checking if adding the constraint X less than y make the system unsatisfiable so in indifference logic this is x minus y That's not equal to minus one we just add it to our other set of constraints and if this system is unsatisfiable we can replace less than of X Y by zero similarly we can check if the system if the constraint X greater than or equal to Y make the system unsatisfiable in that case we can replace less than of X Y by one and inside the if body we can add the additional constraint that x minus x x is less than y in difference logic that is x minus y less than or equal to -1 and then we can keep doing our symbolic computation so going back to the problem from the beginning um here is like a version of the same like solidity chord in you we have three variables X Y and Z they all read from call data they are not 100 equivalent but more or less these are this is how they will call would look like and we have three um we have three if statements uh the first one has less than of X comma Y the second one is less than of Y comma z n the third one has less than of Z comma X and the last one would reward if we can reach there and the question we want to know is if the last less than of X that can ever be zero sorry it can it ever be true and if it's never true we can replace it by F 0 which is what we want let's think about how to encode uh the problem now so we have three variables XYZ that are integers uh we don't have any extra conditions for call data law because it's uh we can't really tell anything about it um we add a dummy variables Arrow as I said before now we add the constraints that these variables are 256-bit numbers that is 0 less than or equal to a less than or equal to the U N Max so the first set of constraints are simply saying that X is x y and z are positive you can see that and the second one would say that x y and z are bounded by the maximum value of u n 256. and inside the if branch in the first Branch you can add the first constraint x minus y less than or equal to minus 1. inside the second if Branch we can add the constraint y minus Z less than or equal to minus one the third one we can add a similar one Z minus X less than or equal to minus one and we learned quite a bit here so how do we represent all these constraints as a single graph so you can see the nodes um X Y and Z you can also see the node 0. uh these are the constraints for the positivity and also the boundedness so here m is the the maximum value of a two after six bit number and uh sorry um this this constraint is a z minus X less than or equal to minus one similarly the others are on the outer I mean the outer edges of the node and the question we want to ask now is is there a negative cycle in this graph and turns out there is one that the one and on the outside is the negative cycle which means that the system has no Solutions um so now we can we can actually replace this F of less than x z by F of 0 and once we have this first zero we can just completely remove this uh branch and um after that these branches are simply empty you could actually remove the entirety of the code um so in case of the difference Logic the solver is very simple um as I said you can write it in Solarity layouts and that however in general the solver can get quite complex and the question of correctness will always come up the biggest priority for solidity is like the correctness of the compiler and we really want to like minimize trusting external tools as much as possible when they influence the code generated so so in case of a symbolic solver if there's a way to verify that um the result is indeed true then that is very good for us so in case of a difference logic you can ask is it possible to produce a proof that the system is indeed unsatisfiable that there are no Solutions turns out you can actually do that uh the proof of unsatisfiability in this case would be just giving a set of constraints where the if you add up the left hand side it's going to be zero if you add up the right hand side it's going to be a negative number so you get 0 less than or equal to minus one which is contradiction and the solver can just tell you that oh these are the constraints that would add up to zero on the left and a negative number on the right and this can be verified by a whatever tool that is going to use the result of the solver but in general you can you can get proofs for a lot of symbolic um logic um I mean you can get proofs from a lot of symbolic solvers but it's not always possible um I mean maybe the example was like very simple like who cares about this three if branches but let's look at the more real world example where this could be actually useful um a lot of times users would like to add their own uh text before compiler checks so here is an array that uses read from the ra and you read from one of the index um and um and the user want to check if the the index is going to be greater than equal to or rate of length which means there is an auto back bounce access and the user want to like reward uh by um this customer uh however the compiler will automatically do this check when I mean in this code so doing this is doing this check manually is actually like wasting gas and it's redundant uh but this is like a good pattern like so sometimes users want to like reward with their own error messages but we can actually use different logic to see that the second um the second like constant I mean once you get out of this uh if Brands you can add the constraint that the index is less than less than array of length because this branch is always terminating like if you get into this Branch it's going to revert but in general it can also be a branch where it's always going to return um so we can actually add this extra constraint so and if we have this check once again you can actually prove that this uh check is going to be unreachable and then you can optimize out those brands so how do we improve um yeah how do we where do we go from the difference logic so we could only encode less than greater than an e0 but once we graduate from the constraints of the from X less than or equal to y less than or equal to k Sorry x minus k x minus y less than equal to K we can't think about generalizing this so one generalization would be constraints of the form a1x plus a to X until a and x n less than or equal to B um where a i and a i and B are constants and x i is a symbolic uh variable in integers uh we can actually solve that using what's called like linear programs and the Simplex method and once you can do that you can encode addition and subtractions because addition would be like X1 plus X2 and subtraction would be X1 minus X2 they would satisfy that that like form um but there are some nuances here because um addition and evm has a wrapping Behavior so you have to have some kind of branching to deal with this model though um modulo but it's doable um we can also encode multiplication and division uh similarly where one of the one of them has to be a constant and the other one can be symbolic in case of division the the second one has to be uh constant and get a case of multiplication you can anything one of the variable has to be symbolic and the other one constant yeah I think that that's it from uh for my talk okay thank you Harry um we have some time for questions so please raise your hand yeah I see a hand over there um in the back the volunteer is coming to you with a mic one second oh and another one there nice oh and another one there yeah hey um I'm wondering how do you know as a solidity developer when your checks are redundant and what can you do to inspect that yeah I think the only way to do that would be perhaps um do both the tests I mean write both of them and see if they have the same gas or check the um yeah it's like the Intermediate representation perhaps or the assembly and make a diff I mean none of the things I mentioned here are implemented so far there are some branches these are just mostly experimental features sorry what yeah but it's almost cool maybe just to use it as a segue which parts are already implemented and what else might we see in the future um so we have an experimental solver that uses an smt Checker um an experimental optimization States already it's called the reasoning based simplifier it uses the full power of an smt solver uh but it's it's uh it's disabled by default in the compiler but you can probably enable it if you specify the optimization sequence that includes uh this one step um so you can still get some of this if you add an extra um an extra like um letter to the optimization sequence but the rest of so the wall point of the talk is you can build a very simple logic that can do a small subset of computations this is not done uh this is not in the the master yet um but most of the SSA transform it's already there no idea I I thought I saw a third question somewhere please raise your hand if you have one if not a big round of applause for Harry thank you we got one more one more question one more okay one more last one um so you said we should be uh doing this optimization on your rather than uh evm by code or solidity or or it's more conducive um and I'm wondering uh it seems like people would use this sort of uh use this sort of analysis to potentially like look at Mev constraints um and I'm wondering how you could apply this to on-chain by code or uh if that's infeasible yeah you can probably like um decompile evm into maybe some Yule and try to do similar analysis on it there is this tool that's getting built by Leo and some other people called Yules you can probably check the status of that can do some of this uh symbolic computation it doesn't I mean it uses like the full power of SMD solvers that no but you can you can probably check it out so once you have a translator from evm to you you can apply these tools or you can use scvm which works on evm by code directly another possibility awesome thank you so much Hari I hope your brains got kind of preheated throughout the last two talks because now we are going even deeper and we can I guess already use the time while I announce the next speaker to set up because this will be more of a workshop setup Zina maybe you want to come up on the stage already to set your infrastructure up the next speaker we will have here is Xena he's part of the go ethereum team and in the team has focused days mostly on tracing the Json RPC and the graphql apis and today he'll be talking about evm tracing and Geth um yeah brace yourselves for a very detailed download on basic tracing commonly faced problems as well as an introduction to the more recently shipped features and how to write efficient tracers welcome Xena big Applause [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] con I hope that's fine um some of you might know that I'm involved in the continuous Defcon which is basically this idea to make the Defcon experience more holistic more Community empowered and Community Driven and generally a really nice event and experience for all that includes the community hubs on the first floor but also the hacker basement and also the fact that the event venue is open until 11 pm and I thought I could use the time until the guys are set up to show some of the events that are happening tonight that are yeah organized and driven by the community um first of all we have the let me check by the way a quick tip for the event app so if you go to app.devcon.org you can of course you just get your in your browser as well but it also is a progressive web app which means you can download it by adding it to your home screen the only issue with that is it doesn't automatically update so it might make sense to sometimes delete it and edit again to get the actual content if something in the schedule has been changed or added so yeah I recommend doing that maybe once per day or so or checking the website on the agenda on the website so uh yeah tonight we have actually an interesting mix of Music stuff happening and also some workshops um so at 6 PM in the chiva Lounge that's this area with Achieva bus which is outside but with tents we have Tomo zaito DJ session then also at 6 PM in the ethereum jungle that's on the second floor of this decompression room we have a session by doing good a meditation and public goods Workshop then in the hacker basement at 6 PM we today have a live coding session which I think is is pretty cool it's a mix between art and coding and music and yeah you should really check it out and at 5 PM we have the token engineering approach to building communities and membership also in the hacker basement so if you don't know what to do at 5 PM after this Workshop head down to the hacker basement and as per usual from six to eight pm today we have a happy hour again which means we will have some drinks beer and also a typical Colombian drink which I recommend you guys trying out a hot drink with alcohol sweet sugar cane syrup and some Anis yeah it tastes really good um and last but not least in the workshop rooms at 6 30 we have the web 3 developer relations Gathering so if you're into developer relations check that out and then for the rest of the evening we have more um Yama music throughout the evening in the chiva Lounge and we have the attractor LP migrants performance in the hacker basement at 9 00 PM so if you're not The Party Guy check out the hacker basement there will be definitely some something going on in the evenings as well are we now good to go I think so okay so awesome in case you forget what what you were here for it's Xena from the guest team who will now give a workshop on evm tracing in go ethereum uh okay is it on yeah hey uh everyone I'm Cena sorry for the it's not a presentation if there's no technical failure so um and unfortunately like no this is wrong sorry um yeah this is a this is gonna be a workshop so I will be working with the terminal I will show you some code and show you how to use gas uh like I guess tracing feature that's why we specifically needed my laptop so yeah sorry uh for for having to wait um also yeah because I will be showing code um it would be great if you guys could come forward if you want to see what's happening because I imagine the phones will be small uh so yeah I'm gonna give you a bit of time to come forward and while you're doing that I want to talk a bit something a bit personal uh so I come from Iran and uh I don't know if you heard but there's some stuff happening there I just wanted to give a quick shout out to the people who are going to the streets there they're protesting for their civil liberties for freedom of speech um so yeah there's been three weeks of protest uh now uh internet has been disconnected um and it all started when the girl on the left who is 22 years old uh was in custody because of showing a bit of hair and she somehow happened to die in custody and this outraged people who went on the streets and during this protest close to 200 people have died among them Nika who was 16 years old and by like when running away from the police she also died so yeah just want to give a quick shout out to them they're risking their lives asking for their rights and for for freedom but this is a technical topic and I don't want to waste your time anymore so let's get straight to it foreign so what is uh tracing what would you need tracing um I just want to show you an example of this is etherscan you everybody knows etherscan right and the internal transaction section of etherscan is basically a trace of that transaction it shows you what happened within that transaction every call that happened there who called who and and so on and this was a this was actually probably not from a guest note but I'm guessing from open ethereum or Aragon but we have a very we have a similar feature to this and this is not the only use case um there's many of them just to name a few like I just just before coming here I was talking to somebody who's developing a zkevm who are also heavily dependent on the tracing feature of gas I've seen uh talks about from me researchers who are using tracing features and many more yeah this is also another tab in the etherscan uh the details where it shows you the the state like the state difference uh we also have as you will see later this is also something that you can get from get all right so before we get to in actually we get into tracing um you know that you can use get to execute a call on top of a given state so you probably know East call you give it a set of parameters like as if it would be as if you want to send a transaction to the network but this is actually not sent to the network it's just executed locally and you see the result uh it's a simulation so uh I actually you have to give me a bit of time because I had to restart my laptop and I lost all my setup I have to bring up the terminal because I wanted to show you how this actually goes foreign sorry for some reason my laptop is misbehaving I want yeah no I want to open the terminal but it's not working do you know search okay I'm just gonna use this okay so I just want to run an instant of gas it's kind of annoying that I have to oh let me increase the font size also while I'm at edit can you guys see yeah okay that's good so yeah I have a I have a girly note on my machine that is a kind of thing to uh to the network uh okay yeah is it behind me yeah maybe I move it here all right so um let's do a call wait I have them all stored in a file so I don't have to type it out because yeah that that research really set me back I'm sorry okay yeah I also copied the line number sorry I will capital v y oh yeah yeah true yeah let's let me mirror the screen that's much easier if my my my neck is breaking here uh foreign thank you fancy for the tip all right so now we got it um what was the command you said foreign no this is this is yeah okay we got it there we go yeah okay foreign foreign I'm losing too much on time on this so I'm just gonna go I already lost a lot of time and you all know how eight call goes okay let's let's move on to the next one um I'm just gonna explain to you so basically uh this uh I'm standing um I'm spending like I'm simulating a call to the wrapped East contract on Gurley to get the balance of of an account of account zero right uh this will only give you the result like in this case the result is as zero like it shouldn't be I have to figure this out like I think I I have the wrong address here um like this address the the zero address should have some balance but anyway what I wanted to show is that when you when you uh do a scroll you only get the execution result what is returned from the contract back but this is oftentimes not enough information we want usually more information we want to see what happened inside the transaction right and that you can do by doing the same thing but using a debug method Trace call which will simulate the call for you and trace it at the same time I would give you more information so if I were to run this now wait I have to see what where this address is wrong yeah no this is not the raft ether I think this is the mainnet contract I was using um I was going to present with mainnet node but then I found out the Wi-Fi here is not so good so I can SSH to my server foreign here we can already see that there is a lot more information as it basically shows you all the steps that happen during the execution of this transaction now my TMax is not working so um yeah there is going to be a lot of steps uh I'm not going to go to show you all of them but basically you you will see okay like what is the first op code what is the programming Contour at that point which opcode was executed how much gas do we have at this point um like what is the storage at this point and so on so it will give you almost the the full information that the evm has uh when it runs through the transaction no I mean yeah you can ask questions sir this stack is before the instruction yeah so all of these information are from before we execute the opcode um here I showed you in this example I showed you like tracing a call of my own craft like basically I said Okay I want to like do from to input and so on um but of course guest supports tracing existing transactions the one that were already mined on the Chain um so historical transactions so to say those you can do via these endpoints you have a debug Trace transaction that uh traces a historical transaction you just give it a TX hash you can also do it at the Block level you can trace a block by using twice block by number or hash or you can do even a whole range of blocks by using a trace chain but Trace chain mind you is a bit different it's not a simple like it's not using the simple Json RPC request you have to it's a bit based on the subscription API so you have to use web socket for it so the usage of it is a bit different I'm not going to show you here but yeah it's cool for when you need to run over a range of blocks yeah okay so what what we saw in in the previous Trace was the default Tracer it's basically the opcode Tracer like it shows you every step but sometimes you need the etheroscan screenshot you want to see uh what calls happen or like you want to get different information and get has a bunch of built-in tracers that you can just call them by name and and get the information you want so here like in this section I want to show you uh three of them the first one being the one that we saw I'm just gonna like here in this table you will see all the information that you will get from the opcode Tracer um which is yeah I'm going to give you a second if you want to see what's in there but yeah basically you get the optical information all the gas related information like how much this up code cost to execute or will cost to execute how much gas there's left the the whole memory snapshot uh of the transaction uh the the stock return data of the last call like when a call finishes in The Next Step you will get the return data for it uh storages for the storage slots for the contract the depth of the execution refund and if there was any error but please note that if you want to use this Tracer there are some things to note as I said memory stuck as storage and return data these are Dynamic values and they can grow large especially uh memory you have to watch out for memory because uh mainnet transaction these days are very heavy uh so like if you have memory enabled it can kill the node basically like yeah somebody for one of our users reported that they tried to trace a Blog with uh on a server with a 64 gigabytes of memory and it crashed basically but because of this um the the Tracer accepts options to disable all these features so all of the ones that you will see in this list they can be disabled some of them are enabled by default some are not but they're all like you can toggle all of them also if you if you really need to trace uh uh you you need a memory let's say and Json RPC is not sufficient for it and you control the node then there's an endpoint a standard Trace block to file which will trace the block and save it on a file alongside the node that you can uh yeah you can easily access so it's not like a Json obviously because uh the next one is the call Tracer this is arguably one of the more like most interesting tracers uh it will basically give you all the call information like all the internal calls that happened uh during the execution uh I'm gonna show you in in the previous example like like uh basically when we call wrapped ether balance of uh what information we will get in return oh so it's kind of down here so you can see um that balance off is a simple call it's not going to call anything else and there's there's one call it shows you who is the sender uh who is the recipient who's the contract what is it call or is it create uh was there any value transfer and what was the call data what was the input but yeah this is a more like boring example we can um I hope these transaction hashes are okay I feel like the file wasn't saved let's try yeah okay it works um so I'm just going to run it show you guys uh so here I'm I'm trying like I'm not doing Trace call but I'm using a historical transaction that was mined on Gurley so I'm using Trace transaction endpoint and this is the transaction hash and here like as an option you can specify uh is this working you can specify what which Tracer you want to use so if you don't provide anything then it will default to the opcode Tracer but you can say I want to use the call Tracer as we did here and you will get a bunch of information foreign yeah so this is this is going to be the main call and notice that we can already see like this call uh errored like there it reverted we can also see from since recently we can see the revert reason which is system time outdated this is the solidity uh revert error that was returned the input output and then here from here we have the nestled calls so basically the the the main contract did some uh called another one that we can see here which which also reverted and like up there you can see like there is um there's more nested calls but that it's not being displayed here but you can you can get that information also so yeah call Tracer you would use when you need information about the internal calls uh it also since recently it accepts a Nifty option so I'm gonna provide here like a you can you can basically configure the tracers you can say like I want only top call um this is this is just useful if you need only the top call information like only if you want to know okay why did this transaction revert then I don't need to see all the nestled call information I just get the top call and I check the revert reason and I know okay there was a time outdated or something foreign on to the next um this one is the pre-state Tracer um it has two modes of execution by default when you just you just say I want to like trace this with the preset Tracer then it will give you all the accounts that are needed to execute the call or the transaction um you can think of it as something similar to access list or Witnesses so if you somehow need to simulate a transaction and don't store the whole state you can get the state via the Free State tracer and then you can execute your transaction locally and in in the second mode you can see all the state state modifications that happen during during the transaction basically you can say you can see okay the balance of this account increased to such value the the storage slots changed such and such um uh yeah so if if we were to see it in see it live it's this one so here I'm um I need to change the address I'm going to execute the same wrapped ether balance of call and pass the to see the pre-state and yeah as you can see like the zero address has like shows the balance of the zero address this is the the contract itself has a balance the code uh nouns and like the all the storage slots that were needed to execute these transactions these are not all the story stalls of this contract but only those that were required here and let's say you wanted to see the the div you just pass the parameter div mode and yeah basically we can see like the biggest change here is that the non of so you have like a pre and post State and you you have to compare them so you can see here like we have the zero address the nonce is emitted because it's zero so nouns was zero and now it's one so this is the only uh the only change that happened in the state during this transaction but of course if we were to run it with a more complicated transaction you would see yeah you see like there's going to be much more stuff this is and the div yeah and this is what parts of the state had changed during this particular transaction like here we had also balance updates both changed and balance of zero account change and yeah here we have like um the the format works as follows basically whenever something is created let's say a contract is created it will not show up in the pre-object it will only show in the post object and whenever something is deleted it will only show up in the pre-object but not in the post object so by by comparing these two objects you can you can see um how the how the state was modified okay back to the slides uh so now I want to talk a bit about how gas stores stage because it is very relevant when you're doing tracing like if you've been been tracing you've probably saw this error required historical State unavailable specifically when you want to trace a historical transaction one that was mined uh now now why is this so first of all how do we prepare the state uh for simulating a transaction or re-running a transaction it's basically like you have to we have to find okay the transaction in Block n so we need we will fetch the state for Block n minus one the pro state of block n minus 1 and start executing all of the transactions within the target block until we reach the transaction that we want at this point we have the transaction we have the state uh the pre-state basically for our transaction and we can we can execute it but what happens if the state for this block n minus 1 is not available in the database that basically that's hint that's what when you get this error so what uh so now comes the question the state would be which blocks are actually persisted um let's get the archive note out of the picture because it's the easiest so for archive node you have all the states next one is full sync by forcing I specifically mean that you start executing all of the blocks from Genesis up until the head here it's worth to note that always uh gas stores the latest 128 blocks the state for them in memory so you always have like while your node is thinking you always have the state for the last 128 blocks now anything beyond that um is stored on like is persisted to this only periodically and that is roughly every two hours so every two hours um the state for a block will be persisted to disk and because we think from Genesis that means we have the state of a block roughly every two hours from Genesis on until the more recent ones and the difference between forcing and snap sync is that in synapse you won't actually start you won't actually execute the transaction you want to actually execute the blocks from the beginning but from some pivot point so let's say here is the point when you start like you fetch the state from the from the network and you start executing transactions so that's why we only have the checkpoints from this point on and like so we basically we cannot execute any transaction that that happened before we we did the snap sync so it's important to know like based on your use case um which think mode you should go for and now we can complete the picture again so how do we prepare the state for for executing a transaction we fetch the state of the parent block from the database if it's not available what do we do guests will go back in the blocks for for a number of logs and checks if it has the state available for any of them uh in on disk and if so then it will basically re-execute all of those blocks and prepare the state for your transaction so here you can see in the error we have we have the error and we have some more information like re-exec equals 128. this means that guests tried the 100 went 128 blocks uh backwards to find some State on disk that it could use to to go forward but it didn't find any and this is a this is a um this is a parameter that you can give so basically when when you do Trace transaction you can provide this re-exact parameter and say okay I'm willing to go back 2000 or 2 million blocks to find a state that I can use to uh to execute uh this this transaction but of course the more you're willing to go the more you go back the longer it's going to take because all of those blocks have to be re-executed to to compute this state and we have uh there's a there's a method that you can use to see States for which blocks are actually stored on disk and that's debug gets accessible state so let's say like I want to see um I want to see like five five thousand blocks behind the head do I have any any state there Computing no State found so I don't have anything between the last five thousand and four thousand blocks okay I I can try again or I can enlarge in this the search parameters it's probably also going to say no oh actually no it found it so here it says like I have there's this block that I have the state memory area between address 0 and address zero developer developers this is just a straight one wonderful 25 in our list of operation of uh message that value and making sure like you can't send ether to a non-payable function but that is not really relevant to us because our function isn't payable so we're going to go ahead and skip that um great so uh on 25 we're going to push 4 under this deck once again where's this four coming from and then 26 we're going to encounter our first blockchain specific uh opcode and what we're doing here is we're pushing the size of the input data onto the stack and as you can see we pushed four onto the stack now we have four on the stack again so that means our imp so this is awesome we have everything that we need we have eight we have zero all we have to do now is save this and we're done right but wait a second we see that in our transaction we duplicated we ran dupe2 so we duplicated the second from the top word of the stack okay then we swapped the first and second words on the stack okay uh interesting and then all right great so now we ran s store so s store store in storage um we're saving eight to the zero with storage slot that's what we initially wanted to do but then we also have a pop after that because we have this extra eight on here and like what is this eight doing why why is it here uh this doesn't make a lot of sense right like why are we duplicating and swapping here uh why are we popping at the end uh the solidity is doing extra memory management that we don't want right because we want to be as gas efficient as possible and be as um uh yeah be as gas efficient as possible so we should jump down into Yule and try to optimize this code right uh well uh let's talk about gas optimization uh using yule um if you're not familiar Yule is simply a language that we use to write assembly code in solidity it's not exactly like writing op codes not exactly like writing evm assembly it has for Loops if and switch statements and it also disallows some commands uh like jump statements because they quickly become very very difficult to reason about um and yeah all right so we added you we have this optimized total Supply function it's awesome uh we have an s store with a zero and an eight so we're saving eight to the zeroth slot this is great this is going to be way better right uh go ahead and hello let's go ahead and measure our um our contract so once again or let's go ahead and look at the op code representation of our contract first so once again uh we take a look at the op codes and we see that in fact optimize set total Supply does have less opcodes than subtotal Supply this is fantastic we know that op codes cost gas we see that uh set total Supply and optimize set total Supply have the same op codes except set total supply has more uh we just like saved a lot of time and money right but let's like measure it to be extra extra sure so here uh we uh this is I think hard hat gas reporter and We've ran the set total Supply function 100 times and we see that the average gas cost is 22 599. cool and then hey now we ran optimize set total Supply and we see that the gas cost is 23 591. awesome eight gas we have just saved our users like an enormous amount of money right well uh not really I kind of rubbed you guys here um so if you notice Optimizer and enabled is equal to false if you're not familiar with Optimizer it's something that you can use in a hard hat and other tools when you're deploying smart contracts so let's go ahead and enable our Optimizer in hard hat rule set runs to 200 enable it and re-measure and in fact when we have the optimizer enabled it looks like the two functions cost approximately the same amount of gas right so all our time spent studying you will and learning it and learning about a store was wasted right well uh I wouldn't argue that exactly but I will say that optimizing smart contracts is hard and chances are that you're not going to do a better job than the compiler unless you really know what you're doing contrast containing assembly are generally harder to reason about in order to audit than contracts written in solidity and Viper so what you might gain in gas optimization you will probably be making a trade-off in a contract or user security and the other thing that's important is if you're writing your own assembly code always measure and make sure that your implementation is better than the compilers because chances are there are some very very very smart people working on the compilers and optimizers that know something that you don't when it comes to memory management or safety um and I guess that's essentially my last Point remember a lot of the memory management stuff solidity does under the hood is there for safety reasons and just because an OP code looks like it's unnecessary it doesn't mean that it actually is that being said optimizing in Yule and an assembly is definitely something that uh is needed and useful especially in D5 and on mainnet when our gas costs are you know lower now but quite higher than a lot of the other chains um but yeah I guess do so at your own risk so uh thank you guys once again my name is Alex I'm a lead developer at tally ho and we are hiring solidity and typescript developers so if this kind of stuff is interesting to you and you're looking for a change uh please reach out and then finally I want to give a big thanks to Gilbert Garza he had a lecture at Xerox macro that inspired this talk and there are a few research resources on here if you would like to dig deeper on any of the topics uh covered today thanks so much [Applause] thank you um we would have some time for questions if you like uh sure just offering in case there are questions yes there are some so yeah let's take some questions thanks for the talk why the optimizing function spend the same amount of gas if it has more uh sorry less up codes so the app codes that we were looking at were for the uh for the unoptimized functions and we were looking at them because it is like actually it's easier and possible to dissect them in a talk so when we saw those two uh op code uh I guess of like optimized what was it set total Supply and set total Supply those are the unoptimized out codes and then and then when uh after the optimizer runs the opcode representation of those two functions actually becomes the same so what those two up codes we are doing on the unoptimized function I'm sorry could you repeat your question what were the those two up codes two extra upcomes doing the swaps we're doing on the unoptimized uh compiled version so the the optimize compile version um was a effectively the same as our op codes um secure I can show you so our C so here we have uh our unoptimized set total Supply and our optimized set total Supply in our code where the optimizer has not run before after the optimizer runs uh both of these functions have this have the same exact op code and in fact even though we have two functions in our contract the optimizer is smart enough to know that they're doing basically the same thing so the when we were talking earlier about uh these selectors over here right um so this selector jumps to a destination and then there would be a different function selector for another function um for for the other function they would actually both jump to the same destination because effectively what they are doing is the same uh what would be some good use cases for managing the assembly code within contracts and do you recommend any good resources for learning you'll yeah so I think that the the best use case that I have seen is a fairly common one where uh if you're adding two numbers that you are 100 sure will not overflow the uh the integer limit then in assembly you can uh you can add them with a uh there's an unchecked flag which basically tells the uh the compiler to not check for uh integer overflow I believe that in solidity nine point something in some fairly recent version of solidity uh uh safe math became I I guess like so uh so there are a lot of checks that run under the hood you can turn off those checks if you're sure that you don't need them but and that'll save gas but once again that can be very dangerous and as far as uh resources for learning UL I think just like reading smart contracts and trying to figure out what's going on there is the best way to do it there are unfortunately no good resources to you or to learn you will or really evm assembly and which is that I want to just talk just like yeah sure um just to like follow up on that have you heard of um trim yeah yeah okay so my question is this right like say for instance if I'm writing trim and I'm basically you know trans transpiling my smart contract into into trim like um what are the I guess like the pros and cons to that as opposed to doing it um you know your style like within solidity and then don't use Yule unfortunately I can't uh I can't answer that question because I'm not familiar enough with trim I do believe that uh Gilbert was the the person that I mentioned in my talk was the person that developed trim um or one of the developers and I would encourage you to reach out to him on Twitter but I'm not familiar enough with trim to answer that hey thanks for the talk um I saw somebody asked about you today there was an awesome Workshop about that it's like two hours long you should watch that and then uh I had a question for Optimizer runs what number do you recommend what number um I I just choose 200 because it's standard I know there's I know that there is a we're just not standard it's just like standard in what I've written I know that there is a there's eventually a trade-off size between uh contract size and and gas efficiency that you get if you set your Optimizer to like I don't know 500 000 a million um or maybe less but I generally go with 200. um I don't work in the D5 space so I'm not like too concerned with ultra Ultra Ultra optimization but 200 seems to be a good number for what I'm seeing if that was it thank you so much thank you guys [Applause] I actually made the first ever yield stickers that were available today at the solidity impact Booth if you are very quick maybe there's some left but I I'm not sure how many they are even glow in the dark um but yeah moving on to our next talk um and actually also the last talk already for today um we have Alexandra he is a data scientist and open source Advocate and a fun fact I really liked about his speaker bios that he got into Bitcoin in 2013 and actually launched the first Canadian Bitcoin payments processor but now he's into building high performance Data Solutions in the space and in this session he will introduce the graph substreams for high performance indexing so please give a warm welcome to him foreign there oh my gosh are we already started half a second there no name okay I'll skip this part we'll see where we go after it foreign hey hi my name is Alexander I'm CTO at streaming fast and I'm also a Pianist a data scientist whatever that means I'm a father of eight beautiful children two of whom are there I love designing and crafting software which I've done since I was 12 and I'm here today because one day in 2013 I read the Bitcoin white paper and that changed the trajectory of my life and fast forward to today streaming faster company based in Montreal Canada is now one of the core developers of the graph and we joined the graph a bit more than a year ago in a bizarre m a 2.0 fashion or lawyers still don't understand what happened but anyway we said thanks and goodbye to our VCS and shifted our Focus to make the graph the greatest data platform on Earth so today I'm here to introduce substreams which is a powerful new paralyzed engine to process blockchain data and before I can do that I just want to set a bit of context perhaps you can raise your hand if you know what sub graph are raise your hand if you know you're good okay so slip grass can be thought of an ETL process right extract transform and load and subgraphs add that little Q there that graphql layer to it and some grass today provide that sort of simple approachable end-to-end solution to blockchain indexing and the graph node is responsible for all of these components right the extraction is done through hitting Json RPC nodes and then transformation you provide some assembly script you guys know that composite wasm running in a distributed environment and then you have the load aspect which the graph node does puts that into postgres and offers you a rich you know and beautiful graphql interface on top and one of the reason one of the reasons we were brought in was that so we could push the graph to New Height in terms of performances so to do that we brought first thing the fire hose something at the extraction layer to our take to boosting performance by one two three orders of magnitude the first layer extraction it's the method of extracting data from blockchain nodes imagine prying an egg open where the data is exfiltrated as fast as possible and all the juicy data gets then thrown in a grpc stream as well as into flat files and you can think of that as sort of a the bin log replication stream for blockchains where you'd find in a Master Slave replication engine like in databases so we'll get back to fire hose in a minute then substreams is sort of reasoning of this rethinking of the second box the transformation layer here instead of the traditional sub graph handlers and assembly script you will write substreams modules in Rust and those can be executed in real time as well as in parallel with unprecedented performance so let me give you first a primer on fire hose because there's a lot of benefits of substreams that come directly from the fire hose so a streaming fast for many years we've been thinking hard about all these indexing problems from first principles and we needed at first a robust extraction layer we wanted something that is that was extremely low latency something that would push data out the moment the transaction was executed within a block within a blockchain node Json obviously was not going to cut it and we didn't want to have to deal with those large bulky nodes right hang on a thread occupied to with managing High write throughput we have kept everything in a key Value Store behind a you know a Json RPC request and it was really heavy in Ram and CPU and you needed super optimized ssds this is really annoying and all these things are much costlier than what's needed when our goal was to get to the data inside so we also wanted proper decoupling between the processes producing the data so the blockchain nodes and its intricacies and it's it's request response model and they're all different and the data itself wanted the data to be the interface and we wanted something also extremely reliable in the sense that we could avoid hitting load balanced nodes that had all sorts of different views of the world and that we need to have like client code to like latency inducing code to to resolve what's happening there if there's another Fort you need to query nodes again and and you know for a reorganization heuristics for example but also you know we wanted something better than even the websocket streams that pretend to be sort of you know linear uh that the nodes have implemented because when they would send you a signal that let's say this block was removed it can leave you hang if if you happen to be disconnected for just half a second you'd reconnect you'd miss the signal so the reliability was not built in so we wanted something to address that and above all we wanted something that is able to process networks in 20 minutes well okay an hour or two but you know we're never three weeks or things were waiting linearly and that's still our goal today and when we say Network history I mean executing guests and extracting data executed into flat files that's the extraction layer but also any sort of indexing after the fact we wanted to be able to have massive parallelization like there was no other way to have reliable and durable in performance without parallelization so our solution was the fire hose and the fire hose fell off all of these issues in a radical way we took a radical approach because we wanted to solve those problems definitively like meaning that there would be no further optimization possible except attempting to bend sort of space-time Continuum itself right so with streaming we with even multiple nodes pushing out data multiple nodes are actually racing to push the data the first sort of consuming process gets the first to get out like you can't really add remove more latency there and um there can be nothing faster than immediately when the transaction has just executed from your node and then like regarding the stateful processes and cost flat files flat fast for the win we have a hashtag for that right flat files are the cheapest much cheaper than processes they're easier to work with there's nothing simpler nor cheaper in terms of computing resources these storage facilities have been optimized like crazy and it's also where data science is headed these days and there's one common thing to every blockchain protocol that it processes data data is also the right abstraction for the this technology not an API that's common to all chain data so fire hose clearly delineates responsibilities and and the contract between the extraction and transformation layers is again the data model fire hose creates and for every chain you can imagine the best data model the most complete and that's what we've done for fire for for ethereum for example the the data model for ethereum within fire hose is the richest there is like you have in there the full call tree internal transactions you have the inputs and outputs as raw bytes you have the logs obviously you have the state changes like you see on etherscan down to the internal transaction level you have balance changes the same way with the prior value and the next value so when you're doing like navigation backwards or forward you get you have the data you need you have also gas costs at different places and there's that important notion of total ordering between things happening within the logs State changes and calls all of of these things happening during execution are totally ordered so you get in there everything parity traces would give you and more and everything you would need to build full archive node from Flat files and everything there is goes to the transaction level not rounded at the Block Level which is crucial if you want to index with Precision right it doesn't rounding of blockchain information at the Block Level is sort of a was meant for helping in consensus right but it doesn't mean that what happens mid block is of less value than what happens at the boundaries so okay so that's very interesting and now regarding reliability whoops no not too fast regarding reliability the fire hose grpc stream provides reorg messages like new block or undo this block or this block is now final accompanied by a precious cursor I think that's really key here with each message so if you get disconnected and upon reconnection you give back that cursor you'll continue exactly where you left off and potentially receiving that undo signal that you would not have seen were you disconnected right so you will get it so with the guarantees or linearity of the stream so no websocket implementation would do that because it doesn't make sense for a single node to track all the force possible even two days after the fact and undo messages come with full payloads so you get all the Delta so you can just turn around to your database and apply the reverse or you know pluck again in the the full payload of what happened in the block and decide what to do is so it doesn't pose on the reader to store what happened like at that previous block if the signal just remove block 7000 right okay and when you commit that cursor to your transfer database well you get through that you know finally some consistency guarantees within your you know your your back end so some of our users told us they could cut ninety percent nine zero of their code reading the chain because they were relying on some that reliable stream and okay and it also lays lays down the foundation for massively paralyzed operation files plus the stream and so this is the future of the graph unbeatable performance and it's core to our multi-chain strategy because you know any blockchain can have that data model uh now let's dig into sub streams substreams is a powerful clustered engine to process blockchain data it's a streaming first engine and it's powered by the fire hose underneath and its data models of the chain so let's dig in here a few quick facts it's invoked as a single J grpc call and within the request we provide all the transformation code like you'll have in there oh it's too low you'll have the code some wisnom modules relationships within the modules and uh you know all the transaction the Transformations within the request it's not a long running process except if you run it for long it's not a service you you spin up right and the backing nodes are stateless which provide nice scalability properties modules for Transformations are written in Rust they compile to was them and they're running a secure sandbox on on the infrastructure they're similar to the subgraphs and the ultimate data source being the blockchain data being deterministic all the transformation outputs are also deterministic and the request if the request you send involves process prior history even if it's 15 million blocks well the substream's runtime will then turn around and orchestrate the execution of a multitude of smaller jobs in parallel fuse the results on the Fly for you and aggregate the results to simulate a linear execution so you would see a dime a difference and all the results are streamed back to you as fast as possible with same guarantees provided by the fire hose with a block per block cursor and a transparent handoff from batching and always historical processing to the real-time low latency rewards aware stream of the head of the chain so let me show you if you're interested how we create one of these things raise your hand if you're curious okay you're good okay so let's start we start with the manifest like that do you see that down here you can move the podium I can't so there's package information you know some metadata there you have pointers to the Proto buff that you'll use again contracts between modules are about data so they're protobuf models similar to the protobuf models of the root chain of the chains the layer ones and you have pointers to the binary that you're working on your drive and all that and you have Imports and imports are actually very interesting because you can import third-party substreams packages and these yaml can be packaged and so you can import from someone else's package you can write your own or combine both that means substreams enables composition at transformation time which I think is pretty unique in a pretty game changer and then follow up and there you have the module sections which defines the relation between the different modules and you see it defines the directed a cyclic graph you have modules that slowly refine the data and so there's two types of modules one the mapper the first up there map pools and this one takes input does transformation and outputs it's parallelizable down to its core block wise so massively paralyzable and then there's the store input I think it's awesome this one takes any inputs and outputs a key value store that also have an accumulated in a stateful way and stores can then be queried by Downstream modules and uh okay so we'll see a bit more after then the name corresponds to the function in the waslam code and the inputs can be of a few things either the raw fire hose feed so for example The Source here that means the block with all transactions you know for for that particular block and it can be the output of another module like you see down here the input of map map pools so you'll get the data as bytes and it costs will be be a store which would be a reference we'll see in the next slide there and on the store pools here you see there's an update policy which sets constraints on what you can do with the store and it defines a merged strategy for when you're running paralyzed operation like I'll get to that I'll do a little later and the value type field will help anyone decoding understand what bytes there is in that store so you're going to UI you can jsonify them in your code consuming can automatically you know decode them with protobuf all languages supported otherwise the key value is just Keys as strings bytes values very simple and one thing to note here is that because it has deterministic input it's possible to Hash a module like the kind and all of its inputs and the pointers to its parent and including the initial block so you have a fully determined and hashable let's say cash location for all of the history similar to git right all of the history of data produced by by then you'd hash also the wasm code right so it makes it for an extremely cachable system and highly shareable and cross-verifiable output of modules which opens really interesting possibilities you know for uh collaboration within the graph ecosystem and imagine that one has large disk another one has large CPUs or you know sleeping CPUs they could pull resources together to build something bigger than themselves okay and see the relation there so this gets piped to that and if we add another module here you see how the graph comes together this one computes the price this is unit swap V3 thing uh it computes the price but it you want to get them for certain pools because maybe you want to use the decimal placements in the pool we'll see a little bit more there and when you're running let's say you're running that at block 15 million well you're guaranteed the runtime guarantees that the store you'll have to execute code at block 50 million will have been synced linearly or in parallel but you wouldn't know but it'll give you a full in-memory store eventually backed by some disc but whatever and you can query the key value store at each block it's guaranteed to be synced for you uh that's exciting no okay uh so you see the dags fully being built huh right so the dependency so now let's let's this leads us to composability see each color here means a different author and modules written by different people ideally the most competent for each right like we would hope they would crop you know analyze what's on chain and refine the data and Abstract it to new heights and the contract between the handoffs is always data it's the model of data so you take a modulus bytes in bytes out and so you see here we can get the prices from unusual V2 and price this produced by B3 and Sushi and chain link and whatever and have someone reality a module that takes this input at transformation time and then averages them out and whatnot right and then that you'd have like sort of one beautiful Universal price module that you can then hook on top and feed to some who knows maybe if someone feeds that back onto the chain for some reason and then soon enough you know all of that while someone wants to build on top of it and ah something like that if someone wants to compute you know the USD denominated volumes aggregation of nft sales on openc you know you'll take some sales you merge it with a price and we see here that little Trader Inc he sees he maybe he wants to feed that into his trading bot because it's a streaming engine we're not storing that in the database yet right but this begs the question where is that all that beautiful data land where does it get piped that's where it thinks head up like substream being limited to the transformation stage of the ETL analogy remember it doesn't really care where you load it and that could be anywhere these are just a few examples you can load that in databases we already have a sync for postgres and you hook to sub streams and it just loads it into postgres with a data model that we've agreed upon right if you write it in a certain way it just thinks over there and or message cues or whatever you know data lakes or some Bots or some trading algorithm you know so I don't know some whale detector you want to hook directly on this stream or also something I think big for doing some ad hoc data science because you know you have a really fast engine allows you to process the whole history in like it can take a few minutes to process the whole of ethereum image to go and pluck some new insights so you can write your code send it to the network and then you know stream out the results similar to for those who know bigquery you know the cluster the big cluster the service by Google that's what they do send the request it just charted everything they send you back the request well all of a sudden substream's engine can allow you to do some things like that ad hoc and it you can write any program uh that that supports grpc in protobuf which are many and the last one here not the least subgras through graph node well so we're working to make substreams feed directly into sub into graph node to then provide the same loading experience and then querying experience that you've come to know and love and you will be able to deploy a sub graph this time not containing assembly script but a stud streams package with an entry point and it would process the history in parallel and load that in your database in Crazy speeds so stay tuned for that that's not out yet but you know soon okay and so this is a simple example in Python it's not really longer than that you have two one or two dependencies like grpc so that you can use the query so we're leveraging a lot there and you can you know see that spkg there we can use that to code gen python classes and helpers and all of that because it turns out that the Manifest through the spkg there is for those who know protobuf is they file descriptor set it contains all of the things all the protobuf definitions so the spkg also contains all the wasm code the module graph information you know the dependencies the inputs and all that and even some documentation everything is needed is in there so you can pass it down to the you know modules you take it from from the disk and boom you send the request to the server and it's running so you can deploy packages also very easily and consume them very simply this way there's a few Imports we've omitted there but it's simple this is the show okay and let's look at a simplified data model for Union swap V3 and I'll show some code making use of it okay so this here the pool is a list of pool this is actually what gets handed off from you know our mapper which finds the pool that were created down to the store pool which we're going to look also and so it has a list of pools and the pools you can imagine an address and the two tokens that are concerned here and we have a reference to the Token also which is going to be very useful to enrich the data down so we will have the decimals right at hand like we won't need to do much loading it's going to be very very close so we can enrich all these U ins 250 79 000 and you know put the the comma where it belongs um so let's see what happens in the mappers so this is sample rust code raise your hand if you love rust raise your hand if you no rust okay it's very simple here I'm going to go through you have the map tools function corresponding to the Manifest there has one input the block this is the fire hose block with all transactions all logs All State chains you can craft your own triggers in there as you wish but we have a simple version see that that line there blocks events you have a thing that goes through transactions and it's going to trig on pool created and that pool created object in Rust was actually code gen from the Json API so you can just give the instruction and we're going to filter it for only the V3 swap Factory and then that beautiful filter map will give us the log okay and then we'll we'll output we're going to collect some of these things into one list of pools and it's assigned to the pools object there and and notice that little thing here so the RPC create unit swap token thing this actually hits you know an eth call on a node behind similar to what we have in sub graphs and that's actually very important it means that once we've processed this layer once and we've done it for the history it can be cached very efficiently so anyone relying on that thing will never need to reprocess it again you can give the package to someone and they can access the stores that's been cached by other people immediately so you could go to block 15 million and you'll have the list of all pool created that you can query super fast you can depend on it also so I think that's pretty cool and here that's the store modules the store module is pretty simple here it receives C the pulls from the output of the prior module and it does it Loops through the little pools there and calls output set and see the key there is pool colon the address is going to store the Proto buff encoded stuff of the pool with the token decimal for both right and so did I say that to be constrained so the store here is constrained in two ways in order to preserve parallelized ability the stores are right only you cannot read your rights otherwise that would make them potentially cyclic and they expose only the function defined by the update policy in this case it's set so let's see what happens if we run things in parallel so here we have two jobs covering two segments of the chain you know one million block each and to see those ugly arrows there they correspond to a pool created event and so in our code you've seen we would write a key for each of them and uh so in the first partial run we'd have a what we call a partial store with four keys and the next one we have two keys and so when we run the merge operation we would apply the set merge policy which says basically take one store take the other store cycle through keys and the last key wins if you do that you can paralyze endlessly so we'd have here a complete store with six keys and now at that place we have a snapshot we can have periodic snapshots and so if you want to go and explore the chain at any point in time you have a snapshot plus a little partial you can have the state synced at any block height so so this one you know has the last key win policy but you have a few others like min max ad and another one like first key wins so if you merge them you have set if not exists and then that that allows us to build different aggregations right you'd like to see that running live okay I have a few minutes now I need to bring that other window up so do you see that okay that's good enough huh okay so let's imagine we want to see that pool pools creative thing okay do you see that I want to see the output I'm going to run that I hope everything is good you know the demo gods are connecting okay okay whoa not too fast so this is going through starting at the beginning and we have there a pool created event and see that we have everything decoded because it's the protobuf thing we have the thing to decode it we could feed that it arrives on the wire as bytes and properly serialized bytes and then we see that the token address is there we have the decimal we have the address and so what that means is pretty crazy already oh do I see that okay what that means is that you can inspect the chain with your code at any place in time for a mapper especially you can go there and I could run it again here and say and say I want to run the mapper let's say I block I don't know uh something more recent give me a recent block what's the block yesterday fifteen seven million I don't know something like that okay and see is there anything recent so there's some stuff right some things are recent can I see that someone still created a new pool and I can inspect my code to make sure it works where is that come on right this one was wrapped ether and infinity they were just created that address as a new pool so you can go and test their code everywhere what's that's done well you're set right you can go then to the next data dependency so this really changes Dynamics for debugging and this can also work for stores they can you can ask for a store and it's going to process it in parallel and then you can inspect all the keys that exist there or see the Deltas coming through okay and let me show you something running in parallel uh-huh graph out now this is very interesting because oh no let's start it at 15 million let's say I want graph out at 15 million I didn't run it again before and I want so let's run it so this starts a whole bunch of parallel processes and you see up there the number of blocks per second yesterday I had that 8 000 on Solana blocks I had 16 000 depends on the power you put behind there but this all like you said the pool counts as a dependency on the pools the pools is further down so we're able to schedule things and all that just massively parallel and once that's ready let's say everything was done I would start streaming and get all the content and so let me show you the graph out it's very interesting because because graph out graph out has refined the data up to entities that we're talking about database tables and Fields and you get and you get out of that see it do you not see it okay that wait a second here so let's imagine see we have token and an update and you have the field derived if and you have the old value and the new value I don't know if you've seen this thing in the data Science World this looks very much like change change data capture cdcs that can power a lot of large-scale systems and uh and you have the prior and after so you can feed that to your let's say postgres apply the changes and when you have an undo signal it gives you back that payload you then just flip everything and you have guaranteed linearities through your store with a cursor and it's Flawless it's just extremely simple to keep your things in sync you also want to have a slack bot you can have it undo message remove the message if you have a thing coming through right so I think that's pretty cool what do you think okay okay that's cool can I shut that down here do I have another window uh I'm close to them prepare your question please ask ask them succinctly we have a half a minute I just wanted to have a final final note there gosh can I close that I'm so sorry um so as a final note as a final note I wanted to say I want to share with you a little bit of my vision for the graph okay I don't know where the window is so whatever it just says fine it's fine uh I think the graph becoming that sort of huge worldwide cluster of processing and storage capabilities and something like Google's bigquery but where people join because it's better together right instead of running it alone where you need to have all the resources alone and I see also a new era of composability which means more collaboration and in a tighter community of working together more intimately in those in those uh you know with those data contracts and I see also a new mix of collaboration between indexers like exchanging data or sharing resources in terms of compute and storage and whatnot therefore introducing new value flows and also I'm seeing new products new Services being offered directly on the network you know to satisfy some needs that perhaps couldn't be addressed before and I mean there's a place for you in there as a developer as an index there as someone who realizes the radical benefits of such a platform and who who builds on it and promotes it so my ask to you is you go to go try substreams put pressure on your favorite layer ones or that they do integrate the fire hose natively that's pristine's Aptos has done that recently some others start where I think so that makes it everything we've seen today becomes immediately available to them sell them on the goodies so also join our Discord I'd love for all follow-up questions and come see me afterwards I love feedbacks on these sort of things all of this is open source so let's dig and build something of the like together the biggest blockchain platform on Earth thank you for your time today do we have time for two three questions we're the last one so if you have a question okay go ahead hey uh so one question uh modularity and composability of these substreams is super super powerful but still if I look at this compared to SQL and like DBT models right it's a lot more complex so how can we enable people to really kind of learn this and like build these kind of hyper modular data streams so it's a good question but the transformation layer is not the SQL layer like this is powering going through history it's an adult transform with stateful storage but you would pipeline into SQL store to do other things right you have refinement you have Knowledge from the community as to how to analyze this in that protocol ever ever you know increasing refinements but then you might store that in your store with off-chain data and maybe that's best fit for you maybe you fit it into a subgraph that's what you need you had a total decentralized solution and you don't need to host anything so this is enabler at a lower level it's not a it doesn't seek to replace SQL but it puts itself at a place where we can feed all the systems on Earth with enriched data which you would need to do in SQL and it's it's really not fun so you leave that to the community right gotcha thank you my pleasure another question yeah stand up hey uh my question is about when we have uh old subgraph which is working without sorry can you hear me yeah yeah go ahead yeah okay we have our old subgraph which is pretty slow and we would like to transform it to the new type of sub graft yes should I only read some code on Rust and that's it or something else so it is not the same Paradigm to enable parallelization you need to distinguish the data dependencies and that infers the number of stage of parallelization that is needed it's not easy at all actually it's pretty crazy to try to parallelize the subgraphs we try that that's what yield us to design sub streams by cutting you know uni swap stuff so you will want to go and write in Rust modules and it's a different Paradigm so it's not just an easy switch I admit but it brings us to the next stage an evolution you know of blockchain indexing yeah I see thanks I guess then we're wrapping it up if you have more questions I think Alexander will be available for more questions after this um and yeah thank you so much Alexander my pleasure [Applause] and with this we are wrapping the program up for today in this room so thank you so much for joining everybody if you want to stick around feel free to check out the first floor the second floor or the hacker basement and everything will be open until 11 pm so feel free to hang around Network enjoy the happy hour and have a nice evening foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] foreign [Music] 