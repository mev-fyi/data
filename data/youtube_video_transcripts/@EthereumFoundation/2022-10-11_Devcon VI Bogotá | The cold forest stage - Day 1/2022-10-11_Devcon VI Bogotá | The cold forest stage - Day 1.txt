foreign [Music] thank you foreign foreign foreign foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] thank you [Music] thank you foreign [Music] thank you foreign [Music] questions [Music] [Music] thank you [Music] foreign [Music] thank you [Music] thank you [Music] [Music] [Music] [Music] foreign [Music] thank you [Music] thank you [Music] foreign [Music] [Music] thank you [Music] [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] thank you [Music] thank you [Music] stories [Music] [Music] thank you [Music] [Music] thank you [Music] thank you thank you [Music] thank you [Music] foreign [Music] [Music] [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] thank you thank you foreign [Music] foreign [Music] [Music] pictures [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] for this room for today so welcome and come through if you're standing by the entrance because we have a couple of very very nice talks lined up for this afternoon mostly covering developer infrastructure but also later in the day talking about the EP case Zero knowledge of the KPS so yeah there's a lot of cool talks in store but since we are already running a little bit late with our first talk I'm not gonna use too much time talking about what we have in store but Dive Right In and announce our first speaker which is Piper yeah a long time core contributor and core developer to the ethereum protocol today he will talk about the portal Network which is all about lightweight access to the ethereum protocol he will explain what this all is about there has been development going on for I guess five years already so yeah learn what the partner network is about how it works and why it matters for ethereum without further Ado please give a warm welcome to Piper thank you all right you guys can hear me I can see my slides uh good to see all of you today um I'm Piper as she said and I'm here to talk to you guys about the portal Network um so let's just get right into it um the I work for the ethereum foundation I already got introduced um so this is about actually finally bringing lightweight decentralized access to the protocol and uh and yes this has been kind of a a long-term project to get us to where we're at um here we go so let's like dive into kind of at this high level what the portal Network is for me it's a giant white whale that I've been hunting for a long time and hopefully doesn't eat me in the end um but the portal network is uh five new decentralized special purpose storage Networks that serve all of the data that is necessary for interacting with the ethereum protocol and this has been a long road to get to here we spent time way back trying to build lightweight clients on the existing networks and where we ended up was that the existing networks don't give us what we need to actually deliver lightweight protocol access and thus we have these five special purpose storage networks that we are building out to serve this data and to essentially realize this uh this dream of lightweight access to the protocol um the project has kind of these high-level design goals that kind of informed what we needed to build and how it needed to get built and one of the main things is that the portal network is really user focused um all of the clients that you hear about today Lighthouse all of that stuff right it's infrastructure for the protocol and and those clients are built with the protocol in mind and and the user-facing stuff right the Json RPC API they're not the lowest priority but at the end of the day those clients serve the protocol and not the users um one way to look at this is that if you want to interact with the ethereum protocol today you have two choices right it's the are we upper right or lower left hand corner of this graph you can run a full node they're very heavy and they're also awesomely decentralized our Network there's a number of choices for what to run but in general when we're talking execution layer you're talking about very heavy pieces of software we'll get into the details of this in a minute on the other end of the spectrum you've got lightweight but centralized options like Alchemy or inferior things like that somebody at the last conference told me that I was a boomer for calling it inferior I thought that was awesome um so anyways you've got these lightweight options but they're also centralized and they can do things like correlating your IP address with the transaction you send or selling your data and things like that right we there are two options at the far opposite ends of the spectrum and we want to build this thing in the upper left hand corner this kind of adorable pink smart car that is both lightweight and decentralized which supposedly we care about um this brings us to this lightweight concept um like I said ethereum clients are heavy and we want we need a network that allows lightweight devices um participate ethereum clients are heavy today because they have to do a lot of things evm execution is CPU intensive running the transaction pool the CPU intensive and there's gigabytes upon gigabytes of history and state data and things like that that they need to do this means that running a traditional ethereum client is a inherently heavy thing and you generally can't do it on things like Raspberry Pi's or phones um over here on the left we've got this nice little strong guy who can hold it all up that's your like traditional execution layer client our goal is building out a network that lets you spread things out that takes all of the load for all of this data and distributes it around all of the participants of the network in a nice kind of even way um the other thing that we focus on is kind of removing some of these height restrictions and by height restrictions I mean essentially Hardware restrictions that keep you from joining the network this is one of the things that blocked us from Building light a lightweight client years ago was that you've got these sort of you must be this tall to ride things um you are not allowed as a uh as a participant in the the dev P2P Network that's the network that supports execution layer clients you can't be part of this unless you have all of the state and all of the history and enough processing power to process every block and enough processing power to run the transaction in pool right if you aren't tall enough you're not allowed into that Network we focused on a different model we wanted people to to be able people clients computers whatever we needed a network that allowed you to show up with whatever you had and to contribute it to the network if you're so willing the idea is that all of these networks operate on this exact same principle which is that as a client to the network you can tune some parameters that dictate how much storage space the network is going to ask of you that dictate how much processing power the network is going to ask from you and one of the other major things that we focused on is kind of this ux thing which means to me elimination of sync times uh traditional clients have bad ux in terms of like the user-facing stuff and that's because you either have to when you start it up you've got hours or days to wait for it to sync and if you go offline for some period of time you often have additional time to catch back up to the tip of the chain these sync times make ux for kind of like end user interactions kind of basically unbearable this is why almost all of the userland traffic goes through services like Alchemy or inferior we've designed a system where all that you need to do is have a view of what you think what you trust as the front of the chain and from there all of the data is accessible to you which means the difference between hours or days of sync time from from zero to being able to actually internet interact with the network to being able to be interacting with the network in a matter of seconds or at the worst network conditions a minute or two as it has to appear with some some other peers in the network so really focusing on on user-facing things like this uh the other piece that we needed was something that was scalable um and we're not talking this scaling this isn't sharding scaling or transactions per second scaling this is a number of of network participants this is having potentially millions of nodes be part of this network some of the past work towards uh ethereum like clients is Les that's the light ethereum subprotocol and this has been around for five years maybe um and it has never really delivered on this goal and the main reason here is that it exists in this client server architecture Les nodes on the network are dependent on full nodes serving them the data and what happens over time is that a full node who's serving this data ends up getting kind of just assaulted by all of these Les nodes constantly asking them for information and they're expensive requests that these nodes are making um and it hasn't turned out super well um Les has not proven uh has has not delivered a reliable like light protocol access and the main reason is this imbalance between client server stuff um that there's no incentive that there aren't incentives to run Les servers running an Les server just costs you something and so the ones that are out there are being run by the goodness of hobbyists and other people's hearts or people who misconfigured their client on accident or something but either way it it is it is a it costs you something and the clients in this network are not able to contribute back as an Les client you are purely taking from the network and that is just the way the network is designed inferior Alchemy they're this centralized model right their servers go down everything stops working Les is this decentralized model which we like but because there aren't enough incentives or anything for people to run Les servers uh it hasn't worked out super well um we've moved all the way into this distributed model where we have a homogeneous Network where everybody in the network is a client and everybody in the network is a server uh one way to think about this is very akin to BitTorrent where the in Les you have this like kind of degenerative thing where the more nodes you add into the system they take up a limited amount of capacity and once you exceed that it degrades service for everybody in the portal Network context we have built these networks around this idea that the more nodes you throw at it the more powerful it gets and that's kind of the whole core part of this all right um let's look at a practical example of what uh uh how you would serve this um essentially a balance inquiry from the portal Network I'll remind you we've got a number of different networks here and the idea is that they're all sort of special purpose partitioned off from each other clients can be part of any number of them that they want this example is going to touch three of our Networks so what we're going to do here it's a very simple example we're looking up your ether balance in a tradition so this is our traditional client you've got databases over here on the right where they're storing information it's running this Json RPC server a request comes in to query my balance the Json RPC server is going to do a couple of things here it's going to reach into an index to figure out what the client thinks the head of the chain is it's going to look up at the header for that header from its store from whatever database it stores headers in once it gets that back it can look at a field inside of that to see the state route and then it reaches into the state database to actually read your account balance right this all happens very quickly under the hood and the reason that a traditional ethereum client can do this is because it is maintaining these three these databases that it is constantly online and it is constantly keeping these things populated the portal Network concept is very similar right there's very little right this is like oh God no wrong direction too much too much uh so in the portal Network context when this eth get balance uh request comes in instead of reading from local databases what a client's going to do is it's going to actually reach out into this net these networks that it's part of to get the data we have a network for essentially tracking the head of the chain that provides uh the Beacon Light protocol data your client's going to reach into there to know what the front of the chain looks like which what the head of the chain is it's going to use that to pull the header from the history Network which stores all of the historical block bodies headers receipts things like that and once it has that it can look up what state route it's supposed to be looking up things under and it can reach into the state Network to grab that state look up your balance and return it to you uh this is a very simplistic example but this is very representative of what the majority of requests are going to look like there'll be a little bit of sampling of data from different networks in order to get the information that you need before it's returned to the user all right where are we at um like I said this has been a long road to get here we had to build some of the wrong things to figure out what the right thing to build was um and and it and at this stage this we are we are past the research stage we are purely in the get it built and get it out the door stage uh we have three different Implement client implementations um this is fantastic I'm so happy about this because we wanted to build a protocol not a singular client we wanted to build something um that that have many clients to it and instead of just one reference implementation we've got Trend written in Rust by my team at the ethereum foundation we've got ultralight written by in JavaScript by the JavaScript team at the ethereum foundation and we've got fluffy written by the Nimbus team run by status and um and here we are uh so here's our rough timeline right now um software estimates are garbage uh imminently we are right at the edge of getting our first Network really fully up and running that's the history Network um in parallel to this the merge has sort of kicked off us having this Beacon Light Network and that's sort of like our next major priority and after that over the course of 2023 we are going to be getting the remaining networks online uh the zero to one is a lot harder than building the subsequent networks that come after that we've spent a lot of r d uh getting to this stage where we almost have the history Network up and running um that is what I've got for you today if you want to get involved we are findable on the internet like Danny has often said the doors are all wide open and unlocked if this is a project you'd like to get involved in uh please feel free to reach out to me and I've got some time for questions if that's something we can do I think we've got a guy with a mic walking around uh I have a question related to like the API endpoints which the portal Network and so will it be able to serve like debug endpoints start like need like a whole archival stories on notes if I understand correctly you're asking about the debug spaced Json RPC endpoints yeah uh not initially all of the data to do things with those will in theory be present um in the network but we are really focused on like um human driven wallet interactions that's kind of like our primary use case that we're that we're focused on delivering and the debug endpoints just don't play a role in that um and in general are going to involve a much heavier level of requests and like data access than is uh traditionally involved in in kind of standard wallet interactions so no the debug names based in points are not officially supported thanks thank you um from the perspective of a application layer client is the thinking that an application would be like speaking directly to like an application would want to run one of these like clients itself and speak to that or is the thinking that for some reason you would bundle the client itself into the application or neither of those things just trying to grok what the use cases I think I understand the question um so the the there's a lot of ideas on the table and exactly which ones are going to stick and which ones aren't is right we'll find out as time goes on but the general idea was to build a network where the clients can be lightweight enough that you might have two or three of them actually running on your machine at any given time because in theory they're lightweight enough to actually embed so if you're you know downloading a desktop wallet um and the portal Network's up and secure you know uh up and live and production ready there's it's entirely likely that it might just bake the client right into the into the application that you're running and there might be two or three others running alongside of it um one of the things that my team's going to be focusing on is sort of like a system level process that's really easy to download and install that just runs the thing in the background which makes it easy for you to do things like connect metamask up to it or things like that so I don't think that there's one model here I think some applications might embed it some might treat it as an external dependency hi there yeah there we go yeah so something that I'm familiar with the Elias client that um is that when you want to execute for like a smart contract call you might need to do several round trips because you are basically every time that you need to query State you are going to go to the full node and ask for something right and that maybe creates like again several round trips which increase the latency of like whatever you're trying to do so is with the important Network you kind of have some solution for that like trying to kind of batch that uh I think the question is because the requests are going out to the network there's going to be some inherent latency overheads that come with that um not that precisely but that you're going to do multiple requests so for example if you want to do rc20 balance off you need to download the smart contract then you need to start executing and every time you need to access some part of like the state uh database you're going to just go again and make a query and so you are basically doing multiple multiple concurrent Json RPC requests that's the kind of concept here yeah I think currently you you do done sequentially and so basically you are increasing the total latency of like balance off um but this you can find a way of like doing that concurrently or batching up that you only do a single request I think that's going to be a thing that individual Proto clients figure out on their own there's nothing inherent about the networks that keep you from looking up large swaths of data in parallel at the same time and anything that can be parallelized at that networking level will definitely benefit um total amounts of latency that users experience okay thank you where do you or do you see Zero knowledge being used in an implementation of a like client for the portal Network currently it isn't part of any piece of our roadmap it's not my expertise so I don't know I think is the concise answer there okay thank you um hey Piper great talk man um just had one quick question uh what what prevents me from running a light client and making it like making it a freeloader that does no work how do you prevent against uh we don't um so there's two things that I'll say here um one is that we had to pick some cutoff points for what we were building because this is big like like I I I took a lot of stabs at making lightweight protocol access in smaller ways and this is what came out of Really Trying a bunch of things that they're not working so so in order to deliver this we needed to build something that was much bigger than I originally thought we were going to have to build and in that we had to like kind of cut it off at a point um the thing that we're building is attackable there is a tax surface that we that that exists and the general idea here is that those are solvable problems and we're not going to focus on absolutely trying to make sure that we have them all solved on Day Zero the portal network is not core infrastructure not at the protocol level the protocol as as we know it does not depend on the portal Network for anything and so if the portal Network Falls over ethereum does just fine and initially it's possible that somebody attacks it and that probably means we're doing something that's working because it's worth attacking so that's the one piece which is that we have built something that we know we are going to have to hone and fine-tune and work on the security part of it thank you I held up two fingers and I swear I had a second thing um I have a second question that might be leading I'm kind of also curious if uh if I choose to make my light client act maliciously and you ask me for that there we go freeloaded the network is designed to like like if there are too many freeloaders it'll degrade performance for everybody I'm relying on essentially two things which are uh the laziness of people so people are inherently lazy and so going and configuring your client differently than how it ships is like something that people often won't do if it's working just fine and if we ship it with sensible defaults that aren't running your fan speeds at full speed and aren't filling up your hard disk the chances of you taking the time to go in and tweak those settings are pretty small and there's a and and you can call it altruism or you can call it laziness but we're fundamentally built on this idea that these small contributions of lots and lots of people add up to it a lot right BitTorrent works you can download things pretty fast on it because there's a lot of people doing it and most of them aren't screwing with their settings too much to just Leech from the network so um can we get the mic over sorry yes I'll get you next uh yeah just a small question you mentioned the beacon line Lite client Network as well so what sort of data are you planning to distribute in the beacon Dateline Network that you're talking um there's I believe three data types that we'll be working with I am going to potentially botch this if I list it off the top of my head um Kim on the fluffy team is the one kind of leading the like r d or the the specification for what that Network serves but it's the like client update objects and there's like one or two more but essentially it's the minimal objects that we need to be able to do the Beacon Light protocol to jump to the head of the beacon chain let's bring in a mic oh you have a mic sorry I can't hear you I'll repeat your question I was just wondering um when you're testing on a day-to-day uh what user flows are you using for your tests well so we're designing at the Json RPC API level so we're not necessarily building interfaces for people we're taking the Json RPC API which is like the standard API that execution layer clients exposed to users right this is what Alchemy generally exposes these are the things that metamask is like calling to and so while we are user focused we are building out clients that can serve the Json RPC API which is still a low-level thing right you're still talking about computers talking to computers um the wallets and things that get built on top of this that's I think the type of user testing you're you're potentially asking about and that's kind of like outside of our purview um are I I guess you could say that the wallets are our primary clients and that the users are the wallets client if that makes sense is that okay two questions are we talking about one to one Json RPC compatibility and then the second question is what does the roadmap look like for l2s because like clients are inherently very useful for cheap operations and that's kind of where stuff is going so that's it I think you asked whether we're building out a like one-to-one to the Json RPC API we are not trying to redefine the Json RPC API it is established and has been successful and is generally the backbone of any kind of ethereum interactions that wallets and things are making and so we are building off of the existing standard um I don't have an answer for you on the L2 thing um it's an open question we'll see how it goes and that is all of my time thank you all thank you so much Piper and also uh thank you for the audience this was a super nice q a session keep the questions coming for the next speakers as well um so moving on from the Potter Network to our next talk I'd like to invite David to the stage he will talk about debugging the ethereum merch with parallel universes and in his talk he will explore the difficulties of developing uh stateful systems like a blockchain with high confidence and will explain how the merge required special testing in order to find the worst possible bugs in these systems beforehand so please give it up for David [Applause] David I'm just going to present them all right oh okay ah excellent okay can you all hear me are we ready to go oh cool excellent well welcome so um so you're if you're in the wrong room this room is talking about testing the merge uh with parallel universes um my name is David Searle and I'm the head of Amir for a small startup that's been working with the ethereum foundation for just over a year now um who likes Bugs okay okay well I brought some bugs along with me today so you're gonna have to be like who would like a bug because I've got some bugs that actually I'd like to give to various members of the audience anybody you'd like one okay here we go just after a little you're gonna catching so little bugs um we've been spending the last year kind of really stress testing for my boat so see this is why you should start with the front so um I've got three more to go there we go see so the it talks about testing ethereum and it's it's basically looking at using a pretty sophisticated and unique testing methodology which is based from a company called antithesis which I belong to um the company is involved in using deterministic simulation techniques to allow us to basically explore a whole host of iterations of of complex distributed uh technology that's running in a simulated environment so let me just get my my little Clicker here we go let's see if we can do that I brought a few uh Stars along to the show um obviously I've just explained who I am um little did you know that Dr Strange and Cheryl are also going to participate in this this presentation um I've got about 25 years worth of experiences in the tech industry for the last two and a bit years now I've been working for this business antithesis and they've um you know basically made me into somebody of that there's a bug Hunter buyer by trade I never thought testing could be fun but there's nothing like hitting a seg fault and actually getting a real kick out of it and that's what I do on a day-to-day basis um so this is my house in the UK you can tell from my my accent um and as you can see I've mostly got a few gpus I've got to sell uh so if you're interested please let me know um no joke um but some the the day of the merge came around and we've been doing a lot of work with with all the various clients um all that hard arduous testing um making sure that we call every last bug um what was going to happen was it really going to be smooth sailing yeah I think we saw this morning which obviously is great news um but um yeah I was wondering if we were going to see this you have to press play on the uh the video there is sound as well but uh um I couldn't believe this when I saw this but uh this is real yeah it's pretty bad eh um not my front garden again so um testing is really important and this is where you know we talk about it but the merge was a significant event that's why we had so so much celebration across the world about kind of what was really going on uh when when the merge took place um so congratulations to anybody who's involved in the merge you know phenomenal job and uh yeah I think we can you know hold our heads up high there so I say testing is hard and you only got to use Cura to basically you know ask a question you know what you know what makes software testing difficult yeah it's a bit of a high level question you know apologies for that um but you know these two really um kind of uh sort of struck me here like the first one you know this is the basically fundamentally this is an impossible requirement because absence of evidence is not the evidence of absence and so often we run these simulations in parallel universes hitting you know hitting a whole host of code and it's clear and you're like well is that good who knows like you know we we can only really understand how much coverage we've got and it may be maybe good maybe bad who knows so that's that's a phenomenal kind of you know kind of you know requirement the the second one is then about kind of understanding like testers have to think about all those possible scenarios where issues may arise and ensure that they are handled by the code and you know that's just not it's just like how do you how do you approach that with a distributed you know kind of architecture like we have with ethereum um we've got a whole host of different clients using different architectures we've got the execution layout we've got the consensus layer it's just it's amazing to see it actually working and and but the complexity of that and is just is just phenomenal um if we then look at kind of what ethereum has done to approach this task and this is you know this is not looking specific antithesis we are a part of the equation um and I think we're very much of a complementary is is that ethereum is using unit testing test Nets we've got a list of Shadow forks and test net mergers occurred um some technologies that have been used called Hive and ketosis um that do similar um heaters to the equation we then have antithesis that is then doing this deterministic piece which allows us to to Really you know get into the detail of of all the different iterations um that are possible and then we we have a host of different fuzzing technologies that the ethereum are using um to to really try and isolate and kind of hammer areas that are of concern so you know throughout the entire year you know a huge amount of testing is being done and on top of that they've also used static analysis to just you know help with doing code Audits and making sure that we catch things before the merge was to take place so I'd step back a little bit in terms of kind of the testing as hard statement you know it is a phenomenal kind of like you know obstacle to try and get over like how how do we find every iteration and how do we find those hard to reach bugs that you know may not be common ground but if they do occur will be catastrophic um and so if we just look at the right hand side this is a representation of a distribution Network so on the right we're going to we're going to call this node we're going to call this node um you know using Lighthouse and Geth so the lighthouse and Geth are running on this node they're running on an operating system leveraging CPU um we've got a file system they've got different processes happening and then inside those processes we've got different threads being used as well so just in its own sort of node there's a lot of complexity about what's going on and we can we can test a node we can test maybe a combination of Lighthouse and getth working together making sure that we're kind of you know pushing the boundaries but if we then think about how we then have a network and we have a whole host of different nodes running together and may have other underlying pieces like databases and other pieces kind of actually on on top of that um the the complexity of all those different communication channels operating just means the search space for Hidden Away Little Gems I call them gems but you know they're bugs basically those those they can hide away for a long time and you may never find that search when you're looking at your testing you may never find that that particular condition where the network was slow and there's some there was a bit of thread pausing going on in a certain node and you know the the the the number of things that have to happen could could have a bug manifest and so this is really where where we step in because you know we then we then have you know the complete picture and this is what we build inside our simulation um we have the ability to kind of run the entire collection of consensus clients and execution clients in in a simulation run it and and understand exactly kind of every way in which we can actually kind of see outcomes um that's pretty intense you know how do you do that and fundamentally how do you do that in a deterministic fashion if we find a certain situation where Lighthouse crashes well can we represent that again can we replay it can we yes the first thing you do right can we reproduce it and often bugs are not reproducible they often you know they might try something different they might move around what like how do we how do we debug this and and this is again where we step in so what do we do we leveraged the ability to Auto generate networks inside this simulation environment and stand up all the containers that were necessary to bring up ethereum um we then were able to then use Network faults and other types of faults to just inject chaos into that running environment um obviously it's great to see the software handle that it's designed to handle these types of things so you know under duress we see things going wrong um and we we basically use fuzzing technology to to basically hit the entire system um not just individual pieces of software but the entire system is fuzzed and that allows us so basically kind of you know deterministically replay the complete orchestration of a situation not just one particular application but we know that in this combination different clients under these Network conditions something goes wrong um and so we use strategies inside the system to allow us to seek rare events and so when you when I start looking at the numbers and kind of what we've been doing over the last year you know we hear a huge amount of code edges like you know a huge amount and so how do we how do we find and allow ourselves to to seek out those edge cases that you know if they do happen again it can be can be pretty catastrophic um but you know get to them and and uncover them to help the client teams debug and fix so we have we have the ability to have all this wrapped up in in a a tool set um that is available to ourselves and we share it to the rest of the client teams and the F and that's been really really useful I think if we look at it we have we have the the individual testing going on which is fantastic we continue to promote that and say keep keep going with that we have the test Nets and Shadow Forks again I think you know we can all say that they've been tremendously successful and useful and then we have our parallel universes we have the ability to run not just one simulation but we're running simulations literally every day that are generating and using you know exploring the various search spaces that exist across any number of different branches inside the uh the repositories so what does this mean for us well there's a little Little Help from Doctor Strange here hopefully you've seen it because it's not playing here we go this is what I look like when I'm doing the work [Music] for it in time to view alternate Futures to see all the possible outcomes of the coming conflict how many did you see 14 million 605. how many we win [Music] one so so for us winning is finding these edge cases right you know winning isn't like oh all tests passed yay um we want to find those those really intricate kind of piece you know combinations and iterations where we've covered 14 million different scenarios and we've got you know maybe not more than one but we at least have one you know we can actually hold our hat on so that's that's an example of what we're doing and we think that kind of represents you know pretty pretty good sort of analogy um if we move forward we've seen it again here we are this is kind of what it looks like in our world um you know we're looking for bugs um this this is an example of an output of just one run so we we ran this um 13 hours worth of water wall clock time which is if you look at the wall and there's a clock on it um it will last 13 hours um but uh it actually allows us to to exhaust 536 hours worth of testing um and you can see here we are we're talking about an enormous amount of edges being seen here um the branches are kind of how we get decision points we we get like an if then else on a kind of statement inside the code very simple example but just just take it um we can basically Branch off at that point and exhaust both Avenues and actually see what happens in both situations underneath those and you know under those test conditions um that's an example of a branch and obviously the code The Edge is seen is we've got the instrumentation happening so we can see what kind of functions and and pieces of code are actually being executed pretty pretty insane you know that's one run I've got 180 000 edges that we're seeing across across the entire network it's a busy Network um and obviously with all the different clients happening that's uh there's a lot going on so we see ourselves as a complimentary piece we you know we we've we've been looking at this um as a you know a great example of a project where we can basically work alongside all the other pieces that the EF are kind of using and the different client teams are using um so we're just another layer in in the growing list of of of strategies being being used and I think that's sort of Testament to the approach that EF have used um to to Really hold on to this resilience you know making sure that we take advantage of every way we can we can make sure that you know nothing untoward happens in the future um you know more more and more upgrades coming I'm sure so you know we've only got 4844 around the corner I'll say around the corner we'll see um but uh it's certainly something to look at in terms of um you know making sure that uh we're involved at every step of the way so this is a very simple sort of uh illustration of what we've been doing we've been building all of the clients um giving them in in place and actually establishing Genesis so we actually have been using this at Genesis block and then moving it forward towards the merge in our in our world we would have the merge kind of positioned um we obviously start fault injection we've established a chain and then we start fault injection um to get to a point where um we've got a whole host of different faults you can see them here labeled below so anything from partitioning um that's a great one obviously to encourage forking of the chains um we've then also got you know delays happening drops of packets um the nodes themselves we can stop you know we do we do we put this thing through its faces we stop things we pause things we kill things we bring them back up I suppose in some respects it's quite realistic you know things do get rebooted and violators come back up again we we have all that kind of like happening inside the simulation and then from that we then have you know any number of threads being paused released there's there's yeah it's it's busy um every type of this sort of configuration of faults that we put into a simulation is complete again completely deterministic so if we know that this ended up with a segful we could we know we can literally put exactly the same into the into the environment it's basically creating a pure function of the entire system and we know we can get the exact same output which I think is a tremendous piece of technology which we have under the hood so what are the numbers like well yeah you know in one year we've we've conducted 31 years worth of 24 7 non-stop testing um that that has processed and and explored over 50 million edges of code um you know out of that and you can see there you know out of that we've got 45 validated errors some of which were catered For but 30 of the 33 of them were logged bugs um that were pretty catastrophic um things that really would bring either an actual note down so panics seg faults um we'd have you know nil pointer exceptions we'd have there's there's one I'm going to bring up in a minute with a little little example but um things that you don't want to have like living in your code base and and I think that's you know we would forward those through to the different client teams and they've been able to eradicate them and uh and obviously through the successful merge um it's been great to see our efforts uh put to put to the test as he wants an example or I'll give you I'll give you some um so here's here's an output of our of our sort of one of the runs this would come through from an email and hopefully you can see it's Biven our chart you've seen the side of the select the stuff on the right hand side but down here you can see a whole host of assertions that we've got running across across the entire network um this this is actually flagged up by fail and they've got a fail on a seg fault this is this is you know something that is is absolutely real zigzag V illegal storage access attempt to read from nil question mark I guess I'm really interested ones you get like you please report this oh please repeat please fix this um so this is just an example here and we're like What do we do now how do we how do we take it to the next step because it's great saying oh you've got a problem um this you can see here is actually for the node that's running Nimbus and Geth so we know there's an issue there so okay fine have you got any other occurrences on either any of the combinations we do um so okay that's cool um well let's let's look at an example uh of an actual kind of log entry that's coming through so this is this is a unified log the serialized activity that's occurring across the entire simulated environment bit of mouthful so you see you can see here on the third column the blue column here again this is just showing us all the different combinations so we've got Cloud we've got prism client Aragon we've got um there's nimbers never mind nimbuspessu all kind of serialized and you can see that we've got a concept of time running through so this blown up piece is basically the stack trace of showing us kind of what is happening and Nimbus is having some issue it's receiving some Json trying to figure out not doing too well for it so because it basically crashes the entire node um what you know so we go okay this is cool this is this is why I kind of go yeah you know there's got something something of Interest here um another example here this is just from from Prism um again I don't love sort of seeing panics but like you know it does it does show that there's something like of value here that we're really stressing um the the entire environment so again invalid memory address yeah okay cool what do we do now you know do we descend it to the client team well yeah you can and they might I wrote that code I can see what's going on here um well she had to do it didn't she share that's just If She Could Turn Back Time well the great news about deterministic environments is you can turn back time right so we're going to basically jump into the actual environment and actually say okay right how you know when does this manifest like is it okay it crashes was it a second ago was it five seconds ago what happened in the execution pass of all the different Opera operating nodes with all the fall injection going on or or was it um yeah what where where do we where do we see this like actual kind of thing manifest and so we we want to look back so many seconds we may want to turn on car packet capture pack capture isn't by default just because the amount of data but it isn't turned on by default we want to be quite selective about kind of where we see packet capture happening and and then we can look at the data see if again we can rerun it um again obviously determinism you're stressing a little bit of kind of will it still be deterministic if I suddenly turn on Packer Gap during a moment in time was have to look at that but but what we've seen is actually yeah you generally if you've seen something you know manifest then we can we can actively kind of kind of turn on things to help kind of debug and work out what's going on got some really interesting stuff coming through next year which I can't share but really cool so this is what we can do today this is what we've done with the with the uh is basically look at this mountain of data like you think of all the stuff that's going on all the different scenarios branches Paths of execution that have happened we can we have all these available in a big massive data set so we can we can look at this kind of analysis and look at the x-axis which is your time versus the probability of the bug occurring on the y-axis now the top right is where the bug is seen we can then look at the common routes that are happening across all the data and try and then bring it together to see what is happening you know where where does the execution occur where suddenly the probability of this actually becoming a bug yeah happen and and lo and behold we have a huge jump here from literally 0.05 of kind of the the kind of bug occurring to over 50 this jump here in the middle we can start to go okay right we know how many seconds to go back now we can actually Replay that simulation and turn back time I won't sing it um and and and see what's going on there because otherwise you're looking for a needle in a haystack like how do you do that and that that's really been really valuable um in in our in our efforts so what's next well merger's great love it we're doing some Pokemon testing on some stale branches which isn't you know we've got you know things in a fairly good order there um but we know ethereum's not standing still we've got obviously EIP 4844 just around the corner I'll say it again um thanks sharding withdrawals you know a whole host of new capability which is you know brand new products brand new code brand new testing um there's other pieces in there around you know actually the one that's interesting is using things like malicious clients you know what about if I brought up an environment that has a client that is you know not doing what it should be doing um you know can that cause issues you know how does that kind of part of the of the code for the rest of the the network handle handle that client and does it does it operate it in a different manner so other pieces here and you merge clothes clean up you know there's changes to you know kind of established pieces of code that are in in place now all those changes introduce you know probably good and it could appear uh you know Downstream so pretty cool um we're working actively with the clients um and open to really you know kind of broadening that relationship further so that's the end of that q a um any questions I'm not sure if that's two minutes of questions or two minutes of the end so lady in the middle do you wanna a bug I was like late in my mouth sorry yeah uh you showed us uh it's like calculating the probability of a buck and I was wondering how did you calculate the probability of being a bug in time so I couldn't quite hear that excuse me repeat again please yeah um you showed us a slide with the probability of finding a bug how did you calculate the probability which model you were using or how complex is you know the calculations and that stuff that's a good question so the just because the amounts of data we've got available we we know every kind of path execution that's occurring that the outcome doesn't occur with the bug and we have then obviously all the different outcomes that do translate to the bug happening um and so where the ability to you know do some simple calculation we can see that you know the path the path that's sort of trodden and worn and you know this but this if this is if we see this happening then we can calculate the bug occurring there are obviously other branches down that that well-trodden path where my bird doesn't go somewhere else and the bug doesn't manifest so we that's that's how we at every point in that graph we can we can calculate if you know if it does contribute or not contribute to the bug hopefully I'll answer your question um a quick quick add-on to that question how do you kind of isolate unique bugs so I don't know it's like the same bug that's a good question and so we we see a huge amount of bugs that do manifest and show us like yeah duplication like you know like the one you saw with Nimbus and and Geth that we can we can run that literary day in there and then it will you know with the right commit code which is in history but we would be able to see that and we'd see we we have counts on that occurrence so we can see very quickly this isn't just like an edge case that's just happened on one sort of combined client set it's it's like it's a problem across the board and so um we have all of that wrapped up into our reporting yeah you know what do you think about applying formal verification in this huge environment it is it um do you think a possibility or I'm actually not the best person to answer that question but bring it bring it to us at the end because I've got some people that can answer that question for you okay Christians oh okay well please great chance of us keep keep you know keep the conversation going and uh I hope you enjoyed the talk I have two more anybody want thank you David and have fun with the Bucks guys [Laughter] okay um moving on from testing the merge and debugging the merch to actual test Nets I'd like to introduce the next speaker to the stage which is our free he's the head of protocol engineering at chainsafe but not only this he also is known as the initiator of the girly test net initiative and organizer of East Berlin and in his free time he's the maintainer of the open source ethereum and Ruby and Crystal libraries but today he will talk about test Nets and specifically post merge test Nets the merch has introduced quite a few changes that we are all aware of and how they those are affecting the testing infrastructure and which test Nets you should be using in future um afri will tell us everything about it so please give it up for free thank you thank you Francie yeah let's talk about test Nets um yeah briefly about me I'm head of protocol um AT chainsafe Systems thank you Francie for the introduction I'm also one of the co-organizers of East Berlin and um I used to work on various EF and ESP grants in the past for both execution layer and consensus layer clients and among others as France you already mentioned I launched the early testnet with a bunch of cool people that are also sitting here in the room uh in 2019 so what I'm going to cover today is I want to briefly dive into the ethereum protocol history prior to the merge and then specifically take a close look at the implications of the merge on the test Nets and finally give a quick guide to selecting the test net for your project so yeah let's dive right into the protocol history um everything started with a test net does anyone here know what's the meaning of C serum Genesis extra data is you might know the Bitcoin extra data or against this accelerator contains a news headline does anyone know what the meaning of the ethereum accelerators it's actually a block hash from a test net ethereum was a publicly fairly launched by announcing a block number on a Olympic test net that was one million 28 201 and once this test net was mined on the test once this block was mined on the test net this hash could be inserted into the ethereum Genesis and the public mining on ethium mainnet could start Olympic was the last pre-release testnet before the issue mainnet was launched and the first public assumed testnet after the launch was more than was a proof of work testing it using the same algorithm as mainnet and interestingly it had a custom starting launch of 2 to the power of 20 to prevent transaction replays on mainnet most of us probably remember the Shanghai attacks during Defcon 2 in 2016. an attacker exploited the long discrete time of the X code size op code um in one of the client implementations which caused the denial of service of the issue magnet here's a photo of the amazing guest team investigating the issues and coming up with the solution address this issue to subsequent hard Forks were scheduled for ethereum the Tango Universal Fork to fix the attack vector and dispers dragon Fork to allow cleaning up the state afterwards however this protocol change caused the consensus failure on Modern this was specifically caused by the different ways how the clients handled this custom starting loans and it was then decided it's not not a good idea to have a custom protocol on the test net and therefore modern was discontinued and replaced by a new test net called rubesne Robson was the first tester to launch with chain ID right from Genesis the chain ID served the purpose of simple replay protection according to EIP 155 so the roofs and test net was also powered by the esash proof of work engine and we all know proof of work is fairly permissionless so there's no good incentive to secure a zero value proof of work test bet so attackers moved to DOS rubstone and the test net was unusable for weeks in response parity launched the coven proof of authority test net and guests launched during the proof of authority test net for the first time ethereum had a way to uh have more stable test net environment however kovan used the parity or consensus engine and drink be used to guess click consensus engine and they were not con compatible coven was therefore only accessible through the parity ethereum stack whereas ring B was only accessible by gas or tooling that relied on the gas stack so in 2018 the girl initiative attempted to address this by ideating a cross-client proof of authority test net in the end the initiative managed to implement click and parity and subsequently in 2019 the early testnet was launched as a first cross-client proof of authority test mode with validators from both guests and parity ethereum at the same time coincidentally after Defcon in Prague I believe the pantheon client was released so there was uh even a third client uh available for running validators on this test net and soon after also nazarme joined the validator set yeah later this same year parity exits uh ethereum and um leaving coven fairly unmaintained unfortunately now going fast forward in time uh just earlier this year to prepare for the merge some of the older testnets had to be deprecated the protocol support team announced the end of life for Robson and Rigby on the amazing ethereum Foundation block they argued the older test Nets are harder to maintain due to Growing history and state and the growing complexity to run the test net setup so in foresight however a new testnet was launched sipolia interestingly sepolia um that was just launched not even a year ago it was launched as a proof of work um test net using the same esash algorithm as main net back in the day there was a first time since uh the rubstone launched in 2017 that we actually launched a new proof of work test net the reason was potentially to have another test match similar to mainnet to allow for once again testing the merge and now mainnet conditions foreign is fairly dead it was replaced by Robson in 2017. groups and then again was just deprecated a couple of weeks ago um coven diet was poverty leaving ethereum Rigby was also deprecated just a couple of months ago and that leaves us with Scully and sepulia as the latest test nets for ethereum [Music] um and now let's just take a look at what it actually means to merge uh yeah I took a lot of time to build these slides so in ethereum this the merge stands for an event where two blockchains are literally glued together you need a consensus layer Beacon chain that takes over finality and Fork Choice considerations here shown in blue and this consensus layer Beacon chain replaces the Legacy proof of work consensus or the other consensus mechanisms on the execution layer here depicted in red so the slide shows the timeline of the test net mergers and I added a lot of details here I'm standing in front of it I had a lot of details here but let's revisit this slide in a bit after uh talking a bit about the implications so um just to recap a proof of work consensus mechanism is permissionless as per Nakamoto consensus so you can just turn on your CPU Miner and hope for a block in theory on the other hand a proof of authority is permissioned obviously only authorized accounts may take part in consensus and then again proof of stake is fairly permissionless you have a token and you can stake it to take part in consensus so what does it mean for SEO mainnet the merge obviously moving from proof of work to proof of stake um just does not change much in this perspective it was permissionless before the merge is still permissionless after the merge but how does it look like for the test Nets so girly was merged was a Prada Beacon chain test net and um to access to get access to the consensus on the growly test net prior to the merge uh was con was permissioned so girly was running a community maintained validator set however after the merge it transitioned to a proof of stake consensus interestingly and suddenly everyone can participate in consensus given you have access to girlies so girly used to be permissioned is now permissionless but what about zipolia as I mentioned prior to the merge it was powered by the permissionless esash a proof of work algorithm here you can see my sepolia minor I was happy that I once again can mine a test net and yeah I did not ask for permission I can just I was just able to mine the blocks I got a lot of zipolia easer um but now there's something unique about um the C polier Beacon chain um since it was apparent that girly will no longer be permissioned there was actually the the Quest for having another permissioned test net to have a certain stability guarantees so it was decided that the polio gets a beacon chain with a modified deposit contract that does not accept regular ether deposits instead attracts the valid data sets through an erc20 token so in that case zipolia is a proof of authority version of The Beacon chain and therefore the access to the polio consensus is permissioned so let's take a look at this overview again um this is a timeline actually on the x-axis axis the first test net to merge was rubson um Robinson was fairly similar to mainnet it was proof of work and after the merge it was proof of stake right so permission less permission less then sepolier transitioned with this modified uh Beacon chain so technically the consensus algorithm is still proof of stake but through this permissioned um erc20 token that you require to take part in consensus I call it for Simplicity proof of authority here because access to this consensus is actually permissioned um and then girly merch was Prada and test net were exactly the opposite happened uh the proof of authority equality test net became a proof of stake girly test net after the merge and then yeah we are here eventually just a couple of weeks ago the mainnet merge occurred so yeah just take a look at the post merge test net landscape I showed you that groups merged however it has been deprecated by the ethereum foundations a protocol support team and it leaves us with uh girly and simpleia is the only long-standing public post effect so if you have an application that you want to deploy Deploy on girly and you require other applications or interfaces you might find some on zipolia regularly but not on zipolia because sympoli is fairly new and not many applications and libraries are deployed yet however it's also longer to sync and more involved to run a girly infrastructure so to summarize this please do use girly test net as it is most similar to the assume mainnet girly is especially interesting for you if you plan to test a beacon chain Valley data if you want to test your setup if you want to test upgrading client versions if you want to test going through protocol upgrades girly is potentially the best or if not even the only test Network you can actually conduct this [Music] um yes please also do use the sepolia testnet um sepolia comes with the best stability guarantees due to the permissioned validator set since it's fairly new it is fastest to sync and also it has the best long-term guarantees so I would lean towards a recommending uh to test your applications or even migrate your applications um to sepolia instead of curly at this point yeah do not use groups and obviously um I um when I prepared my slides I realized that we are just in this transition phase where big service providers already start shutting down infrastructure so you have to expect interruptions and downtime um don't use coven for obvious reasons and yes also I would not recommend using Rigby even though there is some long-term support plan for Rigby for almost another year but you will potentially not get more protocol upgrades on that Network so I would also not recommend using ring B in that regard foreign yeah right there are some caveats so as I mentioned um the girly test net has uh ether supply issues due to the sheer amount of users trying to test their validator set up each validator requires 32 East and if you want to have a more involved setup with I don't know 1000 validators it quickly becomes a huge burden huge problem to access these uh required test net tokens this is something we still need to figure out going forward so if you have an idea please hit me up um yeah and in terms of test net age Gurley is also fairly old so it has it comes with a fairly big um a rich history and State and is therefore more difficult to synchronize especially now that it's merged with prata which also comes with a fairly old Beacon chain uh in in combination and yes the polio is uh but if you're new as I mentioned it I wrote down lack of infrastructure I put it in Brackets because it's changing really rapidly right now um I just noticed while preparing my slides that uh even metamask has a sepolier test net switch now so um it's happening I'm really happy that we get all these Integrations now but yeah okay um this is my subjective recommendation if you want to test infrastructure if you want to test your validators I would recommend you to run them on girly if you want to test your applications I would even encourage you to skip early and go straight to run your stack on sepolia okay this was my talk please use girly or sepolio going forward um find me for some spare Visa cards from East Berlin they are pre-loaded with Squirrely and sepulia Esa so if you want some I will not throw them into the audience if I just fight me after the talk um and yes since we have a couple of minutes left please ask questions thanks for the presentation is there any tutorial or manual how to be a validator in sepolio how to be to help in the infrastructure uh short answer is you can't um because as I mentioned even though it's proof of stake you need a special erc20 token to get access to this value data set and you would basically have to Google the polar GitHub repository and there is an issue where you can request to be added to this value data set but in general not many teams will be accepted just to keep it really a small level data set maybe two questions here one when eip1559 came around it was very hard to test that in advance because you have these proof of authority networks that had no fee market and then you have these proof of work networks that had like kind of a fee Market you could kind of Reason about sort of what that change would do sorry I am leading into question here I'm just giving some background and then coven is if I seem to remember right was like parody Technologies maintained it for the old open ethereum parity climb sort of the idea and I think like now the execution layer is a spec there's some value behind um you know testing clients so I so I guess Mike just to sum it up to a question both testing features an execution layer specs is there any thought in behind testing those kind of things moving forward spinning up test Nets or Forks of test Nets to do that so I'm sorry I don't think I got the question it's also I don't have the speakers if you could just try to speak up a bit or so so specific testing specific features of ethereum forks like a fee Market changes or testing the or validating the the execution spec uh was sort of the goal because like you had like you know open ethereum or parity and Geth and whether they synced up or not in terms of where they're executing uh you know evm basically correctly was always a question right if they ever split so so I that's that's what I'm asking where are the where is the test net infrastructure sort of going in terms of forks and things they're just gonna spin it up whenever or whatever I want to beat this to death um yeah I hope I don't understand the question uh correctly um so I was talking about public test Nets today and public testness is always a really final stage of testing everything so if we want to test execution layer changes the contents layer changes and whatnot you usually go from from like a local simulations to to permission to deafness to um through like a lot of hurls until you actually get to a public test net and these test Nets like when I recommend Gurley or sepolier they will only get protocol upgrades that have been sufficiently tested before um I don't know if this answers the question okay um do you know why did the mergers happen the way around they did could wouldn't it have been more logical for for goalie to continue as a proof of authority sorry I think I have to come down I'm just wondering why the mergers happened the way around that they did as opposed to gawley continuing as a proof of authority in sepolia uh transitioning yeah I think that's a very good question the reason is that it merged with the product test net which you used to be a consensus layer testment for a very long time where many teams already tested um running validators for more than a year or one and a half year so it was already this proof of um stake test net available and it was decided not to um launch a new test Beacon chain test net um for currently specifically and just use the existing one so this is that's why we had this flip that was also discussed so maybe have a new uh deposit contract on Gurley but in the end they decided it's actually worse for testing the merge to do the big big merge because prata was also the only uh consent layer test method had about approximately the same amount of evaluators and mainnet as mainland yes and it was very very valuable for Gene image or a new version of the code base and how often does that happen during the test net phases what's the question uh if there's a bug fix on the test net how often do you have to rebuild your clients yeah what's the workflow when there is bug fixes applied to the test Nets if you are a validator there might be a protocol bug so does the validators get coordinated and they redeploy their nodes multiple times during the test net rollout uh yes this is basically uh yeah the answer is yes but um this happens very rarely because most bugs are usually not caught on test Nets but much much earlier I think I'm out of time yes so um thank you everyone and don't forget to run a test net awesome thank you afri yeah guys don't forget to collect these sweet ether cards that afri has with him because we all know or if you guys are developing on a test that you know that girly eat there's various scars all the faucets are always empty because of some evil Bots draining them so if you want to get your hands on some very sweet girly eat and sepulia is uh come and find him afterwards I would recommend not in front of the stage but maybe somewhere else um yeah moving on to the next topic and we are really on time so that's nice please guys come in for the next talk next up we will have Khan who is the main developer of certify at the ethereum foundation and today he will talk about human friendly contract interactions and explain how Source verification projects like sourcify can help web3 users take more informed decision before signing a trans action so if any of you are guilty of just signing random hex strings transactions you will now find out what we can do about this as an ecosystem collectively please give a warm welcome to Khan [Applause] it's working all right so hello everyone my name is I'm working on the ethereum foundation at the film foundation on the project sourceify and today I will be talking about how we can enable more human friendly contract interactions using source5 verification foreign you guys something you're all familiar with I would say if you're a web tree user for a while um just a normal day in webtree and I guess this will this will be like something you see every day and this so you see every day in webg you see things like this and you basically have no idea you're not a machine you don't understand what's going on you're like trying to make sense of it am I doing the right thing am I talking to the right contract is this doing actually what I want to do and basically what you do is telling them to like shut up and take your money like you have no idea what's going on you're just giving out your money and we say today a typical web pre-interaction is still a YOLO signing nightmare what we call Yellow signing it's just they give you things to sign and you just like sign and hope for the best so at the end of the day what we want to have is more to do something on the right hand side from left hand side to right hand side so I know this has changed actually for many wallets so many wallets actually started to decode things like metamask using truffle um and decoding API but still we have a long way to go and we have a lot of things that we can improve the human the the user experience so what can you do to achieve this there are two sides of of this coin so you can there are things you can do as the wallet developer and there are things you can do as the um as the smart contract developer so let's dive in what you can do as a smart contract developer the first thing you can do is using a nutspec documentation and as well as doing the source code verification on sourcify so what is not spec documentation not spec documentation is what is called ethereum natural language specification format it's actually part of the solidity spec and probably you have seen this if you have seen a contract before this is how it looks like you put the comment the documentation above the function and you have the developer documentation you have the user documentation with the at notice field and you have the documentation for the parameters uh so another nice thing about nutspec is it has the specification for dynamic expression so the field you see here the old owner and new owner parameters in back quotes these actually can be filled dynamically with the value they are being called so this replaces the owner the address gets filled can get filled and the new one can also get filled in in the parameters okay so you you did you did your job you made the user documentation developer documentation so where do you find it then where can I can find it it is uh in the solid contract metadata so who actually knows what solidity contract metadata is anyone just a few people cool that's why I'm here um so contract metadata is actually something introduced early on in 2016 in the earlier versions but it was actually not really picked up by the community it is actually a Json file uh generated by the compiler itself and it contains the metadata okay but what is metadata it has the ABI the user doc Dev dock as well as compilation info and source file info so the first two it feels actually is concerned with how to interact with the contract so how to interface with the contract and then the second two is about how to reproduce a contract compilation so it's uh embeds the information during the contract compilation so that can be reproduced the file looks like this it's a Json file it has a set it says compiler language settings source file information and here for example in the output you can see the user Doc and devdoc and if we open that field those fields again you have the methods and with the methods for each method you have the notice field or the dev doc field if there's a devlog and you have the for example in this case the replaces the new owner with the old owner with new owner the comment we have seen before uh you can get the con the metadata with the metadata flag on the compiler itself uh on with the Frameworks you can find it inside the build files so travel for example put this inside the build contracts the contract name Json under that you can find the metadata fields and the the metadata is there hard hat also started to Output metadata for inside the build file again you can find the metadata here yeah yeah you can also find the traces of the metadata in inside the bytecode so the bytecode this is an example contract bytecode and the bytecode has actually a special field at the end of the bytecode this is appended by the compiler again a question who knows what this is okay again just a few people and again what's that's why I'm here so this field uh the compiler actually takes the ipfs hash of the metadata file the file we saw and it encodes the ipfs hash of that file here alongside some other information maybe I'll pull this as well alongside some other information and yeah you can see how this works in our playground playground Source by Dev we basically show how the encoding is done what the encoding contains and we also try to metadata from ipfs it's already there so it's a nice tool there are some example contracts you can click on or you can just provide us with the contract address or just paste the contract bytecode and we will try to visualize how this thing works all right let's let's go down to the second thing source code verification on source5 but before what is source code verification so this is for all of you know this is like you probably have seen this if you have seen a contract before and you see a green check mark you're happy you know the contract right um okay but how does this work um maybe before that like it is uh the reason we need why we need source code verification is that the contracts actually live on blockchain as bytecos like they are we write we humans write code in human language but the machines read it and bytes so the code gets compiled and deployed to the blockchain and this information is lost in the process so we need to somehow make sure uh just a random code UC is actually the same the code behind the contract so that's the process of knowing this code is actually the one that is running the contract is source code verification so how does this work uh you have the contracts solidity files in this case you have a Target contract you also have compilation settings like the version the optimizer settings the other things and we feed those into a compiler and we recompile the contract and remember this is actually when the second part of the Meta Meta data information comes in handy so we use this compilation info to reproduce the compilation of the contract again so we feed this into the compiler and the compiler gives us a byte code and we also have the contract that we want to verify so we get the code of the contract from the blockchain this will give us a bytecode then we'll see if these actually match and in source5 you also have two types of matches we have the partial match when bytecode match and we have the full match when both the bytecode and the metadata field field match so in right now today when you are verifying on ethoscan or any other verifier they actually ignore this field like they don't make use of this field they just trim it out and actually that there have been cases that this was um exploited so it wasn't a serious thing but this could this wasn't really doing was being done properly but yeah with the full match you have a complete match of the bytecode uh and here the metadata actually acts as a compilation fingerprint so if you match the metadata as well it means the compilation is exactly the same as the original or as when the contract was deployed so and the full matches actually cryptographically guarantee that the whole compilation is exactly the same including the solidifiles comments spaces variable names anything like you even if you change a space a variable name it will break the match so how does this work let's see how this actually works again when you are compiling your contract the compiler takes the hash of each file each each solidity file then the hashes of these files are actually embedded inside the metadata file or the the metadata file that we saw as well as the other sources not just one then as said the compiler takes the ipfs hash of this whole file and then this ipfs hash is embedded at the end of the bytecode and then we see if these match if it's say full match it's a match it's a full match stay match and let's see what happens when you change something when you make a slight change changes space change your variable name any comments so we have it my contract div this time so the hash of the file will change then the hash inside the metadata will change and in in turn the hash of the metadata file itself will change so this field will be different this time and that means this will not be a full match and but this will be a partial match assuming you didn't make a change that will change the functionality of the contract just a comment or variable okay but then how to verify so you can use the source file UI you can give us the source code and the metadata file you need the metadata file to be able to verify either from your computer or etherscan remote GitHub however you like then you give us the contract address contract chain we try to wait file you can use the API we have an endpoint and other several API endpoints as well you can check them out in docs.sourcify Dev uh we have some detailed talks about this and we also have the tooling so if you are using hard hat there's a hard hat deploy plugin and with the plugin when after you deploy your contract you can just pass the network and then say sourcify and use the verify your contract we have the remix plugin if you are using remix you can provide the contract address the chain then we will verify we recently have a Foundry support so using Foundry you can also easily verify your contracts and we also have some automatic verification so what we call monitor so we have a monitor running that is listening on on several chains we are right now listening to ethereum main net test Nets as well as some Roll-Ups out as far as you remember so the monitor what it does it catches contract creations and then when it finds a contract creation it will fetch the metadata as you remember it's like ipfs hash is over there so it will get the ipfs try to fetch it from ipfs and also the metadata file has the source hashes source ipfs hashes it will try to get the source files from ipfs as well if it finds them then it will automatically compile and try to verify the contract so that means actually the Second Step here the source code verification on source5 has become publishing your metadata so you don't even have to take the extra step and go to sourcify and verify your contract if you just publish it and pin it on ipfs we'll just do it for you yeah um so we have this contract repo of all verified contracts it is served over HTTP and ipfs under report.sourcify Dev um so we pin the verified Source verified contract source files and the metadata so that they will be accessible by decoding the bytecode so here remember there is the ipfs hash here and anyone if it's verified on source5 we will be pinning it and there are other people uh pinning our repo as well so you they will be accessible by their ipfs hash and yeah we also served repo under an ipns name so you can also see the contract repo and see the files access all of them download the whole repo if you want so yeah uh okay so we have seen what you can do as a smart contract Dev let's see what you can do as a wallet developer so maybe a short recap what we are trying to do again so we have a contract call we are talking to a contract and instead of showing instead of this byte string we want to show something more user friendly so one thing to do is obviously to decode this call the despite string call via the API Json you can show the function name the variable names Etc and then you want to show some human readable description of what the user is trying to do if you have documented your code well um so what you what you can do as a vault developer you go to sourcify you report that sourcefy Dev you get a chain ID track and the metadata no please don't do that you don't come to us because it's already on ipfs and it is the neat thing is it is content content address so you know the file you're getting is actually the right file so you just your wallet just gets the bytecode of the contract decodes the ipfs hash here at the end of the bytecode fetch the metadata that we pinned it for you and the metadata file has as we have seen the API and the documentation yeah this is where the first two Fields come in handy how to interface with the contract and then decode the API and populate the nutspec commands of the track so hopefully at the end of today we will have something more on the right rather than something on the left but source file is actually not the only way for human friendliness the idea behind sourceify is to have human readable descriptions via nut spec comments found in the metadata there are other ways to achieve this as well so one is for example these two eips by Richard Moore and Nick Johnson so the idea there is to have an extra function name an extra function a described function so to say that will return the user something unreadable so it can be anything any custom any custom string and uh the contract will return the string to the user and continue executing the executing the actual function and here the nice thing is it can decode things like ens commit that is normally uh like it's it's a hash commit and it doesn't have a meaning to the user but you can add actually some more custom strings custom messages to the user that they can make sense of it but obviously this discuss extra gas uh the other one is the other MP proposal by Dan Finley the there the idea is uh to give the user this information at the first point of contact so say you want to do an exchange for the first time in at uniswap uniswap app.uniswap will give you the contract metadata your Vault will store it and then uh you your wallet will have the API and the describers so that it can show you some something more human readable uh the advantage here is it's backwards compatible so we don't need to change change the contracts right now most of the contracts don't have any documentation or anything and or they are not actually documented with the human friendliness in mind so this will be backwards compatible but at the same time this means it's mutable so it's like it can be changed uh so it's a trade-off but yeah and also there are many ways to better ux so we can actually show the users many things so is it you can decode the contract call you can warn if the user has never been talked to this contract uh show the user if the contract is verified uh block if it's a scam address many things as well as other types of things such as how many times this contract was interacted with when was it deployed because a scam the contract would be like likely more recent and less interacted with um is this contract audited and is it by audit by whom so there are actually many ways we can do better so as a recap what is sourceify technically it is an open source automatic smart contract verification service our monitor it's a user interface server API and tooling to verify contracts manually it's a public decentralized content address storage of verified contracts our repo and more generally we are a base layer and a public good for other tools to build on top of us and we are an initiative to Foster the use of solidity metadata nutspack and full verification and as well as we are an ongoing effort to improve smart contract ux safety and transparency so thank you for listening uh if you have if you're interested you can find us in Twitter join our Matrix chats uh our code is also at here it is a term sourceify visit our website and yeah I'll be happy to take any other questions if you have thank you [Applause] hello thanks very much for your talk I'm wondering how well these Solutions can handle translation and internationalizations to provide descriptions in multiple languages yeah uh that's also one consideration there uh we have the idea of maybe having a custom nuts Big Field for translations and in that field you can actually link to another translations file so that would be inside the metadata for example the translations file and that will be another ipfs hash so that you can fetch it and you can have other languages and translations there's one over there foreign are you thinking about ux regression testing automation somehow sorry are you thinking about end-to-end regression testing automation with sourceify so being able to include the depths in the whole cycle of testing how things look like like what can the aptitude to make things easier for end users to so having I'm sorry is it having the sourceify at the whole development pipeline you mean or I'm not sure if I get customers correct um like currently in order for a developer to somehow test the end-to-end user experience it's nearly impossible to include the web app and the wallet interaction and the onshine interactions I'm not sure if I get the question correctly but we are more like a um well so we're not we just say people here it is here are the tools here are the files just please make use of it so we don't actively get involved in the ux contract interactions or that's not not in your complete pipeline I would say thank you hi um for user protection am I right in thinking that the reputational and statistical characteristics you mentioned are really important um right here because I'm just thinking and correct me if I'm wrong but I'm malicious deployer could create a malicious contract this uh describe it in a malicious way with Matt Speck and then yeah take advantage of the user yeah um I mean obviously we have the assumption that the contract deployer is uh nine it's not malicious so it's uh we verify the content of the uh the contract but there are as I said there are other ways to do that for example audits scam lists so this is another aspect so we have like this neutral eye to what's inside the contract and it's up to the community and the other types of methods to actually see it's not a malicious contract cool uh hi uh thanks for the talk uh so question I had was for contract coverage like what is it like is it limited to what's verified on ether scan or um like I like yeah in terms of like abis that might not be fully complete like what is sourceify fully covered what what do you mean by cover in terms of like contract coverage like if someone deploys a new contract it's not verified on either scan then do you guys still provide the human readable aspect or yes I mean source file is a completely different thing than etherscan etherscan is both a verification service and a block Explorer but source file is not a block Explorer we just we are just a contract verification service and I would say we have different contract sets than it can so you can actually import but it's a different contract set so even for like new contracts you guys instantly get into the pipeline verify and then make it readable yeah exactly gotcha any contract that is deployed we can just verify gotcha okay and even also different chains like we don't we have I think like 30 something evm chains right now so you can even verify contracts on other chains but at the end of the day actually we we have support for different chains but we actually want everyone at some point to run their own sourcify for their own chains cool okay thank you maybe last question um is it in the scope of sortsify to like maintain reputation of the commands or maybe even validate the comments actually reflect the code like sound here is not so good uh is it in this clock of source if I to uh maintain like a reputation scope for the comments or to validate if the comments reflect the code the application comments you say yeah the comments comments um I mean no we just as I said we are a tool to achieve this and at the end of the day the developers have to document and comment their code so they have to keep in mind that this will this might be a user facing method and properly document it thanks yeah um I think we're out of time just find me after the talk or also you can yeah find sourceify and reach us out there thank you [Applause] hey thank you so much Khan um I think we are actually running 10 minutes early so we are trying to delay that again a little bit so that people who are actually coming for the talk that are coming for are here on time so first of all I'm just going to talk a little bit so that we are not running 10 minutes early anymore and uh secondly I see that there are many people in the back feel free to also move uh to the front there's a seating space available here and yeah to basically bridge between the two talks that we've just had and that we will have next moving from solidity metadata and contract verification we now advance for the future so and welcome Daniel to the stage big round of applause for him please welcome him [Applause] is that working yeah sounds like it okay yeah hi I'm Daniel I'm in the compiler team for yeah over four years now and yeah originally the plan was for the store to be held by Chris who couldn't make it to Bogota so I have to improvise a bit I brought a lot of code Snippets which will explain what we have currently in the compiler currently the state of the language and where we are headed with this so yeah let me just dive directly into it yeah I guess you if you are familiar with Trinity which I hope most of you are because it was kind of like kind of physical requirement for the talk uh you probably know a new doubles by now it's been around for for a while and it's a very highly appreciated feature from uh our impression which yeah I still will quickly explain how they work that's the owner of arrival here is looks like a state variable but it's not a really proper State variable in storage but a mutable variable which can only be assigned to Once In The Constructor and then can use this many times as one once read only in the runtime code of the contract but there it's the access is very cheap it doesn't require an S load but it's basically inlined in the uh code as if it was a literal so yeah you'll probably know this and people liked it a lot and an apparent and a very obvious extension they asked for is why do we only support that for Value types that's the Restriction we had so far that it's only integers on the addresses but not arrays of things for example for example so this was for example it's taken from a GitHub issue that a thing opens Apple in open but that's to yeah ask for an array of immutables which would then yeah be initialized by index accessing and otherwise work just as as one is used with immutables and we're working towards that end but there are some issues with that so I mean we have this assignment here where randomly assigned uh into uh elements of this array and in general I cannot check whether this is really assigned only once I mean the question is is there still an immutable thing in that sense also if I start having a race as immutables I will want to have local references to them which I could then reassign so if I keep the name immutable I just change what this points to so this is really it doesn't look immutable immutable it doesn't fit this concept anymore probably so let's yeah let me explain a bit further how Universe actually work in the Constructor owner is actually a position in memory so we store whatever you write to this variable in memory and then when the actual runtime code of the of the contract part of deploy is copy the memory to be returned by The Constructor we fill in from this network location into the byte code the value that is there which in the end results in the runtime code to actually have it as a literal in the bytecode so that's variable in the Constructor actually is a memory reliable in runtime it's actually something that lives in code but filling in literal values in some so basically the push argument into the bytecode won't work anymore for dynamic types if we have statically sized arrays they could still do that and we probably will for efficiency reasons but at least for dynamic types to go full way we cannot do that anymore because we don't know the length of the thing so we can't reserve space invite code for that so instead we need to rely on code copy and yeah I already mentioned we will probably want to pass the immutables by around by reference we will want to slice them which if you think about all of that together makes immutables not an annotation for a safe arrival anymore but it will become a proper whole data location so this will probably look like this in the end as I said they arrive in the end we live in code so code and the op code that will access it is code copy so code is a natural name for these things so here are the data arrival that bites the dynamic array in code which I can then in the Constructor just treat like any old memory variable assigned to it modify it freely you can drop the requirement that it's only written to Once In The Constructor because that was always an artificial requirement so if we actually yeah call it but it is a code variable that will actually be inserted or used as in the code in the end we can freely modify it and then in the runtime code it basically behaves like a call data reference a read-only reference only that it doesn't come from call data but from code so I can slice it have local references to it and pass it around the functions all with very little cost this will be a bit tricky to write to type check in the end because yeah in the Constructor uh it is a memory variable in the runtime code apis differently so if the Constructor calls functions we need to type check everything twice but we have different costs for refract factor in the type Checker to actually do that as well so we will do that and there's still some considerations of gas it would be very handy if I had a code load opcode similar to a call data output to actually read single words from code but we need to refactor the type Checker to have VMware flexible for this we will need that as well and yeah maybe we will also have linear types which is how rust's follow Checker is constructed but yeah I don't know have some examples with some made up syntax and some made up constructs but I'm going to say this is all early research stage so we have nothing fixed here yet I'm just telling you where we're headed and what we want to do in the next however long it will take so yeah the go-to down-to-earth example of a use of generics is something like a resizable array some container which yeah in this case it's just an array that if you want to append something to it and it the array is already full you just reallocate with twice the size copy things over and yeah otherwise you can just add the element to the end of the array and yeah then we can also Define an index access as a user-defined operator which will then have this work as expected and yeah a length field could be added to that all defined in language which has the advantage we had often the request to allow slicing for memory types which we can't because the representation I'll show here doesn't allow it since we expect the size to be at the first memory offset where the memory pointer that is the representation type of this points to is the size uh we can just slice away from the first element for that to work memory arrays would have to work different similarly to call data types there you have offset and size on stack it would be a huge effort for us currently to change the entire compiler to come to change the representation of memory types if we had things defined like this it's minimal changes I can just say now the memory array is defined as a tuple of Stack slots one of them is pointer to data area the other so it's the size and yeah have a similar definition as before slide changes but uh a few Source changes in our standard Library would then be the same as changing the entire layout of uh of memory types which would be yeah month of month of work why we maintained uh everything hard-coded in the compiler of course it's all the disadvantages if we actually keep this extremely generic like that we will lose semantic information that will actually make memory optimization harder because yeah but the compiler sees it's just a bunch of Stack slots there's no idea that that's actually memory areas we're talking about that are allocated may only be allocated temporarily and stuff like that so I'm not sure whether we will actually go this far and if so we would maybe probably do this in a yeah compiler internal manner where we will still assume certain semantic properties about these functions without supporting similar optimizations that we can do if we know what these things do uh in their random user code but yeah why did I say stack slot all the time that's yeah maybe obvious but uh just to mention the stack slot would be the one primitive type maybe apart from uh from product types and even the basic uh integer types size initial types we have right now can be defined generically just yeah like the other cases we had so I mean this will really reduce the footprint you can hear could also then distinguish between types that are checked arithmetic and unchecked arithmetic by having very few functions that are generally written and yeah so yeah that's not to give uh to make you expect this to happen too soon we are still in early design phase for generics there's a lot of questions this is a very complex thing to do and a very dangerous thing to do because yeah all of this needs to be logically well defined to not bite in the back in the end so I mean we will take some time to design this properly and syntax is also a question that's very much a differing opinions on how the Syntax for these kinds of things will end up being I'm not that concerned by that now I would first want to get the semantics right but eventually we also need a good Syntax for it and yeah what I just said we need to decide what to do with this trade-off between making this the language really self-defined in the in the very deepest sense or to have some fixed functions which are fixed in the compiler which means we can assume that semantics or there are compromises between that but we'll see so yeah to summarize uh what I was talking about and what I wasn't talking about and we will still do hopefully we will in the future try to allow more pre-competation either in the Constructor but the code data location or in compiler term by compiler constant expression evaluation which is something yeah a lot of people have asked for and which obviously makes it easier to write things in that you don't need for example magic constants embedded in a contract or whatever because you can uh compute them on the fly without it costing and he had a huge a huge topic for the future will be to make the language extensible and self-defining by means of improving user-defined data types pushing the standard library and making a move for generics but we also of course can't just ignore is that we're still wasting a lot of memory I mean whoever has used memory and solidity will know that yeah we basically don't free memory which for a long time wasn't the main concern but in the meantime it's very huge pain point in uh yeah for cost of contracts there were several uh approaches we discussed so far for improving the situation there we a long time we wanted to deal with this on the viewer level it turns out that may not be as simple so maybe we will move actually to analyze the solidity which has the properties right there we shied our way of doing that for facility being the more complex thing to analyze but maybe it's fine we will see and yeah we will also of course try to move completely towards via our core generation but we have some burdens there to overcome still like the performance of the optimizer better tooling support you still need to Define got the background data for the tooling tooling to consume to actually make the experience as nice as with the Legacy code generation the details there would be that the tooling expects certain patterns to remain in the in the bytecode in the end whereas the new optimization pipeline will mess them up by optimizing better but tooling needs to understand that we need to Output data for it being able to understand that but yeah that's hopefully you've ever had it and yeah I'll close with that if you want to give us any feedback help us with designing generics or criticize what we are planning and saying we are crazy to do with any of that reach out to us that's the channels we've just said and yeah thank you [Applause] okay that took a bit longer than I had hoped for but we have time for a few questions at least foreign yeah I would have assumed you knew but let me explain that uh yeah it used to be the case that the compiler has two back-end paths at the moment so I mean it used to be the case that solidity was directly translated to evm bytecode uh and then only of the Omni optimizations that took place were uh uh on the bytecode level for the past years we have uh moved away from that and have a different new code generation pipeline that translates solidity first into yield into an intermediate language which preserves some structure and which allows for more complex optimizations for inlining more analysis uh and then only to uh translate Yule to evm bytecode as a second step which can reduce gas costs significantly in some cases in some cases it's the same as before but uh yeah and yeah the new pipeline the Via IRS via the intermediate representation so compilation bio okay another question I'm also not sure whether it's still ahead of time and I can okay thank you for the talk you mentioned generics I'm wondering if you could speak to how you're planning on implementing that whether you're going through monomorphization because I worry that the code contract size will balloon if you start you know doing the C plus plus style duplication of implementations or if there's some you know uniform representations you can do Allah o camel or you know Java I think there's not much you can do actually I mean the generics of C plus are different in the sense that they're analyzed differently and you only get errors in uh on instantiations but we will still need to instantiate uh and generate code for each specific case but that's not worth a worse than what you get now what you get now is writing by hand for different types different functions that would end up separately in by code so yeah it's not nothing is worse than that that's a duplication in code and in byte code if we have generics we at least only have it in source in only in the background uh hey hi Dan so my question is related to that question regarding genetics but from a different aspect so type system I understand but once we get generics into solidity wouldn't the developer have to focus on 10 more things instead of focusing on writing business logic uh a good question uh I would I would think that the uh yeah down to earth go to smart contract writer will not bother with this it's mainly something for us for defining a senate library and for people writing libraries to support smart contract developer first so I mean uh the language supporting generics and having generic types doesn't force you to use them and it doesn't mean that anybody has to use them but it will make the language the the evolution of a language much uh faster and more streamlined we can in the future ask people if they propose a feature to just implement it in a standard Library way and then standardize it in the end if it works out which will yeah have all the advantages this one can think about that but for user code for the in smart or contract code the difference is not that large probably I can go I am going if the man Lambda functions because it saves a lot of code all right are you going to implement Lambda functions just with this equation uh definitely eventually I would say I'm not sure whether I mean getting the basic type system uh going and all that will already take some time but yeah eventually this is of course something that will make things easier to read easier to write and are beautiful so and maybe not I mean as long as you don't want these things to capture variables it's easy capturing even maybe something at some point we can also consider but yeah not that's not the first thing we will do but eventually have you been thinking about integrating on trying computation and off-term computation in the one source file this is of course Very problematic but in some moment in future we might need it not necessarily in solidity but maybe you should try to think about that yeah I mean on the solidity level we don't yet uh interact that much with yeah often accommodations Layer Two stuff or whatever but yeah we are aware that we need to interact with that and support that where we can you know sure okay I think okay okay um I think you mentioned something about the performance of via AR uh did you mean the how long it takes to actually use it uh yeah I mean the compilation time yeah computation time is kind of like 10 times or even worse in some complex contracts exactly anyone that's used via AR today has experienced that so how much of an improvement do you think we are going to see I mean uh so far the Via our compilation pipeline has not been written with any performance considerations in mind at all if you've written it for correctness first and only now are starting to yeah realize how bad that got and uh that we need to do something about performance there so I could imagine that we can get quite a way but yeah it's hard to tell before we're actually doing it um regarding generics um how much thought I was put into the auditability for external code Auditors will it improve the story make it worse more training can they forget stuff I think it will actually improve things I mean uh we will be able to have the standard Library definitions of all the built-in functionality which can be exported which can be analyzed I think at the point where we have generics going in a standard Library going we will actually at some point not promising that happening soon either but at some point be able to Define a form of semantics for the core language that remains which can actually help hormonal verification a lot and things like that so I think reducing the language core that is built in and hidden in the compiler is actually a good thing for a formal education and auditing okay it's that more time from like for questions okay then we still have some time if there is any more yeah uh you were you mentioned that the data location is going to be coming part of the type instead of being associated with the variable does that mean that we are going to be able to start writing things like an in-memory array of storage pointers or a in-storage array of code data pointers because those are all well-formed but something like a storage array of call data pointers makes no sense right so what does the well-formedness look like how would that how does that sort of you see that restricting that I mean uh first part of the question yes this is what this means this is what will be allowed second part of course there are invalid combinations there are combinations that don't make sense which then the type is simply reject okay I okay then thank you again [Applause] thank you Daniel from the solidity compiler team we are staying in the space of solidity and moving a little bit towards upgradeability next up I'd like to introduce our next speaker Alejandro Santander he is the creator of the ethernet CTF game and founder of the ethernet Dao if you are a developer and interested in that stuff I really highly recommend you checking out this game and also the ethernet Dao they have lots of good um solidity tips and tricks on their Twitter account and today he will be talking about Alison proxy land um he'll walk through the struggle of creating upgradable smart contracts and different proxy architectures so big round of applause for Alejandro [Applause] for coming to the talk um so yeah I'm I've been a a salute the auditor and open sapling then I worked uh with Aragon for a bit and now I'm working with synthetics mainly addressing the problem of how do you build a super complex smart contract system in a way that you can iterate through it and fix bugs and and improve an experiment and other than that I I consider myself a bit of a an educator in the space I just it's not that I know a lot of things I I just love to empower other people with with knowledge right so what we're going to do today is talk about proxies and like in general and then we're gonna talk about a pretty sophisticated uh type of proxy that we're using in synthetics so I think that it's critical for for everyone to understand how proxies work under the hood I think that it's not okay for a Dev to use proxies and not know how they work but the good news is it's that they're really easy like in the end there's like no mystery it's very easy to demystify it right so I I insist uh besides the proxy I'm going to show I think the the essence of this talk is to to promote the awareness of how proxies work and if you're going to use one like make sure you understand how they work so to illustrate this we're going to play them like we're going to start with a with a very simple contract and make it upgradable and see what happens so this is the contract I don't know if it's a good idea to bring in code to a talk but I don't know if you can see it but it's just a contract that sets a value right it has a function set value you can set the value and it records who said the value it's message sender and emits an event that's it so this this contract is deployed at the 0x1 address right and then after deploying it Bob calls uh set value 42 right then 42 is set in storage slot zero because the variable is declared it's the first variable declared and uh zero X Bob which is the address of Bob uh gets stored in the second slot which is slot one okay that's how solidity lays out storage automatically when you declare variables in a contract and an event is emitted from 0x1 now they decide to make their contract upgradable all right how does this work they they decide to deploy a proxy which is just a function a contract that has a function you can tell it what the current implementation is which is going to be the other contract and then it has a like a magic assembly function which forwards anything using call to the implementation contract so this is deployed at 0x2 then Bob calls set implementation 0x1 and in the proxy storage it's not zero now holds 0x1 because the address implementation variable is declared in slot it's the first one declared so it's saved in slot zero then they're now connected this is the proxy and this is implementation and Bob calls value right it forwards the call to that uh implicit getter that solid generates and everything's return 32 which is expected right yeah now uh Bob called set value one three three seven let's see what happens uh it gets forwarded using call to the set value function in the implementation and it affects the storage of the implementation not of the proxy and it stores one three three seven in slot zero and 0x2 installed uh one that's weird right that's message sender so the the problem that we have here is that call makes the execution context to be the implementation not the proxy like the event is submitted from the from the implementation which is also not a good thing right because you don't want to have a protocol and and tell people like to be changing addresses every time you update the implementation right so the problem that we have with this particular proxy that uses call is that the execution context is here right and we don't want that so what is an execution context is when you run code basically uh what determines uh which storage uh space to use uh who message sender is and where emits come out from right um there's more to it but that's like pretty much it so how can we take the proxy the execution context to the proxy we just need to use delegate call so call wants the code that we're running in the current context and delegate call runs the code in the context of the caller right so here we have a second proxy right which is the only difference is that it uses delicate call right it's deployed at 03 0x3 it's connected to the implementation the same one as before and now when Bob calls set value one three three seven it uses delay core so the execution context is this right we are using the storage space of the proxy which is good the event is coming from the proxy which is good uh now Bob calls value like the getter right the execution context is still that it's fine now this is going to delegate call to whatever is stored in the implementation and the implementation is in slot zero and the value now holds one three three seven right so what are we delegating call to to some contract at one three three seven which there's probably nothing there right so we just we have a storage Collision right we overwrote the address of the implementation with a number right so we basically bricked this proxy right so daily call is awesome but it is dangerous because you have storage collisions so to solve this Bob goes to the next level and and these structures the proxy storage so what is this structuring it's basically choosing where to put to put to store something that's it it's not using solidity's Uh custom slots but just choosing where you put it um so for solidity first variable is it zero second at one Etc and you have infinite slots destruction is just picking a custom slot right so we have the third proxy here is called unstructured proxy and the difference is that we are not using solidities like regular storage slots but we are declaring where we store things at slot 1000 right and using that the the code looks a little bit weirder but that's it so we deploy these proxies he works for we connect it to implementation oh this is important can you see the storage it's the implementation address is stored at the custom slot of 1000. so that's destructured right now so now we call set value it makes a delegate call the execution context is that we write the new value values but they don't like step over let's say the implementation address right and the event comes from from the proxy which is fine so we have a proxy that works right and now we can upgrade the implementation because we we know that it works so we have value holder V2 the only difference is that we added a new variable called date right just added it on top uh and just whenever someone sets the value we also record record when that happened right so now we connect the implementation we call Value right a delegate calls and value is whatever is stored at slot one right according to solidity storage layout and at one we have coax pop so another problem um this is another type of storage Collision we shifted the the implementation storage uh and we have a collision between versions of the implementation right we have income incompatible storage layout so Paul understands that to to avoid this you in an implementation you you only append to the storage instead of like putting it anywhere right so he uh moves the date variable to the end of the previous storage layout um and that's it that's pretty a pretty simple fix that's another rule of like using proxies always append to um and now this value is gonna get whatever is stored at slot zero which is one three three seven so it's fine so we we avoided that Collision so storage collisions it's it's critical to understand when they occur and it's basically the two types of collisions that I just showed you that's it if you get that you you can pretty much think about any type of collision things to consider the execution context is always the proxy so everything is stored in the in the proxy um there's two types of collisions that we just talked about um the first kind can be avoided by unstructuring uh the storage layout and the the second one can be avoided by just making sure that the uh updates to the storage layout and implementations is valid always append and something to consider and this is critical multiple inheritance um flattens your contracts so you cannot protect the storage layout so you can add a new inherited contract to you to your like super contract and it can add like five new variables in an unpredicted uh part of the layout uh part of the layout so it becomes hard to detect when you have invalid storage mutations and you need to use tooling if you use proxies I see a lot of people using proxies and not using tooling and it's not a good idea and even if you detect them and this is not something that tooling fixes it can be very hard hard to avoid the invalid mutation like if you have a inherited contract and it it causes this Collision to avoid it you you have to do some weird stuff right so why not use that technique of unstructuring on everything not just the proxy storage but the the implementation storage definitions so here we have value holder before the implementation B4 and the difference is that it uses the same technique that the proxy use and stores everything at slot 5000 right everything else is the same so we deploy this we connect it to our proxy which we're not changing anymore because it works um we call set value right it makes it delegate call to the implementation and we can see that we have the gray storage we're not using anymore we have the proxy name storage name space let's say there and the new implementation namespace uh here and they don't Collide right so we've unstructured the implementation storage right now so we have we're pretty happy with this proxy like configuration we we the the context is kept at the proxy and collisions are avoided using unstructured storage or storage namespace absolutely everywhere tooling should still be used to guarantee that there's like no uh storage Collision that you don't notice but the thing is that uh this like custom or manual use of storage uh makes um storage layouts much easier to control so now that we understand these like basic principles of of proxies let's talk about like what a multi-contract system looks like in in solidity so there's no ideal standard solution for multi-contract systems people often use Registries which is basically a contract that knows every other contract right and whenever a contract a needs to talk to contract B it needs to go to the registry and ask hey I want to talk to B who's B here's B and then it makes a call to contract B right and then B needs if it's a a sensitive operation needs to say okay who's calling me a is a from the system it asks the registry the registry goes yes and then okay then you can perform this it's complicated and it gets it gets uh messy pretty fast so let's let's try a pretty crazy solution which is uh we're calling it the router proxy um so we basically have a new contract which is another contract that has one variable it's called cool value and it's also using these uh this storage name space system instead of solidities like own a storage layout thingy but that's it it just records a variable right it gets the store sets the the stores value and then it's about an event so we deploy it at 0x8 and then this is the tricky part bear with me with this part uh Bob uses tooling to build a router right so this is basically a table right it has the addresses of the two contracts uh value holder and another contract hard-coded and it's a fallback function basically has to do like this binary search algorithm to determine which implementation has that function right is it valueholder or is it another contract right and it just checks the incoming selector and forwards it to the appropriate implementation and that's it and then it just makes the the regular delegate call uh proxy forwarding so this is deployed at 2x9 and then we set the router as the proxies implementation right 0x9 is the new implementation and now we have this we have the proxy over here we have the router over here and we have the implementations over here right so when Bob called said value seven you don't I don't think you guys see it but it's it's calling set value seven with the number seven it makes it really a call to the to the router and then another daily call to the to the another contact implementation because the router figures out that the the function set values in that contract let's keep in mind that the execution context is still the proxy it doesn't matter how many deleg calls you make it will always be the the the entry point so that works like it sets uh the the other contracts Uh custom slot what was 9 000 so it stores seven right there and the event is still emitted from the the execution context so if Bob wants to upgrade the system all he needs to do like here we're making a like a silly change right we're just multiplying the incoming value just by seven [Music] um this new another contract B2 is deployed at 2x10 a new router is generated by the tooling it just has the only difference is up there in in another contracts has a new address everything else is the same and Bob sets the implementation of the proxy to that new router so that's how you upgrade the any contract in your system so what would a more complex system look like maybe like this you have the main proxy right you have the different storage name spaces of that proxy and then you have the router which you keep changing every time you upgrade the system and you have the different modules that specify uh a particular behavior of your system and then you have this thing which is really cool cool because uh called makes sense because it allows intermodular Communication in a way that we we're going to see that's really efficient and really easy um so yeah we don't use any storage other than the execution context storage gas efficiency uh it's like a concern with this pattern because you're used doing two delay calls um keep in mind that transparent proxy is the ones almost everyone uses uh costs like 3 000 gas Universal proxies about 1600 and this system uh uses only uh 2 600 gas which is all right and then intermodial intermodular Communications how would a module talk to another module you could you could cast your module as the other module and just call its function because every module is this the system but the problem with this is that message sender would be lost because it's it's a call you you break the delegate call chain right so you need to delay call to the other module just the the same like self-casting mechanism but with daily code uh it works but there's something much better which is mixins which are pieces of code that know how to interact with another module storage they're like delegates for that particular part of the logic right and they're not deployed they're just inherited it's like modulate inherits a bit of the code of more UB and the nice thing is that you can tell the mix in to interact with the other module without even making a call so communication becomes super cheap let's let's see an example we have owner storage which just declares a struct with what single variable uh that mechanism to to get custom storage thoughts then we have the owner mix in which knows that storage and only has an only only owner modifier right that does the typical check right and then in owner module we inherit the mixing which gives us the only owner modifier access right and we have a getter for the owner and now we have a new version of value holder B5 right that has a single change which is we're using the only owner modifier here from another module using the owner mixing so if you want to use this uh you only have to change your code style a little bit it's kind of weird but you get used to it fast because it's simple you just need to use storage name spaces instead instead of like regular variables and yeah you get used to it should soluti do this under the hood there's a proposal from maxim4 um so it is something that the solidity team is considering uh this could be a language supported feature to have like a contract hub and yeah why use the router you don't have a contract size limitations anymore because you can just combine like I think I I tested it once with like 800 functions which is pretty crazy um it's like the router merges all the contracts into a single contract so then as we just saw we have good easy Communications between the models without having to use a registry or authentication or anything it's ideal for complex experimental systems and the other nice thing is that the router is since the addresses are hard coded it's very explicit so it's good for for governance if you want to make an update to the system you show your community like this is what we're going to change this is what the configuration of the system will look like it's not hidden in some uh Dynamic storage somewhere it's right there uh and yeah that's why we're using it as a core component of synthetics B3 because it's it's a complex system that needs to not have all the complications of intermodular communications and all that and well if you want to try it out it's a hard help plugin it's uh synthetics B3 uh hard hat router it generates the router source it manages storage name spaces for you it performs validations to ensure that there's no there are no storage collisions and that's pretty much it that's what the plugin does so thank you very much thank you um are there any questions okay hi um I just want to know if I'm missing something but this proxy router could be the same as the multi-facet proxy you know the diamond proxy but with the hardcodile implementations uh yes I I couldn't hear it perfectly but I think you are asking about the diamond proxy right yeah yeah yeah yeah this is this is based on the diamond proxy so it is that that the diamond proxy but without getting the implementation from the storage yeah but instead hardcoding them yeah basically okay it's it's some people are calling the diamond proxy a dynamic router and this one a static router and we like this one because uh for our project because it it saves storage reads because the the values are hard-coded and it's also more explicit like that what we don't like about diamonds is that you don't like if you're a Community member or whatever and you want to know what's the current composition of the system you need to query it a lot right but yeah it's the same otherwise hi uh just one question so you only use the sort of ins and addresses and stuff what happens with mappings and arrays in regards to storage Collision so can you repeat a bit louder so if you have an array yeah does that affect storage Collision well if if you declare your array or any Dynamic type inside of those storage namespace drugs then solidity is like regular storage layouts um system is used which uses unstructured storage under the hood like if you have a dynamic array the position of that array I think it's going to be it's slot say if it's 9003 because it's the third variable instruct uh the hash of that so it's going to be some other random place so it probabilistically the even though they're in in structure some things are going to be spread out but the the probability of a collision is very low like insignificant so you mentioned a couple times that we should use Tooling in order to check for storage collisions what sort of tooling do you recommend contract developers use sorry I I can't hear really really well from here I'm just going to stand there okay so in your talk you mentioned a couple times that you should be using Tooling in order to detect storage collisions what sort of tooling should developers use in order to do these checks all sort of tools okay um yeah so if you're using open settlements proxies you should use their tooling if you're using uh the router as you can see we just we didn't just offer like a solution to generate the code but we have we have a it checks your the storage layout of your entire project so in that case you can use our code right so I would say always use the tooling of whoever is providing you the code the smart contract code of the proxy um were there any attempts to solve this garage Collision thing on the evm or compiler level like to sandbox each contact for example I don't think there's a need to um I mean the way uh solidity like these structures arrays and mappings and all that it's it's theoretically impossible to to get a collision so there's no need to sandbox it the problem with collisions is when people use like a design that's not supported at a language or at a protocol layer like the evm right and they get collisions between two contracts right um so so I don't I'm not aware of any attempt at that level to avoid coalitions is it is it possible to migrate and open Zeppelin proxy to this one the route approxier what do we have to start from scratch um code wise it's pretty much the same like you can use a new Universal proxy as your entry point so you could probably just use that and the migration is the the crazy thing about this is that the the router routing occurs in the implementation right so code wise you're okay then uh storage wise right you just need to probably choose new namespacences and populate the data right or accept that your modules are going to use existing search right and make sure that new modules uh declare like a new namespace or something but yeah sure you can do it even if so if if solidity like makes this a language feature you just stop using uh generate routers and deploy a solution Hub right so it's completely future proof I think how standardized it is is proxy are you are you the only ones using it or is someone else using it in production doesn't mean first question and the second question is uh do you think it will be useful to have something in the uh like a public function in the in the proxy to share the signatures that are being used so let's say a user or someone that wants to check what is being used doesn't have to dive into the source code yep uh so standards not many right now if you deploy this you won't see anything on etherscan for example etherscan doesn't know how to interpret a proxy that has multiple implementations which is unfortunate but we're trying to solve that pretty fast it shouldn't be hard um and your second question um you would just add a module that adopts ERC 165 is it that uh just like replies it it has that function I don't remember the name of the function that gives you the entire interface of the whole system okay that's it thank you very much thank you so much Alejandro all right moving on I think this is almost our last no it's not our last talk in the developer infrastructure We are continuing with developer infrastructure um as our next speaker I would like to introduce yet again another OG Lefteris is ethereum developer since 2014 he contributed to all kinds of different projects from the solidity compiler to C plus plus ethereum to the Dao and trade and now he is founder of rodkey um an open source portfolio tracking tool today he will talk about um how we can build open source tools that help users understand evm transactions without the need of being a shadow shadowing super coder so without further Ado welcome lifters [Applause] okay uh welcome everybody oh all right that's my slides good cool magic so I'll be talking about uh basically decoding um evm transactions in an open source accessible and modular way uh uh Francis introduced me but for those of you who do not know who I am I've been in ethereum since 2014 uh worked in C plus client in the solidity compiler back then then I worked on L2 payments for many years and found it rodkey today I'm talking about user transactions and how we can start to understand them like what are they and um how can we decode them in a human readable format so everybody has used ethereum in here and we have all faced these two big problems like you do not know um when you have a transaction you do not know how to get it like there is no built-in way to get transactions for an address you need to utilize some kind of third-party service and that will never be decentralized also once you have these transactions you really don't know what I mean there is a complete lack of a universal tool that will decode a transaction to human readable format there is some third-party services but again they are centralized and they tend to be protocol specific um so what does the transaction look like everybody has opened Network and sees this you know hex block there is no metadata there is no human readable info that's what every new user to ethereum is greeted with there is no way to understand what um your transaction like three four years ago has done there are ways we are no longer in 2014 there are ways to gain insights so you can see stuff in the etherscan with the graph and other centralized apis such as covalent Morales Etc um so we have all used others can what are the ways to um decode the transaction in the understanding is that it looks like this is a complicated transaction that does swaps over multiple protocols um it's easy to use you just type in other scan you know transactions last and it gives you useful insight and it's totally free uh of course it there are Cons with other kind of centralized it's proprietary and closer that means that you there is no way for me as a developer to see how they do what they do or extend it in any way um they know everything about you I mean I know the guys from understand they are good guys but you never know who in there could be malicious uh they can Maasai peace to your addresses and know who owns what address and where they're located and it actually does not decode everything we've all seen that there are transactions that data can doesn't like have insights for and there are other tools that actually can do this um then there's the graph the graph um is a way it's kind of an indexer per protocol you they have sub graphs and they index the chain for a particular protocol and the engine query this index and get any insight you want um the cons so the process is that it's very good for single protocol data so for example here we have the other V2 theorem by massari um this is a subgraph that say the other guys can run in their interface and query any information they want for their protocol but it has many many disadvantages it needs payment per query this is the revision that everybody should be paying for every query uh it's built for single protocol data so they subgraphs so there is no generic solution if you have a portfolio tracker or a wallet or something that wants to decode every single transaction no matter what protocol it is in subclass will not work you will have to basically query every single subgraph and create a subgraph for everything that doesn't is unsupported yet um and it does not work with the local apps what I like to call true dabs so basically uh when the company that makes the application hosts um uh host the code they can have an API key and pay for the queries but when it's a local application there is no way for this to to happen and there are these other centralized apis like covalent Morales and Alchemy they're easy to use they have pretty cool apis um which can decode transactions uh give you all the transactions but the same Concepts errors can apply like it's centralized it knows everything about you it's proprietary so you cannot extend it it's not modular at all um so having seen the ways that we can get insights about data let's see how we can actually get accurate historical data and let's go to the original scene of ethereum like this is absolutely bonkers if any of you has tried to get the history of transactions for your address you will find out very easily that there is absolutely no building way to do this for um for ethereum there is no RPC method this is all due to the way that evm works and how the clients are built but it is really like it's absolutely crazy that there is no way for you as a user to get all of the transactions for your address someone that comes as a developer outside from web3 and comes to ethereum and sees this they think that we are we're just crazy that this is broken uh it's it's not all gloom and Dumbo there are ways to do this effort can again comes to the rescue they have many apis and if you combine free I think so this one for for transactions then there is one for reality transfer and one for nfts if you combine all of them you get a pretty accurate picture of what transactions your address has done of course it has Dropbox right it doesn't detect all others appearances it's rate limited but you can pay for bigger limits but rate limiting means that it takes time for your query to actually work it is centralized so it can go down they can cut access to the API or um they can do what I said before that they can monitor you and map IP to your address the truly decentralized way to go around this is something by my friend Thomas de Ross It's called true blocks it is really the best and most complete way to get transaction data through blocks it takes all appearances of an address really I have seen demos um where TJ basically shows etherscan and then Compares it with true blocks and you can see that for some addresses through box does indeed detect more appearances than others can it is decentralized so it runs on top of your local node so you do not need to to do any other network queries it's super fast like it's really like milliseconds or seconds depending on the amount of addresses that you query for uh and it's built to share the this index with others of course like like if everything is software there is drawbacks um it is hard to set up so TJ is like who's a long wolf since I met him in Shanghai in 2016. now he has a bit bigger team but it's it takes time to build something that's easy to use by others it does require a local node so you need to be running an ergon node I think I'm not sure if it works with others and of course you require through blocks itself to create the index um so building on this I would like to like present what I try to call the stack of 3D centralization which is something that we should be driving striving for in crypto so everybody should try to run their own node so something like a tab node or a Raspberry Pi with whatever setup you guys want to have um you should run your own client right like for whatever chain you have run a client for that saying that you want to um to use and triples actually works for all chains all evm chains so you can have an index that like Roblox on top that will index attain and provide you an answer to the question how the heck um so what address is uh sorry what other sizes does my address have and on top of it all to come and bind it all um you have the aggregating and decoding level that something like rotkey but not uh um like a decoding so Roti right now is a an application but imagine a platform where you can have a generic way to uh go from transactions to a common readable format for what they do and it's actually consumable by humans so um going from how we get data to what actually would go into this decoding platform that was that my talk is about so once you have all the data like either from method or from True blocks what kind of data is this um if you have tried to play with understanding transaction history of ethereum you know that there is two ways to get data it's either a trace a transaction trace or transactionary seats um there are two kinds of traces one is the gift style trace and the other is a parity Trace I'm gonna go through them a bit fast for those of you who do not know what they are so give style Trace is the tracing that comes with the git client um when a transaction happens it touches a multi so it touches multiple contracts right so you make a transaction to a contract and this one may make a call to another contract and so on and so forth and as they do this they touch the state of these contracts and they they make some changes so this is what the trace of the transaction is and the give style Trace is the most uh completely like super detailed it has every single step of the execution with um the op code the program counter the storage diff Etc it's super detailed it's very hard to use because for complicated transactions you have like a huge thing that you don't really know what it does and it can grow extremely huge like in the gigabytes for really complicated transactions then we have the parity style Trace which comes from the now defunct parity client but it is used I think Aragon for sure another mind and maybe maybe better I'm not sure um they have three commands one is a trace that's pretty cool and useful um it gives you a call stack like this of what did your transaction do there like the cold trace of the transaction and this actually does not require an archive node by the way the screenshots is from um a very nice article by bantag on traces that came out like two months ago I think so Google like bunt against transaction traces and you will um uh you can read about it in more detail and the other thing that you can use to understand how a transaction um what has it done is the parity style Trace div and this gives you really oops sorry let's go back it gives you a state the for each account that you touch it gives you the difference in Balance code nodes and storage the cool thing is here if you have the API you can play with it a bit and you can have readable names for the storage slots and how did they change so this is a very useful Insight on what did the transaction do and of course then transaction receipts um we've all um seen like the how a shortcut of a contract looks like they have events these events are actually contained in something that's called the transactionary seat so let's say for a token transfer it's a I don't know like transfer Source destination and value or something um almost everything generates them um it looks like like this this it's all hex but if you have the ABI of the contract you can decode this into a human audible format um so uh this is how you gather that data but gathering this data is actually expensive it takes time um and exactly because this is expensive in um in resources persistence is key so any kind of platform that you create and the thing that we have created the draft key needs to have data persistence um you can choose various ways we've gone with a simple sqlite database for now but um this way when you have gathered all of the data and you know that they are true and will not change then um you can just take it out of the database and reuse it instead of having to re-query again through blocks or etherscan or make a trace again so we talked about uh where you can get data how to get it and then I'm going to go to the mid of the presentation which is the decoders themselves so we want to get human readable format of the transaction data yes of the transaction data so we have gotten either receipts or traces and with this Amazing Escape graphic that I made guys I'm really proud of myself for this we can see that um you can check for its um receipt for its log the address and then send it depending on the address to uh either the generic erc20 transfer decoder if it's a uni swap swap to the units of decoder and so on and so forth like Ave Etc and all of this will at the end emit a common uh event format what's more uh some decoders feed data to other decoders so oh I have to get used to reusing this the esc-20 transfers create the rc20 transfer event and this gets fed to unisop which translates it into swabs um so yeah I started decoder platform is made on modularity uh this is um rodky's repo and this is where we have all the decoders and it's like a huge list it doesn't have all of them um because they don't fit on the screenshot but the idea is that it's easy to write easy to use and drag and drop that uses drag so you drop it in there and it's caught by the the system and then a new decoder is taking into account whenever we um we we decode a transaction that's the idea uh we're not there yet we build binaries and uh this is not as modular as it should be but the idea is that it should just um big drag and drop um this is how the source code looks like uh this is I think a hop protocol two bridge to another chain so um it's hard to read probably but the idea is that um you get from the erc20 transfer decoder you get the erc20 transfer and then you see oh it's a spend the counterparty is the F bits of uh hop and the asset of fmatism the amount matches so then we transform it into the common event format for uh hop and give it a nice suitable um um explanation which like breeds the amount of if to um either your own address or to some other address in the chain so the name of the chain via hop protocol uh I talk a lot about the common event format it's kind of a POC because we are we are only consumer right now uh it's uh changing so this is how it looks in the code um it has like a sequence index inside the transaction so where did it happen in the transaction timestamp location location is mostly something that we use in rotkey because we subtract everything into this format not only ethereum transactions but pure Kraken trades uh your ethereum stating everything gets subtracted into this less common denominator format um we have the history type A History event type and history event subtype so this is what defines the mid of how you define um an event the asset and the balance change and then some extra stuff like the location label is along the counterparty is if I I send it from me to someone or if I got something else and some extra data like if it's a CDP for maker we have the CTP ID here Etc um as I said everything is broken down into this thing uh like a swap is three of these events so it's a amount out amount in and fee out or it can be two if there is no fee uh we are working on this uh it's not final but this is like the idea uh this is how a front-end can consume and solve this um code um this should be read unfortunately because uh I didn't take a nice cases from the bottom to the top so I claim my Badger airdrop so it has two events the gas fee that's burnt and the airdrop claiming then approval to one inch V2 and the gas fee for this and then the swap for uh in one ends for basically immediately dumping the the tokens this is the same thing that you saw in the in the previous in the previous screenshot basically these events each one of them is one line here So This Is How They are consumed in in rotkey itself which is a portfolic running application um so uh I would like to actually also talk a little bit about the abstraction layer of of the vision of rodkey which is like if you take this a step further and not just focus on uh just evm code but anything like any event you can get into like an open source middleware that offers an abstraction layer for everything accounts balances p l over multiple protocols and jurisdictions so this is kind of the vision that we would like at some point to go with rodkey um so we went from a portfolio turning up to a common evm decoder and now more towards a middleware that would offer an abstraction for everything in in accounting for crypto why right like people would ask why the heck why would you need this because everybody is Reinventing the wheel uh there is again as I said never in protocols different uh exchanges chains uh jurisdictions it's impossible to keep up I have talked I have spoken with people in uh both small startups and big names in the field for portfolio trading and crypto accounting everybody's saying that this is just too much to give up and maintaining just one module is a full-time job um so I believe that there is a solution to this problem so the problem of everybody Reinventing the wheel has a solution that um we can have an open source uh platform middleware if you want maintained by a core team but with contributors from the entire industry and used by multiple projects and for the problem of different protocols and jurisdictions and being it impossible for a single organization to keep up I would like to propose a solution that we can have people incentivized from each stain and protocol with appropriate know how to come and Implement modules in this platform again my amazing inkscape skills um imagine a middleware where you have like someone who wants to use um to do Port authoritarian accounting has um the the core uses Bitcoin and ethereum plugs in the Yen module the other module he also wants to do accounting plugs in the accounting module and because he's in Germany he plugs in the German accounting uh with fifo multiple Depot and early for accounting methods uh and imagine this middleware basically being used by many people in the field and um in the end just everything plugging into this because it's better to use a common open source uh middleware rather than every single application Reinventing the wheel uh any such platform would have some super basic requirements it needs to be open source like everybody does tries to reinvent the wheel the proprietary closest way this is absolutely idiotic it needs to have a modular architecture so that as we saw before like be pluggable have pluggable modules um you you're in a different country than Germany you can just plug the I don't know Netherlands accounting module you don't use um ethereum you use kusama you do the substrate uh module and you can do all the polka.com Etc it needs to have a this is a hard requirement to achieve but it needs to be multilingual it needs to have multilingual bindings because we attract your python uh house we know how to use other languages but most of our code is in Python such a middleware should not uh limit the user to so we cannot ask the entire industry to spot to python if they are to to use such a thing the platform should be built in a multilingual way um and as for incentivization the creators and maintenance or modules should be incentivized to actually contribute to this platform if it becomes an open source standard then everybody should be like oh wait I mean we made this new platform uh we need to write about the module for it also because otherwise it just like nobody will use it and the core team that builds and maintains it also needs to be incentivized in in some way in order to be able to keep uh building the ways that this can be is through support to the various teams or through uh slas for um software level agreements for companies that may not want to have open source code so you can have do a licensing um so it's a bit funny I wanted to show the I saw the timeline thing and I was in the template that they gave us and I thought hey why not put a timeline so how the heck did we get here 2017 I just need to do my taxes in crypto and I was like okay I'm not gonna what is the the way to do this there was Bitcoin to attacks there was nothing else back then I'm not going to use a centralized service okay I just don't trust them so I just made some python CLI scripts it worked I've not been sued by the German government yet so it won't I don't know um and later build a UI around them in 2020 we made it into a company we were a team of two people and maybe we had 200 users 300 and maybe 10 paid um so last year the app had grown we hired one more developer and we were 2000 users and 200 paying users in the beginning of this year uh there is many people who use rotkey right now some like it some complain with they always want more and more but it is at a level that many people can use it we are a team of seven now six thousand uh around six thousand users it's hard to know because we it's an open source app and we don't have Analytics um and 500 550 pen users we came all this way without uh anything like it was just completely bootstrapped and from um uh basically your donations through Bitcoin and from Integrations with um uh other uh companies like uh Optimus gave us a grant lately before that there were kusama and so on and so forth so for getting this POC that I described here to the full rot division we would need to uh go further from here and try to grow and potentially get some funding because with the current team that we have it's well impossible to actually build this Vision the POC cannot grow to a level of something that can be used by the entire industry with just six people six developers this is just impossible um so with this I'm coming to the closing notes um so if you like restaurant you had like open source Locker first the modular thing that can be used by everybody in the industry um then please talk to me or check out yeah this thing again check out roti.com jobs we have some open positions um we came here thanks to you like seriously it's a boost up uh project and we would not be here without git coin donations and without uh our premium users so keep supporting us um you can donate in git going grants or in um buy premium uh subscription and unlock all of the features of uh rodkey and you can join our community in Twitter or Discord like that's where all the support is it would be pretty cool for you to if you can um uh join the chat and join our community and if you're interested in helping us grow in realizing this visual that I try to present here uh then talk to me either like any day in the conference or write an email to Lefteris rodkey.com um yes with that that's all uh thank you very much [Applause] any questions either has a question I say it's a question I say question I say question we good afternoon you mentioned the graph on true blocks and one thing that I don't understand entirely understand about these two tools to query historical data is that they use a client node and this node basically is in charge of storing data in a acql SQL database right so isn't this like a centralized way of saving data is this when ipfs comes into play and if that's the case can you explain how ipfs and SQL database work together to solve this I'm sorry I really cannot hear you I heard the graph and that's all I had like okay like you mentioned the graph and true Vlogs one thing that I don't understand about these two tools is the two query historical data is that they use a client node and this node is basically in charge of the storing this data in an SQL database isn't this uh Central as way of saving data is this when ipfs comes into play and if that's the case can you explain how ipfs and SQL database work together to solve this ifs doesn't come anywhere in there like uh the graph is uh and turbo is completely different things so the graph is um it creates an index on top of your already existing node data and Roblox does the same but true blocks does it in a generic way for all of your transactions while the graph has specialized subgraphs written by developers of particular protocol that uh basically write an index address for this protocol and this lives on top of your node data it is decentralized like the graph by Design is also decentralized true blocks is itself also decentralized it creates this index and this index is said um I think it's pinned in ipfs and shared with others who use true blocks I I'm not totally sure on the readers of sharing of true blocks because I'm not a developer but I think that this is how they do it and for the graph they have a decentralized network foreign great talk um so if I'm understanding this right the idea is if we build this out um and get it out there then like we could get around using services like tenderly and just basically run tenderly at home for transaction tracing and simulations and all that yeah um I view it more from a historical perspective and Tenderly is a current emulator but yeah I suppose that you could also do the same correct me if I'm wrong then release proprietary right yeah yes yeah I think that they work with trades they're pretty good but yeah proprietary with such a design yes I believe that we could uh uh yeah use this as an alternate here here foreign presentation great work uh from the developer perspective on on a solid developer I think will be very useful for for example I write a smart contract and I write the decoder let's say I write a decoder and I host it on ipfs have you thought about that like how we can have that I can standardized I know let's say Parable where we Define the URI or hash where we set our decoder and you guys can use it because you have a list of because on GitHub right yeah yeah yeah I mean this is just a different medium of of delivering the decoder but yes there is exactly what you described so I'm not going to write every decoder my team is not going to write every decoder that's impossible but the idea is exactly that you when you're at your smart contract for a protocol then you say okay I'm also going to write a decoder for this and then somehow it should be delivered to this middleware yes it can be through a link it can this is just a POC of what we have right now the drag and drop in the folder in GitHub I think it does it a lot of value if you can also validate the decoder because if I do it I can buy the malicious decoder but we will need like a validation system and these are this is a place where I think you can add a lot of value okay yeah that's that's good feedback thank you guys awesome thank you so much left Harris that was a really exciting Outlook and vision and of course yay for open source from one developer Legend to the next I am very excited to announce our upcoming speaker it's Richard who's the author of ethers and the co-author of firefly and today he will walk us through what's new in version 6 of ethos so please give a warm welcome to Richard oh let's push buttons and see what happens thank you thank you hello buenos dias everyone how's Stephanie yeah okay um so the for those uh let's jump in I think my slides explain most things I'm still pushing there we go um so my name is Richard Moore I go away Rick moo online and so I write ethereum a library called ethers Js um so what is ether's it aims to be a complete compact and friendly ethereum library that developers can use but also part of user friendliness is safety like you don't let user don't let the developer do things that like bites in the ass without realizing it um so one of the cool things is the default provider I don't know where I should stand so with the default provider will haul basically it lets you just connect to ethereum um historically you had to either run a node or you have to like sign up for inferior you have to do something to start using ether ethereum right away so we've got like a bunch of relationships with these third-party providers and right off the bat you can just connect to ethereum start doing Simple Things it's heavily throttled but you can at least do something before deciding whether you want to like get your own inferior ID and all that um this is all the old stuff so I'm going to go over quickly um it's written in typescript now very few dependencies 26 000 and growing test cases ens is a first-class citizen everything including all dependencies are MIT licensed and there's extensive documentation and in V6 the documentation is getting much better um and an important thing is to understand like I made ethers because of something I needed and something that I use a lot and so I mean it's in my best interest to keep it getting better over time and that sort of thing I think dog fooding is very important to keeping a library good otherwise you get a library is very good at hello world but very difficult at anything more complex um so modern so getting into the new things then V6 that's coming out there's currently a V6 available both on npm and GitHub uh it's still very beta but it's there to try out so one of the biggest features is modern es features currently V5 it targets ES3 so if you are trying to run ethers in IE 3 from 2002 it'll probably still work mostly but that's no longer a priority and so the goal is to start adding um kind of like new age JavaScript things heavily reduces the code size because rather than having this big class for big numbers you can actually just use the built-in big numbers that JavaScript now provides you um so uh I mean you can now use like the equals equals equal sign which is super exciting if you want to use a big number you just put an n on the end and that tells JavaScript this number should be treated as a big literal uh don't go doing IEEE or yeah IEEE 754 truncation to it and and that sort of thing um I'm trying going to try to go through this faster ish if I'm talking too fast let me know but basically it'd be nice to get to the end so people who have questions can ask questions um so another really cool feature that uh modern JS offers is proxies for those that don't know proxies basically and are are an object and if you call a property that doesn't exist code gets to run first and decide whether or not it should let you think something exists and continue so this heavily improves how um the contract object Works historically you had like a ABI with multiple different um signatures for for different methods it really required you to understand all the rules for how formatting Works to normalize the signature in order to access it so now you can do whatever you want um oh wrong button e there we go so like basically you could have all sorts of white space extra little letters and and whatnot in there and this will all figure it out at runtime and map it back into that and get the right thing for you um basically for all those people who've ever had duplicate ABI definition errors and have filed an issue saying how to get rid of them this is for you um typed values are other cool things so going back to the same situation where you have uh two different methods Foo and Foo and they take in different things in V5 there's no way for it to know which one if you pass in two parameters an address looks very much like a number of 160 bit number but it looks like a number there's no way for ethers to actually know which one you meant to call and so now you can do uh so you yeah so this would be an error it doesn't know what this is this looks like it could be a number this is a perfectly valid u in 256. so now you can force it and tell it that it's a typed object of an address and then if you do this it'll automatically know oh this is the one you wanted um so again really cool things we can do this is sort of related to proxies but kind of its own thing likewise if you have um if you're doing programmatic things sometimes you just have a bunch of keyworded objects and so for example you've got transfer from it's got an from a two and a value so this is how you would do it in V5 you can still do this in V6 this works fine using positional parameters for those that are used to python they call them positional versus keywords but now you can also do types.keywords and pass in a from a two and a value and it knows that those things should get deconstructed into the from to some value this order doesn't matter either if you wanted to construct this object from a bunch of other lines of code reading stuff in you can just build an object up and add a two if it's not null and out of this if it's not that um and then it's Off to the Races okay another big thing uh is things are now classes okay I'm down to 19 minutes um so things have class basically um oh yes so I'll dive into this a bit more because I think this is a really cool feature in general of something we should be doing more in solidity this is kind of steering away from ethers specifically into more solidity generically um but uh if you're using V5 a signature is literally just some dumb object it's not a class it's just an object with a values r s and V and that's all you get by making it a class it can take in anything whether it's a like a raw signature or whether it's an RS and Y parity RS and v r and Y parody are our y pardon s whatever random combination of things you have feed it in it'll figure it out give you back the signature you want and then if you do something like update a value if you said for example Sig dot y parity equals zero and it used to be one it'll also change the V for you it'll update all the y pyridine s it'll it'll reflect all the changes so that the signature stays consistent which means um there's a straight slide for that um right so you can get all the stuff out of it and that sort of thing but the cool thing I want to show off is um maybe I'll come back to this in a second that's how things are done today um maybe I will explain this quickly so this other things are basically done today so this is basically people that pass in a signature for um for solidity for doing EC recover on they pass into bytes or they deconstruct it themselves so that'll be the third slide for now but they'll pass this bytes object in and then you've got this expensive weird byte manipulation Library I mean it's well written it's by open Zeppelin I think but you're doing a bunch of string manipulation on byte arrays to figure out where the RS and V sit it's also a huge amount of data otherwise and then this is what your code looks like the cool thing with these new things are classed you could for example in your code have a signature that has an R an S and a v and then you can just use the Sig type from the struct and this is your EC recovery so historically you might have also seen a verify that takes into bytes3 to digest a bytes 32 V bytes or sorry bytes you meant eight V byte search You Are by Sergio s and go off the races but with this you can kind of pack them all together this is the same size for the ABI point of view and then you just pass in the signature and because it's a class when it's starting to build out that encoded piece of information it knows to take the r and pack it together with the S and pack it Delta V in a nice compact format and then you're off to the races so one important thing to note with this the next slide is this line here bloop does not change and so you see you're still creating a signature you're just passing the signature in verbatim and um yeah basically it's because the ABI is encoding it it can look at this object and figure out okay I need to take the difference in this case is the r and the Y parity and S for those that aren't familiar with um uh EIP 2098 basically this allows you signatures are much bigger than need to be so basically you're used to RS and V that V is actually just one bit and thanks to Homestead uh every s value in ethereum is actually always got the top bit set to zero so we kind of just like slot that in and now your signatures are 60 30 smaller and uh yeah again feel free to ask questions because these are going to be a little complicated but you need a little bit of more math to help decouple that I'm going to bug the the um facility guys to see if we can kind of get this built more into solidity so you're just pass in the signature directly but in the meantime this is still much simpler than the like couple dozen of lines of bite manipulation byte manipulation libraries that's going on um if you pass into bytes um so quick comparison so if you use the raw bytes like most people do today it's 160 bytes which is by itself just for the call data is 1444 gas expensive so if you use decompose that's 96 bytes but if you use the compact representation it's 64 bytes um yeah I'm not gonna go to more than that uh that was just a quick thing for people who are looking at the slides afterwards transactions are also now an object so if you decouple a transaction if you just feed in a bunch of raw like a raw transaction object or a bunch of stuff if you start updating the stuff it updates the other parts of it as well um yes basically well we only have just an object and you set some property on it you're only opening that property but with this when you set a property it sets that property but also updates all the all the entangled properties for example if you set the gas price then the serialized version of that transaction should update um and you can just set things you can set them to anything that's valid you could so in this case I'm using a bigint but you could pass it a string you could pass in a hex string you pass in pretty much anything you want that makes sense for a Max fee per gas and it'll be reflected uh 13 minutes bits so this is just a little odds and ends of like random things so pausing providers so for example a lot of people find that people keep their their customers keep their their app open in a tab that they're not even looking at anymore which means that their events are still firing it's still hitting this inferior gobbling up all their all the requests um well because there's now these visibility apis available in browsers you can tell ethers by the way when when I'm not visible anymore pause the provider um that way if they go to another page for a week and then they find that tab that's left around from like a week ago or a month ago or we all have really old tabs um this way they haven't been consuming your your bandwidth every time that you've been they've been doing nothing with your site so you can pause your provider and then when it comes when the tab becomes visibly visible again you can resume and you get the choice of whether you want to replay all the events that would have happened during the time that was paused or whether you kind of watched you just drop them and keep keep on going both situations have like valid use cases so up to your own personal use case um uh the cool thing with this feature is because we can now pause and unpause uh providers all the underlying API and framework changes that went into making this happen it means that if you have for example a websocket and the websocket disconnects it can now reconnect and re-subscribe to all your events before it starts feeding you the new events that start coming in it can actually get the events that happen from the time it disconnected from the time that is now and so all your events come back in order and you don't miss a beat and you don't even know that your websocket crashed um and ordinance um I'll just go on these quickly basically networks are now a plug-in they have a plug-in system there's a lot of really strange networks out there and I always get like requests saying like oh my network computes hashes in this strange way or my network doesn't doesn't have an author in the block or this sort of thing in the other so this makes it possible that ethers can stop capitulating to each of these individual things and all those all those Oddities which are fine I mean you know competition ecosystem is good but then we can like push all those differences into the uh the network object and let it handle it foreign package exports I'm moving away from a mono repo which uses a bajillion sub packages all by ethers because it used to be like at ether's project ABI at ether's project providers and that sort of thing package exports are awesome they're supported by all major bundlers now and it means that there's no complicated weird process going on for ether's build for those who've used ether's or tried modifying ethers at all the build process is absolutely insane because I try to Target again ES3 and react and I try to build for everything and as a result people who are using more modern utilities like uh people complain about Vite a lot or if you're trying to build a bundle for node it just fails so the nice thing is by using package exports instead of all this crazy custom scripts it just kind of works with all these tools and everyone's happier um I hope we'll see that after I launch the non-beta um there's better and fewer dependencies I'm actually going to grab a bit of water um so yes there's basically I think I'm at five dependencies ish right now but down to four authors um which is an important thing I feel like we've all seen we've all like recently been pretty worried about like uh supply chain attacks going on from mpm installing a bajillion things and so right now there's four well-established authors that are responsible for all the library so there's me I accept myself well established um there's uh Paul Miller who does an awesome library for hashing and for signing and Microsoft writes TS lib it's a tiny little thing it just helps save some space and websocket there's a popular websocket library that works in node if you're using a browser you don't have to use that one so now you're down to like three authors and and four dependencies um and it saves me a lot of work as well because there's so many times where like elliptic has some bug in it and I'm trying to track down on him to get him to update things and once that's fixed I now have to update everything and put a new build out so the fewer dependencies the the happier my life is as well and that's all I've got I've got nine minutes for questions if anybody's questions oh and come find me afterwards I've got these little like ahiva I brought one up for anyone who's like curious about V6 I've actually started to write oh I should explain that so yes basically I'm trying to write a bunch of little apps right now against V6 to help test it um so this one's actually found quite a few bugs already um it's an nft it's got a little scratch off hologram when you scratch it off you can scan it and claim it and you can you know fold your little papercraft robot up and like do this and then take pictures of him and post it into the contract and you can like I'm thinking of more of like that little gnome from Amelie you know you can take it around and show you those things visited different places um right now it's only deployed to Gourley for anyone who actually like goes through the effort of like submitting this I will be migrating your tokens to mainnet um if you want to wait till the main version launches the same QR codes will work on mainnet and you can claim it there um so that's just like a little demo I'm throwing together to to test and make sure V6 works and it's definitely not ready for production as it stands right now but it's getting closer and I'll probably be doing a few more little weird projects like this just to kind of like find those little foibles and and things that happened during the last two years worth of rewriting and rewriting and rewriting so I've got 7 Minutes 49 seconds left for if there's any questions out there yes I think seller is one of the ones that has do they have a different way of computing the transaction hash yes yes so I think they're actually one of the ones that was like an early headache that led to the reason for me putting trying to put that stuff into some place that was a little more isolated from uh like the core providers so yes sellers is I'm 90 sure one of the ones that yes oh I go ahead there's lights I can't see I'll I'll Trust the the moderators to hand out my and ask questions and just maybe jump and swing your hands if you're okay oh there you go yes first of all oh my gosh thank you so much Richard I don't know where we'd be without you like like open source hero thank you uh yeah so you know again thanks so much for all you do the ecosystem will not be the same without you but just curious like at this point in the state of ethereum at this point in the state of ethers what kind of support are you getting in terms of like uh audit helps especially as we see more like front-end attacks and things like that right where it's like you know smart contract auditing is important but the interaction layer is equally as important um especially now as people are getting more clever like like you know not to not to throw shade but you know like when I saw uh you know string normalization for for ABI it calls you know I instantly thought like just made me like uncomfortable in my gut of like different things that could happen so just curious like um like who's helping you audit who's helping you kind of like comb through this code and go through uh different edge cases and things like that right so basically auditing is all done by me which is terrible right now one of the cool things though is between I've got like oh sorry the EF helps sponsor me I get GitHub grants get coin get coin grants and a few sponsors and so now I'm starting to develop an endowment so I am hoping to be able to get like a proper uh audit at some point um there was one team at one point that was going to formally verify a bunch of things um I have heard from them in a while so I don't know if that's still a thing but uh I think four of her I don't I I'm on the festival verification like it helps in some regards but it's not a Magic Bullet um I think absolutely like proper auditing would work and I'm also a big that's why I'm a big fan of tests I don't want issues to happen um so I feel like test I mean tests can only find the presence of bugs not the absence of bugs um so I mean it's a great question talk to me afterwards if you've got some ideas uh I'm starting to get some money off to the side now that I can actually start throwing if you know any like auditing firms that do typescript um what you mean should definitely pay for auditing those those people are so skilled and valuable oh okay sure well I mean you kind of are I mean thank you Bitcoin uh bitcoin's been awesome so here hi oh um I was wondering if you are gonna add um but transactions in this version do you sorry it's a little bit hard to hear with the echo are you guys know about batch Json RPC but transactions sorry oh bash transactions um by bachelor's actions like via Smart contract wallets sorry like by you mean through smart contracts or yes yeah yeah I mean that all already Works uh you just need the ABI for it um there's not currently in ethereum an official like batch standard um but you can do it with smart contracts like smart contract well it's like a gnosis safe for example allows you to send multiple is that I'm sorry is that what you mean like being able to send multiple transactions at once an atomic atomically sorry no no I have an application and I will I I want to to send that in my node application not through contracts not not through Westbury [Music] contracts um so that you can't currently in ethereum in general send multiple transactions atomically or at once um you basically if you will have a bunch of transactions you're trying to send serially um so the nonce manager already does that uh basically it's a wrapper that goes around a signer and you can just quickly fire off transactions and it will automatically bump up the nonce from the initial nonce fetched um there are some issues with that as well if you just you know flood the mempool with a bunch of transactions from the same address that are all serialized it could be seen from the Network's point of view as a Dos attack and so it might drop them so that's a feature I want to add to the nonce manager in V6 as well is kind of the ability to rebroadcast or whenever something succeeds kind of like have a callback that lets you know something succeeds succeeded and so you can kind of nudge or tell it to rebroadcast the transactions that are still around I think if I understand your question I'm thank you hey Richard I can't understate how useful what you did for the whole ethereum Community we use what you do every single day back in front of them everywhere so uh thanks for that thanks so yeah more than deserved one question about philosophy and this is this is going very specific on being that guy but one thing that ethers has a philosophy is really the how it returns undefined and actually it doesn't return undefined for waiting for transactions and we actually talked once in one of the PRS of typescript really giving the option of of returning undefined one-day transaction you wait on it and it doesn't return anything for example um and you mentioned that that's not really part of the philosophy of how you're building the types for for ethers so I missed that last part so we mainly set a transaction and you get back to null or um let's say let's say it's a hash that doesn't exist okay um what what is the philosophy of ether's returning that would it be undefined because in their case the type script for that doesn't contain undefined it always says oh I know you're talking about now okay so that's okay that's a big thing in V6 as well I should have probably put on the odds and ends okay uh so we four came for exactly right so right okay so for a quick background basically ether's V5 was started two years ago I was still kind of learning typescript it did not have strict null checks enabled which basically means anything that returns anything I'll turn that anything or null and so in V6 I've got no typing in some strict null checked type whatever that what plague is it's disabled and so for example the provider's method for get transaction it will be the return type is null or transaction response whereas in V5 it was just transaction response and I just trusted people knew that uh it could be null but don't worry about it so yes uh yes V6 has been fixed it helps fix a lot of people's libraries as well that are like dependent on it and are kind of crawling into the ethers tree to like run their linting rules so yes that's absolutely changed um will there be backward compatibility between the big in type and the big number sorry I can't I can't see your it's now it's better now and will there be backwards compatibility between those new uh big in type and the old big number type like will pick in the big number-ish sir will what be big numbers add the new Big in time oh yes the big I mean for input types you mean yeah yes there's still a big number-ish type because uh like so for example the the slide up there that demonstrated I lost the clicker um the slide that shows you um uh like the transaction.max gas equals so that takes in a big numbers so you can pass in a bigint you can pass in a string that happens to be decimal number you can pass in a string that's a hex number um that'll all be munged into a big int type but you can still pass anything anything that's not ambiguous ethers will accept it will not accept things that it can't possibly interpret like uh for example non-zero X prefixed things because if it did that it wouldn't know whether one one was hexadecimal 17 or binary 11 and so it requires you to be completely nonambiguous um so if you pass in a string that's one one it's going to assume as decimal but yes it's uh it's still a big number it's still a big number-ish um and there's functions as well for your own purposes that will convert any big number-ish into a big number or any numeric which is another type that it now has into a um number I've got 22 seconds left and oh also as a quick note I'll be standing outside after this for anybody else who has like uh questions to like uh poke around okay okay I'll see you aside I'll be handing these little cards out as well and like so thank you thank you thank you so much Richard guys make sure to collect the cute robot I definitely want one um all right um that brings us to our last talk for today in the developer infrastructure category before we move on to some zero knowledge stuff so I'm excited to introduce POTUS he's a core developer at Prismatic labs and he enjoys the types of puzzles that arise when trying to hack the protocol today he'll talk about the right way to Hash a marker tree so please give it up for him thank you uh did you happen to have the clicker ah there he is thank you thank you all right so thank you very much for letting me speak here uh this is going to be a very different kind of talk and it's going it's my first talk in this community so I expect something I don't promise anything I just want to sell you something I just wanted to get right to it I want to sell you this if you're hashing if by any chance you need to perform hashes that this happens quite a lot in the consensus layer uh I want to tell you this library that will hash at the very least at the very least it will hash 20 faster than your current backend without any changes to your code and if you see here in the bottom do I have a pointer here I guess no this is just a click okay so if you see here in the bottom you see that for large trees this is hashing four and a half times faster than the than the common back end so I just want to tell you this and um since I can sell you in two sentences I'm gonna spend most of my dog just telling you what is sha 256 which is this what this Library does but the techniques of this Library can be applied to whatever other hashing uh on the Sha families that I know of so let me tell you what sha 256 is so let's start with this slide this Shah the hashing library is something that generically just takes any message any an arbitrary length message as in a sequence of bits of any length and it just spits out of that 32 bytes the digest of this thing is 256 bits and the procedure to go through those is consists of three parts so I'll just go over the three parts briefly the first part is breaking your message into multiples of uh 64 bytes so those are the chunks 512 bits the second part consists on scheduling words Computing a bunch of double Words which are four bytes each and the third part consists on performing a bunch of rounds which is just the function that actually does the hashing itself the function that is not invertible so here's just a brief uh description of this this slide I just stole from somewhere in the internet but uh I want to go in some detail in each one of the three parts so let's start with the easiest one which is scheduling words so scheduling words as you see here you're giving a message let's suppose that we've already broken the message in pieces each one of them is 64 bytes so the first thing you do is you compute 48 double words four bytes each the first you need 64 in total the first 16 are your your message are the 64 bytes that you're giving these are 16 words and with those 16 words you can compute the next 16 words and with those 16 words that you just completed complete the next ones and the next one and you compare the 64 that you needed and the important thing that you need to remember from this slide is that to compute those scheduled words the only thing that you need is the previous words nothing else that means that when you're scheduling words well I think the consensus people are starting to come in anyways I've already started guys uh so when you're scheduling words the important thing is that you don't need to know the previous state of the Hashi you only need to know what were the previous scheduled words in particular it only depends on the chunk that you're hashing now and it doesn't depend on the other chunks that you're going to hatch so if your message consists of 10 000 chunks you can compute the scheduled words for the ten times and challenge without caring about how you attach each one of them and without caring about the rounds part so scheduling words only requires the previewed scheduled words so the diagram there is taken out of an Angel paper describing what were two new instructions that do this scheduling of four words at a time with only two instructions this can be done now on Modern CPUs uh and this is just a sketch that how diagrammatically you compute four words at a time but it's irrelevant the method that you use to compute the scheduled words what I want you to remember is that to compute them you need to know the previous words and nothing else rounds we don't care about what rounds is we only care that it's a function that is not invertible that is hard that is computationally hard to invert but the important thing that we need to remember is this we already we were given the message we broke it into pieces of 64 bytes that I haven't told you how uh we computed those scheduled words that we need and then what we do is we pass an incoming digest if it's the very first chunk that you're slash that you're hashing we pass a constant digest that the method has if it's the third chunk you're gonna pass the past that the second chunk produced the point is that you have a status which is your current hash and you pass it through this function that takes this status the hash the 32 bytes it takes one of the scheduled words that you computed and it takes another constant word that the protocol has so it takes this data and it produces for you a new hash so what do you need to remember from this is that you need to have computed at least that scheduled word before passing through the rounds and you cannot do this in parallel to pass through the round you need to have computer already hash before so this is something that you cannot do in paragraph foreign okay so the padding block so this is the first part of the three uh process which is breaking the message into multiple of 64 bytes how do you do this well you just break your message into multiple of 64 bytes you add one bit at the very end of the message just to Signal this message has ended so here the message is less than 64 bytes is 24 bits this three bytes there a b and c this one there is showing that we added that extra bit to show the message has finished and then you pad with zero bits up to a multiple of 64 bytes minus eight because you're going to use the last eight bytes or 64 bits to encode the length of the whole message as is there in binary that's the number 24 which is the actual length of this message okay so this is the procedure to pad uh your message which was arbitrary length into a multiple of 64 bytes all right so vectorization which is the main topic of this talk is vectorization so the first thing to notice is that you're not going to beat any of the hazards out there every implementation that I've reviewed before I got into this and I'm not an expert at role on this I I come from a completely different subjects but every implementation I've seen is equivalent to Intel's original paper white paper on this and it's equivalent to what opens the cell for example does all of them implement the following you can compute your scheduled words as I told you without knowing what the previously hashed uh what the previous what the current status of the Russian is so all of them use Vector instructions to compute several words at a time if your CPU supports uh 128 bits registers like these ones and almost every CPU now does then you're gonna compute four words at a time four double words at a time you start with your 16 double Words which is your message and you can put those 16 double words in only four registers if your computer supports uh avx2 that's equal to electric and registers that are 256 bits then you're gonna compute eight you're going to use only two registers and you're gonna compute eight words at a time if your computer supports AVX 512 you're going to compute all of them the 16 of them at a time and well idx 1024 does not exist yet but it's already in the books uh there's two options into Computing This Modern implementations either do this this vectorization or if your CPU implements cryptographic extensions like the what was there in a picture then they will use 128 bits registers regardless if your computer has larger registers because cryptographic extensions can compute forwards at a time much faster than vectorized computations but but the point I want to make is that this vectorization is there in every equivalent implementation this is since Intel's uh uh since that's white paper also Computing the words like this is useful because since you can compute the words and at the same time uh pass through some rounds so let's say that you computed the fifth word then you can pass up to the fifth round then you can mix scalar operations with Vector operations and the CPU will perform this in parallel the spews can take has several ports and can take different types of operations at the same time so you can be Computing the fifth round that is a computation that it has to be done on the scalar part of the CPU because it has to be uh it cannot be in parallel and at the same time you're Computing the sixth word in parallel on Vector registers okay so I've covered what is what is a typical uh implementation of hashing and this is the the thing to remember from the whole part from the whole purpose of the talk is that the hasher signature is this in either language is something like this it's something that takes an arbitrary length bite slice and it gives you back 32 bytes which is the hash okay so he takes an arbitrary length message and it gives you one it gives a digest and this is something that you're not going to beat you're not going to implement this better than open SSL no one's going to do you perhaps can do it for one CPU you're not going to write an implementation faster than what is already there however we use hashing in a very restricted scenario we use hashing to Hash Merkel traits a Merkel Trace are not arbitrary length Merkel trees are in the case of the consensus layer for example where the nodes are 32 bytes they're something like this each one of these nodes represent 32 bytes and each parent node has only two children and these two children are hashed together so the two children are 64 bytes in both concatenated you hash them and you get the hash of the part you get the what what goes in the BART so this is what we we hash typically and we observe two things well actually the execution layer has something completely different but still the same technique is going to apply uh what we observe immediately is the following thing first of all we're not hashing arbitrary length we're hashing every single time 64 bytes and second of all we can hash this in parallel you don't need to you can hash the blue one by knowing the entire left subtree and hash the yellow one completely in parallel in the other side of course you can do this in parallel if you have a CPU that if you have two CPUs you can just use two threads to Hash this and this is exploited in the consensus layer I don't know if any other besides Lighthouse uses this but I want to use this I want to say to talk about a different kind of parallelization you can parallelize this in different threads but the point I want to make is that you can do this by using Vector instructions so again this this is the the typical this is the one that is in the specs in the consensus layer specs this is the implementation I think this is vitalik's implementation this is a flat array approach to having a miracle tree I highlighted the line where you're Computing the hash of the parent by hashing the two children that are concatenated the top one is uh different memory layout this is Proto lambda's implementation on remarkable the point is that it's the same kind of hashing you take the two children and you hash them and you hash two blocks at a time this is fairly fairly inefficient this is a goal implementation this is also Jim's uh this is James implementation this is a production a production implementation and again the same thing is slightly more complicated because it takes into account other things but the highlighted line is the point is the point where you're hashing and you hash one node as the hash of the two children and you do this on a loop for each pair of children you have once you call the hasher and you get one touch okay so I want to tell you what is the right way of hash in a miracle tree and it's fairly fairly simple so there's two things that we want to exploit we want to exploit the fact that though this takes an arbitrary by its life so this is the layer that you want to hash and it gives you back and a slice of hashes of 32 bites these hatches all of them at the same time uh I think I might have gotten this correctly in Rust after several iterations and this would be in Python uh we have a library uh we have two libraries we have one in go assembly and we have one in user assembly with C bindings if you are going to use that let me know because crypto extensions on arm is still not implemented it's gonna take like 10 minutes to add this but this is the signature that the library is going to use if you're using a c bindings all right so that's all I want to sell you and that's that's it [Applause] sure I think we started even before the the hour was scheduled yeah yeah that doesn't exist in libraries already is that right it does so so Intel has this Library where uh in tallahassee's library where you send you give them different messages and it hashes them all at once so that I just copied everything nothing here is mine I just grabbed Intel's implementation and bitcoin's implementation and put it in a library the thing with intelity is that it takes Arbiter length things and it just hashes it and it gives you back just the hashes for all the messages that you give them okay sure I'll just repeat that for the sake of the recording I was just asking if um uh the the ability to process multiple different hashes in parallel was already implemented in libraries that was the question to the answer um so so that so there's two things that you have here right one is to like pre-compute the padding block and then the second is the ability to to in parallel process multiple different hash values and then produce multiple different hash values right correct and so there are already implementations that do multiple different hash values at once and you just modified them to deal with the padding block is that right yes and yes so so there are two modifications here one is uh the thing that you're going to use the padding block uh the padding block has this constants hardcoded and then there's other modification which is the fact that you're expected to get a list of 64 bytes chunks and then you pipeline this so what this Library does is it grabs all of the blocks consecutively memory it gets a matrix on the vector on the vector registers it transposes this Matrix and now you have on all of your registers the different mess the different messages so then you can use Intel's Machinery to Hash those messages in parallel you print the you output these hashes and then you just Loop it back so that's cool so you you have so the result is basically like so it's inco assembly what you have right sorry your your implementation is in go assembly I yeah so we the original implementation was just purely assembly this is there it has C bindings but it the C go overhead is horrible so it ended up being slower than using the go implementation the standard library and go so we needed to write a Google assembly library to use ourselves yeah okay cool well done very good very impressive thanks hope Peter okay I'll just say I wonder after you've done this have you like considered it whether we're doing anything practical would it would be interesting if someone tried to design a hash function specifically for hashing Merkel trees and whether they could do it differently and how big the savings would be for that if there was like a special hash function for Merkle trees I'm sorry I I I didn't get this uh sorry I'll repeat the question so I was curious you've shown that there are always optimizations you have to do to get around the general purpose nature of these General perfect hash functions do you think it would be possible to design a hash function that was like a special hash function only for like oh oh that's a good question uh and I believe oh that's a very good question I need to think I I I don't think I can answer this here uh I'm not sure if you can do better than the current implementations like uh you can take the sponge type implementations shot 3 and Company and you can try to adapt this for this I don't know I think yeah your question is completely open it's it's a very good question I think we should think about this so if I answered your question as can there be a method that is designed for Merkle trades uh instead of like a generic one it might be I don't know oh sorry to hug the mic um in your implementation does it so say there's some really big Merkle trades in in the beacon state right yeah um is it the job of the implementation to kind of split that Merkle tree up into smaller subtrees that fit the size of your CPU no no so the the so you're saying if you have this large tree if if it's a job of the implementation in splitting into smaller trees no no this this is this is completely orthogonal to that so what you guys are doing in Lighthouse is splitting this into smaller trees and you send this on different threads to to compute them in parallel this is completely different you just pass the entire slot so big trees is the big gain for this you're gonna be at least hashing four times faster than the standard Library you just pass the entire slice like all of the slides that you of the bottom slice and what what this thing is going to do is not split into subterest what this thing is going to do is grab as many blocks as possible that it can fit in your registers and then just go to the next chunk and the next chunk and the next chunk like this so it doesn't split in sub trees okay and so you faded all of the leaves and then it then it produces the intermediately it produces the next layer and then you just feed it the next layer entirely and it produces the next one oh yeah okay so for the state so you're thinking in the in the beacon State for the state this is incredibly fast but this is not how we hash the state because we have it in cashier typically and you just have a few a few nodes that you're you're you're you're changing so when you hash the dirty trees the dirty leaves then well there's two things that happen so sometimes you have several vendor consecutive and then you can you can be smart and pass this consecutive layer to this hash or you can just use whatever you're using now to scratch two blocks at a time you're not going to get the vectorization impact but you're going to get at least the 20 of the hard quality the the padding block yeah Okay cool so I guess there's maybe like an argument for maybe it's quite useful for small trades um you don't get a lot from caching yeah it's it's not really useful for small trees you only get the 20 of the padding block yeah it's very very useful on large trees and here large means more than eight blocks so anything that has depth more than two this is already four times faster nice thank you thank you thank you thank you guys all right since we are still very good on time I am going to use a few minutes to talk about something else so that the ZK people have time to come in as well um yeah first of all if you want to ask questions please do wait for the microphone because there is also a live stream and people can only hear the questions in the live stream if we use the mic and also for recording purposes would be nice thank you um secondly uh this year in Defcon we're doing a couple of things different than we used to do we are very experimental in some ways which I'm kind of excited about so I will use some minutes to talk about this before I hand over to the next speaker so I'm not sure if you guys saw it on the website or maybe on Twitter or in some of the communications around but this year we have introduced something called continuous Devcon which is basically our way towards a more holistic decentralized Community Driven Defcon and as part of that we are trying out various Concepts one of which is that the venue will be open until 11 pm so if you want to meet up with somebody if you just want to have I don't know a catch catch up with somebody if you don't feel like partying if you want to hang out here uh feel free to use the venue and um yeah basically the the basement the first floor in the second floor will remain open until 11 pm also there's a happy hour from 6 p.m to 8 A.M in the first floor so feel free to mingle there as well and then lastly in order to make everything a little bit more Community Driven we have allocated a space on the first floor to six selected communities and it's called Community hubs so in there you can find various different spaces that are run by communities one of which being the region Hub the temporary Anonymous Zone the ZK Community the crypto governance and economics Hub crypto economics and um what was it again talking engineering Hub I don't know uh this is powered by token engineering and smart contract Research Forum and uh the women leaders in web 3 have uh so yeah a lot and the design Hub so lots of stuff to explore there please feel free to check it out and lastly the hacker basement so we transformed the parking lot which is in the basement into a hacker space and it's definitely worth looking at it because it really is beautiful first of all secondly there is also some sort of art exhibition going on in there and 30 if you want some quiet time if you want to hack something on your computer or if you just need like a break from the masses I can really recommend the hacker basement for all of that stuff yay okay I think now we can move over to the ZK part of the day um first I'd like to introduce is building zocrates at the ethereum foundation and today he will give us an update on surprise zokates is a toolbox and a set of tools aimed at making developing ZK snark applications easier using a high level language so please give a warm welcome to TiVo foreign after so many great talks from the development tools track I have my goal on this talk is to convince you that this is not Moon math this is also developing this is also tooling for Developers so today I'm going to present Socrates I'm going to give a quick introduction to what it is for those of you who don't know it uh and then I will present a few things that we've been working on and that I'm excited to share so what is zocoties so if you want to program snarks today there's different tools that you can use maybe some of you have heard of Socrates of circum of Halo 2 and all those tools have different trade-offs in terms of the power that they give to developers and how easy they are to use in high high level or low level they are Socrates has a positioning as a very high level language so when you write Socrates it looks very similar to if you were writing rust or python or something like that um it compiles to r1cs so this is uh the kind of snarks that are easy to verify today on ethereum so you can verify directly on your smart contracts it uses modular backend implementations which means that documents does not develop a proof system itself but rather uses back-end implementations from the community for different proof systems and just targets that and uses those great implementations in order to have such a high level language but still have something that's really efficient at the at the low level zocoties Max use of optimizations in the compiler which is something that's also different from some other tools which give you a very low level access to what's going on on the constraint level but don't really apply a lot of optimizations um so where does it run so the part of Socrates which allows you to compile so the compiler you can run it natively on your machine we have a remix plugin as well and we also now have a playground that you can access at play dot socrate.es which I encourage you to to start that's probably the easiest way to get started with Socrates and just write some simple programs in terms of the scheme so this back-end implementations that we support we currently have support for a graph 16 which is uh some of you may know the the snark that is the smallest and the fastest to verify for gm17 which is in a evolution of graph 16 let's say for Moreland which is a universal snark which I'll touch on a bit later and for Nova which I'll also go into which is a new type of exciting snark that enables new use cases for proving so the particular implementations we rely on Bellman Arc Bel person and also snark.js I'll also go into a bit more detail on on that integration and in terms of the verifier we have uh you can verify in JavaScript or in this in the CLI and for some of the schemes that are compatible with devm we generate a verifier contract so that you can directly send your proofs to your smart contracts and verify them and and Link them to your dab so to give a bit of a Hello word of zocoties here's a a program which takes two private inputs A and B and one public input C and then asserts that a times B is equals to C so technically this is a program where with which you can prove that you know the factorization of a number um the way you would run that is you would first compile that program which would turn it into this low-level representation uh that's let's say r1cs then you could execute that program with an input here three three and nine um then you could generate a proof using a particular proof system and you could also generate a verifier that you can then Deploy on the evm and then send you proof to and then convince basically convince the network that you actually knew this factorization for an example of something that's a little bit more advanced to make the point that this is actually a high level language here's the implementation endocrates of the shot 256 function um so just to give you a few things that are expected in a high level language we have a module import system you can import constants as well um we have for Loops of course we have function calls and also one exciting thing that we added kind of recently and maybe some of you who use rust are familiar with that feature there's a notion of uh constant generics so in this case the shot 25 shot 256 function is a hash function that can take an arbitrary number of bytes as input however in circuits all the inputs are always static so all of these uh the size of the input will always have to be known at compile time but this is something that we do you can still Define this uh as something that is generic over K and then have a number of rounds of this round function but then when you compile your program and actually use the function the shot 5 6. this this variable K is going to have a concrete value which will then compile to the exact number of blocks that you're hashing and if you're trying to do something that's Dynamic calling this function on something that's not whose size does not know that compile time is just going to fail at compile time so we can have a very idiomatic implementation of chapter five six of course there's more complexity initial round but if you look at the code it's almost Line to Line equivalent to uh an implementation that you would see on Wikipedia for example pseudocode implementation um now I'm going to go in a bit more detail on a a detail of the shot 256 implementation so inside the Sha round um there is this expression needs to be calculated a lot of times it basically takes three unsigned integers a b and c of 32 bits and calculates a and BS or a and six or B and C and this is something that you can just write like this in Socrates today compile and it will translate that to a number of constraints at the low level however if you do the math and look at the let's say the first bit of a the first bit of B and the first bit of C you can see that if you considering them as numbers so as field elements they actually verify so the result actually verifies those two equations so you define or you constrain a new variable BC to V equals to equal to B times C and here I just want to clarify that these constraints are the low level constraints that we deal with so the r1cn's constraints they look like this they have one side which is linear so in this case it's only one variable but you can have a sum of different variables one side which is quadratic right so this first one just defines BC or like constrains a new variable BC to equal to B times C and then introduces this res variable here which is our result for the first bit and then constrains it in this way and this is actually more efficient that what the compiler would generate itself because here we have more knowledge of what we're actually trying to do than the compiler does on the flip side here what we're doing is that we're introducing a new variable res and then constraining it with this with this equation however when you're dealing with such low level details in snacks it's really easy to introduce new variables but fail to constrain them sufficiently so that you actually introduce vulnerabilities in your circuits so this is something that Socrates does not expose at the moment to the developer which means that you can only do this one which is less efficient if you look at more lower level tools they let you use these things but then it's at your own risk and then it's likely that you're going to introduce vulnerabilities so what I want to showcase today is uh the addition to zocates a way to actually encode this thing and have um the performance from this thing in the context of the high level language um yeah so I have a video now if you could start the video right so this is using the actually the Socrates playground so here I defined the default function which I call the default function where I just I'll just use the compiler uh to generate the constraints for this the this expression here I'm going to create an uh an entry point for this program so taking also a b and c returning a u32 and I'm just going to call the default function and see how many constraints are created in the process so I compile and then I get the result 260 constraints here um and now what I'm going to do is to Define another version of this function which hopefully will reduce the number of constraints by leveraging uh this lower level implementation so I call it hand optimized has the same signature a b and c also return a u32 so as I described earlier I want to operate on each individual bit of this year 32 so the first thing I'm going to do is turning this u32 into an array of booleans we have sort of a magic tool in our standard library to do that which is called a cast function and which can do this conversion for you and here I want to point out that this is actually free because the u32 type is actually represented as 32 bits under the hood so we're not paying any constraints for this so just cast the three of them the next thing I'm going to do is introduce a new array of booleans for the result here one relatively new feature we have is that everything is immutable by default so here I have to declare this bubble mutable if I want to be able to modify it after okay so now that I have all the bits I can start a for Loop so for I from 0 to 32 and here I'm going to consider the if bit of a b and c and if we want to have access to those low-level constraints um we need to reason at the level of field elements so we need to turn those booleans into field Elements which is a lower level um representation for this I call this Bluetooth field function which I'm going to Define in a second I do the same for B and C and here I'm going to Define this function and again this is something that's going to be free that's not going to create any constraints for the same reason as earlier because the Boolean is actually presented as a field element at the low level it's just that it can only be value 0 for false and one for true so I can just return that using a ternary expression okay so now I have ABC as svl elements now I have this first constraint which was uh BC equals B times C and actually this constraint I can already Define in in the high level language because I'm doing both constraining and assigning BC so I do that so I have that first constraint is done then I declare this result all again mutable and this is where the interesting new thing happens I have this assembly block that I can create and here as I have access to and two special kinds of statements the first one is going to be just an assignment so I introduce I just assign the value BC minus this other expression to Res but I did not create any constraints it's purely just an assignment so this has no influence on the constraint system and then I want to be able to use this um this res value later but to use it I need to make sure that it's really constrained in the constraint system so after this I did this assignment I add actually constrained which um which makes sure that everything is uh is set in stone so here BC minus res equals this multiplication you can see that again this can be any expression here when I assign but this has to be linear equals quadratic right so here this is this is working okay so now I have everything set up and I'm convinced that res has a result that I need the next thing that I want to do is that I actually want to have a Boolean I'm going to go the other way around and reconstruct my result but I want to have a Boolean and I want to go from a field element to a Boolean and here I want to make it really clear that this is an unsafe a potentially unsafe operation because this res value I know that it's 0 1 because I I wrote this but in theory it could be any value so I need to be really careful when I do this but I can force uh the creation of this Boolean with this value finally I reconstruct the u32 value from the Boolean array using this class function again okay and I changed my entry point to use the hand optimized version so we were at 260 and now we're at 164. so we made uh quite quite a big dent in our constraint count okay oh can I exit this okay so what's the idea here um we want to keep all the guarantees that we have from our higher level language we have types we have things we know that booleans can be only zero one if you don't write assembled books but at the same time we want to have access to this low level thing and here I think there's actually a parallel with rust in a way which says okay we we have a compiler that's like really strict for all these things but we still want to be able to do all these lower level things we want to disable a bunch of checks and we have a similar approach where as a developer you would write most of your program in safe zocots let's say but then for the few parts that need the extra performance you can write them in those ASM blocks and try to make those blocks as small as possible so that when you need to review the code or make sure that things are not unconstrained you know exactly where to look at and these things are relatively small one side effect of this also for us as a compiler team is that we can use this ourselves to reduce the complexity of the compiler so this particular operation and the shot 256 algorithm we in the compiler we have a special case which detects whenever we're doing this and uses this exact constraints but now potentially using assembly blocks what we can do is rewriting some of the internals of the compiler to actually use these things which then reduced the size of the compiler code base and makes it easier to reason about and how to okay the next thing that I want to talk about which is uh unrelated to this uh is the fact that Socrates is not compatible with with snog JS so snack.js is a JavaScript library first nrx from the identity team from the circum team which has a bunch of tools allowing you to work with snarks in a JavaScript context so what we have now available today is uh if you start from your your zocoties program and you run the compilation currently um it returns an output which is the low level zocchi's representation but you can also optionally return and Dot r1cs file which is uh the format that is accepted by um it's not Js and then if you want to execute your program you take your input and uh and the program itself and you can create a witness file which is also compatible with with snack.js and from this point on your instant RGS lens so you can do whatever snog JS enables you to do using different proof systems uh using a powersoft Tau ceremony um and run your verifier in the browser etc etc so any tooling that's compatible with these formats can now be used with circuses as well um another topic that I wanted to touch on quickly is the is incrementally incrementally verifiable computation so this is a scary word but it's basically the idea that if you have a computation which you can split in steps which are basically the same function being run over and over again on the say on on the state and updating the state then you can actually use recursive snarks to prove this computation incrementally so as opposed to Socrates currently to others proof systems where we can think of them as more like an Asic so your circuit is like this Asic that's really set in stone and you can do only one computation and everything is bounded and static in this case you can have a computation where you run uh one round of the computation and then another one later and you can prove the execute the execution of that competition at each step so some use cases for that are succinctly verifiable blockchain so maybe some of you have heard of the Mina blockchain where basically you use this to have a a blockchain where each time you get a new block you you verify the previous block as well as the transaction of this block and it creates a snark and then you have this kind of recursive verification another use case is vdfs because some of the vdfs actually have the structure of having some state which is then to which you um apply a function recursively and then being able to have a snark of this computation can be really really useful so what we're working on now now we're actually pretty close to having this ready in production uh is integrating a um proof system called Nova which does exactly this and the way that I'm not going to go too much in detail here but the way that the API would work for developers today is that you write a function and the only restriction here is that the input needs to be the type the input type needs to be the output type because you want the recursive aspect to to work and then you can compile this function to a specific curve so you need to use this balance curve because under the hood Nova uses cycles of electric curves so this doesn't work for any curve but we support the curves that enable that and then you can basically prove a number of steps starting from a given a given State and even after running this you could run it again starting from the last state that you had this hope this opens a lot of use cases for people who want to to experiment with incrementable incremental verifiable computation so I invite you to to test that it's it's going to be out very soon okay um since I only have two minutes left I'll just go very quickly uh through some things that we that we added recently so there's the powers of Tau ceremony so if you look at graph 16 for example it's uh it requires a trusted setup uh and the powers of Taos ceremony uh enables you to do that using MPC so you can do that directly with this Orchestra CLI um and also in soccer's JS actually we also have support for lock statements uh that you can where you can inspect certain values of your code at runtime we have support for the Marlin proof system that I mentioned earlier which is a universal setup which means that you can do one setup and then use it for different circuits um we also change a lot of the syntax uh following some feedback from from members of the community and finally yeah the this this playground which is now accessible which I invite you to to check out okay I think that's that's all I had to share uh so thanks for listening [Applause] if there's any questions yeah I'm happy to take questions foreign okay yeah um can we use the new Nova support to write another but shorter ZK VM that's a good question um I think in theory yes I'm not sure it would be the most efficient but they were already actually a long time ago VMS that were built on top of recursive snarks but they were using recursion at each cycle of the CPU which was quite inefficient but maybe there's a new take that could be that you can have on on that so that would be one cycle for each op code that that's that was the case in those uh projects 10 years ago it's a project called tiny Ram where that's what they did they basically took the state of the CPU and just ran each time one one up code and then have a recursive snarks uh recursive start each time yeah but maybe there's other approaches to leverage this proof system cool thank you so instead of making users to write very optimized code have you put effort on kind of making the compiler kind of figure out what the user wants and then optimize the circuit based on that so are you saying that okay is your question that we should instead of having low level code have the odds to have the compiler detect when it can do those optimizations right I'm involved but like you will only talk about what the case right so this is something that as I mentioned this is something that we do currently where we identify some of those use cases I mean some of those cases and just have them in the compiler but it's just really hard to cover them to cover them all because it's really use case dependent uh here this is just for one part of the shot 256 function but there's so many different things that you could optimize so I think it's actually a good idea to open that to developers but in a way that that's quite separated from the rest of the high level code thank you hey no more questions then thank you people [Applause] I was made aware of a little mistake that I made so the happy hour tonight is from 6 pm to 8 P.M not 8 am it's not going the entire night I'm sorry guys but still worth checking out the first floor after our last talk and I am also certain that there will be some other cool things actually happening on the second floor as the program of Defcon after that this is the one thing that I didn't tell you about in the other break so as the last thing that we're also doing is continuous Defcon is allowing Community meetups Gatherings initiatives activities to happen in the evenings in the venue that is anything from a chess tournament to I think currently in the hacker basement we have the deck an evening in the dark Forest tomorrow we will have autonomous worlds in the hacker basement tonight A purchased tweeted me that we will have a session demo session with a villian about autoscan Aragon sourcify in the workshop room one so it is definitely worth checking out this Google sheet the public link that's also linked in the app and everywhere to check out what things are going on on the community side of things after the actual program but without further Ado I'd like to introduce our last speaker of the day it's ji Chang and he is a in the PC team of the F and he likes building privacy or scaling related applications and today he will share little things he has learned in developing Halo 2 circuits so big Applause and a round of Welcome to him [Applause] oh thank you for coming hi I'm CeCe from PSE today I'm sharing some my developer experience for programming the circuits CK circuits so we know in ZK snark well we allow the approver and a verifier they agreed on the algorithm and then proverb can provide a proof input and the output to the verifier and then verifier can be convinced that the computation the output is a correct corresponding to the input without re-computing the algorithm again so lots of scaling and privacy we are getting from this Nars and I'm going to unroll this uh process more it's not full story but U.S circuit developer uh you are on the uh left hand side you define some constraints and uh and then you send a verifying key and program key generate generated from the these constraints and send it to a verifier and approver we call this a setup time this is like when you like finish the circuit development and deploy the project and that is ready but at a proven time like whenever you need to send a proof so approval would have run the computation and get the computation traces and fill them to the circuits and using the proving key to generate the proof and then send the proof to the verifier today we don't care about like how sausage is made like we care about like how how do you like like turn the computation into circuits so if you're coming from like a normal programming world for example if you write pythons the rest same thing you might your brand you works with a function loops and if else this kind of good stuff but when you enter the circuit world you need to do the uh agreement algorithmetization and you need to get the circuit tray so you're thinking in the computation phase of the whole computation and you think to verifying these computations with math and equations so it's like that so before we go into introduce some tricks and uh development tips that's uh talk about the rules of the game like how how this uh circuit thing work um so first of all like all computation is uh represented in finite field arithmetics so a financial element is you can think it as an integer and it's a positive integer and less than a number P and let's say if P equals to 3 then if you have two field elements two and two you're adding together you actually got a result of one because we need to do it in module or three modular p and p is usually a very large number for example 254 bits um so uh the takeaway is that we need to represent like your computation in in Via numbers insteads or bits and buys and uh you need to uh watch uh uh watch out for the wrap over and over for all that stuff and then uh we get this uh grid of stuff uh like a paper you can fill in the uh field numbers inside those cells and uh for if you you can expand this grid with more columns and but like for every new columns you edit it it would be more costly and but the raw rows are basically free but they are uh kept up to some limit uh let's say 2 to the 18 like this is uh 260k roughly so uh ideally you want to use as much row as possible until like maybe you have more uh computation you need to do then you add more columns and approving call scoring times verifying costs something like that and we have to uh distinguish some types of columns uh this this columns are distinguished by like who can see what and uh when they are like uh when values are assigned so we have the pink one is the advice column it is only visible to approval it is uh determined at approving time to put a witness the value inside this column we have a fixed column fixed column is a aside at a setup time so uh both approval and the verifier has have a copy and then the instance column so this is uh the value is visible to verifier so we can um the program can fill in the input then the output in a proving time so uh so that we can have the the scenario in the first slide that uh the further within some private values and then some uh public instance values for the input and the output using the proven key to General proof and then verifier can uh verifying the proof and also take the 10 and 100 layer you can verify in a contract or do other verification if you may um right okay so um now we have the grid we have the columns we need to Define like uh what values in the cells are correct or valid like what what of values uh are allowed to put in their cells so these are constraints and I'm going to introduce two types of constraints in Halo 2. the first one is the custom Gates so this defines the content in uh polynomials so the gate looks like this or we have a we we choose the a sales uh from column a sales from column B and sales from currency you can uh choose a whatever column you want and you can choose the relative position for example uh I want the next oh sorry next column of the uh in relative to the H cell and then you can Define the uh sorry you can Define you can divide a polynomial to constraint like okay a must equal to the B plus uh uh b b the next cell of the B and then but like this gate will be applied to all roles in in a column so sometimes we don't want to do that so that's why we have a selector here so this selector will be the one or zero value so if this selector is one then the whole uh then then this expression here must equal to zero then our constraint is enforced but if selector is zero then the whole expression is already zero so uh this constraint doesn't necessarily hold for for that particular row um so oh yeah I just want to call uh the the guitar the sentence from vitalik I heard from his talk he said like everything's a polynomial I'm a polynomial and so to give an example uh a viral example for in terms of Fibonacci circuit example and we want to prove the ends turn of the Fibonacci number and then uh we can fill in the value of uh one one two three uh by anything into the grid and we Define the gates that left plus right uh must equal to the sum and once that means that left plus right minus sum equals to zero and we use the selector Q flip to determine like which uh which row we want to enforce this constraint um so here is one way to do it like we we make the skate looks uh like very straight they are on the same rows but you can see that we are using three columns here um so this is an uh the other way you can do it you can you can make it like uh the sum you can for uh fold it to the next sale and it used less columns and but it works the same way it it still can give you your desired computation results so it's flexible you can uh determine the shape of the gates uh depends on your your problem to solve and the second uh second tools to uh constrain the grid value is copy constraints so I have this uh yellow tab here the group the values of uh two cells together and if they are glued so this value assigned in a cells must equal and this is determined the glue must be determined at a setup time there is no way you can change it at a proven time but it's very cheap and use it as much as possible so uh the final rule is that the approval can be evil like I like to think profile or like everything with the word proof verifier miners things like that as a cyborg they are the human who runs the machine and like machine runs the algorithm but like human are attracted by incentives and they they could do all kind of stuff so if you didn't write your circuit right program can witness the wrong values and still convince the verifier to do stuff so if you are like designing a decentralized mixer for this and further witness some value they can like withdraw the money out of the air then your your project is hacked okay so let's get into tricks um okay let's start from the simple one how do we uh limited uh limiting the options in one cells for example I want this cell uh to allow only values one or two or three uh nothing else so uh to do this we uh we can Define the gates uh that has this expression uh the sale value minus one times seven minus two times sales value minus three equals to zero if you plug in the three into the cell this expression will be zero but if you plug in this 100 then um this expression will be like 99 times 99 times 97 uh it won't be zeros of uh this constraint is not satisfied if you witness zero uh this would satisfy either uh okay next that's converting if and else um so for example we have uh this sample program here we have uh we have input a input B and uh there are like zero elements and happy is a like a boiling value and if happy then we do a plus b uh if we are not happy we do eight times V and then so uh for example we could have a circuit that looks like this um I used the glue to glue the topic instance value to the private value like uh so uh in the left left one we can see that uh uh we witness a S5 and ps6 and happy as one so we are doing the addition test so we got the uh value 11 and the second one we still have sent a b value but heavy zero and the design output is 30. uh we we just gave an example here we haven't constrained them um so the way you you turn this uh if else into the gate expression will be happy times uh a plus b plus one minus Happy Times uh a Milt is by B uh minus output equals to zero so that uh if you uh if you're happy it's one then this expression will be enabled and this expression will be disabled uh the other way is then if happy is zero then this expression is enabled and this expression is disabled so uh that's the um we uh we have like uh example value witness in here uh these are all satisfied um but uh now where you kind of forget something here like football can win us free here and still got the three times uh five plus six and like get the output minus 27 equals to zero so what's wrong here uh it's because that every input is a finite field element but if we want Boolean value here we need another constraint to make it boiling so we need this additional constraint here to limit the happy to be one or zero so using the tricks before okay uh next uh we need to cover the loops into the circuit and this uh let's start with the easy one this function uh initiate the variable rs0 and then it runs the loop five times and at the five at our at 5 to R and Dot it five times and like five is our constants so uh it will be easy to lay down like uh the value of R started with zero and the next plus five plus five plus five all the way to the output and then we constrain the value 0 to 0 25 to the output to the output so uh we will have this uh gate expression like this easy but uh what about this uh this looks like exactly the same Loop uh it said uh like how many times of loop is determined by the approval input and here so this will be uh about trick here because um you might have a uh so we uh we can imagine a good tension Trace looks like this um first uh per input and here and the output 25 here and uh straight or input three here put here and notice that this this algorithm this is basically the a very in efficient way to to find type 5 times n and also note that this end could be like arbitrary large but like we don't have a arbitrary large circuit so like we cannot do the infinite Loops we we can only do like only up to a certain amount of uh computation so we need to like restrict this how big this end could be let's say we receive it to five and then we need another check to make sure the end is actually less than five but it's out of score of this talk um we want to focus on how like to determine that the computation of R inside so uh yeah that's first uh do the coffee gate for zero to zero and the output to Output but because the copy Gates uh output can only locate it at the same place of the a cell so so if the N is 3 we need to like repeat the result here all right uh so um one attempt is uh to solve this is we add the program witnessed uh selector here so uh let's put one one one here uh when we uh when when we are still in a range of N and then zero zero zero here to do a repeat and using the if else trick without learning before we can Define the scale expression uh but then how do you prevent like like the purple doing this so like uh if forever like they don't follow the rules they've uh witnessed zero and like one and then go back to zero and one and uh if you do that math you can realize that program can win the three and like output to to be 10 then it convince you that five times three is ten then yeah it's a it's a failure that people say it's like blockchain is about trust I would say blockchain is all about trust issues and [Laughter] so uh let's observed like like the um like different cases of end like n is zero and it's three and is five and uh we after that s can only like for the uh valid case that uh you can start with zero but once you are zero you can only go with zeroth of the rest of the row you can start with one and you can turn one to zero anytime but like once you turn to zero there is no way to turn back so we can identify the state transition uh like this uh so this is a trigger hand taught me so you uh when you start you can start at a eighth stage that means we need to add five and you can from any state you can go back to a state you can go to the pet state which is which is a zero and like like the RS Remains the Same um but uh the point here is that once you enter the past State there's no way to go back to the edge state so we can Define the constraint here uh and the actions uh the only thing we need is here like if you uh if the current cell s is zero uh for example here then the next cell must be zero so uh this constraint can help you achieve that and also we need to make sure the selector is binary now we have a final touch like to um uh make the input uh accumulate this input value and copy back to input this still follows the same same rules for the program witness selector um so like uh why are we uh doing all these trees why why uh why we need to like doing this uh repeating and like stay transition thing um because like in CPU uh we take instruction one of the time and if you have a if else branch and if the branch you didn't enter that just you can forget it and you don't compute it but for a circuit um you need to flatten all the computation all paths you need to include it in your circuit and then so you can see here even though uh we only witness input 3 here and these three rows we still need to need them and like we need some dummy values there because we need to consider the case that proverb could win us five here and I use all these spaces so the takeaway here is that uh the challenge for the third developer is that uh we are working with the computation Trace instead of the execution itself so we need to flatten the pad and like uh work uh all possible pace of your computation uh the second is that because we are working with a field element instead of bits and bytes so we need to do some math and equations there and the third is that uh we need to work with a verification mindset because program could be witnessed uh malicious values inside your circuits and uh the tricks we talked about here are like using the volume value and one one time uh times the true path and the other uh times the false path this can convert the if else statement into the uh the the circuit expression and for the complicated Loops you need to identify the state transition of of the uh your program logic and then design uh constraints for them yeah that uh that's my talk thank you thank you for listening yeah I think we have time for a couple of questions um yeah the mic is coming hi I thought that in the original plan the copy constraints are pretty expensive can you expand why that's not the case in Halo 2. uh sorry I thought that in Blanc yeah the copy constraints were expensive um can you expand on why they are cheap in Halo 2 um I think they are I think they are the same I think that they are achieved in terms of when you use uh once and like you use it million times they are they are the same cost so I I think yeah that's that's the idea okay thank you great presentation it for somebody that wants to learn more about CK proofs what path do you recommend to go right now I think the secret explanation is really good but I got lost in some beats so what will you recommend me to to read in order to to follow you better so it's a question uh that uh what kind of material I recommend to read okay thanks um uh like there are too many I don't have a good one but like I would recommend try to find a simple circuit project and try to run it try to tweak it like removing something and and try to study I think that would be the good way to learn how circuit works thank you thank you um you mentioned like in the yeah here in the career points that you have to prevent the prover from achieving how often do you see when you're riding the circuits that how often do you find bugs because your circuit was under constrained and the proofer can cheat oh good question uh so the question is that uh like how do we find if there are bugs in there or how often does it happen that you find a bug because it's under constrained and the proofer can cheat um or because the constraints if the constraints are wrong if the gates are wrong or just trying to compare it to like usual software development when you just basically write the logic wrong right how often do you see that the gate that you come up with are doing the wrong thing um so when we are developing the circuit uh oftentimes we'll uh like realize oh we forget this connection and like program could witness that bad values uh like we found a lot of bugs in like development stage but like once you want to go to the production uh like we would like do some external updates and like try to find as much bugs as possible and uh like when you and really enter the production uh like you can only like cross your fingers and but like I heard vitalik has an idea that says they they can use like two Proverbs for the Roll-Ups and like if they can witness like they can create a value proof or two different outputs then you can like stop the raw or stop your application then and then do the fix yeah thank you thank you so in yeah sorry for hijacking the questions just let me know and I'll stop um so you when you write the Halo 2 circuits you're writing and using the rust lab right um do you think that that adds a lot of overhead compared to if you were using a DSL like circom or zocories for example uh uh I definitely feel like they're calm or like like DSL are more readable more outdidable but the thing is when you don't have DSL you just have to so you so you would like to have a DSL for Halo 2 . I like what would you like to have a DSL for hello too yeah yeah yeah yeah that would be that would be desired and great to have a DSL for help too yeah I thought someone released one some months ago didn't they or maybe I dreamed that but I don't know any anyone up here no I think it was someone from dark fight that posted on the the ZK podcast telegram group oh yeah maybe I also dreamed that I don't know yeah thank you yeah I'll go check it out thanks thanks for the info okay more questions if not thank you CeCe thank you and this concludes the program of the cold first stage today thank you all for coming if you didn't get enough yet you can still hang around in the second first and basement second and first floor and basement we will be closing the upper floors soon-ish so if you want a network hang around or whatever make your way to the second floor or lower and yeah thanks all for coming see you tomorrow again and have a great evening [Music] thank you foreign [Music] [Music] [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] 