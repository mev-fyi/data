foreign [Music] foreign [Music] [Music] foreign [Music] foreign foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign thank you foreign [Music] foreign [Music] foreign okay okay [Music] [Music] foreign [Music] foreign [Music] [Music] okay oh oh check up hey [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Laughter] [Music] foreign [Music] [Applause] foreign [Music] [Applause] [Applause] foreign [Music] foreign [Music] [Applause] [Music] happy birthday [Music] [Applause] [Applause] [Music] foreign [Applause] [Music] [Applause] [Music] okay [Applause] [Music] foreign [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] thank you [Music] foreign [Music] [Music] thank you [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] oh yeah hi [Music] can you hear me okay cool hi friends um thanks for coming to the 10 a.m future approving system session could we get my slides on the on the display please yeah so my name is Xing Tong and I work at the electric coin company I work on the zcash project um so I write a lot of ZK circuits and this is Barry hello I work at the ethereum foundation on the privacy and scaling exploration team yeah do you want me to interest yeah okay so today we're gonna have the this session is about the future approved systems so like the way that I think about this is that like the last years we've been searching for a proof system and we've spent a long time exploring and like we did a cool a bunch of really interesting tactics to make it easier for us to prime find our proof system and those included like when we abstracted polynomial commitments out of the proving scheme that made it very easy for us to to switch and update our our the proving system we were using so this led to a whole bunch of exciting experimentation with like fry and Planck and Halo 2 and all these different polynomial commitment schemes it also allowed us to like in parallel explore this idea of arithmetization and that meant that like in parallel with exploring other polynomial commitment schemes we were looking for new ways to describe our circuits and we found custom Gates and lookup arguments and this made a whole bunch of things that we thought were impossible possible like ZK VM I think without dkvm we wouldn't be able to we wouldn't be able to without lookup arguments we wouldn't be able to do the ZK VM or be prohibitably expensive but like that's the kind of historic stuff that's the past what is the the way that things are going to develop in the future so so like my my view on this or my thought is that we're we're going to spend so we spend a whole bunch of time finding these proof systems and the next thing we're going to do is is like compose these proof systems together that I think that this is this is the like the the sort of the the sub topic of today's session that we're going to see a lot of talks about all these different things so like when we when we think about this an important thing to remember or to keep in mind is that for like the proof systems that we found already we were always looking for them as like an external proof system we weren't we weren't planning to take them and insert them inside another proof system so this means that these proof systems may not be ideally suited to our new requirements so this leads us to a whole bunch of other interesting Explorations where we go back into the literature and find things that like didn't work out in the first iteration or the first kind of searching but make more sense now for example GK or is something I'm really excited about so that's that's kind of what I think I think target is going to give up yeah I think definitely how we wanted to frame this session is um looking at ways the different proof systems different projects could collaborate and could find common infrastructure that improves all of our implementations so I'm gonna give you a preview of our sessions today um the format is going to be short talks and um but in the middle we'll have this panel on dsls and intermediate representations for ZK circuits so I think I want to make the panel like a whiteboard session as well so people can like sketch out their dsls but anyway um a first up we'll we'll have a Showcase of sorts from Xerox Parks Halo 2 working group so these are projects that came out of the learning group as well as the 10-week working group and they've built some really unreasonable things in Halo 2 and sort of this is the future of proving systems in the sense that they are pushing the limits of what these circuits can do the next uh we'll look at optimizations in current proof systems that are in production so we invited um people from Aztec and ganark because these are two of the um production proof systems that are they're two of the fastest ones that I know of and they do a lot of cool tricks um together and then after that we'll have the panel um which is going to be fun I I think dsls and IRS is a good topic for a panel because um I think we'll gain a lot um from figuring out how these dsls relate to each other in the language stack and I hope that from this panel we can see some ways forward where the DSL and IRS can Converge on certain standards or infrastructure and we're going to close out with yeah like um the underlying theme of this session uh recursion aggregation and composition so we'll have talks from people who have actually composed proof systems um different proof systems um yeah so these this happened to both be ZK evm projects and I don't think that's a coincidence because I think a ZK avam is one of the most um um computationally taxing ZK circuits that we know of right now yeah that's what we have in store today and I think without further Ado I'm gonna invite our first speaker isan up on stage so Easton's gonna talk about Foster elliptic curve cryptography in Halo 2. hello can you hear me um that'd be great all right uh welcome everyone and thanks to Barry and Ying Tong for the great introductions I'm excited to tell you about this elliptic curve Library we've been building in Halo 2. okay so first let me tell you what the library can do so it's a Halo 2 library for doing various elliptic curve operations that in particular has a configurable trade-off between the proving time and verifier cost so the operations we can support is that for generic elliptic curve you have to put in a few constants we can do addition doubling multi-scaler multiplication and then for pairing friendly curves we can do optimal eight pairing verification and we can also do ecdsa signature check so we've also put this Library into the scroll and PSC aggregation circuits for the kzg back end of Halo 2. okay so before I tell you more let me show you some numbers so for ecd CS for ecdsa signature verification you can see that if we have a small proof size at the top we have a longer time around 10 seconds a little over and then if we increase the proof size we're able to get things down to around three seconds on a MacBook Air weirdly we found that on the CPU the MacBook Air beats a fancy AWS box uh for a multi-scaler multiplication with 100 base points we can get things to around 56 seconds using larger group size and as you can see that's still an improvement over the small proof size setting and finally for pairing check on bn254 which is the curve on ethereum we can do that in about 15 seconds again on the MacBook Air okay so let me tell you how we build all of these circuits so we have a very modular approach so in Halo 2 you're allowed to use a lot of custom Gates but we're very simple minded so we use a single custom gate our gate is has four vertical cells and unfortunately I forgot to highlight them but starting from the top our gate is just a plus b times C equals d so in this example on the right if we want to compute a DOT product of one comma 3 with 2 comma four we're first going to apply a gate on the top four cells so that will say 0 plus 1 times 2 equals two and then we're going to overlap it with a gate on the bottom four cells so that would say that 2 plus 3 times 4 equals fourteen so this simple overlap optimization gains us about a 25 gain for all dot product operations which turn out to make up maybe eighty percent of all of the operations we do um so our setup is that we have a number of advice columns with this custom enabled we have a single lookup table for range checks and then we have a bunch of selector columns okay so we set wrote a pseudo layout or in halotube which allows us to configure where these columns are placed in different regions so as a configuration to our circuit we allow a fixed number of advice and fixed columns and we sort of Tetris these vertical concatenations of our basic gate into the columns on top of this we built a basic Library called Halo 2 base of basic gadgets like inner product range check indexing into an array doing various bitwise operations and doing comparisons on top of this gate and that allows us to almost completely abstract away the manual signing of cells that you may be familiar with if you've written some Halo 2. so on top of that we build our elliptic curve Library mostly maybe 95 only using these abstractions one thing to note is we found it's actually very difficult to outperform using this very simple gate using fancier custom Gates we tried a number of what we thought were very clever things that did not improve the proving speed finally I wanted to mention one very critical optimization that we made which is that we previously enabled lookup arguments on every single column and that was crucial in doing our range checks we did one optimization to have special columns whose only purpose is to hold cells that are looked up and we copy all cells we want to look up to those columns although this sounds a bit trivial it actually reduces the proving speed by about the approving Time by about 50 percent and again we can configure how many of these special lookup columns we need depending on the circuit okay so just to show some plots on the results if we're if we vary the number of columns the as the number of columns grows the proof size grows and typically speaking the proving speed will increase so you can see for the optimal eight pairing for a single column setting the proving speed is about 250 seconds on the left and as we expand the number of columns to around 15 we get pretty sharp drops in the proving time but things sort of plateau after that on the right TC a very similar phenomenon for ecdsa verification um we were a little bit surprised that things Trail off at this small number of columns and we're sort of exploring whether there's something in the back end which could let it let us push it further all right finally I want to tell you how we plug this in to the existing aggregation circuits so if you have a kzg backend Halo 2 proof then to recursively verify the proof you need to do two things first you need to do a bunch of multi-scalar multiplication on the kcg commitments that comprise the proof so these would be multi-scalar multiplications on the BN 254 curve and secondly you need to do a pairing check so in the current aggregation setup the pairing check is deferred meaning that in the aggregation circuits only the multi-scalar multiplication is done so our Target was to integrate our library into the existing aggregation circuits so to our knowledge there are two one is out of PSC it's this Planck verifier repo and one is out of scroll which is Halo 2 snark aggregator the way that both of these work is that there are various generic traits for an elliptic curve library that they use and they sort of abstractly write on top of these elliptic curve traits so very nicely that that allows us to plug our library into these aggregators simply by you know re-implementing all these traits in our language and again a nice feature is that both of these libraries are able to Output evm verification code for the final aggregated circuit so here's the results so in a setting where we have seven advice columns and eight fixed columns so of the advice columns six are standard columns with this basic gate enabled and one is a lookup column we're able to verify at the actual zkvm circuits recursively so we just tried out the evm opcode circuit and then the combination of the evm and state opcode circuit and so even on a MacBook Air we're able to do the recursive aggregation of the evm circuit in a little bit over you know around 10 minutes note that the scroll and PSE numbers are not precisely comparable because we have slightly different versions of the state circuit inside uh unfortunately when we aggregate the evm and state circuit together it exceeds the memory con constraints of our MacBook Air but we'll try it on the server soon enough okay so that's it so in summary we have this Library Halo to ECC we can support pretty much all the operations that we know of that you'd want to do on elliptic curves so some highlights would be the pairing check the ecdsa signature verification and the ability to configure the base curve without too much work and we really wanted to highlight that our base component Library Halo 2 base which we open source this week which allows us to write these circuits in a much more modular and configurable way thanks and I'm happy to take any questions [Applause] were there any questions okay I I have a question so I I am one of the authors of the Halo 2 Library so if if you could change two things about Halo 2 what would they be no more if you ask me for two things the first thing would probably be and this might be very difficult to change so Halo 2 has a function called synthesize which constructs your circuit but the design of the library is that synthesize is called multiple times with different purposes and different interpretations of the input so when we're developing that's actually the biggest source of bugs because we sort of forget that on one of the times the mean semantic meaning of some of the inputs is a little bit different I don't know how that would be pulled out but that's the biggest difficulty right now okay um and also oh is there a question no um Halo 2 is a very low level um API in the sense that you can configure really every detail about your circuit so have you found that to be more helpful or more annoying we really like the configurability and that's why we're spending so much time in the Halo 2 ecosystem uh the way we found initially we found it very difficult to manage but the way we found to be productive in it is to sort of really restrict ourselves as you've seen here and then slowly add on more complexity almost as not really a pre-compile is not the right word but just almost as a special addition to our library and then restrict ourselves to only work within this sort of constrained universe of the freedom that Halo 2 offers but the reason we like that freedom is that if we ever need something more fancy we know that we can sort of add it to our base components cool and my last question is what are you using Halo 2 ECC for yeah so we're building a new project called Axiom which is a trusses indexer which allows you to take any facts a historic fact from ethereum and prove it on chain for use in your smart contract cool thanks see [Applause] so up next we have Jason um I have your slides yeah how do I full screen this can I get my screen oh thank you so we have Jason Martin um also part of the Halo 2 working group showcase um and he's going to tell us about foreign thank you issue the magical aggregation so how do you generate a graph 16 or a Planck proof right now you perform set of ceremony and then you set it for a circuit and then you prove a specific input on the circuit and then you verify or you generate is already called Data and what does auction verification look like basically you have a verifier contract that verifies the proofs so on L1 the cost comprises of execution cost as well as called it a cost but on L2 the majority of the cost is the call data cost so for a growth 16 proof the execution cost is 210 000 units call data cost is 128 bytes for a long proof the execution cost is 300 000 units and we call it a cost is around 400 bytes but what if we want to verify 50 growth 60 proofs on chain it will obviously take times 50 right that would be around 10.6 million gas units and 6400 bytes but what if I told you that you didn't have to live in that world anymore what if we could prove 50 proofs at the cost of one and yes we can do that and we can do that using aggregation what is proof aggregation now basically you can get several proofs into a single proof and that is it and that is what True Value relation allows you to do and how do you do it you can do that using maze it's a tool that helps you irrigate individual flock proofs that you can generate using Snapchats into a single aggregated proof and why should we use it basically two zero bytes so it doesn't matter whether you're aggregating 20 proofs 40 proofs 50 proofs the call data cost will always remain the same and the same goes for any the same goes for the execution cost that would be approximately around 600 000 gas units so let's try it out to build an aggregation circuit you need a bunch of prompt proofs for which You'll wish you will build the aggression circuit so we generate the Planck proofs using snark.js and then we build the illusion circuit using maze so I've already done the necessary setup for generating plan proofs using Snapchats you can see the circuit is here and we have the proven key file over here now we'll use the circuit Files The Proven key file to generate a bunch of proofs for which we will generate the for which we will build the execution circuit so for that first we need to install a focus log.js that adds this new command called snatches Planck setup mails which helps you output necessary files for building the ignition circuit so it takes inputs.json which contains several inputs for which you need to generate the proof so over here we have two inputs this and this and then it takes the circuit file and the circuit.zk file which is the proven key for a single prong proof and will generate proof.json public signals and the verification key so let's run this command and it will generate the necessary files so we have got two proofs public signals and a verification key so now we'll use the mace tool to build an aviation circuit so let's check it out so Maize tool has a bunch of commands the first one is mock setup mock setup builds the aggregation circuit and runs a mock prover on it and then we can also generate a email verify for the execution circuit and we can create a proof verify the proof and we can also simulate even verification of the proof so for the demo purposes I'll stick to mock setup because rest of the commands are pretty computationally expensive so let's just run the command maze mock setup on the files over here so we have the verification key we've got the proofs which we need to Aggregate and the public signals and there you go so we build the existing circuit for two proofs and now it is so it will take a while until then let's just try to understand how does it all work so basically in the verification of a plan proof you have two parts the first part is the cheap part that performs the computation required for pairing check and the second part is the expensive part that is the preparing check so the trick is that in the aggregation circuit we verify the cheap part of each block proof and within the aviation circuit we accumulate the expensive part of each block proof into a single accumulator single pairing check and that is the final accumulator and we expose the final accumulator app as the public input of proof now the verifier would will first verify the proof of the aggregation circuit and then we'll perform a single pairing chain on the final accumulator and after this the verify would be satisfied of the proof of the aggregation aggregated proofs and that is it so now let's go back and check whether the equation secure is so successful or not and you can see over here it was a success and it took 22 seconds so that means our aggregation Circuit of this of these two proofs is satisfied so now you might be wondering okay fine we're able to aggregate a lot of proofs into a single proof and we're able to save a lot of lot and call data and the execution but what are we incurring in exchange the thing that we incur in exchange is the approval cost it is very competitively expensive to generate aggregated tools so for 25 proofs it takes 27 minutes and for 50 proofs it takes 58 minutes on a pretty beefy machine so that is it and that is it from my side thank you for listening thank you so Jay is on the zoom and um can take questions are there any questions I'm I can't actually see but any questions so just shout at me okay well I have a question oh no my zoom got disconnected I think yeah it really disconnected okay well yeah I can't do anything about that sorry jay um so are we back hi Jay can you hear us is my audio still linked to the it is uh okay we cannot hear you now we can't hear you okay let's let's send JR questions async then oh but we can see you but we cannot hear you anyway this is Jay he has a face okay um yeah I I am gonna move on because I'm three minutes behind schedule okay um but thanks this is a great presentation um our next top speaker also couldn't make it let me just so that concludes actually the Halo 2 working group showcase um and now we're going to enter the second section which is optimizations like cryptographic optimizations of proof systems that are in production um so Adrian from Aztec can you hear me now oh we can are you Jay uh can you hear me now yeah hello we can hear you so I had a question for you which is um uh uh no you cannot hear me yes so I'm actually simultaneously learning okay I'm gonna message Jay privately yeah okay so yeah the next um presentation is by Adrian from Aztec uh the future of punkish systems and I expect Adrian on the zoom later yeah hi everybody I'm Adrian and I work at Aztec basically making snarks faster can we make it louder I hope you're all having a wonderful time in Bogota this week um unfortunately I wasn't able to make it due to getting sick at the last moment last week and so this is why this presentation has been pre-recorded I'm going to be talking about some thoughts and hypotheses I have about what the next generation of plunkish systems is going to look like um by the way there's going to be a q a once this video finishes so write down any questions you may have with slight numbers I'll do my best to answer it at that point and here's my Twitter and telegram handle if ever you have any questions that come up later so I'm going to start with a brief recap of the evolution of plonk until now in 2019 Zach and Ariel came up with this certainly named proof system that kind of revolutionizes the ZK space um the state of the art until then was basically growth 16 which is based on r1cs um but the per circuit trusted setup made it a bit more complicated to deploy with Planck now we had a ZK snark with very similar performance profile it only required a one-time trusted setup but really I think it's one of the main Innovations was that we're now able to represent circuits in a much more practical way which made writing circuits easier because they re the resulting representation looked a lot more like the actual functionality you wanted to compute over the years it followed many new techniques and tools and improvements were introduced that basically tried to tackle concrete efficiency um there's the introduction of different custom Gates um which allowed for more efficient elliptic curve uh group operations for uh different hash functions or really any kind of uh more specific computation you might have in mind that might appear several times inside of a circuits we also got to lookup protocols which allowed us to essentially pre-compute the inputs and outputs of more complicated functions there might be a lot harder to do inside of a circuit and in a way that we can just look up the results from these huge tables and this made a big difference especially for uh not for operations or functions which are hard to do inside of a circle with that works over finite fields um it also made range checks a lot easier to check that an integer is in a certain range but beyond that there's also a lot of work done on just improving the tooling and the introduction of quite a few languages and um and dsls like Noir or Halo 2 which essentially help make more optimized circuits and one thing I want to note with all these improvements is that they all try to tackle the same thing which is prover time with a lot of these what you're trying to do is make the circuits smaller more optimized but the real concrete effect of this is that the proverb has to do less work unfortunately there's only so much we can do and we're still faced with the same roadblocks like the ffts which mean that the proving time is going to be lower bounded by n log n so you may wonder what's next for plunk well recently at ZK Summit 8 in Berlin uh Benedict buns from espresso system announced hyperplunk and it claims to drastically efficially increase the efficiency of uh plunkish circuits the main Improvement is that there are no more ffts involved and the proof can now be generated in time linear in the circuit size at the heart is subject an incredibly efficient protocol from the 90s even though it's been out for almost 30 years at this point I don't think it's received nearly as much attention as it deserves as especially in the context of snarks this is great because this means we've got a brand new problem that's ripe for optimizations so in this talk I want to go over some of my personal hypotheses about what changes were likely going to see around CK snark circuits and deployments now that we've got this new proof system to work with so instead of a snark we need an efficient way of checking that n different polynomial equations vanish over a predetermined subset of a finite field previously this was done using a quotient test that relied heavily on ffts however we've now found out that this is doable with some check as well from a high level this table shows the main differences that come up when we use one system over another first of all we have to change the way we represent polynomials instead of interpolating a vector as a univariate polynomial over the roots of unity we use multilinear polynomials over a binary hypercube efficiency we notice something particular we have to sacrifice small proofs and fast verifiers in exchange for faster provers but when you want to implement snark these asymptotics hide way too much detail and I'm going to argue that the benefits for the prover are actually a lot better than just removing this login overhead from the ffts so let's get into the weed a bit um recall that that we usually want to prove that we have this polynomial P usually called like the Planck identity which is a polynomial which equals zero when it's evaluated Over All Points little H belonging to a larger subset H now what this this Planck identity really look like well I'm going to give you a simplified view of it but essentially what it does is it is a sum of all the different Gates um that are make up our circuits and multiply it by different selectors which either activate or disactivate a gate depending on which operation you're actually trying to do in each row now I want to emphasize that um this is a curse simplification of what is actually happening mainly to get everything to fit in the slides um and so don't go implementing this stuff um if we look at this identity I wrote down what we have is a certain number of custom Gates and each of them are multiplied by this multilinear binary selector polynomial SJ so this is just a vector the interpolation of a vector of zeros and ones and it multiplies this more complex gate polynomial which is going to be zero if the the gate is Satisfied by the input wires W along with some fixed constants uh Q so both W and Q are multilinear polynomials and G is just a function which combines these now if we take a closer look at the selector we can see that it's just the an indicator function over the over specific set HJ now I'm choosing to partition H in this way because it makes it more clear like in which parts of the circuit in which rows each gate is active and this is going to help us a bit later on basically the circuit is valid as this polynomial vanishes which is only the case when either the selector is zero for each gate or the gate itself equals zero the way you'd prove this in standard Planck is just by Computing the polynomial which divides this polynomial P um where the the division is by the X N minus 1 which equals zero over all the roots of unity how do we do it with some check though now the way is usually presented is that it allows a pervert to prove a claim of the type Sigma is equal to the sum of all the evaluations over h of a certain polynomial function p now we we this doesn't help us right away but instead if we said P Prime is equal to P multiplied by this weird random equality polynomial that's often used uh in some check um proof systems well we can set Sigma equal to zero and because this random linear combination zero well proving the sub check essentially is equivalent to a Vanishing test now unfortunately due to the time constraints I I'm not going to be able to get into more detail about how to perform the actual sum check uh these are all selfies involved and I need to make some big small vacations but one of the one of the observations I wanted to make is that if we look at the some checksum for one particular gate um so we just take the gate from the previous uh slide and multiply it by this equality polynomial this selector polynomial plays a big role because we notice that when H ranges over big H well sfj is going to be zero whenever the gate is not active so this means that we can rewrite the sum ranging over um with a set of points at which H at which G is active now concretely this means that the prover only actually has to evaluate this polynomial equation G in the number of times that it's actually used in other words the probing time is now much more closely linked to the time it would take to evaluate the actual circuits because we don't need to evaluate the gates in at indexes where they're not active um and the verifier time basically only depends on the maximum size of circuits again this is due to some check but for each additional custom gate that we have it only adds a constant amount of overhead so if we take a step back what do we actually get when we use some chicken practice so what I expect to see is it will have provers to find with way more custom Gates than we previously have and you may have some which have way more complexity which are able to do a lot more work at once even if those gates are not used very often because essentially due to those sparsity that's preserved by not having to do the ffts the prover is only going to be running the gates that are actually active in other words you're only paying for what you use to me this kind of feels like we're heading to a more uh an architecture for circuit that resembles a bit what we had with like x86 processors which are complex instruction set uh processors where you've got many different kind of functions um this is compared to the risk or reduced instruction sets type of processors where you only have a couple of instructions to use from so it's kind of interesting to see this difference in How We Do circuits compared to processors um however this isn't um the only thing that we get from some check in fact um if you've got like rather optimized implementations you 'll be able to see way smaller memory Footprints uh than what we currently have and another nice thing about the memory is that the access pattern is a lot better everything is more sequential which means that you're able to way more easily parallelize this some check procedure now unfortunately we're still gonna have to deal with this annoying login uh verifier cost and proof size but one thing that's happened over the last few years is we we got a lot better at doing recursion and we understand it a lot better and what this means is we will be able to more efficiently and more easily um write a standard plunkish circuit that just performs the some check based plunk proof verification and therefore we end up with a Best of Both Worlds situation where we got a really fast prover with relatively small overhead to generate a proof and the final proof is going to be constant sized with a fast verification another really nice thing is that some check doesn't really rely anymore on on a specific field so we because we don't do any more ffts but we don't really need any specific field which has a large um root of unity which means that we might be able to deploy snacks over different curves now um although that also depends on the trusted setups we have available Etc so with that I hope I've convinced you that um there's a lot to look forward to in terms of improvements for snarks and the future is going to be bright full of possible optimizations and I in any case can't wait to see what everybody is going to come up with um please let me know if you have any questions and again here's my handle if ever you have any questions that come up later thank you thank you Adrian oh yay hi Adrian can you hear us can you hear me okay there's a delay yeah but we can hear you um so any questions for Adrian gonna put my glasses on this one right here I actually have a question um oh you have a question go go for it yeah thanks for the presentation I have a question about the some check argument uh how we can handle not these things canceling out to zero are we going to need to choose a specific field size so because we are just adding up this stuff and we are assuming if they are all zero addition will be zero but what if sometimes are not zero and they somehow cancel each other out would that be possible or why it's not possible please so there is a delay um let's agent did you hear the question Zoom but I just go from the video I don't know if I'm a little late or anything but if I get your question right it's like do when you have the sum over all the gates uh what I'm missing here is like the linear combination Factor like gamma like the the kind of one you would have in uh plunk as well that answers your question yeah it's like a random linear combination oh I get that we can do that for uh combining the addition for different circuits but for each circuit we have a sum right on HJ so why that summation would not be zero even if there are cases that all the terms are not zero does it make sense I I got your question there's a delay um I give up um right so I missed like all this uh like the actual soundtrack protocol what happened actually happens is you're proving that some by sending uh uniberate restriction polynomials so you you sum over like part of the points and you keep one free variable and you said in log n the universe polynomials which need to satisfy some property and like that's taking care of the Assumption okay we can continue this discussion async but thank you Adrian um cool bye [Applause] all right that's all our recorded presentations for today um could I get my screen turned off for for a few seconds please I yeah I'm aware um so our next speaker is Yousef from ganark are you in the room oh could you come up thank you um and Yousef is going to show us some of the cryptographic optimizations um that they've done in good night I have your slides yeah or I'm downloading them cool do you want to Clicker can you try clicking okay uh hi everyone so my name is Yusef husnir I work at consensus and I'm gonna try to set you clock well actually I'm gonna talk about some of the algorithmic optimizations we have in Canal that makes it fast so we are a team of five so far and we are building two libraries in go so one is called gnoc which is a easy to use open source library for snacks and the other one is Gnar crypto which is a cryptographic library in Glo so uh Glock under the hood is basically composed of these components so you have a front end when you write your circuit a backend for per generation for verification and then underneath you have this you can architecture Library which is bearing based cryptography on the electric curve based cryptography and finite field arithmetic Library so we have all the stock written in group no dependencies so in the back end we have so far got 16 and Planck with two polynomial commitments so kcg and Phi we have in the front side a standard library with the mimc ecdsa eddsa banks in circuit builders in circuits and we have both apis for a native field and non-native field arithmetic and in Glock rate so we have a bunch of ellipticals ba and BLS 12 BLS 24 Two Chains with BLS 12 or 24 Twisted Edwards we have fast multi-scalar multiplication fast pairings kcg 5 pillow caps and we have already recently implemented some check protocol gkl and in the finite field arithmetic we have different sizes ranging from 768 to 256 to cold Deluxe as well and it performs very very well on different targets so the usual workflow of snacks so the same in Glock so you have a circuit that we write in Blingo so it's not in a DSL and then we compile it to some constraint system and then you call setup move and verify API so it is fairly easy to change the elliptical you want to use and the constraint system you want so you just change for example here BN to which so far by bns12 and the l1cs which is class 16 by SCS which stands for sparse constraint system which is blocked so we have a playground where you can play with it in the browser with ghost 16 and Planck to see how you can write your circuits in go and then you can download the constraints and look like look at what they look like both in row 16 and snack so why Glock so we have no DSL plain go so no dependencies at all we compile large circuits in a few seconds so when you write your circuits in playing go you can use go to Links standard go to links to the back test Benchmark your circuit but also we've developed a cool cool thing which we call constraints profiler so by just adding two lines of code you can have this key this this figure here where you see in each function how many constraints does it consume um yeah and the several packages are already audited by algon and fast tested by get for one year I guess now and there is one code base that performs well in different architectures CPU mobile wasn't so in the mobile we have 70 faster than the Baseline in the Z price um so the question is why Clark is that fast so here I give an example of course 16 snack prover on bn254 okay so this means msm's computations fft computations and parallelism so I took the examples of two of the most used libraries so Arc Works in us and so you come with a rapid snark back and in C plus plus so two kind of uh circuit sizes so one is 65k the other one is 8 million constraints so this is a AWS AMD machine and we see that Canal performs very well for both kind of circuits so there are Supply levels that are heavily optimized or large circuits for instance Glock is optimized for most so this is for the previous site for the verifier site which is got 16 snap verifier so which is mainly pairing computation of lbn 254. so again we see that on same machine um Glock is very fast so it's uh a bit more than one milliseconds on this machine to verify to computer to verifiable which is mainly a multi-explanator bearing computation um yeah so it's a PDF it was interactive but anyway so the question is why gnarok is that fast so remember this diagram from the very beginning so we have a front end back end I'm gonna click tool underneath and the question is so I will go through some of the algorithms that are highly optimized in knock so we start by writing a circuit see we generate the proof Pi of C which means that we will call to compute ffts and mostly msms over so yeah so I'm giving the example of a recursive course 16 proof and just to concentrate over the algorithmic optimizations I'm using at 2J so there is no long field arithmetic but it works also with a long video arithmetic um so it's called the animation of a BLS 2577 and then we write a circuit C Prime of the both pi we generate the both Pi Prime of this circuit C Prime so the aggregation which means that we will call again to compute a multi-scalar multiplication over the bw6761 curve which is the object curve and we verify the proof step five which means that we will call an architecture to compute the pair so I will be talking about the optimizations in those points that are in that are in boxes so mainly msms and pairings and writing circuits um so Amazon's over BLS 12377 so I this is a graph from the zpice submission uh so I'm comparing to Arc works because it implements VLS 12377 so we are 40 up to 47 faster for 100 points surrounding from two to the a to 2 to the 18. so this is tested on mobile so on a Samsung Galaxy and we have two so here I have two versions of Glock one using Twisted Edwards curves and the other one is using shortwise curve so the one is 40 to 47 and the other one is 20 to 35 faster and the question is why so um so both implementations do not use by computations but use parallelisms but in a different way I'm not going to talk about this so we use two enough buckets which reduces the size of the buckets by twice this is not using networks but most importantly here is the curve form and the coordinate system so we proved that any inner curve can be written in a Twisted Edwards curve with a equal to -1 and we extended a coordinate system so we call it custom xyt in order to make computations faster so why is that so um so I call a b bit MSM is an MSM of size n with scalars of B bit so generally so all the libraries implement this variants of piplanger which is bucket list method so it goes in three steps so you'd use the B bit MSM into CBS MSM for some success for some fixed C we solve each C bit MSM efficiently and then we combine the C bits MSM into the final bibet MSM so the overall cost is this one so -1 is in blue is when you use this in enough encoding of the scholars otherwise so you have two to the scene but this overall cost can be uh explicitly broken into what I call mixed additions additions and additions and doublings so um so for large msms so what is most important is the mixed the additions because they scale with the number of points and the others are constant so if you look at all the shapes of the elliptic curves and all the coordinate systems that are over there so you can look at efd webpage so Twisted Edwards with a equal to minus one with extended coordinate system have seven multiplication for dedicated addition uh compared to for example 11 in artworks with Jacobian coordinates um so what we did is basically when I say we addition is so those points are re-added in the package so they are the same so when you look at this unified addition so which means that we will not have any band any F else branches to to handle exception case so it is one multiplication plus but the multiplication is a multiplication by a constant which is 2 to the D so we come up with a custom coordinate system which is instead of having the Tuple X and Y you have y minus X Y plus X and 2 times D times x times y so you can do unified additions without version at the same cost of the a dedicated addition to seven multiplication yeah and this is basically one of the the optimizations that makes MSN faster in crack this sandbox was right in the circuit C Prime of the verification of Pi so in the previous presentations we've talked about parent check inside a circuit and how much it is expensive right so there is a long line of research of pairing computation outside of the circuit and we can do computations of BLS appearing of BLS 377 outside of circuit in less than one millisecond but if you put muchaches mutant is those optimizations inside of a circuit the number of constraints is roughly 80 000 in iOS so we were able to reduce it to eight so 11.5 constraints so there are a few implementations so far so there is one in our quarks one in lips knocks and I believe one a new one in the ZK pairing by zero EXP in artworks which was the state of the Arts for this computation it was 19 000 constraints but we were able to reduce it to 11.5 so the the paper is here uh you can check it uh but the main ideas are basically um so the inverse is not costly and I almost here so you can do double and add enough fine so not double and but double and uh we have a different representation of the line so that we have sparse multiplications by the line in our CS wise we use Choice based arithmetic inside of the circuit so it uses inverses that's why it wasn't used outside of the circuit but inside of the circuits it makes sense and the finance pronunciation is also using a simple Atomic square instead instead of Granger squats which is not used outside of the circuit and for both the Miller Loop and the final exponentiation we use short addition chains but the trick here is that normally in a short addition chain for of double and ads you would like to have to optimize the the the doublings because doublings are faster than additions but constraints wise it's the opposite a doubling customer constraint than addition because the line slope is has a square so it scores two constraints instead of one just switched division so the idea is cross section is only in 19 K constraints BLD signature is 14.8 constraint case constraint and kcg verifier is just uh 20 000 constraints um the last box so pairings over bw6761 um so we can compute this on a inside machine here in 1.22 milliseconds compared to 1.76 71 milliseconds and the question is why so actually we do not implement the same formula so um appearing is so of p and Q is this m of p and Q which is what we call the middle Loop so it is a loop and a finite exponentiation which is the X pronunciation by Q to the x minus 1 out of r so the optimization comes from the middle Loop so the original paper of bw6761 has this formula for computing the pairing which is f of U plus plus one and F of u q minus uh Square minus U so which are two Miller Loops of size U plus one and uq minus U Square minus U so when you see those uh so they have bit size 64 and 190 and they humming weight in two and a half is 7 and 31. so what we did in gnoc is that we observed that U minus 1 square the Hamming weights of it in two half is just 12 compared to 31 and we really arrange the equation so that we uh have this second equation and basically you have two Miller Loops still but the second middle Loop uh this one is using the results of this one and it's not starting at one as as in these two and this is just a line computation and this point is already computed in this middle so you have really just two Miller Loops as in here but with uh size with the loop size way smaller uh the X pronunciation by by Q are are cheap because these are just four videos so we have also a paper about this it is only punch you can check it out and we have a hack MD here blog where it explains uh the changes between these two and other algorithms because this one is just for one pairing and for multi-pairings we have other algorithms so a novel algorithm that we call Twisted States it was because in this kind of ellipticals bw6 G1 and G2 are on the same field so you can use State instead of eight and you can use the endomorphism then for multipacks it was way faster than than what we had previously so um this is just a couple of optimizations that I talked about in in Glock so it is really optimized in all the stack um if you have any questions feel free to contact us on by email or on Twitter there are some useful links here and I'm happy to take any questions thank you [Applause] any questions hi good talk thank you I want to know why did you choose go instead of rust for example uh good question so um this work was started before I joined sectors already didn't go but yeah so um um so we think that go is still also fast but also because there are many libraries and projects over there that are using last and there are many blockchains that are using go so it was nice to have the snack Library written in Ingo so that we can have native uh integration with them you mentioned you guys had support for lookups in garc is that already released or yeah so it is but we do not have plonk with P Lucas we have we have just the pillow cap argument in Calcutta that will be used for the zika evm uh sorry I guess so you're saying you can use plookup but how to use it without the Planck integration so we do not have the integration so far but we have just the pillowcap arguments okay got it cool thank you Yousef um so yeah thanks this is a good segue into our panel can I call Jordy um Louis Brandon and Kevin J on stage and can I get someone to help me move this whiteboard on stage please yeah so our next panel is on dsls and IRS for ZK circuits oh foreign cool yeah yeah we have only three mics [Music] um so yeah these are all um implementers and authors of some very popular CK dsls and IRS um and so today we're gonna get together and um I've prepared a few high-level questions here I'm gonna read them out um so the first one is sort of an exercise to locate each other on the language stack and see how we relate to each other and the second afterwards is to then figure out um to what extent can we share infrastructure or should we even try is it impossible after a certain point um and over here I want to note also a lot of Prior work from the compiler and smt verifiers communities so can we also reuse some of their work and then lastly I had a high level question about you know what programming in ZK should look like what's your ideal DSL is it very different from normal programming yeah so just keep these in your head and I think I want to save maybe 25 minutes for audience questions at the end so I think let's start with a round of introductions um I have like I have pre-prepared some visuals for you um in case you need them so the first first one is pill Jordy so can you give us an introduction to what pill is a language mainly it's Bill means polynomial identity language and mainly what it is is when you define a circuit that's based on polynomials for example a Planck is an example of a way of um circuit that's based on polynomials but you can do much more things much more complex uh that's our based in polynomials that still allows you to just write those identities and then out of those identities just build the proof automatically the same way that you do in a normal uh zero knowledge language the same way that you did in circum you can do it in peel here you need to do something else because you need to maybe write the weakness somehow but it's just a kind of abstraction where you can just focus in writing what are the identities uh that you want to fulfill and yeah this is mainly what Spiel and Jordy also is the author of circom um which is a very another popular Library so how does pill relate to circom well pill is for polynomials and circuit is more for uh signals you're just working we are not working with polynomials at all you are just building uh you have just signals and you just build constraints around those signals not constraints around polynomials so um the idea of circum is uh it's circum is a low level language because you can write everything every every uh every circuit you can write it in circum circum does not have any special library right it's better than you need any plugin special so all the circum is rated in circum so it's the on there but at the same time is a high level language because it allows you connections so you can have you have blocks and and then you connect blocks even small blocks you do Big Blocks Big Blocks you do a bigger blocks and so in this sense you can go all the way down and on the way up uh on circum that's why it's so flexible and then it connects very good with snark.js and other proving systems and yeah it has been there for years has been already a full rebrite we now introduce uh introduced uh these new extensions of the language it's mainly um mainly our Anonymous templates and we also already are adapting to Planck circum was born in Darwin CS era okay and nobody know who works perfectly in Planck so we have also custom Gates and yeah we're extending uh the language and it's a full community that's working on there thanks Jody so yeah to me it's interesting this um distinction between a language for polynomials versus first signals and I'm gonna skip to noar because as I understand it nor can Target both Planck and r1cs and air and even more back-ends so Kev what's Noir about um can you hear me yeah so Noir is a lot more high level uh it compiles it compiles down to something called a Sear and essentially you never tell the proving system how to do something uh you just tell it what you want to do so for example uh if you want to do a char-256 uh you don't tell it to use custom Gates the proving system just decides uh what's the best way to do it so in this way we're sort of back-end diagnostic fully so for example you'll never see like uh custom Gates inside of Noir or any sort of that syntax or lookup tables you might see maps for example yeah so yeah cool um I'll just go around I have questions I'll just go around the intros first um so this is the picture I found for Cairo I really apologize I'm sure that better pictures exist but damn I feel this is the best either produced uh 100 um so yeah so I guess Cairo deferred to um to some extent I wouldn't be able to actually go into a bunch of these friends but Carol is just a turing complete language so you were writing carrot you were the same way you were writing solidity or in any smart contract language that you can think of so from the perspective of Cairo you write a program this program is completely to a carabite code which is run into the Cairo VM and proved among all the programs that are proven within the same proof so in that sense it's like there is not things much much difference you don't have things now you don't have lookups you just have like um you know programming and sometime for some operations we do have this notion of built-in but built in are just something you call as part of the function and the best for last kimchi slash snarky JS um Brandon hello um thank you yes uh so uh this is snarky JS with a Y different from snark.js had a few people confused this week um yeah uh you know name name might evolve I don't know um so uh yeah so kimchi is our is the proof system that snarky JS is built on top of uh right now um the way that it's built we can plug into other proof systems we just haven't yet um kimchi is the proof system that is in uh Mina or well in in the test net of mina that will be hard forked pending Community voting um but the idea is I mean kimchi is a Planck 15 type proof system it has no trusted setup it's Universal uh and uh we support like infinite and efficient recursion which is important for mina this is sync blockchain um but it but the cool thing is you can also use this proof system to build Uh custom zkps which are our smart contracts on Mina um and uh and you can build circuits outside of mina as well so uh I I don't know there's not much in this picture I want to explain I guess the the one really important thing about snarky JS you can see on the top left there um the the code snippet um it is typescript and is extremely high level language um and and there's this sort of uh I guess there's a lot of reasons why we designed it the way we did I'll just point out a couple things one is we get all the tooling and infrastructure and ecosystem that comes with typescript um so you know npm is great vs code is great all the Integrations already work and are good um and uh we think that you know when people are writing zero knowledge proofs or writing any programs like the model that people are used to thinking about computations is you know you have a function it takes some input and you produce some output so one of the things that snarky.js does is it lets you write your computations with your circuits at the same time and the output of those functions gets sort of fed in as a public input automatically for you so in the end your your code actually looks like normal code cool thank you so I'm gonna start with an opinionated question which is um what is what do you think of um intermediate representations so what Cersei is this project um that's inspired very much by llvm and everyone's turning to look at the screen but so the idea behind that is to define a common standard IR um to which many different front ends compile and from which many backend many different back-ends can compile so how well do you think your project would fit into a model like that like yeah do you already okay yeah I'll start I mean yeah so so uh snarky JS is like I said like super high level um and right now it's directly connected to kimchi but we could and probably should connect to some intermediate representation so that we can you know share that work of connecting to custom back-ends in the case of circum actually it exports to two different languages [Music] internally it has an intermediate representation already is a second one so exporting that to other language like rust or like go is in the in the road map and uh okay exporting to maybe some other intermediate representation if it looks like uh good enough uh that's uh that that would be definitely possible here I'm not sure about the specification I have not read it yet but for example there are things that how you paralyze uh witness computation in circum we have a way that you have different components because there are components you can say that this company in this company is is can be run in parallel so the windows computation can be run in parallel this is very important especially when you are doing big circuits so uh you know I'm not sure maybe some somebody that's explained that would that would be maybe an answer for for them but it's uh sometimes there are details there that are not that that easy so in the context of Cairo um so it won't really even work um I mean because Cairo as of now doesn't have internet representation and directly by basically just a very high level you know like with like a series of representation just like compared to assembly and Carol itself Carol the current version at least to just directly generated by code that said Cairo we have Cairo one coming in a couple of months uh and camera one has don't have something called Sierra and Sierra is which I literally means safe intimate representation and the purpose of Sierra is to enable uh the language to turn deterministic actually so Cairo is not deterministic but Sarah will make it deeper Mystic because when you make a language for an L1 of course on F2 for freedom for blockchain itself if you want determinism you want determinism for a very specific reason actually which is you want to be able to prove Fair transactions without determinism you screw it for that and so we for now we don't have it and it is a dosed actor that we control by making by right now whitelisting the contract but tomorrow when we're gonna go for permissionlessness and then video centralization you can obviously do that so we are working to have this safe intimate representation which would be deterministic and this intermediate step will be able to actually compile to multiple backends so either Cabo or like an optimize for xc6 for execution layer and potentially something like this but in that case we'll probably lose most of what you're gaining and what you wanted so uh I I don't see I don't know if it's practical to to compile for I I don't know about Cersei but I would be it's a great name for Stark by the way uh but uh yeah I don't know uh I don't expect so I don't know if you could do that yeah so we did once have an effort to integrate a Seer with Cersei uh we dropped it just due to priorities uh from what I remember it was fairly easy if we extend Cersei to have uh these black box functions uh just because the Cersei IR just can't optimize uh Freddie's Black Box functions um so yeah it's possible we just haven't picked back up on it yeah I I think nor is is probably the newest shiniest language like it just dropped um and I'm I have lots of questions about Acer acir is noir's intermediate representation so it can compile both to r1cs and Planck um so how does this IR capture both of these very different arithmetizations um essentially you have these arithmetic Gates which are just linear Expressions which aren't bounded so for r1cs that's quite simple to compile down to for plunk because you usually have like wit 4 or some uh fixed width we have something in a sear which basically chops the gates into the perfect width for a blank for the Black Box functions it's really just the proving system just takes in the witness indices and just fills in the constraints for them so it's quite simple what it's doing it's not doing anything complicated yeah that makes sense Brandon did you have thoughts um on the IR thing yeah well I I think I spoke in the in the beginning a bit I mean yeah I I guess yeah I don't have anything else to add um cool um so what I'm hearing is like basically compiling to an IR you could stand to lose some um some very specific performance optimizations uh unless the IR allows you to Black Box things easily cool um I did have a question for Louie as well what do you mean when you say uh Cairo is not deterministic and that your IR is deterministic so so Cairo today so proof our dkps or language domestic you can create hint and that break vitaminism um but we we Carriage day is not domestic so for men to be more specific we actually have two flavors of Cairo uh what we like what I like to call Pure Caro which is the one you would just directly prove a program within within sharp or sharp being the shareproof infrastructure that we provide or um or um that's your pure Caro and what's Stark axis stockx is built on and even Stark net itself is built with and you have what we call Stark net Cairo and Stockton Cairo does have a notion of State Dove has a notion of like Cisco and plenty of things that you get from the OS itself so you have like Cairo for a start net you have Cairo for your pure Cairo so the thing that the Cairo Force darknet cannot be non-deterministic and that is for a very pure incentive reason because if you don't have determinism that you can make transactions that are not provable meaning I'm the only one we know that this transaction is started and I'm the one with other hands and therefore the rest of the word cannot prove it and so for instance let's give an example that people like to choose you know I want to do a force transaction from L1 you know being that optimistic roll up push I mean talks a lot about um then in this context with dynamism I can't this is a dose Vector because I don't know if the transactions are getting pushed to me is provable by me I don't know that and so and another thing that is important is that fair transactions so fair transaction can be valid you know the price of this pull on uni swap move and L4 my transaction failed and otherwise it can be uh malicious so unspamming the network with transaction hoping to get what I want but if I don't I don't think I don't pay anything and that's why I can spam the network run everything and I need to run it and so creating those Vector so Force darknet we have to make Cairo deterministic or I mean the version of Cairo for Stockton will be deterministic for that purpose and to make it deterministic we need to prove that the program was compiled into a deterministic version of Cairo which is a safe intermediate representation called Sierra and the Sierra will compile to Cairo itself and so itself will be proven within stock net when I'm deploying a contract I will be proving to use darknet that this contract was compiled with using Sierra so this is what I mean by Carol become deterministic in that context [Music] I guess my next question is uh number three which is yeah when you were writing your languages what features did you prioritize so for example um Noir is very rust-like they even have a version of cargo called nargo um and then uh snarky js's like typescript um Cairo is also a pretty high level yeah it's mixed right now it's a mix of scene but right in the new version will be very rust-like which is coming into three months two months right and for example ganark was just written directly in go Halo 2 is written directly in Rust but but pill and circum are like kind of closer to the metal I think so is Halo the metal the circuit I mean um and so is Halo 2. so yeah what are your thoughts on uh the trade-offs between like higher level representations and lower level representations and um how did you you know decide on the designs that you did can maybe start so the first that's actually an interesting story from from uh from Star War expertise experience so the first version of Star Kick so Stark X was and it is the first product in stock where it did which is basically a centralized but non-custodial backhand for uh dabs who need scalability so you know customers are dydx diversify uh immutable and software and so the first version were written you know very low level literally in polynoms uh and this thing does transfer and swaps and so we started to work in Cairo it was 2019 and it became production ready uh in uh in me 2020 um the first case of actually uh where we used it for what's for the ready to buy Bake Off in June 2020. um and so um the funny story that the improve the performance of the of the system massively improved when we start using Cairo because it was much higher level and the crew had a lot more flexibility in the way we could reason about the system in the way we could write about the system Not only was again in philosophy in velocity but also again in actual Pure Performance and so um there is you know in some green of and when you need that performance Improvement we have these things that are built in the same built-in as equivalent to Cairo to those specific shape on your CPU like the one that does 64 bits means and so if you really need like optimization at the very low level for a specific operation like say proving um right now we did it first okay check we did it for um um for um uh for a series of signatures we in the range proof and sort of things that we write them in those and for the rest we just write in color and so there was not really any trade-off in any meaningful way except that much simpler and nicer to write nowhere too yeah that's interesting that like a higher level language actually improve your performance but yeah I guess so you're pushing a lot of the manual optimization to the compilers and like stuff that computers are better at and then the commonly used so-called pre-compiles um you do hand write that when we started when when I started to write circum the main purpose is as I said before is circum is at the end it's a low level language so it is that if you're writing silicon you should understand what you are doing you should understand what's in the what's behind the the circuit level of course you can do things in um in a high level because you can connect things but the idea is that you have precise and this is some something that's very specific to circum is you it should not be any hidden constraint anywhere so all the constraints are there you can check the code maybe you need to go inside the circum leap or someplace in the library but you will find all the constraints there and you can track them and you can see them so you can have full control of what you are doing this of course is dangerous I'm not saying that that's why because you know you need to understand if you're I've seen people that writing circum language that they don't know what they are writing so it's not going to work okay and this is we we understand this is a a thing but this is what circum is uh for circum is for writing I would say Circle means for people that understand what's happening underneath that's the that's the thing okay um set that and this is a little bit linking with what he said is I think that in the future nobody will have to write circuits and because uh Siri is a very specific thing for things and and then you can write you you will be able to write things in in Cairo for example or things in maybe some high level or even in in solidity the ckvm at the end is a language that you're writing a language and you you're building a zero knowledge you're building a zero knowledge there so you can uh there is other projects like Risk 5 that are also building on top of that so it's uh uh the the zkbm what brings behind to the to the to the world is that you can have any any processor any base risk five uh an ebm or Cairo like or some some specific uh virtual machine so some processor and then you build on top of that processor but the circuit is the processor the circuit is like the hardware okay so it's like maybe we can talk about Hardware language and software language uh circle is a hardware language okay and so for language should be normal so for language C uh solidity uh python uh or any other language that builds on top of that I don't see a space for um I don't see but this is a it's opinionated on me but I don't see a space for a specific language is a specific high level language because I think that very high level language should be enough to to run on top of that can I just add one thing but what you said um I tend to agree with the entire statement about however language there is one thing though um I don't I I'm not sure and I don't think necessary that you know things like python or us will run enable you to write vkps tomorrow because there is still a very strong Paradigm competing Paradigm Beyond it various between differences which might require you still to have like a different accepting that you're working on something different yeah probably this is but this is this is at this point we are running faster so it's uh I think it's just a matter of uh times and yeah and how difficult it is you know right now uh depending is if you are if you want to run a very very complex uh uh language the very complex thing it's one thing but if it's complex it's going to be also difficult to write a complexity so I I think the way to go is high level language cool yeah so um I uh I and my team have a very very strong opinion about this so I guess for for background um so Mina the the Mina protocol um we started building it in uh 2017. um and uh for those that don't know Mina basically is just like an enormous uh like zkp circuit that that powers everything so there's um a recursive uh a like linearly recursive proof for uh compressing the blockchain that follows you know the rules of blockchains um uh there's like consensus logic there's verifiable random functions where like uh fractional numbers are approximated using like crazy uh calculus things basically enormous enormous programs and and enormous circuits and and we realized very early on in 2017 that like we could not write the circuits manually um and and I agree it's I I would I would sort of I mean in some ways it's like it's like programming directly with Hardware but I think you can also kind of think of it metaphorically like writing Assembly Language um and and you know you can't like uh these days when you're trying to write a really complex program you don't use Assembly Language you use a higher level language so um so we so we built the first version of snarky which is an embedded ocamel DSL in 2017 and we used it for and we still use it you know whatever five years six years later um and uh it's great and we've learned a lot and and the the specific um you know we've taken those ideas and brought them to snarky JS which is just sort of like bringing that into into typescript instead of ocamel um the uh you know we we thought about like okay if we could build a high level language that compiles into something and then like Implement a bunch of optimizations but the the landscape of proof systems is we thought would evolve quickly it has evolved quickly it continues to evolve quickly um and so uh it makes sense right now um I guess it's similar to what we were just talking about like uh you you you can't today efficiently use like a regular language directly like uh like you can't write a rust program um and have it run efficiently uh in you know for some definition of efficiency um uh but um but you can uh you can build like a little library or framework in in a language that already exists embedded in it so that's what we did with o camel it's what we did with typescript um and uh and and and you can you can build um you can expose the lowest level so so like the there are people who are like experts at optimizing constraints for the proof system and then you can build abstractions over that uh and with types builds um you know type structures build computations um and and this has worked for building this enormous uh system that we have in Mina and we see it starting to work as well for building really complicated applications with crazy recursion and all this stuff in in snacky.js yeah [Music] um I think I disagree with everybody that's okay that's why you're here um from like uh I I think that using rust at least for our use case um it would be a bit clunky to encode like um semantics for like privacy like private State and public state so I mean it it's possible but um I think for the user just be a bit clunkier which is why uh I think maybe if you're not using it for the Privacy use case then high level languages like rust and go might work um for a for for using typescript and uh languages like that right now um when we were designing Noir it just seemed like we needed to restrict the user from doing uh very specific things that it seems like in typescript they could just counteract it and I guess one example that I always bring up is the if statements like uh if you're using like a high level language you sort of can't just do if something else something because in typescript or high level languages that works different from like a circuit based if frequently so so in Cairo we actually do uh a cattle at the runtime so we don't evaluate if statements we only evaluate the path we take that's why uh yeah uh of course I have a counter opinion to that um yeah I think uh um the the costs uh of of using a custom language in in my opinion in my team's opinion uh severely outweigh the the benefits so I guess just to talk about the if example yes you you can't use the built-in typescript if in the context of uh like within a circuit but if you are if you are Computing a value that you're then using in a circuit you can use if statements and and if you're if you're actually in the circuit there are tools in in the typescript ecosystem that can help us actually Warn and or error to the user like we can use eslint to provide an error if you're using an if statement in in a place that uh you shouldn't be and and that and of course that won't be perfect but um we've found that that actually uh is is working really well so I actually I'm agreeing with both of you so I'm gonna explain what I mean by that um I agree that you should have um basically your specific language at the same time that um that I mean that tends to help you think about the Computing model you are in and at the same time you also want a language that are familiar to the public or people are already using and so one solution that we I mean first of all and some say I didn't say about Cairo so Cairo actually been CPU air so roughly Cairo can evaluate any program and it was clear to everyone here but it's and notably what it means that there is two things that you get that you don't get in circuit usually which you you only get you get recursion um very easy recursion you don't have to evaluate your circuit the second thing that you get for free is you only evaluate the if statement the patch you're taking so when you have many many ifs then you actually only you don't pay for all the branches you only pay for the the one you're using and so um one consequence of this one thing that we were able to do or I think all the team built is they starting to see trans compiler from one for some high level other language to carry itself so we have one right now a language compiler called skyro made by your team out of Switzerland which is combining Idris being a functional language to Cairo for the story if you are if you want to use it you'll be the first one to ever use it so have fun uh I actually never found anyone who wants to use a functional language so maybe there is one in this room uh but uh just they like functional languages so go for it uh Idris um so seriously if you want to do have fun go for it seriously it's crazy but uh but so I'm trying to say that um I don't know if it's the ultimate path but uh um they there is like there is a middle ground can I uh yeah so um yeah so just uh for for in in snarky.js today connected to kimchi I mean this is powered by kimchi of course but uh we we also can handle recursion infinite recursion and in snarky.js we've built an interface that's just it's like essentially the same is writing a recursive function so you just you you know you you put your function in a JavaScript object that's the only difference um and whatever arguments you have if you type them as like proof then uh the system will know that you are trying to do something recursively and it'll do all the complicated for you uh and in your in your code you can just say like you know if if like P is your proof you just say like P dot verify and that sets up all the constraints for your circuit uh and then and then the other bit about the the if statement branching thing like today in I think any proof system that I'm aware of including kimchi including the current version of kimchi um yeah you have to pay for both branches and you can kind of approximate branching by like multiplying by zero or one um but we are actually working on an extension to the proof system that will will allow you to do these kind of like arbitrary jumps in an efficient way and that though is uh you know I'm not working on that and it's always not the CPU like you know like basically running in the arbitrary program then um yeah I like that noted VM basically at that point um you you get essentially the the power of a VM with the performance of direct circuit yeah so it's kind of the Holy Grail and and the I mean the intuition behind it is like uh there's you know there's like lookup tables and and custom Gates and all these things in Planck um you can sort of extend that to uh allow um for like arbitrary Ram like random access memory and then you can kind of do another fancy thing to get arbitrary jumps efficiently um and the details of this I'd have to connect you with a cryptographer on my team but um I am told that this is the path that we're going towards I've seen that I've seen that in your repo is the dynamic lookup table right yeah dirty has been silent for a while you know this this is what are you writing Hardware or software if you're writing Hardware you need a hardware language if you're writing software my my my opinion is just use a standard software language and uh why because the hardware and the problem is that you need the piece of Hardware you need to process or the hardware that that can compile to this software but this is there so this is where to go cool um I want to save some time for the audience to ask questions so we have about 15 minutes so any questions from the audience so we actually touched on Hardware a lot during this conversation it'd be interesting to hear uh from the different panelists like what you actually see the kind of future of ZK Hardware to be and like what direction it's going not that kind of Hardware we were talking about I think we're talking about but yeah we can talk about that too but we should we we can't we can't talk about like real physical Hardware as well yeah we're talking about the polynomial Hardware it's like no arithmetic Hardware I recognize that I I can't no it's we're making a joke but let's uh on the artwork acceleration so I'm going to take a hot what's the odd stake here um Stark where at this stage do not believe that we need them um as simple as that um the reason is because the software itself is being improving so fast and the structure in the architecture of how you do things has been improving so far that at this stage we we don't know what the future will look like and so what it means by this is hardware development is you're roughly a year and a half before getting into production and a year and a half from a year and a half ago uh Cairo were barely working started with in premises uh recursion wasn't working um we and and so for instance the thing that we are working on right now so we already do recursive recursive proof on production today of approve ourself but like for instance one architecture we're not doing it at the moment is um basically a three of proofs like basically right now the only thing we do is we have what you call jobs and jobs are like those Stark Net stock where infrastructure used all of that use sharp and so all the customers and you start getting included of the infrastructure are sharing the same proof so every Star every one of that is using is basically everyone is paying a marginal cost of the proof cost on that and so um so for instance diversify or immutable like right now is having like 70ps in their environment and they're using a significant amount of Step within this proof so they do one job and we have other proof and also another type of of the job is approved so we um so we don't really we didn't went all the way yet into that uh tree architecture and so full Hardware um there is already a significant like you know the the thing Central fact that we can't really say this is the original that said I have a friend of mine I guess from uh the company called wait uh it's uh inguinal which is a very good friend of mine he's also already working on this making uh groundbreaking Improvement on outdoor explanation Shake pronunciation or even like you know uh MSM which I have no idea what it means but uh I would let him to you he's not here today but yeah I would let him uh answer that another day um I I yeah so different I guess slightly different opinion um so in in uh in snarky when you use snarky JS for writing applications on Mina or just in general we're encouraging um the the proof computation to happen actually like on the user's machine um in this way you get uh privacy and and uh you know there's there are ways that you can sort of do part of the computation on the machine to get the privacy and do the rest on a server all these things but um when you have a proof system that has a lot of cool features in it uh like kimchi um it's it's slower than a proof system that has a lot less features in it um and uh Hardware acceleration would then kind of you know make it faster I I think it would be uh cool I guess like you know we're encouraging people with snarky.js to write uh uh circuits with recursion with with linear recursion or tree recursion that's been working in Mina and Mina's mainnet for a year and a half sort of internally and and now on our decentralized testnet we have uh you know you can do it with custom circuits and and settle on chain and all that stuff and it works um uh and and yeah and it's a little bit slow right now we're working on optimizing it um but I I am imagining a world like I think a cool world would be one in which like The Ledger that we all have um or something like that uh that has our wallet on it also would have like a a snark accelerator uh companionship or something um I I don't think that's coming anytime soon but something like that could be cool and and even like obviously in the shorter term taking advantage of like gpus fpgas and silicon like at any level I think would be helpful uh because it would it makes these proof systems that are I think like have a lot more features um uh more you know faster so you can do different kinds of applications than you could if if they were slower I would distinguish this is actually a very um very different so talk about zero knowledge proof we can distinguish between a small small proofs and uh big proofs because the the proof are for the kvms and for uh roll ups and for when you want to do aggregation and these high computation stuff and then small proof more questions but they are more related to something like Identity or a game or something that you want to run especially in the client uh I think in the client has a lot of sense because it's it's where the hardware you know you are building you do a a big investment but then you you you you can spread to many mobile phones or to many uh places so this makes a lot of sense and for things like you know this privacy things for for things like uh gaming for things like identity uh just proving things about your data I know all this that so uh I think this makes a lot of sense in in in that direction for the hardware um so for the big things and I'm talking more specific to maybe a specific project which is the ckvm but so representativity is that for us we are I tell you why we are interested in Hardware uh maybe here you will you will you will see why the idea is that um uh in a so if you want to have a competition the days that you want to have a competition uh that approving competition you know just there is a market for for for uh generating proofs big those big proofs uh here you can have there is a problem is that you have a centralized maybe you have some provider that has a lot of power and these monopolites the the the system in the case of ckvm this is not a big problem because you can only build the proof that's already deterministic so you cannot do crazy things but you can degrade the network so the problem is that somebody that has a huge power that's able to generate a lot of proofs maybe just stops and stops doing that and and and then uh the the network just degrades so in for us the way for us to to to to to avoid that is to have the best the best proverb available even in Hardware when I'm saying Hardware at least in fpgas or whatever even Asics if it's required so but the idea is to have the best approver a valuable open source and a valuable for everybody so that there is no monopolistic uh proverb that can take that that's why for us it's so important that the proverb is open source that the property is available for everybody and not only that that the technology to build the Brewer is available for for the community because this is the guarantee that we have that the network would not be degraded it won't even have to on top of this um in the context of stock net the way we are looking we're going to look at this proving Market is by actually having uh like a kind of POS exam mechanism where we have a leader who would be the proving one and we have like a in Cascade another one and second one would come afterwards uh if there is degradation and so um the the other thing that I just want to say always I forgot to say let's talk about that but um people have been very focusing a lot on on um on acceleration because stock are very slow and there is no because the operation that you did curve and so on uh and some means I mean I mean but I mean at least uh Jordy and I uh working more on stocks which are much faster and much easier to to use to to work on on regular servers so I think that maybe a difference of nor take related to that machine I mean uh so I'm gonna answer the question I wanted to hear instead of the one you actually asked her because I don't really know much about uh Hardware acceleration uh in terms of uh programming for Hardware versus programming for a software I think proving systems are changing too much for at least us to focus on uh sort of being so close to the metal just because the muscle might change next year or the year or the year after uh I will say that for privacy it's sort of a different space uh if you have privacy you need to sort of think about how to accelerate approver like because we usually use wasm whereas if you don't have privacy it's really fpgas and gpus like how do you accelerate uh this this massive beefy machine I I guess just to um there was this discussion about like uh you know markets for decentralized provers um and this is just my I just have to say this because too many people are surprised by this at this conference sorry if it's a little bit off topic but Mina has a decentralized sequencers decentralized provers a Marketplace for producing proofs um that are sort of needed by the system and this has been in production for a while and uh we um encourage more people to look at it when they're thinking about their marketplaces and everything cool great answers thanks guys I have a question it's like maybe continuing on the hardware thing but most of the I mean a lot of people kind of compiling Target now is actually solidity so actually a lot of people are moving on ethereum and kind of this is Defcon so like uh the most artificial kind of constraints that I'm seeing is probably the fact that we have one pre-compiled curve to work with but we can only work with BN 128 so one thing what's your intuition why why don't we have more curves to walk with in your opinion I don't know it seems like it will kind of release a lot of the bottleneck so okay I I can make two comments uh the first one in the pure application Level and why people should care about what you're describing um so I think that one of the biggest original sin that if you're made when they were built in order to see in retrospect right we couldn't know is to have had EOS I think he always was a mistake but and and um and you're right having new curve like you know uh being able to use the p2056 the one that is on your phone or even being able to do RSA verification I mean just to throw random ideas right uh would be unlocking a lot of use cases I mean just to give you an example um starting it right through the company on gaming that is using the construction to verify p2056 the the you're using your securing clip of your phone to sign signatures for gaming that's awesome which is awesome um that's actually not for me the biggest you know lock of the verifying the evm the main difference the one problem is that um either you the Computing Paradigm of the vvm is very different from when you have nvk I mean bits Boolean logic is expensive and ZK is cheap on zvm right so this is kind of the starting point um and um while if like you know a lot of team are looking into I mean diving I mean or you could talk more about that than me um about you know proving vvm uh we didn't make the choice because we were in our opinion we were optimizing for performance of The Proving system and then now that we have that can we back work make it Backward Compatible and so we have in our case warp which is like a tooling that enables you to transpire from uh solidity to Cairo and the practicality of it I can't really judge no one really used it in production but they managed to recently announce they managed to compile to unifa B3 which is a significant bit and um there are an existing effort right now to rewrite the VM in Cairo so I don't know once again if it's a POC if it's for fun or I don't know maybe maybe it would be practical to trophy that I want to see the numbers I won't believe it going to your question um why uh why not others I think maybe you should ask to the EF people maybe better but what's clear is that at least the BLS 381 this is a must and this is will be introduced sooner or later uh in in ethereum among other things because signatures so because all the all the staking stuff it's it's done with this BLS stuff and also bien well at the beginning was that was not safe because there was something discovered but then is much more safe that we thought that was safe so it's it's there you know it's some there are some doubts of the BN Corp so I think that sooner or later ethereum will have there has been some efforts I know that there has been some uh stops uh at some point uh on these this this adding this and here is for example uh we are implementing the fully VM right now we don't have to compile anything like that we just get the code and put it there uh doing that what has been an effort but what are the difficult things what what what's the what are the piece that's really difficult to add so we were able to to build the evm but there is one one piece that's complex we'll do it but it's really difficult to do is um this pre-compiled contract of the bn1 so complicating the complicating the uh the layer ones uh VMS uh it's not a good thing and this is uh there are you know there are some of course there are some mistakes with the IBM was created one was created uh wasm was not even an option at that point just for you to just people to realize on that of CK nobody knows they didn't exist but so here and and what I've been hearing at what I see is that uh the ABM should tend to simplify not to get more not to get more complex and this is I think this is a feeling that uh in ethereum the people that's working system we have uh ebm is probably too complex and we need to tend to simplify the gas models the the sum of codes and and if you see the proposals that are coming are more in the not in extending the ABM but in simplifying DVM and said that I think that the BLS is yeah so um I guess similar similar to uh Stark where we we started not with evm compatibility in mind for for mina um and snarky.js and our proof system um but differently from starkware we didn't we didn't start with like performance in mind um because we're we're not trying to build like uh CK ZK roll-up scalability solution um we we were like okay there's all these cool things you can do with ZK uh with with privacy and recursion um and and we want to build a proof system uh that enables uh people to take advantage of all those cool things um and uh and and so um in the same vein as what you're talking about like there's all these cool things now that you can do in in our proof system and on Mina and you know we're working on an uh ethereum Bridge so that the mean estate can just be fully verified from within ethereum and and the idea here is like we want to bring all these great features uh to ethereum Developers um but uh you know like I guess typescript is you know people are writing front ends into typescript anyway so hopefully it's not too bad um but uh but yeah rather rather than like kind of trying to put all these things inside evm it's like okay let's use you know efficient uh elliptic curves and all these things uh to to do these interesting privacy and recursion all this stuff and then uh wrap it up in a nice present that can be verified quickly on evm I can say my question is because uh you know start the standards are evolving sometimes and this is like a like the the hottest the metal will be in terms of zero knowledge standards so some of the time the constraints on the standards come from things that are not necessarily very efficient and eventually Downstream you you don't understand how you got something that is using a certain aspect so a lot of it down Downstream decisions are being taken because of of that right we have baby jab jab because it fits the prime field and and I mean it could be more efficient so my skin like uh kind of from a philosophical kind of aspect of at some point you need to leave with with there what's the point and and and you you can I mean you know you can we just we we could wish and we did many times you know uh research specific can but the reality that you you won't get to adapt I mean we may be all convinced here that decay the future but maybe other people outside or not and so we it's a consensus everyone to agree on what's what's good so I tend to agree with Jordy we need actually a much more in favor of simplifying the network than doing it than trying to bring more more things to make our life simpler right now and uh just to give you yeah that's basically yeah I don't know for this is uh look in in the second circuit we are using a different Prime field we are using the goldilock spring field we are building a stark we are building a recursion of a stark we are doing a lot of crazy cryptography in there and just in the last moment we just built the glow 16 or a plonk that says but this is the last the last piece so it's like the the adapter if you want to the network if if so maybe then that so maybe the IBM you you just need one thing hash functions how many hash functions needs uh requires the ABM well we have ketchak we have a shadow 56 recently 30 mad at Blake uh is it necessary maybe with Shadow 56 would be more more than enough from the beginning you couldn't argue that the pre-compile could have been you know like we could have made things cheaper or in first place or maybe you know made many things that are wide is more so much more efficient of the spring you know there's many things you could discuss in the back but this is why why and why this is possible right now and maybe and this has changed from three four years ago it's because we have this validity proof we can compute things of chain and validate them on chain so we just need to care how we validate these things on chain and bn128 or if you want to BLS it's more than is more than enough so probably if the if the ethereum had to be done now uh it would be less cryptographic Primitives and uh more basic thing because we can we can we can accelerate that of chain and and validate that uh with uh with visibility proof on chain and just to conclude on this if you want to have much more impact and make one change changing I mean it's going very bossy changing by poster don't might be more helpful no because you change we pass it on in which which in which playing field yeah yeah but that's that's that's that's the thing you know it's and I've been thinking you know all of us we haven't have been thinking on that this why Poseidon is not in the OR quite uh so why don't we have a milk C at some point uh it's not in the in the ABM and and this is the thing but as we evolve as we evolve we see that this is less important because we have because this computational proof are getting much faster and much better so we require less this so we can sacrifice a little bit of optimization right now we can look in in the ABN circuit we can we can validate for 500 catch acts in the in the proof we can validate uh I don't know like something like 100 or 200 uh cdsas with the cdsa ethereum Corps in there we can do paintings inside there we can do everything in there silly it's not optimal but doable and it's it's getting every time more efficient so at the point that we have this we require less help from the L1 we require less Health from that so so that's why I think that this tendency to simplify is is is is is is a trend oh yeah I I also feel like proto-dunk sharding is moving in that direction like they introduced the pre-compile for Point evaluation for kcg and you know then you can understand kcg proofs but beyond that it's all blobs you know it's all um delegated to the rollups well we are we are over time so there's one last question okay ask it ask it thanks thanks yeah I just wanted to know that I see a lot of sense in like breaking this evm eyes and creating DSL not only for like opening a new area of innovation and like clear question to basically Amina I mean does it make sense for you also like for example to collaborate with Cairo and uh maybe like you know since like it's better model for the ZK then uh like uh imperative languages and uh yeah basically work on better tooling for uh yeah I mean operational verification I I think we are we already are right I mean we we have uh someone on on at o1 Labs which is like the the team that incubated Mina um is is looking into getting Cairo to work with with kimchi yeah and there is even one in the room who actually took her work and made it into work with Winterfell so you can look at the guy in a very nice blue shirt over there Max shootout so yeah Nike says a practical Cairo very very fire and prover uh in Winterfell today um I mean to be honest it's actually go back to the point question about simplifying stuff um why we have now l2s we have L2 that can do stuff um let's use their their advantage to bridge where we could do it before and so like you know an example uh there is a team actually maybe in the room actually over there Snapchat is working right now to do N1 voting using L2 to make them cheap and that's something you couldn't do before so that's this kind of thing that we can do with our tools now that we couldn't do and then one we don't have to change that one for that so simplify please cool thank you all panelists [Applause] [Music] okay jordy's gonna just stay on the stage I think Oh no you're kidding so um we didn't put in a break in this programming I realized maybe that was a mistake um but we have our final two talks for the day could I get my screen on the thank you yeah it's like foreign [Music] oh yeah I'll give it to him oh thank you so um our upcoming talk is by Jordy and we're moving into the last section for the future of proving system session so this last session is called uh recursion aggregation composition and it's looking at basically how people are taking proof systems apart and like putting them back together in weird ways and to get the performance gains so this is one way of doing it which is verifying Starks and snarks yeah there you go oh yeah could yeah cool perfect hello good morning I'm jordina I'm the technical lead at polygon Hermes and I'm gonna explain a specific piece of the zig ABM that we're building that's mainly how we just can can you close the door please so I'm going to explain a small piece of the of the zkbm that's how we are verifying uh a stark with a snark as I explained it before in the panel uh the last piece of the of the of the proverb is mainly converting we have a stark and then we want to prove that on chain and what we do is we create a circuit that verifies this start and uh then we we prove it in in in the snark okay um well this is uh well this is the the KVM that we're building mainly we are using pill this polynomial identity language to build out the full system and at the end what we have is a set of polynomials with a set of identities that needs to fulfill with this can generate automatically the idea of this is that once you have this pill you can generate automatically a stark and not only that that uh with this with this Stark uh you can also automatically generate a start that verifies that that that other Stark okay so we have our recursion Starks and you can we can do that many times and we can even aggregate uh we can even Aggregate and then we can build a system that looks something like that we can have many proofs of many blocks and then we can pack them together in a kind of a tree and then at the end in the last step before before sending to the ethereum we just uh convert this Stark to the snark and this is this is done quite automatically so you need to care just about building the pill and the rest is quite uh straightforward okay so let's see how how we do that okay so what we want to do is we want to build a circuit actually we want to build this uh circum circuit that as an input you have a start proof okay so this is the start proof so in the circuit you have to validate this star proof so we need to dip in what's inside the star proof what what's what Circle what means this of a star proof well the first thing that one things that you have the first thing that you have is uh you have the transcript transcript are mainly hashes okay so you need to ask these random numbers that goes back and forth this is something that we need to compute inside the inside the circuit these are mainly hashes okay as a transcript in this case we are using uh well a hash function in this case we are using uh for the Stark we are going to start with a hash function that's a fairly hash function in this case a Poseidon with a bien Corp we are just using this hash function to compute the to compute the the the randomness the of the of the transcript okay another piece is well if you have a polynomial evaluations and then you need to check uh you need to check that this polynomials evaluations are are okay that they are fitting that means that you need to do a lot of arithmetic and in the case of us we are using the Goldilocks brain field and but here we are in the BN so we need to work with this cross Prime field we need to do this cross brain field operations so we need to do cross Prime field inside the inside the the the inside the inside the circuit um the next thing that we have to do is we need to do openings okay so the start mainly is a miracle trees and we need to just validate these Miracle trees here the trick is again use the Poseidon hash function so if you use a Poseidon so a snack friendly hash function then doing that that inside this inside the circuit is relatively cheap so we do that uh for the for the openings itself okay once you have the openings you need to evaluate this opening polynomial which is a derivative polynomial you need to evaluate those openings at that point okay so you need here more arithmetic operations okay and finally we have the fry okay fry is openings again so lots of openings that's one thing so every time that you go to us back in the step you do on uh you do openings so this is again Poseidon uh here and then you have to do polynomial evaluations but this is our in the cross Prime this is in the Goldilocks uh Prime field okay the best way to do a polynomial evaluation is mainly doing an fft kind of a kind of an fft okay so and finally in the last start we also need to do uh an fft for the for the final polynomial that we need to check that the final polynomial of certain degree mainly you do an fft and check that the half of the or that part of the bits are zero okay so this is a structure of the of the of the star so here the I'm going to jump this so here the the well the the the trick here is how much so how we do this cross Prime field operation this is probably the most uh simple thing and the idea is where we are just doing in a naive so in a very nice way okay so we just uh if we're doing an operation like that like a multiplication uh in this case is uh well we just do a couple of uh rain checks okay that means that uh more or less uh Prime cross so multiplication with a gold Delux multiplication it costs 146 constraints okay the fry and this is a one one thing that's specific on the fry the Goldilocks is a very small Prime field so uh once you work with a random number you cannot work with the base field because it's very small so you need to work with extension Fields so you work with extension in this in our case we're working with an extension three of the Goldilocks okay so that means that doing a multiplication in the extension field three this is uh something that's 400 400 438 constraints which is a lot you know compared to one but it's not that much okay so here the thing is okay let's let's do some numbers of some examples of some Starks uh how it would look like okay well this is the the the for the hash functions we are using a 16 inputs so we can do uh trees that goes at four at four and then a constraint is just 612 constraints just doing a single hash which is quite optimal uh if we do it in that way okay so let's put some example this is these numbers are not exactly but are similar to the ones that we are using for the ckvm circuit in this case we are talking about let's talk about uh 1000 polynomials okay uh we have a let's do a lower factor of two that means that we need to do a lot of queries in the in the in the snark in this case is 128 queries uh this is we have that it's important this raw factor of two because if you go for example a lower factor of four that means that we are doubling the proving the proving time okay so we need to go to the minimal especially when we have so many polynomials here so that's why we use a row of factor of two okay so let's talk with uh sides of a polynomials of 2 to 24 after the lower factors so 2 to 23 to 224 okay and of course we have constraints let's assume that the number of constraints is more or less the same number of polynomials this is just uh just uh subjection and then we have in each constraint we have like two multiplications for each constraint okay these numbers of course depend on the circuit and depend what you're doing and so on but this is just would say it would be a good example of a zkbm circuit-like okay it's a big circuit okay so for this is okay here let's see how much boot it costs this uh in in number of constraints here of course the transcript is not that much the checking the polynomial constraints uh at the random Point well it's start to being an important number but not that much uh the number of queries we have so many polynomials so we need there are so many polynomials so mainly what we do is a linear hash of the evaluations and then do the miracle lights but this is this is just for the the queries the first query is not in the fry yet but for the first queries of all the polynomials this is our DOT and here is the big probably the biggest part okay the biggest part is when we need to compute this opening point polynomial there are so many polynomials we need to do so many multiplications out of the field that this is a number that grows a lot okay and here is the the big part of the proof is this 112 Millions what else we have we have the well this is the queries of the fry here are the ffts the the the polynomial operations the entities into going deep in the fry and the final and the final and the final polynomial evaluation so this gives us something that's huge is 127 million constraints but it's it's doable this is a circuit that's for example the Hermes one circuit had this number of constraints more or less so and you can do that in about 100 seconds in a big CPU okay so in Rapid snark so that's something that's doable but you know it's going to be a big circuit okay the thing is that with uh uh what we can do the technique that we are using is that instead of having a snark so we have a we have this big circuit so instead of this proving directly with a snark what you can do is you can do different level of different levels of recursion so you can build a Stark that verifies another Stark so you get a proof that's much smaller maybe you you can do repeat that again you do another Stark that verifies the Stark so you get even a smaller proof and at the end you can have a final proof that can be much much much smaller okay and this is this is actually more or less what we are doing here is here I put you the real numbers of the current structure this is a structure of the proverb that we are running right now in the public testnet okay so this is a three steps okay the first step is uh with this number of polynomials but you need to count that this is about 1000 polynomials but then we have 43 block apps one permutation check but we have different different properties so this is a big circuit we are working with 2223 below a factor of two here you see the structure the way that the structure we are of the fry so we are building the fry okay so this goes to another point so it goes to another proof second proof is a proof that's much smaller it's just 20 polynomials with uh uh sorry 12 polynomials 20 of them are constant uh 10 intermediary polynomials no block ups just one connection check and uh yeah and 23 23 then it is and then we will go to the third one this is also you see here it is the GL GL means Road Delux this is a a a star does ability with Goldilocks but the last one is built with BN 128 okay and the last one is just a smaller it's just 20. it has only 12 polynomials but the and the size of the polynomial is 2 to the this is just 24 uh so 2 to 24 is what it's 222 and the block factor of four in this case we could do it smaller okay but at the end here is a compromise sometimes you you get it smaller but then the proving time of this smaller is bigger so at the end is better maybe to have a bigger last circuit that it's a bigger last circuit than uh that just squeezing to the end through the smaller and keeping uh uh and keeping beer so this is our balances and checking what's the best structure uh uh on that okay so if we go to more uh something that we can go far so that we can squeeze more uh the last circuit you can do well five can this well let me just with something that's 60 polynomials with a blower factor of uh 16 uh so two to the four and uh um this you with this you just require 16 queries which is to having the same 120 128 bits uh um security uh you can go a polynomial after overall factor of 2 to 24 so it would be a polynomial of two to a 20 that goes to 2 to 24 and yeah this is our number of constraints and if we do the numbers with this see that the numbers are getting much much smaller at the end we can if we add them all together we can have something that's below 2 million constraints and this is something that can be built in less than two seconds it's really uh it's quite insignificant uh compared to the to the proving to the proving to the proving time so um this is very much the the conclusion I just put this a slide from the kbm but this is uh the full just to to to to explain you what are the current times of the of the full ckvm improver okay so currently in an uh in uh big machine you know this is a 192 course machine it's ten dollars per hour machine more or less in AWS okay uh the this proof it takes nine minutes okay uh we can process four million gas in this proof so this would be equivalent more or less a cost per transaction will be less than a cent so zero seven cents per dollar but you see that this is actually uh it's a proof that's actually running the public that's not is running so and you can see the code and see how the approver works and when you can build your proof uh in there okay this is just a CPU approver and here there is lots of margins to to improve here from GPU we expect maybe one order of magnitude faster and fpgas the new CPUs are doing are performing much better yeah there are a lot of things that that are happening but this is just a a have you want some numbers some idea of where the proving times uh are on the recursion another thing that's not here but somebody already put it in floor and is that verifying a growth 16 in evm this is 200k around 200k gas which is probably the cheapest way the cheapest thing that you can that you can do plonk is 300K which is not that much you save the tourist wrap so probably it's going to be the options that we will put it in mainnet and yeah that's mainly my presentation I don't know if there are questions thank you foreign from scroll Network and um yeah is gonna tell us about basically how to optimize proof systems uh for ZK roll-ups so both are talks in the recursion aggregation composition section have been from zika evm projects and I don't think it's a coincidence that the biggest circuits basically are the most in need of the performance gains that we can get from composing proof systems cool so this is yeah from scroll uh hello everyone my name is yeah today I'd like to talk about some some of my thoughts about the through system choice for for the Hero apps and uh so today there are three parts in the first part I will talk about some concepts for modulizer Pro system and in the second part I will talk about like how we are actually improve system for our DQ Ops and finally I will conclude with some like you know your your practical considerations when you are choosing this proof system so now let's start with the modulated processor so why why you want to join a proof for some computation the first thing you need to do is that you need to do some optimization so basically like normally people call that circuit but but uh more formal word is called a constraint system and there are many different ways for the circular organization uh to give you some example for example in r1cs so assume that you have a huge circuit and you you put all the wires at the variables and you lay out that as a vector like from uh like it should be W1 to WN and then you lay out that all the witness as a vector so the form of rncs is that you can Define the linear combination of those the vectors of your witness times the linear combination equal to a linear combination so that's a form for for one constraints in Arabian CS so for example you can access any any of those written cells um like using this line combination but the limitation is that you can only have one multiplication so it's very good for for some programs for example if you have multiple like one large linear combinations but with a few like multiplication and so this is one example like if you want to access like so like uh Omega 1 Omega 2 and Omega and minus a minus two and another example is that you can basically access any weakness cells using this r1cs so this is like yeah the form of rncs another commonly used optimization is called punctual organization basically what you can do is that you are not layouting all your witness as a vector but instead you you lay out your witness in a table like for example you have like three three rows and then like what you can Define that you can define something which is more flexible like for example you can define a a degree degree uh degree to degree three constraints like for example you you access some sales and then you define those as a very specialized case or specialized custom constraints and another thing you can Define that you can Define some membership membership relationship for example this Tuple belongs to like two columns in the table so using this you can you can do like range proof really efficiently for example you can have a fake column and you believe that you you prove that this this element belongs to that table so that's another thing you can Define and the third thing you can Define is some permutation like you define uh like some cells are equal and so why this is used for because that in the first custom gate when you are defining after defining multiple case you need to connect them together so you need this permutation to like for example Define uh the first case output equal to the second base gets input so those are three constraints you can Define in a plug-in optimization so it's custom gate lookup and permutation so from the modulus perspective uh there are three like commonly used front-end one is rncs as I mentioned like with which relies on linear combination and second is trunkage which relies on custom Gates lookup and permutation and third is error which is usually in Stark it basically contains like translation constraint which is very similar to custom case but but only a success two rows and also some boundary constraints for like defining your your starting point so that's that's a front-endable for for a proof system and after you get this constraint system when you need to get a really you know implementable proving system like in practice you also need to pass it through a informational theoretical compiler so what does this mean so this compiler will Define some like interaction model between the program so in this information compiler you only Define how the program and Wi-Fi are interacting and assuming that there is some Oracle where you can for example provide some message from Oracle and then verifier can like later query from this article so they are nothing like you know concrete for example what kind of commitment you are using so you just send some message to the article Oracle and the Wi-Fi can query so that's what happened in this information serial compiler and so this is an example for interactive Oracle proof and the short form is IOP and another example which is commonly used is called polynomial IOP which the message is in the form of a polynomial like for example you use the p1x to represent your message and then like what a verifier can do is that during the query phase stage it can query at a random point to get this evaluation and you also do some interaction between the approval and the Wi-Fi you can do multiple rounds like which the like for the model there is constant round polynomial IOP which for example during that like morning is using that a lot of like uh stink a lot of proof system with things of verification you are using this constant constant run polynomial IOP and then after you get this IOP to make that really practical you also need some cryptographic compiler to compile down to an argument which you know can be implemented in practice so I will give you an example so that's it so in this polynomial IOP like model uh one one pre-cryptal graphic compiler you can you can you can do that because there is a internal active and you need verify to send a challenge every time to the to the approver so what you can do is that you can initiate this challenge model using some free shamia and you just harshly transcript at the the next round challenge so that's what one thing you can do and another thing is that when you are because there is still a like magical Oracle there where you are sending this polynomial to and verify can query from right so you need to initiate what's happening here so you initiate the concrete polynomial commit scheme to replace this Oracle so for example there is qg and there is Firebase there's Dory there's dark there's many polynomial scheme so you just use those schemes to commit to a polynomial and later at a open at a random point so that's you know after those those stages you finally get your your protocol so just as a summary for commonly used zero proof system you have front-end and you have backend so on the front end uh you have rncs plankish and error and on the back end you usually use polynomial IOP plus a polynomial communist scheme and uh a quick summary for the advantage between those different front end so rncs are really good for linear combination like what I'm saying is I'm specifically talking about growth 16. because in for example your modeling or Spartan or some other backend which also supports r1cs is totally different because the complexity actually relies on the surrosity for your for your Matrix so which is a different like you know uh like from for for evaluating the the efficiency and it's also more General because each constraint can access any within a cell like you do linear combination for that you don't need permutation because you know all the weakness are already in laid out to a vector um and uh the plantation air stuff is more useful for some uniform circuits so uniform circuits that basically you have repeated structure and you can Define one custom case for those repeated structure you just need to increase the length of your execution trades and to to Really efficiently prove that and it's also more customized for example you have you can have lookup you can have variable like uh components so that's a front-end difference and on the back end what really influenced your your concrete property is that the polynomial communion scheme you are using for example it will influence your trusted setup whether you will have that and some security assumption and I will give some example later and also influence your concrete approval efficiency because like for example if you are operating over a group operation it's less efficient than some scale operation and also proof size and verifier efficiency and some commonly used polynomial game is kdg flight based and some inner product argument which is derived from bulletproof and another called multilinear PCS so for kdg if you in initiate your proof system with kdg then you you have Universal setup you have D log security assumption and the approval is is relatively slow but but but but it's easy to parallel because it's mostly doing some multi-scale multiplication and the verifier only needs to do pairing and the proof size is really small and for Firebase you don't need to charge your setup and it's based on harsh do you use the Mercury and the restore line code to commit to a polynomial but but the tricky thing here is that when you are really using frying practice you also have some other like parameter Choice which will influence your practical security assumption um and the plural is mostly doing hashes and I have teeth and Wi-Fi is doing hashes and it has really large fruit but there are some improvements from for example 22 which can make the proof really easy to uh to to do recursion and for for the inner product argument it doesn't have set up because it's derived from bulletproof and it has dialogue assumption and verify are both doing remote scale multiplications and you can use some techniques like pasta curve where you have two cycles to make that uh like easy to reverse and you have like a middle child proof and those are three polynomial commitments which are we are commonly used for committing to a univariate polynomial like it's a it only has one one variable but the degree might be high and another interesting Direction which at least blockchain people or in like industry people aren't really looking into it called multilinear polynomial communicate so it's usually commonly used in some check based constructions so some check basically so this model linear polynomial commission scheme is basically you have a polynomial but you have invariables and you need to commit to this unvariable polynomial and then open at an unrounded point and you can do some interaction to really reduce that to just one one opening so this is very useful for for many like schemes which I would introduce later um and so just to decompose some commonly used particles for example to use Plantation Planck IOP and use IPS phonomic means game or the community version is using qg and so basically when you are describing a a concrete protocol you can like divide that into to three three parts what kind of personalization you are using what kind of polynomial skin you are using what kind of like IOP or like the the information theoretical model you are underlying and started doing R and Stark IOP and some some fry use as this polynomial convenience scheme and unfortunately grow 16 is is not for into this case not for into this category because it's basically based on some linear PCP which you you do some like you you include some trapdoor you'll try to set up and do the query there so it's a very special case it's done flowing to this PCS modular uh like diagram and there are some new protocols as I mentioned like which is based on multilinear polynomial scheme which is showing one more like potential one is pattern which use r1cs as a the front end it has its own IOP to handle this r1cs and the polynomial is IPA so the good thing for Spartan is that the approver is only doing one large multi-exponentiation so you can use GPU to really make that faster because it doesn't mean fft it doesn't need some other operation so it's highly parallel and then another new scheme called breakdown it's also RSS based and it's derived from linear time in Cold War code which makes your prayer time complexity becomes linear so it's linear to the scalar operation it's not two group operation and also another advantage of breakdown is that you can use arbitrary field for your for your weakness so one thing like even even if you are using Firebase one you you are only based on Mercury and harsh because you use resolvement code because so so you need to ffts a lot so you have to choose your field which is like has a large tour density to do in the life of teeth but breakdown can use using breakdown you don't even like need to like limit your field to be like fft friendly and there is hyperplunk which is from espresso and they do this plunkage organization and high performance IOP and you can use kdg or a freight derived like multilinear polynomial scheme which has some like potential and uh there is also Nova and uh so for Nova like you you can't actually really fit into this IOP diagram it's it's RNC space it's really good for doing recursion and when you're doing repeated computation you can use normal to do recursive really easily has some nice property there but unfortunately I do say like those multilinear uh polynomial scheme are most only support rncs front end so which that's why I like you know hyperplunk brings you this interesting property where you can define something more customized now after talking about the proof system let's look at take a look at what we are using in the key drop so the idea behind the crop is that you send all the transaction to one layer to prover and this layer to prove our generative proof and some music proof function with some necessary data for verifying this proof and so the proof system really matters in executive right because the approval time your approval calls your precise all influence you are the money you are spending on each transaction and also verification cost which which influence your guests your spending so now let's take it let's think about what proof system we should use for for such a general purpose liquid rub so as I mentioned like you first need to to know that what you are really proving so what's what's computation you are you are executing so for General general purpose uh like the Hero app you are actually you are actually executing the evm execution logic so you need to think of evm as some type of computation and you need to prove that so to constrain this evm virtual machine you have multiple like uh limitations for example your evm word size is 256 unless you use some Rim based structure it will always be the non-native field so you need some efficient ring proof and there are some they can unfriendly op codes which means you need some specialized circuits and then you because if you put everything in the same circuit it will be huge with a large requirement for the memory and for for the machine you are using so you need to decompose the circuit into different types and you need to eventually connect different components and thirdly that is a virtual machine circuit so you need read and write efficiency so you need some kind of efficient mapping from read and write and also like one last thing is that evm is very different from static circuit you are using for for some fixed program because the execution Trace is dynamic like because different transactions have different trades right it fills up this this table like even in the same position it might have different op codes you are approving so that's why like you might need some like efficient of selectors so the first three drive us to your circulation how to support lookup because you know that you have efficient really proof there you can connect circuit there and also you can do this efficient mapping and the last one drive you to some more uniform presentation for a circuit you are defining some IR in between two for the four instructions and also selected some at the point you want so that's the reason why like we have to use Plantation customizations or start based because they can also support something more customized and now let's take a look at the event we are actually using so in the KVM we have like two layers one layer is proving the evm logic directly for example it contains events okay to prove the stage transition it contains a ram circuit to prove the read and write consistency story circuit for State update and other circuits for example ecds is okay for Signature and some other circuits and they are directly used for proving the evm itself and then because you result in so many proofs so you have an aggregation layer which okay aggregate mobile proof and into one proof so one way of thinking of how how we are choosing the proof system uh it's actually two layer so think about the requirement for the first layer the first layer really need to be expressive because you need to express really large circuit and so that's why like you have to support custom gate and look up smart lookup lookup tables and also you have to use some Hardware friendly proverb to lower your prover cost because you know your input computation will be the largest one you you are directly handling evm evm not the the verification so that's why I like you know you need some Hardware friendly approver by saying Hardware friendly there are two things one is paralyzable and second is low Peak memory because you know if you have low memory then you can run a very cheap machines um and also the verification circuit is relatively small because you get so many proofs and you need to aggregate that in the second layer and the ideally there should be transparent audio Universal setup because uh you might add some new pre-compiles as a new circuit to this existing diagram which makes the the whole the whole thing like you know if you are even closing you have to do setup again again and so there are some promising candidates for example plunky 2 star key and they are using right to do this like really efficiently and there is a hero tool but but you know for the kdg version It's not that promising because you know although the the verification size is very small but it's on another field so one one potential is that you can still use the password for hello to doing all the recursions for your dvl the KVM and contain a lot of like Pro a lot of computation and then use something like a bridge to bridge this hello to approver or verifier to your final verification and there are some new iops with our ffts because fft you need a like large memory and it's less parallel for example there is hyperplunk there are some new constructions which remove fft in your PCS and so that's that's two promising candidates and also there are some some check based particle and like by Design don't have ft it only involves some group operations so for example Spartan Virgo and all those those constructions or Nova but unfortunately the only sport RNC so if one day they can support function stuff then like you know it will be more easy to use and in this in the second layer uh as I mentioned like because the requirement is actually uh efficiently verification on evm so even if some Wi-Fi is efficient if it's not an evm then it doesn't make too much sense on for for the key drop so and also the second secondly that you need to prove the verification circuit from the former layer efficiently for example like if your formal layer involves some non-native stuff then your second layer is better to explore some funky stuff because you need some customized stuff to handle your verification circuit and ideally it's Hardware friendly and ideally it's transparent or Universal I said ideally here just because it's not a very harsh requirement because your the largest computation has already been done by the circuit first first layer and then like for the aggregation circuit you need to do like smaller amount of computation so that's why like maybe not the uh the big stuff and so some problems in candidates is that when they grow 16 which I think Harmony are already using in practice and second choice is actually Punk with very few columns for example we can configure that to be just one or one otherwise column one lookup and one fixed volume it can be even sometimes even more efficient than grow 16 like you use kdg or you use that flunk but proposed by the Aztec team which you the trade-off is that you uniflunk you might have a like more efficient verifier than grow 16 but the cheat of that you you triple your Brewing time and and also there The Catch Fire which uh with with the large code rate and which you have a smaller verification circuit and you can yeah you can basically do verification either so this is like our our construction we choose the first layer to be hello to and second layer also hello two but the hash function uses is different because it has those Nest properties uh since we are running a little bit of time I will just skip this slide and so it's just basically good performance with GPU proverb and the application circuit is more since it's distinct and the second layer is that you know because you need to prove for the first layers like verification you need custom gate and uh so there are some future work we are like actually thinking of and I want to have some special is that we want to generalize this framework a bit so uh we believe that the front end the NL goal of the front end will be Halo 2 because it provides really flexibility for writing circuit like you have different rotations you have different layout configurations so we we want to generalize this framework to to support using the same Halo 2 front hand but for different polynomial iops for example like hyperplunk IOP and for you know you can add fry to the Halo tool and also there like we need to significantly improve the Halo 220 sport like because I heard a lot of complaints from developers who are really using hello to like you know might be there might be some DSL you need and the bug reporting is really poor and other like you know any feedback from from developers who are really using that and one last slide um I will talk about some other considerations even beside the efficiency for program Wi-Fi so there are some concrete considerations here first is your ecosystem so if you are using a zero and approved to develop your application once you need to to think about it that whether it's compatible with existing libraries so if you for example and whether so there are so many projects and gadgets implemented there because for example if you want to just build a simple application if there are so many Implement gadgets you can directly use that which can simplify your demand process a lot so that's what I mean by ecosystem and second is implementation so even if some new paper coming out with very very nice complexity you still need to think about what's a implementation how long does that take for example like industry implementation is usually like more solid than with better performance and better security consumption like considerations than the academic one and you also need to consider the best practice like for example if you are running a benchmark it shows really slow but if you know from the algorithm side that is very easy to parallel using GPU kernel then it's not a big deal and also there is license and audit and uh so if we can really standardize the the framework for for the proofing we're using and we will have a very large community and even have this specific for making this kind of proof system faster and making that really great so I think that's that's pretty much how I want to cover and thanks for for having me [Applause] thank you yeah um questions offline um so yeah that was the last talk from the future of proven systems session I hope you liked it and I think upcoming we haven't applied ZK showcase Althea hello um I'm going to give us a couple of minutes to set up here so um grab some water or coffee or whatever you need um and we're going to have Uma Roy up first talking about snorkeling clients I think she's here somewhere so this is [Music] [Music] [Applause] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] thank you [Music] so much [Applause] [Music] foreign [Music] foreign [Music] good videos [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Applause] [Music] [Applause] [Music] [Music] foreign [Music] [Applause] [Applause] foreign [Music] [Applause] [Music] foreign [Music] [Applause] [Music] hey everybody thanks for joining us sorry we're starting a couple minutes late um I'm not really going to say anything other than to introduce our speakers um I guess I will say my name is Althea I work with the privacy and scaling Explorations team at the ethereum foundation and we kind of organized this with Xerox Park um really excited to just get a whole rather quick download of all kinds of things happening with um applications using zero knowledge proofs so up first we have Uma and John talking about succinct verification of consensus with CK snarks test okay hello I'm Uma and this is John and we're going to be talking about succinct verification of consensus um and we're part of succinct labs okay so let's just start with the super high overview of the multi-chain and cost chain landscape so in the past few years a bunch of different l1s and l2s and app chains have come online and they all lie on very different points of the trade-off curve in terms of decentralization security transaction cost and throughput but basically as the number of applications on these different l1s and l2s has increased uh it's important that users are able to interact between these different applications and in this future in the multi-chain future Bridges have become critical infrastructure to make blockchains interoperable it's really important that users funds and assets are not siled in one ecosystem and they can interact with all the applications that they want to interact with in a seamless manner for the best user experience so bridges are really important um and so let's talk a little bit about what Bridges look like today so today most L1 bridges are built with multi-cigs or off-chain oracles and so the high level design is you have a multi-sig run by a centralized entity that watches for deposits on one chain and then we'll sign off on withdrawals on another chain and these multi-stig designs are generally pretty bad for a lot of reasons so they're you know sensorable they're not permissionless they're very centralized and they're actually you know empirically have been very insecure and there's been many such Bridge hacks uh and I would argue that these Bridge hacks are not only bad for the users who've lost billions of dollars of funds but they're actually broadly bad for the entire space it reduces The credibility of the entire space and then leads to severe Downstream consequences like regulation okay so that's kind of like an overview of the current problem and so let's start talking about Solutions so what does a maximally secure and Trust minimized bridge look like between l1s well we already have a mechanism for a decentralized group of people to come to agreement on the state of a chain that's called the consensus protocol and so the key idea here very simply is that bridge security should be based on the same mechanisms that validators already use to agree on state which is verification of consensus so if you're able to verify the consensus of a source chain in the execution layer of our Target chain then in a trust minimized way you can know the state of the source chain without having a centralized intermediary like a multi-sig that has to sign off on what the state of a source chain looks like so now that Uma has painted a picture of why these proof-based uh proof of consensus based Bridges make a lot of sense let's think from a high level overview what implementing the system end to end might look like so it means that we have some blockchain like ethereum and over the course of its lifetime it's naturally producing information about its consensus such as block headers validator signatures attestations and other important metadata and normally the peer-to-peer network of ethereum would broadcast and gossip this information to all the other honest validators and these validators will also verify the consensus algorithm but what if instead of just broadcasting this information to other honest validators we broadcast this information to a Smart contract on the execution layer of another chain which will re-implement the honest validator's logic the reason why this is so powerful is that if we can verify the consensus algorithm in a smart contract that means we can essentially run a like client on chain and this means that essentially we can trustlessly access any state from The Source Chain by simply providing a Merkel proof reproves the inclusion of some sort of data and inside these block headers we have access to basically commitments to the entire state of the source chain such as like you know how much eth I have in my wallet what transactions were sent in the past and what events were emitted in contracts and essentially we can access all this state with just a simple Merkel proof If we have this like client running on chain so the question is why hasn't anyone done this before and the big problem especially with these proof of stake based blockchains is that verifying the consensus algorithm is really expensive and in particular with ethereum the challenge is that the validator set is so large there's over 400 000 validators and to run this verification of consensus you have to keep track of all 400 000 validators they're public Keys how much they've staked whether some new validators have came in whether it's old Auditors have uh unbought in their stake and this is quite difficult to implement a contract in a gas efficient way the second problem is that the signature scheme used by ethereums to Beacon chain is the signatures immunized BLS and unfortunately the elliptic curve needed to compute the signature is not currently supported by the execution layer on many evm blockchains so even outside the context where we want to do this in a smart contract basically running a like client for for a proof of stake blockchain like ethereum is just ridiculously expensive even on like consumer Hardware like iPhones or laptops which is why the consensus folks essentially implemented a specific consensus algorithm for like clients and this protocol is known as the sync committee so essentially instead of verifying consensus against 400 000 validators the sync committee reduces this problem down to listening to the signatures of 512 validators which are randomly chosen every 27 hours and it works exactly as how you'd imagine essentially these validators will sign every block and if enough valid or sign up from the Block you know the block is Justified and after some finalization rules the block is also considered finalized and obviously at the cost of being much cheaper to verify the sync committee uh sync committee provides much weaker security guarantees and it requires a two-thirds honesty assumption however we originally explored the possibility of verifying this like client protocol on chain and what we found is that even in this scenario where there's only 500 tote validators actually verifying this consensus on chain is still too expensive so as to concrete reasons why it's too expensive it has to do with the fact that you still have to store 512 of these public keys on chain and you have to rotate them every 27 hours and storing a state on chain is still very expensive furthermore like I said before the current evm doesn't support a pre-compiles for the specific elliptic curve needed to verify these signature schemes which means that we have to implement you know this this elliptic curve natively in solidity and in terms of gas costs this is also probably expensive so our key Insight basically is you know you know to implement the honest validators Logic on chain is obviously a super competition expensive task but what we have available to us is the power of zero knowledge proofs which have this magical property of succinctness which basically means that for any arbitrary long computation we can generate a succinct proof which can be verified on chain so essentially you know the code and solidity is going to look something like this where we have some function that validates a new block and you know obviously you have some pre-processing step you have some step to verify that the current validator set is valid and that we rotate the stakes and whatnot and then finally of course we have to verify this these BLS signatures and as I've mentioned in these previous slides I've talked a lot about how verifying the vouchers that and verifying these BL signatures is quite expensive but what we can do is we can just compute a ZK proof that does these two expensive operations off chain and instead on chain we just implement the same function but we swap out these two expensive Parts with just the verification of this narc which will verifiably prove that we computed those things correctly and I think this is really exciting because this is just this is a framework that generalizes outside of like this ethereum sync committee and can be generalized to other blockchains to basically make it very easy to verify the consensus algorithm in the state of other blockchains in any execution layer that supports the op code for verifying ZK proofs and I think in the same way that ZK is being used to scale like the throughput of blockchains um this is really exciting because it's showing that we can now scale the verification of these consensus algorithms and yeah for these reasons we're trying to coin this term called proof of consensus we're essentially trying to build these Bridges which use ZK snarks to generate a validity proof of the state of some blockchain and yeah we believe that these things uh these things called succinct live clients will be sort of the end game for cross-chain interoperability between uh many different ecosystems okay so John gave a really great overview of you know how we're going to use snarks to make these succinct like clients so now I'll talk more about the details of how we did this for the ethereum sync committee so as John mentioned the sync committee does two things one is for every single block header the 512 validators will sign that header and they'll produce an aggregate BLS signature of the particular block header and then the other thing they'll do is they have to sign off on the new sync committee that gets rotated every 27 hours and it's really important the validators in the sync committee get rotated every so often for like security reasons and like making making sure that set is decentralized ETC so we actually have two different snarks so one snark verifies an aggregate BLS signature of a particular block header and make sure that like the signature is coming from the set of validators in the sync committee so every block header that we want to have accessible in the execution layer of the other chain uh we need to generate a proof that this BLS signature was like actually verified for that header and we have to send that to the like client contract on the different chain and then once every 27 hours we have to generate another proof that will update the sync committee validators and set the new validator set that we're going to verify against so we have two different snarks so without going into too much detail but to kind of cover some of the Primitives that we had to build uh to produce those snarks that I mentioned on the previous slide we used a pro called cross 16 and we used the programming language circom invented by Jordy and his team at item three and so our suite of circuits was pretty complicated and resulted in over 70 million constraints which is one of the largest circuits that we've heard of being used at least in surcom and so some of the Primitives we had to build were a public key Edition and aggregate verification so you basically have to add up all the validator public keys to produce an aggregate public key we also had to implement verification of the block headers so this involved implementing pairing in a snark and then having the pairing check the BLS signature which we other collaborators at xerx Park Yi and Jonathan and Vincent uh worked on and then we also had to implement uh serialization methods to basically check that these public keys are actually being are actually like the correct public keys so we had to implement the SSD serialization that two uses which is a Shaw hash function basically and then we also had to implement Poseidon commitments to a vector of public Keys which is a snark-friendly hash function and there's one trick that we use that actually helped us significantly save on gas costs of storing public keys so I'll go into that a little bit because I think it's pretty interesting so at a high level snarks have public and private inputs so the public inputs are in red because they're bad because if we have to if we have to verify a proof on chain then the public inputs means we also have to put that data on chain which is expensive private inputs are transparent like we don't need to put them on chain when we're verifying this proof so if we were to implement the verification snark we would have all the public Keys as a public input because we need to make sure the public Keys correspond to the correct validator Set uh unfortunately storing public keys on chain is really expensive so our idea was we could store to the public Keys instead so we can store uh the public Keys as a private input in the snark as you can see here it turns into green and then the public input becomes this commitment which is basically like a hash of the public keys and it's like much shorter but then the question is how do we update the commitment rotates so one idea is that when the current sync committee is signing the rotation of the Newson committee they basically sign an SSD which is this eth2 serialization they use of the new public keys and so we could just use this SSD serialization of the new public Keys as the commitment but the problem is SSD is really snark unfriendly because it's a bunch of Shaw hashes so it's really expensive to compute in every header verification snark and so what we our idea was what we could do is we could map this snark unfriendly SSD to a snark-friendly commitment and so our second snark that I mentioned below for the sync committee rotation basically Maps it takes in the public keys and then it produces the SSD commitment and then also produces a snark friendly commitment called a Poseidon commitment and then it just asserts that those two things are the same and the bottom line is that we were able to save around 70 million constraints in our header verification snark which we run every update that we want which is like a huge savings in terms of proving time and just for some benchmarks so our sync committee rotation snark has the SSD computation so that's why it has 68 million constraints which is quite a lot uh thankfully the proving time uh is not that bad it's four minutes and that's the snark we run every 27 hours and then we have another that's verifying the signed header that has around 20 million constraints and it's proving times also around four minutes yeah so everything that me anduma have we've described here we have like working prototypes of it and we actually built a two-way like client bridge between gorily and gnosis chain which is actually um another L1 which implements ethereum because that's exactly so we're able to reuse many of our circuits so we have a demo demo succinct XYZ and fair warning it's in beta and it works much better on your laptop but here we have some screenshots of like what it can do so basically you know you choose your networks right now we only have like one pair obviously and you choose your currency and yeah you can just Bridge the tokens you send a deposit transaction and essentially what's going on behind the scenes here is quite interesting so when you make your deposit transaction it basically stores some data on some contract on Gorly that indicates that you've made a deposit and what you have to wait for is essentially for the like client on the target chain gnosis in this case to be updated with the block header that can now reflect your new deposit so you're going to wait for the finalization of an ethereum block which is around 12 minutes and on top of that you're going to have to wait for our proving time which is right now is three minutes and after that period we can send a transaction to the like client to update it to the latest block header and then to initiate your withdrawal transaction essentially you have to provide a Merkle proof proving that you made a deposit on Gourley and this basically this Merkel proof will essentially unlock funds on the target chain and you can see yeah over after some time like the top emoji becomes like green check mark and uh yeah yeah so yeah to zoom out a bit I want to take some time to compare what the trade-offs are between you know this proof-based bridge versus the bridges that exist today because I think they very much um exist at different trade-off points and I think they have different use cases so with these proof-based Bridges you know obviously the really big benefits are that there's much higher security guarantees you know assuming that the ZK uh Stark security is fine essentially you're able to borrow security directly from the source chain which I think is something really powerful and that many of the existing methods don't have at all another property is that because we don't have any additional trust assumptions besides trusting the l1's validators um this this protocol can be very permissionless and censorship resistant because theoretically anyone can be running the operator that generates the proofs and anyone can be updating the like clients and yeah for these reasons it's much more permission than decentralized now the cons are I think you might imagine is that you know verifying this ZK snark proof is much more expensive than verifying you know a threshold signature scheme that a multi-sigma implement but thankfully with like graph 16 and stuff we expect that the verification cost would be around 300K gas and obviously another con is that you have to wait for the proof generation time and in our case you know our circuits we didn't spend that much time optimizing them but you know right now it's three minutes and I think in the context of ethereum this is okay because the finalization period is already so long but for other consensus protocols you know you might want to have a much faster proving uh improving time and that's something you could probably fix by using your generation improving systems and many of the techniques that a lot of the ZK VM people are exploring actually furthermore another challenge with this approach compared to a multi-sig approach is that for every new chain you want to adopt or a new L1 or even when a consensus algorithm on ethereum changes you sort of have to rebuild the circuit and you have to like kind of hand design these snarks to verify the consensus algorithm and I think from the developer's perspective this can mean that onboarding new chains can be quite difficult so in terms of our roadmap we're trying to take what we currently built for ethereum to production and I think our end goal is essentially to be this like trust minimized entropy layer for ethereum and other decentralized platforms and we want this to all be powered by proof of consensus and adopting these values of decentralization and permissionlessness and I think in the future the two things we're really excited about is building these succinct line Titans for other blockchain ecosystems and also exploring these new approving systems to decrease the proving time significantly and yeah finally we wanted to give a big shout out to gnosis Dao who originally funded this work and super um were super helpful and also Xerox Park um where we worked uh for the summer and the community provide us a lot of support and help thank you all right thank you Uma and John unfortunately we don't really have time for questions right now so catch him outside we have Oscar uh to talk about using Arlen we stairs are awkwardly over there yeah I'm using Arlen in waku for better P2P messaging foreign [Music] service network we'll show a demo with vacuum for we're using our land for private economic spam protection and what this means for you as a user and node operator and finally we look at how you can actively participate in and contributor Network itself as a node operator developer or end user and just as a warning I gave a talk yesterday that's going to be fairly similar to this one but this one is going to be slightly more higher level and targeted towards depth Developers so first briefly about vac and me some director of research back we built public good protocols for the same size web with a focus on privacy communication we do applied research based on which we build protocols libraries and publications and we're also the custodians of protocols that reflect a set of principles it has its origins in the status app and basically trying to improve on the online protocols and infrastructure and we build back among other things so as adapt developer uh you have you might have like a front end and some smart contact and the front end could be a website something else like a binary but what about all the other interactions so in the original web free Vision you had the whole Trinity with ethereum for compute consensus phone for storage and Whisper for messaging a lot of these centralized applications tabs they make sacrifices how they function from the way domains work to our websites are hosted and Reliance essential Services when it comes to communication and we see this time and time again where centralized single points of failure systems they work for a while but then eventually they fail and often individual users they might not care enough and is tempting for platforms to take shortcuts and that's why it's important to be principal but at the same time be pragmatic in terms of trade-offs that you allow on top and we'll touch more on this when it comes to design goals around modularity of Equis uh so privacy is the power to selectively reveal yourself and his requirement for freedom and self-determination and just like you need decentralization in order to get sensor resistance you need privacy to enable freedom of expression and to build applications that are decentralized and private protecting you need a base layer the infrastructure itself to have those properties synonyms proofs are a wonderful new tool and just like smart contacts enable programmable money Synology proofs allows us to express fundamentally new things and in line with the great tradition of trust minimization we can prove statements while revealing depth with minimum information necessary and this fits the definition of privacy the power to flexibly reveal itself perfectly and so I don't need to tell the one in this room but this is truly revolutionary technology is advancing extremely fast and often it's our imagination is limit so breakdown backers what is it it's a set of modeler protocols for peer-to-day communication it has a focus on privacy security and being able to run anywhere it's basically the spiritual successor to whisper and by modeler we mean that you can pick and choose protocols and how you use them depending on various constraints and trade-offs for example bandwidth uses versus privacy it's designed to work in resources secreted environments like mobile phones and in web browsers it's decentralized sensor persistent and blocks in agnostic so for example status is using it for its chat functionality World connect is partly using it for session management railgun for its relay Network and you can use it for things like multi-synchronation within also safe and there are currently three main implementations uh JS back over browsers go vacuum which is optimized for usage on mobile and desktop and then also has binding for things like kotlin and Swift and Macco in Nim which is their sort of Main Service node and we're all the research is happening and there's also an experimental version in Rust just briefly on some some protocols that are used so we specify how our messages are formatted to facilitate things like encryptions and centers a relay for sending and receiving messages and this is based on lipid because sub and then there are also protocols for a lot more light usage like filter light push in store um these are you node used for by nodes that are maybe not always online or they might have some connectivity or bandwidth restrictions and it's also all end relay which is used for peer-to-peer economic spam protection using Seeker stocks which I'll go into soon and there are other products here as well but these are the main ones of Interest right now so one way of looking at vacuo is as an open service network there are nodes with varying degrees um with capabilities and requirements for example when it comes to things like bandwidth usage storage uptime privacy requirements latest requirements and connectivity restrictions we have this concept of adaptive nodes that run a variety of protocols and node operating can sort of choose which protocols they want to run and naturally be some nodes that do more consumption and other nodes that do more provisioning and this gives kind of rise to the idea of a service network with servers are provided for and consumed uh this is one way of looking at the vacuum Network we have a bunch of nodes with different capabilities and they run different protocols and for the vacuum Network there are a few problems that arise here um for example when it comes to network spam and in centralizing service nodes and we want to address these while keeping private Securities of the base layer so spam problem arises on the gossip player where anyone can overwhelm the network with Masters and is a problem when nodes don't directly benefit provisioning of a certain service this can happen if they're not using the protocol directly themselves as part of normal operation or if they're on social inclined to provide a certain service and this depends on a lot on how individual platforms decide to use this network in this talk I'm only going to go into the first one but I also gave a talk yesterday where I touched on the services a bit more since the peer-to-peer relay network is open to N1 there's a problem with Spam and if we look at some existing solutions for dealing with Spam in traditional missing systems a lot of entities like Google Facebook Twitter telegram Discord they use phone number verification and while this is largely stable assistant it's centralized and not private at all historically wispy use proof work which isn't really good for heterogeneous networks and things like peer scoring but it's open to simple attacks and doesn't directly address spam protection spam protection in an anonymous peer-to-peer Network the key idea here is to use online for private economic spam protection using CK stocks I'm not going to go into too much detail of our land here if you're interested there's some write-ups on Vector Dev by synapse has been pushing out of this from outside and Taylor is giving a talk which goes into the online circuit in more detail in a few hours in this room um so but I'll just talk briefly about what is so Orlando stands for rate limiting nullifier it's an anonymous rate limiting mechanism based on CK stocks and by rate limiting we mean that you can only send n messages in a given period And by anonymity we mean that you can't link message to a publisher we configure it as a voting booth where you're only allowed to vote once every election it can be used for things like spam protection in p2pmsing system systems and also rates limiting in general such as for a decentralized captcha and there are basically three parts to it so you register somewhere and then you can signal and file in just a verification slashing phase you put some Capital at risk and this can need to be economic economical or social and if you double signal you get slashed so each user has a secret identity key and Reddit is a commitment of that in a contract and deposits some funds and this makes them a member of that outline group and the key to withdraw that fund is the secret entry key and if someone knows that secret they can take their Associated funds and for each Epoch the user sends a message by proving that it's member of the online group and it reveals smear secret of its secret identity key the sales derived in such a way that two of success in the same Epoch allows construction of the corresponding secret ID key and the secret sharing has another element which is an application a defined identifier which is their online identifier and with that you can sort of make the secret sharing unique for your application so for example if a user wants to use the same membership credentials for two different apps for messaging uh then it can do so without being worried about getting slashed by combining two shares from two different applications so here's a brief recorded video see here is playing so what this is showing is that on the right here you have Json in browser and you connect to your wallet and then sort of pulls down the state from the all-in contract including the number of members that are there and then after that you generate all anchor installs locally so that's uh yeah the secret key and then also the kind of the public key and then you basically register it in the smart contract it's going to take a while until it gets it gets through and then that's on the left here you see in vacu which is sort of this service node written in him um and that's that's going to listen for events from the contract and there's also this example chat application that you see in the top left it will take a bit of time to go through and then basically the next step after you have registered is going to be the signaling right so this is uh using vacuum in the kind of light mode using light push stuff to that uh here you can see on the bottom left you see that the new key has been added so it's receiving the public keys from the smart contract and basically uppling is local tree because it needs that in order to do verification and slashing [Music] uh now the transaction has been included see here yeah that was a bit slow one second right so here you actually sort of sending as in this case it's just using this very light uh mode to connect to the vacuum Network um and then you're sending a message and once you send a message it's going to construct a proof as well as some additional metadata that's needed and then it's going to sort of basically propagate it throughout the rest of the network and all the these nodes that are running all in relay will sort of verify that verify that proof as well it makes us make sure that this is yeah the seeking proof is valid that it's not double signaling and and it's in the right Epoch and these types of things so you can see here it's been received um yeah and it's gonna and it's going to keep reeling into other notes I'm actually against skip to the next one uh here just to show how spamming works well this is playing right right so in this case we have I think it's a Unix the epochist is basically like a Unix time uh like 10 seconds uh buffer so that means that if you send more than two misses in a 10 second period and then that will sort of um you will it will be an invalid message basically temperature right here we go so now you're gonna see in the bottom left that it's uh yes bam so spams has been found and it's going to drop the message for validation so it means it's not going to propagate it to other nodes basically uh and slashing is not fully implemented yet in the in the sort of this client but it's work in progress um yeah so just some backup slides in case the damage didn't work here right so now whatever you can you can build stuff with vacuum today and regardless of which platform what platform you're on if it's a mobile browser on a server whatever you can take it to evacu.org we'll talk to some people from Nevada team here I also didn't go into too much detail of this here but if you want to play with this Library we're building um you can also do this with the circuit and this basically enables you to use all learnable applications uh outside of vacuum in different environments so maybe it's not just a browse application but you want to do something in in rust or with the python or whatever um so it can be pretty useful because as a cffi API and awesome API as well and you can also run a node and this helps the network and it's a fun way to sort of get more Hands-On when it comes to peer-to-peer and CK infra even if you aren't super technical um yeah I'm going to check out our Discord and say hi as a cure code here direct is good here's some links if you find any of these exciting to work on feel free to come up to talk to me and we also are hiring um any questions or maybe that's offline no questions so the we kind of um get back to the drawing board thinking which kind of abstractions we need to provide for the ZK application developers and this is our abstraction we're coming out you can see it's extremely simple the name we call it elastic anonymity set right in the high level it's a set right and second it has anonymity there's only two things you need to know about that right so it has several apis right the first API is the content contents means you provide a zero to proof and then it verifies whether this element is contained in this an anonymity set or not and second insert you basically you can insert a new element to the set and last but not least remove you need to provide a lot of fires and provide provides their noise proof right and we kind of surveys like at least tens of physique applications we find this is the common ground all the applications are actually using right right and you hear the word elastic right elastic basically means um so okay so sorry it lasted basically means that we are doing automatic scaling for that um I wouldn't dive into detail yet please come to me if you want to know the details but from a high level point of view right so basically you set uh we call it anonymity guarantee right it could be one in 1000 it could be one in one million and this Anonymous guarantee actually or in fact how big your circuit is and we do automatic scaling right so let's say if you set the an Olympic guarantee to 1000 doesn't mean the set size have to be 1000 it just means the Privacy guarantee is one one thousand oh by the way the one in 1000 is the current uh industry standard for example like apple Facebook all using canva this is standard right um so how do we make this evm compatible with this es right so the way we do that is that we basically create a special node in the ethereum's uh Marco patrika tree and we built this uh Merkel full of all the es so that you still have evm compatible storage for for public smart contract and for this DK smart contract you can using um like Es as the first as a storage primitive right and because um like for example our prostato is a pre-compile and did a bunch of optimizations we estimates this um uh es is at least two thousand two thousand times faster to prove the compare with proving cash checking circuit right um so that's about yes right right next I'm going to talk about circular and so this circuit we think again using unit analogy this is kind of like a standard library right we just find like there are so many circuits can be reusable and can be compostable so that we should be building the VM itself the first circuit um I'm going to talk about is ZK asset and zq assets the idea here is a programmable private asset and this is actually huge influenced by um a research work called flex and by waydai um let me first tell you what does programmable even mean here and in this programmable we actually mean two things the first is that this private assets need to interact with public smart contract for example it can interact with your favorite ethereum D apps and secondly and this private assets should interact with other kind of ZK Primitives so that you can build a private and also ZK powered application system based on the private asset I will show more later right but that's a high level idea and from the standard interface point of view right so this ZK asset uh well actually start support erc20 erc721 and erc1155 right so let's Dig Down Deeper to see what are the apis we're providing for the ZK assets so basically we call it zsp which is short for zkas that show the pool so from again from the high level it has several functionalities first is deposit basically you are converting uh public assets to a utxo based private asset second is transfer you can privately transfer between two private utxos you can convert the private GTX those back to public assets and what's make it really makes us really different okay three minutes right okay sorry um I'll I'll tell you Craig sorry about that um what would make totally different is that you have a private contract call right right basically for any contract calls as long as this contract course side effect is only on the standard asset it can wrap the smart contract costs as using private asset and for time's sake um I will skip how does how it exactly work and second yeah yeah I do want to cover more right so and so and for the the second one we call the configurable as a disclosure uh which means that let's say you can have a circuit you can prove for example you own a ape but you don't show which particular app you own right so last but not least is that Suzuki signal right we have many amazing speakers tell you how the sum of our work how the ZK signal work and I will I'll skip here as well right but there's a slight difference but it doesn't matter right so let's say how can we uh so and again this is only start and there are many other circuits will prevent building for example Mev prevention mental poker for ZK basic gaming and please tell me your favorite asset we want to mix a standard libraries as a community process right so I think I will skip all this crypto detail here just show me show you one idea right that's one thing distinguish personal VM from anywhere else is that you actually can build uh like you actually can using standard library and purely using solidity to build a to build a z cap so this is this is one of my favorite ZK app example is that you're deciding like apefests 2000 2023 do a non-voting right so how does voting just you can see you can just using um like less than 10 lines for solidity right let me go through one by one right first you call our zsp circuit right you prove set say hey I own this particular utxo and here this is some technical detail we call it commitment basically this is the communication mechanism between different circuits right secondly you prove that hey I own an ape but don't tell you which if I owe last one this is similar to someone for protocol basically it's the ZK signal to show that you are not double voting you only voted in this event once and then you increment this this vote count right so I I can't quickly show you right just using solidity you can build Decay apps and in our view the 95 percent of Z caps in the market right now right you can purely using solid disappear um so that's a um so that's a that's a let's talk about composition I'll skip skip the trade-off here and also there is no trade-off presidential VM is best um so um yeah so okay so last I want to conclude and don't want to uh text the other speaker time level right so why I'm here right so um I show you a person on VM this is a the the game changer here is that we're building layered programming abstractions for zcap development and we want to support their cheap seek applications and we want to have maximum UVM compatibility right and uh that's my talk [Applause] follow this up with a quick break if you want to take like one question I think we'd be okay with that does anybody have one question oh too bad you can't I guess I knew later okay we're gonna break for just a couple minutes and then we'll be back with back to back ZK machine learning with Jason Morton and yisun thanks everyone [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Applause] foreign [Music] [Music] [Music] okay I think we're gonna jump back in if everybody can take your seats and quiet it to a whisper um and introducing Jason Morton to talk about ZK machine learning take it away Jason is me um theoretical exercise uh but it has is becoming increasingly relevant as we build zero knowledge applications um uh for those of you that aren't as familiar I like to say that zero knowledge proofs started digital signatures as ethereum is to bitcoin we're replacing a digital signature with a any program right and the thesis of sort of zkml is that the most interesting program actually to put in a zero knowledge proof is a machine learning model and the reason for that is that it makes it as though the model the AI is actually running on chain so we're giving intelligence to ethereum or whatever chain it's running on um and it's kind of like we're giving it Eyes Ears you know sensory organs for which with which to perceive the physical world um with which to make decisions about physical reality and the most important bits of physical reality to us which are the humans right what our intent is so it makes it possible for humans not just field elements to control their destination to control digital assets so imagining that in the future we'll be able to say just like we talked to our smart speakers you know hey ethereum you know please transfer 10 F to Althea and that will work right because some Ensemble of machine learning models will be able to combine all the signals of my intent all the data exhausts that I've given off in my life and decide whether that is true or not for a human that exercise is Trivial if we've talked for 10 minutes um and I do I make that request two years from now you'd be able to authenticate and send that message the fact we can't do it yet with uh with ethereum is just a just technical problem um and finally you could think about it as a way to let smart contracts exercise judgment right deal with any kind of ambiguous situation decide if a contract is satisfied decide if a news story says that a hurricane has hit the coast that a thing and this little chart here is just a reminder that both the input and the parameters we can choose either private or public and all four of those squares are interesting everything's public we can think about the news model scalable Oracle if everything is private maybe we're thinking about you know decentralized kaggle or some kind of um medical situation where on the one hand the model has to be kept secret for reasons and the date patient data has to be kept secret um so what we've built is a program called Ezekiel it is a tool to build turn Onyx model which is an export that you can make from pytorch or tensorflow so a baked machine learning model that's been trained into a zero knowledge proof we can prove and verify at the command line export to a binary or a contract a smart contract or wasm we're adding new layers to it daily and it's enough for sort of small production models the scale is increasing very quickly sort of two to eight times a month depending on how much time we choose to spend on optimization as opposed to adding features and I expect that we'll be able to do some really exciting kind of identity stuff within maybe six to 12 months maybe sooner um and we've been building in the open since July it's Apache 2.0 license and uh that's the repo we would love any contributed contributions from you if you find this interesting um so here's an example of the kind of thing we can do right now uh if you like python this is very relaxing as opposed to writing circuits you can define a forward model X Y and Z or tensors they're with a shape that's determined at runtime and I can just sort of Define somewhat arbitrary functions of those tensors Matrix multiplications Powers non-linearities compose them and then the tool will translate that into determine a quantization strategy figure out the runtime shape of the tensors and translate that into a something that can be run as a zero knowledge proof um so the pieces we can do right now are sort of the describe what we're going to do do a mock or a full proof so the back end is all Halo 2. um we can do a complete a proof and we can verify a proof um and then there's some sort of technical parameters this applications talk so I want to kind of give you a sense of what's possible and then we'll talk about what applications are cool um right so this is an example of computing the table from a very simple model that just has an input a weight and a bias a matrix multiplication and a relu we can do a proof looking at different uh back ends proving back ends and we can do a verification the the proof will construct a file a little Json file that can that contains the data of the proof that you can upload to whatever the verification thing is and then you can run also at the command line of verifier to check that proof um and okay so now we go back to applications that I always like to show a little bit of code so that we can think about kind of what is it what is it what stimulate imagination what I like to think about this is kind of allowing scalable automated oracles right so there's three stages to that the first stage is ingesting some kind of signed signed data now it works fine with not signed data but there's sort of an adversarial problem so we need to somehow rate limit the amount of data that's coming in or or make sure that it's been signed off on by like say a news organization or camera or something like that then we run a model maybe it's a text model maybe it's image classification it makes a decision about what the data was and finally there's a unchain verification that then feeds back into the sort of attestation Loop and can be used in the next model um so interesting things in the sign ingestion this is this thing this space is really an interesting stage so there are solutions there's something called a sxg s XG which is a signed HTTP standard promulgated by Google kind of replacement to amp um there's an nginx plug-in there's a one-click button on cloudflare a lot of people promise to do it almost no one has um there's email there's a few other people from Xerox Park that have worked on the email that you can hear about from ayush um there's images at the publisher so the news organization New York Times Etc might sign off on their images so that other people can't say oh New York Times said this bad thing happened um and then their third party notaries uh like TRS notary lip protocol that you can use to um create signatures uh as sort of a semi-trusted third party and they all use things that we can now verify um but I think we need we're going to need an industry at a push like https to push everyone to sign their data it's technically easy it lets us compose things in zero knowledge proofs where people just don't bother to do it right now although they've all promised you uh so the second thing is the text model classification what kind of models can we build and that's sort of a scaling process um I describe our roadmap as kind of ontology recapitulates biology which is to say we look at all the models that have been built over the history of uh of machine learning and then you can download them you try to run them your tool on it doesn't quite work because they're scaling problems or quantization problems or because you haven't implemented all the nodes and then you fix it right and then you repeat so as I said open source project would love you to join me on this rotation and then we have to worry about scaling and there's kind of five ions that we use for scaling so optimization I won't say much about that I think you will talk about that a little bit in his talk aggregation which is a tool for combining multiple proofs into one proof and checking it checking it once recursion which is being able to verify the last snark inside the new one and that kind of lets you do something like uh do a separate proof for each layer that gives you another scalability tool and between the current aggregation and recursion what you're going to see is the is instead of memory constraint which is what usually the problem is in for the size of the models we can handle um it's just money constraint right how how much how much computation we like to spend it's not it has a lot of overhead but it kind of eliminates the limits on how big a model that's why I can make grandiose claims about the size of the model that we're going to get to um and then finally Fusion is kind of the strategy of once we have a higher level understanding of the intent of the programmer the computational circuit that they're expressing in python or whatever it's easy to swap out more sophisticated zero knowledge arguments instead of looking at the level of constraints I look at the level of matrix multiplication or convolution and you can use those arguments and make it invisible to the person who's created the um the the machine learning model the machine learning model doesn't change at all but it kind of gets faster and faster in the back end um right so finally on chain verification um uh this is as I think you've heard uh you know severely constrained by the pre-copiles we have available in ethereum and a lot of the the strategies just getting it to fit into a bn128 um so the way that it can be done now is you have and probably the future honestly just because of the nature of of zero knowledge proofs is you have a first stage which is you know some sort of input and then you have easy to make yet hard to verify proofs right sort of wide proofs those proofs are aggregated using an aggregation strategy which might require a fairly Hefty machine so one example I did used 450 gigabyte machine RAM and then that produces it hard to make but easy to verify proof that you can then be checked on ethereum and right now it costs about 600 000 gas to do that but there's some things that we should hopefully improve that over time um right so putting all this together what do we get we get kind of this scalable on-chain data feeds you can sort of think of it as oh ethereum can read the news or instead of having a network of nodes that make a decision like in a chain link about something that's happened and then vote on it and there's a complex crypto economic process by which they penalize people who who attested wrongly you just have one person they download the signed data from Bloomberg they produce the zero knowledge proof they upload it and now everyone can trust that information so that enables a much more scalable fire hose of data coming from off chain from web 2 to on-chain um and it's my belief that uh zkml will actually be table Stakes for chains over the next sort of five to ten years that while delivering on This Promise of blockchain to a mass audience means that we really need this kind of a human not a field element owns all my assets right um there's lots of solutions I've heard at this at this meeting towards that and this is part of that you can imagine for example when the account abstraction starts working that part of the account abstraction check that you're doing is to submit a zero knowledge proof of your identity right using this kind of 10 000 Factor everything about me um as one of the pieces and so zkml oracles will be simpler faster and more scalable um to put arbitrary on off chain data on chain and just I think really opens the fire hose to what we can do um and finally as I said before a zkm model you can think of as a smart judge that interprets ambiguous events and in the remaining two and a half minutes let me just talk about a couple of examples ideas of things that I hope people will build with this um so one that's sort of timely is zkyc right so we can take a person and an ID and prove that they match and that the ID number is not in some sanction database right okay of course that's technically something we can do very soon however Regulators won't accept it right banks have a kyc rule it has it's it says specifically that they have to know the customer not that they have to know the customers not on a sanction list so it seems like it won't work however maybe if we had done that it would have prevented the tornado sections right it does if you're a D5 developer or a mixer developer you add that to your pipeline it sort of makes you less of a Target you know it lets you run faster than your friend when the bear is chasing you um or at least let it uh does something to prevent you know unwanted actors from interacting with your contract so even though it's not perfect it's interesting um of course I talked about prediction markets you could imagine setting up a a contract that pays if a news story classifies to a particular thing someone won an election a hurricane of a certain intensity hit a coast a car received a lot of damage and then a small classification model because it's relatively few classes can be used to to decide whether that happened and anyone can you know download the science story run the model submit the proof another thing is kind of fraud checks so gut checks for smart contracts you could imagine uh the abstracted account or the smart contract just has another little zkml check that is a rate limiter and a fraud check and makes it harder for people to scan that contract right um a proof of humanity for example would work but lots of other kind of fraud checks or checks of the state of the network and you could imagine getting baked in a little bit like civil protection but it's okay if it's weak because it's it's just a layer of security um I think it's exciting to think about this is putting the a in Dao so really making taking a situation where humans make a judgment over what happened vote and then some multi-sig signatories uh actually execute that judgment and actually putting that all with an effectively on-chain AI right so instead of working for a doubt we all work for the the all-knowing AI in the cloud um it's gonna be good or bad but interesting uh and you know they could use for example to test whether or someone who promised to do some work for the Dow did a good job did the work right you can imagine making a classifier that did that um you can think about using it for genetic screening situations where for example the model has to be secret because it's been trained on on non-consented data the data has to be secret you know just like when someone wants to get an STD test or something they would like to choose whether or not to reveal it to anyone and maintain anonymity um so that's another application um and finally it composes well with not just NPC but also things like differential privacy you can imagine a data owner uh this has nothing to do with blockchain necessarily but you can imagine a data owner that wants to protect their data they can release noisy summaries in a differential privacy sense and then receive the the the query or can run the model and torture the data and send a final query which can then be run in a proof to show the real model matches the commitment it was used to create the noisy marginal summary and when it was run on their data It produced the real result okay so hey come out thank you thanks Jason if you want to take one or two questions um well his son gets set up Assuming he's he's here here hello um uh Houston is going to talk about another aspect of DK machine learning specifically in neural networks but did I see a question over seven questions yep she learning models are often kind of already black boxes how do you verify that the right machine learning model did the prediction or the verification yeah you can you commit to the model so that's particularly easy when the model is presented as an onyx file you just take the hash of the file and that's your model for example or you can commit to the parameters themselves so you make you make some kind of basically a hash of the parameters or hash of the whole thing and that's how you know it's the model you want it to be do we have one more quick one everybody else can take them offline sorry really cool stuff um has the ckyc sort of being worked on is that a theoretical possible application of something like this or like what's the state of yeah I'd say it's it's absolutely being worked on I think it will be a little while before it's feasible um but you know it's definitely something that I expect to see probably multiple people create in the next year so yeah love to talk to you all right well actually we're having some are you good or all right never mind technical difficulties resolved all right welcome and take it away hi everyone uh so this is a bit of a book report about what happened when I try to convince some machine learning people that zero knowledge proofs were very relevant to them uh so in that line it's this is Joint work with a bunch of machine learning professors okay so the motivation we came to this problem with is that right now today a lot of the uh least trusted parties in society are running machine learning models that have broad societal impacts now you may think that blockchain is a very low trust space but I bet that people trust Twitter like even less and I don't think people trust credit scores very much either nonetheless these things are changing a lot of the way that we interact in society um a second thread is that machine learning as a service has been growing in popularity especially recently with the rise of large Foundation models so in this Paradigm you know you're maybe making a slide and you want a cool image so you'll send hey openai I want an image of teddy bears working on AI research underwater and hoping I will send you back this beautiful image to put in this slide now the core problem in both of these use cases is that well as the user you have no insight into what the model Runner is actually doing you have no idea if an open AI actually ran you know Dolly 2 or they just have a farm of digital artists that are really exploited okay so we want some notion of trust and verification in this type of model inference okay so if you ask a computer scientist they'll tell you that there are actually many approaches to this Beyond ZK okay so we had to learn about them and the first that they'll bring up is multi-party computation so in this case a number of separate parties will perform the machine learning operation together and of course because they're collaborating this requires that the parties are online simultaneously and there's a one of n trust model for privacy and correct execution that means that you need to trust that only one of the end parties is actually behaving honestly so it's possible for NPC to provide both privacy and validity but unfortunately it requires pretty high interaction and high bandwidth and at least at present there's a really high compute overhead if you want to ensure validity so as a result we looked into the literature and it looks like MPC can only handle relatively small models today another technique that's pretty popular is homomorphic encryption the idea here is you take your input data encrypt it and send it to some server that runs a machine learning model on the encrypted data without knowing about it so in this case homomorphic encryption is only targeted at privacy and not at guaranteeing validity the benefits are that you don't need to interact with the server Beyond sending your encrypted input but the downside is that there's a really massive compute overhead for NPC I said there was a big compute overhead but trust me this is really massive and so as a consequence MPC today as far as I know can only handle quite tiny models I mean recognizing digits okay so now we come to ZK so what is ZK adding that NPC and homomorphic encryption don't bring to the table here so with ZK while the server is doing inference it can generate a proof of valid execution and with that proof anyone can verify that the output was correctly generated so in this case ZK can ensure validity that is that the model was correctly executed and it can ensure a weaker notion of privacy than MPC and homomorphic encryption that is for ZK the model runner needs to know everything about the input whereas that's not necessary in NBC and homorific encryption but once you have the proof the input can be private from the rest of the world another upside is that there's no interaction required and a downside is that there's still a pretty massive computation overhead over the cost of inference so in this talk I'm going to talk about a method we developed to scale this the application of ZK by reducing the compute overhead from pretty small models to relatively large models okay so let me first situate you in the task and what I mean by small and big so a lot of Prior work in ZK for neural network inference focused on these two Benchmark tasks mnist and c410 the task in mnist is you get a black and white digit and you have to recognize which of those digits it is the post office uses something like this in cfar there are 32 by 32 pixels of 10 classes of things like car automobile horse boat and you have to choose for each image which of those 10 glasses applies so in our work we scale things up to approach the much larger imagenet data set so in this case we have 1000 classes image that happens to contain many breeds of dog so actually there are 200 breeds of dog at 1 000 classes the images are much higher resolution they're 224 by 224 and this is really the first standard large Benchmark data set for this image classification task okay so here's what we can do we chose a model called mobilenet V2 which was developed originally to be run in low resource environments such as mobile phones for a range of input resolutions we're able to completely snark the execution of the model and as you can see the proving time for a single input is still relatively large okay so even for the smallest input resolution of 96 by 96 we take over two minutes but we are able to scale to the full imagenet resolution in about 20 minutes so this is using the Halo 2 back end with the Original IPA proving system as you can see the proof size is relatively small and the compete verification time thankfully is not as large as the proving time although perhaps still quite large okay so let me in the rest of the talk let me talk a little bit about how we did this and what we can do with it so when you think about snarking execution of a neural network we came at it by dividing the problem into three pieces the first piece is it turns out that the notion of difficulty of execution between neural networks run on gpus and snarking things in a zero case circuit is quite different so we tried to select the best architecture for this task the second is we had to devise a pretty optimal way of arithmetizing this neural network inference operation into a ZK circuit and last we selected a proving system largely because we were most familiar with Halo 2. so that's not a very principal choice but one thing that I wanted to note is we decided to go with a general purpose proving system instead of a proving system devised specifically for neural networks and the reason we wanted to do that was to plug into the existing tooling ecosystem around Halo 2 and other similar general purpose proving systems okay so to dive in a little bit more on what type of architectures are easy to do in ZK the first thing to note is that it's very hard to deal with floating Point numbers within a ZK circuit since every variable in a ZK circuit is under the hood a large Prime field element so that essentially forces us to work with quantized models that is models that were specifically designed to have all of their weights be 8-bit integers for now the level of quantization in machine learning generally gives you a trade-off between accuracy and how few bits you've quantized the model into and in ZK this translates to a trade-off between accuracy and proving cost so we did a first pass at this among just off-the-shelf models and it seems like models optimized for low resource environments particularly on the edge are quite good at this and so we chose to snark mobilenet V2 you can download this model from the tensorflow website the second thing we did to make our search over architectures a bit simpler is we wrote a transpiler from the tensorflow Lite model format to Halo 2 circuits and then we applied all of our optimizations to the individual building blocks of this transpiler so this allowed us to handle the many weird low resource neural networks without massive programming overhead finally in the arithmetization back end we use the planckish authorization for Halo 2. and so we actually only use two types of features of Halo 2 we handle all the linear layers via custom Gates and we use the lookup tables to essentially build a lookup table of all all non-linearities and finally there's a small subtlety that we have to readjust the fixed point in the quantization and we're able to do some optimization by shoving that into the lookup table okay let me now finish by just giving you a general sense of one application that we can do with this so there's sort of four settings as Json alluded to so where the you can make the model either public or private you can make the data either public or private and you can combinatorially combine these so we have a couple ideas in each quadrant so if your model is private and your data is public maybe you're trying to sell your model and you want to demonstrate to the buyer that your model is any good without just revealing the model altogether if your data and model are both public then we think that this could be useful for on-chain verification as Json alluded to and finally if your data is private then in both the private and public model formats we think that this could be useful for conducting an audit for example in legal Discovery without forcing revelation of the entire data set that you hold okay so just to give one sample back of the envelope calculation of how much it would cost to use our snark to do verified machine learning model accuracy we made a very basic protocol where a model prospective buyer asked the model seller to verify the accuracy of a model on a randomly chosen test set so in this case you want to randomly sample from the test set find the accuracy on the random sample and from that you're going to get some statistical estimates of your overall model accuracy um so we ran the numbers and it turns out that if you want to have um you know within you have want to know the accuracy within five percent at a 95 confidence interval you need to sample 600 times and that will cost about ninety dollars to verify if you want to be within one percent that's going to cost you a little over two thousand dollars uh with our current implementation and just to give you a sense of whether that could be reasonable people generally are willing to spend in the high five figures maybe low six figures just to acquire data to train their model so maybe it's worthwhile to pay a couple thousand dollars to verify that what you're buying is legitimate all right so just to summarize uh we constructed the first snark we think that can scale to imagenet level and for this we had to choose the correctly quantized mile model write a compiler from tensorflow Lite to a Halo 2 plonkish arithmetization and then we ran some benchmarks on whether this sort of technique is reasonable for some concrete applications of verified inference so going forward we're excited to explore some more applications of verified inference and also try to scale this to different types of models particularly Transformers thanks and I don't know if there's any time for questions thank you thank you one gonna be oh and what do you know it's our next presenter that's so smooth I'll just hand you this mic and you can keep it wonderful um so one question you mentioned a couple of optimizations you did can you reveal some optimizations you think you could do in the future that's a very hard question since if there was an optimization we thought we could do we generally just did it uh one thing I didn't mention is we tried to reduce the number of lookup arguments by sharing them across layers um in gen this actually modifies the computation slightly so there's a little trade-off between the machine learning model accuracy and the proving speed foreign thank you so much and up next we have Remco who's gonna I think use a whiteboard to tell us about yes zero knowledge identity I spent the past three hours trying to make good slides in hack MD but the Wi-Fi here unfortunately so hi all I am Remco from a crazy project called World coin uh you might have seen some of these funky sci-fi looking devices around [Music] um they are very conveniently round um I'm going to talk a bit about where we apply zkp in our stack because it turns out there's actually a ton of really cool application for what we're trying to do now a little bit of what we're actually trying to do we're trying to create bootstrap Universal basic income through um through crypto and one of the things you need to do as soon as you start handing out free stuff which Ubi basically is is make sure that people cannot fraudulently claim it basically simple protection so we have to solve the Civil protection problem there's a couple of approaches you can do kyc is one of them but not very privacy preserving and we are all ckp people so we like privacy turns out that the only feasible strategy up to let's say a billion people foreign is to use Iris based Biometrics and you need really good Hardware to do that better than what you can currently find on the market so that's that is what this device is now the way it works is um you take you take your wallet you download the app on your phone um it generates a public private key pair in a system called semaphore that I'll go to soon you show the public key as a QR code to the device the device interacts with you make sure you're a live human being takes your iris and Bets the iris through a neural network all on this Hardware um in a 128 dimensional vector and then signs the public key this short Vector as a message goes to our server we make sure that it's sufficiently distant from everyone who signed up before add the factor to the database and then just send the public key to the semaphore smart contract where it gets added to the on-chain Merkle tree that's the sign up part after this it's just you your wallet and the blockchain we're no longer really involved is from there on it's fully trustless and decentralized and the way you use this system is your wallet creates a grot16 zero knowledge proofs that prove that you know the private key to one of the public keys in the Merkle tree um you don't actually reveal which public key that is you're completely Anonymous in that sense you generate a unique nullifier which is kind of like a pseudo random number for the specific context that you're using to make sure that you haven't done the particular thing before and then you can attach a message to the thing so you can sign some message with this concept so where can you use this for example now we use this to airdrop tokens to our users they can claim a token by generating proof that they haven't that they're a unique human being and haven't claimed this token before another really cool use case is for dowels where you can vote on proposals and you can build mechanisms like one person one vote or quadratic voting and have the proper simple protection to actually make this work now the way the anonymity actually pans out is so strong that even if you vote on multiple proposals you wouldn't even be able to trace that behavior back to the same human being so privacy is extremely well preserved in this system so this is where we are now but um there's a couple things we would like to do better one thing we need to do is right now the public key gets added to the Merkle Tree on chain which costs about a million gas which is completely not feasible on ethereum mainnet where we want to launch this thing so the first thing we did is build growth 16 proofs that aggregate multiple insertions and create a batch insertion operation and basically turn the insert part of the whole stack into a CK roll-up thing so there's like a back-end server a sequencer that creates these batches and sends them now um the second part where we could do this is in the claim part which is currently individual proofs 300K gas probably good enough for now but we can also help out there by allowing people to deploy a d apps they don't use it to deploy their own sequencers have proof aggregation going on and get that cost of usage down as well which for voting you want because you want fees to participate in Dallas to be as low as possible dare things already get more complicated because now we need to have a recursive proof system the users produce these proofs on their wallet and then we want to batch them together in lot 16 this is currently very hard so one of the things we're working on right now is migrating the semaphore proof system over to plonky2 and then have recursive clunky 2 proofs and then have a final growth 16 verifier of the plunky 2 proof that turns it into a growth 16 proof that can be very easily verified on chain so now you have a proof system where you have very fast proofs with spunky two which blanky2 allows you can recurse those proofs and then at the end you can turn it into a grot16 proof and you get all the benefits of the cheap verification and the white support that was 16 has really excited about that thing um what else is going on so another thing um another thing that's going on is we want to sign up a billion people and right now ethereum l2s simply don't have the capacity to handle that we're also going to sign up people with relatively low balances we want this to be very inclusive so assume the average user has a 10 balance and wants to use that on an L2 right now on ethereum that is like five cents per transaction which is kind of okay is but it's because of the bear Market um a year ago it was two dollars per transaction so that's not good enough so how can we fix this well there is a plan on the roadmap already it is called dank sharding and proto-dink sharding so we're very actively involved in in the client the development around 4844 the EIP that implements this we've been helping out implementing The Trusted setup ceremony that Carl presented during the opening ceremony and we're going to be helping out the clients implementing the crypto there just to get dank sharding in just so that we can launch on an evm compatible L2 and still have something our users can use um another area where cryptography comes into the equation is right now the neural network basically runs on trusted Hardware but like you've heard in the past two talks we can actually do better than that zkml is feasible a year ago I was experimenting with running a machine learning small little Network in a plonky 2 proof and I got I managed to do 10 million parameters on my laptop in less than a minute so technology seems like and that was without any significant optimization so presumably we can get much further than this and this is good enough to do the biometric stuff on your wallet now what is the advantage there and how do we get there in the first place like right now if we update the neural network which will happen a couple of times we would need to ask all our users to go back to the hardware so that they can trustlessly create the updated factors If instead we turn the orb into an authenticating camera like Jason mentioned the orb will just sign um sign your raw data send it to your wallet you can keep it on your wallet self-custody you're in control and then whenever we have a new version of the neural network we can just push the parameters to your phone and you can just recompute the thing locally and then send us a zero knowledge proof that hey I have this um I have this raw diametric data it's authentic it's signed by an orb and here is the updated embedding and we can update it in the system um another cool area is um looking at NPCs to get rid of this Central database and make it a little bit more decentralized we're not actively doing that actually at the moment but we are actively looking at is using mixnets or private information retrieval to make sure that when you query one of our servers for example to help you generate the miracle proof instead of downloading all the raw data from the chain make sure that we don't learn your IP address in the process um let's see what ah yes one more application of the zkml stuff longer term so right now what I've described here is a way to do a scalable privacy preserving proof of personhood that we can deploy on ethereum mainnet scales to a billion people and then can be used on all the chains that we can copy state to which is basically any smart contract enabled blockchain out there um but proof of personnel is only the beginning you would kind of soon as want to do a little bit more like that you would for example want to make sure that someone is in a country that your the app is legally allowed to support I don't know um there will be applications that require such things uh how could we integrate that so one of the things we were thinking of is um we could have the orb also take an authenticated image of your face just like with the iOS image send that authenticated image to your phone you can scan your passports all modern passports have signed data on it which includes like this the stuff that is basically on the front of the password your photo your date of birth your nationality and so on and what you can then do is use a zero knowledge machine learning to verify that the two phases are the same the one that we know is you because of the orb and the one that is on the passport verified that those two are the same verified that the passport is signed by a key from some government or like is in the list of non-government root certificates basically and then you can you have a chain of trust establish established and now you can fulfill arbitrary queries like is the name in the overact list is the nationality in the approval set you name it um yeah and I think that covers what we've been doing so far in terms of applied ckp at worldcoin and what we're planning to do in the next couple of couple months years probably um and I'm opening it for 20 questions [Applause] thank you Andrew and we have time for a couple if if there are any questions uh what stops me from stealing your eyes ah yeah there's a very common misconception about Biometrics people think of them as sort of a private key like if I somehow get access to your fingerprints I can steal your accounts doesn't work like that your fingerprints are not secret you leave them on every single glass on every restaurant you've ever been to the thing that uniquely identifies you is that you have that fingerprint on a real life finger so it's very important that you check what is known as the liveness check that you make sure that you the data you're reading actually comes from a real live human being and not from a printout or whatever um fingerprints are actually not that good they're only unique up to a million people we really need irises for that I always have a couple more advantages you don't leave them at restaurants it's actually really hard to get someone's Iris actually accurately so it's it's quite consensual the process like you need to actively participate in order to get a good capture of this information cool and thank you for the talk can you explain one more time the example of a digital camera attesting to the data prior to it being sent and how that gets around the issue of your face changing uh your face changing yeah having to redo it so we um so there's two parts where we do this authenticated data game uh the first is just to make the user experience of updating the model better like instead of having to go to The Trusted Hardware again we can now trust you to do it yourself correctly because of the zero knowledge machine learning the second example I gave is that well what if we have a proof that this phase belongs to this public key and then we can create a chain of proofs that oh we know that this public key become looks at this this phase because the orb signed to that fact then this phase belongs to this passport because we have a Serial knowledge proof that those spaces are the same and then we can do all sorts of further claims based on the fact that we now know that this public that this public key belongs to this name and all of that happens self-custody on your wallet so none of this information ever leaves your domain thank you so much um we're gonna well we're gonna cut for a quick break so I guess we might as well take one more quick question going back to the question about stealing people's eyeballs what's stopping someone like uh let's say an African warlord taking an orb and scanning entire Villages eyeballs and then taking their accounts um I mean ultimately on the social level uh this this problem applies to any any kind of system like if you have a super oppressive regime people can make you do anything um what we do about it is we're just simply not going to those places uh yeah we we our goal is to be as widely distributed as possible and be everywhere on Earth um we're not specifically targeting in such areas we are just as active in Europe and America and you name it uh yes it does a lot of liveness checks um this is actually another interesting discussion that we keep having is the hardware is going to be open source the firmware is going to be open source all of this is open source except for two parts that we're considering keeping private which is like the exact nature of the liveness check because we're confident that they are very very hard to circumvent but still everyone is just better off if if we use Secrets like security two obscurity here and the same thing goes for the hardware tampering like there's a bunch of booby traps in there that prevent you from taking out the signing key and also there we uh made the decision that security to obscurity here is better it's it's one of the unfortunate things you need to deal with once you do try to connect cryptography to the real world you need to deal with the fact that the laws of physics don't have a nice difficult and handshake in them okay sorry we really have to cut right now but I would like to invite you Remco because it seems like there are a lot of questions and I have some also um to maybe hang out in the temporary Anonymous Zone um after this session is done and people could maybe come ask questions we'll be there sound good all right so that's on the first floor um and we're going to take like a two minute break to get set up for our next chunk um thank you all so much foreign [Music] [Music] [Music] foreign [Music] [Music] next [Music] time [Music] foreign [Music] [Applause] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] thank you all right if everybody could kind of settle back in and or move your conversations out of this room that would be amazing we are ready um so introducing a scander to talk about Cryptkeeper yeah can you hear me guys hey everyone thanks for being here um I'm skander uh from BC at the swimming Foundation I'd like to make a presentation today about Kurt keeper one of our projects um it's still in progress not published yet but um we're ranking this okay it's not working uh so Cryptkeeper is a browser extension plugin that enables users to have a management for their zero knowledge identity and an ability to have like a generation proofs for uh semaphore and rln and for those who know doesn't know like what semaphore and Ireland are like there are separate uh presentations for them and like simply ximaphor is um is like a layer and in ethereum that allows users to to join like uh in our groups for both for like having votes and Ireland is like limiting is a protocol zero knowledge protocol for limiting like uh these banners in a group um cool so what are the components uh built in this uh like those are four projects from our team uh ZK get library and interrupt I'm gonna go into interrupt like later and rln and sema4 um so we only now supporting chromium-based browsers um like yeah these browsers um cool we're working still in like and for future releases working on having like a mobile version later um yeah so now it's time to see this in action live live demo uh so the first use case I'm gonna have today is showing Crypt Keeper in an anonymous social network which is uh Z Keller and Z Keller is anonymous social network there is a presentation maybe the next one after this about Z Color in details so um yeah and the Second Use case is uh using uh cryptkiver to prove or generate proofs from SEMA 4 and rln lastly just going through the agile project management board all this open source for those who are like interested in contributions or participating they can start from there um culture let me jump into the videos the internet is so bad so um okay so first off like um it's gonna start uh with like the the first setup is just creating a password for your extension it's not a wallet yet so just an identity extension um so I'll create a password then in order to create an identity I need first to have like a public address connection to my wallet in order to make like signing in order to have like signatures way um so I'm gonna First Connect with my meter mask um then it will be connected like um this is like the signing the signature um so okay I think the video is skipping some parts yeah okay um the videos yeah after connecting to the meter mask it's connected and got the public address in future releases we're planning to have like more support to other wallets um it reads the balance and then I was able to create um like zero knowledge identity by connecting to interrupt but this is not a completed process yet in order to make it complete we're working on like making all completed in the same extension but for now we're going to connect to Z Keller for make this process completed and then I can I can use my identity and Anonymous Network um so as you can see now I'm connected I'm in Z killer um and I added a a user like coming back a user by connecting to a Cryptkeeper so it reads the identity commitment and it's going to connect to internet for generating a proof of reputation this is what interrupt does um and the proof reputation is by going back to Twitter account um and it will like generate some credentials about your identity and then generate like a approved for your identity a kind of reputation for you so to the next one okay all right right yeah yeah okay so here um I have already a stock please so here I have already connected to my Twitter and it generates my reputation my username and my rotation is like server a server reputation so now I can join the group uh and I can anonymously um let me jump to the third video so now like I'm ready to like use my new identity uh using connected to Cryptkeeper uh which like connected to my reputation proof and then um I'm gonna be able to like participate in this uh Network Anonymous Network to write my tweet Hollow Defcon and as you can see like I'm a user with 2K plus followers which like can't approving I'm a person I'm not a spammer I have reputation so you can have a trusting for reading by tweets um yeah so that was like the first use case on the show um the Second Use case is we have like a small dummy uh testing website that just connects to semaphore and RN and for like make this showcase for proof generation um so I'm gonna go connect uh to it so I'm gonna first like showcase how to generate Sima for proof um so yeah the output is presenting here but the video is kinda yeah so now I'm gonna create like the proof from semaphore uh using my wallet uh yeah so for those who like know the details here that's familiar to them like the public signatures those are like the proof generated proof from and the same goes for R Ln and should be presented down here I think yep uh cool and at the down here is the identity commitment this is like the new representation of the users in Anonymous Network uh so yeah I think that covers it um so um maybe I will show now the uh this is like our working repository now um and uh this is the Management Port where like any contributor anyone interested in throwing ideas we can start from here on from there from here or like there are discussions supported here we don't have like a Discord yet but we can like if you have questions or throwing ideas you can start from there um and there are like a lot of bunch draft ideas you can start uh if you would like contribute uh so uh back to my slides uh we have like two other contributors there with me here and uh to sunoku and yeah that's it thank you thank you so much like that so next up we have Vivian come on up Vivian I'm presenting unirp foreign okay hello I'm Vivian and I'm also from PSE team and I will demo the universe social today and universe social is the uh example application of unira protocol so what is unirad so uh you need actually universe is a cross app reputation system so you can imagine that there are many applications there are reputation system upon them but they cannot communicate with each other so if I use in uh to the application and how can I prove that I own this interpretation to a GitHub user so um if you just show the screenshot of the Twitter and then you cannot uh you can easily Forge them so unirp is the reputation system that just uh the neural knowledge proof to let the user to prove your reputation to another application so you can imagine that unirp is a ERC standard and they can uh you can issue a token like reputation token and used it in your application so this is a summary of unirad that this is a private and non-reputable reputation system that based on the KP so to make it more simpler and in you to use in uniweb you can give your reputation and receive your patient and also prove your reputation and one of the universe application is the universe social and it is an anonymous social media and why we use the reputation in the anonymous social media because in in decentralized uh social media we it is hard to um uh it is easily to spend or you can say something irresponsible so with the reputation system you can you can know uh and we can avoid people to say something really bad or we can prevent spend and in this social media if user has uh not enough reputation they cannot post or comment in this social media and I will demo what is in the universe social so you can log in with your old ass like Twitter or GitHub account and then the universe social will offset to see if the account has logged in before or no and then the most important part is that actually we are not signing with Twitter or GitHub account we use the semaphone identity so uh the user has to keep this symbol for identity and we will just use use this symbol for identity in the whole unit wrap social system and then after you deciding you can create a post in this social media and the interesting thing is that um now in this system we don't use the we use the uh it's called subsidy to let users to post and in each airport user has three 30 subsidy and each post costs five five reputation so if after this post you will uh you will minus five reputation oh and also uh to create the post you have to prove that you're signing the uni rep system and also sorry and also you you have to prove that you own the Persona here yeah because in the unirad protocol uh the users will receive reputation by this personas and in this version of universe social we will have reached text editor so you can use markdown to uh to post and also include images okay when you finish your post and you can create a zkp proof to prove that you own the subsidy and also you deciding in this system okay this generating the zkp proof and it will send it to the blockchain yeah after the it is uh the transaction was sent and you can look at the uh like is a scan to see your transaction yeah and uh when the post is published and other users can boost or squash this post so that is why what our that is where the repetition system used because if you see this post is really bad and you get swatched it and you the author of the post will receive this bad reputation and if you receive too much bad reputation and then you might not able to post in the next few epochs yeah and besides post you can also comment in this social media and also comment like you can see here's now there's 25 subsidy here and common also caused uh also requires users to generate a dkp proof to prove you signing in this system and also on this persona yeah so it's pretty similar and then you create a they can be proof and with it on chain and then uh in this universe social system we have a user information User info page so you can see what you post and also what do you have comment before um okay so and I want to also introduce that we are some upcoming features uh the first one is the edit post and common features as you can see uh we we post and comment with the c proof but how can we edit the previous post so uh in this edit function we will generate another uh uh like Persona proof to prove that you are the the author of The Post so you can use the dkp proof to prove you are the author and then you can [Music] um uh submit another transaction to to this post to edit this post and another feature is that we can set the username with this universe protocol uh so what do you how they're using it username work is that a user will query a username request and then the universe social smart contract will give you a reputation that includes this username so in the next in the next episode when you receive this reputation and then you can prove that I own this username so not only the positive reputation and negative reputation and also a lot of things can be proved in uh with the unirp protocol yeah so if you want to learn more about unirad we will have a workshop tomorrow at 10 30. in a ZK Community Hub and also you can follow our Twitter account and join the our Discord server yeah okay thank you thank you Vivian um yeah we can take like uh one question hi thank you for the presentation can you please explain what happens on the when you do the GitHub authorization are you extracting any information from that um sorry so like uh I saw like doing the login there's like a GitHub pop-up for like I guess some oauth stuff what's happening there um we use the us only uh without any information just if the user has signing before or not but yeah for no more yeah operations okay okay thank you thank you so next up we have tsukino uh with skitter um a zero knowledge Anonymous social media application [Music] [Music] foreign [Music] [Music] foreign [Music] hear me hi everyone my name is sukino I am the maker of skidder um working with the PSE team and today I'm going to give a little demo of skater and before I go into them I want to talk a little bit about why I make that um I make skitter so we have a bunch of like social media application these days um what's the problem with that why do we need another one um so I think most people know that these platforms use the data against us they use our data to optimize fees optimize advertisements and kind of like course or you know whatever we want to use but they make us pay more attention to the stuff they want us to pay attention to based on you know how to use the data the problem with that is like if you have a small bias toward a worldview they will actually create an echo chamber to kind of make that small bias and turn that into like something more much more stubborn much more difficult to change um like I think whole opinion by itself is already kind of divided without a slippery slope like this but with all these um optimizations it kind of turned our attention or it kind of make our opinion even wider between the two side of the Spectrum which is something that I think is problematic the feeling that I get when I make a posting on like Twitter or Facebook or you know centralized social media these days like jumping into a ocean full of sharks basically I don't know when I'm gonna get bitten I don't know if they like me I don't know if they like something that I said in the past but it feels like at any point they can like attack me cancel me and if I got canceled I'm basically that socially like I lose my Social reputation I lose my friends it's a problem um here's one of my favorite group man is at least himself and he talks in his own person give him a mess and he will tell you all the truth um which is why I am making skitter as a Anonymous Twitter basically I want to build features that is for any tour Anonymous users allow them to say whatever they want while still having some kind of reputations that way we can identify like what kind of pose is more highly reputable more credible what kind of posts are like trolls and should post so instead of doing a live demo I take screenshot instead because internet is pretty bad but the website is live and I highly encourage everyone to try it out um for this use case I'm demoing how to create a Anonymous users using your wallet so in scheduler you basically can create one of two different identity um you can either create a wallet address identity which is pretty self-explanatory for anonymous users um here's what you do so first of all you connect to your Twitter account and once you connect to your Twitter account you will be assigned one of the few groups based on your reputations um this is in this integration with interware basically allow us to have four tiers and read it I think bronze silver and gold and after that we'll create a semaphore identity um and in order to do that instead of doing a random identity we are deriving it from a sign Matrix through your wallet that way if you lose it you can always connect your wallet and we'll be able to recover your identity so after that you are you know you're done you are you are now able to make different posts um using the enormous identity that you created right here we we can see a couple of different identity already being used there's a tier for Twitter users with over 2000 followers I think that's a isk and sorry for attacking you um we also have a Tac member so if you visit our temporary Anonymous Zone later on today you will be able to join as a THC member and you can make posts without you know showing who you are um and then there's another Twitter tier right there um and next I'm going to next I'm going to demo and other features um so what I just demo is the ability to create groups based on your Twitter reputations um this one is more for users who want to create private groups so here I have a account it's called PBX fan club any Rick and Morty friends would know Mr Poopy butthole has a lot of fans a lot of families um you can basically add your profile make that a group profile and after you make the group profile you can send invitations to different members um and now that I switched to my account I can see that I receive an invitations from Mr Blue people who and after I accept the invitations when I try to make a post I will actually have the options to insert a post at sukino I can choose to post as a member of PVH fan club and people can you know create their own group experiment with you know any kind of things that they want to do and the next features I want to show is CK thread which I think is really cool so basically I kind of built like a regular chat message is entering encrypted um that you can use to talk to anyone in the network the difference is when you initiate a chat with someone say right now I'm trying to talk to sequino as you got me liked I can choose to talk to him as Yagami which you know he will see my message and my profile and my name or I can actually talk to him secretly um he wouldn't know who I am but um I can choose to reveal to him in this chat or not but this kind of give us an idea of like how these reputation different kind of identity can work with each other um and as more and more groups get created you can think of use case such as you have a group for every dial member who has make a vote on certain doubt and then you can speak as a member of the Dao and you can use that identity reach out to someone and that would allow you to again give a person a master would be more likely to tell you the truth and they will be able to give more honest opinion about your protocol your ideas your worldview or whatever um that's kind of it for the demo we have a lot of like new ideas new features coming up feel free to come to me uh come visit me at um the temporary and Amazon I will be there the rest of the day and tomorrow and here's my profile and scooter feel free to check it out again the internet is pretty bad but please check it out at home um yeah thank you thank you um we do have a bit of time for questions I see a couple hands up hello um stupid question why don't you just be anonymous on Twitter um so if you're anonymous on Twitter how do I know um like so when so I do have enormous account on Twitter when I create the account the ability that I lost is all the followers that I used to have I either have to ask for some of my friends to follow me again which I will talk to myself um or I have to like use my real life account to kind of retweet it which again products myself um but with this we can bring in Twitter reputations we can bring in GitHub applications we can bring in the rotation from other places add it to your account without turning back to who you are and now when you create a non account you will have higher visibility you have higher credibility without having to start from zero and sorry my team so I'm going to add also you can choose when you post how much you reveal about your identity within skitter so you can be totally Anonymous when you post or you can sort of Reveal Your scheduler identity so you have a lot more flexibility which I think is really cool um over here foreign optimization of your feed at all I'm just trying to think about if I got a feed that was highly rated stuff but absolutely random it might be hard to use um so you mean if there's any post ranking currently in in the fees like if somebody posted content that would be divisive like to a small subset of people would be appropriate and they'd want to see it but to everybody else they're like why is this being sent to me I'm just trying to understand like how users are served things that they want so right now I'm basically serving things if with a very simple algorithm I'm ranking it um chronologically and I'm assigning them reputation score and that's how I rank each post um like moderation is a big problem that I have not solved yet right now I'm basically trying to make sure the like as much content and data is shown to your users without hiding stuff but the way that I imagine that would happen is you would follow key opinion leaders in the space and in your home fees you'll only see um the content from the people that you that you follow okay thank you uh hi hi I'm not sure if if you touched on that but what's how do you verify that the Twitter account is actually yours and what's out the risk of somebody just renting the Twitter account with 5000 users for people who just want to claim something well that is definitely a problem um Elon Musk their person who just bought Twitter couldn't really solve their problem so for me like it's not really a problem that I'm trying to solve um as in if people bought an account of their friends there's no way for me to verify it um Twitter can verify it and they might shut down their account and then that's how I would like change the reputation I would probably backwards slash that person that way that reputation doesn't exist anymore but that's kind of the problem when you use real life or when you do centralized reputations you're forced to trust the reputation um with deck is provided by the Twitter API versus on skidder their all the data is actually on an open Data Network that you can carry and you can verify um so in a sense like I'm trusting Twitter to provide me with real accounts and real data and maybe some of the account may be fake but that's something that like they have to fix and once they fix it I will move that web application from their users did I see one more over here no I was dreaming okay thank you all right thank you so much visit me at the temporary Anonymous Zone yes please do we have really cool t-shirts welcome Vivek um Vivek is going to be presenting hey Anon um which is another anonymous social media situation but it's happening on actual Twitter and it's also happening right now there's a Devcon group if you visit the temporary Anonymous Zone um where you can post as just somebody at Devcon [Music] thank you hello hi hi everyone I'm Vivek I go by Viv Boop on like Social Media stuff I'm here presenting Persona on behalf of or hey Anna on behalf of persona in Xerox Park uh before I go into what hand on actually is so I want to talk about a little motivation So currently online We have basically two ways of presenting ourselves you can use your entire real-life identity like me I'm Vivek I'm from Boston I grew up there I work at zerox park now and you know this includes also like your social graph your hometown like all the random tweets you said and then the other basic option is to be fully pseudonymous it's an account that's unlinked from everything else and if you bring in a reputation from your real life identity into your pseudonymous account it can it it can be really easy to sort of dox you and so what hand on is is attempting to be an open source app to allow users to attach a subset of their identity and reputation to speech online so they can choose selectively what they actually want to kind of attach with everything they say it uses zero knowledge proofs to prove membership in both off-chain and on-chain groups without revealing your exact identity uh right now we have deployed for ecdsa groups which just includes like various groups on ethereum like nft owners or get coin donors as well some semaphore groups um including like the task group from PSC and we also had a group at SBC also using semaphore um for all these apps like the proof is completely generated client-side it's stored on ipps ipfs and then anywhere where this like message is posted we also attach like um dislike proof and also like a little panel to like verify the proof as well how do you use it so here at Devcon um as PSE folks mentioned like you can just go to the Tas booth and pick up a little card and if you navigate to the hanon section on the experience Pages you'll be able to post a tweet to this Devcon in on feed continue to use this account you still have access to it and you can still reply as an attendee of Devcon uh to other people's tweets or to just say whatever you want and we're going to add more sort of General ethereum groups that more people can post to very soon in addition we've recently shipped a version of an admin panel where people can suggest their own groups this can be stuff that's actually like you know nft owners or like some sort of like new actually existing thing or you can even just make a group with your friends and just post on behalf of like your little friend group if you want uh there's a V1 at this link and we're gonna sort of like clean it up and then add it to the hand on website very soon another sort of goal of of hang on is that just overall like zika terminology is super technical and in my opinion very boring every developer in ZK will tell you that what they're doing just like feels magical like it doesn't make sense that you can get verifiability succinctness and privacy all in one tool but it all sort of just works and I think using terms like proving verifying zero knowledge like all this stuff sort of like magical this is to like an end user and I think like to really convince people how big of a paradigm shift we're making from like you know centralized servers owning all of your data to you owning all of your data and then proving stuff about that we're going to need some like new vocabulary new product new branding work so heavily inspired by conversations with Justin who's actually here we've attempted to use magical terminology for all parts of the ZK stack so instead of a symbol for identity this is your wand with your wand you cast spells and instead of proving like downloading a proven key you use magical equipment to make all this work and so we'll get to see this in action so um hopefully the internet doesn't ruin this but I will demo you good um demo tasks or demo the task group from my phone so this is a Tas website if you navigate down to here to Hannah you can just click on this and it'll take you to the page I've pre-loaded it because I don't want to wait for the internet and so there's a few options so first off you can view the feed you can post or you can apply I'll just go through each of them oops um well now uh okay so here's the feed this is the this is the feed of all the different posts that people have made not a lot of like interesting stuff just random people saying random stuff but if you join you can add some spice to the feed and then if we go back to the main app we can now post anything we want so I'm going to say you should presenting next I'm using this no one can verify this so I'm just going to say it um and then you know it'll generate the proof this is all with semaphore which is all fast enough that it can be around your phone um nice so here if you actually want you can look at the Olympic curve points that make up your proof for most people that doesn't mean anything so we can submit and then boom there it is uh here it is so here's the here's the sort of like uh tweet itself and then if you click on this link it'll take you to a page where you can let's see if this is going to work nice or you can verify this proof in browser I'm just using like the semaphore V key okay so let's go back so vitalik he posted this funny little funny little little tweet about um Bogota and so I'm gonna reply to it as this as a spot actually so if we go back to this um I can just paste in the Tweet link that I want to reply to funny pun Mr ethereum Man I can't type okay and then again it'll do its little thing come on buddy what's going on let's try that again um it might just be some internet thing or like nice um cool we can submit it and then nice I've now replied to vitalik as an anonymous Devcon attendee so yeah that's sort of how this works um and so yeah I just want to go through like a lot of our sort of future directions with this right now everything is very Twitter based but as I said in the introduction like this is really just a mechanism to attach reputation to your speech online and so we want to basically have like um first off like a lot more different groups not just ethereum and some of four groups uh ZK email which is going to be presented next by ayushu is not here uh is gonna give us access to a lot of interesting groups one thing is just like organizations like you can verify that like this person owns like a twitter.com email or like a facebook.com email you can prove you're a top fan of an artist by looking at some Spotify wrapped emails you can prove that you own some amount of stocks with the Robin Hood email so email really opens the gates up to like a bunch of really interesting Twitter groups in addition like GitHub users like if you go to your like username and then type dot Keys it'll store all your public like RSA keys or EDSA keys that you use to do SSH and so from there we can make GitHub groups and then sort of as Jason was getting to I don't know if he's here uh we can also have groups that are like gated with ML models so we can do some sort of like primitive like kyc or like face ID you can even imagine like some sort of thing where it's like if there's some treasure hunt of some item and like you find the item you can take a picture of it and then you can post it some Twitter feed basically or kind of get a get a zkp of that uh in addition like we're working on just in general like improving the ecdsa proving process like right now it still takes like five minutes and can only be done on a computer but we have a lot of exciting work that's making that better uh there's one repo like under the Persona Labs GitHub that is working on basically doing a lot of the signature verification outside of the snark in a way that doesn't actually remove privacy in any way um in addition we're trying to move from circum to Halo 2 we're in the process of taking the Halo 2 ECC circuits from Axiom and getting them to work in browser and also we're interested in potentially trying to get some collaborative ZK snarks working which is where like and different improvers work together to produce a proof over like some shared like or secret shared witness and if one of those end users is actually the like you yourself then sort of privacy is preserved by like the one of n um NPC assumptions and for the product yeah as I was mentioning like uh this isn't supposed to just be Twitter um we want to put this everywhere so this is just going to take like just like writing a bunch of different like like just just posting mechanisms and then one thing I'm really excited about is like for for Reddit for example and also this this this works for like just like Discord and like telegram event is like you can really tune specific like groups to like specific like subreddits or like specific like group chats or channels and so it'll basically just like allow for sort of more fine-grained interesting groups whereas on Twitter it's sort of like this massive public feed and like you you know if you have too many groups of people it can get a bit overwhelming and then sort of asukino was getting at like moderation is really important for all this stuff uh every every sort of good use case of of you know being able to talk pseudonymously like for example if you're in an oppressive regime and you want to speak up against your government but not sort of you know have like get attacked like you can use something like this to say what you want but at the same time like uh let's say you're a very powerful person and and you're in like sort of an anonymous group with other powerful people you can just use this to say really really shitty things without any sort of um like sort of uh like negative feedback coming back to you because you know you're protected by this Veil anonymity so it's gonna be really important that uh there's some sort of moderation built in I think the the most like sort of standard way to do this is just like within your group you either have some admins that approve all your posts similar to like a Facebook group or even potentially just like um any any three people in the group need to approve some posts before it's posted publicly for other people um yeah help us out we uh we have so many projects and stuff to do we really want to release all this stuff it's just it's open source because we think that users should be able to like store their own parts of their identity without also you know Builders should be able to build on top of all this stuff and um there's also a lot of really cool ways to put this on the real world like uh chance from PSC had this really cool idea of basically like arming uh people are trying to unionize against like a you know some some big corporation like Amazon uh basically handing out little semaphore keys to different people so that they could post to a Twitter feed as a verified sort of Amazon employee but sort of not you know face any consequences for for saying what they want to say yeah thank you that's all and this is a link to our repo it's kind of messy because it's just been me hacking but uh we're gonna clean it up soon and uh we want to onboard more people so thank you I think our second mic died but we have a few minutes are there any questions I'll just I'll just run it back and forth better uh wait you haven't asked one yet we should have time for them can we do some kind of voting in the semaphore group to make sure that we can add more people for example if like 10 out of 10 will vote for create another like proof yes can we do voting inside that protocol because you can also like do voting outside the protocol but then what's the purpose of using the protocol um we don't have that built out but yeah that could be added um I don't see no reason why I can't cool thank you um we had one over here so if somebody posts something that causes that Twitter account to get banned uh is it possible to like make a new group without knowing who they are but that they're not in it um it depends I think so if you have nullifiers for every single address um then I think you can basically ban like the sort of like like bad nullifier from like the second group um the point you're making though about Twitter Banning groups is very accurate like Twitter Cadets at any point shut down this entire operation so that is part of the reason why building on these centralized platforms is is tough and why building stuff like zkitter is so important for the for this sort of stuff any other questions now I have two microphones if that's useful for anyone um okay oh one more have you considered tying this into LinkedIn because I think that's an environment where people really have to uphold like the professional identity right and not able to like share information that they actually would like to share with their colleagues or people in their Community Glassdoor is a good example but you cannot really verify if people work at some place like in the setting like that wait what was your question oh yeah yeah for sure I think um once we have like ZK email sort of working um it'll be really easy to sort of bootstrap these groups of like for example like employees of a company um Glasser and blind like works pretty well for this stuff but um again like you're trusting laughter and blind to not reveal this information I think there's some like lawsuit in New Zealand or something where like last year I got sued to like release some of their information so you know this stuff does break down at some level but um yeah we could we could just have a more sort of secure version of that for sure anyone else all right thank you thank you and and come on up um this is our last presentation for the day and this is going to be about ZK email [Applause] [Music] foreign [Music] we are going to start with a quick demo because the Wi-Fi here is not fast enough so we're running this remotely and so we're going to begin that right now yeah so we have a UI buildup it's pretty primitive but the idea is that it we know ZK is not efficient now so it's going to take a couple of minutes to run this proof so hopefully it works I don't know if this internet is gonna oh it did click nice so we're gonna let this proof run and then we'll come back to it at some point later and see if it's generated and we'll explain everything that we just did uh but yeah I'm some pretty I've been hacking with you like sparkle for the couple last couple of months on your knowledge proofs I'm ayush I'm also uh hacked with Xerox park for a while and now I help run Persona Labs uh yeah and I want to say like this project has a lot of people behind it like we've used a work from a lot of Open Source libraries a lot of people who've helped us and behind the scenes and these are some of the names who are also in the crowd but we can't mention all of them so thank you to everyone again thank you So today we're going to talk about trustless email verification on chain is trust us in the sense that we don't run our own server so we don't see any of your private data but at the same time we are trusting because this is an interoperative protocol between web 2 and web3 we are trusting mail servers and DNS to be operating as they should but we do not operate any of our own infrastructure so what exactly is proof of email or a ZK email well every time you receive an email you have to know that it came from the source that it claims to be coming from and so one might imagine a great way to do this is signatures so what they effectively do is every email you receive is RSA signed with a shot 256 of the from address the two address the subject and a body hash along with uh and this entire hash is RSA signed with the key belonging to the email mail server domain so you can imagine that if a user provides this header or the signature of an email then and we verified in zero knowledge that no one including the mail server or a keystroke tracker on someone's computer can verify that someone is using this protocol there's some small email quirks not everything works like this Gmail doesn't sign self emails and didn't even do this until 2016. uh Hotmail doesn't include a two field um and mail servers have slightly different bit keys so the specific demo that we have prepared today is trustless Twitter verification on chain the way it works is you get a Twitter reset password a Twitter reset email into your email from twitter.com we then create a ZK proof of this email we effectively reveal only the sender domain which is uh something like verified twitter.com the RSA modulus which would be from the DNS record of Twitter and the masked email body which only has in zero knowledge the specific keys that we want to release so like in this case it would be for example it says this email was meant for use G right here so we could kind of use that to extract that part and like use that as like the username the things we want to keep private are all the information that does not relate to what the user wants to release that is specifically the entire DCM signature would de-anonymize them because a mail server would know who they are the length of the pre-hash message data or general lens that we pass around to ensure that computation is efficient and all of the raw data so the raw message data the raw slender data the identities of everyone involved in the circuit what we do is we check that that decant signature that we showed you earlier verifies the shot hashes verify the RSA signature is verify and that all the text is well structured in the message and this requires a little bit of regex parsing finally in the smart contract we verify the things that are very either very expensive to do in a snark or have to be done on a decentralized layer so things like verifying the sender domain is actually the center domain that people trust and verifying the claimed RSA key is an RSA key that people Trust um let me go see if the demo is done so it's still going on so I'll just like talk about some of the innovations that we did so obviously one of the things he noticed was regex and we also had to do a bunch of Shadow 56 inside the proof now since we want this to run in the browser like obviously uh we don't want people to have to send their email to a private server that we own and then we run the proof so everything has to be done in the browser which means it has to be very efficient so we had to use some tricks to make sure like our proof runs within like a reasonable amount of time so one of the things was shout to 56 hashing so we have like this really long email body let's say like the one that Twitter sends us is like 16 kilobytes and if we try to just hash that entire thing in like inside a circuit then like it'll take like millions of constraints so what we realized was we could only take the part of the message body that contains the username we want uh which is for example the Twitter username which is at the end of the circuit and we can only do that part of the hash inside the circuit and we can hash the first like let's say 10 000 circuit in like and pass it as a private input now this doesn't really break any security guarantees since we still check we subtract that the the final hash is equal to whatever is like signed by the mail server and if you if you if you could break this then you would basically be calculating a pre-image for the hash so it's the same security properties but we don't have to do as many like computations the other one is obviously verifying regex so email headers and email protocol is like a very tricky thing um oh I didn't I didn't even see this so email is a very tricky thing where like it can be in many different orders and there's no specific structure so it's hard to verify that within the stock and like just using IF else checks and obviously the first thing that comes to mind is kind of like when you try to pass the email addresses to use the regex right so that's kind of the most simplest thing that would provide more security but there's no obvious way to do regex inside a snug in efficient way so the thing we used was there's a way to convert uh all regexes into deterministic finite automator so which is basically like a graph and it's kind of like automators graph and you Traverse the state so for example each each Edge has kind of like a letter with it so if the letter is H then you go from one to two and if there's an e after that and you go from two to three and obviously at any point if like a letter doesn't exist then you kind of fail and you just go back to the beginning so in this case if we get to the last state which is 12 then we accept um so it so this is obviously like a very simple regex for hello world but we can kind of build a more complicated regex which is for like checking your email header so for example checking that there's a two and a from and it uh is like valid according to the SMTP protocol um and we can kind of check that and make sure everything matches so and using this also we can kind of use rejects like uh what is it called catch groups or something like that to reveal what we want so in case we just want to reveal the domain which is in this case 0x but in in the demo we want to do we want to reveal the twitter.com so we'll see how that happens so the regex4 this thing is like a huge graph so obviously I'm not manually coding that in circum so we did some tricks to like automatically generate python circum code uh given like a graph and you can see in that slide on the right side oh yeah auto-generated circom code um it works I don't know how it works but it works um all right let's go back to the demo hopefully it's done oh no I have to type my password on screen all right maybe we don't do this I I still have a like kind of like a video showing the same thing um because trust me is this this is exactly what happens so after we go through all of this okay I'll type my password if you guys don't believe it okay uh but maybe at the end so anyways as you can see the proof got generated at the end and this proof is obviously something you can send to the smart contract and it also gives some some extra data which is like the public data that we're talking about so I'll just let this play here uh if so as you can see like we have all the extra data masked out and only the things we want to show so like for example twitter.com to gmail.com and then if we go down to the end we have the username which is uh uses username from the circuit so are those are the things that only the public input from the circuit and everything else remains private and since all of this was verified using a proof you can kind of uh if you submit this to a Smart contract you can verify that this was indeed actually something that happened so some precautions of this technology before we dive into specific use cases or that this enables a tragedy of the comments on institutional reputation now what this means is that using ZK email you can prove processly that you're part of some organization and post or act on behalf of the organization without spending your own personal reputation and as a result for the first time effectively ever you can spend institutional reputation and have no cost to yourself and no account personal accountability um is quite dangerous and so we do think it's important to have something like zika nullifiers or different mitigations to think about how to release this technology safely uh so that we don't enable this uh degenerate spiral one second uh thing to think about is deniability it can be argued that you want to be able to argue at some point in time that you you can deny that you sent an email and so the way that a lot of mail servers do this is every year or six months they'll rotate their mail server keys and release their past keys and some of them will even release the past secret Keys meaning that if you get an email more than a year old that someone could have easily forged it using that secret key that was released and so this poses problems both for the scheme but also is a good question of do we want to enable and encourage this or do we think that this is something that we should hold people accountable to for a long period of time and finally one interesting thing is that if we want a nullifier that is something that would avoid people from claiming again and again and again their identity on chain or possibly selling this identity even then we have to allow another fire which is derived from the information in the email but this would allow the sending mail server to de-anonymize someone and so there's a trade-off here where you can either have uniqueness and anonymity or you can have the mail server able to de-anonymize the people who are interacting with the protocol so I'm going to talk about a couple of different applications you can build that require no server you can build something like proof of personhood and one way you could do this is you can verify in zero knowledge that someone received an email from a kyc provider that checked their kyc but you don't reveal any of the information about their personal information or the kyc information and thus you can prove that someone with an individual human would not reveal who they are you can do something like have Edward Snowden release the NSA emails on chain and prove that they came from the NSA but not that he was Edward Snowden you can do something like proof of company where you prove that you work at a certain company or a certain domain and you mask out all the rest of the information enabling something like ZK Glassdoor ZK blind you can do oracles if oracles send emails to you with information say price feeds or any other information that you want on chain you can simply verify these email headers on chain and not have any centralized server that you trust or a decentralized networker verifies that you have to trust to accurately represent this data I can keep going on and on but you can imagine that any data that you can have in an email you can verify on chain using this primitive this is extremely powerful for decentralized and Anonymous applications I mean this is going to enable a whole new era uh of ideas for residents you can do by proving for instance you have a gas bill or a water bill um The oracle's Glassdoor and uh the number of anonymous groups you can create is also quite unlimited you can prove for instance groups of people who have 10 million Twitter followers or people you can prove that you made some call option on Robinhood that went really well or you can prove that you're you have some dollar bill bank account uh dollar bills in your Chase account because Chase actually sends your balance to you in plain text if you request it I just want to point out one thing is like right now the one thing we have to rely on is DNS so either we hardcore the DNS are the DNS Keys the keys in the DNS in uh in the contract where we get it from chain link now if only we had something called DNS SEC which had actually taken over because if we could have actually done all of this on in a contract because we could have just verified the certificate like to the root trust but unfortunately nobody ended up using DNS SEC so we can't do that so if you're interested in any of these Primitives we're working on them at Persona labs and we'd love if people reached out and were able to help us or were interested in talking more we think this is a very fascinating interesting space and is going to rapidly change how we think about decentralized identity over the next year or two if you want to see more updates about proof of email including it's not it's not labia but eventually eventually if you want to see more updates about proof of email including our website or open source code and experiments as we release them we recommend following us on proof of email and our front end where you can generate your own email and your own ZK proof from an email is live at zkemail.xyz the user flow is you have to go through these four steps but you'll notice that there is no sending to server involved and we had a great brainstorming session in the morning in which we got a ton of these ideas together so kudos to all the people came to the morning session helped us come up with the ideas that you saw earlier awesome thank you so much and we'll take any questions thank you [Applause] do we have any questions so so the the NSA example is cool because it's like oh Edward Snowden could have done this without revealing his identity but on the flip side a Russian spy was in the NSA could also just reveal all the nsa's information and never reveal who they are how are you guys thinking about how to prevent stuff like this from happening or what safeguards can we build yeah this is a great question and this kind of comes back to the tragedy of the comments point that we brought up I think part of this is that you have to enable some amount of accountability ideally in these situations and this can come in the form of say like upvotes and downvotes or if a certain if you have like say a nullifier associated with a certain account or a certain address and this account gets a certain number of downvotes then you can hide this from feeds that are being shown to users so although the data will forever be immutable Unchained from the perspective of users who might benefit from this or get harmed by this you can at least do your best to Shield them from it but part of it is that it's it might be very difficult to solve these problems and it requires a lot of thought to think about how to do this very carefully that's a great question though is everyone okay thank you I'm from unipas and finally somebody discovered the emails we are also using emails as a proof of ownership of the Smart Control wallet you know that and my question is how is the cost of the dkp on chain like you know you have to verify Isa signatures you have to verify the key proof so we are building the same solution so I wonder what's your benchmark now yeah um so right now since we're hardcoding the DNS uh Keys we don't actually have to verify any signatures on change so the only thing we verify is obviously the verification of the ZK proof which is a gross 16 proof and usually that takes about I think like 400 500k gas it's a constant time thing yeah 400k right yeah 400 yes including the rsa's verification uh which I receive verification you have to verify the tcam signature right it's RSA signature oh that's verified already inside the circle from the VP yeah it's a KP covered this right yeah that's that's only verified inside the circuit but why you have to put the tcam public auction uh because then anybody could just take any random like RSA key and just use that to sign their own header and so it's just like a dpki right availability uh purpose so what do you say dpk and centralized public infrastructure it means you just put the public key on chain to like others can verify yeah yeah exactly and the contract also the category 5 is a simple http request actually yeah so like uh well our front end just makes a DNS request to just check that but okay yeah but like on the smart contract we have to hard code it or we could use a chain link Oracle to fetch the DNS as well so I won't take too much time we can talk later yeah did I see one more hand up I'm good oh my God we're gonna finish exactly on time this is amazing okay thank you thanks so much to all of our presenters and to Xerox park for helping us put this together and for all of you for being here and there's another awesome session coming up in here in 15 minutes so thank you [Music] [Applause] [Music] [Applause] foreign [Music] [Applause] [Music] foreign [Music] foreign [Music] all right foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Applause] [Music] foreign [Music] [Music] just a note for everyone we're going to start at a 4 15 which is about 10 minutes so feel free to chill for the next 10 minutes that's like five minutes Warriors [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] thank you [Music] [Music] [Music] thank you foreign foreign [Music] [Applause] foreign [Music] [Music] [Music] thank you [Music] [Music] thank you [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] foreign [Applause] [Music] [Music] foreign [Music] everyone if you guys can please grab your seats we're going to start in the next minute or so [Music] um foreign [Music] [Music] everyone uh we're going to start the last SLS session of the day I know there's been some of you who have been with us since 10 in the morning so thank you for coming on out um and I will turn things over to Barry and gupsheep to take us through the intro of the security SLS temperature hello everybody thank you for coming to this um right so basically this talk this this session is about like CK security and if you were here for the last session you would have seen that there was all of these different ideas all of these different cool applications built on top of ZK and like a whole bunch of them are like privacy related and some of them are scaling related and like one important thing so there's a couple of things to notice that are different in this world than they are in the ZK world the first one is that like it's impossible to notice a security that it's impossible to tell the difference between a security bug and a and a failure in The Trusted setup or yeah yeah it's impossible to tell the difference between those two things in certain cases so it means that like security becomes really important because it's like the Integrity of the system and you like in a lot of cases you don't know if you got hacked or if the system is just or if people are just like saying things in a certain way uh so that's the first thing that I want to mention the second thing that I want to mention is that like another thing that's really important in our privacy applications is this idea of forward secrecy and like one way to think about this is that like when we're doing encryption we're really really careful that the things we encrypt can't be decrypted for like 20 years because encryption and decryption are important for a long time but with signatures we're not really so careful because like we can change the signature team scheme relatively quickly and like having a signature be broken is something that we can adapt to but like having an encryption scheme be broken is something that we can't really adapt to so quickly so that's the other thing to to keep in mind that like we have this extra requirement of forward secrecy that like we didn't have in a whole bunch of other domains um yeah so those are the two things I would like to to mention as we get started but we have a whole bunch of interesting things to talk about and Brian is going to mention some more about them yeah so um another thing that uh I think is kind of nice about the setup of this applied ZK SLS um the past six hours of presentations as well as the upcoming presentations that we have is like you know we've seen over the last six hours all sorts of just really crazy sort of pineless guys EK constructions and a lot of teams just having fun with like you know putting like a regex parser inside of a snark and putting a virtual machine inside of a snark and all sorts of you know crazy ridiculous things um but now I think you know it's time to get a little bit serious get a little bit responsible and think through uh for all of these applications what does it actually take to to get them to production securely which is why I'm really glad that we're we're ending today on this note of ZK security um the other thing that I want to mention about this session and about this sort of new domain of practice that's emerging around ZK security is that it's an excellent example of a pattern that we've seen multiple times now in the ZK space where ZK has these very natural intersections with various other you know fields of study or engineering domains or things like that so you know the activity in ZK security has really really picked up in the last six to nine months largely as a consequence of a lot of communities kind of coming together and coming to share knowledge with each other so I remember like I talked with Jordy you know a year ago I'm from polygon Hermes and we were talking about this idea of like formal verification for ZK systems and he was just like you know incredibly skeptical and he's like well you know I don't know that much about formal verification uh but like it feels to me like in auditing Hermes probably that would be something that's like completely you know Beyond possibility and then earlier this year we started meeting some various folks with formal verification expertise folks from verdice a student Franklin Wang and we just started having these conversations around like basically sharing each other's languages with each other so we would talk with formal verification Specialists to help them build a mental model of what's going on in zero knowledge we would talk with ZK Specialists and the formal verification folks would help to educate us on like what is the right kind of terminology or language for thinking about different levels of security guarantees for different systems and you know these conversations started without necessarily very much of an agenda around oh here's the specific project or here's the specific goals or guarantees we're going to get to it was more of an exploration of like let's come to a mutual understanding between Security Experts and ZK experts and lo and behold we've discovered all of these very promising avenues for verifying uh and helping to gain confidence in the security of these ZK systems so one thing that I just think this ZK security domain is a just really excellent example of is this idea of building intersections within ZK and for those who are here for the last couple hours of presentations um you may have seen like Jason Yee and Remco talk about ZK machine learning and that's kind of a new project that a lot of us are starting to think about let's bring in the domain experts from say machine learning the domain Specialists from zero knowledge or from applied cryptography and see what happens and usually it turns out that there's a very fruitful intersection just by virtue of how broad and how like nicely mathematically structured a lot of these ZK proving systems and tools are so with that I want to introduce the lineup of speakers that we have for today first we're going to be starting off with John Stephens from verdice who will be talking about finding bugs in ZK circuits an exploration of paradise tools next we're going to have two short kind of lightning talks the first by Kyle charbonnet and the second by Michael Chu and Jason Morton Kyle will be talking about a new Z a bug tracker that we're hoping can be kind of a community effort to help document common bugs in zero knowledge enabled systems Michael and Jason will be talking about benchmarks for verification so this idea of starting to coalesce a standard set of circuits across different proving Stacks with which we can test the accuracy and performance of different formal verification systems and you know potentially even other systems further into the future afterwards Lucas from ethereum Foundation formal verification team will be discussing solving polynomial equation systems over large Prime Fields we'll have a quick break and we'll end with a panel hosted by Barry with highchen from scroll uh Jordy from polygon and David from ethereum Foundation security team on auditing strategy for the ZK evm so with that I will turn it over to John thanks [Applause] all right it seems like we're set up so hi uh my name is John Stevens I am the CTO of verdice and also a co-founder so today I'm going to talk to you about some of the tools that we're building for finding bugs in these ZK circuits and so uh just a little bit about us so first of all you can see a bunch of faces there the co-founders are on top and then also in the bottom row there are some other varadise employees who are here at Devcon and so if you're interested in any of the topics that I talk about today please come and see me or any of the other people that you notice on this Slide the important thing is all of us come from Academia and we're very interested in building tools in order to automatically verify and find bugs in in systems and so what is Paradise doing so we are providing state-of-the-art formal method Solutions so tools uh to find bugs in all layers of the blockchain ecosystem so that means that we're providing tools for smart contracts for zero knowledge proofs and also for blockchains in general and we use these tools right now to Aid in our our smart contracts ZK and blockchain audits but we're also making these tools available to the public so what I'm going to start with is what is formal methods so formal methods is generally just a set of techniques to find bugs and to construct proofs about software and so I've broken formal methods down into a couple of different categories here and the important part is basically as you go to the left or for you to the right uh you're going to get stronger guarantees about um you know the the software in general but generally they also take more or larger amounts of effort both on the machine side and also on The Human Side so uh all the way on the right and the easiest method is automated testing so this is basically uh you know just writing tests or using another technique called fuzzing and so generally what you do is you run a bunch of tests on a set of inputs and hopefully you find bugs however if you don't find bugs that doesn't mean that your program is safe it just means that your fuzzer or your test to not find anything so the next technique that we have is static analysis so static analysis is going to analyze source code for a specific class of bugs so basically it's going to look through your source code and understand the semantics in order to determine whether a specific bug exists in your program and so this is used for you know Finding bugs but also the important thing here is if a bug isn't found then it's proven that it does not exist in your program so the last case that we have over all the way on the left is formal verification so formal verification is going to analyze your software with respect to a specification and the reason why it's going to do this is it's going to try to prove equivalence between your specification and the software that you've written and we do this because one if we're able to verify that the specification is correct or the specification holds then we've proven that your software behaves in a way that's consistent with the specification so hopefully that means it's safe also if there is a violation that violation generally will correspond to some bug which you know should be fixed so I'm going to talk about a number of tools today and most of them are going to be at a fairly high level I've talked about a couple of them in the past but basically the way that I'm going to structure this is I'm going to group our tools into these various categories so we have interactive Theory improvers which provide the highest assurances but they are not automated or not very well automated then we're going to look at some automated or automated verifiers so these produce uh high assurances but normally they can't verify you know as complex a property as an interactive theorem prover which is why they're not as high on the Assurance axis but they provide a high degree of automation and we're also going to look at static analyzers and fuzzers both of which provide um lower assurances but a high degree of automation so the first thing that we're going to look at is a static analyzer called picus some of you might have heard me talk about this in the past so I will go over this fairly quickly so basically what Pikes is going to look for is it's going to look for uniqueness bugs in ZK circuits so basically what you can see right here is in that red circle you end up having some weird math so I'm just gonna do that and so what this is saying is it's saying that it's going to place a constraint in your ZK circuit and it's going to say at the eighth bit of out or the eighth bit of out multiplied by imp minus I is equal to zero and the purpose of this template right here is to zero out all of the bits except for whatever the Imp input signal specifies and so the reason why we have under an under constraint bug is for that formula to be verified or that formula can be satisfied with two conditions either out of I is equal to zero or amplify is equal to zero and so in the case where imp and I are equal to zero then the output signal is unconstrained and that's because you're going to be multiplying out of M by zero which means it's naturally going to hold and so in this case we have an under constraint bug because it allows the user to specify whether or not that bit is going to be 0 or 1. and so in this case that might not be harmful it depends on how the circuit is going to be used but it leaves an attacker an opportunity in order to you know specify a value which might end up causing issues for the protocol and so that's what picus is supposed to do it's going to identify these uniqueness bugs or these potential uniqueness bugs and so basically the way that it's going to do that or there are a couple of ways that you can do this so one of them is what acne does which is it performs a static analysis of the constraints and so the way that it does this is it's going to apply a set of predefined rules and so in this case the predefined rules are if the output is a linear combination of the inputs then the output should be constrained however this doesn't always work it can lead to false positives and so in that case we might want to use something like an smt solver so an smt solver basically is just going to it takes in a mathematical equation and it will say yes this mathematical equation will hold or no it doesn't and so you can actually encode these uniqueness bugs as a mathematical query which you can send to an S and T solver and so uh this is supposedly the query um but the problem is that if you use an smt solver it might be precise so it can prove whether or not a signal is constrained or under constrained however it's not going to scale very well and the reason why that is is because you're going to have thousands of constraints and an smt solver is going to start struggling because it can only deal with you know formulas of such a size and also smt solvers currently as we'll hear about later don't scale very well to large Prime fields on the other hand the static analysis that we looked at previously is scalable but as I mentioned previously it also has false positives so what we've done is we've created a tool called pikis and so what pikis is going to do is it's going to take in a formula or a set of constraints and it's going to Output one of a couple of things so it can output yes this is constrained no it's under constrained or it can just say I don't know and the important part is the way that Pikes does this is it tries to combine the strengths of static analysis in smt and in particular it does this by you know basically iterate iteratively performing a static analysis and an smt phase so it will basically you can consider it as running acne and that will give it a list of constrained signals and then for the set of signals that it maybe couldn't prove were constrained or under constrained it will then invoke an smt solver in order to try to progress further and then it can use those signals to further and form the static analysis and hopefully allow that static analysis to advance and so uh in with the new version of Pikes that we've developed we've found that on a set of benchmarks that we grabbed from circom lib it was able to correctly solve 98 of the benchmarks while just a generic static analysis and smt solver we're able to solve about 75 percent so the static analysis specifically only solved about 75 because it had some false positives and the smt solver timed out on all of the benchmarks that it didn't solve so that's Ficus it's a static analyzer for finding uh uniqueness bugs next we're going to look at an automated verifier or not a and the verifier is going to specifically verify specifications over Cairo programs so basically this is a tool called Magi Magi is going to take in source code and specification and then it's going to Output yes the specification holds or no it doesn't and the important part here is if the specification doesn't hold it's going to give you a concrete counter example saying why that specification doesn't hold and so in this case it's going to provide you an assignment of the variables and if you end up making that into a test case you can end up seeing exactly why or you can develop that into a test case to show why the specification doesn't hold so the way it does this is by using a technique called symbolic execution and so the way symbolic execution works is it is going to essentially represent your the state of your contract in this case as being symbolic and so what I mean by that is rather than having each value mapped to a single concrete value like X is equal to one instead we're going to map values to symbolic values and so a symbolic value in this case can represent any potential value within you know the range of the type so in this case you can see a weird formula on top but the important part is in this case like U Can map to any potential value it's not constrained and so the reason why we do this is because now we can symbolically execute a particular function so the way that this works is we'll just you know execute things the way that you would expect and we're going to whenever like there's a conditional we're going to take both branches in order to explore the entire execution and so here what you can see is if we take that if statement which corresponds to the if statement over there all of a sudden U is equal to 5. which is great but we also need to consider what happens if we take that else branch in that case U is not changed and so it stays symbolic and so if we continue to execute this eventually what we're going to get is a set of um we're going to get a set of formulas which we can then send to an smt solver and then we can use the smt solver in order to ensure that the specification holds so that's basically how symbolic execution works and that is how our tool called Magi basically works so I think at this point what would be best is to show a more concrete example of how Magi actually works in practice so what you can see here is a function that came from maker dials Cairo implementation and you can see a specification on the right so the move function in particular is going to move rad from source to destination so the destination or the the specification you see is that the die balance at the destination should increase or stay the same and the die balance of the source should decrease or stay the same and so if we end up providing this specification to Magi Magi is going to Output no it doesn't hold which might seem weird but it also provides a counter example and so with this counter example we can try to figure out why that specification doesn't hold and in this case it doesn't hold because of a very specific Cairo implementation problem so basically in Cairo because of large Primes your large Prime might not or isn't the same size as a uit 256 so instead they have to split the uint-256 into two halves a lower half and an upper half however they also require the user to go and check and make sure that un256 is properly constrained and so in this case you can see that the counter example specifies that both the lower half and the upper half are 2 to the 129 and just as a spoiler alert the max should be 2 to the 128. and so if we add in the proper constraint which is the the one in green then all of a sudden Magi will go and it will properly verify that move is implemented correctly so we've been using move for a while and testing it against maker dial and we have used it to find at least one bug in their implementation which has been committed into the Repository so next I'm going to talk about Coda and Coda is going to Mark kind of the the last project where um that we've been working on a lot previously and then I'm going to talk a little bit about the future work that verdice is interested in and so basically what an interactive theorem prover is going to do is it's going to prove the equivalence between a specification and a circuit and so the way that it does this is it requires you to translate the circuit and the specification into some into the language for the intermediate or the interactive theorem prover and so for Coda the interactive theorem prover that's being used as and so basically what you need to do is you translate your circuit you translate your specification and then you have to express a proof and which proves that the um which proves that the circuit and the specification are equivalent and so this equivalence proof has to be written and then all is going to do is it's going to say yes your your proof is valid or no it's not and when it says no it's not it will provide you some feedback so more concretely the way this works is you can see a circuit up on top and then you can see a specification so the circuit that you end up seeing is simply a circuit to check and see if the input signal is zero and so the specification you can see just says you know if x is zero then return one otherwise returns zero and then you can see kind of an informal proof of why this is cracked on the bottom and so the way that you would end up verifying this in is you uh you first you know translate the top then the second one and then you express your uh proof and  and that will hopefully return yes it's correct however that can be really difficult it can be difficult to express these proofs and so uh the purpose of Coda is to develop a set of tactics in order to help individuals prove that properties about circuits are correct and so the way that it does this is or the way that tactics can help is they can be created to essentially automatically apply a given set of rules in order to simplify the process of creating proofs so I didn't show what the original proof looked like in the previous slide I just provided an informal proof however I believe it takes like 20 lines of in order to prove it and so if you end up using a tactic you can end up expressing the proof and four lines and they are specifically those four lines and so interactive theorem provers are nice because they can verify very powerful or very complicated properties however as you can see they require a lot of manual effort because even with tactics sometimes the proof can take a lot more effort and so so far we've used Coda to verify the circon Bagent library and while doing so we found two bugs and just for those of you who are interested interactive Theory improving has been used by other have been used by other people to formally verify circuits so the Leo group ended up formally verifying that a set of rewrite rules that they were applying were safe okay so now I'm going to start talking about some of paradise's future work and so specifically first I'm going to talk about a fuzzer that we're creating and so the first thing that I'm going to do is establish what this fuzzer is going to be used for so we specifically want to fuzz implementations of ZK VMS and in order to do so we kind of have a simplified model of a zkevm and I'm going to use this in order to show how the fuzzer actually works so basically you have a transaction it gets submitted to an evm the evm creates a trace and that Trace goes into the zkevm and that will eventually perform the roll up which will go on to the L1 so the goal that we have is to try to find bugs and zkevm circuits and so the way that we're going to do that is we're going to essentially perform fuzzing in two steps so in the first step we're basically going to use a normal fuzzer and so this fuzzer is simply going to generate transactions and it's going to send it to a zkevm the Second Step which I'm going to talk about is going to be a lot more complicated because not only are we going to be fuzzing by generating random transactions but we're also going to be muting mutating traces and I'll tell you why we're going to do that in a second but first i'm going to talk about the whole system fuzzing approach the first approach because it'll be useful to discuss that and kind of show what the weaknesses are in order to kind of talk about why we want to go with the second approach so basically this first approach can be used to look for denial of service bugs in zkevms and so like I said you're going to have a fuzzer which is going to create random transactions and it's going to submit those transactions to an evm the evm will create traces and then those will go to the ZK VM and the zke VM will either basically pass and you know commit things successfully because you know if we have a valid transaction then it should end up verifying it or the other alternative and this is the interesting alternative with respect to the fuzzer is it's going to fail so it can either fail due to a crash or it could reject the trace and so this is what we're going to qualify as a bug and so the nice thing is this you know can possibly find bugs because we're going to use coverage information in order to ensure that we're generating interesting transactions but uh when it comes to the ZK circuits it's not very likely to generate a very interesting bug and the reason why is because we already know that the trace that comes from the evm is valid which means that we can only look for cases where a trace should be accepted but isn't and so what we would rather do is we look for bugs where either a trace shouldn't be accepted and is or is accepted and shouldn't be and so that's the goal of the second fuzzer that we're going to look at however first we've already implemented this fuzzer and it's found 11 denial of service bugs and scroll and those bugs have been found in a number of different circuits or they are possibly in circuits so they're just the files that they're located in and I believe the scroll team is currently looking into them so the next fuzzer that we're going to be talking about is the fuzzer to hopefully find deep logical bugs in in the zkevms and so the way that's going to work is like the previous fuzzer we're going to have we're going to be generating random transactions and then again where those trans or the evm is going to generate traces but now we're going to get to this thing called a trace mutator and so the trace mutator is going to mutate that Trace in a way that is either going to you know guarantee that it should still be accepted or more likely we'll guarantee that it should be rejected and so the important thing here is based off of the mutation we should know whether or not the at the very end after the zkevm executes that Trace that we generated should be accepted or rejected and so that means that now we can also find bugs where uh at the very end a trace is accepted when it shouldn't have been and the important part is here this is going to correspond to an under constrained circuit which is similar to what I talked about previously with pikis so in that case someone would have been able to submit a transaction that should not have been accepted by the zkevm but was which you know obviously could correspond to a very significant bug and so the fuzzer that I showed you previously has already been implemented this fuzzer is a future work that we are currently working on developing now okay so the last thing that I want to talk about today is a project called ZK Vanguard and so this is also future work and it's also a static analyzer but to give you a little bit of context what Vanguard is is a language and blockchain Gnostic static analyzer that we developed for smart contracts and so right now Vanguard can be used to you know take in a smart contract look at it and generate a report and that report will just say like you know X is vulnerable to a reagency access control flash loan Etc and so the way that it does this is uh it takes that smart contract and it translates it to lvm once in lvm we can run it through a number of detectors and these detectors importantly are intended to be easy to write because essentially what they're going to do is they're going to interact with analysis libraries and these analysis libraries can do a number of things so you can interact with smt solvers abstract interpreters taint analysis and so on however right now when we end up invoking Vanguard it only works for smart contracts because we only have a smart contract domain what I mean by that is the Smart contract domain allows us to ask interesting questions about the blockchain or the language that the contract was originally translated from so for example one way to think about this is if you are familiar with Solana it doesn't allow re-entrances whereas like solid or ethereum does and so the smart contract domain just provides you with Source information about the contract and about the the blockchain ecosystem that it's uh that it's intended to be run on and so what we're planning on doing with ZK Vanguard is we're going to introduce a ZK domain and so what the ZK domain is going to do is it's going to do something very similar to uh what we're doing with smart contracts and it's going to provide Source information about the language that you're writing your ZK circuit in so this could be circom Halo 2 or plunk and it's going to look for specific bugs similar to what we saw in picus So eventually we would like to wrap up a number of analyzes that are relevant to zkcon or to ZK circuits into Vanguard like the under constrained circuits that we saw earlier and the reason why we want to do this is because if we wrap it up into Vanguard then it should be language agnostic whereas right now picus is tied to circon and so the important thing here is that we can analyze ZK circuits but the other very interesting thing is because Vanguard will have will basically share have a shared interface for smart contracts and ZK circuits we can also look at the interactions between contracts and circuits and so that means that if there are bugs that we could detect where let's say someone is making a common error where they're using a circuit incorrectly that is also a detector that we could possibly integrate into Vanguard like I said this is a future work that we're well it's under development right now and we're very interested in it and so if any of you are interested in it please come and talk to me so yeah basically what I talked about today was a number of tools they're specifically there and they all have different amounts of Automation and assurances and um I yeah I guess at this point we can take questions and if you're interested please follow us on Twitter any any questions all right if not we will uh transition on oh for the static analysis thing to find like undercus trade under constrained bugs like that's um it's not gonna find all possible under constrained bugs right it's only gonna like this can be like can you like Miss a large amount of like under constraint bugs as well uh no because we're using an smt solver it should be able to find any under constrained bug so basically this is sound I see all right so it shouldn't have any uh false positives I think all of the automated uh verify yeah yeah uh that's a good question um this is something that we're encountering encountering right now with Magi so basically with when you have a project like Magi you now have to start you know making queries about these large Prime fields and current smt solvers don't scale very well when you end up making those queries and I believe there's going to be a talk about that later and so uh in terms of the size of the circuit right now they likely won't be very large however we're working on techniques and other people are working on techniques to make like an automated verifier better and more scalable thank you uh any other questions all right one more round of applause for John [Applause] foreign he had his sleeping from you this set of constraints support and welcome my favorite group that is information you can record this as well [Music] and for instance this is a problem is this unsubscribe it means and given the problem solver yes we're going to introduce a new variable negative details [Music] so now I have to solve that problem solar systems everything like the first man who died [Music] Albert and I found Institute polynomials to find an ideal that described absolutely bring those home variety and a lot of products that are having to test because then what they mean you can extract a solution from the problem very quickly it's more of a problem so five degrees with sovereign the star of the method is for solving polynomial systems is globular basis and it's not a complete solution as I will explain but if you take the input the input polynomials you can transform them into a another set of polynomials that is equivalent that can can be a group in a base has has have this property that is to Middle Terrace immediately tell us that it's not solvable it's it's unsat if the in the set of polynomials generated has the group name base contains the the unit polynomial polynomial that is value in this case we can already know that the system is not satisfiable and in the case that the governor base is calculated is not one then we still have some things to do in order to try to extract a solution from this and prove it it has a solution Governor base is very expensive theoretically it's X space hard but they say in the algebra computer algebra literature that often Works in practice that problem that I showed you before that you both said zitric couldn't solve was proven in 100 milliseconds using Governor base to be on Sat if the system is not on such at least immediately uh you can transform the GroupMe base that they're not only one there are many and the government base is dependent on the order of the monomials of the polynomials there isn't a particular order that's important for enumerating every solution that is the lexical graphical ordering team of polynomials in elimination order so you can eliminate the first polynomial you have one polynomial this is an example this is a original set of polynomial diseases the reduced government based in lexical graph coordinate you have the first is a universal polynomial that you can extract The Roots then you can replace in the next two polynomials to find why and then we're replacing the last polynomial and find X and these will enumerate you every solution for of the system the problem there are two problems here the first one is that translating your lexical graphical ordering is even harder than doing a group name based in magnesium or order the second one is that there may be Roots here that are not in the field so you may have roots that are an extension field and to find a solution that you want in the field you have to well try one by one until you eventually find one or exhaustively search all this Basin decides that not a single one is useful so this looks like an NP complete problem I have a colleague from the form of verification team that says NP complete is is fine another thing you can do after you have the grubner basis we can do this primary decomposition every idea is a unique intersection of okay if if we find a solution in any one of this primary ideals it is also a solution from the idea we are interested in so this is a way to decompose the problem and maybe handle it in parallel or maybe one of the the composite problems can be used we can use the first of your brachiometric method it's not a complete solution but it all it helps us break the problem into possibly simple problems and this doesn't work we I said that the problem with globular bases that it doesn't need middle so the problem is the roots outside of the field if we include these polynomials here x to the P minus X into the system we tell that X the variable X must be on the field so by including this polynomials for every variable we have restrict the the the the solutions to the field the problem is the degree of this polynomial p is 250 many bits and it will take forever to compute the government bases using this polynomial so this this polynomials are used directly many in many algorithms that deals with small Prime fields including a global base but it's not a solution for us another thing you can try in parallel is is if there are two main Solutions there are systems three is to break you do a local search and hope you find a solution but and that's not ex it will not prove on such and well you have to be lucky to find something with a local search method I'm not sure what local search method can be used but I believe one from that are used in such solvers can be adopted I'm not true this one is somewhat promising this model construction constructing satisfiability calculus is used on among linear arithmetic theory for smt and so it's a search SMG search algorithm based on conflict-driven learning and it works by assigned one variable at a time until you get to a point where the path you took is it is impossible you get to uh you cannot solve the the polynomials by using that just set of assignments of variables you you choose so you generate a new Clause called the conflict clause by using in this the case of this work Thomas hard to use use it group near base and elimination Theory to generate this Clause that hopefully will cut out one chunk of the search space for the next iteration of the algorithm so by doing this directly interactively you can eventually prove that the problem is on such it doesn't seem to me that the the government base is used here or the elimination theory is very good it's as good as the the mathematical stuff they use in the non-linear arithmetic to cut out a large space of the search so this may not be as good for for polynomials over Prime Fields then it is for reels and another limitation of this this work in particular is that it relies on the should polynomials so in order to make this work for Life Prime fields we have to figure out figure a way to deal with the roots outside of the the future and prune that up posteriority and not include the the field polynomials directly on the problem what is this okay yes I found this our working associative commutative Congress closure algorithm that is related to Kenneth Bendix completion which is related to government base somehow and there is this algorithm that can calculate this Congress closure algorithm quickly and maybe this can be used to replace government bases on some circumstances for for this problem but I I didn't dive into it very much yet but it sounds bronzing it's on my list okay so far what I learned what I've learned that the bane of large brand Foods is that deals field polynomials because they they have very high degree can be handled by most algorithms and the inside case we can hopefully handle with grubner base for most cases because if there is a conflict a direct conflict that that tells that the polynomials are are to have are not compatible between them now there is no common solution at all not even in the extension field then goblin-based View and so on start quickly quickly more or less uh otherwise we still I at least don't know how to filter out the non-field solutions well this this talk is based on this extended abstract we published in the smt workshop and the full textures over there if you're interested that's it [Applause] first so the government basis algorithm you said like it can find it finds one of many solutions no it it changes the how the problem is it changed the polynomials yeah but like one of the there can be many sets of from the bases right there can be many groups bases yes it's not unique but if it's a group name basis and it's in that particular order I mentioned the the lexical graphical it will give you all the solutions I see and if it's unsaid then it'll always what it finds it'll always have the one thing right yes you'll have a a constant okay polynomial and if it's simplified let me elaborate on this a little uh here I said reduced group name base if you put the reduced word you you put in a normal form that's unique the unit they reduced governance based foreign such is one okay and uh the algorithm for like lexicographic ordering that's MP complete like no no no that that is I think much worse than NP complete okay any other questions oh the front um can you give some intuition on why the Grogan basis method uh is going to do much worse for the SAT case rather than the unset case yeah it has it causes it it's immediate um the government is exhaustive Bears every polynomial and generates more and pair the new generative with every order so it if the polynomials are inherently incompatible let's say or there is no solution between all of them to eventually quickly in this N squared state of bearing everybody with everybody we'll find a constant polynomial then it can be and it's often the case it seems to me for these problems in low degrees in very in higher degrees if you start getting very high degrees you start getting more and more uh Solutions on the extension fields and then maybe it can't find them on such because every solution will have some variable no extension field for instance but as far as small degrees it will find the the the the contradiction by pairing every polynomial with every other the this group base is the original goal in our computer algebra is to decide it's a design decision procedure to tell if any polynomial is part of an ideal so it exhaustively creates a set of rules that you can use to test different polynomials part of that idea I didn't Define a deal but well hey thanks very could you give an intuition for why the the original problem you were trying to solve from Tebow could you could you give an intuition for why not how that works the constraint system yeah this one I think because they've if it was Leo's work they probably encoded the mod large B in the problem and the theory for linearity non-linear arithmetic in Z3 can't deal well with mod but maybe Paradise guys know something more about this all right uh one more round of applause for Lucas [Applause] so now we're going to take uh let's see four minute break uh we'll start again at 5 30 for the last event of the day which is gonna be a panel um and I will uh uh let's yeah let's let's do that take a four minute break [Applause] yeah yeah for sure no I don't know where's pleasure yeah yeah Barry David hi Chen David it's good to meet you yeah for sure oops I guess so we only have hey berries we only have three mics um same as this morning so maybe however you guys want to maybe pass around yeah Barry why don't you take this one and I'll let you introduce the panel itself sure yes so we have three we have two mics so the the topic of today's panel is the ZK AVM security um and how can we be convinced that the ckavm implementations that we're all working on are secure so today I'm joined by Hai Chen who works with scroll Tech and has been like leading engineering there about implementing this KVM I'm with Jordy Molina who's who's part of the polygon Hermes team and has been leading engineering there one thing that strikes me about like the organizations of polygon and scroll is that their their willingness to collaborate they've both been like big collaborators in in Xerox Park and I'm now exploring like how to secure the kvms together I'm also joined by David who's on the uh ethereum Foundation security team and has a traditional background in in security and and all that and things like that so I hope that today we can have a nice kind of exploration of how we're going to be able to make the ZK VMS that we've been working on secure so I that's my first question how can we how can we secure the ckvm how can we secure the ZK evm yeah so yeah hello everyone I'm hijin so I think so the ziki is a very complicated circuit so to actually make sure like the uh to make sure like the decayment is secure I would definitely need to go through like the all several auditings like having Auditors to look at look at the circuits and probably I think like there may be not even like one single team of Auditors maybe have multiple teams look at that again I can uh and then we can like find some bugs and then we can fix that and then after that I think we also should do like the like on Parallel can do some bug Bounty program which like have like the more community members who like the interesting to looking into this stuff and then finally they find in the Box I'll Attack those things uh that's nice thing like that to have this more secure uh and then last I think maybe a little bit like to say like have some more Advanced Techniques like the very nice people and other people like to describe have some more formal verification tools and Frameworks do like model checking those kind of stuff also like I think like initially maybe it's more scalable to a very large circuit but I think we can eventually get through there yeah that's like my maybe just to complement that and first of all uh open source you know that's probably the first step in in securing something uh cryptography by your first question or just you know probably cryptography this this has the humanity already has been tested that this doesn't work we have experience Microsoft Google IBM so big corporations that just pay a lot of money just uh building private cryptography systems we know that this is not the way to go we need as many A's as possible of all kinds to take a look at the at the code at the protocols and that's why it's the first so the first must this is not enough but the first is that this needs to be open source and the the people and especially the users have to have uh access to this code this is the the first step okay and from now on let's start as as in says is is is it's a complex system so the first thing is the first problem that we are challenging is that uh who can review that because it's something that's new it didn't exist before we are creating a lot of Primitives a lot of you know direct meditation all the uh because we're doing a lot of tricks and a lot of things and there is no experience on that side so the first thing is who's going to review that who's going to take a look who has the capacity uh to to to to look at that so first step and this is where we have started the we're partnering here together just to that is to uh to somehow uh start teaching start uh explaining uh straight into the Auditors explaining to the community explaining to anybody that's interested in the system this is the first so this is this is for sure it's the first step if because we need to open and the first the open even if it's open source if nobody can understand what's in the in that repo it's like being closed source so so we need this to to open and to explain to teach to spread how we are who we are working and and and and and Publishing that and this is the the the next step after that so when we have this let's say minimal uh um minimal you know minimal um critical mass of uh Auditors and and so on next step is okay then here is how we organize okay so we organize the the auditing and because we need to be here a little we need to talk about procedures and we need to talk about how we specifications how we write the good specifications how we can split these specifications and uh have them in the procedures how can we clean the code how can we uh go and this is this is going to be the problem the the next step and once we have this clear we have this specification phase quite clear that the things is quite clear then it's a matter of just taking and even that we will never have the warranty like any cryptographic system we will never have the warranty that the system is going to be 100 safe here uh we have the responsibility as projects to invest as much as possible in resources uh of this in the polygon side I'm sure that in of the other projects too they will not launch anything until they feel uh comfortable enough that the system is reasonably reasonably safe um David do you have any comments about this uh yeah uh I think so I think open source multiple audits um I think education is important the for me coming from like a traditional security research background um I think we have very strong software security research capabilities in the community I think that there's uh you know there's multiple components here there's like the L1 verifying contracts and as you've seen in previous talks you know three or four years ago we didn't really we just knew what it re-entrancy was because uh the big Dow hack now we have like 101 best practices for solidity coding so we have like these different components um pillars to stand on unfortunately with security uh it's kind of the weakest link issue here so um like we've got a great bug tracker for ZK stuff but I kind of where I see like a large gap is that we have a lot of academics um that come from like a formal verification or like a mathematical uh background and then we have these people that understand like implementation so it's great to one like multiple audits but we don't really have multiple firms that know how to audit so the education stuff is like critical here some of the tooling stuff is reusable with like uh formal verification but I think like over time it's going to be about this collaboration um I think that this is like critical I think you know you can look at some of these things and say like hey all these l2s are competing and I'm not sitting I'm you know I work for the EF and I care about systemic things so I'm not like on an l2s team here um but in the long run like you could say oh yeah it's a zero-sum game for these l2s but if something catastrophic happens to any of them I guarantee you it's a like a it's a positive sum in the negative direction for everyone nobody is going to trust billions of dollars on TBL here and they're all just going to go to optimistic Roll-Ups right so I think um this approach here I see polygon zke AVM folks in the crowd I see all these people working together and Xerox Park kind of like hosting this community where academics can come together I think that this is kind of the the only path forward because it's the only missing thing that we have here um that that we don't have some like tried and true testing methodology and so I don't think this is anything new for the blockchain uh Community the prereq requisites to understand smart contract security require you to understand decentralization things like oracles all these New Primitives and so we've had this already we've seen people tackle things where you have to not just be an expert in like one domain like software security you have to understand Economic Security and all these things or the whole system falls apart so I think this is just going to be an evolution and I think that there's like we're on the correct path uh forward thank you thank you David um so the ZK VM is a is a very complicated piece of software it has many components and those components interact with each other and my next question is like what components are you all specifically worried about what's what what keeps you up at night uh yeah I seen like this so there are multiple pieces it's like in the uh ZK even circus so it's not like uh so you know architecture is not only one single circuit so it's actually a set of circuits that connect with each other and then so work together to uh to suddenly like verify the ebm behaviors correctly that the trace is correct so I think like that first of all like the most important piece is like the the original the evm circuits which is the most like Central piece so it's kind of like the uh you model like the the evmr codes and then all the state transitions correctly so those kind of things like I think uh so I think like the people usually I can know how to do like that so uh integers like big integer multiplications and then our integer editions but like if you need to be true like that all of the considering is very true to what evm specs it does like they will assume yellow paper defines how event works there's a lot of like the corner cases if you look into the details so those are kind of cases they usually will be easily overlooked so I think that's like the one thing that's very important to do to check and then the second thing is that so that because the circuits need to work together they need to connect to some lookup tables uh to to be showing and then certain things like maybe checking one circuit and a certain property will be guaranteed by another circuit uh then how can we guarantee that this like the combination of the all the circuits is a sound and a complete the ones like to check everything that you have inside the even uh to fully guarantee or the Decay even is correct so I think like this is the kind of two biggest things like that I think it would be uh important to to audit the DK event so so like you're saying that like the two big issues are the corner cases and the interfaces between the different components yeah yeah nice okay yeah I agree I think that makes sense kind of maybe a different architecture uh on that so probably the pieces are different on that side here I would say Parts one is the what we call it the ROM or if you one is the the code that actually implements the evm itself there is a lot of lines of code and this is like it's like writing a smart contract you know it's it's at the end a single uh mistake in one of the lines can screw up everything and and this is well this is concerning this is because it's it's it's critical code and if you see that number of lines that are in there are a lot actually is you can imagine Geth but Brighton in assembly you know it's just it's it's a complex code there okay so this is probably my biggest concern at this point and then there is that in meditation in Orchestra limitation is is probably much more simple because at the end we have just a kind of a processor there so all the Logics is more in the wrong part but uh but it's not a lot it's a very new language it's a very new thing it's something that uh we don't have experience designing identity polynomial identities on that side so and it's very very easy to to to miss some something or don't take in account something and and so on so probably these are the two things that are more concerned but I cannot forget you know even the smart contract that I'm there and then more things that are more uh basic if you want but they need to be safe too right I find that there's this misunderstanding in the space where people equivocate like L want L1 Bridges with L1 to L2 Bridges and I I think that it's the problem is that they call it even Bridges you know I I would like to go back to the double pegging for for chain to chain and and this is maybe it's a bridge but it's a truthless breed it's it's just a smart contract just it's a mechanism to move in funds but it's not what we understand as a classical breach of you know just a kind of a multi-seek where you need to draw some some party yeah absolutely so to be clear L2 bridges are much more secure than L1 Bridges hopefully um yeah so I think I think like this kind of just like points to a lack of like true terminology everybody's marketing right now um so I would say like L2 Bridges I would call them fast Bridges where there's like a third party and they're taking the risk so if there's a double spend on one of these Bridges like the person that's getting the one percent uh fee for transferring your you know off of an optimistic rope and letting you not take the seven day wait period or between two ZK VMS or whatever like they they get a yield and they take the risk and they're the ones that's like you know out of luck if something happens um for me like I'm not sitting here thinking like what's the like worst doomsday scenario for my zkevm um I'm thinking like what happens to eth when something bad happens to zika evm and I I hate to say it but like something bad is gonna happen we've seen double spin bucks in Aztec we've seen them in um Z cash we've like these things are a big deal the one thing that's like the Saving Grace for ethereum here is that we have a native non-privacy like token standard at the base layer and the reason this is important is because you can see when there's insolvency in a contract whereas if you just had like the ZK uh like roll up and this is your entire ecosystem and everything is either private or it's using ZK to scale you might not know that somebody's printed Infinity tokens or that some of their wallet has negative tokens and that's like how your constraints add up and so for me it's like this lack of transparency here I think that there's some like traditional security um like philosophy that we can apply here you can do things like have like buckets and I understand like if you're trying to do something like scale this isn't that big of a deal if you're trying to do something like staying private having buckets for like your your like zero knowledge cash type transfers like you can think of like tornado cash that it reduces the anonymity set so like there are some issues here but there are also things that we're starting to see now um where you might have like a withdrawal like we do see these like chain to chain bridges that aren't really Bridges um they maybe can have like a uh like a Floodgate mechanism like if somebody's going to drain the entire contract that's probably not normal when there's a hundred thousand users that are using your L2 so like if 10 of the contract exits in a 24-hour period maybe having like these kill switches um will be good and then you can do things like have guarantees where if there is an issue everybody takes a 10 or a 20 haircut on their TBL and nobody's left holding like the complete Miss like like lost back I do think though that like you know if there's something systemic in like a like a multi-billion dollar uh zkevm gets basically drained and this is a problem then at least the base layer of ethereum is like still there and and we can still make those like trust and rebuild and so for me the like the scariest part here is the new technology I mean people were afraid they they people don't really understand like the zero spin problem until they see that like Bitcoin has gone up over time and like well yeah there's nothing behind it there's all this stuff but there's somebody willing to buy it from you right now for twenty thousand dollars right and so over time people like start to trust these things and I think of like if you guys have seen the Indiana Jones where he's like stepping on the like different pieces and like one of them falls through or like if you're crossing a creek you kind of want to like feel the rock that you're going to put all your weight on before you go I think that that's going to be the same thing here we're gonna have to like have people start to trust you know the cryptography that they don't understand yet like this is all Moon math to the average person right even like Advanced security Engineers don't understand any of this stuff and so there's this gap between the understanding of like you know this this like high level multi-dimensional Matrix math that you guys all understand and then like the people that like know what like a traditional software bug looks like and so that Gap right there is the is the scariest thing and the lack of transparency if something goes wrong is is also scary to me but you know you can mitigate some of that risk by bucketizing and these other things I mentioned I I think that like what resonates with me about what you were saying was how it's really nice to have this L this roll-up Centric roadmap so that we're able to experiment with these new things and we're able to build systems that with time can can get the kind of trust that we need cool so my next question is like so when I do all this what I normally do is to I try to find things that like people don't know or that the developer didn't know when they were writing the code so what are some of the things that people don't know about about like your code or things that could be interesting for people to pay attention to um yeah I think that's a good question it's like I'm not sure like that you hear audits what other stuff that's like the when they do like other things but I see like there'll be things like I think behind like the foreign for the previous system like whether those interface that you use is correct or like even behind like there's some repository like dependency you use like the um whether that's secure like I think that's uh one thing and then I think another piece that's like there's a lot of optimization tricks and then that could be make the circuits like less readable uh uh which like the I think Auditors may find like that's very uh hard to understand certain part of the logic of the circuits so I think that seems like we should explain uh those kind of things uh to the two Auditors um and I think in addition like there's some certain assumptions we make like in certain uh part of gadgets or part of the circus like you make certain assumptions and I think those things like a very important like maybe only the developers who develop those circuits knows about that but I think you need to those also seems like important to put down into specs so that the audits can know like the why you're doing certain things in a while like you're not doing sentences there are many things but uh one one that's important the concern is that from what we have in in the in the in the program it's a non-deterministic so it's not deterministic circuit it's not a missing means that first of all you want to do the division actually what you do is you put the result and then you check the the multiplication okay and tricks like that uh well there are many for example one when we are where we are scanning transactions actually what we are doing if we are hashing on the transactions but we are putting all the data all this data is we call it free input okay and then we are uh we have a constraint that calculates the hash some call and then the hash is the public that's actually the what needs to be matched okay but you can put anything in there it's okay because you have to Hash but this kind of um this is something that for a normal programmer for a normal um normal drummer something exchange it's something that's not used to it's something that's different and this needs to be explained very well we seem this we have seen for example internally in the team you know in the team maybe it's just the people uh that just learn things there's people that you know they come from uh other backgrounds and uh it's natural that uh what you first write these programs and you do a lot of mistakes because you don't have uh you're deleting absorb this uh concept of course you repeat that you do it again and and then the people is getting but it's something that needs to be explained very well because uh I think this could be one of the big sources of uh problems in there I I actually don't have any real good input for this question so I'll leave it there thank you um my next question is how can traditional tools help us when we're trying to secure a ckvm uh yeah let me just start with like one more simple things it's like setting everything to zero and CFL can pass that's like in a lot of cases so you can pass again which is not correct in the in your circuit so that's like one simple things and another thing like uh some basic scenes like a fuzzing so I think like the inner zero noise circuit I think the first thing could be like slightly different you know like buzzing arbitrary data that's putting inside your circuit with this that's it's very easy you don't like that the uh that like that it won't pass the proof because like it's there's lots of constraint checks so you need to be very careful to make some Corner cases to make sure like the there's some imbalancings that you put into the circuit but then can pass that but I think like in the context of the eke ebm is that you can generate arbitrary Trace like that you can you can fast the trace of the the from the evm and the C like that's uh if that can pass it I think like that's like the what Paradise was just like talking about like those fuzzing other people very helpful like to to use like generate some value trades and some invalid trades and then just and then you can tweak a little certain small amount of things like inside the trace to make things like the uh that should not be passed by if they pass the circuit then there's some bugs inside that yeah in general all these formal verification tools uh with maybe with some adoption with some adoption specifics but can be used at least to to to understand better some parts of the circuit maybe not like everything but there are specific pieces that are quite clear and that work very well or that can work very well uh with this tooling I'm thinking for example they are in Matic extent machine in our case this is very clear you know it's arithmetic you know this must be a multiplication and cannot do anything else this is something that for my purification people love to see this this clear patterns uh in there so I'm sure that some work can be done and there if you go to the main processor that's uh it's not that mathematically well defined maybe there they have more uh more problems and accuratives were free physiological make more sense but uh yeah definitively can can can help and here maybe it's a caller for people that's expert in some it's it's good to understand and to see how this this thing works because I'm sure that there is a lot of tooling and a lot of these things that can be very helpful and we are and we don't even know that those tools exist so here is it's important for the community to be productive here the Xerox Park people is doing a great job on that and there is people that's already looking at that and there are some ideas that things can be done in there uh that's important again it's just explaining because when you when you really understand the circles when you really see who has a disease you you can find people that's expert in that material that that can see ways to that can help a lot um I think so formal verification has been mentioned uh formal verification and it's like I can only do so much in like a regular security testing and so it just so happens that there's like so much math in these circuits here um that it just lends really well to this so I'm I'm really excited about that um first of all it's like the first time I think I've seen it applied in a way where I'm like well this is going to be a big part of my life I need to go read you know brush the dust off of all the textbooks another thing is that there's like a lot of there's like a lot of traditional things um with security audits that that actually lend well to to these also and like a good example would be like typing like strong typing is is serious and regular security you have integer underflows you have integer overflows you have casting errors things can flip negative and you see this in these circuits but you also see like um there's sometimes optimizations and developers do this stuff naturally um a good example is like the Aztec 2.0 uh maybe was not ever exploited but what was reported there was like an input that was 128 bits or 64 bits and the actual constraint was only 32 bits so you could actually provide multiple nullifiers if it passed all the constraints which means you could potentially like withdraw from Z cash type thing more than once even though you only deposited once and that is something that is queryable and you can just have a strong typing system so like the a correct static analysis tool that was that like could have could have been applied to that and just said hey error manual like review required this input over here is a larger is a different type than this like the way it's used and casted over here so anytime there's like a casting error you can have this I think there's going to be like plenty of other queryable examples like this where we can apply traditional security tools towards this cool um so where do we need to make new tools to like Lucas talked earlier about his exploration of trying to make a polynomial solver and like there seems like there's a lot of scope for us to explore like where would you like to see new tools yeah I think like the so a lot of tools like I mentioned I found traditional like formal verification they are now very ready I think for the uh proving that the uh the the ZK like the circuits so I think like those things like will be very good to have and then I think like not only like some tools like from the like for auditing I think that there's more tools will be uh very useful from the proof system side it's like they can provide you a very easy to use interface to uh generate some arbitrary like the uh error cases like you can inside the circuit and also have a very um bug reporting for example where does the circuits fail and those kind of bug reporting will be also very important tools a second auxiliary uh make the auditing and then make the testing like more easier and then easier to find some bugs yeah clearly these uh back reporting we launched the test net the public test net on on Monday we already got a report of uh there was nothing crazy but it was already a bug that we already fix it and that was because we published that and somebody just tested and just put something there so this is the importance of uh testing the things and I agree the the bug reporting is is when you see something that goes wrong you need to investigate until the last and fully understand what's going on because you probably you have something that can be getting worse um I think a formal specification for differential puzzing would be really cool um I think that there's like a value add when you could take like 10 different zkevms and you could potentially throw a test case at each of them and then you can compare the output States afterwards um I think the value out here is everybody wants to be part of it because you're going to know that you're conforming to the spec if you know if your output matches everyone else's this has been incredibly fruitful with our consensus Slayer clients in ethereum for the beacon chain so we have five clients there's certain things that are in the spec like processing an Epoch transition and so we could hand it Like Us form it's a it's a specified beacon block object and we can run it through every one of them and we can diff all the outputs afterwards and that's like incredibly valuable right like that's uncovered multiple bugs it would be like chain splitting bugs and these types of bugs like in the L2 world would mean that you'd have a state diff between the L1 and the L2 which means double spins it means all kinds of other like horrible things can happen another thing that I think is really important is uh like zoomed out from the zika ABM it's more like about L1 contracts in general and that's like a like an open security standard of like ahead of time hey if you're a white hat and you find a bug please report it to us obviously like we'll resolve it privately but there are some cases in the security world where like a vulnerability primitive drops and everybody hears about it at the exact same freaking time and like when that happens you've actually seen this like be fruitful where white hats will front run black hats they'll steal all the money out of the contract and then they'll return it like and you know maybe get a bounty or whatever and so having like a previous specified like hey on your bug Bounty page have a little thing down there that's like yo if if you guys drain a billion dollars a tvl from my contract this is how much the white hat Bounty is and uh we're like legally saying you're off the hook ahead of time if you return it all and you do all this kind of stuff and we are seeing some of this stuff kind of come out and flush out in like the D5 ecosystem there's gonna be like a spec that's being working with worked on right now with the open security standard or Alliance or something like that I think there will be more information about this like in a few months I think this is all just kind of like in the community right now but I think that's going to be huge because if you know ahead of time I know a lot of like white hats that they just don't want to touch stuff like this so there was like uh I think it was like The Nomad hack a bunch of people were able to just replace like the public key to receive funds and replay the attackers uh original like exploit and they were all like a bunch of white hats stepped up and started draining this contract and it was weird you could only drain like a little bit at a time so they were able to like actually recover a certain amount of the funds and you know it sounds weird to say like Hey we're going to give you a White House Bounty if you hack our contract and steel level all of our crap but this is like a phenomenon that keeps reappearing where everybody hears about the vulnerability at the same time and we do need people to drain these contracts at a time so like that's one example of something like that um thank you David yeah sorry to interrupt yeah um so my last next question is like um the harder one so when would you all feel comfortable to put all of it during the assets inside your ckvm yeah I said tricky one so I think like that so one launched the mainland I think that would be assumed like that's audited and that would be like very happy to do that for myself yeah we have the experience of Hermes one and other you know other real production life and yeah it's a hard decision uh General I I look at the Ace of the people that audit uh that and it's who audited that and I'm talking the specific guy I don't care about the the brand and it's just the guy and and and even the kind of uh questions that the auditor made uh how deep they went and uh the experience that this person have when you put all these pieces together so it's just but you need to know personally like each of the Auditors each of the internal and external Auditors and each person of the people you just check to all the places and uh here is just a feeling is is a feeling that okay this is uh this uh should be reasonable safe that's a feeling it's not uh I wouldn't it's difficult to put a parameter because what you say you say oh we need two of these we need three updates we need four persons three persons it's really is really hard because these persons now I see now this I pay a thousands that I'm sure that the guys that made the audit they didn't check the smart contract at all and I've seen that okay so it's not it's okay we have two Audits and and I'm not gonna I'm not I'm not gonna trust depending on the who made the audit okay so that's it's more personal thing it's personal people who made what what the questions they did what scan it where they went and this is when you you I would feel comfortable but it's a very personal decision it's a very uh uh feeling decision so it's it's hard [Music] uh to the answer all of ethereum's assets I say never um I think I think the ZK evm for L1 will be good for having like statelessness and being able to think a like client instantly without having to download all the trends and all the transaction history I think there's like value there but I think just like everything else here um time is the thing that like makes you trust it people trust uniswap because uniswap has a natural bug Bounty of billions of dollars sitting in it right now anyone can go after it so like time will tell the analogy of like kind of stepping and feeling the rock I think that's important but I think that like bounties are are a big deal and and these things naturally have bounties um open source is important you need lots of eyes but shell shock and Heartbleed were in the Linux kernel for like how long I mean every for 10 years every single like 75 of the computers connected to the internet had an arbitrary kernel read and nobody knew about it right somebody was probably using that the whole time so for me like you know this stuff is very new ecdsa tried and true like sure BLS like we we we took some like we can do this because we want to aggregate signature and the trade-off was important but we're not putting ZK in the consensus layer right now for a reason and this is like I mean I sat in a backyard with vitalik and and Dan Binet and you know this guy's been doing ZK stuff but since the 70s when nobody cared about it um and now we're heading to Golden Age of cryptography and we're implementing it and he was like I wouldn't I wouldn't put in the consensus layer so we're taking baby steps in the consensus layer ssle so that we can prevent um we can have privacy about who's proposing next so they can't DDOS like targeted DDOS are proposers and proof of stake that's like the only place we're taking a tiny little baby step a ZK um I think beyond that everything else is there's a reason you have like this there's no privacy and like like ZK scaling in the in the layer one and I talked a little bit about contract solvency I think that like my golden future for ethereum is that all nation states are issuing their cbdc's and they have their privacy they have their kyc all these things in their ZK roll up and then we can all say like yep the US government actually has something to back up their reserves right which is something that's a benefit on our current system if we didn't see the M1 and M2 money chart and if we even trust that we don't actually know how many dollars are out there because nothing's backed by gold anymore so I think there's a reason I think there's always a trade-off I think anytime you take a bridge whether it's ZK or it's an optimistic roll up you end up inheriting the risk of all of the pieces there whether it's the fraud proving or anything and I think that people will do things like like uh have their their large company or their large corporation or their large government having their financial reserves being completely visible on the L1 contract and then all the fun scalable private stuff we can do with ZK we do on top of that and and we take the risk trade-offs the same way that you have a bank account and you have the cash in your wallet thank you David um I think I only have time for one more question uh so my last question and hopefully we can go through this quickly is what's your favorite security bug uh yeah I think uh my very secure bug is like the I think previously for the Z cash they have like The Trusted setup like and then they put like in one extra additional points to there which like allow people to do like infinite like uh counterfeit some C cache tokens on top of that and then that was like wasn't fun for like many years until like every like found out that I've probably said that's a very epic One Security work in general or specific to the to the IBM that we are building in general I can I can tell you the internet the the internal one this is the Luna but it's it's I think it's a good example of this bag that we actually found but this is a good example that is okay we found it it's back but you know we couldn't phone for sure you know I tell you it's like in the sparse Mercury there is a condition some point at some point there is a condition that you need to check that when you are deleting that uh the key you need to check that the key that the keys is the same that that's not the same key that there is uh so there is a check that you need to do that um we didn't well the first implementation the check was not there and when I saw that is it's so easy to forget that check it's so easy that's when you realize how complex this is this is and this was fun by what was by the person that was documenting the was documenting the the this part of the storage but if he sees this what else can be in there and this is the this is the the the the the concern and and the landmarks um I have like a million favorite bucks so I'll say the ZK specific ones I I think that the uh the like lack of auditability because of the Privacy that ZK inherits here is just really cool uh I mean you see these L1 contracts get hacked in like Twitter's excellent at like broadcasting information and like this like distributed Network way where if you follow somebody like you can like infosec Twitter I know about bugs before potentially developers do because like it just gets spread so fast and you cannot see a D5 protocol get hacked now and not know in like 30 minutes if you're keyed into crypto and infosec Twitter um I think it's really cool like that a hacker could like basically steal all the funds slide out the back door and nobody would know for like years potentially because of the ZK I think that's just really cool but cool um thank you everyone thanks for taking the time thank you thank you for the day um so uh feel free to hang out or just exit um hatching algorithm okay yeah that works um and these all work that's in what do you want to do with the space so we're gonna make a panel here I need to connect my computer up there first so thank you it was still like an email engineer ing foreign [Music] so like let's see here foreign things like that okay questions [Music] [Music] find everything [Music] [Music] just [Music] [Music] okay [Music] um [Music] [Music] thank you everybody [Music] [Music] sacrifices [Music] [Music] I don't know exactly what it is just like everybody we can have an incredibly complex how do I sound close you can't hear me because they experience how do I sound how do I sell it hello testing testing how do I sound is this thing on does it sound like this is on testing testing that that works right okay test how does it sound sound good all right bud okay it's a recording place it's feasible it's feasible people are not going to remember to unmute themselves though the crew leaves and they just aren't going to do it because it's a depth con after after one so yeah there's like a there's a stand out there right yeah they're like they like make the beer in front of them so I did start a graduate degree yeah absolutely um certain function s function s and like a dehuman will literally just learn coverage it would be like in so the way the actions could be all of those like you can have an infinite permutations and actions so we could also have running the state transition it's simple enough you could literally have the entire memory spacing every video so you could you couldn't do that in like a native language allocations could be anywhere you know what I mean and like yeah let me tell you nobody has done this foreign games and it's like this thing can make Mario get to the end better than any unit can or anything and learn s for you to take a look at how much right you're guiding Mario it's the same it's yeah yeah yeah I've I've been to the sexual on Twitter and Telegram no it was my 20 projects um [Music] thank you David [Music] foreign foreign [Music] foreign okay right there testing tests is this kind of look for it this is kind of awkward I'm not gonna lie I don't know foreign right on requested this week foreign how do I sound from here how do I sound from here is there a feedback can I am I legible okay cool to see that foreign electron spaces or yeah join the spaces it's on the supercluth for doing that foreign developer relations really happy to be putting on this first web3 developer relations Gathering here at Devcon [Music] [Applause] so why are we here today Deborah it's kind of a nebulous field a lot of us are out here just feeling it through working our way through the darkness and we're really kind of being entrepreneurial so I thought it'd be great to gather us all here today to figure out what best practices might be and hopefully Come Away with some nuggets of knowledge on devrel to you know lift all our boats and improve devrel out of respective organizations so how are we going to do that today we're going to go through this agenda we're going to start off with a panel on dev content creation strategy we're then going to move into a workshop with Yaz from Celestial labs and then another panel on defining and measuring Devil's success we're then going to have just free-for-all networking and you know just share ideas and discuss and network from each other and get to cross-pollinate our ideas our our ideas so let's kick off the first panel first now it's going to be with Sam Albert Emily and I I'm gonna be I'm going to be moderating I'd like to invite you guys to the stage now let's get into it [Applause] so you can like share a microphone all right yeah Albert Albert we're gonna show this microphone buddy all right so Dev content creation it's critical for devrel it's important because it's how we teach and we that's how we you know promote our developer content and it's also a driving factor in the developer branding of our respective organizations and so today we're going to dive into what makes a strong developer content strategy with our three panelists here today so let's start with Emily on the far side and we're going to work our work our way through uh 30 second introductions on what we do yeah 30 second introduction uh I'm Emily I work for truffle as developer relations yeah that's less than 30 seconds that's good hi everyone I'm Albert I'm a developer Advocate at Alchemy Alchemy is a web 3 developer platform um and uh yeah I helped make a lot of content build community and get people to understand how to build in the web 3 space so thanks I'm Sam I do devex as super fluid which is very similar to what these wonderful people to my right do making content and uh building developer tools and all that good stuff awesome I'm Sunny for those of you who are filtering in when I introduce myself I also work with Sam I'm a close colleague of them and I also do developer relations at superfluid so to kick off with our first question I'm going to ask Sam what how do you reason with developer target market with the production of developer contents how do you target the people how do you segment your audience when you're creating developer content yeah so I think what's important is trying to get a clear picture of who you're talking to right so for engineers typically they have a difficult time explaining their work to to newcomers I talked about this in a talk I gave earlier today but I think it's really useful to understand what level at which you're trying to explain uh your protocol right so if you have a hackathon developer who's maybe like a beginner you can obviously tell they're a beginner maybe it's the first time they've built something in web3 you're going to want to tailor that piece of content to them right maybe you need to explain like what a what a wallet is right what a private key is how these these these models work in general and you can usually figure out who you're talking to if you're in person pretty quickly based on like the kind of questions they asked and like looks like a Blank Stare they might give you uh but when you're creating like a piece of YouTube content or even a tutorial or example you want to think through where they're going to be coming from and try to build up from there right and ideally you have examples that work at multiple levels that's what we try to do with super fluid and you know I actually like to hear what you guys do so what do you guys want to take um so the question is like finding finding the right people or like meeting people where they're at Right audience yeah yeah I think um something I I like a framework I've been trying to uh kind of iterate on um is thinking through the user Journey because I feel like every developer um you know who's entering the web 3 space especially because it's so new they're going to go through this journey of like oh what the heck is this new technology then oh now I know a little bit about it and I kind of want to like learn and do some practice stuff then ultimately I want to earn money right I need to make a living and I need to like be sustainable um not necessarily like cash grab or anything but it's just like I need to have a job where I need to like start a startup and so um you know to your point about understanding where people are at I think like for me I want to know for any piece of content that I create like which stage of the user Journey am I targeting and then I think that usually helps me kind of frame it a lot uh better A lot of the a lot of the content that I'm making is like learn right it's like um people are that are entering the space we need to grow The Tam you know and like just get people to to start building so it's like okay introductory let's introduce a wallet let's introduce what this API is um and then a small subset is going to be this like uh earning group where it's like okay you got to know what you're doing here and I'm going to talk about a very very specific problem that I know you have and it's actually preventing you from like up leveling in your job or like you know setting up collab line in your Discord and your community can't function like I know I'm going to solve that particular problem but you're going to need some background to like do this um so yeah that's kind of the framework I use um so just to clarify the question was how do you segment your audience or is maybe that a better way to reframe what everyone else has been saying I'm a little scared because I'm the last person in line I don't know if I have anything extra to add yeah please ask me I think second because I don't want to be the first but I don't want to be the last you know I'll be the the ugly middle child not sorry um okay yeah yeah so I think what you're saying a lot of meat where the people are at especially I think as a truffle developer advocate so I don't know if I introduced it we are the base like blockchain development environment the needs of a user for somebody who is just starting out in solidity is very different than someone who is building like a complex protocol right so when I'm creating content you know trying to communicate truffles value prop a lot of that is like who am I selling to like how do I describe Truffle in a way that I say hey it's actually really easy for you to onboard as a complete beginner versus wait I actually know that you are encountering like high level like I want to say like Advanced problems where does truffles tooling fall in terms of that so I think a lot of what I like to say is um my best the best example probably is you know console log really really like uh what do you call it important for a beginner Dev right I mean for those of you who are devs the first thing you do is just print right but once you get into production and now you care about like debugging on mainnet for example this console log stuff is not going to be Deployable for like a mainnet contract right so then travel comes in here with a mainnet debugger and that is a lot like a much higher value prop for someone who is actually a developer versus someone who's just kind of tipping their toes in so yeah I mean I had mentioned this to you in terms of like I'm trying to sell a motorcycle to someone who drives a tricycle you know it's like where where do I fit can I actually say one more thing on this I think this is actually really that's a really interesting Insight right I think it matters uh like like the what you do right and what level you sit at on the stack will impact the kind of communication you need to put out right so like if I'm building a super fluid application right in an example we sit up higher on the stack than truffle and Alchemy does right like I'll use truffle and I'll use alchemy in my super fluid example application right so if you're if you're doing something what you guys are doing you have to hit a much wider group of people whereas if someone's coming to Super fluid if you don't know like what truffle is or like what Alchemy is you know I'll give you some guidelines but that's not really where my content is going to be valuable to you right I'm I'm targeting someone who's higher up on the stack and is trying to build we're already building on top of what you guys are doing right so you have to think about where you sit in these layers as well awesome that was a lot of great that was a lot of great insight is this on okay um yeah so it's brings a lot of people into the space it helps teach a lot of developers how to use our respective protocols but once we create this content how we make how do we make sure it's effective how do we make sure that Dev eyeballs actually get onto it so I'm curious I'll ask this Albert first um what is kind of your Dev content promo Playbook to have a Content promo Playbook um yeah that's a great question there's a couple of points here I think first is just like of course you have good content like you need to have keep a a pulse on like what people in the industry are really curious about you know a lot of the this past year uh past year and a half uh top two subjects were probably D5 and nfts right so if anything you you wrote about those two things at a top level it could be like a nuanced you know tutorial but like if it hit those two topics then you know you're gonna get at least like a baseline audience who are like curious about what you're gonna have to say so I think like running good content and then like that kind of segues into into positioning as well like especially when you make that top level you know headline title of your blog post or video or it's your thumbnail if you know that defy and nfts are what your audience is kind of like caring about then that should be you know in there somewhat um so I think that first of all helps with just like organic Discovery uh people who are searching are naturally going to find those things um but then there's definitely more of the like proactive you know pushing that content kind of building your own network and it's something that I really love conferences for actually is this prep step of like meeting other amazing people in in the industry and kind of like developing these friendships and Partnerships so that we can kind of lift each other up we can collaborate and so you know I kind of think of it as like I've built my personal like Twitter Mafia or like uh telegram Mafia so every time there's like a post that I think is useful I don't want to like just you know chill and spam all the time but if it's useful to our respective audiences then it's like hey like I wrote this thing you know you know this could be useful for the developers in your ecosystem you know let's like share it with each other so I think that's something that helps a lot too um and then I think another thing that really takes time is consistency and community so something that I saw when I was creating this program called road to web3 which I've already met someone in the audience who is going through the program and loves it so asking about it later um is that like you know it's actually a series of pieces of content so it's not just like one piece of content that I'm trying to get a lot of eyes on it's actually like thinking again about this user journey of like I want to introduce you to some high level topics and then get you down the funnel and like get more intermediate and actually build something that's useful and then at the very end I'm going to give you some ambiguous topics that even I don't know super well but like you're going to be able to like innovate on that yourself and having this Cadence of like oh every week we're going to publish a blog post and a video and if you follow us here you're going to be able to see it if you follow us there you're gonna be able to see it we'll send you an email if you give us your email that really helps because it's a super big value add to these people who are trying to like develop their own Journeys and uh it's great for us because then we can get those eyes as well so that's awesome yeah I think a big part of this is cycling through content we're consistently putting out content that explains different and new nuances to our respective toolings um but at the same time that comes with an issue in that sometimes we have to repeat ourselves right we have to create stuff that kind of explains stuff that we already explained and previous explanation explainers and documentation things like that can become obsolete so I'm curious how you kind of approach that kind of um elastic to Emily how do you approach this kind of cycling through content and the kind of way that previous Concepts may become obsolete how do you maintain visibility of it or how do you kind of approach it from a from a kind of strategic standpoint um this is a really interesting question in the sense that I purposely create my content in such a way that it is made apparent I'm not going to be keeping this up in the sense that right like the date is there so you know when it's published but specifically like the content I create if you guys don't know it's I do a live stream series every single week called web3 Unleashed where we touch on um maybe something that's like very current that's happening so for example one of my very first videos was just like what is an ERC what is the latest ERC that has been released and then we built and implemented a rentable nft and that kind of grew into a Marketplace and has since like Incorporated other apis automated through gelato and and to that extent right like this is not I think there's two types of content we need to think about right we have Evergreen content and that's going to be stuff like what is a wallet like what how do you what is like a proof of work proof of stake right well when it comes to tutorials I think trying to focus on maintaining old content is just not sustainable I think it's more important to try and capture like what people are thinking about now and educating the people who are consuming your content to know how to look for Relevant materials that makes sense that makes sense Sam you have any thoughts on that no I think the differentiating between Evergreen and I guess in the moment content I think is a really really good Insight um yeah I think when it comes to what we do is super fluid a lot of our stuff is gonna like I said it's gonna be a little more specific to what we do uh the kinds of things that you are building content around in each of your respective like Brands and companies it it does have like a pretty large surface area so for us I mean I think we try to keep like make tutorials to keep up with like different updates to our protocol but outside of that you know I think that the Insight is well on trying to keep up with what is hot in the space is also really really good it keeps you in the conversation and it's worth thinking about in terms of keeping you relevant that's awesome um I'm curious like what you got what your thoughts are on like different forms of content so we look at written content and that's very mutable right like if things are updated if a new feature comes out then you can you know kind of tweak it and you can make it and you can refresh it that doesn't quite apply for video content so I think there are like different Dynamics with different kinds of content and they each yield a different kind of Roi so along I'll ask this one to Albert what are kind of like forms of Gabriel media that you feel are like really effective and you know or kind of slept on slept on devrel media um well yeah being a little facetious here but like uh about a year ago I I made this wrap a crypto rap um Shameless plug if you go to my Twitter it's pinned at the top uh at that guy in Tech uh but it it was really effective you know I think combining like this uh you know whatever artistic expression that that you might like enjoy doing with the actual message of what you're trying to send out there I think definitely helps because it's like new you know it's like novel um I think Jonathan Mann does this really well you know he makes he made the merge song he's been singing like a song a day for like five thousand days or something um but he does like crypto folklore music now which I think is like an awesome genre um but yeah that I think that could be something to explore but uh you know also like to my discredit I haven't made a wrap in like over a year so I definitely need to get back on it oh okay cool yeah um like I said I do a live stream I specifically chose live streaming because I think that is a platform that doesn't exist today right uh when people ask me how did I learn and onboard into web 3 I feel like my entire web3 Journey has been a hack I said this in a panel and probably every single panel I will ever attend is the way I Learned web3 was literally I got started and I just got to talk to my co-workers or like I'm incredibly lucky I live with a person who works in web3 there is that like one-on-one interaction that you just don't get from like reading a tutorial or watching like a a video that's just been posted right and so that's why I started to do live streams right because I mean I think it's a proven method if you go to like encode club and I think a lot of like cohort based learning is all around that like in-person touch element and then I think second to piece to that is a lot of maybe how you guys feel is developer relations is a very public-facing role right at some level your job is to be the face of your product and through live streams I think you can definitely communicate authenticity um and personality a lot more than something that just kind of lives on the internet uh that being said the true reason I chose live streaming is because I don't know how to edit videos oh I can just hit play and pause and then it'll be good and everyone will be like oh yeah so relatable she didn't share her screen for 10 minutes of the stream that's fine um and I'm always hiring editor afterwards so that's good yeah and then and then the second piece I had talked to you about this and like I hate it I know I need to do that it's like Tick Tock guys I I like I tell my company this all the time they're like what don't do Tick Tock you don't understand I met someone with 360 000 followers and they're like booming like they're definitely capturing the student Market um I think a lot of times we think about getting people who are like already graduated or they're interested in jobs and stuff like that but you know there's young people too in the space right they live entirely on mobile I was talking to this guy he's like do you think truffle could build like a mobile component of a development environment as like no but it's an interesting question um and it's yeah it's again I think turning back also to know your audience in terms of age Gap how are people interacting with media how are people learning if that is a dance to music maybe find me on Tick Tock in like when I'm able to like stomach that kind of interaction but for now yeah looks like there's multiple age segments that's awesome yeah once took me like an entire day to make a two minute video so it's definitely hard I wish I could do more live streams and kind of achieve that kind of impact um yeah so I think like you guys kind of talked about Deborah is a very kind of public-facing position to be in how do you guys think about like your personal Brands right because you think about you know Ave now you think about Nader David when you think about super fluid you'll think about Sam when you think about alchemy but you'll think about Albert and so on so we're all very kind of like out there in public how do you guys think about your personal Brands and use that to kind of tie in with advocacy of the protocols that you represent I'll start with Sam on that one yeah so this is what I don't think I've actually thought about enough I think the two people somehow right have actually done better than me at this one um with that being said I think that you know I've always had like a complicated relationship with the idea of like trying to like go tweet things and get followers and stuff but I think my mindset shifted especially after like putting a lot of content out in a devrel capacity and just seeing people like see it be very useful and I think building a personal brand in this space if you want to do it in a way where I think you like you know it is legitimate you're not just like grifting and talking about like token prices and stuff uh I think you can get really far by just making useful stuff right if you find something out like if you learn something and you think it's cool share it right and that's what I've started to do this a lot more and what's been eye-opening to me is that like the things you just learned you know like like the fact that you literally went into some new protocols docs for an hour might make you one of like the 20 people in the world that have done that right so yeah I just want to underestimate how much you actually know um that's been an eye-opening thing to me right like I've met there's this guy that I that I've worked with named Josh who um for example like a year ago he just he initially got into web three and started getting really interested in low-level languages right instead of just making Twitter threads about huff and Yule and a lot of ZK stuff and as a result of doing that the guy has like a huge community of people that kind of follow him around and watch all of his stuff and like learn all these new interesting ZK things from him and the guy's only been doing this for a year right so again this space is moving so fast that if you just stay on top of technology and new trends and share what you're learning you will get to the edge of the space in some capacity pretty quickly so I won't underestimate that and it's something that I've really taken to Heart much more as well but I would love to hear your thoughts on it oh for me yo my personal brand I I I'd say is like flustered and cute maybe hopefully [Laughter] um but yeah I think the question like jumping into Deborah that part of like being a public-facing figure was something that was hard to stomach in a sense because there is almost feeling like there's not that element of authenticity to it that you kind of have to put on a face but I think the flip side of being a public figure in terms of the developer relations Is that stems more from people are learning and getting content from you you're adding like value and like what you said like not you're not Shilling a token like I don't know anything about like what what is the hottest thing to buy right but like I know how to make a smart contract um and I really love how you keep calling how like Albert and I are very lucky to be at kind of the base of everything because that allows me to kind of create content that also spans everything so web3 unleash is not about truffle like we happen to build things on top of truffle but normally it follows the kind of format of hey I'm just like randomly interested in this one topic hey consensus is super well connected let me actually invite someone on and then build on it together so I think one really great example of this was I did a live stream with the co-founder of gelato all right so like I said I had been building this nft rental place Marketplace part of it was like how do I automate nightly cleanups right and so with gelato you can do that and so I had met him in Berlin I brought him on he talked a little bit about what gelato did and I was like okay I'm actually I built it I tried it out I wrote a resolver and as I was doing he's like Emily that was wrong um Emily no you should have used this part and whatever and I think that really contributes to the learning aspect of it and when you say like you just trying to I don't know like learn about it is already way more than people have so I think I want to give another example I gave a talk on Wednesday at Devcon about bridging and optimism I work for truffle I don't work for optimism right but we did a truffle box optimism collab and I was able to kind of integrate the two and then afterwards people were asking questions about bridging they're like oh what are your opinions on like layer twos what is like optimistic Roll-Ups and stuff like that and you know I'm not an expert on it but just like that extra piece of you did research oh this is like completely off topic from public facing um but yeah I don't know what to say like I think the end of the day is when you're putting out content you are learning yourself but people are learning from you and it's so rewarding um in that aspect yeah yeah I just want to add to that with an example when I was first getting started into the space I found out about this community called kernel and what the first thing I did because I was like so excited to get started because I got accepted and all this stuff is like I read through like all of their docs and I live streamed me reading the docs even though at that time I had no audience no brand anything and so I like live streamed it and it was like a two-hour video on YouTube just me reading through the dogs and like reacting to it every like five minutes or something but there was one person who saw the video and this was like the main maintainer of the docs and he was like wow I watched the entire video and you gave some really good feedback because there were some docs that were not clear and like this part like didn't make any sense so I went ahead and like I updated here's the pr so in case you wanted to see it and I was like wow that's amazing so that taught me the the benefit of this kind of like learning in public and that helped me start this flywheel of like um you know I'm not trying to manufacture any particular brand I'm just here because I'm really excited to explore and learn and help other people do the same and so I'm willing to be vulnerable and expose that and if that helps people um and uh another thought on like public brand I think is like what I've learned also from the past year of doing developer relations is that like at the very beginning again I have like no idea this was even like a company function um but by the end of it I realized they were kind of like verticals that would help the business in various different ways and public brand definitely contributes to them in different different like magnitudes and so these five verticals that the way I bucket them is five verticals and they're kind of like evangelism which is essentially like marketing um but then you do have the developer flavor to it then you have advocacy which is like more outside in it's more like I'm at the conference I'm at the hackathon I'm learning what people are doing and I'm bringing back that back to the company and saying hey we need to reprioritize our our road map because like people need this stuff and we're not building it and then there's like three other things that are also core but like less public facing I would say uh documentation that's just like table Stakes for people to self-serve your developer product and then there's like the abstraction on top of documentation which is like the guides the live streams the tutorials this is like more and projects that are kind of like given these apis and docs what can you do with them and then the thing that has been the most fuzzy for me but has also been the most important to help like to have a public brand for is community and like with Community I think it's like how do you make people who are using your business and your service feel less alone which is a very interesting thing that's hard to measure business impact for um but and sometimes we like jerk around and we're like what is it what does it take to become like become an Alchemy developer you know or like someone who's like I'm in the web series but I like love Alchemy um and one thing that uh that has worked so far out of like the dozens of things that I've tried is this like road to web 3 Program where it kind of combines all five verticals of like evangelism because we're like talking about alchemy in some of the lessons uh we definitely talked about other you know Partners as well um and then advocacy as well because people are kind of like building on it they give us feedback and they're like oh you know we don't care about this lesson at all we're like this does it makes sense so that helps us kind of build new things it's documentation for sure because we definitely had to like go through and fix some stuff as we were making the courses it's tutorials and docs but then the community stuff really surfaced after we built this thing because I had no idea it would create Community but then we get people who are kind of like showing up at events or like saying hi and saying like wow this helped me get a career or like I talked to someone else in the community and they're my co-founder now and that that to me is like wow I never imagined this would happen but that I guess that's what like Community kind of begins so having a public brand kind of like helps make sure that people know that they can trust the stuff that you're building in each of these devrel verticals is like real and reliable and like is going to be there after a while so that's that's kind of how I've thought about it over time these were some awesome answers I think there's a reason we say that bullying in public is so critical for devrel because curiosity is contagious and it builds communities and communities help bolster utility of our products so um that's it for our Dev content creation strategy Workshop I panel thank you so much guys for coming out today and sharing your wisdom thank you [Applause] thank you all right so for our next talk we have Yaz Guru from Deborah at Celestia he's going to be giving us a workshop on protocol level developer relations it's welcome guys to the stage foreign fortunately expose my Telegram where are you right there is are you okay with this yeah okay cool awesome hey everyone um yeah so for this Workshop I'm going to give a talk about what is protocol devrel in web tree and then overview over as well as um like all the different activities you can do on the protocol side of Devil Within webcrew so about me um Yaz I'm head of Deborah at Celestial labs before that I like a protocol debris seller for two years and before that I wrote the director of devrel at the ethereum classic foundation and so like my devrel experience has only been with layer ones I've never like worked with dabs directly so there's a difference between layer one develop activities versus application layer uh devrel activity which is what we're going to cover today in this stock um so first let's talk about the web due to web3 devrel comparisons and differences so similarities you'd see between webto and web read both cater to advocating for the adoption of a platform or a product whether it's an SDK toolings and both want to improve developer experience and minimize onboarding hurdles but the differences are web free has a monetary element like if you look at smart contract languages why are they so revolutionary is because the concept of a token or a currency is a first-class citizen in your smart contract language which makes it kind of challenging to Advocate to web do devs because a lot of them they don't they're not interested in economic and it's not like you need to be interested in economics but I find with a lot of techies um they avoid anything related to economic topics which makes it kind of challenging to Advocate to pull in web developers to build on web3 but let's talk about web 3 devrel and overview so webwe does have very unique activities that we do in web3 that is different than what's happening in web 2. so one of one activity is hard for coordination So ethereum like um basically made it so popular to have like the concept of a heartfelt coordinator who would like align with the core developers and allow them to uh end the validators or minored back then and allow them to upgrade a network together and that requires um like a lot of devrel elements there's validated relation where anytime you want to um anytime you have a new software release for your core software for your layer one you want to coordinate it with your validators to ensure that the network upgrades uh well there's governance relations so for your end user what is a developer or somebody who owns a token um showing them documentation about how to vote how to stake all that is still part of like the devrel activities that we do there's clear that relation and one good example of the EIP process so all clear devs were facilitated by Tim Baker um there's a huge activities around core Dev relations where you do community calls you align all the court after agree on uh specific proposal and that's how you upgrade the network like the merge is a great example of that uh smart contract advocacy so there when you have a new smart contract language and you want to show what you can do with that language like a great example sway there's move so Liberty and Viper are getting more popular right now uh decentralized application Deborah this is when you're on like focus on the decentralized application part not necessarily only the front-end elements of it but also how do you connect your smart contract to your front-end application after you deploy your smart contract and then finally core protocol devrel which is what I do mostly and I'll cover more about what what you can find in protocol Deborah um but other activities like technical writing Deb Advocates and evangelists program and Community manager there's roughly the same between web 2 and web3 uh okay let's talk about protocol by Brian webberries so I'm going to cover all these topics that are you know we're going to go over each section there's communication with the core team which is super important for any protocol devrel to do validated relation is a huge element of it governance relations specifications and improvements they're also making sure you're aligned with version releases and documentation there's protocol Focus events like not everything tends to be boring we can have a lot of fun when it comes to the protocol stuff and we try to do a lot of cool events uh Community calls and coordination and then there's engineering activities and finally fun tools that if you ever want to do more on the protocol Deborah like I'll just share some tools that are fun to use um and you know improve your learning process as well when you want to do more protocol demo so communication with the core team so this is probably the most important part like if you if you as a Deborah working on a protocol side you're not communicating with the engineers on your team that is a red flag in my opinion because your engineering team relies on you to communicate with the external developers about any new features or whether the external developers found a bug you know sharing it back with the team hopping on that call with the team maybe if you your team has a product manager prioritizing it with a product manager ensuring that everything related to the developer experience of the protocol is important it's like it's actually done well and part of it is testing like a lot of it is going over the documentation after every version release testing and retesting your software making sure the command line don't turn out a bug and you know repeating that um and ensuring that anything gets updated with documentation um you have to be basically very active in the conversation like the way I like to see it is Deborah oh do you know when you're on slack and you're team slack you're the ultimate reply guy for all your engineering teams um anytime they're talking about something reply back oh what do you what do you know what is this what's that ask question like I asked a lot of questions I personally prefer to be the dumbest person in the room because then I can just ask as many questions and learn so much from the engineers about the protocol and that helps me do my job better validated relations so that like it started happening much more recently but with a lot of the software being ended up being used by validator validator not necessarily developers but a lot of them do a lot of crazy development customization Twitter uh uh software to their clients so having that kind of maintaining that kind of relation is super important whether you want to energize validator related activities ensuring that everything is coordinated with them perfectly they're the most fragile component of your layer one like if you as a devrel team you're showing hey this is this cool application for this cool smart contract that you can build on ethereum but there's a problem in ethereum a concessive but none of that matters because as long as the layer one consensus is working well and that validators are aligned everything else can work better but that is the most special part of your ecosystem so making sure that you have a tight relationship with your validator is super important one example of that is security and vulnerabilities so let's say we found a bug in the core protocol and if somebody were to exploit that bug they can shut down the network how do you communicate that how do you align with your validators there's like a lot of strategies you can do where you you would share the binary like like in like you know in private channels would your validators and ensure that they upgrade uh without you know sharing what the code is and then only after like two-third of the network upgrades then you would you know you know when when the Network's safe then you can release what the patch was and stuff and you know you can do a post motor on it and stuff like that hard flow coordination is actually part of biod error relations so part of it is you know ensuring that the network can upgrade securely and a lot of that has to do with test net like upgrading on testnet if you're a smart contract Deborah you played a lot with test Nets they're part of the you know protocol devrel activity is to ensure testnet upgrade properly and if there's issues with testnet we would have to revise everything before you know you know testing things on mainnet um the other thing is nodes um I would recommend anyone who's like you know like who didn't develop to run a node like even a light node it's just like a great activity to actually see what's happening on the like the blockchain side of things like you know seeing blocks being validated and confirmed and stuff is actually super cool um and it's like it's like yes you can use inferior or Alchemy to get an API and connect to the blockchain but seeing it actually in action and seeing the blocks being uh you know being confirmed and stuff actually something really cool and you can actually use it for your development project later governance relation um so a lot of layer ones a lot of protocols actually even applies that pure protocol is actually a smart contract protocol there's a lot of governance that happen there's a lot of voting so there is devrel activity around governance basically you're energizing proposals we create a process around governance proposals like how do I submit a graph what are the specification of the draft what is the template of the graph after I submit it who do I talk to to ensure that this uh proposal actually makes technical sense and it's actually a sound proposal all of that requires that relativity but it's a new kind of devrel activity that you never see in web 2 and then you would have to coordinate with all the token owners on voting on The Proposal when it's ready to be voted on so energizing the token owners I use the word owner but it's like a kosher version of holder and I don't know like I started using it a lot more recently but yeah um also ensuring documentation on how people can vote or clear so that anyone can start voting on those proposals easily and how do they and you as a devra would have to participate in voting not because you know you prefer proposal over another but just to see where the bottlenecks are like if there's a delay in the process somewhere at least you know you can inform the engineering team hey you know let's you know improve the governance propose uh the you know system and ensuring Advocate documentation like let's say you have a user like how do I vote with Leisure live making sure that you have that kind of component ready um is super important for Governor's relationship specifications and improvements so you've all heard of eips I'm sure um so there's a whole lot of work that goes into specification Improvement like the EIP basically you're supporting the core engineering by driving specification work within standard bodies right um being an editor and a steward is another example being that well um I started like you know I became an editor for one of the Improvement proposals which has a volunteer just don't like learn and after that it's not like it's actually really interesting like the whole process that goes into it and it taught me a lot about protocols hosting core Dev calls like they're all clear that I've called that were used to coordinate the merge are one example of that but basically clear def calls happen when you have more than one client team building on that same protocol and you want to align all of them but normally if you don't have more than one client team you can still host core Dev card but you basically just a conversation between one client team and people who are part of the community um and then coordinating hard Forks is based on what specifications you want in your protocol so that actually informed what the hard work would look like Vision releases and documentation so version releases is super important um I can't tell you how many times I've seen like people get stuck on a software being run because they're on the wrong version and the reason they're on the wrong version is because the documentation points to an older version which might be using an older version of go an old older version of JavaScript non-compatible so aligning on version releases with your engineering team and actually engineering when they're like several different team some of them might be like oh we're upgrading for our product we're upgrading go to version 1.19.1 but the other team is still on 1.18.3 and if you need to run both software it's a nightmare from a documentation point of view to tell them hey install Go version 1.18.3 for this software but 1.19.1 for the other software it's a nightmare but it happens and that's why communication with the engineer is super important um when patrogen fixes are coming out to which version like making sure that you know these are being like you know shepherded well and stuff because you know your user your developers are telling you hey I'm stuck on this bug and you know why they're stuck on this bug but you have to wait for that burger to come out and you got to communicate it with them and after the version release ideally you already have a pull request sweaty with um to reflect the new updates and the Virgin release of the software packages protocol focused event so these are some fun stuff that you can do I've seen a lot um fun event when working on a protocol side of things because you want to energize that Community right so hard Fork watch parties are an example of those kind of events so whenever they did a hard work um hard folks are done by blocks so once you get like 10 blocks before the hard Fork activates we kind of like I've done it a few times I hosted a few parties basically just people start counting down block like can it's kind of like a New Year's uh countdown thing it's just like aligns the community really well it's just the hard part but what else are you going to do just to you know get people excited about protocol so that's one example test net launcher or causes for celebration if you're launching a new testnet for your layer one that people are excited about it throw a party you know get people together and show them how to launch the test then um there's other cool protocol events that you can do depending on what your protocol is like if there's like zero if you're an artist snark heavy protocol if you're doing a trusted setup ceremony why don't you put it on a live stream you know get people together and you know people can talk about you know you know doing the computation for their trusted setup ceremony activities so one thing you should totally try to do is start a new testament it doesn't require that much you'd probably like um you would just need two nodes running together one is uh boot node the other one is just a like a POA nerd or it could be a Mining Node or like a validator node in fact we get to see how a network can be generated from scratch advocating for new smart parent replace new smart contract languages so for this specific component is let's say you're playing with sway you're playing with the you know evm you're playing with any kind of um smart contract part of it on the protocol side is knowing what compiler version you're on and knowing what feature learned that compiler because if you're trying to debug a small contract issue with one developer and you and if you don't know that the bug is because they're on an older version of the compiler and they're using new features knowing that would help your job a lot better um being comfortable with basic you know basic cloud computing like running a node in a cloud like I believe anyone who's in devrell should be able to do this to spin up a simple node on digital ocean Association to it and just want to run a you know ethereum like loan right note um building new apps and tutorial in those new smart contract languages so if you have a new smart contract language um try to like any tutorials in that try to make them like games you know like oh like for example on Celestia I love Wardle like I play wordo often like it's one of my favorite games to play so I created a roto application chain on selected to get people to play and like you know help improve it and stuff so a lot of people start referencing it uh to the world game and even now for Engineering onboarding Process everyone's now following my tutorial just to learn more about Celestia um and then you know things that are even common on our application layer is you know doing Workshop to demonstrate UTech we all know this being comfortable with the command line like it's your best friend like I can't emphasize this enough um a lot of people I notice in Deborah they're comfortable enough with the command line only to do some tasks but then they move to the IDE so as long as they do the minimal amount of work on the terminal because for some reason the terminal can be scary to some folks um you shouldn't be scared of terminal like use it more often like use it like learn more what you can do with a terminal and stuff because that would really improve your protocol related Deborah work so bonus kind of tool frontal that if you want to improve your weapery debris skills um you can do it for the protocol side it can improve it for like um even on the application side improve your command line shops right so you don't have to do this but if you're up to the channel and although it's not recommended for everyone private or like a terminal ID like Bim or emacs to test it out it's much faster to code there's a minimal um onboard like you know loading curve but it's not too bad what are the benefits the benefits are I can run any application I want um in the cloud and I can modify parameters in that application I can add some features if I like and compared to writing everything from your local machine learning how to do this is actually going to improve your um your skills at a devrel person so I highly recommend learning how to do this if you want to ask you know if you want to ask for resources later I'm happy to share a resources being comfortable with Association into Cloud instances and installing dependencies so a lot of us like to build things locally I don't I like to spin up like a simple Linux server and just build everything in the cloud because I don't want anything messing with my local machine and to keep things clear cleaner and if you're able to do the top two things you're unstoppable you can literally do anything um without even having to do too much programming if you don't want to but being able to run these things will make you do like you know being able to spin up any kind of node an application and a simple Explorer for that node like you would be able to do a lot of things with this um so there's two tools that I like that I think would be really important for anyone in debru ag is a command tool called the silver social basically it's regex but um It'll like let's say in document if you're doing a lot of documentation work right and then you'd have to modif like let's say the product name is changed you'd have to find every instance of that product name within your documentation AC will find all the case sensitive uh you know references to that word and will allow if you combine it with them with one command you can change all the references um to the new product name and you can do that for a lot of different things like version name if you want to you know update diverse version name you can do it with a lot of those FCF is a buzzy finder a lot of times if you're working in a cloud or if you're working locally from command line um you forget a command line that you ran like maybe two weeks ago this is a great way to look through your history for any kind of command that you're running like if you're running a node you're deploying an application um um and then there's Dockers I'm not sure why it turned off uh I guess it's off oh yeah it's fun I mean it's just like the last two points and stuff um all right cool um we're at the end anyway so using Docker is fine I recommend people play around with Docker but you know would be cooler if you go through the challenge of actually building your binary and installing all the dependencies not because you have to but it actually will make you like more used to playing around with your command line and becoming a better protocol that row um being comfortable with Linux super important um Ubuntu servers are my favorite um you don't have to pick Ubuntu but you know being comfortable with linux's Paramount and thank you um if you have any questions hit me up AC does an awesome talk I think people sometimes sleep on that lower level of devrel so thanks for inviting us today thank you all right let's get right into our next panel subject is defining and measuring devrel's success sometimes devrel's success is hard to quantify raise your hand raise your hand in here if you're working devrel raise your hand if you have a clear quantitative understanding of what success looks like in devrel for your team so we're going to try to unpack that today and get some cool insights into how you can quantify measure and Define Deborah's success like to invite our panelists at the stage looks looked like a fashion show here everybody was like non-speaking nice um so we can start with intros again so we can start with Yaz and introduce yourself again work our way to me yeah hey uh yes was here earlier uh how did Deborah Celestia nice to see you all hi I'm Steph I am a devrel engineer at polygon good to be with you all loves the past panel and also you're talking as yeah big shout out to yes I really like appreciate like much more protocol devrel it's something that you don't see a lot uh yeah Francisco I'm a part of the devil team at consensus lately focusing a lot on metama snaps how many of you guys know snaps nice nice and how many of you are devrels full time nice cool awesome welcome guys to kick things off I just want to ask Steph what the hell is Deborah to me or tiaz uh well the words are Developer relations so something about developers and something about relating to developers it's the easiest definition we're done here um [Music] yeah yeah so I think it's everything surrounding developers and how they understand how to use the products our core Engineers are creating anybody want to add to that um so Depo basically it you're the interface between third-party developer and the core development team that's basically it like and to hold your whole job is it doesn't matter if it's a layer one an application a web 2 product you're advocating on behalf of a platform and that's what it is a platform regardless of its characteristics yeah maybe maybe we could add something like I think the average it's not going only to Rave first uh but uh you know generally speaking I think it's uh everything that has to do with online and offline a developer acquisition for your product and I think today we're doing very product focused Avril and uh we should also focus on the top of the funnel uh to bring those uh developers on on top of the funnel on the product conversion so yeah yeah I think we pretty much hit the hit the nail on the head with those definitions um so I think Yazi kind of touched on devrel kind of being a liaison between developers using the tooling and the actual core team I wonder if you guys have any more insights on where developer developer relations stands in terms of the whole organization what does a proper devrel team look like in terms of interfacing with different teams such as marketing BD as well as the core team because it seems like it's a very cross-functional role I can start uh you want to start um so I've seen organization that put Deborah in the wrong place and that would piss up any developer person like you never put Debo in marketing team ever it's not marketing you know um you don't put that bro in product because product had their own problems and like devrel is only there to tell them which problems are bigger than and which users give out different products right about different features and stuff so they're not in product they're definitely not in engineering because you don't want to report to the director of engineering because he wouldn't care about you're throwing a hackathon party when he cares more about like shipping stuff so Deborah ideally I would like I mean I advocated a lot that Deborah ultimate Celestia reporting directly to the CTO but we're still interfaced with the business development team product marketing and Engineering so ideally it should be its own function a functional team but that interfaces with a lot of different components I it's definitely a big debate right under CTO in products in uh um in marketing um yeah I I don't know like I think like at the end of the day you need to uh bring value and also sell uh like your your team in the company because I think this is also something that I also uh underestimated a lot like literally like evangelizing your you and your team inside a company is one of the most powerful things because I think people see devrel and specifically I'm coming a lot from developer Community grow like as a sales funnel and uh you know like and I think these uh this is a big problem so like the fact that everything they were speaking today uh how we are actually like showing and we create value for the developers is something that is very important uh so anywhere that you want to make make it sit I think like uh and this is an open debate right and uh yeah I I don't really have the answer on on that yeah I think that's really interesting and I think where you put your devrel engineers um kind of determines what types of metrics that and that's the whole point of our talk rate measuring metrics and success for devrels where you put us determines what metrics we're going to be tracking and what we're going to be using to be successful but you really need to understand things about the engineering the product and the marketing teams to be successful because at a core level you need to understand the engineering part so that you know how to help other Engineers use whatever your core devs are creating and then you also need to understand product right because any devs you're talking to they're creating something so you have to understand how whatever your core engineers build fits lots of different products across the ecosystem tons of use cases and then on top of all that you have to have kind of some marketing chops because you have to understand how to amplify the things that you're saying and doing so that they reach everyone that they need to reach otherwise no one's going to know any of the things that your product or engineering teams are doing yeah I think also like to add on that is uh specifically specifically having Endeavor also like working very closely or not even under the same like uh let's say a reporting line with uh developer marketing and developer communities and even maybe maybe events because a lot of decision especially on events are done also by other um basically parts of the organizations and sometimes like they should really hear much more the people on the ground and they can really are on on top of the fact mentally mentoring developers all the time and sometimes this decisions are not like uh yeah taken in consideration because like they've released the support function to be at the hackathon it's not like the driver to bring feedback to uh to how to you know how to resolving the next event and how to be part of the roadmap of the product it's like those are great feedbacks but are not enough to like prioritize because we have already our own roadmap defined those were some awesome points I think that's part of the fun and part of the stress in being in Deborah it's been the fact that you're getting pulled in so many different directions from the developers who are using your products to you know the bunch of different teams in the organization that you interface with I want to just go around and just ask you guys what is the purpose of Deborah what are you guys trying to accomplish for your respective protocols with developer relations so that part of this answer will be you know what does your organization do and then tied it into how Deborah drives the bottom line for this organization these are with yours so I mean like I said before like with Deborah what you're playing around with what your main goal is is a platform you have a platform it doesn't matter what the platform does right so when you have a platform that would benefit from products and services developed by developers you want to give them the minimal amount of onboarding process and great documentation in order for them to build those products and services that would allow you to grow a community um it's the same in web 2 like in like I mean look at the Google Maps API it's being used by like countless different organization and stuff and that's because if they had a really great developer experience that's the same thing that you want to do in debt three enabling the next Deep by application under layer one or somebody who's using your SDK for a specific um application or your API to build like a mobile app um enabling product and services to be built on your platform and showing developers how to do it essentially what it is about from a traditional software engineering background how many other people are also like former sdes that are devrels now most people yeah so I think part of being a good developer relations engineer is having a lot of empathy for other engineers and being an engineer before this lets me put myself in kind of our Engineers shoes so thinking about like what were my pain points what did I need from either docs or a representative or someone and how can I be that thing that I wish I had especially when I was getting into web3 so for me that the job is kind of creating resources that are very accessible so I don't like to have architecture diagrams for example in anything that I'm teaching because I think it's just like kind of hand wavy and doesn't always make sense to people I like to use analogies I like to have code Snippets that people can just grab and put into their projects I want to make it extremely easy for you to take any of my resources and build right away not have to not having to talk to me that's how I kind of like know I've done my job right and if those resources aren't there yet I'm the resource until I create that self-service doc or tutorial or example dap that will help you build I want to clarify before Francesco answers this question when it comes to debrel what is kind of like the business objective that you're trying to push what's like the the business needle that you're trying to move by driving developer experience adoption and Community growth with devrel yeah that's that's very interesting question and I want to add something that uh I'm thinking about something that you guys see and say but I think uh um also like business objective or any objective is like we should think about a use case first and product second like uh like you like our our goal is also to inspire developer building something with different toolings and you know like the the consequence that they're using our tool is is because like you have a better support Community like there is much more uh let's say a vibrate Community behind that and and the support out of it so yeah that's uh that's probably I think like the main also like objective in terms of business objective yeah that's that's I don't know like I'm always scared about saying like those those funneling on like the sales funneling like you see like developer acquisition costs these kind of things because it's always like trying to um uh how do you say like uh cannibalize your own Community right because either you inspire to build stuff or you're trying to convert that's why I'm trying always evangelizing even inside a company uh it consists that we are we're having like specific products um as you probably know but but a very important part of our developer let's say developer funnel is literally like covering the top of the funnel and this is is a product agnostic devrel and I think this is extremely important to understand because I think there are two functions today like that the devil is covering it there is the devil conversion the whatever converter I don't know really a name on that but there is also the devil that is uh let's say Inspire and putting more devrel on top of your product funnel and those are very two different things because the the inverter is actually aiming to you know build better documentation tutorial on how to build things and the and the Inspire is literally like going out and Inspire developer to to just check and be there because he like what what you said and what community uh you are you're building on top of snacks on point I come from a finance background so I tend to like look at these things from a financial angle like what is your developer acquisition cost and sometimes according to Francesco's answer like it's a great point there that you know if you boss yourself in with numbers and stuff you're going to end up kind of stifling the creativity and Innovation and kind of inspiration that you're trying to get across with your with your tooling um I'm curious how you guys have seen developer acquisition costs Approach at your organizations if at all um so I think some models in which you want to quantify how you acquire the developer use the sales funnel right and the sales funnel is flawed when it comes to open source developer advocacy because you're not selling them anything what are you selling them an open source software they're gonna they can Fork it you know um so what do you what are you trying to do like what is the measure of success and it turns out like the new model that came out that's been used a lot by new Deborah team is the orbit model and that model really really helps put things in perspective like you have four different levels of orbit like the closer a developer or Community member is through your orbit um the more they're like either like an extreme fanatic about your product for some reason versus somebody who's far away who might be just signed up for your newsletter and for every orbit level you have a different strategy to how to pull them closer um so that model I prefer better because it caters better to an open source community that is active on Discord or Twitter active on GitHub doing contributions using that model I secured two awesome Engineers who are just present simple required and I got you know I hopped on a call with them I got them hired they're now really great Engineers within our team and stuff but yeah yeah um I'm not part of kind of the salesy measuring of any of this but one thing that I look at a lot is um are doing analytics so understanding how many people have deployed smart contracts on polygon over the last month how is it different from other layers and other blockchains and then if we're at a hackathon or something understanding how many people have built on polygon and also how many people have built based on resources we've created recently you can kind of tell usually especially if a product is new when it's used at a hackathon if someone went straight to the docs and then you can also measure that with like page clicks and time on the page and all of that so kind of a combination of that is what we're using to measure if what we're doing is effective and then we also always ask for feedback like direct and also on Discord all types of feedback are very welcome just want to add one thing so um on top of what you're saying um one other thing that you can use to quantify activity if you have a test net measure how many calls are being made to the faucet and during a specific time period that would tell you how many developers are calling the faucet to load up and then deploy the spot contract I'll give you a great estimate as well yeah thanks for that good yeah that's amazing Alpha yeah I'm a big fan on interact tutorials especially sandbox based IDs because you can literally track what you're saying like every steps in the journey and uh even if it is global during the weekend I just discovered there are those guides and it's super interesting because they're just MD files but every steps is in theory tracked and you will get the quizzes so um yeah that's that's quite an interesting way also to track things but one one topic I don't know if it's really relevant but it's it's maybe six years I'm trying to get a like understanding how if an event or an aquatone is Roi positive and and sell that output to like Executives and one things that kind of like I don't know it's not perfect but I'm trying to qualify and give an estimate value to each uh kind of virtual win like for example hey we have I don't know 25 submission at the hackathon we have like 20 people out of Workshop they did and each basically win is is as a virtual a virtual uh let's say basically money gains right and then you basically like you you call it against like the cost and you can say okay this is what uh what uh what happens I think that's quite an interesting fact but uh uh there is help actually like selling the fact that hey this is Roi positive because we always been in hackathon and we are giving value to those developer we are mentoring we always see an amazing value even on the post uh relationship with the hackathon winner right but a lot of time is difficult to to basically sell those events uh to uh to to different teams inside a company that was some serious outfit that was really cool um I would like to just dive a little bit deeper into that because I'm just so curious like how that devrel analytics side looks like on your on your Deborah teens like what does it kind of look like from a holistic process how do you guys kind of query it how do you guys take actions on it what are some examples of that would love to hear hear about that starting with uh stuff yeah so kind of going back to those like three different facets of devrel marketing uh engineering and product from a marketing standpoint we're measuring Twitter and different socials both for um just like the number of likes and the number of reshares and stuff but also for sentiment analysis around the brand and then from a product standpoint um hackathons like you were saying are hard to measure but one of our goals has been to have successful hackathon projects get funded and get into incubators and really take that next step I think the thing that um people that are farther away from building and hackathons don't realize is that folks come to hackathons to learn new technologies to play to have fun to network there's lots of reasons you do a hackathon and not all of the reasons are to create the next unicorn of course some people are doing that and it's really exciting to see it we actually had a product at the graph hack called orb I don't know anybody have the orb mobile app it's like this super cool lens mobile app not yet hopefully you all do soon yeah no no not yet I see one but they're raising soon and it's really exciting to see because that project started at the graph day hackathon and has continued it's had a Rebrand in a new name but that's um something that I would point to as product success as a result of a hackathon and then on the engineering side it's kind of like how are we working with our core Engineers to figure out what they're building next and then building things to support builders that are using that that was like very long-winded um but like if we have a new product coming out how are we working with that team directly how many resources are we creating and how are we being guinea pigs for whatever they've built um and that's not that's a little hard to measure but maybe you have insight into that yes because uh you've got your new like all the Celestia things how are you doing that so I think when you measure things there's two different ways to measure they're data driven metrics but data-driven metrics are not everything sure you can track things down with Analytics you can track Discord you can track engagement on Discord if you want you can track like I track GitHub mostly like I want to see who are the Superstar developer that we should hire who are also contributing but it doesn't give you the whole picture attending conferences attending Workshop gives you a better picture because it's more of an intuition like you don't know until you go there what developers like so one strategy I like to employ in Deborah like if I you know in devrel team there's I would like to have like having one Deborah person whose only job is to go to Every hackathon and just hack and make fun I'm like just make friends have fun and report back what you learned you know and that is really great it's not scalable you only you would only advocate for a smaller amount of people but the benefits are different but it depends on the person's personality right the benefits there is the intuition around what people are interested in for example eat Denver not this year pink like during the covet time when it was online I think it was 2021 if I'm not mistaken just by talking to people online and stuff asking them what they're interested one alpha came up a lot of developers were interested in ceramic now no data driven metric would you know tell me that ceramic is interesting I mean like the interesting thing that developers are caring about but that only happened by talking to them and then near at the time and I congratulated near on such a move they immediately created a you know co-bounty with ceramic I'm like ah smart you know like way ahead of me but um that kind of thing what I'm trying to say is you know just by talking to developers what you know intuition what they're interested in what they're talking about and stuff with each other that's the kind of upper you get that you can't get by tracking it with metrics um directly yeah I'm onboarding to your products I think it's extremely valuable like we had like even in in Bogota and uh and you need to buy we we created customized guides and we didn't really speak with the developer we just gave them link and they say let's do it together it takes one hour and a half and we we run through this and and this process is amazing and like um when developers see a developer relation person from a specific platform or network hacking with them that's better marketing than your marketing team can come up with yeah I also want to shout out like the the guys from Harmony because sometimes like I remember back in the days I mean two years ago in it Lisbon like they were running with their their blue t-shirts and I remember in front of me they were giving like grants like saying like hey this is a small Grand just keep building what you do like we are just having like bi-weekly calls and in two three months we're we're seeing what what you build I think and now for example EF is starting to give like some I think like the the like traveling uh scholarship like for example now you had to eat bota next week is Biden and at the end of next week there is Eid Panama you know like two 300 bucks you can get and hack maybe on on a specific product those initiatives I think are really valuable and uh obviously like you have that or even like like basically you're saying like from social mention to like customer discuss Discord but uh but sometimes those stats that are uh that are basically known data River are also the most important but I also want to say something like also those stats are difficult and sometimes we are not even selling well in the PM team like like I I for example like have difficulty every time to write like a feedback docs with like multiple and and kind of like go with the PM and going through like each single feedback because sometimes he's always like oh the roadmap is already fixed or like this is already uh done and one thing about feedback like every developer has an opinion about what works what doesn't work normally the only time it becomes a signal if more than one person says the same thing then you would have to try it but if one person said well I'd really like this to be compatible with Arc Linux for example that shouldn't be a high priority unless the whole Community wants our clinics an example yeah yeah I think Sam from our team likes to say that when developers have things to say about your docs they have complaints about about it that's error and that's kind of like something that that calls for kind of addressing and you know user feedback and you know Improvement in that in that sense um so I think a bit oh were you gonna say something and I just I was brainstorming with um with a teammate like this weekend saying like we have the product manager and maybe we should have like a community product manager also reporting to the the head of product and his his goal is literally like prioritizing your roadmap features but with the interest of your developers and community building in those hackathons because I think that's might be this might be a cool thing I don't know the job title yet though at least actually can I add one thing um so that's that's a really interesting idea uh at polygon we have different types of devrels so devrel is almost like a blanket term but like I'm a devrel engineer we also have devrel's that focus more on the marketing or the community side do you have that concept as well other places yeah I mean Deborah has so many different like um like you know sub categories within it like you want you want to have your Debo Engineers who are like in like philosophically like you know developer relation requires a developer relation engineer to know how to develop right and often time to find some people saying I'm a developer relation engineer but I don't know how to code okay there's been word the engineering part in it but then you have developer Advocates um which are technically developer relations engineer but the way like the way it works is like for Deb Advocates they probably spend 60 to 70 percent on documentation coding and like supporting the team in the community maybe 30 to 40 on going to work during Workshop you know going giving talks um hosting community events then you have the developer evangelists who are like 60 to 70 percent more talks and conferences and you know doing the whole you know circuit but like 30 maybe doing a pull request here or there some engineering so you have to kind of two different uh categories one is more forward-facing one note how to work with Engineers both have their pros and cons then you have technical writers Community managers um we have like integration Engineers like like under Deborah like you know an integration and veneer who exclusively works with Partners um just to teach them how to integrate with like Celestia um and you can even have a program manager for your hackathon for your events and stuff so there's a lot of different ways that you can be part of a devro um team but essentially at the top it has to be like the developer relations Engineers who are like running the show because it's you have the best context about the product about the technical challenges of the product and every like every other activity is more about enhancing the product experience I think also to add on that the academy and the education part is extremely important because this is tying in also on on the on the on the side I agree the debral kind of title can be pretty right at times because we do so many things um I really like that orbit model that you kind of talked about um it kind of is a nicer way to put sales funnel I think um how so it seems like the one of the key metrics of kind of a successful developer relations program is the amount of those kinds of really loyal developers who are repeats who keep on billion on top of you um what are ways that you try to create and retain these kinds of individuals in your ecosystem and in your community um you know I think we touched on allowing them to be ambassadors allowing them to come to hackathons fostering that kind of um continual engagement but I'm wondering what other kinds of uh strategies you guys think about when it comes to fostering these kinds of repeat and loyal devs yeah I mean I can give you like a like a user flow walkthrough like from me as an example going through a new platform that I haven't seen before or or might be using so I remember there's four levels of orbits right and that the center is orbit zero which is your core development team right so Orbit One during the Titus they're the biggest fan orbit four the farthest away um they're just like lurkers um they might be casually interested so let's say hypothetically I want to build a python app with Django now I don't I haven't read Django documentation before I don't really I'm not I'm not a fanatic I don't have an opinion about Django versus another MVC product right so I would be an orbit or because maybe I would sign up uh for like information about a tutorial on Django but maybe I'm not going to explore it me going to orbit 3 means I'm going to the documentation and I'm being able to build my application without having to interact with one of their python members you know team members and stuff that would be ideal in my scenario because there's enough questions being answered on stack Overflow enough documentation an example that I don't have to do anything orbit 2 you start being involved more in the community you want to support but you're not doing it full-time but like you have ideas you're volunteering you're creating application but you still have a full-time thing or you can't dedicate full-time to it orbit one is when it gets interesting because you know pulling them into that orbit means here's a grant please pursue this project or like you know we can invest in your you know project long term and you know if they're like valuable community members like offering like you know just inviting them for dinner with your core development team because they look at your core development team as Superstars especially if they're like have a large following on Twitter and having them you know have dinner with them and give them like limited so I like I mean I'm not a big swag person personally but like I know that other people like swag but um you know for people like you know who care about it you know having that limited edition is super important but like also opening up a lot of doors for them hey you can be part of the incubator program you can be part of this accelerator program by this VC company that is supportive of our Network or our application so there's a lot of ways to do it but once you start at orbit 4 you're just like slowly different strategy to each orbit to pull them to the next orbit and the way you quantify is by two different metrics so love how much like they call it love for lack of a better term but yeah it's symmetric how much do they love your platform right by tweeting about it commenting opening up pull requests writing tickets um and then reach how much reach do they have are they a GitHub contributor with a lot of followers are they big on Twitter um are they well known in this space so reach is super important because let's say somebody who's like super excited but doesn't have a lot of reach and is on an orbit tree might not be as valuable as somebody who also had a lot of you know reach but also loved your product a lot so prioritizing it and how to cater different community members required you know like a specific strategy depending on our two metrics yeah orbit love is amazing like uh especially on those incentivization but I want to also say something maybe answer to the to the question on a tactical level I think like everything that you're doing like you need to be gamified and like there need to be some institution that actually like it's it's uh it's a much more engaged I mean shout out to crypto zombies I love like how they're doing like with with uh you know specific like nft and badges because I think that's that's where all the people stay engaged um what I want to say was uh was the part uh on uh yeah on gamification and yeah we should not uh underestimate that uh yeah how how to build something like that yeah that's cool shout out to Albert in the Alchemy team by the way I feel like they've pioneered this idea of kind of gamifying um The Learning Experience and so has build space in others so I love that and then the other thing that we do is my boss always says the developer is the athlete so and I like to think of that as the developers the main character so main character energy um but understanding that developers come into web3 and then they get better like they don't always need beginner tutorials at a really great moment with a builder this weekend and she was like could you guys make something harder like a harder tutorial and that just showed me people are only beginners for a certain amount of time and they're all there are always new beginners but if people are building forever they need to have more and more examples of different types of things they can build so for me that's been really fun because then I can go to like tableland and say how can I build on tableland with polygon or the graph and figure out how do we connect those two things so figuring out how to connect everything to everything else because eventually someone's going to need it for their app and we want to make it super easy and then beyond that also giving people different touch points with the ecosystem whether that's grants accelerators other opportunities to pitch I I'm always thinking about the people that I've met at different conferences if opportunities come up because I'm thinking like who could I involve in this that may not have that platform and that's how I kind of keep keep them in the ecosystem or at least know that they're really appreciated as a builder and they'll have long-term benefits building on polygon and I just want to summarize one thing is like I think developer programs are are key but one thing that we need to do as an exercise is understanding what's the developer personas and which program is targeting what like one good exercise is really like what's the value that that the value get like from from the developer and and the value really like what what are you trying to perceive because some people are incentivized not by grants they want to be part of your roadmap they wanna or maybe people just want to be on your website so that they can put like a LinkedIn they are they're an ambassador program Masters or whatever yeah I mean one good example of that that blew my mind recently is people like you know I set up the documentation in a way with you know crowded integration um because for some reason I was like trying to practice my French and I thought it would be fun like I decide activity to try to translate to talk to French bad idea right but anyway I forgot about it and then somebody in the community started asking about translation efforts and I said yeah I mean we got the crowd in uh you know integration if you want to get started like um and over the next two weeks um we had like over like five different languages completely translated by different community members could they want to be part of that road map they just want to contribute somehow and it like now we have Ukrainian uh Chinese and um like Spanish and um French like you know French made it there eventually right but um yeah seeing that kind of effort and seeing on the Discord chat translator have more activity than validator who blew my mind because um the one thing about Celestia is like a lot of people want to be validated on our chain to the point that I try to like avoid a lot of conversation about this but like to see translation you know activity by con you know community members being hired and validated at some point in time showed me that it's an interesting way that people want to contribute and those people you can turn them into not necessarily like developers but like technically proficient folks who can maintain the dogs uh improve it with continuous integration so that it automatically pulled in the latest translation from Karen and a lot of times I spend time with them like like they asked me questions I'm like how about we spend an hour together on a call I'll show you how I would do it how I would make uh this contribution how I would make that PR how would I would do that integration and you know if you want to take notes and start doing it and a lot of times they love it because they just want to learn they want to learn new skills they want to like be able to build things um and that's like I think that's an essential quality in empowering Community member being able to teach them tools that will help them learn um and I know I'm great at that because yesterday um I taught my friend how to use terminal in them within and she's non-technical and it was like she started writing code and it like it's marked down for now but like so you know it's really cool to see that yeah and then I want to say all the people are shy and even if the contribution is small you should always shout out them like like I see like people opening issues and you know the the internal engineering saying oh this is not nothing like let's not prioritize so let's not do that but it's always like our like our job to to even the Expo contribution like to to basically like being recognized so that this person will be so so happy that even like even if you run like a community call or he was part of like a PR that there was those issues like I think this is an amazing part of where people gain confidence yeah that was awesome um so it seems like a big part of success as Deborah is it's less tangible it's more interpersonal right and I'm curious like how do you guys think about reputation when it comes to your devrel's side of your protocols so like for instance when I think of polygon I think of like your table and you guys are really friendly you guys know how to approach a developer from a more personal standpoint what are some ways that you guys kind of think about and just kind of approach being the more interpersonal um more interpersonal side of things and just trying to like create a better reputation around your protocol and the way it feels to developers that are like kind of interacting with they come kind of like customer service in a sense how do you guys feel about that I I don't know I think like uh like people should fail art and learn and be transparents with you with your developer's Journey like uh we we I mean and you know you reach the levels like the the developer in in their let's say in their in their funnel at different stages and you always try different tactics the most important part is saying oh this is not working maybe check it out this it's helping you because every person is different in in terms of how to learn so I think this is one one part that a lot of companies are always trying to figure out the best tactics and and and never really like show those vulnerabilities to the to the community yeah um yeah I agree with that I think um yeah reputation can be kind of tricky and um web 3 because it at the end of the day you as a Deborah person like I remember like the panel before that's talking about you know your personal brand and I think personal brands are super important but at the same time you have to lift the brand of the platform you're advocating for you know so it can't be like more building a personal band over your pla you know you know the brand of the platform because in a way like if the platforms succeed everyone on your team succeed and everyone who's building on that platform succeeds right um so I like like from a reputation point of view I like to align with the brand of the platform I'm advocating for so like with celestead there's a huge notion about like a brand about intellectual honesty and that's like major part of that brand you know like you know and like Giga brains and stuff at cello it was more about emerging markets and like you know looking out for folks who don't have market and how do you support them with web3 so aligning around that brand was super important reputation with it ethereum classic where good nightmare so I don't even want to comment on it but but yeah I mean this yeah uh so polygons slogan or catchphrase is web three for all and that's something that I'm very aligned with so that part is easy for me uh but one thing that I've seen feedback on Twitter and things for is just that some devrels all they do is show the company they work for or show their personal brand um or not their personal brand their company's brand and I think you kind of need to almost take a step back from being a maxi for whoever you work for and kind of understand the entire landscape because then when you're talking to a Dev and they say why should I actually build with whatever your product is you'll have an intelligent answer and be able to say this is why and it's not just use Shilling whatever product you're supposed to be it's more intelligent yeah and I think also we're going back always to the same kind of concept like a use case first send product second because I think people are inspired by by the Journey of building something like I'm always like trying to tell people like okay start to build that use trough like during the journey like deploy this with a specific like infrastructure provider and then while while during this journey like use metamask like but the fact that you're using this product is not like the the the the the the the proposition like the fact that you're building like a cool use case uh around like IPO like I don't know like as a like like a social platform from scratch or like a small supply chain the app or like those are like the the things that people get inspired to be like from from A to Z is not the product they're using that's why like I'm much more like up for like core you know like was was an incorporation oh yeah it's like at the end of the day is is the the product that are choosing is because of different reasons that they get inspired on the building the use case yeah and I think it even builds trust to acknowledge that there's other alternative ways to build um and just knowing like oh you can use ipfs or are we for this for in similar use cases um and not having such an opinion opinionated way to build also is kind of cool um one other thing to add as a devil person the more that you contribute outside of just um what you're working on like in your free time like I mean I'll give you an example like volunteering or a specific organization supporting with like you know doing workshops like I did like I like um for the eea um Enterprise ethereum Alliance it's not a really fun topic Enterprise and blockchain is not a fun topic but you know I was volunteering there because they needed some kind of support there or like hyperledger Foundation um you know these kind of or even flashbacks like these kind of things like you know helps you like you know be a person of Integrity into space because your support you want the whole Space to succeed and stuff um and you know these kind of activities help with that yeah dev's gonna devs can smell a shell from like a mile away so authenticity is the best approach um guys thank you so much for sharing your wisdom on this panel this was so great so much Alpha and uh yeah give it up [Applause] all right so for the rest of this time we're just gonna Network it's a free-for-all um just coming up with something on the spot if you want to talk about documentation I guess you can huddle over here if you want to talk about Community there will be people over here um everyone everywhere else is just like free girl apis over there yeah and this beer over there there's like no exactly yes foreign foreign thank you 