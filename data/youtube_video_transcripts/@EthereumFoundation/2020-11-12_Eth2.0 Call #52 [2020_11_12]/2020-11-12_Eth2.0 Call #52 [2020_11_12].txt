[Music] so [Music] so [Music] [Music] [Music] [Music] okay well uh to the 52nd call does that mean it's been two years it's it yes because we missed a couple along the way so it's definitely been two years okay um all right cool so here is the which is crazy on the agenda today um we will start with testing and release updates you saw the v10 release um i think it's pretty much the same as the candidate release other than some additional testing and things uh as i mentioned last time xiaowei and myself and proto a bit are looking to like revamp the way the test vectors are output um which will be some changes on your end on how you induct them in but will uh greatly reduce test generation time and reduce the uh disk footprint which will allow us to greatly hopefully extend the amount of test factors um you know throw in more random things and lots more operations and that kind of stuff so that's in the works um it's probably not a huge priority to ship that in the next few weeks but next four to six weeks probably get that out maybe maybe we'll wait and get mainnet out first and then change test factors under your feed cool any other testing updates yeah a quick week on beacon fuzz update that's okay yep um cool so over the last few weeks um we've been running our structural differential fuzzer and we found two consensus-related vulnerabilities on prism the first one was an off by one bug in the committee index validation of the attestation processing and the second one was um basically using the wrong epoch uh when validating slashings for opposing slashings um these are all fixed now that's what we can i guess talk about um there was also one minor uh non-exploitable spec diver just for teku in the proposed slashing processing so that was uh found on prism a few weeks ago we started working uh with a future idiom foundation team member uh joining the devops team there who essentially automate our deployment and monitoring um of those fuzzers scale on our aws infrastructure he's built a bunch of pretty handy ansible scripts that are pretty much ready um and he should get that done actually today or tomorrow i'm continuing to work on the custom fuzzing engine sorry um it's it's working but we only got the mutation based version working actually uh structured inputs so uh the next step next thing we're working on now is adding that structural part of um for that custom puzzle um i'll be writing this up in a blog post uh hopefully uh push that out over the next couple of days but that's pretty much it any questions from eddie i mean on that note there's actually going to be two people who are going to be joining the film foundation very soon to help with security and i guess we're still looking for talent and you know people interested so if anyone's listening and knows any you know talented hacker please send an email to each security at ethereum.org yeah additionally i think we're gonna put out a few rfps um first to try to find some more external teams to help with testing um of various things some network testing load testing chaos testing that kind of stuff uh so if you're keen on that kind of stuff keep your eyes peeled for those rfps great um we're going to move any other testing discussion okay cool uh moving on um all of y'all are very aware uh proto's been driving uh getting some v10 test nets up toledo is up and i believe there was an epoch with a hundred percent attestation and participation which is pretty cool um and that's looking stable i know there were a few things to work through i know um but all well there um and then there was some conversation past 36 hours about what to do with a larger scale test net uh proto has prepared a tracked 100 000 validators and keys um and i believe the intention is to share those uh amongst the client teams and the ef uh to kick that off ideally next week so we can get this going firmly before main that launch um and i woke up and there was a lot of conversation around what that actually means what uh paramount pyramid is that how you say that can an australian correct me correct yep paramount um what the fate of pyramid be uh it seems like we will kind of kick it off ourselves then open it up make the config and stuff available for others to join so that people can test mono configurations and stuff and to have that at least and then circle back on the conversation after genesis to think about what a sustainable test net in the long run will like you know should we up the ejection balance um should we change the queue limits and stuff in that config for the sustainability and kind of user experience around that um that said um i believe in the conversation that i caught up with this morning we hadn't picked um when we actually kicked this off so the the deposit contract's there we just need to set the config which uh we can kind of tune the genesis time uh however we want at this point um proto what are your thoughts i know there's some ends up being so on your end i know there's this configuration work for clients and getting various nodes up is this something that we are looking to do tuesday looking to do wednesday thursday of next week so the original id was to launch on tuesday um this was a while ago that and with the leader i think you get better idea like what the efforts are that are involved with getting another test up and running so i'd like to just get some feedback from clients if they confirm if they can confirm tuesday or alternatively they move it forward a few days i think tuesday is fine on our end a couple of days wouldn't hurt though but we could do tuesday two for our prism as well yeah good with tuesday uh for taku say no to in a couple more days so yeah everything is good and during tuesday although a couple days would work better i think you can just do wednesday just as long so maybe just go for events yeah that seems reasonable a little less weekend work exactly okay let's go for wednesday all set up configurations uh assume we can do the same genesis time doing utc works to me yeah cool so if this is going to be a utility to uh validators over the next handful of weeks maybe state in your discord to not be overzealous on the amount of girly eth they dump into this thing because uh one whale could block up the eq at the current limits um and so maybe encourage validators to try one or two obviously that's not in our control but hopefully it can be somewhat of utility to validators in the pics okay any anything else on this before we move on uh so about candida uh the desktop will be running for at least a few more days i wanted to give a shout out to the beacon chain explorer will help us get more insight into this so it does not just performing relatively well if i can say so myself um and going like in the last few days i wanted to test like some miscellaneous things like the new lighthouse slasher and these kind of things where now that we have a nut that we control nothing good good insight in and we'll thanks pretty well yeah great um and as for madasha well for piermont um we will get a launch pad up at a sub-domain so that people can practice using that and for medasha i'm based off of my read i think we're going to at least publicly sunset support for it next week um i think that there may be there's some will keep running nodes to test non-finality and some edge cases and stuff but uh i think publicly will kind of stay fully degraded starting you know over the next few weeks um anyone opposed to publicly sunsetting and and stopping active support okay alright madasha so one thing to keep in mind actually by is um people i think validators me and maybe dude wants to have some kind of environments where they can test their setup before they actually spin spin one up alive so if that's not medasha then what would we direct people to so pyramid uh pyrmont we're going to kick that off validators that we control and uh next week also open it up uh for deposits and we'd be committing to run for your pods for some longer period of just yes at least through uh main genesis after which a couple weeks have a conversation about what we might do to make a test net more sustainable and user friendly in the long term that makes sense great um and i guess format that's in the activation queue currently and is close uh can keep running the nodes and they will be activated um because the way the queue works so great maybe for the new test that we could make the queues faster so that people can get in more quickly and test their setups yeah i'm not terribly opposed um i know summer hasn't it to change configuration uh if this is one of the you know one of the reasons is to test just test main net configuration anybody have any thoughts on that i'd prefer really plain mainnet parameters just to make sure that we're testing the right thing especially during launch let me do whatever yeah again i'm gonna encourage the community um you know try a validator or two not i'd also lean towards having the main net con for now i think it's helpful understanding of the the times that they can expect to run like the the wait times and stuff they expect on main net right we could also ask people to exit voluntarily as well too that would help as well as people exit cleanly instead of just shutting off their nodes yeah agreed i'll put that in zenyatta in as well okay anything else on test nets before we move on to client updates so one thing we're working on some like shorter lift does not just focus on genesis and so we have a configuration file ready to go into a client do genesis then repeat it over and over again ideally so i think like this is an effort we can run in parallel but of like this thing where we do want to repeat genesis more often and to enable us to like configure the client without changing the internals right so like that looks like dash dash data der and things in lighthouse a very specific uh test net configuration um what do you do one very short-lived genesis uh every week until genesis what's the goal here the intent something like that sounds good like for now right now we'll just focus on the pyramid genesis that's big enough and soon enough yeah but then after that it does help to repeat genesis more often yeah and i guess specifically we we've seen time and time again that the genesis uh can be brittle um and we've seen kind of minor issues pop up in a lot of our our test nuts so that's the that's the motivation right there but i mean sure testing genesis is good but there's no actual expectation that we'll launch maine without a base in mainland times and i think everybody's baking in states and expecting people to run with this bacon state yeah you know on 1.0 day but we've also seen other issues um i guess often i guess that following youth one issues uh state mismatch match issues boot note issues doesn't um like and just being able to find peers i i mean maybe we don't have to do it every week until genesis but i would want to get another couple in um if possible yeah but that might be something i mean once every week i know clearly it's like one or two more well i don't know i don't know when genesis is but i think it's super important that we're clear in the in in signaling to people that they do need to update on this box on the week that that mean it will happen right i agree uh that if you're validating in genesis you should expect to be uh updating your software in that time and and being pretty active during the time uh and i will the singer finds communications on that in the recent blog post i think we're really good and if you want to communicate to your users in a similar way i think that'd be really valuable yeah also just to remind everyone i think i like posted this in everyone's issues like have some way of quickly updating all your users that they need to um install something like so that even if it happens one hour before genesis and you want to push something then you get all the stakeholders and not just yeah leave it to chance so i think everyone should have like an email list for critical issues very much agreed okay let's move on to client updates starting with nimbus hi um so uh it was uh very buzzing the last two weeks uh starting with networking uh we have improved again uh the tracking of resources uh so um in the past it was futures and memory now we are tracking streams and channels we are also um uh we have fixed some uh gossip sub audit issues uh this was done late one one was not tested enough um three weeks ago uh when it was uh time for it to be audited so we tested a bit uh later uh on the core parts um we had uh toledo release we are using gossip 1.1 now on it a lot of documentation changes uh regarding infra uh some users mentioned that um they don't like being forced using website gtps support we have also significantly improved uh if one uh chain sync and monitoring and we pre-released 0.6 two days ago with prebuilt linux binaries and we are planning to do a new release uh today as well and uh with that we are gradually moving away from mac files and we will provide pre-compiled executable for all major platforms so right now it's linux but we plan also for windows mac and arm builds we are also uh significantly reworking or release management for mainnet so we are creating sanity checklists we will create a mailing list something that one mentioned uh just a bit before uh some monitoring and the support coverage plan for team members and also we have lessons learned in uh when we did a directory rename for consistency our permission changes for security because those are created through troubles for users either they were holding databases or they were used in system d automated scripts and didn't work after changes that we made so we will improve how we deal with those breaking changes also we used to have a way to create deposits and validate our keys from nimbus now to avoid confusion and fight scam attempts uh this is now undocumented and developers only so uh there is really only one way to deposit and it would be the if ef um launch pad and that's it for us got it thank you how about lodestar hello uh so we uh have not yet released uh a new new release of floatstar we've been wanting previously trying to get to a full spec 1.0 candidate uh compatibility and at this point we've uh got it down to everything but disk 5.1 so that's really what we're focusing on this week we um finished with a rough graph of our blst integration of course it's a lot faster to use blst we may hold off on on that until we can address like the browser compatibility issue because right now it only works in node.js um and we other things we've gotten about 80 of the standard api almost everything i guess but like the step validator status endpoints which i think are still possibly in in in play um and our validators using the standard api so that's good great thank you let's do prism hey guys so in the last two weeks we have closed all the ongoing issues with our trail trail updates audit and the other report should be online and people can take a look we have made improvements to our initial thinking process we've made it better at the exploring force during periods of non-finality we are currently working on conforming to the e2 standard apis and we are working on testing peers drawing as well and we're also working on the slasher interchange format and the most importantly we're focusing on our mainnet v1 release and we are working on a set of issues that we want to close before november 24th and you can track those in our milestone within the repo so yeah that's it thank you cool thank you terence lighthouse hello everyone paul here um so last week or this week we published a blog post about our plans for a version one release um as discussed earlier we're encouraging everyone to update in the week before genesis to so that they can get uh the genesis state they don't have to do the listening to f1 waiting for genesis thing um so if you're gonna be working if you're going to be using lighthouse um from genesis and i recommend reading that blog post on sigma prime i was at lighthouse.secondprime.io um we're also working on the standard api um implanting it and providing feedback to the spec our implementation is quite close so we're working on events now um the slasher is in the final review stage the release next week is lightly likely we're running eight of these slashes on toledo we're setting up a production staking and monitoring infrastructure our second lighthouse security review has been completed and our criticals were found uh we're working to address the points raised um this week and before genesis and we're feeling pretty confident for genesis in a few weeks uh everyone's working hard to complete their pre-genesis issues and pr's that's it from me great thanks paul and taku yeah hey uh so um we have published a mainnet ready uh release and we'll be making mainly at the default uh in release candidates next week we now have mostly complete implementation of the common api and we are going to duplicate legacy api on the next release uh we have completed almost all spec released in the date zero updates so the last one remaining gossip message id is now ready to be merged and will be in the next release i hope [Music] we had the feature to support us uh for snapshots seeing from state alone and it's actually very effective and allows to get fully up and running um in a couple of minutes or so um we've made significant improvements to memory consumption uh by utilizing the proto array more widely and also we have a couple of serious issues uh fixed one is related to uh out of memory uh after long periods of non-finalization and another one is um related to uh stability of death on is one main net and also there's a workaround transaction size limitation in infra that's it from tickle excellent thank you and i believe that is everyone for today great let's move on to research updates you want to get us started small update about oops go ahead okay i have a short one uh about weak subjectivity so uh lighthouse prism tecu and nimbus support um weak subjectivity sync starting from a block route um and we have one major block explorer who is going to support um who's going to be an eats to weak subjectivity provider right from genesis that's ether scan beacon j.i.n is going to do it a shortly after launch i'll be making a release for the week's objectivity server that everyone on this call is encouraged to run but uh overall it's a it's a great job everyone uh eats 2 will have weak subjectivity security right from genesis thanks for getting that out the door did you yep moving on so great i've so um on uh data availability sampling phase one um i uh wrote up a uh kind of pre-spec of two versions of data availability focus stuff is one one of which basically assumes the beacon proposer is the bottleneck for all blocks and the other assume assumes that it's the shard proposers that have the primary responsibility so both of those uh and i think we're still in the process of kind of discussing them understanding what the trade-offs are uh understanding what the trade-offs of uh having a kind of formal in protocol chaining of blocks versus not bothering with that and a bunch of other finer grain strain uh trade-offs but generally moving forward and trying to get to a uh a concrete spec assume um as soon as that makes sense um and then in addition there is still a kind of small research questions to do with the availability sampling popping up like how to calculate these how to verify yes proofs of uh proofs of a block size as efficiently as possible and so forth and so just working through those two um another spin spec related thing that and i've started to do is uh separated oh i did a ton of draft pr separating out the light client spec into or the uh basically into its own file so that it's kind of theoretically independent of phase one uh so the goal of this is to basically try to modularize the phase zero thing so that we we don't we're not kind of hamstrung into a particular order and we can just do things in whatever order they become ready and and my clients are theoretically or the likelihood protocol changes are theoretically really simple so we should be able to do those very quickly so the pr exists yeah nice thank you um any other research updates yes txrx has a small update um we are working on the executable beacon chain proposal um the proposal is about enshrining if one execution into beacon chain and making it first-class citizen with the respect to consensus um comparing to ether this proposal significantly reduces the communication complexity between execution and data charts and it also opens an opportunity for advanced use cases like instant deposits and instant withdrawals and many other use cases that require synchronous synchronous process um document is almost ready and they're gonna be pretty published pretty soon uh on this research and uh yeah like since uh the next week so awesome thank you mikhail any other updates um danny i have a couple of figures that i would like to show is that possible uh yeah [Music] all right can you see it yes okay good um so we did a short experiment with um with the clients uh all five of them and we were trying to see um how to behave uh in terms of resources and synchronization with the change so the figure that you can see here is basically the time is in the x axis so we run all the clients for one to two days sometimes even more almost three days uh depending on sometimes they crash and you have basically here and the y axis is the the synchronization slot in thousands and so um well from from this figure we can see um for example that lighthouse is and prisma ones that kind of are a bit faster to synchronize in comparison to on the other slot start take several hours to actually start like four hours start actually syncing um process and there are a couple of flat moments for lighthouse in hours 10 to 12 and four hours and i will come back to this in the next figure in the next couple of figures i will analyze the number of peers that each client connects because these are reporting the logs and so you see here for the same five clients so using the same colors um the number of beers they are connected um deco is the one that is usually connected to mod players over 70 and lighthouse is 50 but there is a couple of hours where the the number of connections drops uh for some time and again i will come back to this a little bit later there is something that happened at this point i'm not really sure what it is but where they started we're still investigating uh prism is super stable in terms of number of beers nice it's always kind of around 30 and then um well star and nimbus also kind of more or less stable between 20 and 40 years reaching 20 and 34 lots start we analyze the disc usage for all the five clients so in this figure you have actually two white axes so because we want to compare how the disk usage relates to synchronization with the chain um we see that teku has a kind of interesting behavior in kind of a zigzag in which the uh this qc goes up and then it goes down dramatically and this is repeated periodically so we believe there is some kind of cleaning process that happens in in the client that makes this pattern maybe the thick hook guys can explain this better um and then uh well nimbus and prism have uh both quite low uh disk usage uh but there is a weird thing that happens for uh there is a sharp rise in this usage after our i think number three and then uh it's stabilized and then it drops again so i think that this kind of cleaning process that happened and then it came back uh and it was uh going steadily again until um hour 19 or 20 where it starts to increase uh the discussion sharply i i don't know if this was um a non-finalization um period or something related to this um but uh it arrived to the point where uh it's related the whole disk of this um 35 gb um so it crashed at that point it could not sink anymore um and so the the flat becomes line i i have to precise that all the clients were running on on the same notes but they were running at different times so tech one lighthouse were running together in two different nodes prism and nimbus were running two days later in the same two nodes and then lots start two days later and then when the cpu usage we can see that um previous nimbus and lotstar is usually around 50 percent of cpu usage and this node with just one core but two threads so there is it per trading it since these three nodes three clients are not kind of using hyper trading so they use probably just one thread it stays about 50 most of the time deco and lighthouse on the other hand they do have about 100 utilization of the cpu most of it except for the moments where the synchronization got become flat in the case of lighthouse and then we see that there is a clear kind of zero percent usage of cpu at that time that also happens for reason at some point i think in hour 35 or something like this there was one hour in which the there was no synchronization so the syncing line became flat and there was zero cpu usage for like one hour or so we also measured uh memory and so uh in this case we can see that seku and and prisma are the ones that use more memory so the nodes had about five gigabytes of memory we see some kind of periodic sharp decreases some prism as yes some prism memory usage so i don't know if this is some kind of a state cleaning or something that happens periodically inside the client bikes kind of periodically in terms of memory resources the one that is more light is nimbus uh so it looks like this is a clan that is really made for um you know light uh hardware and so it requires a little memory usage and then uh in between we have an and lots of star uh again lighthouse uh have these um like flat lines of memory when when there is desynchronization issues uh and uh these queues start to increase we can see that also the memory starts to increase until the length and hour 24 when it came so become flat and then the memory and becomes kind of flat um we also analyze this network but i don't want to make this too long so i'm going to just stop here and we're going to publish short on all these so um if uh people from the clients can have some explanations for some of these things or for youtube this is something interesting come to reach us and maybe we can discuss so i'm gonna post um in the chat a first draft of this report and i will be happy to receive as much feedback as possible that's it thank you can you say when you did some tests because i guess all clients are changing a lot uh from weeks to weeks yes uh all the tests were done uh pretty much last week um so i can i can uh from which time and we time from which time to which time each client was running and we can also provide you the version of the client that we use um and any other details that that you need yeah we can post all that in the report i think it's not in the in the in the draft that i will post the link right now but we can we can add this into the report yes cool thanks leo share if you have some stuff you want to get some input on uh share it before um and then we can just bring it up for questions rather than going over it all live okay uh but thanks um cool other research updates okay great networking um thank you for the feedback on the issue i opened up a couple weeks ago um we been kind of had my hands full with uh things uh so but i have a work in progress pr to update based off of that conversation and i'll get that up for review today um that's with the kind of how to handle block sync and weak subjectivity in some cases there any other networking items people want to discuss today just quickly tested the scoring parameters that was being suggested on toledo um they seem to be really well so uh we're merging that down or have merged out down to master so if other people are looking for scoring parameters the ones we're suggesting to suit the network great have you uh dropped an adversarial note up there to see what happens not yet um i kind of want some uh some validators in there so i talked to proto about that gotcha uh and you've been working on some you know uh modification of the client to make it evil uh does that work still in progress yeah uh that one i've kind of put on hold because we're focusing on some maintenance things but i i should take that off class and get that back in there to see to test the screen yeah i gotcha other networking items other networking items okay great um general spec discussion um what do people think about this idea of going of unbundling all of the post zero items more and basically treating them as kind of separate components with no kind of necessary assumption of sequentialness so the three i'm thinking of in particular would be one is sharding the other the second is white clients and then the third is the merge um i'm pro especially because on the sharding and the merge stuff there's a lot of like so the exact timeline is unclear i think it'd be good to have like client infrastructure beginning to be developed before anyway i think it's a very good idea wasn't the point though that you would want to have a light client infrastructure before the match so um you can basically continue just continuously have like client support for for each one yeah absolutely in terms of dependency support before the merge is definitely a strongly preferred but i guess i'm not worrying about this because light client support is by far the easiest of the three items and so like i'm recommending we even sticking it you stick it into the first fork after genesis like five months after or whatever if we can right i mean from a from a consensus standpoint like kind of support is extremely minimal and reuses uh infrastructure that we have been using uh gossip and committees essentially uh gossip and a committee and so even even the complexities of supporting light clients uh then we can build on after but uh getting that support kind of native into the consensus is very easy so something that i think makes sense to ship asap okay i mean uh you heard that you heard that coin here about stability i've re-juggled the road map again does anyone actually listen to this call at this point do we still think that that phase two is is like uh has any will ever have like any meaningful um uh instantiation anymore or is that just basically something that that is just nowadays the term at this point then it's probably an outdated term and that it implies that it will happen but i personally think we have enough on our plate in terms of data availability sharding and emerge uh to get done and kind of see how the rest of the ecosystem develops you know if in that in the meantime uh the use of data availability for scaling and things is like a total unbelievable flop uh then that would be information to continue moving forward with a full-blown phase two but in terms of r d effort i think that it's definitely on ice right now yeah that makes a lot of sense to me cool anything else on just general spec and thank you trenton i'm glad that you're listening kevin too uh but yeah any anything else great uh open discussion i think that probably a lot of y'all saw that we the ef announced this like staking community grants round i just want to plug that here um anything this is really to kind of enhance and start to support more actively and things beyond the core infrastructure the the core client stuff uh so if you're listening and you're building stuff that makes the client the validation experience more delightful or you want to start building something in that domain uh check it out on the ef blog there's a form and you can it's not it's not on the blog but i will blog about it tomorrow uh there's a link but check it out i post it on twitter um and client teams if you have some stuff that you would like to see and it's not on that wish list or you have some teams that you're friendly with that you want to get involved with this please share okay anything else before we close today [Music] i wanted to bring extra attention to the slashing database regeneration problem um that seems especially important for people staking on at home and on unreliable storage media like sd cards on raspberry pi um does anyone have any insight still are tough this is to implement and what it might take to get this through so specifically this is um looking back at consensus and constructing the slashing database as best as you can from messages that actually got unchained correct correct this is for the condition where um you have backups for your validator keys and everything but um the storage media you're using um for the machine that's running your validated client right that fails and you lose your slashing database but you still have your keys so you can restart your validator and you would need to regenerate your slashing database from on chain data right which gets you potentially in a good spot and not guaranteed i this could be done independent of the clients now that we have this interchange format so maybe maybe we want to support um a script that uses the common it can can do this independent and give you that format and then you can bring it into any client rather than asking all clients to implement it at this point asking for opinions about how tough or easy this is and what path is the best to go ahead if we're going to regenerate it from chain then we're only going to get we're never going to get like things that we cast like like say way in the future like a few hours in the future or whatever so we're only going to get things up to like the time now because they're the only things that exist on chain so perhaps instead of bothering with like going back through all the history and collecting everything we just use the slashing interchange format has like a like a low water mark we just say like you know from now or last epoch just never produce another attestation again yeah that actually makes a lot of sense the so the format does have this like minimal format where you just say don't do anything before now that that stops you from joining a chain that's not finalizing potentially so that would be the only disadvantage of that and that might so i agree like most in most of the time that solves that problem but there are times where you perform something more advanced and that's yeah you're right so if justification's not advancing then you'll be able to uh produce an accusation yeah yeah that's a good point [Music] so it's never going to be perfect like fully agreed it could be that someone got an attestation on me and never published it and and then yeah they just use it to slash you later but i mean that case is very rare i would guess that most of the time it's just like you innocently lost your storage for some reason yeah you could that's a good point you could also argue that if there's a lot of there's not much finalization there's quite potentially a lot of forking so it's probably not super safe in that case but that's that's just a guess we can't we can't understand you there's a lot of wins right sorry it's windy uh we can ideally would search mempools and i don't know block explorers and stuff but yeah i suppose a black explorer could provide it as a service um i have a question related to that paul you mentioned the like if you sign things far in the future and maybe had broadcasted them or something uh and that's in particular an issue because you can create these surround conditions and that was something that dunker brought up on a youth research post and a small note in the validator guide made it into the recent release on essentially a validator should kind of protect itself from signing things that advance time in very unexpected ways into the future to prevent these inescapable uh message signings that create really bad surround conditions our clients have heard of that and is there any um effort to try to protect users against this case not from our end it sounds a bit tricky because you you're into timing territory like if you can't trust your system it gets pretty hairy we i i personally wasn't aware of it i'm not aware of any works on our end to mitigate it so my suggestion would be like stop like refuse to start the validator claim client if the last entry in the slashing database is more than x hours say six hours which shouldn't happen in normal operation um and if you're just restarting after an extended outage then you probably have an operator present anyway and they can just set an override flag yeah i know this happened and like start anyway so that that seems like a very simple solution that should cover almost everything yeah the case is the case is definitely concerning you know if somebody gets you to sign essentially the last epoch of all time uh as your target then you've created this like ultimate surround condition for yourself um let's i'd say client teams think about a little bit maybe we'll bring it up in some internal chats uh it's definitely something i think worth considering on how to protect the users there okay anything else today before we close great good meeting appreciate y'all coming um and happy paramount launch thanks everyone thanks guys thank you thanks everybody [Music] [Music] [Music] [Music] you 