foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Applause] [Music] foreign [Music] foreign [Music] [Applause] [Music] all right [Music] [Applause] oh [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] foreign [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] oh God [Music] [Applause] [Music] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Applause] [Music] [Music] foreign [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Applause] [Music] thank you [Music] [Music] [Music] [Music] [Music] [Music] thank you foreign [Music] [Music] foreign [Applause] [Music] [Applause] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] [Music] [Music] it's one of those [Music] first time in my life [Music] what do you know [Music] matter how hard I try to feel the ground [Music] [Applause] thank you foreign [Music] foreign [Music] [Music] foreign [Music] [Applause] [Music] [Applause] foreign [Music] [Music] foreign [Applause] [Music] [Applause] [Music] [Applause] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Music] hello hello everyone I'm Lorena I will be the presenter for this following session our first speaker is Gregor fan who will talk about tokenizing brand three key learning welcome [Applause] thank you my name is [Music] founder and CTO of smart token labs we are a team about 40 people and we have been working in Organization for four years and in last two years we have witnessed a lot of brands and take their huge interest in in the nft and blockchain space and we helped many of them to tokenize their brands and this is our learning first there is a strong demand for brand Brands to connect their audience with an nft there is a difference between a brand lover and a holder of brand token that gives the brand access to Their audience allow their users to access the ecosystem and put a value into their loyalty but there are two barriers first wallet not many people in the Brand's audience user base have wallet and it's not easy to get it there the second signature so a lot of users hesitate to sign any message or transaction when they are holding brand tokens and understandably so I'm going to talk about three cases where we helped friends to tokenize and I will explain how we overcome or amended these barriers and share our learnings so first library is a Switzerland luxury skin care brand is the world's number one and their user base a high net worth people with a love for art so they do art exhibition art review art auction and that's basically their Market position what we did for them is a space beyond an ft project for Everlasting digital art however these are not traditional these are not technical audience these are traditional art buyers and you can imagine most of them never had a wallet before and the format of the sales is quite traditional as well you can imagine a joint Hall with a rotating light and display of digital art moving slowly and then people holding champagne in their hand and sales was done in that moment and you can understand that this is not a perfect moment to teach them how to install a wallet and get the seed freeze so what we did for them is we allowed them to purchase the art with their credit card and we will send them an email with a magic link that contains the cryptographic information needed to later prove their ownership of this digital art if they need to use that with the smart contract or debt so after the sales the buyers can set up their wallet in a later time and this sales was attached was linked with their email address and they need to obtain the email address attestation I'll link that with their ethereum address and from that moment on depths and smart contracts will recognize them as the legit owner of this digital art this we effectively have pushed the moment of the install wallet to a much later date therefore smooth out the onboarding process and that's the first example the second one is actually the step count and the target audience is everybody in this room this this time understandably our audience is more sophisticated and most most of us have a wallet but we still find it beneficial to smooth out the sales process use a testation and there are a user experience improvements using this technology so we did permissionless perk which is um a project where third-party websites can recognize devkin ticket holders and give them special perks and offers this is done by sending an email to every ticket buyer with the ticket a test station magic link if you click that link the cryptography information to prove that your ticket holder is stored in your browser and later when you access third-party websites this information will be working to prove that you are ticket holder and again if you should sign a message sign a message or do a blockchain transaction the link the cryptographic information will be linked with your wallet that can be configured later stage therefore users can buy their contacts with credit card without worrying about wallet and get a wallet set up later beautiful thing about this design is Devcon does not have to verify the ownership it's not like Google for example every time when you use Google ID Google has to be online and you connect to them Devcon can be offline when you're using permissionless perks so that's um what we did for devcom and the last example is Shopify where we provide worked on the VIP shopping experience via nft the idea is if you hold an ft you can access gated content special discount or special offers on the Shopify website what we discovered is user hesitate to connect to their wallet on e-commerce website so we give them our product brand connector which allows users the option that they can be brought to a safe website connect to the web token to their wallet choose which token to use in the shopping process and the proof will be sent back to the shopping website for the shopping experience and this allows us to allows users to not having to expose their wallet address to the Shopify websites and the users will not have to sign a message on the Shopify website so these are the three projects we did that I want to share with you today and if You observe all these and you will notice that all these projects about how websites can interact with token holders through tokens and all the technical details such as wallet connection and signature are just infrastructure to support this happening and in terms of user user experience they are actually the hurdles so that brings me that that brings me to this point that we are working on the infrastructure to support use of token and websites but there is a much bigger infrastructure issue at play that is today's web 3 websites lack the trust anchors now before I explain the trust anchors let me ask a question we know today that web 2 websites are built around centralized points such as Facebook Google and Apple and we know that web 3 is decentralized web which does not depend on this centralized services and we have been building depths for that future but which depth we built today can replace Google and Facebook and Apple to provide these essential internet Central Services I will elaborate this within with example so suppose you are buying the ticket from movie theater let's say the minions you want you go to movie theater login you probably will log in with Google ID you probably will pay with Google pay or Apple pay once you've got movie ticket it can be saved into your Apple wallet and once you finish purchase you might want to share it on Facebook or you might want to add it in your Google calendar or you might want to get a Google map pointer this is a typical web 3 web 2 experience it is very smooth in five minutes you use the five or six Central Services provided by webtool by Google Facebook and Apple these are not just Central Services these are trust anchors because the movie theater website cannot do this by themselves even they have the software and code they cannot provide Google at ID login even if they have the login code they would have to if they don't use it they would have to end up with user management password they cannot share it on your shared movie purchase to our friends because you are not entrusting them the movie theater to manage your friendship is not because they don't have the code they can have open source social media but they they couldn't they don't have that trust anchor to do so so today's web 2 experience was enabled much by these trust anchors and today's customers are already used to that they will not go back they will expect no less from web3 but where are the things that can serve at this function web3 and for tabs will not do that we've been building that but that's not for it three reasons first user demand convenience users don't like to go to a third party website to go to while they are shopping go to a Dev website prove they have the token and come back to the shopping similarly in web 2 they don't want to go to PayPal finalize payment and go back to the checkout process second depth suffer from the same security and availability issue debts are just normal websites they are not secured by blockchain consensus blockchain smart contracts are devs are not they are repeatedly hacked and they are sometimes not available today you will go to aav Joy ether and then go to the movie theater and pay it it's not integrated process smart contracts are composable devs are not so there's three reasons steps couldn't do the trust anchor role can it be tokens we have been working on tokenization for three for four years and we have seen a lot of cases and I'm going to share my experience with you on token working and the trust anchor but let's hold on for a moment to think about it today everything is based on the existence of the central trust anchors it's actually quite hard to imagine web 3 without them it's hard to imagine what will the trust anchor be if we don't have Google will the Google login Google ID be an identity token will the Google Calendar become the calendar item token that is in user's wallet will the Friendship the connections you have on Facebook be tokens it's difficult to have a picture of the web 3 with new trust anchors we don't know what they will exactly look like and how will they exactly work but we figured out four requirements for token to act as a trust anchor and these four requirements are trust interoperability privacy and security happened to be tips for you to remember tips trust means that users should not trust websites as much as they trust their tokens again take the movie ticket for example if you have nft let's see the minions and if you want to buy discount ticket with that you don't want you won't trust the movie theater website as much as you trust your minion token you don't want to connect your token there connect your wallet and do any transaction the website might be insecure and steal your meaning so trust anchor should be on tokens not on websites that uses tokens the second interoperability that means you should not depend on websites to have the correct code for your token so you don't want to go to website which tells you sorry you cannot serve you because we only recognize version two of the minion token you have version three you don't want to wait for them to upgrade or you don't want them to um if you have a new token takes them the next cycle to understand your new token therefore token and the website code should be separate and into operate more in between and then we have privacy which simply means that the website should not know everything in your wallet it's not okay if you go to movie theater and now they know that how much ether you have or everything you have and finally there's security meaning that you cannot trust you cannot dedicate transactions to the websites let's say you have a Dominion token let's say you can lend it to a friend to get Discount Ticket you don't want the movie theater to do the landing because how do you know the movie theater website is doing it correctly and not taking away your your money or your your minion token so the transaction should be done by secure code that comes with token and with these four requirements satisfied we develop a our core technology called token script which is enabling technology for token to work as trust anchor I don't have um too much time to go to technical details but I will just discuss I will explain the design principles and the technical characteristics of token script so first it is strongly bound to contracts meaning that the users should trust the token token script as much as they trust contracts they are run they are written by the same team and there's cryptographic proof about their linkage and second token script is not executed on a blockchain they are executed used in the user's wallet in user's agent for example Alpha wallet which is our wallet product the leading open source wallet with more than 700 Fox it has a runtime environment for token script so that token code can write in the wallet and interact with the website and then it is composable meaning that you can use multiple tokens in one web process which is not often seen in today's steps so you can you can use one token for payment one token for Discount and and you can use the you can you can get a movie ticket as a new token download it from the website into a wallet it's composable on the front not only on the in the brand blockchain and finally it has both the UI and API and this is a [Music] this is different than most tokenization solutions so most organization Solutions today build tokens like API for example the graph you can inquire a token and get for example which the the data of creation for the token who is the current owner but it's crucial for token to be a trust anchor is crucial that tokens have user interface if you go to a movie theater website login as Google you will hesitate if it doesn't display the Google logo or if the login box deviate from the Google experience similarly for if you want to use the minion token on the movie website token has to control the part of the UI that interacts with the website so you see it's essential that token generate UI in a trusted environment so um we have eight minutes left and um for my talk and then I have finished sharing all my knowledge but I want to pause here and then let all do a thought experiment we have been living in the world enabled by blockchain Center points for so long it's actually hard to imagine what internet could look like if we did not have to depend on the center points for example if you have a Tesla car they have integration with Google with the with apple wallet Tesla car key can be an apple wallet and you can send it to a friend if he wants to borrow your car you do this on iOS as long as you're in the same ecosystem this is the organization it's done in the iOS system it's not down on the blockchain but can you can do this now today but can you flow to your Tesla car on the rental market by ours through the same technology you cannot do this today because apple is not allowing it I don't know why they probably don't see it profitable or probably it's too complicated but you need Apple's permission to do this what internet would have looked like if you didn't have to depend on Google to permit you I put permission to do this you would be able to set up a rental market very easily as long as tokens support them and the Smart Talk Smart Car vendors will produce their own Smart Car tokens to allow this to happen you can rent a market in Bogota or any sleep City you don't have to be a world leader to operate in rental market that would that that is I'm not getting it what is the license of the code base like is it 2.4 sarpash or something um license okay um so our implementation is in a MIT license so that's why we have uh that actually right now it's embedded in Alpha wallet and it's not a standalone product we were making a standard balloon product so right now Alpha wallet has 700 forks and most of them didn't tell me about it because we allow MIT yeah yeah hello yeah hello how are you I was going to ask what do you think are the main barriers for tokenizing traditional banking or Finance products such as portfolio purchases and what do you think would be the position of traditional banking system towards this kind of Innovations if history is proven true again they will be the last wave to be tokenized so so they will be under a lot of pressure and eventually move there as as if you think yes how do we know which token providers can we trust because oh that's that's we don't know the the thing is um uh although token can be acted although token is used that trust anchor to enable web search services that doesn't require users trust them as much as token token itself needs to be trusted and the level it can be trusted is actually again free market competition what City on the calendar how would they know that this that this uh company uh isn't let's say a phishing scam and I actually do not don't know I imagine that the wallet will will have a calendar integration so if you enable wallet calendar integration the wallet will create a calendar token on your neck on your behalf on your name and then but whether or not other people will believe that you will actually realize your promise if you sell your calendar item to other one that would require summer tests or some sort of form of testation like you have reputation stuff like that we are building these things but I don't know how it would look in the end yeah well thank you so much thank you for the presentation thank you so it's a nice Marketplace and I would like to take your contact details um yeah yes I would love to provide them and meanwhile we have some teammates here who is working on marketing side [Music] [Applause] [Music] [Music] [Music] hahaha [Music] [Applause] foreign [Music] [Applause] happy birthday [Music] foreign [Music] foreign [Applause] [Music] [Applause] [Music] oh [Applause] [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] no no no no no no no no no no no no no no no no [Applause] [Music] [Applause] [Music] [Applause] [Music] foreign [Music] [Applause] [Music] [Applause] [Music] [Applause] [Music] I don't know [Music] [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Applause] [Music] thank you [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Music] foreign [Music] guys this is working yes I'm from reflexor Dao and I'm here to invite you guys to join us in Orion so who in the audience have heard about uh die raise your hands nice so when the audience has heard about Rye nice and who has used right up to now yeah a few people we need to work more on that I guess so Rai is a fork of maker Dao dying so with a bunch of differences it has eth only as collateral it has a floating Peg so it's not one USD as back to one USD as dies it's self-stabilizing with a pi controller and it has minimal governance too that's what we call in governance and we're pretty sure this is the future of Finance so here we have a quote by Nikolai he's the technical co-founder of maker and the main architect or the main head behind die and I'll actually read his his quote because it's very relevant to the presentation until this is in textbooks as a historical turning point for theory of Central Banking we're still early so yeah suffice to say we're we're still early so before uh I go into what the system is and how it works I'll tell you guys uh a story more of a personal story actually so let's do the interactive thing again and please raise your hands if you have lived in a place with inflation over a hundred percent a year yeah there's quite a few it's latam right so that's great kudos to the yeah for making the event here so keep your hands up if you've lived through over a thousand percent a year so yeah a few of them right two thousand three thousand Good Times so I'm Brazilian and as a kid I remembered uh when my family got you know their paycheck we would run to the store and buy stuff because if you wait until the weekend next day you'd pay you know you got less stuff for your money basically prices were raising every day in a peak inflation in Brazil we had uh over three thousand percent yearly inflation between 93 and 94 uh the average yearly inflation was over two thousand percent so here we have the roller coaster that it was right it wasn't like really smooth uh after the military dictatorship uh in Brazil uh from the 60s till mid 80s they left with a really messy economy so all the leaders that came in they came in with a plan basically the focus was you know how do we deal with inflation some of those plans were disastrous right they involved freezing prices freezing wages in this one the color plan they actually seized people's savings it never gave it back so it was a roller coaster yep this is actually new to me I learned this while preparing to the presentation here we have a graph of the number of people in the monetary Council of Brazil or the guys you know setting rates and the inflation in Red so you see how they correlate right so the less people you have it seems to be better right so yeah and this sharp decline in the inflation was in 94. it was called Plano it was really interesting I mean they did some obvious things right cut government spending uh stop printing money right and they also did one thing that was not really obvious they created another unit of value so things were priced in this new unit that was called urv or when he died or unit of real value but they were still paid in the currency of the time that was the what's it called crusadero and after a while once people were you know free from the inflation mindset urv became the Brazilian Herald that we have to this day this plan was mostly successful so we went from over two thousand percent inflation to mostly single digits since then so now we think okay the US is exempt from this right so far so good but uh yeah actually they seem to be losing control there so during the covet years they printed 30 percent of the money supply and they seem to have underestimated the resulting inflation from it so they took long to act and now they seem to be overreacting throwing the whole world into a recession right let me and now going back to crypto so we have like a speed run of what we're talking about right so oops yep so we had here the Luna uh UST you know Fiasco graph basically uh let's do another one raise your hands if you lost money in Luna UST so yeah lots of people and yeah please the other ones should pay a beer you know to those guys so yeah I know it was painful so vitalik wrote a an article recently about stable coins just after the Fiasco right comparing Rye to UST and the main takeaway for me at least was that if your stable coin cannot contract if if it cannot wind down it's likely a Ponzi right so here we have you know the the graph showing you know when Luna or UST had to win down for the first time it was ugly 50 million was erased from the market in a so that's uh billion with a B right so lots of money uh now think about what happens if the USD has to contract the USD with a d right how would it work so but I'll leave this you know as an exercise for you guys to think about this so here having lived through hyperinflation I can't help but wonder you know how do we get ourselves out of this Dark Age so these are some of the questions we were obsessed when building rhyme so what if the humans are the actual main source of instability what if our currency was honest what if your currency was not a Ponzi what if our money supply automatically expanded and contracted what if proper Central Banking has never been tried so here's a picture of Milton Friedman he's a famous economist and we're trying to achieve you know his his vision here so back to what is right we will now go through you know not that you guys know why I care so much about this thing uh we will go through and explain each one of these bullets to you guys so like die rise over collateralized you open a collateralize that position by depositing eth and minting right we call it a safe it's not to be confused with hypnosis safe you know so the die CDP is a safe for us if our collateral ratio drops below the minimum 135 percent your East will go to a liquidation auction just like die we have no SM there so you haven't one time one hour time delay to react in sharp you know price changes and like mkr we have flx so that works as a domain backstop if liquidations fail and Rye is backed by eth only this is very very important because centralized collaterals have a way of taking over if you're not careful dye is now generated from around 80 percent centralized collateral the vast majority being usdc so we all know after the tornado cash events recently that you know they can freeze uh any holder including you know pools and systems if that happens you know die holders will suffer die will lose most of the value by only backing Rye with eth we want right to inherit the money-ness that eth has earned we also made the decision you know to prevent sort of like this actually prevents Rye from really scaling right can only grow so much you know and we expect that it will grow as it grows in in value too I'll get back to this uh towards the end of the the presentation so people often ask us you know how is it stable if it's not one USD so the way we see this it's more like um like a foreign exchange like a foreign currency right so here we have the exchange rates of several currencies the largest ones against USD and the Rye Redemption price is the smoother line in green there so as you can see it's not much different than well foreign exchange we're doing about as good as China there so packed stable coins have a big problem also highlighted by vitalik they're either pegged when everything is okay you know or they're off bag and everyone is freaking out you know what the is going on someone's censoring me here so so right has one mode it's always off pack you know that's business as usual it's a matter of degree so we're not getting out of bed because it's soft pack right see how cozy it is Pepe is and here we go to the pi controller that's the heart of the system the coolest part too so we have a graph here the market price from uniswap curve is the red line over there Redemption price is the smoother line in Gray so when the market price goes above the Redemption price or Redemption price is basically the internal system Peg right when the market price goes above the Redemption price the Redemption rate goes down and starts pushing the Redemption price down the opposite is also true so if the Redemption price or if the market price goes below the Redemption price the Redemption rate will go up and here below we have actually the rates the Redemption rate is formed by two rates we have the P rate in blue and The Irate in Orange the period in blue is instantaneous so when the price market price crosses the Redemption price it will immediately cross to the other side we can see recently in the recent days it's been the first time in a while where the market price went below the Redemption price and the period immediately goes positive there The Irate starts the error of the last 30 days so it's like a memory it makes the controller stickier this is why it's not yet positive right The Irate is a lot higher than the or a lot higher to the negative side than the P rate there so stability for Rye means balancing right supply and demand and not begging it to one USD so with Rye you know where the yield is coming from when rates are negative right holders are paying right borrowers when rates are positive right borrowers are paying right holders this value transfer between right holders and Borrowers is mediated by the pi controller we saw previously and they the controller actually incentivizes the balancing the supply and demand for Rye so we have a saying that the money God always wins so we say this because it's pointless to fight against the money gun if you push against the controller the rates increases potentially against you and you burn money basically you don't want the angry money God against you so you want to keep him happy if you help him work though you can make money out of it now on to ungovernance that's what we call you know minimal governance so the system has been controlled by the community since July this year when we started we started with the usual team multi-sig and we completely transitioned to the Dow now so the team multisig is no longer active and has no control of the system but before doing that we actually locked down pretty much everything you know to prevent governance from being used as an attack Factor so unlike in die where the governance can do whatever it pleases in right governance cannot print right cannot print flx cannot steal collateral and so on so the only things it does right now or it can do right now is govern external dependencies like oracles like saviors and some of the parameters the system has I mean some parameters we felt we were not ready to set in stone like the pi controller parameters everyone's still learning so the community can't govern them through a vote I mean so yeah and now let's do another interactive thing raise your hand if you hate meetings well very few people hate meetings here surprised so we believe Rye represents the future of Finance central banks in the future will be open source programs the cost of operating a central bank will drop by a thousand times even more you know they were transparently and autonomously so the way the Fed works today is like maker down they get together in meetings and they decide on rates as autistic people we know you don't need meetings to set rates one day everyone else will learn this too you know today we treat currency management as a holy art that only some Arcane Wizards can perform in truth the more we can demonstrate simplicity the more we can dispel the Illusions around how money should work we believe in a future where communities can take ownership of their own money where the cost of deploying and maintaining a currency is as easy as the deploying you know some open source code so as amine said there we can stay autistic longer than can stay thanks so how can you get involved suck so if Lords you guys should join us right if you have eth as a collateral in a system that has risk from centralized uh collaterals you should move right deposit is like borrow rice pended you know long Rye short Rye do basically work with the money God this is a tweet interesting one from cure and I uh it talks about a position vitalik has in the system so vitalik in the recent months with the negative rates he made 75 000 by shorting right so he's helping the money God and making money you guys should maybe try it too you know uh also join us on Discord meme the dream right help us write explainer content uh there's a really interesting research Channel where people discuss monetary policy control theory and whatnot uh enlist also in our Dev Reserve Force we're building right now and there are plenty of bounties available there also not only technical you know the response is for everyone so uh check it out and lastly we're also building the money God League so what what do we call the money God League basically we know Rye won't scale so instead of envisioning one huge stable coin to rule them all we envision a future with thousands of ride-like systems so as you can see over here let me get off up front makers discussing becoming a ride like system now right abandoning the bags so they can get rid of usdc so maybe they join us in the future you know by getting back to the original Nikolai Vision there uh there's also H2O from Ocean protocol it's a fork of ride that is collateralized by their data tokens there's also a group working on Thai which is a multi-collateral version and maybe you should join us too you know if you have a good idea you want to build another flavor of the system come talk to me join our Discord we're happy to help you know the more systems out there the better right sort of like the Hydra vision of the thing we don't need a huge stable coin we need several smaller ones to contain failures too you know and allow for experimentation uh oops let's turn the Riot into a revolution right so this meme was made by Charlie Noe from Paradigm it's some true VC value add here so I'll leave this running here while I take any questions from you guys we still have eight minutes left so um so good morning Fabricio two questions could it be like a PID controller instead of just Pi controller and other could it be checking the price of it corrected by inflation or that's like impossible to do yeah uh interesting questions I mean we started with a p only controller actually now it's a pi I don't see why not could be a PID too so in its experimentation right there's still room for a lot of experimentation with Rye we try to make it like as pure as possible so only if minimal governance now with the forks you know I mean there's a ton of possibilities we see like Thai for example the guys explained that for a multi-collateral version you need more governance right to onboard new collaterals and their goal is of course to try to keep it to decentralized collaterals uh onto the other question I forgot what was the second one please remind me that price could be checked by corrected by inflation not just USD yeah so pegging is a true challenge right by having notepad I mean uh uh I went through the like the advantages of it so if you're pegging to an inflation index for example you have the same problems of a packed currency I mean it could be done but then the negative rates could not be applied to price as we do right they could maybe be applied to the stability fees right or the borrow fees people pay you know on their borrowed amount so yeah I guess it could you know if you feel this is a nice idea I guess you know come join the money God league and yeah good it working to help so hmm hi thank you since there is no Peg when does right fail and under what scenarios yeah that's what has happened what are the 30 months yeah that's uh hard to say I mean uh there's no Peg and when and I mean the there are a bunch of failure scenarios too right I mean we could as in the black Thursday you know maker fail to liquidate cdps this could happen uh regarding the controller you know there are some cases in control theory where the controllers go crazy too right you've seen like robots going out of control and spinning like crazy and then destroying yeah that's a PID controller failing that happens we would need to Global settle the system as maker did with Psy we had a proto-ri before right to we Global settled so if the system becomes unsustainable it can still be closed down and that everyone both uh safeholders safe owners and Rye holders can go back there and get their East back so it's not a complete loss but we have to keep an eye on this this is also an experiment right so we're trying to be conservative but I don't know in future iterations it could be that not on rye rice pretty much closed down but I mean on other money God League members this could happen I don't know we just have to keep an eye to see if you know controller is not going crazy on us thank you yep um this summer during the uh over here in front of you dear right hello okay um this summer when it was um when the market and Redemption price were different for like months and we were seeing like 30 different rates it I mean um it kind of struck me that um why can't the system tolerate some sort of steady state error where market and Redemption price are just like different for a while instead of the eye just punishing us more and more and more yeah that that's actually a good point uh this happened at the time we're not sure why you know like Nikolai claimed we were being attacked by some actor you know trying to prove us wrong uh some people think you know because of the UST Fiasco then because of the tornado cash thing you know a bunch of people Market bought right and then the money got got mad you know and punished everyone and vitalik made a bunch of money that's the so yeah we're still discussing this there's actually there will be a proposal to tweak this we do feel the I term right now is too strong maybe it could be punishing less you know they make it less sticky so yeah please join the discussion you'll like it uh there will be a discussion in the Forum soon and until the end of the year we will tweak the perms we try not to tweak him really like with a lot of with uh I mean too quickly so we see the results you know we see the thing how it plays out but yeah there will be one change in this year uh it's been like that for over six months now so and it I mean it withstanded some pretty big shocks in between you know we switched from uh Raging Bull Run to a bear market right and then things collapsed and that there were I mean people tried started valuing decentralization more so uh yeah I guess let's see if next year is more stable too with new params and we will see the idea is to tweak less and less and and on a smoother way you know like uh until we can set the programs in stone and just ungovern the controller too so what what are the incentives for executing liquidations and maintaining the the system Health um like for example like maker has sdking uh you know auction where like Ave you can have you'll find out there's a lot of a lot of people that should be liquidated that are not liquidated but maker everybody eventually gets liquidated so what's the incentives and how is that maintained and so what Telemetry do you have into to see what needs to be liquidated so when we started the collateral auction house was a fixed discount collateral auction house so it provided a fixed discount on it you know against the market price then after six months or so even before maker liquidations 2.0 we started with an increasing discount auction so it's pretty similar to maker I mean the implementation is completely different but the the mechanism is the same and we're decentralization freaks too so every call for Keepers in the system is incentivized somehow uh actually the system right now for the keepers out there is grossly overpaying calls so you can make a lot of money by helping the system uh there will be a proposal soon to lower the rewards because they were adjusted for gas prices from the bull run too and now the network is you know a lot I mean block space is cheaper so we can lower the rewards and you know the system can save money until it's needed to raise them again for other things I mean for everything there's an incentive there I mean it depends on the call and on the on the on what you're doing so hello um what's the point of using a rye instead of straight eat if it's free floating and it's just backed by eth yeah Rye is supposed to I mean it's free-floating but it's uh way less volatile than eth so while it's it is free-floating it is a stable coin right it's kind of like the Euro against the USD you know of course it's gonna oscillate right tomorrow it's gonna have a different price but it's not gonna go crazy on you and crash 50 from today to tomorrow right so right is a very dampened version of eth basically and we early on it's actually quite interesting because we had like we didn't want to call it a stable coin because everyone called stable coins you know the one USD thing and now we sort of we're sort of reclaiming the name so we feel right is the only true stable coin and all the other ones are what we call peggies or pegged coins you know so yeah that's it so if you need a stable client that retains you know the moneyness of it that's right that's the goal of it yep as the money God grows and adoption grows as a currency what do you see or what do you expect from central banks and other institutions trying to corrupt or crack down on coins like right uh so yeah the more decentralized you are the harder it is for them right so we're taking our precautions from day one uh the idea is to have it you know almost trustless in the future so yeah they can try to stop ethereum I mean it's gonna be hard right and yeah our main goal is actually you know uh some Central Banking Bankers are actually noticing this so we feel that Central Bankers should not be setting rates right because they're people and they're like subject to politicians you know making pressure and influencing them so in the future as we see you know they need to be very predictable on setting rates so in the future as we see they will have a software right might not be ours might be theirs but you know they will have a set of you know an algorithm setting rates and then Central Bankers instead of monthly setting rates they will just tweak the parameters you know until they perfect that their their own controller right yeah isn't there a danger though in where there will be Crackdown let's say on the use case and then the attention towards stable coins like rise will just yeah I mean there is I mean everyone is noticing this right uh we feel rise last in line though uh the risk is there I mean I agree with you but for example if if you look at die they have usdc inside so you know they're one action from circle from being frozen uh they also have you know a huge governance thing going with lots of people uh that can tweak and that can do many things in the system we prevented that so we try to close these doors there's also another thing right when you look at stablecoin regulation we're lucky that Regulators think stable coins are all packed to one so I think they're overlooking still you know like all right works and how stable it is so yeah I guess this is going to happen eventually but we feel rise the most resistant of all you know and we're keen to improve it too you know with new designs new iterations of it so that's amazing yeah okay thank you thank you so much for your presentation it's a great presentation foreign [Music] all right [Music] boys [Music] [Music] all right [Music] foreign [Music] [Music] foreign our next speaker is Stephen Valerie from heavy who will be speaking of g-a-o thank you and welcome to the snapcom all right there we go hot start uh what's going on everyone my name is Steven Valeri I am a smart contract Dev at the Ave companies um I've been there for about a year and a half have worked on um some things like our uh cross chain governance Bridge our V3 deployment and most recently uh have been spending time working on uh our stablecoin project go and I'm really excited to tell you guys all about that but before I get into go I kind of want to take a step back and walk through the current stablecoin landscape and in doing that I think that will give some helpful context as to you know what is go and how it fits into that broader ecosystem um and I don't know if who else was here for the previous talk um but it was a great talk about Rye which is a stable coin um and I think it perfectly kind of highlights why understanding the landscape is important because what the heck is a stable coin there's so many things that fall under this definition um that it creates confusion it creates risk um and you know just saying that you're working with a stable coin isn't sufficient to understand what the heck type of asset you're working with um so let's try to answer some basic questions around stable coins and I think we'll pretty quickly identify uh why it's so challenging to understand what people are talking about so if we could go to the next slide here do I have a quicker oh there's a clicker nice sweet um so what is a stable coin um I think at the broadest level you could say a stable coin is an asset that attempts to maintain a stable price um I think very commonly we're seeing Peg stable coins um so they're pegged to some other assets value most commonly I think and most dominant in the market is US dollar Peg stable coins um but we're also starting to see stable coins that are pegged to other FX uh tokens um and there's also tokens that just aren't pegged so again rye as an example um it's a stable coin but it's not pegged to a specific price um so it's a token that aims to have less volatility but I think generally at this point if someone were to say yeah I traded into a bunch of stables you would probably assume that they're talking about a US dollar pegged stable coin so uh how does the stable coin work we answered one question of what is a stable coin and I think here at a second question it's pretty obvious that there's not really a straightforward answer it depends it's totally different depending on the type of asset that you're working with um and you know this creates some challenges and presents risks right if you just assume that the token that you're using is in fact stable um and without understanding how the underlying stability mechanism works or other risks that are present in that asset you know you may be taking on far more risk in your portfolio or whatever you're trying to do than you're intending um so yeah again just these things are so different you can't compare apples and oranges um you know to the you know users here who know it dies usdcr if you were to ask them are these the same people would be like no very different um and I thought this analogy played very well and I've just realized that I'm going to in fact be comparing stable coins throughout this presentation so maybe not the best analogy um so let's take a look at the current landscape um as you can see here the one-to-one backed centralized stable coins are totally dominant uh you can see tether usdc busd um are in the tens of billions in terms of market cap um just kind of totally Far and Away dominant um next in line is die so dye is a decentralized over collateralized cryptocurrency backed stablecoin um and that's really the the next leader in the pack um and definitely the leader in the pack in terms of this type of asset um you know there are other types there are other Assets in this category um like MIM magic internet money uh and then go uh aves uh Native stablecoin which were um developing will fit into this category as well um also here there's other stable coins that have uh much smaller market caps but are using new stability mechanisms things like fracks things like Rye which we'll talk about as well um and these things are just so different um that you know even having them in the same category presents challenges and talking about them and comparing them um you know and trying to use them for the same purpose um so one other thing I'd like to call out here and I think that's probably the biggest example of why this stuff is important is an asset that's not included here which is uh UST so the stable coin associated with the terror project it was approximately 18 billion dollars worth of value at its top market cap that has essentially disappeared right and that is comprised of users and people it not really double clicking into what is this stable coin how does it work what are the risks how does it compare to other stable coins um so I you know I think that's kind of the ultimate example of why we're even having this conversation around you know well what really is a stable coin so we've talked about those top three um they're one type of stable coin which is a centralized one-to-one backed stable coin uh examples are usdc usdt busdt-d true USD in this case they are generally Assets in which you trust a centralized institution to custody in the case of a US dollar Peg stablecoin one US dollar per each stablecoin that they Mint or issue there's caveats around this though right you're trusting them to uh maybe hold about one uh dollars worth of Assets in their reserve for each thing that they meant uh and you're you're also in some cases trusting them to hold about one dollars worth of US dollar equivalents um so again you know per each of these institutions um and their processes and how they work you know kind of double clicking into what this asset's actually backed by is it one to one what are they doing are they using treasuries um is important um so here we can highlight some of the risks of working with an asset like this um you know obviously it being centralized is to some extent counter to the general decentralized ethos of blockchain environments um and in having this trusted entity we take on risk right we're trusting this entity to be managing their reserves appropriately right like they aren't just investing all of the reserves into risky assets um so that's very important uh to mitigate this some companies are using audit firms so center with circle they hire Grant Thornton top five accounting firm in the world to publish uh reports to give users confidence that they have reserves of US dollars US dollar equivalents that are equal to or more than the amount of value they've issued on chains another risk you're taking with this trusted entity is just generally um you know how do they function as a business you could think of things like I.T security right who has access within their systems to make transfers for their reserves this is something you know kind of basic but very important and then I.T security specific to web 3 who has access and how do they manage their wallets that can make really important transactions or freeze users um oh one other point on the um Reserve ecosystem and getting confidence that these reserves are backed true USD instead of having an auditor has introduced an or a solution with chain link called proof of reserves in which they have oracles that report the status of all of their accounts so by doing that the system can never mint uh more than you know the reserves that they have so that's another interesting solution and you know attempt to give users confidence that these assets are in fact backed and redeemable um another risk here entrusting this institution and entity is that uh these these stable coins often have the ability to freeze uh and address or it could also be a pool um and in doing that you know you could potentially lose access to your funds um at the discretion of these institutions um you know of course there are times when this makes sense right in the most recent BNB hack um tether was quite early to freezing the hackers funds but you know you're dependent on this institution making the appropriate decisions in the future which you might be hesitant to do so the next type uh decentralized over collateralized crypto-backed stable coins the most obvious example of this is dye there's also magic internet money um each of these tokens gets minted that's minted is backed by uh crypto collateral this can be you know ethereum Bitcoin other one-to-one stable coins um and when they in order to Mint them you need to deposit your collateral to a Smart contract so this is a clear difference from the previous example this smart contract is on chain you can clearly see and have confidence that the smart contract holds enough assets to collateralize the stable coin which has been minted um you also have confidence around the rules in which the system works the it's controlled by a smart contract um which uh uh is open source software right you can see the code you know how the reserves will be handled so you know you have confidence around um how that will work um in addition to the smart contract in some cases you have certain parameters within the system that can be configurable or it could in fact be an immutable smart contract so once the system's deployed there's no configurations to change I think more commonly most of these systems have a few configuration variables at least um and these are often set by Dows which is a decentralized set of token holders hopefully incentivized to maintain the stable coins Peg and this encourages and and helps uh de-risk the fact that and and not be dependent on one single entity uh for the stability of the system that being said these also have a lot of risks um just because you move away from some of those centralized concerns um there's still risks with this different type of asset um so first of all there's the stability mechanism how does this token maintain its Peg so some common ways in which this is done uh is an oracle within the system that's responsible for telling the system how much this stable coin is worth gets pegged to one dollar and that's regardless of whether the market price is you know above one dollar less than one dollar and by doing that it creates Arbitrage opportunities in which uh which helps stabilize the um token and if we have time later we'll talk through some examples of that um additionally interest rates can be used so you can help manage the supply of the asset based on interest rates with high interest rates people will be paying back their positions which will decrease the supply impacting the price same with low interest rates if you have low interest rates more people will be interested in taking out positions increasing the supply again impacting price another risk here is the collateral that's being used for this decentralized cryptocurrency backed stable coin um the stability of the token and you know it's collateral is dependent on the value of its collateral so if a um in some cases those all the risks of the collateral come through uh and are present within the stable coin so for example we said we've de-risked these types of coins by removing the centralized entity and how the system is controlled but that being said well there's collateral um that backs a decentralized stable coin um which is a one-to-one centralized stable coin um you know you're still exposed to those risks of how that centralized entity is managing um their stable coin Additionally you generally don't want volatile assets you would like assets that are relatively stable in order to minimize the amount of liquidations that are required and reduce the possibility of having under collateralized positions within the system I think that gets to flash crashes um again around the price of your stable coin when you have an over collateralized stable coin you need the collateral to be worth more than um your the amount that you have minted uh over time generally if assets if the price of assets decreases uh that are your collateral that's okay there's liquidation processes in place in which someone can come they'll pay off your debt and they get your collateral and the collateral is worth more than your debt so they make out a little bit of extra money really where a risk comes into play is if the price of your asset crashes the collateral asset crashes so quickly that Liquidators do not have time to perform those Liquidations in that case you result in under collateralized positions which likely result in a deepeg of your coin um again we talked about you know there's different aspects of these protocols that are configurable so making sure that a it's configured appropriately and the per the person or the entity or Dao who is configuring them is knowledgeable in the space and is making informed decisions is important and then finally there's smart contract risk right there's always a risk that there's going to be a bug in the code you can do as much as you can to try to eliminate and mitigate this risk from responsible development practices internally internal reviews external audits which I think we see throughout the space [Music] yeah so you know you can do as much as you can to try to reduce the risk of smart contract bugs but at the end of the day they'll exist in the space so briefly other types of stable coins I think we already can see there's a huge difference between one to one backed centralized stable coins decentralized over collateralized stable coins and now a couple of others that I hesitate to even try to name what type they are because they're so different and function you know very interestingly um so one is frax frax is a stable coin that is partially backed by collateral and then also partially maintains its stability through algorithmically um so you know obviously that's quite different than a Dye or usdc or you could look at Rye which is a stable coin that is not pegged to any specific asset and its stability mechanism Works through understanding the reduction rate so how often are people taking out positions how often are people repaying their positions and based on the rate under which that is occurring the price of the asset will slowly vary one way or the other um and you know this isn't to give you you know a full understanding of what these systems are and how they work but it's to say these are totally different types of assets that we're talking about in the same category of stable coins and you know now if you go to think about stable coins or work with a stable coin you should say what is the stability mechanism how does this actually work is it Peg does it not pegged it's more to you know kind of promote that thinking of what what are the risks of this coin and is it actually going to maintain its stability cool um so now I get to talk a little bit about go which falls into that category of decentralized over collateralized crypto-backed stable coins and it will fit natively into the Ave Market so so uh uh one thing we recognize that Ave looking at are existing liquidity pool uh protocol was that there's a lot of functionality in our General market that would be required for this type of stable coin you know things like depositing collateral things like liquidation processes these things exist within the Ave protocol and also are exactly what you need to facilitate a decentralized stable coin so I think that's something that's really nice even on the development side is that go fits into the existing Ave market so a lot of the processes and you know ways that you will interact with it should be very familiar to interacting with go it's really a new custom asset that's getting added to the market um so you know while there are differences in the implementation and we'll go through those you know in some detail um at a high level when you go to interact with Ave and borrow go uh it's pretty much the same as borrowing another asset you deposit your collateral you borrow go uh you hold it for some amount of time it accrues interest and eventually you pay it back so that's pretty much the same process as any other asset um that being said when it gets added the way that this asset is configured has differences and the differences are what help it maintain its stability um so so let's walk through those um the first is that this asset isn't supplied to the market so if you think about borrowing some other asset you know if you deposit with and borrow wrapped Bitcoin somebody else has to have deposited that wrap Bitcoin in order for you to later borrow it um in terms of go that's not required when you deposit your rap deth and borrow go nobody has to have deposited it instead the protocol reaches out to the go contract and mince that go on demand um same goes for when you repay that go is actually burned rather than going back to the suppliers the Oracle price is fixed to one dollar so of course other assets the Oracle reports the actual market price so that the protocol knows you know how much value is this asset how much can you borrow against it you know maintaining that over collateralization in this case go is pegged to oneusd despite what the market price is so this helps maintain stability it creates Arbitrage opportunities say the price is above one dollar in the market when the protocol thinks it's just one dollar you can mint a bunch of go uh you know that is debt at the cost of a dollar you can sell it at the market say for a dollar and five cents that increases Supply which should reduce the price and then eventually you can buy back that go at one dollar pay back your dollar debt and you know five cents on the dollar is arbitrage so uh the next stability mechanism and difference in how this asset works is in its interest rates normal assets uh the interest rates are based on the utilization of a pool so you know if you have a hundred tokens supplied and 10 of them borrowed the interest rate will be relatively low if you have a hundred tokens supplied and 90 assets borrowed the interest rate will be quite high in this case there are no suppliers so interest rates work differently to start interest rates will be set by governance and again this serves as a stability mechanism you can increase Supply by reducing interest rates and the inverse holds true as well um another difference to highlight is that interest that accrues on positions will be repaid to the Ave governance so rather than having to pay suppliers because there are no suppliers that interest can be redirected um some other differentiating factors for this token is that discounts are available to users who stake Ave in the safety module so you know if governance has set the interest rate to three percent by staking Ave you can get some discount on that interest um there's also multicolateral positions so you have the flexibility to [Music] um use different assets as collateral within the same position this is different than dive where you have a single Vault you have one collateral type so this introduces a little bit more flexibility for some use cases that might make sense where you know if you need to re-up your collateral you don't necessarily need to swap between assets to fund that position and then thirdly you can earn interest on these positions so by supplying your collateral to the Ave protocol other users will be borrowing it and you'll earn interest on that which effectively reduces the interest that you're paying on your borrowed positions um we highlighted in a lot of the earlier part of this talk was around risks right so let's talk through what are the risks here and how are they mitigated in the Ave Market um so collateral Asset Risk um the Ave protocol has a pretty strong history of being relatively conservative and effective in managing the assets that are used for collateral and you know setting the configurations around those so making sure you have safe uh over collateralization ratios Etc similar goes to the flash crash risk this risk exists but we've seen that the Ave protocol is a relatively resilient Market that has gone through pretty turbulent uh times already again protocol configurations pretty similar to Collateral Asset Risk where you know we work with the Dow the Dow works with gauntlet um and it was relatively conservative um there's the smart contract risks and stability mechanisms I think we've covered those enough so when go uh soon TM there's a few dependencies in governance right now including deploying the V3 Market whether we'll be upgrading the V2 Market in place or deploying a new market for V3 so there's some dependencies around that which will uh as soon as those are done we'll move we'll look to move forward with go and goes future um you know we're hopeful that uh we can move go to l2s I think we think there's a really large opportunity there with lower fees um to you know have this as a token that's used widely both by crypto native users and um you know more of the general population another interesting functionality is that we have facilitators so um go Ave is the first facilitator of go which means that they have permission to Mint um more go and in the future Ave government I'm getting the times up to talk fast Ave governance can add additional facilitators which will have the ability to uh mint go uh if they are approved so obviously there's a lot that goes into that a lot of reviewing I think that will probably be more long-term but something cool to look forward to that's it thank you guys thank you thank you so much Stephen IE company okay okay yes [Music] [Music] we're co-founders of mobile labs [Music] [Music] foreign [Music] [Music] [Music] all right guys [Music] [Music] okay [Music] [Music] [Music] all right [Music] boys [Music] hello guys [Music] [Music] thank you [Music] hello again our next speakers are Colleen Meyer and always in kind co-founders of labs who will talk about how to design DVT while ensuring no correlation welcome to Dev Concepts [Applause] um good morning everyone uh happy day three of Devcon six um before we get started today I just want to send a huge shout out to all the organizers um all the people that came from all over the world uh and everyone who is at this talk today so appreciate it so today um I'm Colin Myers I'm co-founder of mobile Labs with ocean and today we are going to discuss designing DVT for non-correlation um so before we get started who is oval labs uh best way to think of us is that we're an R D team we're focused on building a different ethereum infrastructure Technologies for staking uh today the core team consists of 16 members across eight countries so we're quite distributed um right now our main focus is delivering distributed validator technology for ethereum uh as a primary Network to begin with so before we dive a little bit deeper in today let's kind of talk about just ethereum as a whole and kind of the the road map of scalability so a lot of it we just had the merge and if anyone went to the opening ceremonies you know there's this ossification discussion happening which is essentially like hardening the things that we've put together the things that we've created and what we see here the three main areas that we've been focusing on um over the past few years to scale ethereum into its next chapter so on the left here the first area is the execution layer this is the most mature of all the different scalability Technologies and research studies that have been done over the years cryptographic Primitives that are used for this problem are optimistic in ZK Roll-Ups this comes through in the form of Layer Two um it's actually been quite interesting this week to see that layer two is taking over um as they should and Layer Two honestly in my eyes seems to be the future of of what ethereum will build out of from a research perspective the second area ties to layer or two and as it matures more we are getting in front of the next problem which is the data layer so the data layer must also scale while execution scales common methodologies used for this is called Data availability sampling and and here are some examples of some few projects that are focusing on that the third piece which is kind of the most immature and not only like research and adoption and implementation is scaling the consensus layer like scaling consensus to date has basically been getting to Genesis and then getting to the merge and now that we're in this post merge World we're all kind of re-evaluating the consensus layer and saying how do we scale it more today what we're using is DVT as a cryptographic primitive and protocol to help scale the consensus layer so now I'm going to take a million to describe what distributed validator technology is and a bit more words so at its most simple a distributed validator is an ethereum validator that runs across multiple machines and why you might want to do this is to protects you from a lot of Technical and social risks from validation and one of the first ones is software issues if you're running just one node and just one software client you know software generally doesn't like Run Forever without issue and similarly I'm running a distributed validator makes you more protected from Hardware risks as everyone kind of expects servers don't last forever and they die regularly enough um going one bit further if you take a distributed validator and you run it with other people so each person only has you know a subset of the private key that makes private key much more difficult uh privately compromised much more difficult because you have to compromise three or more different operators to get the full private key to you know flash or do whatever with it and the last one is a bit more abstract but it's the idea of if your validator goes Haywire they become a bad actor they've been hacked remote code execution whatever you'd like um generally you have no Safeguard if you just have all of your money with one validator it's just you're in trouble but if you have your money in a distributed validator and you have protection so long it's only a subset of validators you know become Byzantine or go away well or whatever you want to call it um another thing that's super important is to talk about who benefit from DVT and the great thing about it if we think more or less everyone involved in validation can benefit from it and the liquid staking providers themselves they at the moment are trusting huge amounts of stake and to single operators and they're just kind of crossing their fingers and praying that nothing bad happens they would much prefer to be able to do to divide their stake across operator and then if you know anything bad happens to one there's you know no risk or no loss to the liquid staking protocol similarly the centralized providers can gain a lot from using distributed validator technology I had the pleasure of building block Demon's eth2 staking platform um at Genesis and I know that you know you could reduce um devops risk of you know you have to wake them up in the middle of the night on a machine dive if you know you have a distributed validator similarly you can reduce your Hardware costs because most people put a very small amount of validators for any given machine because if it goes offline there's nothing you can do but if you have a fault tolerance validator you can start to safely increase the amount of keys per node and that can reduce your operating costs and then on the more extreme or like the further outer side of things a lot of centralized operators either ensure they're staking and slashing either unbalance sheet or they go to a third-party provider for insurance and this is currently extremely expensive and not even every provider offers it because the thing to ensure and slashing is you know potentially you have to pay back everyone like all of that stake so if you have an operator with hundreds of millions of dollars and an insurer has to potentially pay all of that stake you're going to have a really high premium but if you were instead running in a distributed validator and you know insurer says okay what's going to happen if they get compromised the um you know the worst case scenario is much lower and as a result the premium is hopefully going to be much lower the next thing I want to do is want to have Colin talk a bit about the market for distributed validators cool so how is stake accumulating um these were taken from uh Elias from raided uh his liquid staking dashboard on Dune um data in this area for for those of you watching and who were looking at data this is probably some of the best data on the status of pools and how they've accumulated over time and I would highly recommend it um so where are we at today this chart here starts on the left at Genesis which was December 2020 and it shows the the dramatic increase in pooled assets as a percentage of total staked assets over time so as we can see it's it's kind of quite frankly like you know exponentially growing today numbers where are we there's about 6 million each locked in pools that equates to around eight billion dollars today and it represents nearly 37 percent of stake in the network so the 37 percent of stake in the network is like probably the most important thing I want to touch on as it ties to like what our thesis is which ties to how we believe DVT should be designed um we believe that over the next 12 to 18 months that pooling will become dominant uh inside of most networks especially ethereum and we suspect that more than two-thirds of stake in the near future uh will be inside of pools DBT is uh today on the focal road map of pools to help decentralize and to help build themselves out therefore we believe that DVT saturation inside of these pools will also end up being 75 plus lastly this is important because DBT is ultimately a security protocol it comes in a middleware however it's based upon security so different than Mev boost right you use Mev boost to get more uh with DVT you use it to protect the largest reward which is consensus Rewards um we believe that DBT adoption will be kind of a winner take all Market due to its like security premises um and the purpose of what it's used for so to kind of set the tone before we go into correlation here I just wanted to to give a size of the market our expectations of how DVT will sit inside of pools and then how large pools would be as a percentage of total stake uh therefore if if this thesis does play out it's incredibly important that DVT stays helpful at scale instead of becoming more correlated and unhelpful so now I will spend a little bit of time on slashing uh what it is and what it isn't so slashing um the best way to think of slashing for for those of you as if it's a New Concept is uh slashing is like the defense mechanism or the immune system uh of proof of stake networks uh and what it's here to do it's it's punished it's here to punish you for not following the rules in an attempt to keep the network credible and to follow the rules of the protocol uh today there's two primary ways to get slashed the first one is a double sign back in proof of work this was called a double spend and this has been mostly predominantly um what the top slashing event in the network to date has been through and we'll get into like how how that's happened uh the second one is a bit more abstract um best way to think of it is walking back a slot height that you previously said was finalized now a more layman's term way of describing this is uh in the future you try to change something that you said was true in the past so therefore you're trying to like change State uh inside of the network and if you do that you will be slashed for it inside of slashing there are different levels in degrees of slashing there is correlated slashing and uncorrelated slashing uncorrelated slashing today is predominantly that the most of what's taken place in the network we've been fortunate enough on mainnet to not see a large correlated event aside from maybe a couple small incidents and in an uncorrelated sense you can expect to lose up to a minimum of one eighth based on the state of what's happening in the network when it comes to correlated slashing I won't spend a whole lot of time digging into it but what everyone needs to know is is that in the event of like a a network-wide event correlated slashing can result in a validator losing all of their ether up to 32 which is much more of a catastrophic event so as we begin to like look at the future of slashing there's certain Technologies best practices that have kind of gotten over this uncorrelated slashing we'll definitely see a few in the network it's okay but now what we need to begin to prepare care for is as like stake scales and there's more products and more infrastructure providers how do we prevent correlated slashing because it's like what could take the whole network down or take all a validator stake okay so what is an activity penalty um this is super important because it's not slashing many people who are coming into the space now try to say that inactivity is a slashable defense by definition it's not and the way that we should think about inactivity is if your machine goes offline it's okay you can take the time to bring it back up online and you are you can expect to lose what you would have made so there is a penalty associated with it but it's not a slashing and it's not because you broke the rules it's just due to downtime um inactivity can also be more severe in scale um so if a large portion of the network is inactive a big driving force in the E2 economics and design is this uh average percent uptime in the network so if that drops below 66 the inactivity penalty across the whole network will grow and grow and grow in an attempt to push ether out to rebalance it to get back to 66 percent so when it comes inactivity large inactivity events will result in sizable offenses for people still don't want to call them slashings but you could think of this mechanism as a way to rebalance the system to promote 66 percent of machines to turn their machines back on sweet so now we've discussed a bit what is distributed validator technology what it's useful for and then we like you know had a bit of a refresher on slashing and inactivity now we'd like to discuss how we at Opel have made certain design decisions to minimize the correlation risk because as I said the tighter the slide is or this talk is how not to make things worse when we're trying to make them better so the first thing I wanted to talk about is the private keys because you know in a validator the private keys are more or less everything um we've taken the decision to not allow any one entity to have the full private key that's not the customer that's not only one operator and instead all the operators come together and they do a distributed key generation which means they all take part in a bit of a protocol and they like contribute to entropy to it and at the end they all get their private keys and they get a proof that everyone like took part and followed the rule fairly but there was never any moment where one person had the full private key for the validator this is super important because if somebody does you know they can run it in the validator and get you flashed um we don't put these private Keys you know more or less anywhere they're created on the machine where the validator will run we don't encrypt them and put them on chain or leave them anywhere anywhere where people could attack and compromise a large number of them um to follow through with that once we've helped create these validator private Keys we don't actually keep control over them um Caron which is the name of the software opal has written is a non-custodial middleware which is a fancy way of saying it doesn't have the private Keys it just asks the validator to sign something for it um it sits in between the um it sits in between the beacon client and the like the beacon API and a validator client and it intercepts the traffic going between them um your current clients um all they really do is they play a consensus game and they say what are we going to ask the validators to sign and once they come to consensus on that they present that to each validator every validator says okay let me check my slashing rules have I already signed something at this height no okay that's good have I said financially with further than what I'm being asked to say no okay sweet I'm not going to get flashed I'll sign this message and I'll send it back to what I think is a beacon note we are actually sitting in the middleware there we intercept that signature we gossip them to the other different like nodes in the cluster and once any of them have enough of the partial signatures they can reconstruct they can reconstruct the aggregate signature and send it on to the network and the rest of the network says yep that's just a normal validator it looks good to me um what's nice about this middleware and not having the private Keys is you know if we are compromised the worst we can do is show something slashable to a validator client but we get to rely on the fact that the validator clients are independent have their own code probably don't have the same bugs and you know are not going to sign anything flashable um most likely however if you only had just one validator client there is a scenario where you know you find a zero day in that and you can convince them to to sign something slashable so one of the things we've been very focused on is being multi-client and what that means is you can run any of the validators that implement the spec and specifically we're still working on support for all of them but you can use Lighthouse and techu already and generally speaking in an Enterprise operator like distributed validator you want to have a mix of all the different Els all the different CLS all the different validator clients because if there is you know a bug in a client you don't want it to have a majority of your distributed validator because then it will sign and you might get flashed and the only day data point we have for a client Bob like this was when prism had a timing book back in the medasha test net and that was the most severe slashings across on a test later mainnet to date and yeah so the next thing I wanted to talk about is the networking side of things so um over the last three design decisions we talked about we said how are we going to set up the private Keys you know how do we keep them maximally safe step one then step two is how do we keep our software you know out of the driver's seat we don't want to have total control over these private keys because if we can sign arbitrary data that's a threat you know you don't want you know something running more than 33 of the network if you know they can sign whatever without you know any oversight the third piece is obviously are multi-client implementation trying not to have exposure to any one piece of implementation makes you more like resilient to bugs and makes you less correlated and then the last one is more on the inactivity side so the three before have been all about preventing flashing preventing flashing preventing slashing but we also try and prevent downtime of course if you're running a highly available validator the pitch is that you're going to be up more often than if you're on a solo note and similarly if we had a distributed validator architecture where something goes wrong and we knocked off every distributed validator all at once the inactivity pattern to be rather severe and we would make things worse when we were trying to make them better so we have architected Karen's networking stack is they are totally independent from one another one group of people running a distributed validator over here has no common infrastructure with someone running it over in a separate place we think this is super important because it allows you to have non-correlation effectively if the alternative to this was if you kind of bound all of these together into a shared like networking layer like message boss or gossip Network the problem with that is well for a start it's usually slower you have to go multiple hops around the internet and with the distributed validator time is money so you don't want to be doing that um but also if anything happens to this network you knock off all distributed validated simultaneously and that's something that we don't want to do so if you have a scenario where every distributed validator has their own independent networking and you know if anything does go wrong they go wrong separately and the nice piece about this is you know going through this like effort of making everyone set up new note like set up new networking is definitely a bit more hassle but the benefit of having independent clusters is you can also have independent versioning and what I mean by that is you know one of the scary things in software development is releasing an update because you're not really sure it's going to work and it's particularly scary if you're securing billions of dollars um so the last thing we want to do is introduce a correlated outage by pushing a new version and it is you know pragmatic to assume that you know you'll push a broken version at some point we saw guests do it last month and you know they're probably the most experienced team in the space so it can more or less happen to anyone you shouldn't assume you're just not going to do it so instead you're like okay if we did push a bad version like how you know how can we minimize the issues and this is where having independent networking comes in if you have one cluster that's totally separate to another one you can update that one to version two and see what happens and if that's all healthy great you can update a second and a third and a fourth and you can gradually roll out this update across the network and if anything goes wrong you can kind of abort and you can kind of panic if you go the other way of we have shared messaging and you know everyone has to talk the same protocol you have to do what the kind of Els and CLS do which is you pick a slot number and you release a new version and you're like add everyone on Discord and you pray to God that they have running the new version in time because if they're not running the new version they go offline and you also pray that the version is right because if there's anything wrong with it you knock everybody else off as well so yeah the inactivity side of things like the penalties aren't as severe but it is also super important to us when designing distributed validators not to make things worse when we're trying to make them better and I'm going to hand it back to Colin for the last few closing thoughts cool yeah so to close it down um again appreciate everyone coming out um DBT is this I guess we've been working on quite a long time but since uh East Denver at the beginning of this year it's really now begun to have its turn essentially of adoption um our North Star is a project today what we're most focused on is convincing everyone that validators are communities not individuals um today the way that the network topography sits is validators are a single key validators are a single person or validators are a single entity and with this technology we can migrate that Paradigm into validators being run predominantly by communities um so we have about five minutes left again since dbt's kind of been this new topic that's been introduced over the past few months we wanted to make sure we take time to answer anyone's questions and again thanks everyone for coming out [Applause] thank you yo yo um so of course you mentioned before how you have are you trying to keep it decentralized by having everyone set up with different um or how can I word it I'm sorry brain freeze but um long story short is there a way to check the actual Hardware that the individuals are running like for instance let's just say we have a group of validators on digital Ocean or AWS do you have a fellowship to make sure that no one's on one exact um just like um infrastructure Tech where bazel snaps his fingers and you just like oh screw it oh it's like you're screwed very good question so the short answer is probably no it is not possible to objectively say where someone is you know running their infrastructure generally you can do kind of stochastic you can like analyze the latencies here always the same they have the same outages and stuff you can start to figure it out but yes we when we're designing oval we were really worried about what we call the lazy validator or you know trying to like programmatically disincentivize you know bad behavior but what we realized is as a protocol you can't really tell where something is running and you also don't want to punish someone kind of slow because they might be the only one running in a different data center so generally what we decided was to adopt a more trusted model where you run distributed validators with people you know so you're setting it up with companies you you know and you're making like an agreement to run them together in the kind of far future we would like to have a more permission scenario where you're running with people you don't know and don't trust but as you said there's then no guarantee that you know people aren't civil attacking you and there's not actually three people running this when you think they're independent yeah we went to more I don't know identity based way where the a launch pad you can go to like launchpad.opal.dev and have a look at it but basically you type in someone else's ether dress you're going to pick their ens names you're like I'm going to make this with you know someone.e someone.e someone that eat and send it to them and they have to be able to sign with that Wallace to prove it's them and that's our work around for trying to make sure people are decentralized if you can't do it programmatically but you can't give people the tools to like verify their counterparties and that's the route we went down to kind of make sure you're running validated with who you expect pushing and incentivizing people to um have diversity in where the machines are is actually the real problem there right there's a variety of validators large ones that choose to run on-prem there's a variety that they choose to run in the cloud um we were at dinner last night and the menu was running on you know us Amazon East 2 and we just laughed because like everyone's running on us Amazon East to so there are like other types of correlated problems with internet usage that we necessarily can't fight but still susceptible to them hey Colin thanks for the talk uh I was wondering about the dkg itself recorded about the ceremony and how do you think about the risk that there is some problem with some data data centers or something and somehow like at one point two out of four Keys get lost like how probable do you think this is and what do you think about possible mitigations to to this problem yeah this is a very good question and this is um usually referred to as like verifiable secret sharing in like Academia and this is the thing of doing a dkg is one thing but how do you know it actually worked how do you know that you know everyone actually contributed everyone got a piece like they expected and usually there are two types of ways to prove a dkg is fair they're called verifiable secret sharing and publicly verifiable secret sharing But ultimately speaking at the end of a dkg everyone that's taking part they get a private key and they get a proof in a verifiable secret share you need to have one of the private keys to check the proof in a publicly verifiable chair it's like a zero knowledge proof anyone can just check that it's verified so yes it is actually super important to make sure that everyone took part and usually there's like challenge rounds built into dkg which says you know does everything look right does anyone hit the abort or is everyone going to like sign off on this and yeah at the end of a dkg normally there's a proof I think right now we have a VSS proof meaning only the peers know it was legit But ultimately we would like to add a zero knowledge proof so even like you know the Izzy alido can say yeah those four operators definitely do the dkg correctly because they wouldn't have been able to produce a VK proof otherwise my question was more about later on they are running the distributed validator and if they somehow are qualified in the same Data Center and uh like two keys are both the centers flooded or something like that like how well like what do you think what's the chance of that happening or anything of having like perhaps more key shares with some guardians or something like that for sure so the question because he's not miked up was you know what happens if people lose you know more than a subset of keys and honestly the answer is if you lose you know more than the threshold you're probably gonna you're probably out of luck to be perfectly honest for generally speaking that's why you want four different operators and they should all make you know independent backups if you have two operators that didn't make backups and left them in the same Data Center and everything blew up uh yeah that's probably a big problem to be perfectly honest um and yeah that would be one of the ways where Force exits would be nice because then the person at the withdrawala key could be like uh oh please send it back to me but what would realistically happen is right now you'd be offline and you'd be offline and should you hit 16 ether and get kicked out so yes if people don't back up their private Keys you know even in a distributed validator if you lose you know too many of the private Keys you're still screwed so much for the presentation thanks everyone thank you very much everyone [Music] [Music] guys [Music] [Music] all right [Music] hola hola hola well next the next the next speaker is Stephen Crow from estereo he will talk about like a KinderCare bookstack sorry ethereum note in under three minutes with stereo welcome to Defcon 6. that works yes perfect now we can go so sterium the ethereum note setup ah that's a little bit of a lag I guess maybe I need to stand over here it doesn't work to go back please no worries thank you okay yeah that looks better yes so I will start from the beginning so ethereum the ethereum node setup what's ethereum sterium is the One-Stop solution to run an ethereum node the easy way not like you used to I've doesn't work yes so ethereum originated in the beginning of 2020 when we started with a Toca pose setup and now we have a full-fledged UI um that is focused on ease of use and um flexibility so how do you set up stereo so the the requirements are you need some kind of a desktop like Windows um Linux desktop or a Mac OS and you need some Ubuntu 22.04 to run the note on we will support other operating systems later on but for now we are stuck with Ubuntu and this is how it looks you start you download the software start it and discrete you so you just enter your SSH credentials of the Ubuntu I mentioned earlier we support password authentication as well as sh keys of course then the ethereum launcher checks your operating system if it's supported or not it gives you a thumbs up after that just click on the one click installer and you can choose between running on mainnet or test net in this example we just run on testnet and setup is taking note after that you can see the setup already pre-chose a execution client and a consensus client this is of course changeable so the user can easily with the just clicking on it choose something else right now we we support three execution clients three different execution clients and four different consensus clients you can also use checkpoint sync if you like uh it's a little bit faster than um syncing from scratch the next screen will just confirm what you just entered it shows you where to install where it will get installed and what gets installed and you might notice there is also some monitoring tools installed namely promoters and kafana and then the setup already starts it downloads some operating system packages installs them that's all for making ethereum work on your Ubuntu um after that did also configures the firewall and of course write some configuration files for steering itself and the services and that's how it looks when it's done then we have the control page this is really nice because on the left side you have all the um all the services that are running and in the middle and the right size is a big panel that shows a lot of widgets some of them are still work in progress but it shows you the sync status it shows you how many peer counts you have from the different Services also some other stats from the host like CPUs Edge and memory usage and that's done in three minutes so how about staking well that's the staking screen looks quite similar right um and it's pretty easy to stake you just drag and drop the files the Json key files on the serum launcher or you press the big button on the bottom and then I file the selector appears you can choose them that's it next the Orem will ask you where you want to stick them you might notice the small button at the bottom so we also support we will soon support multiple um validator clients to stay gone then you just enter the password and the staking process starts the import process starts if it's successful so is your password Works correct then it's taking starts already and the sticking key gets listed in the list on the right side you can also modify the graffiti you will use and of course you can remove the keys in case you want to migrate to another sterium server on the top of the page is always two small buttons that's for confinement committees you can easily access them just click on the button and the and steering will open up a tab or a window of your default browser of your OS and make an SSH tunnel to access your graph online parameters directly so how do you configure and make it a little bit more customizable what we see here is the service configuration we have an expert mode where you can basically configure the whole thing you can add it or add and remove flags from tiku for example here you can also Tinker with the docker settings that we use the other two options are a little bit easier to configure because it's a text-based it's just yeah select how much memory limit tiku has and of course for stickers very important the default fee recipient we also provide an update panel that shows you not only the launchers version but also the notes version and of course the service versions all updates will be all available updates will be of course on this panel visible to the user also some other settings for steering itself well what's next for ethereum first of all in the next couple of weeks we will release a new update that will need you to use custom installations these customers and installations will allow the user to not only um not only remove Services they don't like but also add new Services it will also enable them to run multiple consensus clients execution clients validator services on the same machine this is just a small peak of what features we already implemented and what will come in a new future so we also have an impact booth just around the corner if you have any questions please drop by and we will show you how to set it up and answer all your questions because unfortunately my time is already running out see you bye [Applause] foreign I will present France in accordance his own word he's a hackathon Bounty Hunter well he will talk about math nfts ERC 727214 for blockchain welcome to that concert thank you hello yeah welcome everyone thank you so much for the introduction today I will talk about Atomic Arbitrage or nft Arbitrage and we'll show some ways uh around it and just waiting for the slides I guess so when the room is familiar with Mev just maybe raise their hands [Music] so like 60 50 is good and who's familiar with Arbitrage also kind of 50 50. um yeah let's see so yeah what started this talk it started in April there was a tweet by bird Miller from flashbots about a moon bird that was actually bought for 44 East and then immediately sold in one single transaction for 248th and when I saw this I was like wow this is pretty insane and we uh I really dived into the topic so at first I'm going to give a short introduction what is Mev um basically who who can say here what happens if you click like swap on uni Swap and in a one sentence Maybe sorry what that's correct the transaction this transaction gets submitted to the mempool uh and the TR the technical view is you have uh the public map Pool where you can submit the transaction and then traditional Finance you only have like a centralized server and then the transactions get uh executed with the public mempool though people can see your transaction and they can insert their own transaction before or even uh I mean the miner itself can insert the transactions manipulate them block them and yeah there's a more technical explanation about like what's different in POS in the last slide but uh Due Time Yeah so basically Mev exists also in like old auctions it's basically if you skip the auction make a contract with the seller itself in this case the seller is the block space and yeah what is an nft um for me it was kind of interesting to understand that nft is is kind of like the same as an erc20 token but with an ID that was much more understandable to me than like non-fungible token and um they're like if you look at Google search volume you will see that like one to ten thousand people Google how to create a erc20 token each month and like how people want to create an nft it's like 10 000 to 100 000 people so you could assume that nfts are like very mainstream and um also the the gas if you look at ethereum is kind of high uh from nft projects so like open C is always in the like top 10 of gas vendors um you can see the beautiful nft project on the side and um so another question what happens if you like click make a collection bit uh on a nft site it does work on every Marketplace but maybe uh someone knows it but uh no one okay now so basically if you um click and make a collection bid on an nft Marketplace usually the option is off chain and um so but once the the the the like the contract gets held in the mempool people can still like uh do Arbitrage so let's dive into this rabbit hole so what is an nft Arbitrage basically you buy the nft at One Marketplace and you sell it at the other Marketplace and what what is special about it is in crypto you can do it in a single transaction in one block so there's some particular cases about like nfts and Mev or in general like value extraction so uh the the moonberg example is uh pretty uh extreme I find and also the ape coin maybe you've heard of it they had a a token for every nft holder and there was uh a a Trader who basically lent the nfts um claimed the tokens and then sold them immediately so he netted more than like six hundred thousand dollars I think so yeah so what what can we do about it because I mean if you look at the the the life cycle of nft it's like pretty mainstream and I don't assume like Bill Murray uh knows like what happens in the mempool and um so there's a couple of resources to educate yourself um this is like website Mev tools which shows like also how to like mint an nft or like submitted transactions through flashbots uh which is basically making the skipping the mempool and there's other ways you can avoid it right so if you have an option where you have a commit reveal you first commit and then once you you place a bit you reveal your bid and you you cannot like front run it because you need to commit first So when you buy an ens domain you have to wait one minute that's basically the reason why so the life cycle of an empty you have the minting process um here it can like lead to gas Wars so I don't know there was a soccer project that actually had like one million dollars in gas fees and like the the nft mint was like less valuable than like it's crazy right and if you like submit a transaction to flashpots um it basically if it fails you you don't need to pay gas so that's important but also um there's been cases where an nft project might be super successful and people were like bribing the miners to make sure that they mend it so even if you're like a normal consumer you don't have any chance to Mint it right and the holding time of an nft I mean it indicates basically if the nfts are flipped and yeah the transfer function um is basically for like auctions and um yeah so what can we do and one one idea is we just Fork the erc20 erc71 uh contract and if you think about okay what is atomic Arbitrage you sell it and buy it in one block so we just simply lock the last transfer and we uh block the the ID from being transferred twice per block um I mean it's it's more like it's a very simple solution um but uh yeah obviously some some contracts might not work with it um so what what are what else can we do what are like conclusions um yeah first of all there's different stakeholders so the markets they just want to sell the the artists just want to sell the users I mean they want to don't maybe don't spend so much money uh nft is a bit like long time there's been a case where somebody just wrote a function for uh I think it was a potato or something and the ver the volume is like pretty uh variable so it might not be uh the the best trading for like a high volume um Trading Company and yeah there's like different resources we've been working on a extension which shows like Mev activity phishing activity and yeah time is over here's the sources thank you so much uh thanks to Alan Bert Miller and so many more maybe I I forgot some and you can find me on Twitter under uh filmfronts thank you so much thank you France foreign [Music] protocol he will talk about how to integrate diverse sources of reputational evidence learning from the lens protocol reputation system welcome to Defcon 6. thank you thank you hey everybody so um I'm Pedro alcocer I am a staff data scientist at the Ave companies and I'm the lead data scientist on the lens protocol and uh my talk today is going to answer this question how do I integrate different sources of reputational evidence um reputation is something that lots and lots and lots of teams are working on right now um and I'm seeing some patterns that I think are less than ideal and I want to sort of address that uh my whole talk really is like in this slide right so the answer to the question I just asked is what I want you to do is represent your beliefs about your reputational signals as beta distributions and then do Bayesian updating to combine them and I'll sort of explain what all these things mean okay so this is what you should not do right so lots of teams what they're doing is that they're taking um a weight vector and a signal Vector they're doing the dot product and out comes your reputation right another way to think about that is like this so you know you have your weights you have your signals um you multiply them up and then you sum them up and then out comes a reputation number um I think this is less than ideal and I'll explain why so um information about uh the variance of your signals is lost when you do what I just showed and variance is important to know because it's a measure of quality it's how you represent the quality of the signal and I'll explain a little bit more about what I mean by quality in a minute so what it means if you were throwing away the variants is that you're throwing away quality information so now you don't know if your final reputation is coming from lots of like sort of low quality signals or maybe a combination of high and low or you don't know anymore um this is what waiting looks like without variance right it's just one number so in this case it's maybe like 0.8 um and that's really all you get you get this like one weight but what if you did it like this right so you still have this uh 0.8 sort of mean uh Point estimate uh but now you have like confidence intervals around it right so the the underlying distribution behind this weight is actually something kind of wide right that this is what I would call a low quality signal versus something like this which again is mean 0.8 but has much narrower confidence intervals so we're much more confident about the value of this weight than the previous one this is a high quality signal so you know yeah now if you have these two signals one with uh sort of a high value but maybe low quality and maybe one with like a low value but high quality you know what are you doing to combine these how do you do that um and uh part of the answer is that you have to represent these as beta distributions so a beta distribution is just a kind of statistical distribution which is bound between zero and one and takes two parameters Alpha and beta and I'll explain sort of what these parameters mean in a moment so um look like just do some concrete examples of some potential signals so a really high a high quality signal is like do you have a pull app from Rave right so this is a very difficult to Sybil uh signal that maybe I would say maybe one out of every thousand uh users uh who have this Robbie poapp are sybils they somehow sibled it so that's pretty high quality versus something like um having an ens address which is harder to sip it's like medium hard disable because you still have to pay for it uh but it's you know that's anybody can sort of get many uh ens addresses so you know the beta distribution comes in where uh you know the the way to think about the alpha parameter is that it's the number of non-sibles that you have in like your sample and then you sort you you fix the uh beta uh parameter to one and you sort of adjust the alpha parameter accordingly to sort of match this like real intuition about like one in a thousand are sybils or one in 100 or one in ten are sybils so then to combine them uh we use Bayesian updating and for the beta distribution Bayesian updating is really easy because all you have to do is just add up the alphas and betas so to give you an example if uh I was combining the three signals I just showed uh I'd add up you know the alphas and the betas I'm going to get this outcome like posterior distribution that better represents uh my belief about the true reputation and sort of how confident I am in the like the final value foreign but here's like a little bit of a more wild example so um say you had 50 signals and they were all low quality uh so beta three one it's very like one in four are sybils uh so but if you you know do Bayesian updating you combine all these beta distributions the outcome distribution is beta 150 uh comma 50 which is actually a pretty high quality distribution because the uh the confidence interval is really small on it so uh to sort of motivate this whole thing uh so let's say you have these two outcome reputations um again like let's say this one is like that it has that mean this one is like this and has this mean the means are identical but uh this one we're much more likely to trust than this user who might actually be kind of low potentially or maybe really high but maybe low right so uh what we're asking this guy we really know that that's sort of where reputationally uh this user lives so again to come back to my original slide which hopefully maybe has a little bit more context now and maybe it's a little bit easier to understand what I want you to do is to uh represent your beliefs about reputational signals as beta distributions and then combine them using Bayesian updating uh so that is essentially Bayesian inference in like seven minutes uh so thank you for listening um I am still out of breath from this this altitude uh my name is Pedro coaster these are my contact details I'm going to be releasing a lot more information about this kind of approach which I think is kind of a very useful approach it's the one we're going to be using for the lens uh protocol reputation system so you can find me on the leaf apps payalco.lens The Bird app at Palco and then on telegram calco XYZ thank you okay thank you [Music] [Music] hi our next speakers are Brian Moreno and Carolina Pinson from Shopify and they talk about the challenges and learning of implementing wallet connection on mobile welcome to Defcon 6. ready all right thank you all for coming um we're gonna be talking today about the challenges and learnings of implementing wallet connection on mobile apps so first off my name is Brian Moreno and I'm a full stack developer at Shopify I Come From bellary Zone to Brazil but now I'm based in Berlin Germany and I'm Carolina but I prefer people call Mikado actually and I'm from here from Bogota I currently live in Canada in Vancouver and yeah I've been a front and developer at Shopify for the past two years and Shopify I don't know if you know about his number one e-commerce platform for all business so whether you want to start your own business is a place that you can do it and more specifically in Shopify we worked at this a product called GM shop exactly so what is GM shop well GM shop is our take on token gated Commerce on a mobile mobile platform so the premise is what if we could unite brands with their most loyal fans and granting those fans access to exclusive products like gated by the nfts they already own and to build such a product the first thing we need to do is to let the users connect their wallets and to know which nfts they are they own right so our constraints were to build proof of ownership in as few steps as possible in the most seamless way like the best ux experience as possible and that's what we set out to do that's what we learned yes yes that like this could be done and that we could connect our wallet and we focus on metamask and rainbow and we did like we started using wallet connect I don't know if you have heard of it but they react that which is like out of the box a application that you can install and use and yes this works for the MVP but we started seeing that it was like really flaky we didn't have the robustness that we needed and we started like okay let's try different things the next thing that we did was for metamask specifically we looked into deep linking connection and we actually found like really good results especially on iOS this was what like the the protocol that we used and this also it also wait for coinbase wallet although this did improve our results well it was still not enough it was still not done so we were not seeing the success rate we wanted we wanted a more robust solution and we still had like failures for no apparent reasons because we needed to replace the connection with another one sometimes when the user didn't have the right nft and that was failing sometimes so our hypothesis is because like the react library of wallet connect was kind of closed like out of the box plus closed so we assumed that they set a persistent connection and that was getting in our way so what we did to fix that was actually I'm gonna get to that in a second but just to remember we needed proof of ownership and as few steps as possible right so we could just get the connection a signature and drop the session because the back end could do the rest of the checks like whether the user had to write an ft or not and that sort of thing the client could could just drop the connection so what we did to fix that well we rewrote our wallet connector code this is a screenshot of the pr that that done that and yeah we got rid of this out of the box library and we use wallet connects core package to rebuild our wallet connection code and focus on this one-time connection that we could just easily replace or drop if we didn't need it and well the results were pretty good our user feedback improved the internal feedback improved and the numbers were significantly better you can see a screenshot of it this was the version we shipped to our users but you're going to notice this also has Ledger live besides metamask well rainbow and coinbase wallet so like uh important feedback that we got from the user was that we didn't have Hardware wallet support we have only talked about server wallets and it was really important because we have we we don't want our users to we want our users to connect like wallets that have a valuable tokens but we don't want them to to move their tokens to a software wallet to connect to our application and it's also a bad practice like we want to encourage good practice so we tried what we tried doing was introducing Ledger live through wallet connect and yes they sort of did the trick but it was really flaky to be honest like and when it failed we didn't really know what was happening like our only support that we could give it was okay just restart everything and try it again so yeah we didn't nail this part and even though we did Implement support for Ledger wallets well guess what it was still not enough I mean for some users like some users ask for support for even more Hardware wallets because they had extra security concerns especially about Ledger and so a portion of our users asked for this and it's kind of hard to solve because because not all of those Hardware wallets have Mobile support so what could we do in this case well wallet connect on the desktop is actually built as a first class citizen it it works so for these wallets we could just let the user connect via the desktop and propagate that connection back to the app um however that creates many challenges especially ux wise because you need to get this experience right you're creating extra friction right so take this advice so to say with a grain of salt and this is something we haven't implemented yet we were still in in progress of doing this yeah so there's still a long way to go with the wallet connection so this is like a little bit of the summary of what we have right now working like the leader also the different implementations that we have as you can see like it changes from platform and also from a wallet and like we do want to focus or like make clear that we worked on this for one year and we understand that we like our requirements were to have a really robust wallet connection but that's not the case for everyone like maybe you just need an MVP that you can connect like a wallet metamask and that's it so it just depends on like the team that you have the time that you have and what you need for your application so yeah the solutions can change um yes and I don't know something that um made me think about all these mobile and web3 GAP that we have is that I have been working on web3 for almost five years and honestly this was the first time that I worked on mobile during that time and that shows us that like yes as Brian said a web3 is a desktop first basically but in order for us to reach the Gap we need to make it mobile first especially like us as developers we do have the power I guess to shape the industry because what we built is what people will actually use so the only way to bridge in this Gap is for us to like focus in on mobile and trying to make those two batters because we shape our tools and therefore our tools shape us and yeah also like a lot of people don't have easy access to desktop they and the access to mobile is way more accessible so yeah I would say in order for us to bring web3 to an ethereum to billions we need to make it accessible people and that's pretty much it thank you very much and thank you thank you um just a quick parenthesis parentheses um we're still doing research around the topic so we gotta get coin Bounty up for grabs um around air gap 12 verification around assets so please check it out and if you're interested in helping us out like it would be great yeah thank you so much [Applause] thank you very much to all of you to attend this session we will reset 60 minutes and we return 1 30 p.m thank you [Music] [Music] all right [Music] [Music] [Music] [Music] thank you foreign [Music] [Music] [Applause] [Music] thank you [Music] okay cool I introduce you and I walk away uh okay thank you for the Stream good afternoon Defcon has everybody had lunch yes oh great then we're all well fed and rested for the next talk um I'm Robert I'll be your host for the afternoon and um most of the time you won't hear me but our amazing speakers and the first one's already here and he will talk about um oh my God my schedule got mixed up um give me one second sorry this has never happened before yeah okay um yes thank you sorry sorry you're better Roberto saltini with a talk on the formal specification and verification of the distributed validator technology protocol give me a warm welcome thank you hi everybody well thank you for making it today um as the title says I'll be presenting on our work on formally specifying and formally verifying the distributed validator technology protocol and the way I'm going to do so is by answering to four key questions why do we need a distributed validator in the first place why do we need to formally specify informally verify the distributed by data technology protocol how does our formula specification look like and finally what they've actually achieved so far and what is left to do but before diving into answering to these questions some new acknowledgments this work has been carried out by consensus r d but as I received a grant co-founded by the theorem Foundation or ball and SSV Network also I'm here presenting the work but this was done together with a tonight run who is one of my colleagues in the distributed system formal verification team at consensus currently so now that I've acknowledged everybody I think so at least I can start answering to the first quick questions why do you need a distributed validator now if you are here before the talk by calling Eisen you may already know about why but I'll briefly go over it actually the way I go over it it will be by answering to another question uh what can go wrong with an ethereum validator well there are two things at least that can go wrong with an ethereum validator the first one is that your ethereum no ethereum validator May Fail and let's see what this means now you know you have your slots in some of the slots you have the proposed and now that you are a tester and the original validator is working what this means is that it produces blocks you get you get rewards to do this at the stations you get Rewards but now suppose that your node fails but what's going to happen is you start missing uh your duties so if you miss a block you don't receive rewards if you miss that station you don't receive the rules you also get penalized what this means basically with what can happen with a node failure is that you miss the rewards and you also can incorp penalties the second thing that can go wrong is that the validator key might be compromised and let's see what this means assuming we are ethereum validator with your validate or signing key now in malicious note is somehow able to steal it what these malicious note can do can actually produce for example two at the station slashable attestation sign with your validator signing key and package all of this into a slash improve signed with a malicious node standing key and propose a block submitted to the network what's going to happen is that of course you're going to get slashed and some of your funds get actually um kind of transfer to the malicious note so while this while the malicious note doesn't have the withdrawal key they can still somehow get some of your funds and this is very bad it's bad for you as a Staker it's bad for the network because it means that Malaysians get can get more and more power more and more stake and we don't want this now you know a way to fix this is not a straightforward as one might think just thinking about the first issue which is if you want the simplest one might think well let's just run more than one ethereum validator and well we know from the early day of the beacon chain this is the perfect recipe for getting yourself slash too yeah we know we don't can't do that this is why um by the female Foundation car and come up with the concept of distributable data technology protocol which is um simplified here so what you need essentially you need a coordination layer between the different nodes you need this coordination layer because you need to ensure that whatever attestations are submitted potentially by more than one no they are not flashable and this relies on a consensus protocol that so basically have a middleware between the beacon nodes and the remote signer and this middleware which is called distributable Data client runs underneath a consensus protocol plus another layer on top that ensures that we must ensure that you never get slashed and um there's a protocol that allows basically splitting the distributable data signing key into different key shares and it's a remote sign has its own Keyshia so none of the remote signer as the full key this is what uh ensures protection and against uh later signing key staining okay now we know why it's important well but why do we need to formally specify and verify such a protocol well the good news about this protocol is that we have removed any single point of failure and so we have higher resiliency however you know there is no such a thing as a free lunch uh we have increased the complexity of the system now we have a distributed system in place of a single piece of code so we have higher senses of bugs and this is where firmware verification can really help us and to give you an idea of how it can really help us I want just to draw a comparison to testing and the key concept here is that formal verification is exhaustive and testing is not to give you an idea with formal verification we can consider networks of any size you pick a number we can deal with it you can't do that with testing you have only small Network sizes and it's very important when it comes to Byzantine Behavior because with formal verification we can really consider any any Byzantine Behavior with testing even with fuzzing you have already you can consider a limited set of Byzantine behaviors perhaps you know it doesn't respond but you can't really exhaustively consider all of them also if you want to make sure that a property is insured well and and this property actually doesn't thought so there is a bug in the protocol well with formal verification it can be detected with testing maybe yes maybe no it depends uh it's not exhaustive so you might get lucky moreover if a property is actually true well with phone notification we can prove that is true when we're testing we cannot do it because again it's not exhaustive so this is the benefit this is why formal verification is very well suited for this protocol now I want to give you an idea of what from a verification consists of um you have a formal specification which essentially is a mathematical definition of how the protocol is supposed to behave which says basically when you receive message X you need to send message y then you have a property definition which is a mathematical definition of the properties that the protocol is expecting to guarantee something like never commit a slashable offense although this is kind of packaged together in a formal proof which is a mathematical proof that the protocol specification ensures the properties now what we do we take this a step further and we work with machine checkable proofs it means that our proofs are very are checked by a computer and this provides a higher guarantee that the proofs actually correct okay now we know why we need to do that and now we can have a look how the specification actually looks like now the specification is not a piece it's not a single piece is modular we need to specify the different things we need to specify how the single node you know the DVC the middle layer behaves but you also need to specify how the Byzantine node behaves and actually what we need to do is specify what it cannot do so there are things the percentile node cannot do cannot for Signature also you cannot change the state of another node so all these things need to be coded in the specification we also have a specification for the consensus protocol this is because the DVT protocol is agnostic of the consensus protocol used and so what we need to do we need to specify the behavior that we expect from the consensus protocol we also need to specify how the network is supposed to behave for example which sequencing of messages you might event I'll go over on this one in more detail later and all of this is put together is linked together in the distributed validator specification what we have actually we also have an actually uh executable reference implementation that implements the single node specification and all of this is written for this project in Daphne which is a programming language um it says a python JavaScript like syntax and it's a formally verification aware language and so I'm going to go over with the spec by starting from the bottom has been a bit say vague abstract I want to give you some practical code to look at so I start from the executable reference implementation what I want to show here this is the method that is called when the consensus protocol decides on a new attestation what I want to show here is that um the code is pretty easy I would say to understand it's a really I think that any developer will be able to read it um it's really a mix of JavaScript and python you have some form of some statements that are required for formal verification those are the top if you are just interested in say look in the reference implementation and having your own implementation you can just ignore them there are also some specific syntax like this one at the bottom which is kind of the equivalent in some sense of exception propagation in in some of the common languages but aside from this is pretty simple and this is one of the reason we have chosen definitely for this project to to to to to basically make it easier as easy as possible to for the developer development Community to really use and this prototypical protocol but now let's go up a bit oops okay how does the actual specification looks like so what the specification does really defines how the system transition from one state to the next and the system State includes the state of all the various nodes uh the state of the network the state of the adversary the state of the consensus so all these things is encapsulated in the in different fields I say of the system state now the transition says well on a given event which can be a new set of Duties uh time to execute the next Duty message I received you move from one state to the next slide from the yellow state to the Green State one very important characteristic of the way this specification is written which is commonly also to our Kelly plus way of writing specification in TLA plus we allow non-deterministic Behavior now what you have on the and explain why this is important what you have on the left hand side is not really non-deterministic but close to it so what can happen is that from one state you might have more than one event that can move you to two different states so so for example a node might receive a message from node two operation note 3. and this this is what allows to capture okay Network asynchray so and this is one of the important differences to testing we can really reason on any possible sequences of messages any that you can imagine this is coded in the other point I want to make is really about the non-deterministic behavior so from one state the same event can get you to two different states and now why this can happen mainly because of that adversary so if you have a given state the adversity can behave in multiple ways on an event and this is a what allows us to capture this while I speak about this I want to draw a comparison to model checking which is a different technique and with model checking you have the problem of State explosion and it's very it will take it's very hard for model Checker to really being able to check a property considering Byzantine Behavior because it will take a lot of time and with formal verification takes a lot of time to put together the proof but once the proof is put together it's quite quick to be run through now looking I want to show a bit of code again how actually the formal specification is Britain and Daphne um let's let's focus on the top top of the slide so let's assume you have this transition from the yellow state to the green and purple State when the gray event is received the way we code it we essentially have a function that takes a source date an event and a destination State and returns a Boolean now the way that the transition is encoded is that the function if the function returns true on a given Source event and destination state it means that the system can transition from source to destination given the event if it returns Force there is no such transition so in this example for example is true for a yellow gray green and purple but not false for any other combination and this is you can see at the bottom is coded in this is how it's coded in definitely now this is essentially a simplification of this um but it's it's a um this is expressed using first order logic you know this is something we don't see often in standard uh programming languages but it's not much of a structure say what you can see here this is essentially an end of different state transition and here we have the state transition for the honest node and you can see here we say basically this much true for hollowness nodes that's what the for all does and then we put this in end with the state transition for Network adversary and consensus you know what you can see here is this exists is what gives us the non-determinism because we're saying this function returns true as long as there exists a set of messages received messages sent and decided value says that they satisfy all the various constraints and the end expression at the bottom now before going into what we actually achieved I need to make sort of a disclosure uh that are part of this say system that needs to be trusted so these are the paths that need to be trusted and um and these are the parts that actually we don't trust now if you look at this it seems the area it's quite of the trusted and non-trusted but you know the error doesn't really represent the value of complexity also it's not really a problem of formal verification it's a sort of existential problem you you need to assume how the adversary will behave in your system even if you just buy software you do you write tests whatever you do you need to assume now that you have a consensus that or a library that works correctly you have a network you assume how the network will behave um saying things for properties you assume that you have a set of properties that are those that you are really interested into so this it's it's just something that yes a form of verification doesn't doesn't solve but it solves a good chunk of problems in ensuring that well you're a single node as it is specified will does will guarantee those properties also there are things one can do to increase the chances that the trusted path is correct one is to keep the trusted specification as simple as possible one is to have a peer review uh also for the tool it's important to use formal verification tools that have good support we use Daphne Daphne is very active bugs get fixed very quickly also uh in distribute the system usually you have two types of properties safety properties which say something bad will never happen and you have live in this property where they say something good will eventually happen now being if you formally prove that a specification current is both it's very it it really increases the chances that the trusted part is correct it's quite hard to put together a specification guarantees both both properties and this is incorrect so now we can go over what you actually achieved so far and what is left to do so on the right hand side you have what we've been able to prove on the left hand side you have the assumption to have been able to prove is that slashable attestation signed by the distributable data signing key the one that is shared they can never be created in other words the account associated with the distributed validator signing key can never get slashed because of attestation signing and this is done under the assumptions you see on the left hand side and assumption is as again we have any we allow for any message delay we don't constrain this at all we had allow messages to be lost we also allow the Biko nodes okay typical note here to be actually out of sync for this proof we don't require them to be in sync they could be serving completely different duties and they prove still old and then we have some assumption on the um number of nodes that can that are either Byzantine or with a compromise key to be less than one so then this is um theoretical bound on consensus protocols as well we assume that signatures cannot be forged and they are unique and also that you have a sound two third signature scheme and then these assumptions we've been able to prove the property we've also been able to prove that the reference implementation adhes to the specification now what this means is that it is that if you deploy a system uh running the reference implementation of the of the distributable data client node what you get is a system that ensures the non-slashable attestation property and so looking that what is left to do so we put together this proof we have the specification we have the reference implementation and it's all open sourced on the on the GitHub repo that you can see here what is left to do is to extend the specification and the reference implementation to distribute the block proposing and block header signing to extend the funnel proof of noise lash about the station to no slashable blocks and then to at the proof of liveness as I mentioned before so proving that a distributed validator will always eventually create a valid attestation valid blocks and valid header signatures and this is all for my for me thank you for listening open for a question [Applause] questions you were first yes could you explain what techniques were used to prove that the reference implementation adheres to the specification so um okay so we have used the refinement technique so we proved the through refinement basically yeah showing that if there is a relationship between the uh rest implementation State and the specification state that and on a transition that is maintained how far away do you think we are from being able to use validator pools for either eth2 or other protocols I probably this is a question I need to to the fair to someone else sorry I yeah it's only this is about implementation so yeah I I don't have that answer all right one more how difficult is the learning curve to kind of become someone who can contribute to this uh you know formal verification things and be able to to start working on that it's uh reasonably say steep not too much though I mean nowadays with those like Daphne I think it's much simpler than having to learn something like Isabel or this tournament proves that are a bit not more you say up through syntax and way of proving things so things are improving but that is that is a bit in terms of contributing to the specification say or reference implementation I think it's not my view not as much harder as contributed to a python uh specification uh so it won't contribute just to the specification you know necessarily to the proof anyone else yeah in the front how big is your team and how long have you been working on this proof to get it to this point now so well uh the the theme consists of four people um but on this one I've been working just myself and tanai and uh we've been working on this on this uh on the we've been working initially in the specification so on specifically on the proof I say since June roughly just for the proof there was a bit of fixing up of the specification before that anyone else oh yeah in the back uh so why Daphne and not other languages like coco or Isabel as you said yeah one of the reason is that uh is for the said the fact that we think it's easier for developers to understand Daphne compared to or lean or Isabel that that's one of the of the reason also when you can compile uh I still I think or Isabel but it the way you write it is far more complex yeah yeah also um okay I think I'm not pretty sure like we definitely can kind of um use a SP as a construct that is as the concept of classes and state which is very similar to programming and you can have some give some of the things unimplemented and open for external implementation anyone else if not then I'd like to thank you Roberto and please give him more thank you okay next up we have Evan Maya zono if I pronounce it correctly of course uh with Hyper certs for regenerative crypto economics yes and let's all forgive a warm welcome for Evan laughs let's take a moment for everyone to settle in all right just a minute yeah there is I acknowledge that there is a significant topical shift between the very concrete affordable verification and the very high level uh proposal of a novel economic system so okay I'll leave you to it thank you very much um is there a good place for me to stand specifically um welcome thank you for coming to hear about hyperserts um those are actually quick question have how many people have heard of impact certificates generally in the audience okay um so this is going to be a proposal for a system that looks very very different from what we have now economically I I want to come to you as if we are people in a barter economy discussing what money could look like and if you imagine trying to explain the mechanics of a supermarket does someone from a barter economy you can imagine how weird and incomprehensible some of these things will be at first I hope that you'll bear with me and then eventually we'll get to something that uh seems very compelling it makes a lot of sense and feels like the kind of thing that you know where you could take this and start building on it extending it incorporating this into things that you're working on my name is Evan miazono I uh lead Network Goods formerly PL research at protocol Labs protocol Labs you might have heard of for ipfs filecoin lip P2P one of our many other small projects I'm talking about none of those what I'm talking about is going to be a new project that's about uh we proposed this about uh I think nine or ten months ago and we've been uh sprinting on it I'd say for the last six months or so uh overall the structure for the talk is that I'm going to talk about trying to get this clicker to work um is there a oh okay apparently there's a line of people waiting to come in uh so I will slow play my opening and say that the over overall structure is going to be what was possible yesterday what is now possible today and where we think that we should bring tomorrow uh starting off generally with the claim that we didn't really have much in the way of tracking of actions yesterday compared to what you might imagine yesterday people did things you might even say people just did the things instead of trying to do additional things in addition to doing stuff um I would I would generalize this to say that like specifically I'm thinking that you can make things you can talk about things you can Fork projects steal things um modify them share them but there is scene and unseen impact and all of you have are familiar with the kinds of things that I would say are seen if you if you're talking about things that are seen by the economy this is really obvious you have things that people can pay for or not um when we talk about things that are unseen the specific framing is uh these are externalities these are indirect costs or benefits um respectively negative and positive externalities that affect third parties based on things that are done somewhere else you can see very concrete examples of these um I have two things that you might do create a parking lot or create an apple orchard with both of these there are clear costs to making it there are clear benefits to making it and then there are also some things that you may or may not be able to capture or describe very clearly um parking lots and asphalt generally has been shown to raise the temperature of the city just because it absorbs more sunlight Than Dirt does similarly an apple orchard might reduce flash flood risks compared to parking lot but it might also create issues around uh the chemical treatments used to grow or protect the crop there's no way currently to track or reward or even like there's no handle that the market can get to grab onto to think about the positive externalities like the oxygen that is created by by this apple orchard and you'll frequently see tables like this talking about rival risks or excludable Goods when we talk about these externalities often they fall into the non-excludable category of public goods and Commons and you'll see a lot of discussion of these These are rigorous economic terms with formal definitions but I think that talking about them in terms of externalities is a really clean and simplified way of saying like there are things that are hard to track hard to reward hard to say like you did you did great here here's here's a prize and like prizes themselves all like the Nobel Prize for instance is like a way of tracking the externalities of the benefits of scientific research you'll see a lot of themes resonant with the public goods funding models the retropgf stuff um the get coin regen Finance type uh like all of these things sort of end up thinking about and talking about the same types of models the same types of goods that are really hard to incentivize support the creation of Etc I would also highlight that this is a very simplified model I really enjoyed this talk about uh how you might frame different types of goods in addition to rivalrous or non-rivals you could have semi-rival riskings you can also have anti-rivalrous things and these uh this framing is useful to approximate but really I would encourage the focus on externalities people do it these impactful things and you can imagine that there's one action that results in a uh possibly uncountably infinite number of impacts externalities positive effects that result from this and there are ways of thinking about how you might want to try to track all of those effects you could try to track those I would claim that it is far simpler and far more effective to just talk about accounting for this initial piece of work and let this and potentially specify some of this but mostly just track the things that people are doing and if you have that then you can start building off of that to understand what the impacts of a specific action are and talk about whether those are good what are those impacts are good or bad and whether or not we want to support them incentivize them Etc uh today I'm pleased to announce that we have an ability to track actions in a novel way we're calling them hyper certs these are a specific interoperable data layer to account for actions that are expected to have positive impact you can think of it as roughly analogous to equity interest in a non-profit Enterprise um naturally if you think about like a startup that is expected to have profit the value of the equity is proportional to the uh present discounted future cash flows of that Enterprise that you might expect with some uncertainty tacked on and if you expect to have impact that is not profitable maybe you're going to cure a disease or develop a mechanism to incentivize the Earth being carbon negative or net carbon neutral then you could imagine that because there's no Revenue there might not be a value to these uh to the equity-like analog of those systems and instead we have a an expectation that we could design and Implement a paradigm shift that is a tool this tool actually coupled with really a philosophical change about how we think about and how we reward things that we think are useful or valuable and if we have both of these together then we can potentially create a shift where you're not just uh you're not just paying for things that you can personally own and take home with you you're paying for things that you can enjoy in the same way that we all enjoy either rule of law Clean Air National Defense uh or things like software infrastructure a like fairly cheap deeply subsidized access to block space on some on your particular blockchain when I say that this is specific there are six data fields that we think are sufficient to specify what actions were taken with whatever resolution you might be interested in really and specifically the contributors would be a like robustly identified ideally but we should be able to deal with identity that is not cryptographically verified um in these hyperserts the scope of work would be being able to say like oh it was work on this project or this other project or this aspect of this project community outreach software development Road mapping the time of work would be when the work was done in contrast to the time of impact where you could potentially uh talk about how this particular work had impact that was at this time like last year versus the impact that it will have in five years and think about those as separable objects in addition to the scope of work where maybe your apple orchard both uh reduces flood risks and uh and provides carbon sequestration and oxygen and the rights would be this generalized field in which you could incorporate bragging rights along into the embed and transfer bragging rights IP rights Etc with the uh with the hypersert object itself and these aren't at this point wholly abstract well actually I'll get to that moment one thing that's very important um when you're talking about these this data structure this data layer is the ability to do robust accounting you could imagine that the way that you would generate these might not be splitting them in the ways that where the pieces are most recognizable and as valuable for their output so you could imagine generating these as the work you do on a project quarterly but the thing that you really want to support as an or something the thing someone really might want to support would be specifically like one one feature that was built and that's the feature that makes this product particularly useful for them or you could imagine uh one particular type of Outreach that led to a huge growth in users that uh and a whole new use case that made this project particularly valuable to a user or a community or a company um I say Accounting in that this is roughly analogous to uh Double Entry bookkeeping which I would claim is another fairly revolutionary structure like coordination mechanism among humans at scale uh this is not some hypothetical thing we actually have a pilot app that is live on the girly test net the back end is an ERC 3525 which extends extends 721 and so these are these do exist as nfts um we've partnered with uh Kevin owaki and others at gitcoin to make uh fairly nice uh rendering svgs for these so that when you look at them you can see not just some set of text Fields but this is very generalizable and my claim is that tomorrow people actually uh yeah you so you can go on uh to pilot.hypersearch.xyz and create mint these play with them and I will be sharing the roadmap later on on what this looks like and uh what our plans are for a broader launch and the claim is that tomorrow people will use these to coordinate in new ways this is my hope I think that this would be if possible if adopted a significantly brighter future in which anything from uh creating novel knowledge to open data to uh scientific breakthroughs to Community Support to a carbon sequestration could be things that you can suddenly support efforts from in the same way that like I can't as an American citizen support a particular NASA mission because there isn't a like physical affordance for me to do that in the same way that I can support food banks and homeless shelters and hypersuits should generalize that to be able to support any kind of action that people are taking it is worth noting that when we talk about hyper certs and impact certificates people typically think that all these actions have to have happened in the past and I want to hopefully disabuse you of that notion and explain that there is a incredibly impactful way that you can influence future decisions with retroactive funding and that is what we're targeting for our first use cases for hyperserts specifically you could imagine that uh oh I do not have a I do not have a laser pointer I do not think um so this is a diagram that shows progression of time you can imagine that typically when these actions take place that you want to see uh the impact before the public is willing to provide to provide money to this for having been successful and it would be really nice to be able to take that money from the future pull it back into the present and use it to fund this action whenever you see this as an economist or in finance you can see that this bottom line uh this role is played by an investor typically there's some benefit that is expected to be taken because there's some likely some possible possibility of failure and so you expect that risk to be offset by profits but generally you could imagine that there could be impact investors who are investing in activities generally and I think that we should try to and we are targeting the creation of a particular Bounty slash prize programs with uh non-profits who fund science or environmental programs to set up programs where they will fund or they will pledge to award the team that succeeds most in pursuing some goal and they will reward not just that team but also anyone who contributed to that team and do so in a proportion that the teams and contributors and collaborators agree is fair at the time of collaboration so you can start to see the analog to equity interest in these projects um the only like there is no profitability of these it it would just be a chance at potentially sharing in the reputation and funding from a prize but you can start uh benefiting from all of the de-risking that you get from retrospective funding there's like the the grant maker who's funding retrospectively knows exactly what the outcome was and so they know exactly what they're get that they're getting what they're paying for um but they also get the causal impact of prospective funding where they can actually de-risk this for more people they can make it easier for people to take on these in large Endeavors because suddenly they've pledged not just support they're not just fun retrospectively teams that succeed but also anyone who provides them with workspace or resources or collaborations or introductions and so it should really facilitate the ability for these contributors to a priority go out and do these useful things so uh overall you don't I will admit you do not need hyper certs for this you could do this with napkin agreements in the same way that like many cap tables are set up initially with like a signed napkin of like okay the founders at 60 40 or 50 50 or whatever and that's good enough that is possible here the reason hyper certs get interesting is because I I foresee and predict and hope to help create an ecosystem where it's not just one-off programs sometimes you'll see these programs Cascade where maybe the contribution to a small scoped retrospective funding program ends up being integral to a much larger effort and you could imagine this very easily for some particular carbon capture technology that becomes integral to a larger program around capturing at gigaton Carbon scale uh globally and so I think that these things will Cascade in time and you'll see these embedded in each other and when you start doing that having a robust and very formal and consistent way of accounting for and listing all of the pieces of impact becomes very useful because you want to be able to either trade or exchange or share or attribute within each program in a way that is visible to and measurable in the larger program I expect that these will also have a property where you'll have multiple funding programs simultaneously running where the same activity could apply and when that work is being done you'll need to do splitting and merging of these contributions to say which parts exactly were being rewarded by which grant program even though it may look like a monolithic activity from one company or one team or one Dao I also will claim that this should generalize so far I've talked about how a grant program looks like you have teams or activities being funded with grants in some kind of historical pay and prey as a grant maker you kind of just have to send the money and hope for the best and this could be done by some simple format and I'm proposing that we use hyperserts for this of course uh actually I should mention that the breakdown of these into categories I think are useful layers of abstraction because for many of these you can actually separate or you can replace the component with other things that you might want to use depending on what you're trying to achieve what level of consensus you might have I think it's very important that we also be able to fund things retrospectively so not just teams and activities but also outcomes outputs and the impact that those outcomes have and when you have this it starts to become interesting to look at how instead of grants for retrospective funding you could have bounties and retrospective evaluation for these uh sorry grants for prospective funding you could have bounties and retrospective funding for uh the retrospective outcomes or outputs and there is no real bound to the type of decision mechanisms you could use for this and so to name two completely different things uh that definitely don't cover the space but just give you some inspiration of how you could start thinking about using these you could imagine having a retrospective funding program on a subsequent Bitcoin grants round where we use quadratic voting to price the hyper certs for useful uh breakthroughs in cryptography for instance and in addition you could have you could imagine bringing on or bring salience to those projects for prospective funders and impact investor types to subsidize future work that could be incredibly impactful but perhaps cryptography researchers can't spend time on because Grant makers are focused on what Google and Facebook and Amazon think are interesting research projects within those countries um alternatively you could imagine actually just using hyperserts for internal performance tracking uh I'm interested like my entire team working on this is interested in using this to attribute and share and track and list like oh thank you for helping me with this project I'm going to give you five percent of the impact certificate that I have I'm minting on this work that I've been doing internally that doesn't even need to be sold just having a useful metric to track what is what people are doing and look back on this in a way that is consistent I think will be profoundly useful and there are many things you can imagine in between of uh High net worth individuals wanting to fund programs of uh non-prof endowed non-profits trying to use this for increasing the impact that they have in their Investments or you being becoming prospective funders in these and I think that this becomes incredibly impactful if we can convince at some point um that or if we can convince nation-state scale funders Sovereign currency issuers that they can fund these because I think that there are certain aspects of modern monetary theory that imply robustly that this could be scalable to fund all sorts of public goods at a scale that we haven't actually seen yet and cannot curling sustain in our current economic Paradigm so that's getting a little bit abstract as far as timeline the plan is to do run a security audit and launch this possibly this quarter ideally within the beginning of q1 on mainnet and have people start to actually use this we're going to be deploying hackathons recruiting program managers and Grant programs funders for both prospective and retrospective funding for impact certificates we definitely plan to integrate with Git coin and the get coin passport because of our current collaboration and their enthusiasm and we don't yet have full merge and split capabilities yet or the ability to have attestations I didn't get into at all the fact that you need external evaluators to decide how much they think or how valuable they think the impact for an outcome actually is that needs to be a deeply subjective thing and in the same way that like I can look at strawberries in the supermarket and say do I want to pay that for it or not is do I think this is worthwhile you could imagine having a robust market clearing price even the absence of robust inter-subjective agreement on the value of some of these things because if someone's willing to pay for it that economically will set the price of that positive externality or that the work that results in that positive externality um we do have a roadmap that has us uh oh supporting the allocation of 50 million dollars in Impact certificate in in hypersearch specifically by Q3 of next year I would say we are roughly on track for that and I would love to encourage you to like keep on us to make sure that this is happening this is not a business model this is a pure public good we are not taking fees we have no plans to take this and extract money from it we think that this is just going to enable a revolutionary coordination model for humans in terms of the people actually doing this uh my team or my sub team the network funding team um and we're also working with Ray Guild and uh Kevin owaki and Octavian at Super modular and obviously have to thank protocol Labs Antoine for their support and um would definitely encourage you to take pictures of this I think we'll link this talk and the slides on hypersearch.xyz once uh the video is up and in addition um I also wanted to plug funding the commons it's a conference we're running uh fairly frequently I think the target is to do roughly 20 next year two large events many pop-ups a large number of community events this is actually the shirt from the one that we did on Monday at shelling point and other than that if you are interested in helping implement this either tooling front end back end helping audit the smart contract we were putting together a decentralized organ a permission decentralized organization to try and make sure that when this is ready for Humanity that it is in the most useful form um I am at 50 seconds left so I'm happy to take one question and that's what I get for ending so abruptly actually no no yeah thank you first of all let's give an Applause for this Evan this was a time for your presentation so now the time starts for the Q a so we have more than enough time for questions who's first uh hi Soho I know he was listening at this at this uh I reminded like a stock ranking at companies where to show like your performance you have to show individual contributions like okay I did this I did that have you talked about people on these sector about this like discussions informal whatever I I don't think that we've our our plans currently have been that you can Define these at whatever resolution you want so if you want to Define it for your team and you never want to break it down into individual individuals you don't have to the idea of being able to merge and split these is purely because in the same way that it's easier to create a liquid market for art if you're not selling an entire collection all at once it might be that people want to support individual contributions to a project and it like it's unfortunate that we you have to decide based on uh roughly it's unfortunate that those who have money to buy these get to decide whatever they want to support but it's also the reality and by creating this the hope is that we can make it easier for more people to support more things and we will have to lean into making it easier and uh encouraging people to support things that they see are valuable please talk in the mic for the Stream I'm curious about your decision not to have a viable business model around this and whether that's kind of short-sighted for long-term impact um and I don't know I see this a lot in crypto where there's these sort of very short um projects that don't have really like long-term sustainable you know structures I suppose so I just I'd love to hear about why why you've chosen to do that I have so much thinking about this and I'm really glad you asked um I will not get into all of it I would be happy to take additional questions in the hallway but uh the top the top level take is that my team within protocol Labs is actually intended to fund unprofitable impactful work and the only reason that works is because we can roughly draw a boundary around an ecosystem in the same way that a nation can draw boundary around an economy tax things within the economy tax certain things within the economy and then fund infrastructure I am the fund infrastructure side of that equation we have other value capture mechanisms that exist within protocol labs and so this is roughly deeply subsidized by and supported by filecoin um if other people would like to support the creation of this that would be obviously welcome and we have plans that using hyperserts can make certain subsets of networks or economies so much more productive that strategic investments in those ecosystems can provide the revenue and the business model that we might need so we have no value capture in within hyperserts but if we invest in things that use hyperserts because that is their competitive advantage that becomes a business model and that's just one of a few different ways you could think about trying to make that sustainable follow-up question I'm curious um also about the duplication of credit for One impact moment so for example if you have a million vaccines that are created and the funder the vaccines gets a hyper cert for funding them the transporter gets you know a credit for that the injector gets credit for that like how do you ensure that you're not giving away five million vaccines worth of credit for one million vaccines worth of execution the hypersearch itself should include specif like specifically what it was that they did and then if you wanted to you could merge the whole thing together um in proportions that the initial issuers thought was fair merge it into one thing and reward the whole thing monolithically there's also an important factor where you need to be able to scope or you need to be able to have anyone say I think this is the impact or the value of this particular hypersert and if it turns out that like there was it was only valid because the transporter rushed it through and like made it in time they then people might like people should be able to subjectively evaluate that and say I'm putting this subjective right I'm subjectively quantifying the value of this as higher for these reasons and then the people who are funding these might decide that is or is not aligned with their value metric hey thanks for talk uh I'm Marco I'm with assisted and uh I'm wondering uh have you thought about how if it this would be a good tool for dolls like d5l's or dowels in crypto General to implement and um how easy would it be to to implement something like this or what could be the benefits I really do hope that implement this I think that they are like Dows are going to be closest to understanding the and extrapolating the potential for this framework I do think that there's a challenge around the fact that this does not this is not a source of funding and if that was reach for hypersearch as a way of raising money no no I mean like the dolls that have money but allocating money yes we want to like help to grow the ecosystem right we need yeah a lot of structure built for the Dow that that that could be that is currently being built by Grant you'll be able to identify perfect analogs for this but I'm really excited about one application where for uh a file coin conference or an ipfs conference we could say 50 off your ticket if you have a hyper cert that that shows you've supported either one of our like either ipfs or one of its technological dependencies and then for a 50 discount we sell all of our hyper search for 55 so either you're donating five dollars to us or we are subsidizing you onboarding and funding the projects that we depend on and then you can demonstrate that support by bringing a hyper certain saying look I decided to support this particular like JavaScript library that live P2P depends on that ipfs clearly depends on and as a result um I've like protocol ads can create a network effect and start subsidizing the creation of public goods in this way and I would hope that Dows and other conferences could start doing this and you could see this start to spread and really increase the amount of public goods that are funded and we're going to the final question hi Evan I've been involved in quite a lot of long-running and some short-running non-profit projects where further down the line it's very hard for everyone to remember and Achieve consensus on who made what contribution and is there any any sort of Deadlock resolution or conflict resolution in the job of in in the system for you know when you're trying to get around to quantifying and subjective as you said yeah I think that it needs to or does that just stay in the human sphere I think it lays in the human sphere um quantifying this objective since 2021. um I I think that it needs to be like taking only what is incredibly important to have in the data model itself having that on chain and having everything else be something that some evaluator has attached to one of these impact certificates and saying like this is why um like this is why this is the person who actually contributed this thing or like this contribution was the most important and then at some point because these exist in a market hypothetically then the market would decide which of those sub components are valuable you could also and should also have robust systems where eventually Sovereign currency issuing countries are deciding that they are using their governance structures or even uh Network States could be issuing their own currencies and doing this where they're supporting the creation of infrastructure as they see fit and then you can see more robust governance systems other than just markets showing up for that and with that thank you Evan for your insights thank you all for your questions now before I announce the next talk um everyone who wants to go to another one right now please uh take a moment to go through the gates we have questions people waiting outside to get in oh and please uh yes thank you please use the back part as well [Music] [Applause] thank you [Music] thank you so much [Music] [Music] [Music] foreign [Music] [Applause] [Music] [Applause] welcome everyone please take a seat as far in the room as possible so we can use all the seats [Music] we have some empty seats here in the back make some new friends come join us here [Music] [Music] foreign [Music] [Music] [Music] thank you [Applause] [Music] okay hello okay um hi everyone uh my name is Jolene and I work at flashbots research and today I'm going to be talking to you about evaluating the architecture called proposer Builder separation given the things that we've learned on the insights that we've gained since the deployment of math boost and I'm also happy to take some questions at the end if we have time um so as a brief introduction uh what's the point of this how are PBS and mevpus linked or how are they different so probably most of you are familiar with these ideas already but just to make sure that we're all on the same page um proposer Builder separation refers to an architecture that separates the functions of block building and block proposing within the core ethereum protocol so instead of the block proposer in this case the validator going to the mempool and building their own block which they would then proposed to the rest of the network all block construction would be outsourced to actors that we called block Builders so the idea is that the only thing that the validator would have to do Under full PBS would be to choose the highest value block that's presented to it when it's time for its thought so block building is a you can think of it as like a revenue generating activity there are economies of scale involved in it and opportunities for things like private order flow that might give you an advantage over other builders so basically there are incentives for Block Builders to centralize and to become more powerful as time goes on so that's a trade-off that was kind of known about and made and accepted at the time the PBS would take this centralizing Force out of the ethereum consensus layer and put it into the Builder Market and then logically the only kind of interface point would be where the validator is sending these calls to the Builder Market on picking the best block that comes back like I said so the idea is that kind of stuff can get uh very ugly in the Builder Market but we don't care because the core of ethereum the consensus layer is safe from the effects of Mev um so where does Mev boots come into this and how is this related so math boost allows validators to engage in an experimental version of PBS on this Builder market so it's completely optional but if you have a validator and you're now running move boost you can add source to your block building to a kind of a burgeoning market of block Builders relays Searchers that are all competing to give you the best block and the reason that this happened was because um if you're familiar with the old flashbots auction this wasn't going to work after the merge so we knew that we needed to kind of upgrade it and do something different and so flashbots worked with the ethereum foundation um to make this piece of Open Source software that could act as a prototype for this longer term Mev solution for ethereum which was PBS so in this way we are able to test this idea of PBS Through Math boost rather than kind of putting it in the protocol and then being in trouble if it doesn't work or something bad happens so here's a quick diagram of what I'm talking about when I say the Builder Market um and I really just want to go through this quickly just to make sure that we're all on the same page because the later things will be a little confusing if you don't understand this so what we have right now kind of on the left hand side is a validator running mode Boost Mobile Boost allows it to connect to this Builder Market if the validator is not running mate boost officially it should not be participating in this Builder Market so in order for this to work there are certain rules that need to be carried out in the Builder market so over on the right hand side we start with the users so they originate transactions and those transactions either go to the public mempool um on the top or maybe some private mempool that Searchers or Builders are allowed to access um and then we have Searchers who would be monitoring the public mempool as well as any private sources of transactions that they have and they would be forwarding their bundles along to these specialized actors that we call block builders block Builders will be receiving bundles from searchers maybe they're also monitoring the public mindful maybe they also have their own private sources of transactions too and they will be trying to build the most profitable block for a slot so they'll forward blocks to relays and the relays here it's different to if you're familiar with the flashbots architecture before the relay here kind of has a different role so it allows validators to register with it and then when a validator requests a block for a particular slot the relay will forward the block that has the highest payment attached to it and then that validator will receive that payment for proposing this block to the rest of the network so when I say the Builder Market experiment webmav boost this is what I mean uh and when we talk about full and protocol PBS we are trying to figure out a version of this that has the important bits that enable this Market kind of baked into the ethereum protocol so that means no math boost and probably no relays as well oh um so the question that I wanted to ask and kind of answer try to answer in this presentation is how is this experiment going so math boost in this early version of the Builder Market have been it's been alive for about a month now so can we tell if things are going the way that they thought we would would it be safe to put PBS in the core protocol today you know as this experiment is without any further features or enhancements and are there any warning signs or maybe early indicators of things that we should be worried about so when I was thinking about this uh question like how could we tell if the experiment is working I started thinking about PBS as a kind of an organizational principle so it seems to be very widely accepted that this kind of clean separation um between the Mev activity on the rest of ethereum with only this Market in between as an interface point is desirable and also possible so it's probably not very helpful that the idea of PBS is I think relatively simple and easy to understand and sometimes we can be biased towards things that are that are like that when the real problem that we're trying to address is much more nuanced and complicated um so the previous question kind of turned into these questions is abstracting the nav away from the core protocol a good obstruction or does it leak are there more interface points between where the Mev is happening on the core protocol um then people would lead you to believe and if things get very very bad in the Builder Market maybe imagine a very small number of very powerful very centralized actors that dominate block building are there really no knock-on effects on core ethereum so assuming at some point in the future we have a full and protocol PBS in this uh in protocol PBS world I started thinking about what parts of ethereum are still influenced by mefe and I came up with kind of two families of effects so the first order effects are kind of the Direct effects on the core protocol so these are places where even though this Mev is supposedly over on the other side of the room and it doesn't affect ethereum it's still it still influences part of it and the other type are more kind of second order effects so how would how are actors incentivized to try and influence the base layer if it meant that they could gain more favorable conditions for themselves so my idea for this was to try and identify a few concrete examples of these effects and then kind of check data to see if there's any indication that these things are happening already and then if these effects were to get more pronounced as the Builder Market develops are these really an issue and if so what kind of medications might we need um so let's have a look at the first kind what parts of ethereum are still affected by Mev on their full in protocol PBS so it might be more than this but these are the main ones that I came up with for now so the first one is the contents of the block so we are now getting these blocks from the Builder market so now all transactions in the block can be directly influenced by Mev and this wasn't the case before um because it was possible for minors or validators to kind of opt out of using my food's term of Keith if they really wanted to um so in the future if most or all of the blocks come from the Builder Market the purpose of the builders is to reorder and rearrange transactions their block can be as profitable as possible so hopefully that one's kind of straightforward this kind of idea of like losing control of plot content the second one is maybe not so obvious so the contents of the public mempool are influenced by Mev Under full PBS so how can that be well in a world where we have full-in protocol PBS Builders are incentivized to have private sources of transactions that we talked about earlier and we call this exclusive order flow so if Builders are competing just on the contents of the public mempool there is and there's no difference between the kinds of transactions that they have available to them then their success comes down to maybe their algorithm or their trading strategies however the more order flow that they have access to that other builders do not the more likely it is that they can build a better block with that um on our flashbots we generally think that order flow is going to be a big differentiator in how successful block Builders are going to be and if this order flow is exclusive in that sorry in that um it's going to a private mempool instead of the public mental um it's it's not going into the public mempool so you you could have this situation where um as as there is more Mev activity because the the Builder um what is happening with the builders is associated with this Mev then the contents of the public mempool might get smaller um and then the last one which kind of relates to the first but maybe a little bit of a different Focus um blog posters as block proposers used to have control over um what the contents of their blog would be so Under full PBS there used to be um sorry the the big difference would be the validators will now accept full Blocks From the Block Builders and that didn't happen before so in the old kind of flashbots model you we would now never have um full blocks coming from um external sources um and then the second thing is that uh I'm not sure if this is for certain yet but we think that you know Under full in protocol PBS there may not be an option for Builders to or for validators to opt out of that so you might have no choice but to get your blocks from um from an external Market and then kind of going on to the second family effects so how are actors in the Builder Market incentivized to mess with the base layer the consensus layer in order to gain an advantage on the big issue that I see with this is that the Builder Market is actually out of protocol so there's nothing enforcing um participants in the Builder Market to stick to the structure that we saw in the diagram earlier the math boost architecture that exists is kind of a template for what is safest for like independent ecosystem aligned parties like flashbots to run and there are a certain number of assumptions baked into it in reality we can't stop anyone from using for example their profits to buy validators their profits from building creating their own private version of mevust or offering services around Builders or other parts of the system and as we're about to see measuring what comes through a relay might not necessarily be the full picture of what's happening in the Builder market so the only part of the Builder Market that's in the core protocol is this Builder specification which defines like the API calls and sequence between the validator and some software that looks like MAV boost on that side of that kind of Anything Goes so by now we've identified a few kind of concrete areas where we think that Mev might still affect the core protocol Under full PBS so what can we do with this well there have been a lot of really awesome data collection efforts by various parties in the ecosystem in the last few weeks since the merge what if we took a look at some of these and see if there are any clues based on these areas that we've identified it that this prototype Builder market via Mev boost is starting to affect the core protocol in an undesirable way um and so the first question that we want to ask is going back to what we identified previously how much block content right now is currently decided by the Builder market so here is a graph that measures how many blocks have come from relays since the merge so as you can see um as of today we're up to 60 of blocks coming through relays and I looked at these this graph last week and it was about 45 so it's increasing uh quite quickly and again it's important to note here that this might not be the full picture so many of these dashboards measure what comes from relays and this might not be an accurate reflection of what blocks are coming from the Builder Market as a whole so it's probably best to think of this as a lower bound on how much block content is coming from the Builder Market that seems to be increasing over time um and then answer the second question so is the public mempool changing since Merv boost launched so here is a graph of the count of unconfirmed transactions in the mempool over the last 30 days and if we look back at when the the merge happens September 15th it it looks like something may be different the Baseline seems to be a little bit lower and what I would expect to see over time if my speculation is correct is this Baseline kind of trending downwards however I don't think I could say at this point that this is definitely because of Mev um my plan is to to keep monitoring this and look into other things that might be affecting the mempool since the merge and the next question is linked to the idea of things kind of getting nasty in the Builder market and if there are actors with big advantages that they might you know use they might be in a position to use those to start to influence the base layer so are there early indicators that Builders are centralizing so this chart measures block Builder's success since the merge so percentage of blocks that landed that came from a particular Builder so if we think of that 60 that we saw in the diagram a few slides ago um how many of that 60 came from a particular Builder and the orange one is flashbots so that doesn't really look great right now however if you look closely it does seem like flashbots is losing its lead a little bit as time goes on or as some other people have pointed out to me that the non-flashbot slice looks as though it might be kind of diversifying a little bit but the point that I'm trying to make with this is that it doesn't look like flashbots you know is as the dominant Builder has a lead that's growing and growing over time um but this is probably another good thing to keep an eye on um and so when I talked about actors trying to influence core ethereum in ways that we haven't thought of yet I think the big one for me is whether Builders and validators will eventually decide not to bother with the out of protocol Builder market and just start making deals with each other directly and this is the kind of activity that's much more difficult to see we really need to keep in mind that the Builder market so this thing that validators are able to send their requests to at the moment on what goes through Mev boost on the relays is not the same thing so what we would need to find out here is if there is a way to tell if there are blocks produced via some external source that are not going through math boost or a relay and I think this is possible but and managed to do it in time for this talk but by examining this number over time we could deduce that perhaps Builders are being connected directly to validators and bypassing the relay components entirely because if you're a big party with a lot of order flow and either access to a lot of validators or perhaps the means to acquire validators you don't need to participate in all this relay and Builder competition business so I think we want to keep as much of this activity in the light as possible this is something that I think is really important and will continue to work on so looking at the indicators that we found are these things actually problems if the trends that we identified continue and possibly become more pronounced what negative effects on ethereum could we expect so we saw previously that we are currently around 60 of ethereum blocks now coming from the Builder market so extrapolating from this should we be worried if ninety percent of blocks are coming from the Builder market and I think the main problem with this is censorship and hopefully you can see by now why censorship is the big kind of topical issue with PBS that a lot of people are talking about if the centralizing Dynamics that we're expecting start to play out it's possible that you know 90 plus of blocks start to come from a few actors and as we know it's it's easier to censor at the Block Builder level than it is to censor at the consensus level uh we also saw that there seems to be a decrease in transaction volume in the public mount pool post merge and if that speculation is correct and it turns out that this is a symptom of transactions being diverted from the public mempool and into private pools what implication might this have and one of the solutions that people propose um or maybe one of the kind of additional features that people want to see for Mev boost or full PBS is this idea of a full box solution so that would be allowing you know if something goes wrong you allow your validator to build blocks locally and then compare those to buildermarket blocks that have been produced and then you would only accept the Builder Market blocks under certain circumstances so the issue here would be if the public mempool you know maybe got so small that effective fallback Solutions like this either weren't possible or relied on the validator being altruistic in that building from the public mempool alone is completely not competitive compared to um the Builder Market um and for this so we still don't know if this is happening yet but if it did happen that we end up with builders that have ad size profits it might make sense for Builders to start staking their profits and running validators and there might be entities that own a lot of validators already that want to start block building and there's likely to be a business model where Builders can offer services based on how many validators that they own so I don't know if anyone saw Alex Stokes talk yesterday on block building after the merge but he had a slide with like a giant list of different ways that you know block Builders could monetize this kind of relationship and decentralizing the Builder role itself might not be feasible here as we're kind of out of protocol at this point and therefore Limited in what we can force Builders to do so we've identified a few different things that if left unchecked might cause US problems down the road um and I think there's good awareness around most of these already especially the censorship resistance schemes which is very topical um but there are a few other things that given these early indicators might also be um a priority so censorship resistance I think we know about that already it's something called transaction inclusion lists I believe is being actively worked on by flashbots and other people in the ethereum ecosystem um many so for the other things many of the effects that I talked about can be lessened if we take steps to ensure that things can't get very centralized and ugly in the Builder market so trying to figure out how to effectively decentralize the Builder role might be important while also keeping in mind that a decentralized builder has to be competitive compared to a centralized builder in order to make any difference another thing that I think is important are mechanisms to ensure that Under full in protocol PBS validators can still build their own block or perhaps augment blocks that they receive and the reason that this is important is that a lot of these kind of extreme effects that we extrapolated assume that Under full PBS 100 of proposers will be getting blocks from the Builder market so this would be a way to kind of get back control of dot block content and then if at all possible reducing incentives from validators and block Builders to work together um is something else that I think is important and following on from that working on the ability to monitor the Builder Market as a whole by putting something in the protocol that can tell whether blocks have been built locally or not so that's pretty much it thank you very much for coming if anyone is interested in talking about these things further my email address is right there please reach out there's also this flashpot's QR code that I think takes us to our discourse Forum where we're you know talking about a lot of these open questions if you're interested in working on any of these topics in return for money our jobs link is there as well thank you everyone I can take questions if there are any thank you Jolene we have room for two short questions okay one in the front Hey Jillian awesome talk thank you um I'm wondering if there's a method where some of these solutions could be made by the community and be somewhat attached to something like math boost kind of almost like a module that you can add if you opt-in yeah um I think I think that's a really great idea uh I think it depends on the solution math boost at the moment is a fairly lightweight piece of software and we have thought of these things a lot of the things that we want to do with like transaction lists and other things like that require knowledge of State of the ethereum State on map boost just doesn't have that but I think that there are definitely maybe some other things and other ways that mebus can be improved and um you may have already seen we're also trying to make math boost more of a kind of community effort as well and take Solutions and from from other other parties yeah hi Jolene thank you that was a great talk just wondering the the um solution where you allow Builders um sorry validators to augment the block doesn't that still require or still rely on them being altruistic like isn't there is there is there a situation where it just doesn't make sense because it's so much more profitable and not much goes into public Med pool perhaps yeah um I I think I think you're completely correct about that in terms of incentives and I generally think that it's it's really important for us to kind of design these things in a way to make sure that people aren't missing out by doing things that are good for the protocol um so yeah but I I think one maybe one example of where this has actually worked well is around the time of the merge uh when there were some was there some problem with my Boost or something and a lot of people said that you know they weren't going to run math boost so it was more kind of like a social like altruistic thing so I think ethereum is kind of special enough in that um it does happen that people have an interest in in the protocol and want to do the right thing by it but you're completely correct yeah if it got to the case where it's just not profitable to augment your block we'll have to think of something else with that I'd like to thank Jolene all of us let's give him a round of applause [Applause] before I announce the next speakers I'd like to ask you to form an orderly line uh because we have people waiting outside to join in if you're staying for the next talk see if you can find the spot more on this side of the room because we want to make enough space for everyone [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] [Applause] [Music] okay everyone thank you all for coming be careful um please for the last people to come in and take your seats and we have some actually one spot over there left one in the back over there left if you want to sit and with that I would like to introduce our next speakers um who will be talking about designing autonomous markets for stablecoin monetary policy and we will have for that stefanzuka and arya's mood give them up lost people [Applause] thanks yeah great to be here um so I'm Stefan and today I want to talk about some of the design work we did for our stable coin that's called gyroscope and this is Joint work with my co-founder Arya glagasmond so um imagine you want to build a stable coin so you have a you have a cryptocurrency and you want to keep it stable and in some senses it's always going to look like this you have some kind of reserve of assets and then people can mint and redeem coins against that assets and that's going to stabilize the price right if the price is too high they can they can meet new coins if the price is too low they can redeem coins it's going to stabilize the market price and we call the primary Market mechanism excuse me the primary Market mechanism the thing that grants access to these assets and that intermediates between mintus and redeemers which are basically arbitious and the asset Reserve um now what are these assets well that depends on your design um it could be many different things in a sanyaraj shared design it could be it's basically an equity share in a sense for something like a basis design it's something like a bond and in a reserve back design at some portfolio of other assets and by the way this is a design we are using for gyroscope it's reserved backed um now there are different situations we need to look at the kind of boring case is when the system is fully collateralized and all the assets are perfectly liquid because then this is just basically a pricing problem price the reserve assets um and the somewhat more tricky case is the system might be 100 collateralized but not all of these assets might be liquid so you cannot necessarily pay them out when people redeem um and then there's the crisis situation where the system actually becomes under collateralized and then you need to decide what to do and my talk is going to be about the design of this component that basically designs what to do in such a situation um what we are implementing for gyroscope is a reserve of only liquid assets so we are not really considering this case but it would be a maybe straightforward extension of what we're doing so let's look at some examples of this um if you look at die um you have the PSM or plexibility mechanism and it basically looks like this that your reserve almost only consists of usdc and this is what people sometimes say that dye has become or is going in the direction of wrapped usdc um the the numbers are like 60 or like the PSM usdc then you have another 20 of LP shares against usdc and of course in that situation um something you need to ask is are there many risks associated with that so for usdc for example there could be some regulatory risk which may amount to counterparty risk censorship risk and so on um kind of the goal of gyroscope was to build something like a PSM 2.0 so how might you improve on this design and I think there are like two ways the first thing is you may want to diversify your reserves you may not want to hold only usdc and the second thing is to implement something like a programmatic risk control that reacts autonomously to market conditions so that that kind of can be seen in two parts the first one is if you have Indie the different PSM vaults are independent um but you probably want to coordinate them somehow to react in the same way to Market strategies and the other one is um to price the stable coin depending on market conditions especially when the system should the system become under reserved and this is kind of where we're going so kind of to to build such a system there are like many different challenges and I just want to give you like the um the super quick overview um basically we want picture right so we have a reserve that consists of different asset classes that are probably structured in some way and then you need to answer several questions like which of these which assets should that be which risks are these exposed to how to structure the whole thing so that the risks are somehow contained um how do you generate details on the whole structure you probably want to price these things with oracles and so on these are all things I'm not going to talk about but questions we still had to answer for gyroscope and today I want to answer um this mechanism basically the question is someone comes to your system the system may be under reserved or illiquid to some degree they want to redeem a stable coin what is the amount of assets you offer them um and to do that we introduce a more General tool which is what we call the Redemption curve and this is a general tool to analyze any stable coin design basically um so the Redemption curve is the following curve you have two axes on one axis you have the Redemption level which is basically the amount of stable coins that are getting redeemed and on the other axis you have the Redemption price that the mechanism offers to the Redeemer and now we assume now we look at what happens when people redeem more and more and market conditions don't change for the beginning so nothing happens no prices change but people just redeem more and more and ideally of course what you want is this curve you can always redeem at the dollar because then you always stabilize your pack at exactly a dollar um and the higher your curvature here is the less you support the peg as more and more people redeem um kind of the default that is also happening for pack fiat currency is that you try to support the pack as long as possible until your reserve is empty and then you don't support it anymore and and kind of my argument will be that maybe this Redemption curve is not ideal and many other Redemption curves aren't ideal either so um we can see kind of this this type of curve or that type of behavior in the in the real world or in the Fiat world um for example the attack on the British pound in the 90s was basically um a continuous outflow um from the from the British pound basically a continuous Redemption of pounds against other assets until the central bank was no longer willing to support the peg um and then you see this this kind of drop where some people made a lot of money I think the recent crash of the pound is probably not like that I can't say anything about that um if we look at if we look at stable coins we've seen similar behaviors to start decided a little bit outdated but there was this period in like 2021 where like many creative stable coin designs were created and many of them crashed and in a somewhat similar way very abruptly um I want to look at one like design case study which would be Fey the original design of Faye which was also not very successful in its original form um and so Fey had these direct incentives which basically means um the the more of pack the whole system is the worse the price you're getting and that if you kind of look at it this leads to a Redemption curve that is very Steep and that looks about like this which means that your pack is not going to be stabilized very long and then they remove the direct incentives and then their a Redemption curve looked much much less steep um and we can see kind of the effect of these like steep Redemption curve here I hope this is kind of visible this is like the price of a after launch um and it had like a like a huge uh price drop with a lot of volatility um I should probably talk about the the elephant in the room or maybe not an elephant anymore um sany Road shares and the way I think about senior chairs is essentially um that you redeem at one dollar but your backing is an endogenous asset that's basically uh very very tied into your project itself and this can lead to some negative feedback spirals [Music] um so essentially it's the same story as from the Fiat world before you provide Redemption at a dollar until the willingness of the market to buy your senior shares is exhausted and then you basically crash very rapidly and I should probably show this um right um this is the the Supply right here has this Supply inflation at the same moment but it doesn't really matter because people don't want to the amount of central shares people want to buy doesn't increase or the amount of dollars people want to put into senior chairs I should say um I I should point out one thing about this particular talk this was when I gave the first version of this talk yeah um so that's that um this was not without precedent we had a very similar system called iron you may remember which also crashed in a very similar way so here you have the stable coin that crashed at the same time the endogenous collateral crashed also um so so that that's not good and that's why we made gyroscope Reserve backed um and the basic idea of what I want to talk about today is maybe we should think about the Redemption curve as a design problem and maybe we should choose a Redemption curve that we think is useful and reasonable and then we go and Implement that in our primary Market maker um so the the basic design is going to look like this right you remember this picture um and we're assuming that the assets here are exogenous so we don't need to worry about these feedback effects but what do you want to do if the system is under collateralized or illiquid um and we have like like I want to give you these results in three parts um first we thought about what are actually desert rata for a good Redemption curve what do we really want and then I'm going to show you one such curve that satisfies the Digital Data and then I'm going to talk to you about how to actually Implement that because as you remember the Redemption curve assumes that no Market condition change over time and of course that doesn't happen in reality so this step from picking a curve and then implementing is is making it Dynamic and this is going to lead to this type of dynamic bonding curve if you want to think about it like that um great let's first think about what we would ideally want of our Redemption curve um the first thing I want to I want to note is that probably your collateralization should be should stay above some lower bound your Redemption shift Redemption curve should not exhaust your reserve if that's at all possible and the reason for that is to enable the system to recover later you if you exhaust your reserve you're basically destroying your your system and that's bad and then you probably also want the Redemption price to stay above the lower bound if that's possible to support the peg at least to some degree um of course this is Trivial we usually want our stable count to be stable at a dollar you probably want some kind of continuity of that curve you probably don't want these abrupt crashes and um I would say that the main reason for that is uh to prevent speculation because these kind of discontinuous crashes are something people could speculate on very easily and that could also lead to like all kinds of Market upheavals um um and and then this is like a little bit of a bonus but you want your Redemption curve to be easy to use so probably what you want is um you want the execution if you redeem a certain amount of a certain amount of stable coins our stable coins can be called Drive levels apology for that um you probably want that to be easy and especially you don't want there to be an incentive to subdivide your redemptions and to be somehow clever about the way how people redeem um we have like a few bonus desiderata that are um with respect to like several transactions or several blocks um you probably want your the reserve exhaustion to take a long time unless of course you reserve just just crashes so the system should uh also kind of think about what what happens what happens tomorrow what happens when the system runs continuously um you should be able to regain your pack and of course you need to implement the whole thing on chain um ah yes the first math equation so before I show you how we implement this I have to introduce this nice equation we assume that the Redemption pressure is computed as a Time discounted some if you don't care about that that's totally fine and now I'm going to show you one simplified design that satisfies almost all of the desireata and this is a simplified Redemption curve we're not using this but it's good for explanation so what's a curve does is that we support a price of one dollar so people can redeem at a price of one dollar um up until a certain amount of redemption so on this axis we have redemptions um and then we are gonna drop the Redemption price so that the Redemption price is equal to the collateralization ratio and this is a sustainable way of doing it so for example if your system is 80 collateralized the Redemption price would be 80 cents and you can just do that indefinitely and redeem the rest of the money and of course this has all of the nice properties almost all of them um you don't run out of you don't run out of Reserves um you can actually configure a trade-off that exists here of course the longer you support a pack of one dollar the lower your eventual collateralization is going to be but it's something that you can choose um exactly so so we have like this long-term survivability of the system um if you actually want to do this like this would be like the equation um but of course the problem is that this is like this ugly discontinuous jump that I mentioned before and so what we really do is that we introduce a linear segment there looks like this um where the price would first be a dollar then it put decrease as more and more is redeemed and then if it if it has reached a point where we have a sustainable collateralization ratio it would just give you that price um and if you want to do that the mouth to do this looks like this um but it's like it's fine right it's fine they're like case distinctions and fractions but it's not that bad um and one thing you you can see here is that governance can configure this using some hyper parameters so you can say what is the minimum collateralization that I want of my system um and this is something that governance would set because it's actually a meaningful parameter you can say um what is the steepness here that I'm willing to tolerate another parameter and then if you solve this equation it's going to respond autonomously to these hyper parameters um now this curve is beautiful but it has a problem it's not very useful because it doesn't reflect reality because we assume there is a Redemption pressure of zero um we have some kind of initial collateralization of the system which we call an anchor rate or we could we label this like ra um and then you start from there the curve tells you what happens when you start at zero Redemption pressure at the anchor rate nothing changes in the market to implement that we need to live in a situation where the market will of course change and we can still use this curve and this is what I'm going to talk about today so the goal is make the whole thing dynamic um make it react to the current market state and gonna show you how to do it hopefully in a picture and how we do it goes as follows I showed you this curve and if you know your anchor collateralization which is on this axis and you know a certain amount of redemption pressure then you can go and integrate the curve from before and it's going to tell you what your collateralization at that point is going to be and that's up here um and now if we do that for all possible sorry anchor values we get a three-dimensional surface um and what we just discussed was if you know this and this you can compute this but it turns out that you can also do it in reverse so if you know the current collateralization of the system on this axis and the current Redemption pressure here which is something that you can just observe in in real time then you can actually compute this initial collateralization ratio which of course at this point is a purely it's purely a modeling tool but it tells you if nothing had happened in the market and we are now at the state where should we have started if this model was true okay so so many complicated words here um and so basically this theorem says that you can reconstruct that it's a monotonistic argument um it's it's fine and and so then once you have the initial State we can offer Redemption prices um based on uh based on this Redemption curve as a model um and that's pretty convenient because it's a way to respond to current market conditions it's completely autonomous governance only has to set meaningful hyper parameters it doesn't have to do anything in the moment and it's it's completely predictable for other people so everyone knows what's going to happen so you have this you have this huge transparency advantage over a system where governance would just come in and then make a vote and of course other Market participants have no idea what the outcome of this vote is going to be now I should probably talk about implementation because you might be looking at the theorem and you're like yeah this is great if you're doing pure math but we're not doing that at all and I'm going to talk a little bit about implementation and the implementation works in two steps the first like core idea is that um you saw you remember the scary slide I showed you before with like many formulas where everyone in the room gasped so that had a couple cases stations in it and if you do like the cross product of all the case distinctions you end up with a bunch of regions and you can partition your space into all of these different regions and so you may believe me that this is possible the theorem is that you can actually detect in which region you are based on only the current market state um and then it turns out that once you know in which region you are um Computing like this value on this axis is very easy it's basically solving quadratic equations then um and that brings us to the algorithm so we detect the region based on the current state we reconstruct the anchor State we take that as a model and we compute our Redemption amount which is basically integrating over this exact curve I showed you before and then when you like are being very careful and you count all the things you have to do to get this is actually very cheap you just need to do some arithmetic a little bit square root um right so this system has a number of interesting properties and I'm going to show you just just like two um one result that you get like very easily is that there's no incentive to split up redemptions and like mathematically that's because you're Computing an integral and you can always split up an integral in different parts and it doesn't matter if you compute the parts or the whole thing so mathematically it's not very interesting um you have a path deficiency result which is basically that and this is like a little bit more interesting that the protocol state is going to improve over time so if you implement the system your reserve does not crash um completely to zero um and people just come and redeem then your system is going to recapitalize itself over time and of course this is exactly what we want we want long-term survivability of the system and this is what this looks like and now I want to conclude so if you're taking away anything from this talk it's that you can design your Redemption curves when you're building a stablecon system and this is a very attractive way to get good properties of your system it's also a very useful tool to compare different stable coins and to think about if the stable coin design is actually solid I showed you a design for one desirable Redemption curve and then I showed you how to make that Dynamic and make it react to market conditions and if you want to kind of be be involved into how we actually do these things we have implemented this in gyroscopes Dynamic stability mechanism gyroscope is our new stable coin launches planned towards the end of the year um and hopefully all these properties are going to make a gyroscope very robust if you're interested in the paper and you should scan this QR code and if you want to get in touch with us you can go here or follow us here thank you very much we have room for some questions thanks a ton for the nice talk uh why did you guys choose a like a piecewise linear approach for example and not something that's really smooth like a sigmoid or something like that are there nice properties that you get out of this then you would not get otherwise yeah let me just just jump back to the slide so the [Music] should I give the honest answer or the line no I'm gonna get the answer so if you're honest answer is because it's easy to implement um because and like the reason is that this is linear so it's integral it's quadratic and solving quadratic equations is easy and solving anything else is is hard so I have more of a philosophical question in the last let's say two or three centuries the tenancy in the state-backed currencies has always been to leave Beijing to go toward the free voting money why do you think that in the crypto economy we are seeing a Resurgence of Peg money that seem to be quite prevalent yeah yeah that's that's a deep question I wasn't expecting that uh um why why do we have a Resurgence of packed coins um I think my intuition is that people think in USD or in some other fiat currency that is not really represented in crypto maybe at some point we have a crypto native currency um that that doesn't need a pack because it's very stable against other Fiat currencies but I think we're not there yet and that's why we need to represent some some kind of measure some kind of numerator that people are familiar with and that matters in people's lives yeah let me just add a little bit to that so it's a common strategy when you're kind of like developing a new economy to do something like a currency Peg and it seems to work well in those situations because people want to use a more trusted uh unit of account like a dollar and so I think that's has a large influence here but I also just want to note that the uh the autonomous monetary policy that we've been developing can be more General than just maintaining a a currency Peg you can choose parameters in different ways to do more arbitrary monetary policy too thanks for the tour guys uh I would want to know how do you think about once the system gets under stressed and you you get to the Redemption price like what would be the the mechanisms or the incentive to because it's kind of a uh I think there is some loss of trust in the Market at that point uh how do you how do you think about how to regenerate the system and and getting back to the pack like this yeah so the the gyroscope design contains a number of other uh like lines of defense and sort of ways that uh that the reserve could replenish over time um and so basically this monetary policy the idea is to buy the system time uh so that those other mechanisms might be able to kick in uh some examples of that is basically like uh Reserve assets are deployed in Risk segregated ways but they can still potentially earn some yield and so there's sort of a tendency to increase collateralization over time because of that and then there's also sort of this idea of forward guidance from the the the monetary policy itself like basically the as the out level of outflows this like time discounted sum that uh that Stefan was saying decreases over time then there's upward pressure on the peg from the reserve assets again kind of according to this uh this policy and then there's other mechanisms too you can check it out in our Docs and happy to talk more after uh after this as well and with that I'd like to thank both Stefan and Arya for the talk let's give everyone Applause thanks so let's take a moment [Music] foreign [Music] foreign foreign [Music] [Music] foreign [Music] for everyone else [Music] thank you [Music] [Applause] [Music] [Applause] okay now that everyone has found their spot please enjoy I'd like to welcome our next guest who will be talking about amplifying consensus participation with block space markets ladies and gentlemen Julian traversa traversa hey do you feel welcome hello hello okay uh awesome awesome uh thanks for having me guys I'm Julian from swivel Finance I'm technically the CEO but I do a lot over there um and today we're generally going to be talking about eth2 and the way that we can really use D5 products to kind of amplify both the Securities of the ecosystem generally as well as just the attractiveness of what we can offer in defy itself so to start really what is block space right block space is generally just a reservation of some computation space on the ethereum network for specifically the mutation of state right so generally speaking when you try to submit a transaction it's going to require a certain amount of computation and you then must reserve a certain amount of block space of course to transfer those funds or whatever you're doing also generally speaking right this is supplied by validators in eth2 it's applied by miners traditionally and the demand really then just comes from purely user transactions or Arbitrage or whatever it might be right so yeah going towards demand you see that generally speaking it actually is insanely volatile right it's very difficult to predict demand uh there are a number of just confounding factors that continue to screw up any sort of models people have made and really at the end of the day um this volatility is fueled by mutually extractable opportunities right Whenever there is volatility in the market itself uh there's liquidation opportunities Arbitrage opportunities Etc Whenever there are nft drops like that massive one up to 450 gas for an entire day um it just and it screws things up for really how people must price and create instruments around the eth2 market on the other side right we see that Supply is kind of weird right you you have an increasing number of validators um and this will continue to increase it really hasn't started to plateau in my opinion yet uh but on the other side of that right the actual block space within a given block is actually static so you really look at this and kind of have to think of it in two ways right and and when it comes to uh the factors of block space right you have to look at a number of things that will impact the yields of the validators and how that really uh makes the product attractive to them right you kind of have to make things financially incentivized um so then the question comes right when you're looking at block space in these supply and demand factors how can you actually properly price these right to start obviously everyone knows what base rewards um most people actually don't know that this kind of it does shift depending on how much demand there is within a short period right so if a block is full there's more rewards in the next block and essentially they just try to to move around on this direction um but on the other side of this right what most people are most familiar with is just purely the fees that you pay right if you want to get a transaction through in a reasonable amount of time you have to pay a fee and I mean the important factor is really that this has been very clearly an increasingly impactful mechanism over the past few years right you cannot really look at purely any of these base Rewards or other factors alone you have to really get abstract with your thinking so on the other side of that right you can also disincentivize negative activity um and this really just introduces more risks for validators than uh really people recognize at this point in time right there's an entire row of of blocks that were entirely uh completely destroyed in the past um and realistically you end up having to to sorry I actually took the wrong slide next one um you end up getting slashed and and and a validator ended up getting slashed I think on about 20 slots in a row um so it's pretty horrible this happens and there are real risks there that users must kind of accept really at the end of the day as well okay on the demand side or rather the the the apy that you can earn in the yield um we're seeing increasing amounts of this not coming from the traditional mempool tips and Etc but large blocks that are proposed by off-chain relays primarily flashbots right in a row here right you have about 20 or no I think that's about 10 within about 100 transactions um I know actually that's only about 20 blocks um and really these transactions allow people to do a lot and particularly right as a validator it's themselves you can begin to actually extract these opportunities from others interestingly we haven't seen a lot of malicious validators do this yet but that also does present a risk right and the final Factor right when you talk about pricing block space has to be dilution um generally speaking again I said that this isn't plateauing uh it's very consistent because there's a certain amount of validators that can actually be introduced per day and at the end of the day we've seen that over the past year um regardless of increases of yield based on you know Mev and eth2 merge validated yields have actually just been diluted by 50 if this continues right we will see yields crunch but you need to introduce more incentives for this to continue right you need to ensure that the the network is secure and you do that by continuing to introduce incentives for these parties so at the end of the day really what is the massive risk whether there's going to be demand and that and for pretty much every Factor other than slashing demand is just a function for it right so then when you actually try to apply this you get this massive formula where effectively you have the base fees the fees themselves MAV and tips and as well dilution all really kind of dependent on on demand as well as these time factors right if you know that six months from now there's going to be a five percent yield well you can actually predict things pretty accurately however if this is going to be massively volatile over that time period you need to actually do some pretty complicated stuff in there right so the question becomes why right there's all this math there's all this honestly it's crap involved in trying to do this um why actually create products that can can do it and at the end of the day it's really because you can create validator experiences that honestly exceed anything that is available today so getting into really how you do this right what are block space Capital markets at their core block space Capital markets allow users to take the future yield that they might generate on some amount of block space that they can commit and sell that to other people to reserve today whether that's in direct reservations like Eden Network whether that's through peer-to-peer transactions directly like alchemia or whether that's through larger validator pools and everything like Lido and Etc through the stack of swivel and on our end I think that some something that's very important to note right we split up the components into yield tokens and principal tokens so if you took a a deposit on Lido and you instead immediately gave it to us we would take that deposit we would split it into those two components the yield tokens representing the yield that would actually be generated and the principal tokens representing your actual deposit itself and allow you to actually trade those right so the most common use case there is then well I want to trade away my yield tokens that future yield to somebody else and the result really is that as a Staker you then are completely insulated from any slashing risks any dilution risks any volatility and on the other side the party is effectively leveraging massively on the rate right you can either hedge your costs if you are a validator yourself you can hedge variability or I mean at this point in time rate the gas price is literally on on a regular basis down to two uh you could speculate that it will increase over the next year right these are very very important dynamics that are standard honestly in almost any commodity but for some reason right now they aren't really an eth but then okay there are multiple designs to do something like this and the question really becomes what can we do with these designs right you have a base layer of staking to then and continue to increase the capabilities of your validators and in doing so increase the safety of your network and the question then comes in okay what are the things you are looking for and really at the end of the day it's composability you have the ability with these sorts of uh maturable instruments that are designed about around block space to create composable instruments that are continuously re-hypothecated the first examples being just composable principal tokens right I mentioned that this mechanism creates principle tokens at a redeemable one for one this is extremely important because this is a mechanism that is shared by about 10 protocols this then introduced the ability for hyper composability across a number of different yield generation sources ensuring that there is liquidity for these markets and they can continue to be built upon so let's say they can be really been looking back at the two different models okay well how can you continue to build on top of them um alchemia is a very popular project that is working in this direction they take individual validators and they ensure that those validators can find counterparties to purchase effectively those yield tokens um these these Agreements are again individual validators and they all have unique maturities which effectively means that they are completely non-fungible they cannot be rehypothecated generally and you cannot really trade them on secondary markets the upside being on valchemia I don't want to just crap on them you know you can address this Market of individual validators and there are a lot of bespoke validators out there however right if you want to build a more composable product you can combine these sorts of things with Lido with rocket pool and instead of having an agreement that goes directly from validator to a contract buyer you route that liquidity through a third party validator that is composable like Lido light rocket pool and then through them you actually create this contract and by doing that right you then start to have the ability to create more and more interesting instruments a first example this is a friend of our protocols they they let people take those principal tokens and borrow against them at extremely high ltvs around 90 to 95 percent in context that represents the ability to leverage on the stake deal wait for way more than anyone really ever would it allows you to continue to hedge in many directions and at the end of the day it continues to provide utility to these stakehilla products that allow you to make more composable interesting things this is probably my most exciting slide or the one I like most there are really a ton of advantages native to D5 for fixed maturity instruments that are not replicatable in traditional Finance primarily the ability to like I've said rehypothecate atomically and in the context of these maturing instruments a principal token let's say you have in this case a hundred or a thousand principal tokens maturing on December 31st can then potentially be used as collateral to underwrite options right in this case again you'd take that principal token you know it's maturing on December 31st and you would find a counterparty that would accept that knowing that they would also have that face value at December 31st as well again this is a completely unique market for D5 right try going to a bank with your bonds or with your T bills that you bought this year at four percent right that everyone's bragging about and asking them if you can underwrite some puts or anything with it right you will find that they will laugh you out the door and it's like it's just kind of a joke to them um again I think this is probably one of the more exciting things going on right now the ability to take these assets that really are only traditionally used in one way and to combine them into these these Structured Products that allow you to utilize them further in this case specifically you have the ability to lend your stake yield right you're capturing the the base yield the Mev the tips you're capturing it just everything from eip559 if that was happening you're capturing a lending yield on another protocol and then in addition to all that you can capture an options yield and going further down that rabbit hole right here's another protocol that is another friend of ours where they effectively create expirable Futures products uh it's honestly kind of complicated how they do it right uh but the bottom line is they they use fixed rate markets that themselves can be backed by each consensus to create these expirable Futures and in context it's actually very interesting because the most common use case of expirable features it won't go back is hedging options um so at the end of the day you actually are already seeing this this symbiotic ecosystem where users are able to hedge regular just trading markets in both directions while getting additional yield and all of these list liquidity is then routed through the base eth consensus layer so really that's that's the big conclusion I'm trying to to push right is that at the end of the day composability is the king thing or the largest thing that you can do to ensure that the eth consensus is secure and beyond that ensure that users are continuously attracted towards these products right I think that we have an extreme opportunity to at this point capture uh the attraction of people that really at the end of the day are more oriented to Trad Phi uh the the big statement that is always said is that you need to create a user experience that is 10 times better to onboard them from one platform to another and really only these defy Unique Products will be able to do that um so yeah that's really the point at the end of the day every derivative instrument in D5 every pretty much everything other than perps will be being routed through eth2 and at the end of the day as well this will all be being routed through these fixed rate instruments I'm Julian I'm the founder of swivel Finance so feel free to reach out uh follow me and or join our Discord at uh discord.gg mobile file for just swivel um yeah that's it any questions [Applause] yes we have room for well several questions actually who has the first not everyone at the same already yeah oh there yes foreign you're talking about validator supplying block space yeah um but the block space remains constant throughout like you said so what they I mean so how do how do you define as I'm supplying it as opposed to like them constraining it or selling it at the end of the day while the block space from the demand point of view is static the suppliers are still competing to be the one that actually provides it if that makes sense right so when looking at pricing this future yield it'd be ideal to just have exact metrics for Block space directly in a given block but you really have to be looking at heuristics right um gas price and all these things and you know in in context it's just about you know kind of trying to get anything you can to just derive the the base block space rate like something that you should try to price and sell your yield at over here ah so you mentioned uh block space and uh those markets how does the fungibility of blocks block space uh make pricing harder it's extremely difficult to honestly uh create any markets without fungibility honestly we were looking at creating swivel initially with a sort of non-fungible agreement similar to alchemia but when you start to do this you Silo off liquidity and users are no longer able to compare kind of what one other person would do for it right at the end of the day most pricing is actually rather subjective and you need contextual markets in order to price things right um I talk about this a lot more in reference to just the fixed rate space in general is extremely important to ensure that maturity is themselves differentiable for even these these sorts of similar activities foreign the distinction that you you've noticed which is that for these kind of like splitting and like re-hypothecating um future staking yields uh it kind of requires going through uh you know central-ish validator of some kind generally speaking is there any way around that like is there a world in which we can create uh fungible claims to like individual homesteakers a I need to hope that honestly Alchemy is working further in this direction um it's more difficult than one would think I'm sure right because you actually then have to actually you have to create the infrastructure between a validator client and some dap and honestly I don't think anyone is doing that whatsoever yet um really it's going to be a difficult problem for them to solve I think they can you know create more fungible markets is kind of the way I put it by by forcing people to use a certain maturity and only trading within that right um but at the at the end of the day I think it's more of a technical problem than a market Design One probably would the return profile and the volatility of the split out yield aspect of it not just mirror ethereum anyway it would yes that's the point and when you if you split away your ethereum you're not it's not going to do anything what you need to do is find a counterparty to buy the the yield token the split away yield that you're going to be earning over some future date right um and that's why again it's very important to try to create these markets that are fungible in liquid otherwise it becomes increasingly difficult to actually find these counterparties and then as well right if you don't have a secondary market for anyone to sell anything on it's impossible right so you really need really answering your question you just you're selling you're selling those yield tokens to another counterparty and that person is going to be the beneficiary of whatever eat staking Etc yeah awesome okay thank you Julian [Applause] next talk will be a panel with four members we're going to set up the stage for that first so take a moment in the meantime to take a bathroom break or grab a cup of coffee and we'd like to see you back at four o'clock thank you [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Applause] [Music] foreign [Music] [Applause] [Music] [Applause] [Music] foreign [Music] [Applause] [Music] yes before we start please everyone come in grab a seat there's more seats in the back here please everyone come to the back so everyone can fit in the room you get my mic and then at the Q a I take one for you guys okay everyone is in then we're going to the next talk the topic of today will be a battle of the bridges untangling the trade-offs of various Bridge designs we have for that with us Chris Winfrey hartlander tarun chitra and Uma Roy give them a warm welcome everyone [Applause] hello there we go sorry thanks guys so I'll just I want to introduce our moderator um because it's not turning chitra uh uh this was supposed to be our moderator but we canceled him because he had to leave a little early and instead we have his alt an even bigger brain uh moderator wait where are we Guillermo um Guillermo is I want to introduce Guillermo he is the head of research at Bain Capital crypto uh and he also uh is a PhD from Stanford like all these other smart people writes a bunch of research papers and is going to ask us insightful and probing questions to get to the heart of the trade-offs of bridge design and the future of cross-chain communication um so thank you Guillermo for moderating this well thank you for the far too kind intro and in fact because you've said so many nice things about me that's how I know you're full of but it's fine um anyway so yeah so I guess as a first you know let's start with the all the big intros is you know we have you three I know two of you quite well Chris unfortunately I don't quite know you so that means I'm gonna pick on you to start uh give me a 30 second intro of who are you what do you do what have you done to deserve this and what have you done to be up here what so I'm Chris Winfrey I'm one of the co-founders of hop protocol um so hop is one of the only bridges that hasn't been hacked yet oh spicy closing in on uh three billion dollars in in volume so far um before I was doing hop I was a security auditor um covered projects like opens up one framework auger dydx uh to Central and and a lot more is that correlated with the bridge not being hacked yet or I think so all right I think Hearts unfortunately you are next sure guys uh I'm heart Lambert I am the co-founder of a protocol called across which is a cross Chain Bridge we'll talk about the design and how it compares to hop and other things shortly run me like this yeah yeah I won't front right here um uh I am also the founder of a protocol called Uma um not this human correlation no relation um although there is a funny story about how Uma heard about Uma early on but we won't go there uh uh but um we are an optimistic Oracle Uma is an optimist Oracle and we use this option to coraco as a security model for a cross which we'll get into um across also hasn't been hacked um so I think congratulations I think yes uh so far at least so far um but we'll we'll talk more about that design pattern or the design trade-offs going forward hi I'm Uma I'm the co-founder of succinct Labs we're a newer project company uh that started just this past summer and we're working on a proof-based bridge so also we can talk about the design uh people also call this a ZK Bridge there's nothing like zero knowledge about it so I like to call it a proof-based bridge and yeah we can just talk about how all the snarks work and how um yeah our bridge is going to work cool so um I guess to start bridges are kind of a fraught area in a lot of ways not just because of the hacks but I mean like you know even definitionally people are like oh like that's a bridge or that's not a bridge or whatever um and there's a bunch of categories of bridges right so um first things first I guess can one of you whoever is feeling the spiciest or at least spicy depending on your day uh describe to me like what are the categories for like what what like there's like an ecosystem of different types of bridges you know there's like succinct bridges that use like ZK the power of ZK trademark uh there are like kind of you know a bunch of other designs so well actually maybe before we even yeah so let's let's set the definitions first can anyone give me like how these Bridges separate into like their respective categories so to speak I mean I'll shoot and go and say it and I think you can broadly create like talk about asset Bridges bridges for bringing bridging assets between blockchains and let's call it data Bridges or messaging layers or arbitrary message Bridges which are for bridging data okay our do data Bridges contain acid Bridges as a subcategory or not I think there's I don't I think we could probe that I'm not sure I think it depends on design I'd argue every asset Bridge Is underpinned by a data Bridge okay I would generally argue but there's sort of some nuances because for example a cross uses some of the canonical data bridges too um to bridge assets okay so fine so we've been listening to Bridges and then acid Bridges and you're right I think you could maybe do a subset thing here too with like some caveats or asterix's sure and then on the acid Bridges I think one important um thing to discuss and potentially agree on is dividing them into wrapped asset Bridges or what I sometimes call like lock and mint Bridges where you take an asset you lock it on its uh home blockchain and you mint a representation of it on a destination Chain versus what I will call like um liquidity Bridges where there are canonical versions of the asset on both the source and destination chain and the the bridge here functions almost as a convenience metric or convenience tool to easily move that asset from one chain to another and Chris I want to see if you in particular agree with this because I think we are both in the category of like liquidity Bridges I yeah completely agree I think we're both liquidity Bridges and then I think you could even break up each of these categories into a bunch of subcategories because there's you know tons of different uh approaches like snarks the Hub and spoke model and you know the the one that's been kind of the worst in terms of security which is uh multi-sick Bridges okay so fine so you both chose to be uh what do we call it liquidity based Bridge the liquidity Bridge fine uh why not for example a wrapped acid Bridge like does it matter do you care is it an interesting thing so um you know we actually rely on these wrapped asset Bridges to exist so every single roll-up has a wrapped asset bridge and and you you know you deposit into the layer one contract and a representation of that deposit is minted on uh Layer Two so with Roll-Ups you're able to kind of secure the whole wrapped asset Bridge with a smart contract but where we've seen things go wrong is when you have a wrapped asset bridge that is a multi-sig because racked asset Bridges need to hold a lot more tvl than liquidity Bridges because they're not just holding enough liquidity to kind of facilitate cross training transfers they're actually holding all of the liquidity uh of that cross-chain asset and so what liquidity Bridges do is we basically bridge between assets that are produced by wrapped acid Bridges I'll say vertical correctly I think Uma has a spicy take on this well I want to interrupt and put a bit little Nuance in this too because I I want to further differentiate between maybe nuanced types of wrapped asset Bridges so Roll-Ups like Chris is right that when I want to create uh like a version of ethereum on a roll up I'm like locking ethereum on a I'm locking ethon ethereum and producing that version of it on the roll up but I would argue that's not exactly the same type of it's it's different because it's adopting the same security parameters as the roll-up itself and so if you trust the roll up you're fundamentally trusting this supposed wrapped asset so I almost refer to that as like the native version of that asset on the roll up the type of wrapped asset bridge that I think Chris is on and I totally agree um is the one where you literally do just like take the wrapped asset lock in ethereum and then there's like an oracle or like a different security method that's potentially super insecure that's minting a representation that on your destination chain and then for the user of that asset on that destination chain that user has Perpetual risk to the security of that Oracle in perpetuity and if that ever breaks that user that just thought they held this valid asset and left with a donut and those are the designs that I think are really dangerous and not cool so sorry yeah I think your point of these minting Bridges having more value locked up in them uh I like to think of it as basically if you're minting if you're a minting bridge you have the integral of all the volume that's passed through your Bridge whereas if you're a liquidity Bridge you're going to have the derivative because at any moment in time you only need what's going to be in flight and so I think it's actually like a much more elegant solution uh to to the problem because you don't have this huge honey pot sitting there that's like waiting to be attacked and even if it is attacked in liquidity Bridge that's bad but it's not as bad because you just have the instantaneous liquidity you need cool okay so I don't have I don't think I have any additional questions about the specifics of liquidity Bridges I will have additional questions which we'll start right now about um what is your liquidity Bridge like how do you describe it what is the architecture how does one construct it like what is this thing sure so with hop we we've kind of come up with this like Hub and spoke model where uh we use ethereum to pass messages between um different networks so these are layer twos or side chains like polygon or gnosis chain and so we leverage each of the Native message Bridges so each of these kind of Layer Two networks or side chains has a message Bridge with ethereum where we can send messages and what we do is we actually bundle many messages at the source chain and then we'll miraculize those on chain after you know many messages have been sent we can propagate this message or this Merkle root through layer 1 ethereum to the destination and this is how we can do scalable but but very slow messaging and then we add a liquidity layer on top of that where we have liquidity providers that you know kind of front uh liquidity when a user makes a bridge transfer and then as those uh transfers happen many of them get bundled up the Mercury gets to the destination and then everything is settled and so that's how we're able to bridge across different layer twos in a way that that you know does have the full security of ethereum if you're using optimism and arbitrum and then we're able to actually put these stock gaps between each Network because we have this Hub ethereum contract and this lets us support kind of you know side chains that don't have the full security of ethereum without exposing uh users to the risk of those side chains if they're on a you know a true roll-up so essentially but hop only allows you to do layer two two layer two transfers or like anything with where you have a you know one kind of consensus layer a truth layer so to speak that's kind of below it so Hopkins support any Network that has a message Bridge with ethereum okay yeah God I got it got it um heart tell us about so yeah there's there's a lot of similarities and then some important differences so across actually only supports uh chains that have a native asset Bridge with ethereum so what a cross does is we actually only have a single liquidity pool of the protocol the way to think about it is the protocol is effectively the settlement layer so you have a single liquidity pool and the protocol will send assets from that liquidity pool to various uh spoke uh destinations um that can be slow that can be painful that can actually be quite costly so what across does is we have a a third-party actor we call a relayer and so if Uma wants to move assets from one chain to another this relay will effectively front them Capital they'll they'll front they'll send the assets uh immediately so very quickly to them on the destination chain and then it'll request to be repaid optimistically from the liquidity Network in the protocol and so the advantages of this is we since the protocol is kind of continuously rebalancing its liquidity between its hubs and spokes we like to think of this as a very Capital efficient design where we're trying to use as little Capital as possible to do as much volume as we can and kind of the uma's earlier point the sort of thesis here is less tvl better per unit of volume so the more Capital efficient you can have not only are like fees lower um you know tvl has a cost of capital cost of capital you gotta pay for so if you have less tvl you have lower fees um but also you have less security risk there's just less of a Honeypot so essentially in your case the protocol and the relayers themselves kind of act as like market makers right you know they are taking on some like Risk in like kind of doing this optimistic like fronting there's no like so the the relayers are effectively making like two hour loans right um but that is risk right there's risk like there's different it's not really like Market risk they don't have risk to like the price of ethereum going up or down type thing um uh if they're sorry I should be clear if you're denominating your if you're holding that ethereum and you're not planning on selling it if you think of it as like stuff you own you're not taking risk you're making a loan in this in this asset sure right and like roughly speaking but it's I still should be compensated right like in some sense it's ethereum that I don't have but I could just like sell or do something with how am I being compensated for doing that yes you charge a small fee okay so it's basically like you charge like a two-bit fee on this uh this loan two bips over two hours annualizes to a very healthy return and again there's ways where this gave you have more competitive it could be auctions or whatever else the other thing that's kind of interesting about our relayer design is the relayers um they get to choose where they get paid back so for example a relayer if Uma was moving assets from arbitrary optimism the realer could ask it paid back on polygon so a kind of interesting feature of the design is if the relayer is like a market maker or kind of the person that's generally doing arbitrage-like things they could actually get paid to move their assets potentially to a chain where they wanted them just kind of interesting interesting okay okay and then I guess we have a bit of a less economic current Focus Uma but a very interesting you know bridge design yeah I think our Bridge um or what's the Sanctus building is not a liquidity layer it's not even necessarily like a token or asset bridge I think uh we're starting off by really focusing on the arbitrary message passing part so I think that's like a big difference uh the high level design is basically similar to IBC how you have like a light client running in the execution layer of a Target chain for a source chain once you have that light client for a source chain running in the execution layer of the target chain then you have access to the source chain State similar to how you guys I guess use like your the you propagate the Merkel route to send a message here if you have like ethereum's state route on gnosis chain then you can prove things about like oh stuff happened on ethereum so you could have a arbitrary message Bridge contract on ethereum that users send contract calls to and then a relayer would send a proof uh on notices chain for example that like oh this message was sent on ethereum and now you should like execute this corresponding message on gnosis and so when you have arbitrary message passing or arbitrary data passing of course on top of that you can build a token Bridge you can build a liquidity layer you can build all this stuff but yeah I think our main focus is basically on making this like trust minimized cross-chain communication layer to start on the topic of trust minimization which I suspect you have at least some opinions of um maybe unclear um so first things first how does trust affect kind of the safety of a bridge A and B uh you know kind of the maybe like higher maybe mid-width question is uh can you make a safe Bridge right like can you like Bridges seem fairly complicated in some way or another you know it's like a lot of moving Parts potentially you're dealing with like many distributed systems um Can is it possible to make a safe Bridge or are you just are we all just like kind of screwed like is it just like this like you know sofian task of pushing a boulder up a hill repeatedly just to have it hacked or something yeah I think the trust assumptions are really important so currently most bridges have basically like a multi-sig or a multi-sig in some form trust assumption and so that's really bad because of course for something like ethereum where you have many many validators and a ton of Economic Security your multi-sig's never going to have the same amount of security so I think the trust assumptions are really important and if we can basically have a bridge out of ethereum that has ethereum's level of security securing it which in a light client you can have if you're verifying ethereum's consensus in the execution layer of another chain you can okay so it's actually a little more nuanced you're not getting all the way there but you're getting significantly more there than having a multi-sig I think that's a huge Improvement in terms of the trust model of a bridge and I think that's really important I think of course with a lot of bridge hacks there's some Bridge hacks have been based on the trust model so like there's been some protects with a multi-sig where they actually compromise like the multi-stick signer keys and so that's an illustration of oh the multi-sig is very insecure there's a lot of other Bridge hacks which are just smart contract hacks or like smart contract bugs those of course it doesn't matter what the trust model was like you know for example the most best snark based approach could also have Smart contract risk um and so I think it's really important to like be intellectually honest that there's risk in both dimensions like there's risk in the trust model there's also smart contract implementation risk how do we reduce smart contract like implementation risk like do we what do we do formally verify everything do we just go and like add in a bunch of stuff and say like solidity sucks and we go and like write a new language that is like rust or whatever I mean Guillermo you're the crypto researcher I mean I I don't I'm like a fake researcher I mostly do like a bunch of weird math but I can pretend no I mean I think look I think that's a whole other uh panel or conference wide topic of like how do you write but I'm gonna make you give me your two minute uh sorry what do you do you write down the moderator so I can make it yeah I think you do all the things right you um you have really good Engineers that are smart and thoughtful okay step one you audit your code from credible Auditors or you are an auditor and you still hire Auditors so it feels like an unfair Advantage right that's fine all right but uh you audit your code uh formally verifying your code that seems like a great idea right but formal verification it has its own trade-offs I think of understanding that and writing the rule set actually Chris might have opinions on that too um then money program right and then you also just like you you last like part of the reason why you gotta like time the the more surf the longer you survive I truly believe the more credible your product is um because it is a very antagonistic environment out there and so just surviving I think is uh is is increasingly uh it just means you're you're more the longer you survive the more likely you are to survive in my opinion yeah I think there are other smart contracts out there on ethereum that are much much much more complicated than a bridge for example I think if you look at the maker smart contracts or the compound smart contracts compared to a bridge smart contract it's much more complicated there's much more moving pieces and people have seemed you know of course those things have bugs too but like people have seemed to figure out ways to make these smart contracts in ways that are you know bug minimized or like not catastrophic and so I don't think they're theoretically seems like oh it should be impossible to make a secure bridge I actually it's maybe even a good question why hasn't it been done until now because in my opinion it's not even the most complex smart contracts out there you probably have opinions on this Chris I I completely agree I I think those are you know both great answers like we need to build up our our lindiness uh to you know as as Bridges exist for longer and longer without being hacked the you know the more uh we can trust them to not be hacked in the future um but you know one thing I wanted to really Hammer hone which you touched on it's if you're using a trusted Bridge you're not just trusting whoever these parties are you know it might be you know sequoia or a16z that's holding this key but you know what we've seen is that over a billion dollars has been hacked from multi-sig Bridges and it's not that these parties are just like turning malicious and running away with the money you're trusting these people to have world-class security this is not just you know average everyday hackers that are that are going after these Bridges this is you know it's nation state level actors you know like North Korea's Lazarus group they are trying to insert employees into your company they are you know uh doing supply chain attacks where you know introducing uh vulnerabilities like deep down into your software uh stack so you know in terms of trust model it's just you really want to rely on the smart contracts some themselves for your security yeah like I mean again let's emphasize the trustlessness and permissionlessness are like they're not really features they're requirements ultimately and again if if you just want to trust somebody then like you don't need a bridge you just use binance as your Bridge right and like fine my favorite purchase coinbase yeah it look works right if that's what you want right and I think the the other angle to look at this too from the multi-sig angle is also censorship um and Regulatory issues and like wait can that bridge even get shut down because someone says Hey a government says hey don't do this anymore you know yeah I think as the regulatory landscape gets more and more like strict having the multi-sig is going to really be tough just from like a regulatory perspective and I think that's why like the snark approach or this proof-based approach where it's like for these like clients you know theoretically anyone can generate the proof anyone can send the proof to the smart contract on the other side anyone can relay message and it's like truly uncensorable it's like root going to be very very important one thing one thing you said earlier was that your approach is trust minimized but I'm kind of curious why you said that and not trustless because it does seem conceptually trustless to me yeah I think trustless is like a really strong term and a lot of people have a lot of feelings about it uh so yeah we try to be aware of that I think it's trust minimized because ultimately you're still trusting the consensus of the other chains so for example if you know okay ethereum has really good security Economic Security but say I'm bridging from another chain back into ethereum and say that chain only has you know 10 million dollars of Economic Security uh I think it's you know it's not trustless you're still trusting the other chains of validators um yeah cool fair enough so essentially kind of the way I can summarize this latter part of the panel is that like smart contract security anything that's unsafe is just cope right like you are like if you're just you're just not you're just doing it wrong if you just like have I mean right like you were mentioning right maker Dao is like this massive set of contracts and like I don't know has it been hacked as far as I know like so I mean what is your perspective on this like how do you make smart contracts secure more generally I ask you this question so I'm going to put you under that um but sorry it's just happening sure no um so actually so the the most num in terms of number of bridge hacks has uh been just regular smart contract vulnerabilities and yeah there is no reason for these vulnerabilities to be Bridge specific you know a lot of them are uh signature based um or some of them are just you know regular smart contract bugs and so when when we design smart contracts we design them in a way that makes it easier easy for an auditor to look at the code and uh verify our security invariance so you know these security and variants are you know if if a token um kind of like starts a transfer here is it going to be completed here can you know multiple completions happen at the destination uh is there some other way for for someone to kind of like extract value from the bridge um and so if we can you know know basically make it as simple as possible to verify those things the the easier it is for Auditors to kind of catch any bugs that might show up essentially like what you're proposing is well simplification for sure but also some notion of formal verification or is that not actually it I think there's cheaper ways or more cost effective ways to to get security than than formal verification okay and I guess as a follow-up question what does it feel like every bridge has been hacked and not like every smart D5 contract has been hacked is it just like a bridge specific thing is it just like everyone decides to hop on a bridge and be like oh this is cool like we're just gonna like put our money in here like what what is it like it feels that way because it's kind of true um and honestly I think it's because it's a new market and it's a it's a you know there's a big opportunity and so a lot of people are kind of just jumping in a little too fast Auditors were very very constrained during the bull market so it was like really hard and really expensive to book a good auditor and but people still wanted to be you know in that market early and and so we saw a lot of them get hacked yeah I guess I'd say like Chris Kirkman if I'm wrong but I think all the hacks are actually smart contract tags right like all of them none of them have been no I think some one of them the oxy Bridge was a multi like they compromised the keys sure I'm sorry I should say that yeah okay fine multi-state compromise the keys but in terms of like um like your trust model our trust model this optimistic trust model that hasn't been the source of those hacks yet right yeah so the the uh in terms of value lost um it's been uh key compromises and then in terms of number of hacks it's been smart contract vulnerabilities interesting okay and so with that you know it kind of like we're taking a little bit of a digression here but I'm curious so do you think the bridge space is like fundamentally a Cooperative space or do you think it's very much a winner takes all space in your case and what would the winner look like I mean I guess if you knew you would already be building it but or maybe you are building it but like fundamentally it feels like to me but you know a bunch of people are building bridges right now but really like kind of you know if you build a good Bridge there's the best bridge and that's kind of it like it feels like there's like a metric by which we can measure Bridges and be like this is the best damn thing possible anything else sucks we're just going to use that wait you I don't know that there is a single metric you can look at bridges on like let's talk about acid bridges for a second here where you want like there you want three things you want to be fast you want it to be cheap and you want it to be secure and so I want those three things give me Max of all of those right sure um and so I I think that it's not necessarily clear that that's going to and maybe you want permissionlessness too I'd add that in there maybe that's in security so like look I think on the asset bridging uh it's going to be very price competitive asset Bridging the the kind of the model here is fees how much fees are you taking well maintaining a minimum level of security and that minimum level of security should be like really high because you find language um um it's okay yeah trustless and all that yeah thanks for we're all adults here but um uh so I I think that's that's kind of interesting like and I think Chris we can talk about this like acid Bridges it is going to be very competitive it's great for the consumer by the way right um but then I think the other thing we can go into is talking about like data Bridges I find it really interesting how you even charge for that or like what the fee models should be and I also don't think that's at all figured out yet um either but maybe go yeah I agree um you know for an end user the the risk for a bridge is act at least for a liquidity Bridge the risk was actually really really low um you're only exposed to a bridge for for the brief period that you're crossing the bridge and then you're you're out and so you know in terms of security it's really um or in terms of cost you know it it comes down to like uh how secure you are and how much you're going to have to pay uh for liquidity and the more secure you are the less you'll have to pay for your security and the cheaper it is going to be for end users but end users don't need to think uh too much about the security of the bridge because they have such limited uh exposure yeah yeah yeah I think for users like end user token Bridges have slightly different desired properties than for example a data Bridge that's maybe transmitting like governance results or like other actions on you know ethereum broadcasts out or vice versa so I actually don't think it is like one number to optimize that function that depends on like who your user is like if your user is a consumer they're going to care about different things versus like a dow versus you know whoever else it's going to be so I guess well I guess we're gonna add one other thing around this so like your question was is there like a winner takes all Market here and on the one other concept that I think is worth floating here is this idea of it around Capital efficiency where if say for example your Bridge had required 20 million dollars of tvl but could do like a billion dollars of volume a day and you were able to charge like a basis point on that even if that 20 million dollars gets hacked or whatever there's probably still a reasonable model in there like business model where the the the profit of the bridge it was worth the risk of that capital and so again I think the space here where I think the bridge space has kind of gone in the wrong direction is like locking up lots and lots of dollars which is for for well also not making that much revenue and like that's that's you got the ratio the wrong way around right and so a winner tick all thing I don't actually think this is a winner takes all Market on the asset side I think that um being able to do a low volume thing with minimal tvl like lots of capital efficiency I feel like that will be competitive maybe not again Winner Takes all um and then on the data side again I I think some of the ZK stuff that Emma's working on gets more and more compelling and interesting but I'm also I don't know do you think there's been a winner take all on the data side well I think of course I'm a snark Maxi and I wanna I want all day to transfer to go through you know the succinct like clients the succinct you know message Bridge um you know whether that'll happen I I think it's hard to reason about I do think uh one thing I will say is I think this like snark based approach for assisting to like clients and proof of consensus is kind of going to be the end design of like a data Bridge especially where if you for example if you like don't necessarily care a ton about latency like I don't you know and maybe you can layer a liquidity Network on top of it eventually but I think the snark based approach is technically feasible like it works today we just need to do some more engineering and do Implement proof of consensus for all the chains and just implement it but it works today and it's you know I think it is like the end design like I don't necessarily think you can do much better in terms of a trust model yeah I think we'll probably see a power law distribution like a lot of other markets um and the other thing we haven't talked about is you know there's a lot of assets out there and it's hard for liquidity Bridges to support long tail assets because you don't want to just like maintain the liquidity for for an asset that no one's using to bridge right and then there's also going to be longer tail networks in terms of uh what's supported so you know we'll probably see um you know bridges that are kind of further down that power lot that they'll probably attack the longer tail assets the longer tail networks where they can gain market share and and the the bridges that are kind of at the top uh they're going to be focused more on security and and uh making sure that their LPS don't need to uh be paid a ton so um I'll bring it back really quickly to the original question the reason I asked this is you know in a lot of ways what are you paying for when you are like you know giving a fee to somebody is you're paying partially for the fact that like that thing incurs a risk right so in some sense like security and economic efficiency are like highly correlated so you can think of them almost as like one single number right like I don't think you actually do better by being less secure and economic in terms of economic efficiency or at least not a lot better right um so this is this is where this is where the question comes in which is you know in some sense right if you have like the maximally secure Bridge like it's very likely that it's also the most economically efficient Bridge uh and if not I mean like maybe not is it is that the case like do you think that there could be like you could like somehow lose security but still have people not pay very much for like the privilege of like you know bridging their assets yes I I think there's actually three costs so that there's gas costs um there's time value money and then there's the risk that that that um money is taking Beyond just uh you know a riskless uh feat yeah so I'm gonna I'm gonna make the simplifying assumption here that gas costs are are you know with magical scaling results going to zero so let's pretend on our magical fairy world at scaling as we've seen like 70 talks on are all perfect and beautiful and we get like approximately zero or negligible costs that would be great for the snark based bridge that would be great in a lot of ways we're waiting for that future yeah and so it actually becomes a really complicated question like I don't think we could uh you know get all the way to the bottom of capital efficiency here on stage um really there's because uh you know both both across and hop have both active liquidity and passive liquidity uh there's different challenge periods so that gives you know different trade-offs between um you know what these different liquidity providers are are um kind of taking on um and then yeah there's also uh you know different just different models in terms of you know how things are bridged so you know with hop you know you can kind of think uh we kind of think of capital efficiency in like three buckets um so we we have like the active liquidity provider um that uh Cycles every 24 hours we keep very long challenge periods because we think it's important to have like a human response if if things go wrong like you that your team needs to be notified you need boots on the ground and and you know be able to know that if your infrastructure is down you can have your engineers get it back up and and protect your Bridge um and then there's the the passive liquidity providers and so hop uses this amm model that we haven't talked about yet and so we we have this like intermediary accounting asset and we're able to kind of shorten the challenge period from an optimistic roll up from seven days to one day and so that that's our 24 hour challenge period And so you can kind of think of the ammo liquidity as as just taking on the seven day uh Capital lockup only for the net flow not the total flow right this is what Uma was mentioning as well right in some sense you can think about it as the derivative of the flow as opposed to like you know the integral over all of it but sorry okay yeah and and so and so that's partially the amms but also partially you know arbitragers and amms are really that's where they're taking on the seven day of the net flow and then the amms themselves uh that kind of scales to transfer size but not transfer volume so you know we could have a bridge that that has uh two amms that have 10 million uh tvl each and you know that's going to be able to support pretty large transfers very efficiently but that can scale up to you know virtually any volume uh as long as the transfer sizes are are you know below what a 10 million dollar pool could handle so I guess for just for a little bit more context so here the automated Market maker is act seeing as a way of so if I let's say I I don't know I have eth and I want to get like whatever wbtc on some Layer Two I can instead of kind of bridging eth and then swapping it I can just directly kind of like Bridge swap uh in is that correct or yeah okay so because I'm still on unclear also um I guess before we kind of get to that for context we're talking about so there's two layers here that we have right one of them is like the data layer which is kind of what we were talking about previously and what security models we have and there's kind of like the economic layer where we're talking about economic officiency that kind of is built on top of like a messaging layer that you can trust right and I want to I want to make keep these two things separate so now we're right now we're talking about like the thing built on top of like how do you transfer assets correctly and how do you like ensure that you can verify that these houses have been put into one's applied and whatever um so let's while we're on that specific topic afterwards we'll get to the messaging part which I think Uma is uh has some other spicy takes on but um okay so but anyway so sorry describe this amm design or like what what is the point of this automated Market maker I guess at the end of the day sure so so uh the the main purpose is actually to one kind of uh price liquidity flows between different networks and then it also you know there's no way if you're just using the canonical assets to to not take on that that full seven day uh lock up for the the total flow of of assets and so because we have um we basically have this uh intermediary accounting assets so it's like a h token got it got it so you can essentially pay to reduce the risk so like let's say you're waiting 24 hours like instead be like I'm gonna Swap this on some exchange right which lets me actually have the native acid and someone else can take on the risk basically we can exit our these H tokens in 24 hours versus the canonical assets um that are exited in in uh seven days and you know so it's like eight Chief eth is the pair and it's basically a market between the the uh 24-hour exit time in the seven days got it got it got it so essentially prices the risk that you are taking on by waiting whatever seven days to interrupt your asset versus 24 hours like yeah as well as uh demand flows between that's right that's right that's right yeah the thing I'm just going to add just maybe zoom out a little bit um like our take across his take is that there is this is a financial engineering problem like if you solve the data uh and solve the data message you have a financial engineering problem and I think the more Innovative you can be with your financial engineering the more Capital efficient you can be right um and so you know again hopping across share a lot of philosophies the the one thing that we've done that is quite different is we don't have these amms at all instead we are actually just having this single unified liquidity pool and we're using the native Bridges to kind of rebalance assets between this Hub and spoke model and if I'm going to like shill our product a little bit the one the one thing that I kind of like about this is I think that that liquidity model allows us to be a bit more Capital efficient than having amm pools that have to be kind of rebalanced off chain um yeah so so like I said the the amms um it's only based on uh transfer size they just need to scale the transfer size not transfer volume so even you know if if we were to have just like 2 million those in each amm you know we could support you know unlimited volume if as long as you know the transfer sizes are below that threshold and arbitrages are coming in and and are being that out and balanced you need like yeah I'm gonna push back a little bit because you need the flows to go both directions right if I keep on pushing right in the direction you're going to run out of liquid or you can make it an ordinately expensive and that's why I say that that um you know hops cost of capital is you know for the arbitragers it's it's just seven days of the net flow so if you know there's a perfectly balanced flow between arbitrum and optimism um we don't need arbitragers at all okay yeah and it's like this Concepts again like call it like basic basic Financial engineering Concepts or like Market making Concepts is if you can cross flow like if you got a million dollars going this way and you had a million going so I was going the other way it's beautiful and not only that but it means you have like really low costs you don't have to rebalance things just did it all works out beautifully and so again I think a lot of the financial engineering is how can you get how can you incentivize netflows and or have a low cost when there are imbalance flows have the lowest cost of capital and uh there's lots of like fun Financial engineering tricks in that I think but let's maybe go to it so so next up is you know fine so let's go back so when let's we've taken this first part right let's like zoom out and go to the second part which is uh honestly what underpins the entire Financial infrastructure you guys are talking about uh and it is okay so now I ask you the question of do you think that data Bridges will themselves all kind of monopol go into like one single data Bridge right like why not you know kind of but why doesn't this become finite like just like standard infrastructure that's open source that's just like classic normal everyone uses it how does one even monetize this thing um you know for example IBC works pretty damn great right and like the idea would be IBC for like ethereum at L would be very cool and obviously that's kind of what you're working on but how does one monetize that I mean IBC is just I just implement the Ping and then that's the standard yeah I think what we're building at succinct is going to be like open source and we view it as like a public good because it is like a public good for all these ecosystems to have these succinct like clients and have this like snark proof of consensus um for the consensus algorithms and honestly it goes beyond just having a bridge like you can imagine this actually being really useful for like like clients and wallets so that you don't have to connect to a centralized RPC and you can like do something peer-to-peer you know something further down the line in my opinion wallets have a lot of other things they need to do before that issue but yeah down the line I think the assisting light clients are important for things even Beyond Bridges um I think yeah that being said so I think we want to build this in a very public goods oriented way but I think you know just because something's a public good doesn't mean that oh it has to make no money like for example operating these succinct like clients is really hard so you know and believe me like I've had to write the infrastructure to you know watch one chain generate the ZK proofs and then set up the infrastructure to Ping a bunch of different like clients on a bunch of different chains and this is not infrastructure that you know it in a timely manner that you know people are going to want to run themselves like probably there's going to be an operator I mean currently us that does this and then has guarantees around it like okay if you're gonna rely on our like client you probably want to you know know that it's going to have like a 99.999 uptime and so I think I really view this as like where what we're building is a public good of course it's going to be open source of course it's going to be audited the more people that can contribute to it and like we can all make sure it's like canonical infrastructure that's really secure and everyone feels really good about but then operationally it's kind of similar to like an open Core Company where if you have something I don't know like in the web 2 world like mongodb where mongodb is open source but you know  the company runs the open source infrastructure and so I think a similar business model like could make a lot of sense here where you're basically paying for the convenience and like the Peace of Mind of like you know this infrastructure is actually going to work and it's actually going to be uptime that being said we're not going to always be like our own operator like we don't want to be the only operator for like censorship resistance um and so it's important that anyone can actually run the operator themselves um and yeah I think there's like some interesting things you can like do around that in the future is it possible to tell us what the how does one decentralize this I mean it feels hard right like you're kind of relying on someone to reward these things I mean just tell me if I'm wrong but anyone could do this yeah it's just the sense that like this is where no one's going to right other than if you're in some yeah it's like we're going to coordinate all this infrastructure and then you know and we're going to do it the best so you have like the like client that updates the fastest and like you can have like a nice dashboard that's like yes my like client got updated things like that and then in the future I think a lot of the zke EVMS have been thinking about really similar problems where they really need proofs to be generated but they don't want a centralized prover and so they're going to have like a decentralized prover Network like that's separate from their sequencer and I think a really similar thing could also apply here where you're going to have like a bunch of provers that you know basically I kind of think of it as there's this economic exchange going on where there's people who are relying on those things to like clients who want proofs and then there's people who are able to provide proofs and there's like some exchange of value and then maybe you have like a Marketplace and you like take some cut of that or something so I guess you know kind of more generally like let's say I want to prove something that happens only once every once in a while right like what incentivizes me from like paying you know this like big pool this feels like what a lot of things like how a lot of things work right like we you know kind of only Bridge or only like send data across kind of chains like very fairly infrequently right so is the assumption that there's going to be like a large enough volume that like people are you know kind of incentivized to always run these things or do we kind of expect every individual user to just like provide their own proofs as needed I don't know how large these proofs even are I mean how long does it take like yeah I think expecting individual users to generate these proofs is totally infeasible like I think to make it you know timely uh you need like custom Hardware probably and like the limit uh generating the proof takes takes a while um how long and in what machine sorry correct So currently for ethereum our proof of consensus for ethereum takes like four minutes to generate on like a pretty beefy AWS machine I think that time over time that time it takes to generate the proof will definitely Trend down hopefully closer to zero but you need like GPU or there's a you know a million ZK Hardware companies that I'm sure will use that's going to make it better but again yeah a user is not going to run a ZK fpga in their house like that doesn't make sense dear my will gotta gotta minimize that trust you know truly minimize it but yeah the beauty the beautiful thing is if I'm generating the proof it doesn't matter you're not trusting me I'm just generating the proof for you but the chain is verifying the proof so there's no trust assumption if someone else is generating the proof it's just a sense and the important thing for censorship resistance is that you could generate the proof like you could spend 20 minutes and spin up an AWS machine and generate the proof of yourself and send it to the like client you're generally not going to do that and you're probably willing to pay for the convenience of someone else doing it cool um I guess we are like almost getting down to time uh unless there are any last burning thoughts I think I will turn it over to the audience to see if there are any questions are there any last burning thoughts ideas Notions constructions let's get some questions all right let's open up some questions do you need a uh you first but uh or you can shout and we'll repeat them too hey yes I have a question for Uma uh first one is like what is the time gap for you like to propagate the blocks like like from my understanding like it's working similar to like Rainbow Bridge but like with the case so like you should have like some like gaps like you cannot propagate like every hypnosis chain block to ethereum and like basically like it to any chain to if you're you cannot propagate like every block and like the second question like how much does it cost to like uh well it might maintain the like uh relayer like or like validator yeah so the question kind of was how long does it take to propagate a block and presumably you're not propagating every block yeah that's right so uh for us like the block propagation time is first we need to wait for ethereum to finalize uh so that generally takes an expectation around two epochs which is like around 12 minutes and then on top of that we have our proof generation time for the particular block uh so that takes you know another four-ish minutes right now but I think that can go down quite significantly to hopefully someday on the order of seconds um so hopefully that should be no issue and you can probably do things around not having to wait for a finality um that I won't go into now but I think in the future that's also possible um with proof of stake you don't need to propagate every block so in proof of work you actually do because the longest chain is this like sequential thing so you need to verify every single block to verify the chain is the longest but for proof of stake as long as the validator set stays the same you can just send an arbitrary block as long as you have the same validator set and so for us we like send a block you know currently we do it every 10 minutes or something like that um just you know and so there's like an extra 10 minute delay on your Bridge basically um and so there's like a trade-off between how often you want to propagate blocks and the gas costs you're willing to tolerate but if you have like a huge transfer for example that you want to do right now you could like send the block right now yeah I do miss some of this stuff earlier offline but basically it's like 12 to 16 minutes you can send blocks and it costs you like 180k on the destination yeah yeah verifying our proof uh so verifying approved there's like pre-compiles for pairings and so verifying approved takes like 200k ish gas so you have like 200k ish gas to verify a proof and then you have um like whatever smart contract logic that's should be quite minimal because you generally try to stuff it in the snark um to you know keep the like client up to date and then there's some really really fun things you can do with the recursive snarks so say I'm bridging from gnosis chain polygon and you know whatever other chain and I have proofs of consensus for all of them what I can do we don't do this right now but what we're working on is if you've ever proof a consensus for three different chains you can actually recursively prove them in one snark and so what happens is you're amortizing the gas costs across all your partners that you're bridging across and so that's really nice because with recursive snarks you can stuff basically and unbalted amount of computation into 180k gas and so I think in the future when everyone starts using this then actually it becomes gas efficient enough you're talking about the proofs proof generalizer for example for ethereum to uh to another but it's I mean it's quite complicated task right to generate State Pro film and verify this proof do you have some results in terms of I mean CPU cost or gas cost uh for your for your Bridge because I know it's really complicated and it takes a really long long time yeah I think uh what I said earlier we are proof generation currently takes four minutes um so it's like on a pretty beefy CPU um and then the gas cost to verify the proof like because it's a succinct proof no matter how much computation you stuff in there it's always going to be like around 200k gas hey guys Brian um three quick Rings jeremont like pretty timely comments on IBC you should check Twitter after this it's like a Grim Day for IBC right now um but two other questions um one first there's a lot of new um you know everybody has some new validation method that seems awesome and like there's a bunch of abstract stuff happening here it's new it's better it's great but the majority of almost every system that exists right now still has upgradable contracts where the mutability for that contract basically underlies the security of the system so do you all agree that most of that reduces down to like the trust Assumption of the multi-sig that's controlling the upgradable contract and the second question was for Uma um in a world where like all of the hard Technical Services recursive State Journey you know all of this is perfect and fine is there still this issue of like especially when you're expanding to like very long tail stuff that validator sets actually have clear incentives to be adversarial to each other like there's no reason that validator set shouldn't sign or generate a forged block uh because like imagine competitors um arbitment optimism BNB and polygon or whatever we're like it's actually very strongly in their interests to hurt the other chain yet they have no economic incentive tied to let's say a block of a forked chain that they're presenting as a valid proof um yeah those are my questions let's do the first one first so I I Brian I think we're philosophically aligned like upgradable contracts bad right um and they're they're I think there are things you can do where maybe there's like an upgrade path maybe kind of but generally speaking no no no like make immutable things and that does make your upgrade path very painful right across that V1 and V2 and it sucked it really sucked uh going from V1 to V2 but like you did it and works um and but like again like okay I don't want to shoot on chain link because that gets me in trouble but like there's still a multi-sig behind all this stuff too right and so there are the thing we also have to realize is like in defying in crypto there are these very scary multi-siks that are sitting around that could do very bad things and um we I think like Uma spicy take his multisigs are bad Chris same like just those are bad like delete multi-6 uh should be the mo yeah I agree um hop is also not upgradable uh we take the the kind of uniswap approach of launching a new version and everyone can still use the old version it's going to exist forever and yeah and then yeah Emma you want to go on well you can you can insult multi-six too that's okay yeah I also think multisigs are bad uh yeah huge props to hop in across for not having a group of great a couple contracts I think for the ZK stuff since it's all like very neat technology honestly for a while there are going to have to be appropriate guard rails um and I think like a lot of the ZK evm teams are thinking through a lot of the problems or like solution potential Solutions including time lock upgrade where you know you can with emergency withdraw and things like that um so I do think it is important to have a balance of like non-upgradable is the goal but when you're working with really new technology like you definitely do have to keep the guardrails in mind uh so I think that's pretty important I think go ahead it's definitely a trade-off yeah yeah well and be like it could be a little bit more nuanced than that too like a guard there's there's designs I think that exist where like it's not upgradable but there can be like a way to like a stop button like yeah like an emergency shutoff type thing and I think patterns like that are not crazy yeah yeah totally um and then yeah in terms of like the long tail of this stuff okay I think yeah that's a really interesting question so in the long long term if you have a ZK validity proof so right now proof of consensus only proves like header validity but you know with zkevm you could even have state transitionability and then it doesn't matter if the validators set necessarily goes Rogue because well the validators that can go Rogue and sign a totally invalid block but then if you verify validity proof of the state transition then you're not going to accept it I mean that's very far off in the future um and then it you know if they have like slashing conditions on their own chain then you know they also are subject to that I think if you're trusting if you're using a long tail chain and you're trusting their validator set to not do malicious things like I think if the validator set signs off on some totally incorrect header for their bridge to hurt her competitor I think ultimately they're actually hurting their users a lot more and so to me it seems like just a bad strategy for them to do that like I don't necessarily know if they're super incentivized to do that but I do think it's important like I think for us like that's why we are focusing our initial thing we built is like for ethereum where you know it is actually extremely decentralized they have a ton of Economic Security if you validate ethereum's validators they're not going to go Rogue in this way so I think like that's kind of our initial starting point of focus and again for users of bridges It's always important to remember like if you're bridging between chain a and chain B your securities the minimum of the chains you're bridging between and like you can't get around that no matter how secure your bridge is like if you even have proof of consensus and validity proof um so yeah I think that's always important to remember you never know so well unfortunately unfortunately we couldn't hear you but um do we have or we have a few more questions cool I think you back there were prior and then afterwards um my question is do you think the user especially for the liquidity Bridge are pricing correctly the risk that the bridge has and if no like what can we do to better educate them I I think if you're a liquidity provider and you're taking on the risk of every single bridge that that um or sorry every single Network that that bridge supports you are an incredibly altruistic person but to your question like I think the users here again to Chris's earlier point the users here are not the ones with at risk um say what the liquidity providers do have risk and pricing that I think is something that is like hard um you know again the lindiness is I think important here too um but you yeah it's hard and again you go back and even when Chris says like okay some of these Roll-Ups are pretty new technologies right um even using the roll-up not even not even being an LP has some not zero and potentially relatively significant risk so like what does it mean to put you know 10 million bucks of eth onto a roll-up that still might not fully have fraud proofs implemented and all that kind of stuff like they're we are and it's not an insult I'm just I'm just saying that um we're we're there these are still cutting edge Technologies yeah also under disgust Roll-Ups also have Smart contract Risk by the way like people you know say bridges are so terrible they've got all these hacks all there's been all these smart contract hacks and Roll-Ups have the exact same problems like the recent binance hack which was like a Merkel I actually don't know all the technical details but from my understanding it's a Merkle inclusion proof hack like you could have the same thing in a roll-up native bridge too and so that's not to be under under counted absolutely and there have been uh things reported not exploited oh hey that was awesome and uh I hope this is not too open-ended of a question but do you have any comments on shared validator sets like are kind of being introduced to Cosmos and are in polka dot and how that might affect uh the architecture of like the proofs that might need to be generated for a bridge I yeah I'm not super familiar with this like shared validator set thing I'd like heard a little bit a bit about it I should probably read more I think as long as you know I think at the highest level basically you can think of like how would a node validate that you know a particular chain has come to consensus even if they have a shared validator set and then ultimately you're taking whatever computation like a node would do and then you're putting that in a snark so there's no like fundamental inability to do this even in the even if you have a shared validator set like you can always do this I mean okay it might be a little harder to keep track of the validator set if it's like rotating across like three different chains or something like that so I'd have to like look more into the details and actually honestly the details here are often what really get you um but theoretically I don't think there should be any like trouble with it yeah I think just the way I would reason about this and again it's not there's a lot of new stuff here and a lot to wrap your head around but the way I would reason about this is like I forget was Uma or Guillermo's earlier Point um it's the minimal like what's the lowest common denominator of security you have here and so you know in our like optimistic design all you're doing is saying hey this happened on this other thing you have to trust the initial source and if your shared validator said is better great if it's worse great like you kind of just kind of be sort of dumb about it and be like are you trusting the consensus that this thing is reached I don't think I know enough about shared validator sets to give a good answer all right well with that last question thank you panelists everyone please give a big round of applause you guys are awesome and thank you for the great questions thank you Guillermo okay and uh we will be back in a few moments while we uh reshuffle the stage with a range of lightning talks we have several lined up for you each of them in seven minutes um which will start in a moment so we'll see you in a few minutes [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Applause] [Music] [Applause] [Music] [Music] [Applause] [Music] what do you know [Music] everyone welcome please come in to the back we have more seats over here so everyone can get a spot I have some seats over here in the front okay wonderful okay thank you all for coming uh we are going to start a series of lightning talks and the first to start off with is um Santiago Paladino with a talk about being a responsible multi-sig signer take it away and give me an Applause people [Applause] one two three oh okay perfect well welcome everyone thank you very much for coming today we'll be talking about how to be a responsible signer we'll be focusing on multi-six basically because we only have seven minutes but this applies to pretty much any uh proposal based governance system so picture the situation that you have a set of smart contracts set of assets that you want to control you manage them via VIA multi-sig you are one of the signers of the multi-sig and you are presented by this what happens 99 of the time is that no one is going to review this they are going to blindly sign it go ahead and pretty much defeat the purpose of the multi seal like you only have the multi-sig becomes basically one out of any wallet where just a proposal gets whatever they want approved okay so the scope for today is go through a series of tools or techniques that we can do to actually verify understand what we're signing right like you understand what you're saying when whenever you sign something in real life you should do two when dealing with with matching internet money and the important thing is that the tools should be understandable for non-technical people right and multi-sake it's very often that you have the CEO CEO you have contributors to a project that are not not necessarily technical so we want to make sure that they are able to review and understand what they're assigning so we'll Begin by understanding the transaction understanding what's in a proposal and for that we'll go to First sorry um it's not clicking for some reason oh okay Okay so sorry about that there it is so we'll start by going on okay what's actually in a regular ethereum transaction right so we have a recipient we have something that we call usually contract that contract will do stuff we'll most likely go into other contracts we have a piece of data where we say hey we want you to execute this this and that in this case for instance a swap and we may have a valued basically something that we send to it and the proposal in multisig is no different it's basically we are asking our multi-c contract to be calling to be calling on our contract with some information and optionally with some money so they catch the first thing that we can do is this is actually a screenshot from the nosy safe here is understand what what it's doing I'm like parsing that and we can see how much money sent we can see which contact we're interacting with and we can see which function is being called now the function may give us more or less information right and the first thing that we can do with this it's pretty obvious like go to the documentation every serious project has a documentation attached to it and will tell us what the function will do that way we can actually see that that swap exactly for tokens what it's going to do what each argument to it is representing actually understand what we're approving but you guys are not here so that they tell you to read the freaking manual so we'll look into some more interesting stuff something something very very powerful we can do is simulate the transaction so we can use different tools tenderly open sampling Defender block native though for now it's a API only to simulate what would happen if the transaction were executed now granted that will not be the same as when the transaction gets actually executed for instance in swap you may get more or less tokens back later depending on price activations it gives you a pretty good idea right and the idea is that a simulation will give you simulation will give you there it is different information right it will tell you which contracts were involved State changes or events so in terms of contracts at the very least you can see where as we're saying couple minutes ago one contract can call another and another and basically have different different effects on the chain and so by spotting if there is any contract that shouldn't be there that's the first sign of alarm when we're signing something for instance here in the case of a swap we see that initial pool is involved a router the tokens that we're exchanging things that make sense if we are feeling a bit more adventurous we can also look into State changes we can see okay how each of these contracts involved changed probably the easiest thing to go through our balances right like if you're if you are doing a swap we would expect that the balance is of one token will go up another one would go down and again we can see if there are any funky things there usually State however it's difficult to parse or to understand so the main thing that we want to look to look at our events so any reasonably designed contract will emit an event whenever something interesting something important happens so there are a bunch of events that should immediately catch our attention when we're reviewing a transaction namely a transfer moving assets from one place to another and approval which is giving right to someone else to manage our assets anything related to role management to ownership and to upgrades and events will have some extra information that we can look into to get better sense of what's Happening now speaking of upgrades this pretty much deserve a whole chapter of Their Own an upgrade transaction usually looks more or less like this like calling the contracts and telling it hey can you please upgrade to this new address and the only information we have is completely an address and okay let's simulate now we have something new on our toolbox we can see what what it's like sure the simulation will tell us yeah the implementation is moving to its new address and that's all the information we have now that's that's a problem right like what what information does an address carry for sure the address should have a verified source code Associated to it whether it's an ether can sourcify the price that the source code is a bit long right this is an actual Contour from from the graph that URL wrote you're you're over here and it's like thousands of lines of code spread actually there you are spread across 16 different files like no signer is going to review this what a signer can review and can understand is this it's a report that they get from an auditor from a set of developers from someone that say Hey you know we have reviewed the code at this version it looks good to me and they can trust that they can they rely on that and that's what they should be using for making a decision whether they want to upgrade to another version or not but the problem that we have here is Bridging the Gap between that identifier that version of the source code and what gets deployed at an address right so we know that there are a couple of steps involved we have the source code that gets compiled into into a binary artifact and that in turn gets deployed to a particular to a particular address so what we want to do is make this process transparent granted we could ask reviewers to write them to run a script like this and try to replicate the compilation on the deployment but we're trying to give the tools to reviewers that are not technical to be able to do this and let's tell the truth like even developers are not going to do this okay and there is a solution here like it has been in traditional web development for years it's making deployments public reviewable or auditable traceable like just do a deployment to NCI on some on any word that's public that anyone can go there can see hey this version of The Source Code has triggered this build that was deployed at this address make it transparent and easy to understand and to read and embed that information in The Proposal that's being that's being sent to signers so that they can understand that they can bridge that Gap from source code to an address with the code that they are approving and upgrade to so all in all what I'm pushing for is to make sure that multi-6 are actually multi that we make use of all the people that are appointing assigners because they should have the tools to be to be able to understand and Dot security to the process by reviewing everything that's happening instead of blindly signing and if this is not possibly a reviewer remember the burden is on the proposal The Proposal of something needs to give you give you as a signer all the tools that you need to be able to review and fully understand what you're assigning if that doesn't happen you go back and you ask for as much information as you need to be able to understand what's actually happening thank you we have room for one question per speaker who dares not all at the same time okay how do you like Bogota I haven't seen Bogota yet I have been locked inside the venue ever since I got here okay a question related to the subject perhaps someone there was a question here okay yeah they had it here because you were here what was the tool you used at the end there to uh perfectly show the uh deployment address the last thing I I was working on um that we were working on uh so yeah it basically helps you bridge the gap between the bytecode artifact and a deployed version but yeah you could use other tools that you build yourself for that the idea is make sure that the whole process is transparent as possible thanks for asking again thank you Santiago give me more [Applause] [Music] okay and with that I'd like to welcome our next speaker to the stage uh we have Ryan with us on how to not be worth kidnapping I'm really curious give me warm Applause people [Applause] cool just waiting for the slides to come up on the screen do I need to hit a button to make the slides come up on the screen or out there perfect cool hi I'm Ryan Lackey with evertots insurance and happy to be here I love Bogota it's been an amazing conference and Devcon is always a great crowd we're going to talk about a few things but what we're not going to talk about is how not to be kidnapped because I mean it's everyone has a different threat profile everyone has different risk so it's really hard to give advice that's going to be really successful in your case and the world's unpredictable the world changes the most dangerous time is when you're going from like one level of assumptions to another level of assumptions and also this isn't legal advice I'm not a lawyer but you really would not want to take any security advice from lawyers generally lawyers have this amazing habit of they use magical legal privilege dust to protect like anything so because something is illegal you can't do it so as a result no one has to worry about it so yeah so the security advice I think security and Engineering is probably a better approach there so kidnapping in general kidnapping sucks it happens like it's really hard to get statistics but it happens like I'd say 25 30 000 times a year and then the number value of Ransom payments is really hard to estimate it's I mean I've seen numbers anywhere from like 500 million dollars this year to like five billion dollars so let's just say two billion dollars a year in Ransom payments the real economic loss is definitely higher than that because the ransoms are only a small version of it and the rate of this has increased a lot like the the most monitored period I know about is from 2000 to 2010 I think a lot of that increase where it basically doubled was due to the Iraq War and a lot of kidnappings in that place but it's really hard to get incident reports because a lot of the smaller scale things people don't report or they're reported to local authorities and not collected in any sort of centralized way so there's a lot of that stuff and it's usually like certain industries and certain geographies are very very um overrepresented in those statistics uh like a software engineer in Palo Alto California has a very low risk of this a oil oil and gas executive working in like Nigeria has a much higher risk people were working as security contractor is in dangerous countries unfortunately Colombia has had history where it did have a relatively High rate of kidnappings in the past and has had some fluctuations since so this is really like a not really a uniformly distributed risk there's a lot of different types of incidents so jovivo in Puerto Rico pero uh Joe hablo Poco espanol but there's Express kidnappings or Paseo million not mionario which is the millionaires ride so the idea is able to kidnap you in a car and take you to an ATM and say take out all the money you can take out of the ATM and usually let you go and that's like a pretty it's it's bad it's sort of on the spectrum of someone stealing your cell phone or a robbery but it does have the threat of physical violence involved so it's a little bit worse a team of kidnappings are actually child custody like a non-custodial parent who's lost custody of a child will take or retain the child passed and approved monitoring period so that and turns into a kidnapping and it's sort of crazy there's the custodial kidnapping where they take somebody and they hold them for a period of time until somebody pays a ransom that's one category we can talk about there's the political kidnappings where a group is unhappy with the government or with another another entity and decides to kidnap people as a way to put pressure on that other entity the the external energy like the government people that are involved in illegal activities or certain violent Industries are at a much higher risk of kidnapping than people that are involved in like normal things unfortunately also that affects like the some kind of I'd say the worst and the best people so people that are doing charity work in places like Haiti there's a bunch of nuns that were kidnapped while they're doing uh like relief work there and they want a 17 million dollar ransom for a group of nuns and that's not really feasible for them to pay so there's that there was the whole thing off of Somalia where there was the piracy there's the same thing in the Straits of Malacca and other places where ships are basically piracy and the crew is held kidnapping there's a virtual kidnapping which is crazy so someone gets emails saying oh your wife has been kidnapped she's at a conference and uh the guy immediately like he loves his wife and he pays the ransom immediately and then it turns out his wife wasn't actually kidnapped and it was just all like fake and he just paid a bunch of money for this and there's that but the thing that's maybe more relevant to us is something that happened in mostly like the 1800s bank managers where they had Banks back then had huge amounts of gold in their vaults so like every little local bank had huge amounts like multi uh like very very large amounts of money stored in local vaults and this is very liquid very stalable and they would kidnap a bank manager at night when a place are not as then the bank's empty and everything else they would then take the bank manager of the branch and say unlock the bank vault and then take the money out and that's not really great because it was a very profitable thing and it kept happening over and over again so uh what they did is they invented the time lock for safe so the safe could only open during business hours when people are going to be in in the bank and that's kind of like kind of relevant to what we're talking about here so why should we care like most of us are not um like oil and gas Executives or doing missionary work or anything like that but we do work in an industry that has some some risks that are very specific to the industry so that blockchains are great but they're instant final transfers and that's the same as that's like the worst possible model for for Crime because someone can can accomplish a crime and then they're done there is there's some great talks that I've seen on how you have to launder the proceeds of crime but that is a very very good problem to have once you have the cash versus the problem of if you've stolen money from a bank and it can just be revoked by the or stolen electronic bank balance deposits and they just get revoked by the bank it's a very large amount of wealth there's people who have come in and also people that are new entrants to being wealthy that have come in with like more money than they ever had in the past and very recently and uh it's geographically distributed in places that are very high risk um relative to a lot of previous economic booms and a lot of people have like a lifestyle it's kind of hard to secure I have a very boring lifestyle like I live in my house I go to my office my office has 24 7 armed security my apartment is very secure um I don't really go out all that much and things like that but like people that go out to clubs and bars and everything else they can't really have security with them 24 7 and it's like they are not willing to make that sacrifice I think basically old people are willing to do that younger people are not so so it's kind of a risk in the crypto industry there's some basic precautions that everyone can take like uh normal physical security I mean there's a lot of teams here that have that have come that actually paid for um our armed security armored cars driving around it's not because of the getting the phone snatched off the street it's because they're afraid of targeted threats against them as as companies avoid risk and everything else but the crazy thing is this thing called kidnap and Ransom Insurance the problem with kidnapping Ransom insurance is if you advertise that you have kidnap and Ransom Insurance uh that makes you a Target because they know they're going to get a guaranteed payout so most of these programs have a program that you're not allowed to actually advertise that you have it um because one thing worse than being kidnapped for crypto that you do have is being kidnapped for crypto that you don't have there's all sorts of ways you can set up great multi-cigs and custody arrangements and use custodians and everything else so you don't actually have access to the crypto to steal but or to make a transfer but um once you do get kidnapped it becomes a serious problem so so the problem is you need to convince the dumbest possible kidnapper in the world that this system is in place that you don't actually have instant access to the funds or access to the funds alone and um that's pretty hard because very few people that are going to be kidnapping people attend conferences like this and learn about the The Cutting Edge of custody arrangements and everything else so the solution I think is I mean hopefully the solution I think is actually you get this into the mainstream so you build something that has a custody Arrangement that prevents someone from being Ransom successful or being having keys you successfully into mainstream media like a Hollywood movie find incidents where it's a popular incident and basically have those that mechanism foil the crime and have it widely known and publicized into that there's also some novel Insurance products and there's also the government approach here which I'm sure a lot of people don't really like government Government Solutions but certain governments the US Israel a few other countries have a policy of not paying ransoms for people they send in people to go try to rescue them not always successful other countries have a policy of paying ransoms like a lot of European countries do and their people get kidnapped and Ransom successfully so sort of a trade-off but thank you very much um yeah anyone has any questions happy to answer questions we have one question Phil on your last slide you mentioned novel Insurance products can you say something about that yeah so basically yes so from novel Insurance products so the the issue with knr is that k r um it has like the wrong incentives it if you advertise that you have knr then you're more of a threat or more at threat um if you had something that was dedicated to uh uh essentially finding kidnappers like sort of like the jsoc kind of like the government retribution approach so there's a fund and a collective action so either the funds that are paid in Ransom then get blacklisted automatically by every exchange immediately and the rest of the industry says we're not going to support paying ransoms that way that's one solution there's also um having information on chain uh to assist um security forces in recovering people and doing that it's not necessarily a solution but yes okay thank you Ryan okay and with that I'd like to call forward our next speaker Frederick spend us to talk about the bug Bounty program of lithium Foundation Frederick take it away please give him a warm welcome thank you very much um yeah so I'm Frederick I work at the EF as a security researcher I um have quite a few responsibilities but one of them is to manage the bug Bounty program of the EF so I'm just going to discuss a bit about what it is why we have it and yeah why it's valuable basically so um at the EF we have a lot of security researchers they spend all their time trying to break applications um and basically to figure out how the specifications of the protocol might be broken and then trying to sort that out we also take in some external bug hunters from time to time and they kind of focus on a specific task at hand and also to augment that we use a bug Bounty program so the bug bunny program is there in order to allow for external researchers to come in do some vulnerability [Music] bug hunting and then basically submit reports to the backbending program and this is done so that we can provide rewards and other incentives to the bug Hunters so our scope is kind of wide we have you can see a list there basically all the clients we have the deposit contract solid bugs and the specifications of the protocol but something that we don't have which is kind of unusual I guess is that we don't have any scope for websites or for infrastructure or if anything else we focus solely on the protocol itself [Music] you can use two different ways of having a bug body program you can use bug body program as a service basically paying a service provider there are many of them they will come come in set up your back button program they will provide a often very very large set of bug Hunters that are ready to just start hammering on your application or you can do it in-house and there are some pros and cons for each but the um the reason we went for an in-house solution is that ethereum is kind of complex it requires a lot of expertise a lot of digging through it compared to let's say finding an SQL injection in a web application so we didn't feel that there was that much value so that's why I went with our in-house solution and we also have a team that can triage any reports that we're receiving but depending on your situation you might feel the need to go with a service provider so looking at the history we can see that in 2015 we launched the first iteration of the bug boundary program it had a very narrow scope um some execution layer clients and the rewards were 5 BTC for consensus issue which today would be quite a lot but back in the time it was 1500 bucks over the course of the years this was increased um and in 2020 the consensus layer bugabana was introduced at that time the execution layer had a bug Bounty of up to 25 000 and the consensus Larry received an ax Bounty of fifty thousand dollars we also have a specific thing where we will double the rewards during [Music] um basically just before hard works so this is done so that we will get additional eyes on any updates that we're doing to make sure that there aren't any obvious issues and then in 2022 we merged those programs into a single one we also up the rewards to 250 000 as the base Max reward and you can also get a 2X for a hard Fork but for the merge itself we increased it 4X so we had a 1 million Bounty out during the merge this is what it looked like um back in the days with the execution layer bug Bonnet program you can see the leaderboard there and basically you submitted reports via email you could use encryption if you wanted to but the majority didn't do that currently we have two ways we have the email system where you can submit a report or you can also use a Google form what kind of surprised me is that the vast majority of people they want to use a form to do it it's a bit easier to fill out because you have these kind of preset questions that you have to submit but there are some security concerns with that we shall get into soon so in the course of the bhagavani program we've received about yeah 200 plus vulnerabilities that were actual vulnerabilities counting the spam and non-vulnerabilities probably 2 000 reports but it's about 200 plus actual vulnerabilities and you can find these vulnerabilities in a public disclosure repo that we have on GitHub if you're interested in digging in a bit in what vulnerabilities yeah we've seen it's not completely updated we update it after each hard Fork so right now I think the latest update is in May or something like that March maybe but we will update it very soon again given that the merchants now happen so there will be additional ones to be found in this Repository uh oh and if you're interested in some specific vulnerabilities learning more about uh like in detail you can have a look at Mario's talk that he had I think two days ago um so like I said we're using the Google form system right now which isn't super good from a secured perspective because the reports we receive are plain text or clear text you anyone can read them as long as they have access to the Google form system so if a user with access were to be compromised or Google forms were to be compromised then that would pose an issue especially now that we're dealing with vulnerabilities that could affect a lot of funds so we have been building a secure submission form which is based on openpgp.js and this allows [Music] basically bug Hunters to submit vulnerabilities and before they're submitted the the data is encrypted on client-side and this means that even if someone gains access to an email account or something else then they are basically um they can't read anything unless they have the private key that one is not real yet available publicly but we do have a smaller version available that people can download if they're interested in it which is the secure drop looking ahead um we're working on building an encrypted vulnerability lifecycle system this will allow us to basically follow the vulnerability from start to end we will be able to receive the runnability assign it to certain people have comments assign it to client teams and then close it as needed all while being encrypted and we're also looking at additional incentives for bounty hunters one of them that we've been looking at is basically creating bug hunting nfts that will provide some value in real life at events Etc and remix team has been doing something similar so that's it for me thank you very much for presenting the stop we have room for one question thanks for your talk I I wonder how you choose the monetary values for the bounties and if there's any programmatic approach you can take to that like considering the economic like yeah the economic value of what's at stake the attention you get and Etc yeah that's a very big question that we've been discussing very shortly um this has been up for discussion many many many times and we'll be discussing how to do it should the EF be doing this should it be multi parts of the community pulling together and doing the bugabana program because it's not just the EF it's everyone that's kind of involved in in the ethereum network that depends on this um I guess from our part we've kind of put it at a value where we feel that it's enough for bounty hunters to report it instead of using the vulnerability themselves while also not being like outrageously high so we kind of tried to do some kind of something in between that's why we increased it from 50 000 to 250 000 and who knows we might increase it moving on forward but yeah that's our thinking right now okay thank you again Frederick please thank you very much [Applause] and with that I'd like to announce our next speaker and we have Diego mazzo on how to design a local flavor D5 platform in Latin America thank you let's welcome him [Applause] oh hey everyone well it's great to be here defcom finally happened in Bogota so we are amazed with that well we're here to talk about understanding Latin Americans and to design a a local flavor D5 platform so the question straight to the point is how do we understand Latin Americans and this is not a global question it's more local question but we use a unique method in our team a model we created yes to understand all that we can with uh in-depth research and also combining with a community model so what we do is basically we try to engage we try to educate and we evolve with our users but not they are not only users they are contributors so we created a two-level community one part of the community are the global community and then we have the Community Builders why well we want to create long-term relationships and we don't we want to engage with our users we want to to create value together to co-create value and grow in together in a long-term process to to have a better product and to have uh contributors all along this this way so applying this model we we learned a lot of things we brought here five main learnings and we will buy one so the first insight well the first did I there you go the first Insight is that convenience it's decentralization for breakfast what does it mean well basically users normal users average users don't understand decentralization at the beginning so it's a gradual process and we need to help them to educate them and especially to have decentralization as an invisible concept so we we cannot ask our users just to have a ux or an experience that is so complicated for them that they will give up in the beginning then as Latin Americans we kill four dollars dollars are our store of value for the future and local currency is the enemy and most of you probably know it so we need to also adapt to current mental models so in the crypto ecosystem if you have a platform probably you have several stable coins but this is a non-existing mental model Uh current users they will think there is only one dollar so we need to adapt to these existing models just to provide them a better experience if we're talking about savings we think well it's it's good a day our users our contributors will learn about web3 but first we need to teach them how to save this sounds obvious but it is not most of them in Latin America in general is difficult to save it's not a common Habit to save and if you don't choose the right path if you don't know how to apply certain methods to save if you don't have the tools to do it you will have that you you will open the door of abusive loans which is a vicious circle that it's very very difficult to accept so some of our users we understood that it's very important not only to help them save but also to educate them to get out of this vicious circle if we talk about loans that's how I was doing it um first loans are hard to get in Latin America and harder to to pay and we have high interest rates we have a big barrier that is the credit history but especially I would like to to focus on conscious decisions so if we have the urgency to get a loan that's a bad situation there is no negotiation and probably we will get into this vicious circle that I mentioned before so we need to educate our users in our platforms to make conscious decisions we need to provide them the right information to understand the consequences of their decisions because we are talking about financial products we are talking about money and probably the well-being of families finally our last insight is that trust is scarce resource in Latin America so scams are frequent not only in crypto but also in any Financial Service and it's very very complicated to differentiate between uh proper service and scam then in general governments don't protect us and people don't trust governments so it's also a big barrier and Banks take the advantage of its privileged position so this it's a combination of factors which makes everything very particular and especially difficult so with all these that I've explained we believe we are working we are building tropicus that is the platform to to create better personal finances using web web3 and in Latin America and just as a friendly reminder we are not here for the technology we are here for the impact gracias thank you we have room for one question what in your opinion are we top three low-hanging fruits what we can do as Kanaya people involved in the crypto ecosystem to Foster crypto adoption and education here in Latin America hmm that's a good question um so I think that the first part is as we said it's education right so we need to educate in in the tools and especially try to differentiate from scams which is a big problem because this there's many people is not trusting crypto um they don't have the technical knowledge they don't have the the whole context so education I think it's the the main the main tool and [Music] yeah for example in in tropicus we we definitely believe that we need to use certain standards that are used in web 2 to bring to web3 so if we use uh what you log in without all the seed phrase or this kind of technicalities it will help a lot to the average people to understand these kind of platforms okay thank you again Diego and with that I'd like to call forward our next speaker if he is here Drew Tozer do we talk about data and empathy and how to approach ux decisions in web3 please let's give her up loss hey guys thanks so much for coming I'm Drew I have been designing for crypto projects since 2017 and I'm the lead designer at Shipyard software we build dexes we've already shipped Clipper and will soon be launching longship so we have two general methods for conducting user research which is gathering feedback on a digital product with the intention to improve it so we have qualitative and quantitative qualitative research involves conversing questioning and observing users as they interact with your digital product this usually comes in the form of interviews or live tests it's also called participatory design or having your users participate in your users development or at the products development sorry and on the other hand we have quantitative research which is capturing user activity patterns and behaviors with a product using data or Analytics also I'm going to stop using the term users for the rest of this talk because I think it sounds pretty detached and my hope for web3 is that we can stop defining our community members by the loan fact that they use the product and start to see them in the broader context of their complex lives and their complex needs so we're learning about people to better serve them with our digital products right let's unpack those two methodologies starting with quantitative some benefits to quantitative research is that we can determine patterns to investigate further so data can reveal what's going right or wrong based on our products goals it can also reveal insights that leave us curious some drawbacks to quantitative research is that the data doesn't tell you why something is happening and to guess why is fine hypotheses are really important but it's important to treat them as untested subjective results to look into further another drawback is that people are conscious of their privacy so a lot of people in the web 3 Community use Brave or vpns that make their actions off chain untraceable so this means that the data that we pull off chain may never be the full picture of community usage data collection and Analysis is expensive and time consuming and it can also be unethical so a person's data can be used to improve a product and increase a company's profits if that data has value and the person who is creating that data is not collecting any of that value is that ethical maybe if they sold it to you or if they agreed to give it to you without compensation but that's the argument here that to collect somebody's valuable data you should either compensate them or gain their consent data can also be biased and there's a lot of types of bias that can occur here but the most common is called confirmation bias which is when somebody is interpreting the data set and then comes to conclusions based on their own ideas or interests what can also happen here is the manipulation of data visualizations to further justify that bias and present it as truth moving over to qualitative research so some benefits to this is that it can validate those hypotheses you made about your data so you can get to the bottom of Why by asking people by watching them interact with your product and asking them to think out loud it is super effective you're also compensating people for their time and gaining their consent to participate in your study so with those two criteria you're becoming your research is becoming more ethical and legitimate it's also encouraging Community Development and responding to that eagerness to contribute in web3 so people want to be involved so let them reward them encourage them this builds loyalty and buy-in and you're building empathy with your community by talking to them by seeing their faces it's really important to see your community as humans and not data points and it's really easy to detach yourself from this valuable concept if your data is the only source to understand the people that you work for so it does have some drawbacks participatory design can be really challenging in an online International Community many people don't feel comfortable sharing their identities they don't want to show their faces on camera they may not speak your primary language these can all be really intimidating barriers and there can also be confirmation bias here so it's important to keep in mind that this line of research is informal we're not using rigorous controls under the scientific methods This research is always somewhat subjective and open to biases so my Approach for participatory design in web 3 is always changing but here's what's been working so far so I found that by offering tiered approach for users to participate it lets you meet people where they're at so you can have these bottom tier rewards for submitting to long-form Tech surveys this lets people translate them and they can also remain anonymous you can offer mid-tier rewards for having people record looms of themselves screen sharing and performing certain tasks Um this can also be translated this can also be anonymous and they can speak at their own pace if they choose to and then you have a top rewards tier where you have a video call and they perform certain tasks answer questions and this works best when you can both communicate freely with each other systems like this are best managed in Discord or other community tools like crew 3. ultimately I advocate for a participatory design focused approach with data to guide you towards those areas of improvement I think there's a really beautiful opportunity in web3 to practice genuine empathy for our community by working alongside them thank you okay any questions from the audience we have room for one question in the back please in the microphone for the Stream thank you what kind of tools do you use on the quantitative side for user research on the quantitative side at the moment we typically hire somebody to do that for us so we've been working with arcx we've been really great at accumulating data for our products um on the smaller scale side we use Google analytics that's easy for me to interpret but yeah for the bigger questions we Outsource okay thank you drew thank you that's good for an Applause and for our final speaker of the day we have Alim camisa to talk about A playbook for product development in D5 Alim let's welcome you to the stage [Applause] all right how's everyone doing I made it to the end um so my name is Salim camisa I work at element Finance I'm a product manager element Finance is a protocol a D5 protocol for fixed and variable yields and I'm really excited to be here today to talk to you about product development and defy and let's get started so first off I kind of wanted a level set on what does a product manager do and you know there's lots of different definitions um and oftentimes people will think they do one thing or another oftentimes a PM does a lot of things it's everything um so the core area of responsibility is shaping the product shipping the product measuring product success and these are things I'm going to kind of loosely talk about throughout the presentation but synchronizing the team is really important the product person will interface with the rest of the team kind of align on timelines uh hash out kind of product requirements and ensure everyone is on the same page and kind of working in the same direction so I want to talk about building the product roadmap and um how I want to kind of place a little bit of emphasis on R D and how that's really important to kind of creating new product opportunities so defy research teams really need to focus sorry the D5 teams really need to focus on research and development in order to succeed in the space they got to commit to experimentation it's not just about rehashing existing Solutions and it's important to take an additive approach where you're building new D5 Primitives and contributing those Primitives to the open source defy ecosystem and at the same time you're leveraging D5 Lego pieces and this again fits within the whole um you know idea or ethos of composability within the D5 space so having a pipeline of research projects is really important it could really guide the evolution of the product or dap you're working on within the D5 space and it could potentially lead to entirely new products mm-hmm and so with that um you know the the pipeline of research projects can really form the basis of the r d roadmap and the r d roadmap is a critical input a very important input into the product roadmap mind you it's not the only input into the product roadmap there's lots of other things but it is really important I want to talk about how do we structure the product roadmap or lay it out because oftentimes we think about it from a timing perspective by month by quarter um I think the now next later approach works really really well especially in the fast-paced environment of defy or for teams that are working on new product development um you know V1 of a protocol where there's a lot of uncertainty and things are changing really fast so everything you're working on in Flight is kind of in the now bucket everything that's coming up is a next and anything that is low priority kind of gets shifted into the later bucket this kind of gives D5 teams the ability to not be kind of handcuffed to specific timelines when you're developing a V1 and kind of you know develop it right uh and not necessarily take the shortcuts and it just gives them the ability to have more flexibility in terms of how they're developing and timelines overall so this is actually one of my favorite sections of the presentation and I want to get into um you know how do we find a process a product development process that works for uh for your team I have to say that I'm going to caveat this with there are lots of nuances across different teams team size culture if you're working on a D5 protocol versus adapt that's being built on top of a protocol all of these things kind of play into the specific custom product process that you should develop for your team so this is something that's worked really well for element Finance um and with that we are still evolving the process as we should be to suit our needs as we kind of evolve as a team so stage zero is kind of the concept Vision stage um and this is where our smart contract leads and our Founders both of our Founders are um eth2 previously to researchers and come from a very intensive research background so they'll usually kind of come up with a new feature um or something new whether it's a new product concept altogether and then the smart contract team leads will usually try to crystallize that with our Founders and then in stage one the formative stage will have our smart contract teams our front-end teams product and design kind of working together to really hash out and understand the core functionality um and then in stage two the smart contract team usually takes a little bit of a step back and they take on a little bit more of a consultative role in her front-end design teams and our product teams will kind of do the scoping and prototyping so hashing out the UI ux the requirements and of course also kind of creating the project plan for what's to come and then in stage three we get into our development Sprints many of you if you work on product teams today Sprints can be typically two weeks but it could be a week it could be a month it really depends on the team um and this is where we get into the execution cycle important to note that we're actually running two um two sprints in parallel so the smart contract team is is developing all of this solidity code their unit testing they're doing their fuzz testing a formal verification and Audits and then the front end team is um excuse me the front end team is working with product and design to kind of execute uh the development of the UI and the ux and of course we're doing a lot of internal and external stakeholder feedback throughout that entire process to ensure we're validating assumptions so when we get into the Sprint cycle one thing that's really really difficult for product teams to do this is something I found personally very difficult to do is is is uh agile estimation so so estimating the work effort that goes into your tasks and that's really important to Velocity and velocity is really important to understand like how is the team performing are they able to deliver on time are they able to complete all of the tasks in the Sprint on time poor task estimation often leads to sub-optimal Velocity and timeline slipping so I got to give a shout out to my front end team I don't know if I have any of them here today but shout out to them because we came up with this really interesting framework for estimation that I think it works really well so we kind of look at time rough estimate of the time to complete that user story or if it's broken down into the into the development tasks we look at collaboration costs and I think this might be something that might be new to many of you um the level of collaboration with other teams for example if my design team needs to provide new components if there's a lot of uh you know if the product requirements are really intensive and require a ton of time and require lots of working sessions all of those things are going to take away development time and that needs to be factored in so that's part of the collaboration cost the confidence is really a score for the confidence on the time estimate which also includes complexity and the collaboration cost and the key takeaway here is you want to avoid your timelines from blowing up by having too many high collab and too many low confidence scoring tasks in the same sprint that's going to allow the the development team to to have enough time to be able to execute the tasks without getting bogged down on a lot of working sessions and too many meetings and stuff like that um I don't know if I have enough time for simulation modeling so I'm going to skip this but this is a topic that's very near and dear to my heart so if you have questions we can address them a little bit later but I'll just quickly touch on tools like hat cat Gauntlet token spice all of these things I do encourage everyone to to kind of take a look at and explore on their own product Market fit in kpis really important this is something my team has been spending a lot of time on lately um and what's the common metric across D5 right now right it's tvl total value locked right it's it's a good metric is it a great metric I don't think so it's a good metric when it's taken in concert with other metrics alone I don't think it's sufficient to give you a good good idea of product Market fit so tvl is not really sticky Traders often look for the best yields across D5 so loyalty is fickle and that cause that can cause a lot of volatility in the tvl tvl is also um subject to a lot of Market cyclicality bull bear and it's just not a good long-term metric to measure long-term sustainability health and success of a protocol so again it's not a bad metric but it should be taken alongside other key metrics which I'll touch on very quickly ecosystem engagement metrics are crucial understanding Community engagement and frequency developer activity are there other developers building on top of your protocol Integrations are there other protocols integrating with your protocol those are really strong indicators of product Market fit and the number of unique token holders so if the the token distribution is is really high and you know you have a very decentralized token economy you might have more participation in governance and you just have you know more participation in in the protocol that's also a really good sign as well product engagement metrics like daily active users or daily active addresses churn and retention metrics user actions like in a D5 protocol you often have users depositing trading redeeming and other types of metrics are really important um I'm going to skip over this for time because I'm down to a minute and a half but how do we measure some of these metrics it's really important so first of all you may not know this or maybe you do but a lot of D5 projects out there like maker Ave uniswap dydx are collecting user data mind you most of them have the Privacy flag turned on which is something you can do in Google analytics and other tools like mixpanel but they are collecting user data it's anonymized data it's really important for informing product decisions app and protocol evolution and optimizing marketing efforts as well what's really important is that you've integrated consent management right this kind of fits within the ethos of web3 where users have full control over the over their data if they want to opt out of anonymous data collection you need to give them that that opportunity to do that and then you know open source tooling is better I mean than than closed source of course and um the the kind of uh uh the the tech stack that I've outlined here is a typical product marketing analytics Tech stack that we could use that includes all open source tools um and I'm happy to chat about that a little bit more uh later because I know I'm just down to a couple couple seconds um but yeah feel free to follow me on on Twitter um elements underscore Phi on Twitter as well I'll have a blog post coming out which will be very comprehensive and cover all of this in much much more detail so stay tuned for that as well and uh happy to take any questions if we have any time okay for a question I'd like to see a show of hands who has a question let's see one but there must be more right two anyone else two okay then we'll take two yeah all right first of all thank you so much for the presentation I saw that you team used uh retool yeah as a bi tool I I very happy to see that internally I've been Shilling that to my team as well awesome yeah so I have a two question so first I want to ask how much do you participate in the r d process as a product manager and the second question necessarily related to the agent-based assimilation uh if you don't have time we can talk offline but basically what I want to ask is like how much do you rely on the result of that because to my understanding uh each agent's parameter is actually I'm going to just turn this way yeah what did I answer your first question um so the first question was around um uh sorry the first question was around like how much do you participate participation in r d that's a really good question um so I actually joined element Finance after V1 was already out so our smart contract team leads and our Founders were kind of very research oriented uh kind of have a really good handle on how they want to evolve V1 a V2 which is what we're working on right now um so I'm I'm a little bit less involved from an r d perspective for that but I am working with uh my fellow CEO Charles over there on a new product uh and we are doing a lot of research and development on that new product um so I think it it kind of just had it was a timing issue like I kind of came in after V1 very focused on V2 and very much focused on the r d and Innovation that's going to happen on V2 um it really depends team to team if you have a really strong technical PM they might be involved um in in a lot of the r d with a smart contract and any other technical Econo leaders on the team I kind of straddle between the business and the technical so I kind of shift my time accordingly yeah but retool is awesome in my marketing analytics guys been showing that a lot lately so we're really exploring that seriously yeah mm-hmm hey um I just wanted to ask like what do you think are the biggest challenges for product managers that have come from like a web 2 background moving it into web3 and where the biggest like I guess challenges or gaps or knowledge are yeah I I think I think some of the biggest gaps is like you know getting technically deep as quickly as possible understanding how like if you're joining a D5 team really getting into the trenches and understanding how the protocol functions um so that that what I would say is is kind of a a pretty big technical jump for many web 2pms that are jumping into web web3 as part of my blog post that's coming out we're actually going to discuss some of the nuances for web3 like even like interacting with a wallet understanding when a user completes a transaction where do we send them because the transaction isn't necessarily completed yet do we send them to the portfolio page where the transaction is going to be showing us pending it may not even go through um so these exit and entry points are really interesting to understand that's something I've been digging in with my front end team but there's a lot of other nuances um that I'll kind of highlight in the blog post but I think getting technically deep as quickly as possible is really important because I think product managers that are only business focused in the space will have a really hard time to succeed so it's really important to develop that those technical jobs sorry we please I'll come to you so you can repeat it in the microphone but this one that was first you talked about doing two sprints at the same time in parallel with the smart contracts team and the front-end team are they working on the same thing or are they working on different things and then one will integrate the content from the other in a subsequent Sprint yeah that's a good question um they're working so the smart contracts are kind of being developed separately from the front end the front-end team is also developing our SDK through which they'll be able to plug into our smart contracts so the integration kind of happens later but typically the smart contracts are being developed in parallel with front end and there's I mean there's input from all of the team members but the way that we've this is just the way that we've been doing it could there be a more optimal optimal way potentially but that's just been working really well for us right now okay any other questions if not then I'd like to thank you Alim okay thanks very much awesome okay and this concludes our sessions today in this room um have a great day have a great evening and we hope to see you next tomorrow thank you [Music] foreign [Music] 