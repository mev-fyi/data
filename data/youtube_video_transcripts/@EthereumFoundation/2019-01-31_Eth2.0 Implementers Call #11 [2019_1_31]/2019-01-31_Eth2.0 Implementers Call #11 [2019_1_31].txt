[Music] [Music] [Music] [Music] [Music] okay great so I switch the stream over and the YouTube chat box will let us know when they can hear us and I've shared the agenda in the chat lion-o so welcome to call number eleven as we have promised today marks the day that we are releasing the first version of I guess phase zero of the specification oh and I just clicked a button so it's released cool so I guess Mark's generally feature complete there's a few now it's like we're the point-wise shuffle standardization of the BLS one minor thing around deposits but it's approaching its final form with respect to features and stability and we're gonna we're gonna do one release for one release per week through February as we continue to find bugs and clean up the delivery and then we'll slow down the release cycle in March I mention thank you everyone it's been quite an effort from everyone and awesome cool that's all the way we will move to client updates who wants to start how about [Music] cool thank you how about harmony first we have decided on the conception of our initial release it will be a beacon chain emulator where it's possible to change parameters like epic wins slot duration and so forth it will run as much for the daters as gvm instance can support with the same amount memory and also it's difficult to estimate the number but it's gonna be thousands of validators instances and it's very exciting to see how such a big number for leaders who work together almost all components of stand-alone beacon chain already for the moment and there is the work to do on assembling them into into one client and right let's cover a code base with the tests this is one of the biggest issues so far we have started to work on it but it's also trying to start actively collaborated with our tennis team and the moment we stuck into a legal issue I hope it will be solved soon and I want to say thank you thanks a lot to Pegasus team for their help in finding the proper solution so that's it from our side great and that emulator is that a single machine or is this to get multiple genes running and it's gonna be a single machine with thank you great how about prismatic yeah hi I'm Karen spunky so I think so so when we launched the girl to the client we wait for the chain store log and once there's enough deposits the validator will receive the log and then kick up the deviation so so we work on the routine and that part is working we're also working on using a simulated back-end to advance the between chain so this will create some sort of end-to-end test which which will be really nice before the test at lunch and we're also using key store for preserving validate its key we're working on and the education proof or porter's we also work on the engine block operation pool for storing objects such as at the stations that it became a high-density ports other than America were just aligning on code base with E to zero expect just catching up with a stack yeah all the fun stuff that's it cool thank you how about parody related to the 70 were mostly doing housekeeping stuff so basically refactoring and make a code cleaner so I finished the refactoring of two selfies a set hatched set best blogs with resultant blog so we can invoke for choice first law others and only our import block of fusion and we are working on the migrating our code days into last edition 2018 and we are also doing some a fracturing for the substrate a URI engine so we can reuse more coat in the since entering the cracked so that's just about us nice was the the refactor to get the for choice per slot was that a deep change or was that relatively straightforward this mostly straightforward but we had like because we have some is mostly related to things atomically like any through chance arrow of fusions together and to that to that in one single transaction so so that was my takes some time I see okay cool thank you how about IBM stuff hi SSE it's ready and the test generator landed here off hi SSD and but that the test generator will be moved to another repository and I'm posting it here so we next and try and see if we can figure out how to shrink times and we have a new developer study much exciting for us is that bottleneck on the is that when you're doing massive amounts of attestation processing or at least like doing yeah my suspicions okay and is that is that using a flat hash or that tree hash in SOC and is it optimized to cash it's just using a pretty standard tree hash so that it'll doing any bug transition basically requires rewriting the entire validator registry instead of just doing a few surgical branch fixes like making a an SSC library that allows you to have to point wise the gun modifications if a reason a lot of time it's just really critical and I think everyone will probably have to do the right deal rhythm in almost the same way basically it's not too hard like all you have to do is if you have an array of and objects then you like priests or and objects they're like roughly on that on that order that are everything below the everything before them on the tree and then it keeps you an update he would just kind of meat wrap setters and getters that also adjust those and they're recalculate the new hash and to really optimize you can make a somewhat a somewhat better way of doing and updates at the same time but even if you don't get to that than doing that'll get you of most of the benefit and that but but you say hashing was processing the signatures in the authorizations a bottleneck or was this updating the state and hashing try and reduce the amount of iterations we do and then also focus on okay so the entire spec because I was following the commits quite closely missed a few not easier to just reimplemented and then once I'm done with that I think we're gonna continue simple serialize and then start looking into how best to do the p2p stuff and Swift if we should port we should easily code library there or consider something awesome cool how about Pegasus yeah yeah sorry we were gonna make it this morning so we're completed we just completed epoch processing so those a decent chunk in there block rustling is in progress I'd say barring the most recent updates we're approaching you know kind of being up-to-date miss you know with the spec we brought in a be less library the name actually is escaping me what what library brought in a move to age 85 for integration tests and you know like right back at me file you know about the trying to work the other with harmony um and kinda like just like working through some like licensing issues and you know kind of understand everyone's needs and so we're I think we were like pretty close now then East 82 workshop is hearers in Stanford on Stanford campus on Saturday and we're just going to workshop some some to stuff there's gonna be a few presentations and if he can make it that'd be awesome there's you know it's a sold sold out on about right but won't make room for you know II to employers because this would be really awesome if he could make it that's it just a new team members starting next week going to be a live stream for the east to to to so I'll drop that in the chat box we all right thanks great thank you did I miss any teams about one week ago in between we had a lot of issues is that now allegro or implementation from that low passes by a Piscean once everything else is another thing is solution is house or you can generate with plenty of variables the only issue that doesn't work on Windows we and also from time to time when lost are received from party a box [Music] doesn't continue so it's kind of an to maybe check was what should be happening I we started to implement well rap eres a library so that we can that we may have an issue on VLS comm 32-bit platforms we don't know if it's Milagro or something else but it's working perfectly but 42 doesn't work in sync with respect except like client changes and we also are progressing on the deposit contract but I suppose values that are but I can say exactly but it might be just the Milagros OC so okay so there is a branch the pool requests by Chi Cheng wrong where is the branch with the latest fix that work on both sides who is generated through a VM and we can pass them with Milagro so I will put the link of the undershadow scene give me five seconds yeah I don't know issue latest releases and about testings mean issue I have is that the verification test takes a very long time to generate so we probably in to only create a subset like when it's a very long time I killed the job after five openings okay great thank you did I miss anyone this time cool before we move on to research updates I just wanted to give Gregori a chance to introduce himself here's Grigory cool thank you and I I believe so pi vm+ Trinity is a full client and the EVM implementation I imagine is somewhere on the order of an order of magnitude slower but because in the real world a lot of its I own network that it does it's beginning to begin to be moderately performant with you know beginning to be perform it not totally perform it yet right yeah absolutely thank you glad to have you okay let's move on to research on the research side I guess now that phase zero is entering basically yes small change in bug-fix mode its first of all importance there and if keep on keep up the momentum and make sure that we have everything ready for phase one by the time clients get ready for it to develop that so on that side the main and of speckle a whole reunion study LS signatures so we know we don't need to worry about certain kinds of can-do mechanisms for howdy seeds get up the indiv and so forth but we still still do need to have some kind of proof of custody game and for that there's two components to that challenge one is figuring out what the what is the actual hash that goes into the proof of custody so currently in the cross-linked words that we're calling it a short block hash but I actually think I shard block hashes a very suboptimal thing to be I put it to actually be putting into that field so I have some acongress some proposals in issue number of five to nine for basically having a humeral tree of all of the data containing block headers and block bodies and I talked about how some of the advantages there which basically let us if you have sloshing conditions or fraud proof conditions to verify correctness of an entire chain of blocks in a cross-linked to verify them so that clients can don't need to worry about worry about the short chains if they don't need to in stick to the beacon chain along with some other benefits and it's also a nicer for erasure coding which I haven't specified to your program I try to specify soon so that's one part and then the other part is actually figuring out the proof custody game so the latest update on that is that the game that's us sitting around and kind of lying in a half-finished form in phase one that's the weakness that if the data is available and you calculate the proof of custody and then you calculate out the the bit that someone should have made and it turns out that that bit is just too low is wrong then you need to do something like sixteen rounds of I'm asking for more go branch before you can figure out whether or not they actually they actually did something wrong which basically means that it becomes easy to what you would need to extend someone out someone's withdrawal by yet sixteen rounds of like whatever the watch a messaging the way yes and that's the whole note just need to be bought could be imposed even on an innocent validators which is not nice so I've come up with a way to reduce that to something like four rounds but while s still reads retaining the prop the same amount of theater going on chain so I'll probably write that up another important thing to think about is also on the game theoretic issues so the main challenge with proofs of custody is that it's a different kind of game from all the other challenges that we have because for all of the other slashing conditions if you slash someone then the beacon chain can immediately figure out whether you're right or wrong and if you're right then J and if you're wrong with the message is not even valid but here we're talking about forcing burdens of responding to challenges even on innocent validators and we're also talking about the possibility of denial of service attacks around that the possibility of denial of service attacks around malicious validators making and if fake an answerable course of custody for them for themselves or proof custody challenges for themselves and trying to kind of push out to challenge any challenges by made by us someone that actually knows that they're being malicious along with some other issues so actually and if figuring out the economics and specifying that and making sure that that's robust does like one challenge and then also like specifying LP racial coding is another challenge and then that's something that we don't even need to decide which what phase that goes into because that's like in extremely separate and parallel process which is great in phase two so phase one the research challenges are actually where the spec wise are actually relatively small phase one is like meanly yeah I would say a peer-to-peer network engineering challenge and which is realistic way to create if you're an engineering challenge of like pretty unprecedented scale because there just happen to be in shorted lock chains you got there in general like they're having there has never before in human history been a short a decentralized shorted system that attempts to guarantee 100% availability of everything that gets published on to it so it's under state that the challenge here and that this is something that a lot of people who meets the work quite hard on but the good news is that phase 2 is not a very significant networking challenge once you've already done phase 1 but on the other hand phase 2 has more research challenges so from the point of view of getting face to look we're just getting 2.0 launched and usable faster it seems like there's a lot of eye peril I think we can do between those two and as far as like what actually phase 2 contains I'm probably going to write up a lot get some look at some points about like basically what concrete ones can be figure it out what has been figured out what needs to be specified more it's probably still too early to actually start writing a spec because there's still room for and of idea refinements and the choosing ideas but between multiple proposals so the main things there would include I can't into abstraction how the account model works how the rent model works and how hibernation and waking work and a bunch of details around blossom but the components I think are basically figuring out we just need to figure it determine whether or not we're satisfied was I wouldn't remember what more whether what also is talked a lot with the wasum team and see what they think on a lot of these things and going from there I'm not too - won't have any cat math workshop and we'll talk about some of the year and other shorter term research challenges that we have as well another one is also I figure if I continuing to do stuff around CBC and the getting a reasonably a kind of simple and reasonably good number theory number theoretic shuffle and I figured I would inspect and SPECT and approved and implemented yeah I would agree with wimp italic that from the research perspective phase zero and and one are very much under control and we have you know lots of time for for Phase two in terms of research so I'm not going to spend too much time on the face to research in the short term try and focus on getting the phase zero spec finalized as soon as possible try to find the remaining bugs I still think there's there's quite a few non-trivial bugs that are lurking around find simplifications that will make everyone's life easier and cleaner specific things that I've been thinking a little bit for phase zero one is trying to standardize the BLS 1281 curve across multiple block chains so I've approached the block chains I knew I went Macedon BLS both which one which includes a Z cash and Chia and I learnt during yesterday SP during the conference that Definity and Falcone are also interested in that curve and it sounds like people interested in sanitation so standardization specifically includes hash to g1 the hash to g2 includes serialization it includes generators so I think that's a nice effort to try and and so that will likely mean that we would make changes to their current relatively minor changes to the currently specified one question they're given that Z cache is already live with yellow storage anyone they have the least optionality so if the earthing is sane isn't the optimum to just accept it yes but this is possible that they haven't specified all the hash functions for example okay hmm definitely so I can ask you and see what they've done sure yeah I'm also in touch with our current people and they want to do use that curve let me just check exactly what they said keep talking about stuff that'll take might take me two minutes deployment okay the other thing is we haven't really specified right now the the networking topological phase zero especially when it comes to how we would handle the the aggregation for our stations I think one one idea that looks you know very simple which doesn't require a fancy shoddy the network is just have a monolithic gossip sub channel for for everyone and hopefully there won't be too many well if there aren't too many validators in phase zero than that that might actually be a workable solution if not we could we could consider having a hotly feedback from the AMA is looking into ways to try and make beacon if as its called somewhat more transferable or saleable marketable or you know fungible Zak with big words yeah so one of the ideas is to try and make validators be able to change the withdrawal credentials so that would effectively mean that they would sell the whole balance to some other party such as a centralized exchange the other thing we're considering is having a a mechanism to transfer more granular amounts of beacon if to a validator addresses and so that would also facilitate the market and yeah that's it for me cool anyone else from with research updates Leo do you have any updates this week yeah I don't yeah yeah yeah yeah we're having trouble and you know yes much better all right sure my screen maybe I'm just writing the email to introduce the idea the argument yes yes okay good yeah so I just wanted to chalk up a lot of new things that I have implemented in the into the simulator from the last time we talked so this is what you see on the on the top part of the screen is the results that are producing by one of the simulations and on the bottom basically you see the statistics of the imperium Network and I just wanted to compare morale as a couple things with one of the simulations that I have done this morning so this simulation in particular it simulates 256 nodes in the peer-to-peer network so it's that it's a small simulation yet it still is quite is becoming a little bit hard to visualize when when you increment the number of nodes so this is something I have to work on what you see here is the is the peer-to-peer network for those that were not in the cold assign the colors means the number of peers so green colors mean nodes with many tears and if you see on the sides you have dark colors meaning nodes that have a few number of peers so the minimum number of years for this particular simulation is 4 okay you obviously consider this one so here are all the nodes from 0 to 264 no yeah and 256 are and then you see on the y axis the number of peers you see that they very bright from 4 to about 14 and the average number of tears for each net for each node is about 8 more or less ok now I don't know if this is representative of the of the real network but we can this is something that we can change this is something that we can change in the configuration for the simulation I can simulate with two peers or with 20 peers as much as we want and this is something that we can definitely change and undo different experiments on this another thing that I have added is to change the number of messages that are sent at every broadcast so for example if you have even if you have 14 peers I can limit the number of messages in a broadcast to they say 10 and then when you receive a new block for example you will broadcast this information to only 10 nodes among the on the 14 you have and of course if you have less what the engine would transmit to all of those now I have played with this with this parameter couple of times and I have set this parameter quite low and I have noticed that when I do that I managed to create partitions in the network meaning there are parts of the network that get isolated in the in some extremes there is a node in the network that doesn't doesn't receive any information from from from the entire from the whole entire network okay so that is one thing also the simulations produces statistics about the time to block how to produce a block okay so in the Y in the x-axis you see the block number and this is a simulation that was about 4000 seconds which is a bit more than one hour okay and the whole simulation run in under and about 40 seconds okay and then here you see the time to produce the block which you can put you can compare a little bit with this figure that you see in the stats of the hilum Network you see that the image is about 19 seconds in this case more or less do you see that there are some blocks that are produced it like in about 5 seconds sometimes 2 3 seconds and sometimes you can see blocks that are take 70 seconds to get ready now if you compare with this figure here you can see the same or less difference in some blocks that are present every 2 seconds 4 seconds then you see others that are that takes 79 seconds to be produced so more or less took up similar statistics for this particular simulation and and you see that the average block time here is 17 seconds in the case of this malignus about 19 seconds so it's not that different are going down you see the variance in block time excuse me why why are you seeing a variance in block time block sir blocks can be created at each slot correct but there's other slots that are being no so the thing is that I use a kind of you know the for mining I use a random function and the kind of aggregate random solution among all the peers in the network and so sometimes you know none of them find the solution is not really a mining I'm going really but sometimes it takes it takes much time for some of them for any of them to find out a block I don't know if that answer your question I see but in the in I guess in the pre mistake simulation it's not as that should probably be adjusted because there's no there's not like a randomness and finding a block it's a it's more of a time dictated defining block yeah okay so this is these have main chain drugs this is I don't know these are not big honcho joke sorry okay moving to the next figure this is the ankle rate so here they incorporate this represent any number of bank codes for 25 blocks okay so in a bar in history year you have 25 blocks he similar tries to similar to the figure that you see here on this on this network you see most of them I said between zero one or two uncles every 25 blocks okay and that's more or less what you see here this is just one hour of simulation I have other simulations that are longer than two hours or so and you see a little bit more variance but in general of the number of ankles is quite low and and and it's more or less like similar to you know represented people what we see on the Trillium main chain now I have as I said I have played with the number of peers I have played with the number of messages per broadcast and that definitely increases the day anchor rate I have other simulations but I don't want to make a cooperation between as multiple simulations today because I don't want this to take too long I just wanted to show you the feature so but this is something that we can play with the different parameters to produce different ankle rates now this figure is I don't know I didn't find a better way to to plug it I is a saying right now it doesn't give much information basically what is the x-axis is there is a notes and the y-axis is the last block that has been mined and so basically what it shows is that all the nodes are at the same level in the you know I know when the block 200 something and the reason we upload this figure is because by accident that some nodes as I say were isolated and they were not advancing in the in the chain and so that's what I plotted this figure to see if there is any chance any node is getting stuck at some point and doesn't received the updates from the chain so that that's what I I plot this figure yeah it looks are a bit flat but you know at least it shows that all of them are more or less in the same block i moving to the nodes i want to show and this is so you can click on any node in the simulation and you can see all the information about the node so you can see the basically the address they either find out this is completely useless but you know it randomly generates an amount of meter that is going to be news later on to to become validators and you know put your slashing money etc it can be a minor or you cannot some notes of them are minor some of them are not these are not parameter that you can set in the in the simulation is how much what is the percentage of nodes in the network there are miners and what is the percentage of nodes in the network that not - and then you see that the main chain and so you see again the hash the parent the miner so we know that mind the block and the time it was it was mine so is really this is a lot almost 14 seconds is a bit more than one hour then you can see you can go all the way down you see all the blocks that have been mined you can see all the ankles then you have a figure here and it shows the block delay a block delay is more or less what you call hearing this for your blog propagation okay now you can see that in this case the blue propagation is in most of the cases under one second okay you can see that here in this figure the block delay or propagation which is the difference between the time the block was mine and time I receive a notification from there the block was produced somewhere is less is one second now the reason is well one second flat in most of the cases and not milliseconds like 900 milliseconds or so is because the time step granularity for this simulation is one second right now okay now this is something that can be changed in the future we can go a little bit with a finer granularity let's say 100 milliseconds or so I've at this point if time your narrative time is that donor it is one second so that's why we see here some of them are two seconds which is represented before what we do we see here more or less sometimes up to four seconds well it's more dogs that you see here okay now there is another figure also that shows the messages that are transmitted at every single time estate and so in red is the messages that I send and in blue is the messages I receive okay you will see that most times I send messages I send seven messages per time step now the reason for that and this is not for old notes but this is just for this particular note is because I have 1 2 3 4 5 6 7 peers so every time I receive information I broadcast this information and and so this is when you see this this pattern here sometimes I receive multiple blocks at the same time so this is an anchor block and so I send to 2 in for two messages per peer and so that when you see 14 14 messages say at the same time and the integral you see how many messages I received per time step now this is just to have an information about how is the network behaving there are some cases where you can see here is like there's a lot of significantly amount of time that elapsed without without any message being sent office if this is for example one of those blog that took maybe one minute or so to being to being produced okay and I can click on any of my peers and I will see again all the information about the uncles it blocks again the blog propagation the messages transmitted in Surrey you can see in this case how you have seven messages per time step but eight messages I need recon the beers you have one two three four five six seven eight so eight years ago you see this okay now one last thing that you can also look in the in the simulation here is the log for each one of the notes you can look at a very detail all that shows exactly every single message that was sent or received during the entire simulation okay you see here is a new block at twenty seconds after they start of the simulation the source is note 193 and so on and so on and you can see pretty much entirely the information of everything that the note has seen or received or sent during the simulation and these you can see for absolutely any note so you can click on any note and you can see a very tiny look this is great for debugging as you can imagine and it give us quite an amount of information and this can be useful suppose for post post post processing to get more statistics about okay there are couple of models that might be I don't I want here longer but we won thank you I have a couple questions so right now you're simulating a proof-of-work Network correct yes I'm not showing yet the beacon chain I do have with some nations with a beacon chain but I don't join that tryna right so is the intention so you do have beacon chain simulations as well yes yes yes but I'm I'm not really seeing that yet because it's not production-ready okay thank you does anyone have any questions for Leo before we move on cool okay thank you does the pegasus research team have any updates for us today I don't think anyone is present they're just preparing to give the aggregation that scale talk at the SVC tomorrow so they did do some very large simulations on Amazon using AG I remember 50,000 those something like that but they'll be more in the talk tomorrow okay right and that will be live-streamed I believe it will be recorded too and I know it's light I don't think there's a light stream flow SBC but correct me if I'm wrong it would make me happy you could work there are live streaming from SPC like all the talks are being live stream and you - okay if you have a euro that would be great I understood that they recorded that I don't know but I've been yeah okay maybe it's just for on-site but they have like kind of a streaming room but it's coming over YouTube so all what when I get there this morning I'll try and dig up a link yeah I definitely saw it on somebody's computer in a random room so cool okay any other research updates for move on great real quick Diedrich I'm not sure if I got there right great once you have an intro that's yeah I was expecting to be introduced after Gary well you may know me from schematic chart or diagram of the beacon change back oh cool yeah yeah yeah so I worked earlier this week and you may also know me from Eve Singapore you remember me and I worked on a sharding clients there just a hack and then after the holidays continue with bitter work on its and now it's like the life cycle darn have implemented on the ghost and some state machine they haven't implemented all the rules so I wanted to really like look more into the spec and then I thought well let's share it let's make a nice graphic great thank you glad to have you and I have not done a detailed review of that seemingly incredible document but I'm planning on taking a look at it the next few days well and also like I'm working on my own this 4-week client if anyone is looking for a team member for like dirt I'd like a dog see I figure now I'm looking like I'm still studying if you can that's like one a year off and work full-time cool thank you okay the next item on the agenda is a brief discussion on the number theoretic shuffling somebody asked to briefing on the like one or two best candidates at this point sure so probably the we just get the number issue for a number theoretic shuffle again just so everyone can follow along with eggshells you queued the shove the value at any position in one time which is like not nice if you want to just you know whether or not you're the proposer or just want to know some particular committee so the mean the three alternates are there so one of them is this number theoretic shuffle which is just listed in number three - three which just uses the and if X - X cubed plus that K permutation where we have random key values they get so I get selected based off of the hash that comes through the seed the second is there there was the playstyle shuffle that was like a bit later and the third is that we reached out to Stanford academics and they told us about some provably optimal design let's see if I can actually we're gonna go grab it up and extend it over to you and you can probably push it into the chat hold on there so this is like designed to be proved with provably a kind of uniform in the case of at least small including they're a very small set switch so I have ran some tests on the prime shuffle and the feistel shuffle and it turns out the faisal shuffle is basically broken and i think oh the reason why is basically that the it's enough proof of correctness it works well when you have like very largest when you're operating over very large sets but much less so when you're operating over small sets which is what we're doing here according to my statistical test so my statistical tests were basically that I tried running a shuffle with Louisville is either nine other goes nine elements so the factorial of nine is a three six to eight eighty which is like right on the limit of what you tend and then it just shuffled ten million times and check the statistical distribution and check tells you know how uniform it is and it seems to like basically match a random distribution but that's like one test and the reason why I want to talk to academics on these guy two on the same map dais so that we can see what more rigorous tests we can do on it so one approach is starting from practice fractional approach which is and seeing either if you are buying it or making it better the other approach is to start from this existing academic thing which is like provably if either an optimally correct request optimally correct but it's like insanely complex and it's probably even in its current form just too slow for our purposes like the prime shuffles already on the upper edge of what what I think we is acceptable we cast for our our purposes and see if we can kind of start from that and so and simplify things to be efficiency up more while keeping like all or most of the safety so it's the nice thing about it is that it's a good kind of self contains my math / a crypto problem and it's one that should be a fairly understandable even to people who know nothing about block chains so I'll talk about on the math day see what the results are great thank you last week or two weeks ago Alexi had a number of comments and I said we're gonna have it on the agenda this week Alexie's not here but I did want to just if anyone have any follow-ups on its like chain start a number of things with what I think we did we up the minimum threshold for the chain start we talked about it not sure okay one one actually no November we yeah we talked about it and I remember we yeah I think we were undecided but basically Alex's concern was that you could have a kind of majority takeover attack happen with five home with 500 key ether because there's totally cartels that have that amount but if we up it from 500k it's a two meal before the chain starts then like that mitigates that without requiring like any particularly fancy technology and I mean delaying it to to me like you can like even be a healthy thing because if we can't get to know people to sign up that means that week instead change the terms of what we're building anyway right so we'll continue that discussion over time yeah any other comments on chain starts up okay great next is just general spec discussion questions etc as we said at the beginning all we have cut a release first release 0.1 the January release of generally phase zero cool questions comments concerns like you know mentioned we're targeting a test night launch this border and you guys say that you know it's mostly just little bug fixes and there's also some non-trivial bugs but at one point do you think you know look the region will be comfortable with the theme being like hey we're stopping at this commit hash we're gonna be targeting release this anything beyond that we don't promise you envision a good candidate for that might be in a few weeks or for what a testament yeah for us being like hey like we're locking things down like for something like that we have to go run through a algorithmic security audit face first yeah and I just I want to be clear I think targeting your client speaking to other versions of your client is probably a more sane goal in the in q1 if people do have clients that are speaking then that's great but again I think like demonstrating internal networks within a singular client is more sane and just gonna keep you from having to one keep the spec from having to harden entirely and to just there could be a lot of time wasted until we iron out some of these bugs between for consensus bugs especially before we have testing framework like consensus testing in place I think that it's going to probably be very much a headache so I don't expect us to say okay in two weeks or four weeks like this is exactly what all these tests should be targeting but that you should be targeting these releases internally and having begin to have networking internally within your clients cool yeah that's we're bugging yeah I mean if any other games I think you know why try out integrating with the runtime that'd be great but at the moment we're just trying to a basic basic yeah I would imagine that the release today has critical bugs in that like some type of epic epoch transition or some type of an unexpected validator entering would probably cause it to crash so knowing that there's you know I'm not I'm not willing to say like target this because I think it would certainly crash oh okay you have timeline for window stack all big what will will start happening I hope people are kind of at least the teams that are implementing are auditing as well and I hope reading with active eyes and reporting bugs beyond that we plan on bringing more people doing formal analysis both from the software side and on the algorithmic theoretical side with respect to any type of audits beyond that i i'd like to court some people and bring some people into this process but we're not there and I'm I'm not gonna I don't I'm not gonna put a timeline on when we're gonna be there yet before we get to a more stable spec and you know ironed out networking et cetera I'm screwed but if people are listening to this call and out of these types of things please get in touch thing cool anything else yep I was just wondering I've been following the secret of pairing library to see whether it's suitable for a code base and I haven't on that appeal so hashing to be yeah I was wondering it's in your Kassadin if you have any input about that so to give contacts the chats will I just sent an email a few days ago I saw it's all very recent I will I will include you in the discussions if you want to be to raise that yeah my first so I've been working on regenerating the vector the validator shopping young mother dating well they wrote it would be awesome thanks yeah absolutely thank you take a look does does this PR target is this up to date with the release I think we should probably get probably any any test vectors and test generators coming in should target this release so that we can begin this kind of sanely organize that in the testing Rico um it just site I so today's morning Central Europe cool yeah I don't think we changed anything this morning the affected shuffling okay cool thank you anyone else Yannick you joined late I believe did you have any any research updates that you wanted to contribute cool thanks thanks I was just thinking about the caching of positive reaction program started writing would well I'll find it and shall wait is no the our LP implementation doesn't do any sort of caching right yeah good I don't know how the internals are gruff but I don't really know how cushon works well my intention is that they do this scale so it tends to do right could be a good radical yeah if you hear typing that's metallic working on this implementation [Music] working on a more shared effort on the optimal LD go spec implementation rather than just suggesting optimizations is that the is everything there's like a few times and this is well it's certainly thinking like there's nice post it's more like an all-star saying there's a few other optimizations you can also just approach it differently see well you process this on the go through every block process process around trance so if you have this processing going on every block anyway you can just modify a stage for this one block you remove and this one book you act I'd like the previous at the station let's get lost new at station at this added so all you have to do radius change the thoughts you know sort of like a deck as you can do this in optimal way and just go from your edit book back to the initial source tree or the deck and then go back to multipath you walked through the most optimal or the most solid target right so you're saying just keep keep the entire dag and in keep the poets in memory in don't recalculate them every time they can just do linear programming really so you just store for a free hash of every block remember and the best child and the best future far tells it's like the target look and then every time you add a block you basically modifier stack who added you propagate the fault back to the sources tree and then when you want to go find an investor all you basically well from from the source of the tree you find the first look that has first note that has this fart out and they get the the most optimal target walk immediately the only problem is where when you get your thoughts I mean a validator a test a second book a later blow and he needs to remove his fault or the [ __ ] to be removed in the previous address the third and this is a little bit more complex which also linear I hear you I I would be hesitant to introduce that into the specification but well definitely not at the specification of specification should not you should not contain algorithms yeah into some dark we can have like yeah we can link link reference link other implementations and a guide to potential optimizations if you want to write that up we can figure out where to make it live yes and have you worked on implementing that at all it works on the course so thing like this linear thing that's new it's possible that's done on client is just an a normal deck and then every time you process a book you propagate the floats again and then you walk the path in one go see okay thank you shall we do you have a comment all right and you're saying that hi I hope it has some cash but it's not implemented in the PI s SE yet thank you okay anything else spec related or just general anything yes I have a comment some point we were description also having a meeting in April and one of the options there was great was Barcelona and for me is easier to to book a room earlier than at the last minute so I was thinking even if you know at the end we decided to do it in some world I can just cancel the reservation but perhaps it would be good to start to start looking into into that so I don't know if I maybe should have start making arrangements or if we can perhaps that would mean to beit's yeah ok so we are going to be in Sydney and so is lighthouse so that's definitely a possibility but offline I'll kind of begin talking with some of the teams to see if there will be representatives out in Sydney and if not then we will begin to discuss where and when you know if I don't I don't have it I don't have any sort of date so I don't I wouldn't recommend any when you would pre booked a hotel but and I recommends not free bookings yet yeah and even if we don't if we don't do Barcelona for in April and it's totally fine place and we probable to and need to meet up in here at least once a year I'm sure soon yeah so you're right it's like as February February is coming is probably time to begin talking about that but I'll I'll think up with everyone offline and figure out communally what's gonna make sense for us okay thank you think of meetings most of the English team will be in Brussels for this weekend for the first then another week next week and there is a new muted but yeah I believe there will be some amount of people from a bunch of teams of those and also at East End beer as well so I expect a number of kind of informal gatherings and then will again try to do something there more official in April I vote Sydney for April then did I see you unmute did you have something yeah say question just to get some clarity on whether we might meet up I need to decide whether to go to a phone or not if we're gonna meet in Australia and I'll go if not know what okay well maybe I'll just drive everyone okay cool anything else before we close the meeting great I think I'm gonna add a new agenda item next week or for subsequent meetings which would be testing I there's enough work going on there that we should do a sanction a little update beyond that I think two weeks from now makes sense but I need to look at my calendar and make sure there's nothing conflicting I'll let y'all know in the chat as always hit us up in the getter make issues as you find bugs there are bugs read and implement with active eyes please and we will be doing a release X week in February okay thank you everyone see send me all at SPC today and have a great day otherwise right thank you I just remembered why I despise a little ideas like inside of in the binary either just like binary string representation [Music] [Music] [Music] [Music] [Music] [Music] [Music] 