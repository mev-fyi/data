foreign [Music] [Music] foreign [Music] [Music] foreign [Music] questions [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] thank you [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] thank you [Music] foreign [Music] foreign thank you [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] thank you foreign [Music] [Music] thank you [Music] foreign [Music] [Music] [Music] thank you [Music] foreign [Music] foreign [Music] [Music] [Music] thank you foreign [Music] thank you [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] check [Music] thank you [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] thank you foreign [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] thank you foreign [Music] [Music] thank you foreign [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] [Music] thank you foreign [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] come on [Music] foreign foreign [Music] foreign [Music] okay we hello everyone welcome to the Thanksgiving Workshop uh Defcon Edition and cool so first I guess half of you have already know what is okay and so this is the latest Instagram [Music] one moment it's the search foreign sharding design that proposed by Dan Clark he's here today in 2021 and so in this new design that it unlocked so many scaling um I mean The Challenge and it's not that no the charted data we don't we shot it at the data blocks rather than having many EPN structures but the dark jotting protocol is there are still some um technical challenge that we need to fix so right before we have the full time charting uh Proto is also here and uh many of us um they call elaborated and figure out that we can have a more visible Solutions like in the short term to address um this sale links and it can greatly scale the ethereum with their two Roll-Ups in in the very near future so that's the Proto and dankara and this is the Proto then okay so there are some and many common features in both eip4 and the default then sharding so today we will break down these topics and you can see that they both have the kcg commitments so um Democrat will introduce the cryptography part uh first and they also have the blobs transactions so we will also introduce like what is blog today and fee Market is also the shared common features here and um so the challenging part of dangshotting is the PBS and the DS so we will also talk about it later okay there's the agenda today we have a very rich agenda in the next two hours so yeah and that will I will hand it to duncra next to introduce the cryptography in Junction so if you can go through this you will know everything okay thank you thank you sir thank you cool so um I will uh be giving an introduction um to kcg commitments um uh I'll be starting with giving a motivation to um to understand why why we need these Advanced polynomial commitments and why we can't do all this uh simply with uh Merkel routes which we're all quite familiar with um uh so I'll be going through the motivation uh I'll quickly go give an overview over the final fields that we that we use in order to commit to uh polynomials uh um I will kind of motivate ktg commitments as like hashes of polynomials then go to the actual meet which is um how do kcg commitments work and finally because we also use these a lot in our construction now um I will be going through the technique uh which I call random evaluation which is um a nice trick that you can often use to to work with polynomials and that that makes a lot of um things that you want to do with polynomials a lot more efficient um cool so let's talk about uh data available assembling and eraser coding so um so so what is data availability sampling so the idea is uh we somehow have a large blob of data and um and what we're working on is scalability so scalability means that somehow we have to make it so that a node has to to uh do less work to achieve the same thing that we do today so right now every node ensures that all ethereum blocks are available by downloading all the blocks um that's just an implicit part of it like it seems obvious because right now you also execute the full blocks but it's one of the things that don't scale in the current ethereum system so we need a way um to to reduce this this workload but we want to do it in such a way that we don't lose any of the security that this provides and that that's what makes us tricky and um so like the the basic idea is okay but if we take our um data blob and we just check um uh that's random samples um of uh the data are available um so if we do this naively if we just take the data as it is then this this does not uh this does not really work um uh because uh even even missing a tiny amount of data um is catastrophic potentially for a blockchain um but but best by by doing random sampling you can never find out whether a tiny bit is missing you can only see whether major parts of the data are missing so what we'll need to do is in order for this technique to work um is we need to encode the data in such a way that um uh that even having some parts of the data say 50 is enough to guarantee that all the data is available um and so the way we do this is we extend the data using um ISO called resolomon code and um uh if you know a little bit about polynomials like um read Solomon code is nothing else but but extending um the data using polynomials so what you do is let's say like in the simple example um we have four blocks of original data and what we'll do is we'll um we will take these four as evaluation as an over polynomial there will always be a polynomial of degree 3 that goes uh through these four points and then we can evaluate this polynomial for more points and what this means because Four Points always determine a polynomial of degree three that any of these four points are enough to reconstruct exactly the same polynomial it does not matter which four points you have and so this this is like this is the basis of Erasure coding and now because of this the data will be sampling idea that we had here and actually works because now I don't need to ensure that every single bit of the data is available now I only need to know that at least 50 of the data available and then I can always reconstruct everything yeah sure yes yeah we have double the data no so that's the trick so um we do random sampling so as an example we query 30 random blocks so if the data is not available that means the attacker needs to have withheld 50 because if they submitted more than 50 to the to the network it's all there okay so if it's not available then each of these samples because we used local Randomness to query them has only a 50 chance um of succeeding so that means in aggregate the probability that all of them succeed is now 2 to the minus 30 which is one in a billion so this is why it scales you don't need to ensure you don't need to query fifty percent of the data you only need to do like a tiny number of random samples and this number is constant so it does not depend on the amount of data foreign polynomial so we have these evaluations this was the original data d0 to D3 and then we have these extensions that we computed and we just compute an a normal Merkel Tree on top of that and use this root as our datability root so the problem with this is that mercury Roots do not tell you anything about the content of the data so like it could be anything so in this case let's say an attacker wants to trick our data availability system they could just not use this polynomial extension but they could just put random data so basically in coding terms they have provided an invalid code what that means is that any if you get four different chunks of this data you would always get a different polynomial so you would like so consensus is all about agreeing on like something like and and in this case we wouldn't actually have agreed on something because the data is different depending on which um on on which of these um samples we've got so the only way to make this work is if we if we add fraud proofs to the system where you basically prove uh prove that that someone has provided this invalid code um but that that isn't great so that that has some problems um they add a lot of complexity and particularly in this case because this is about the layer 1 itself it would make our system very very difficult to design because now validators would need to basically wait for this fraud proof in order to know which block to even vote for so there would be kind of very Affy to design this um so the interesting question is what if we could find some kind of commitment um that instead always commits to a polynomial so we always know that the encoding is valid cool and that is why we will introduce kcg commitments and we need to start a little bit earlier so I will start from by introducing finite Fields a little bit for those that who are not familiar with it okay so I can return you um okay so what's the finite field okay so to understand uh what a field is um it's basically think about um rational real or complex numbers which you've already learned about and just remind yourself like we have basic operations that we can do in them we can add subtract multiply and divide and we can do that do all of these except division by zero so that you you always um you're always able to to do these operations and you have some uh you have some laws like their associative commutative distributive basically just think of like I mean I I could give like the formal laws here but I don't think that would be the best illustration because you're already very familiar with these rules when you work with um rational or real numbers and the big difference finite Fields is that unlike um uh unlike these uh these fields that we're very familiar with they which all have an infinite number of elements they have a finite number of elements that's quite important because otherwise we can't encode them with a finite number of bits which is yeah kind of like something that we that we need to be able to do um so yeah so that means that each element can be represented using the same number of bits and um as an example uh on of showing how this works here's he has a very small finite field this is F5 and basically the way it works is you uh you take uh the five numbers zero one two three four and um and you you use your normal integer operations um to compute um like the addition subtraction and multiplication uh but whenever you've done that you take the result and do module take the remainder after division by five so you take it modular five and then um when you write it out um basically you'll find that for each element so we haven't yet defined like how do we do division um so if you write down the multiplication table you'll find that each element has actually an inverse so basically um if you take uh for example here like a 2 then you can see that 2 times 3 is 6 but modulo five that's one so it has an inverse okay that's nice and like then the other way around three the inverse is two and for four um if you take four times four it's 16 and it's uh um and that's that's again modulo five that's one um and so we've just found like by just listing these these numbers that um every element has an inverse and the reason for that that is that 5 is a prime number so whenever we to take these modular operations modular prime number um then then we'll find that we actually have a finite field um and so that that's our final Fields um um except that the fields we're going to be working with in practice will have a lot more elements so the prime that we're going to be using will have 255 bits so it's like a very very big number because yeah we want to be able to represent a lot of numbers in this field cool okay um hashing polynomials okay so a quick reminder what's a polynomial and so polynomial as an expression um of this form so it's like a sum over some coefficients f i and uh and uh and terms of the uh times uh term of the form x to the power five um so the property is that um uh this it has to be finite sum so it's a sum from zero to n and we is is the degree of the polynomial and um and basically the other important thing that you have to always remind yourself there can never be any negative terms so you cannot have x to the minus one it's only terms of the form x to the power of 0 1 2 3 and so on um yeah and um and each polynomial defines a polynomial function so it's important to distinguish between the two so polynomial is just an expression of this type so it's just you could think of it even as a list of coefficients um and like and then it defines a faction but like for example in some Fields um like in finite Fields you you will have the property that the same polynomial function can have many polynomials corresponding to it because there's only a finite number of functions but there's an infinite number of unpinomials um this this property you don't have in infinite fields um and so the the core property that polynomers have is that for any K points there will always be a polynomial of degree K Min K minus one or lower that goes to all of these points and um and that's polynomial is unique um and the other um uh properties property is that a polynomial of degree um n that is not constant um has at most n zeros okay um so what what what would be cool if we could imagine a hash function for polynomials so let's imagine that we could have a hash function um that uh takes a polynomial um and hashes it okay that's easy but it should have a have an extra property which is that we can construct proofs of evaluation so basically what we want is that for any Z so any point um uh we want we can evaluate those polynomials comply compute y equals F of that and we want um some um some proof that this is correct so that that um that would be um an interesting hash of polynomials that gives us something new and um and this this hash and the proof um should be small in some sense so um here here's some idea okay um what if we just choose a random number for example let's say we choose the number three um if we want to Hash a polynomial we just evaluate it um at this random number um three so we said we put x equals three um here's a couple of examples how that works if we stay in our small field F5 for between that provide before with just those five numbers so if the opponium is x squared plus 2X plus 4 then the hash is like x squared 9 plus 2X 6 plus 4 modulo 5 is 4. and then here's a second example so a bit of a bigger polynomial module 5 in this case it's zero okay um that that seems a bit stupid to just do it at one point but the interesting thing is if our modulus has 256 bits which is what we're going to work with in practice it's actually extremely unlikely that two randomly chosen polynomials have the same hash and quotation marks just like it is for a normal hash function right so uh that that that's an interesting property like I mean it seems like a very stupid and simple operation but in some ways in one way it already has like a property like a hash okay um okay so if we accept this for now then let's have a look at some of the things we could we could do with it um so for example we can actually um add two hashes of polynomials um so like if we have uh if we have the hash of uh take oops if the take of the hash of two functions uh hash of F and F hash of G then the hash will just be the sum like the hash of the sum of the functions will just be the the hash of F and plus the hash of G um and that's because of this homomorphic property which is Trivial if you write it out in polynomials um and um the same is true if you um uh if you multiply two of these volumes and that's just because polynomial evaluation itself is a homomorphic property like if you you can either first add to polynomials and evaluate them or you can like do evaluate them and then add the result um and the same for multiplication so it has some really cool properties if we could use this hash function but there's one problem um uh if if you use this um then if someone knows this random number right then they could easily create a collision of this polynomial function because um while for random polynomials it's very unlikely that they evaluate to the same point it is very easy to create like manually to polynomials that evaluate to the same value at this random number um so it doesn't quite work as a hash function as we know it um but what um it would be different if somehow instead we could put this random number into a black box so if we could uh if we could find a way of computing with these finite field elements um but instead of giving everyone who wants to evaluate this hash function giving them the actual number you give them a black box um so like we we assume we have a cryptographic way of putting a number into a black box and then we give them our random number as and we give them also like the random number squared and S to the power of three but all of them only in the Black Box um and we do it in such a way like this black box needs to have the property that you can multiply it with another number and you can add two of these but you cannot multiply two numbers in a black box um so if that if you could do that then this would actually work because now the attacker would not be able um to like create these two polynomials because they don't know they don't know this this number um and so they they cannot um they cannot craft handcraft the polynomials that so that they evaluate uh to the same uh number at that point and uh basically the cool thing is that elliptic curves actually um give you give you exactly that so um uh elliptic curves um are basically uh you you can think of them as a way of creating Black Box finite field elements and the finite field that you have to use is the curve order of that elliptic curve so if we have an elliptic curve um which we call G1 why we need this in xg1 we'll come to later but it's just elliptic curve that has a generator which means that that's a point so that the um if you if you add that point again and again um it will generate uh your whole curve and um the the order of the curve is p so that's the number of points um and then basically we have the property that uh that x times G where X is a finite field element x times G1 um it's basically this black box and the reason for that is that it is hard to compute so-called discrete logarithms so it's um it's difficult like when you when you have computed this x times G1 um it is uh it is difficult um to to compute X from that point um so that's that's a cryptographic assumption and so if we have that um then if we take uh two um if we took two elliptic curve elements um G and H um then we can multiply them with field elements like we can compute x times G we can add add the two G plus h and we can compute linear combinations like x times G Plus y times H but what we can't compute is we can't we can't without Computing the discrete logarithm with the chart we can't compute something like G times h and so um just like we uh we said before like we we want this black box so we will introduce the notation um like X and squared brackets one for saying that it's in this uh G1 which is the first elliptic curve we're going to use we need later we'll need another one um we Define that at x times G1 and so basically when you when you see these square brackets think of it as like this is a prime field uh element in this black box in this elliptic curve black box so we can put stuff inside and there's no easy way to take them back out but we can do some computations while they're in there cool and with us we are ready to introduce KCT commitments okay so what we're going to do is we're going to introduce a trusted setup so we're going to assume that um that that someone has um computed has taken a random number s um and they've computed inside this block box and given to us and the powers of s s to the power of uh 0 1 2 3 and so on in our black box and actually forget this second one for now we'll come to that later um and um and so uh if we take a polynomial function so we've defined this previously so it looks like this it's like a sum of coefficients times powers of X and we Define the kcg commitments as um as this sum which we can evaluate so we take the coefficients and we replace x to the power of I by S to the power of I inside this black box and here on the left like so this is something we can obviously compute it's just a linear combination of these elements which we have been given as part of the trusted setup and uh and the cool thing is if you write this out in in effect if it is it is just an F to the power of s um evaluated inside this black box so effectively we've come back to what we said before um it's just this random evaluation but we've managed to now randomly evaluate this polynomial inside a black box at the secret point and um uh yes and this this uh we call the kcg commitment uh to the function f and um now in order to um uh to do interesting things with this um we'll need to introduce um elliptic curve pairing so this is where we where we get our second group so we actually need a total of two two groups and what we'll have is we we have a pairing is a function from two elliptic curve groups and a Target Group which is a different kind of group it's actually not an elliptical but that's not too important here and it takes basically these two elliptic curve elements one in D1 and one G2 and it has the cool property that it is um what we call bilinear and so that what that means is that um you can you can compute this um uh this linear combination so for example if you have the pairing of a times x and z that's basically you can take this a out um the same in the second coefficient and in addition if you have the sum then um uh then basically um what it does uh it it's it splits into these two so it's like a distributive law here X Plus y times Z is e of X that and E of y z and the same goes again in the in the second um parameter of this function and um and basically the cool thing is um that uh that this um what what we couldn't do before um inside yeah his life yes um and so what we couldn't do previously between elliptic curve points which is multiply multiply two elliptic curve points is that we can do in a way between pairing so if we have one of our points in the in this first group and one in the second group then due to this bilinear property we can it actually in in the in the Target group it computes something like x times y right so it has this property we Define we Define this additional notation for the Target group and then we have this very clean and nice equation that the pairing of um X as a black box element y as a black box element is x times y so this is very important basically at this point when we have the pairings and that's why we really need them we can do one multiplication you can we can only do one because afterwards we get this target group element and that we can't really do anything with um but it turns out that this is actually um actually enough to do like a lot of very useful stuff in elliptic curves two polynomials um f and g and we commit um to those polynomials but we can commit to f um in G1 and G in um in G2 in the different groups and then um then basically this pairing actually lets us compute um this like the product of these two commitments um in the Target group so basically um in this really cool polynomial hash that we have defined we can now um if we commit to them in the in the right groups we can now multiply two polynomials um that are committed in this way so we can multiply the commitments without even knowing the polynomials themselves okay cool um okay so we will need uh to introduce one uh one last missing piece in order to fully come to our um how kcg commitments work and how we can construct proofs and that is uh quotient of polynomials okay so um let's say we have foreign we have a polynomial f of x and we have two field elements y and z and then we can we can compute this quotient Q of x um this is a rational function right so like a polynomial divided by a polynomial is in general a rational function um so you can just see this as like a formal expression um but sometimes this quotient is exact so sometimes like this quotient will actually result in another polynomial and basically there's a theorem that's called the factor theorem um it's a relatively elementary math you've probably learned that in school at some point without calling it that basically says that this is a polynomial disclosion um exactly if F of Z equals y and um I mean you can kind of see like that um in One Direction because f of x f of x equals y then um then at the if you set x equals y you get a zero here um f of z f of Z here and you get a zero here so like zero by zero that can that that that that that can only like that yeah so sorry if if the quotient is zero at Z that can only work if this is also zero at that Z So like um so it can only really work if this is this is correct but the other direction is a slightly um slightly more complicated so if you restate this basically we get the fact that we get the polynomial that fulfills the equation this equation so we just put the x minus C on the other side Q of x times x minus D equals f of x minus y if and only a f of Z equals y okay and now we get to how the kcg proofs work so if approver wants to prove that um f of Z equals y the computer desk version Q of X which is f of x minus y divided by x minus Z and Center proof Pi which is Q of s so the commitment to the polynomial polynomial Q um and in order to verify this um uh what the verifier will do is they'll take this quotient and they will multiply it by the commitment to S minus Z and check that this is the same as original polynomial the commitment to that minus y and so this is unfortunate very readable on this background um because if you write it out in this pairing group then you get um on the right hand side Q of s times s minus Z in the Target group equals F of s minus y and this is the same as the second equation so the cool thing is we can verify this equation because we are able to multiply two polynomial commitments inside using the pairing and this way we can verify that the portion was actually computed correctly cool yeah and that that is basically that that is the that is how kcg commitments work so like just to yeah so yeah um so so the idea is just um if you can compute this this quotient then you'll be able to find something that fulfills this equation and using the factor ethereum that we mentioned previously if F of Z is not y then you cannot compute this it doesn't exist it's not a polynomial and we can only commit to polynomials so yeah this is the recap on the kcg commitment we can commit to any polynomial using a single element in G1 and um it is and this is just the version valuation of the polynomial at the secret Point s um inside the black box um we can open the commitment um at um any point so we can compute F of z um we and by Computing the quotient Q of X we can we can compute this proof which is Q of s in the black box and in order to verify that proof we use this pairing equation and um and that that shows a verifier that um this evaluation is correct cool so that is uh kcg how kcg commitments work now this I want to do something slightly more which is a technique that we use um quite a lot we have even using it in uh eip4844 and so I want to give a quick introduction into how it works which is a random evaluation trick um okay um so basically let's recall that kcg commitments are nothing but evaluating a polynomial F at a second Point s inside this elliptic curve Black Box and so in a way this is already like a random evaluation like but basically what we've done is we've we've identified this polynomial using a random evaluation and we kind of we somehow found that this is good enough to like um to Hash a polynomial in a way that uh it's very difficult to create Collision and um more generally this random evasion trick can be used to verify polynomial identities and the reason for that is the amstrad's typical Lemma and I will just formulate as a more General one but let's say what it says in one dimension so let's have a degree a polynomial of degree less than n that is not identical to zero so there's one particular polynomial that is zero everywhere that's just like all zeros right that's a very special polynomial so let's say it's not that now let's take a random uh point that in FP then the probability that F of that is 0 is at most n over p and that's because it can have at most n zeros and so this is a very useful thing because um our p is very very large and our degree is relatively small compared to it in our case so for example in for plaster 381 PS 255 bits say we commit to a polynomial of degree 2 to the 12th then as probability is something like 2 to the minus 240 so like it's a very very small probability um and so here here's the first way um in which uh in which we can use this so like we have this transaction blobs that um we'll Define for four eight four four so it's like uh they are commitments to um polynomials with four thousand six ninety degree 4095 so in total four thousand ninety six points and uh to committing such Computing such a commitment is not very expensive but it is expensive it's like for 50 milliseconds to do this but verify one kcg proof is quite a lot cheaper it only costs about two milliseconds so we can use this to our advantage and so the idea is this we take our commitment to the polymer C and we take the polynomial F itself so what we want to verify is that we have the polynomial that it's given to us in this case we have all the data and we have the commitment and the naive way we can just commitment to compute the commitment from the polynomial but that's expensive okay how can we do it cheaper um we do we compute a random point and one way to get a random point is actually a very cool technique it's called via chamir and we take all our inputs so we compute that as the hash of the commitment and the polynomial why is that kind of random because like if an attacker tries to craft something if they try to adversarially compute either C or F it will always change the point that so it's very hard for them to find some like uh to to craft them in a way that that breaks our construction so basically this is a common technique in in cryptography um to to get something random that the attacker cannot control and so um we evaluate this polynomial why at um at this random point that we've taken and then we compute a kcg proof that F of Z equals y and basically that then what we'll do is we just add this proof um to our transaction blob um wrapper which is the way we're sending uh transactions and then like to verify this you compute he also compute F of Z which you can do because you have the data for that and you check the proof Pi the kcg proof and that's done and that's much much cheaper than Computing the commitment and so that's one way in which we can use random evaluations to like save us um a lot of work and making things more efficient okay um so uh here's another way in which we can use those random evaluation technique and so ZK Roll-Ups um uh they use many different um proof schemes and so um only a handful I don't know if actually there any right now um will use natively kcg commitments over over BLS 12 381. and so the question is like um how do all the other make efficient use of our blob commitments that we want to add with 4844 and then full charting um because like because Computing kcg commitments inside a proof or Computing pairings that is pretty expensive like that that that's a very expensive operation in a zeroid proof foreign [Music] is basically you have to uh you you commit to the data in a different ways so we have uh we have three different inputs so we have our blob data which is this function f itself and um and we have two different type of commitments now we have C which is our blob commitment which is what we'll use um inside ethereum um uh for 4844 and we have another way of committing to this data which is using um the the ZK Roll-Ups native commitment so they will in some way um it will also have some way of committing to data that that works well for the as you know it's proof scheme and so in this case what we'll do is we'll take Z as a hash of C and R like these two different commitments and um we will compute y again as F of Z and we'll add um pi as a proof that F of Z equals y and we'll we'll add this pre-compile that allows us in the ethereum virtual machine to verify that the the kcg proof pi um so we will know that c c is a correct commitment to f um and what we'll need to add is uh to add the proof that R is also in a correct commitment and uh and the ZK roll up can do that inside the proof so they will inside the proof and they also have to somehow get C and R as an input and hash them and compute that and then they can um they can evaluate so they will also have F because the rollup wants to use the data so f is completely available to them and they just have to compute um y equals F of Z and use some technique to verify that the f is the same as they are but that's there are ways to make this easy and then they can verify that they have the same data as was committed through C so that will make it much easier to use these commitments in ZK rollups foreign and um yeah I collected uh some resources if you want to um read further on this um so vitalik wrote a while ago a post on elliptical pairings [Music] um um I uh because there was a lot of interest in that I wrote some notes on how on this last part how to use kcg commitments in ZK roll-ups um for those who are like kind of uh skeptical and they're like wondering do we really need um this like Advanced cryptography and trusted setup and so on um vitalik recently wrote a summary on like what what the difficulties are with um alternatives to KCT commitments um and um here this is uh if if you uh want kind of it's it's very similar to this talk but I I brought a blog post about kcg commitments and then of course if you want to dive deep there's the um case the original kcg paper and if you scan this QR code there will be all these links yep you look at the thing I don't understand why where do you want to open it multiple times so just checking right right are you talking about s or like the the The Trusted setup yes yeah right but this is a cryptographic probability right we're talking about I mean that's why we're setting the security to 2 to the minus 128 so 2 200 yeah is not like probably yes but so we are setting uh in cryptography we are setting our security parameter already in the assumption that an attacker will do a lot of computation to try to break it like 2 to the 50 2 to the 60 or more computation power this is much much more than however we ever use it in the in the actual protocol so like this is all already covered by the um by the cryptographic construction um I mean you can do it but the random probabilities like the probability of randomly hitting that are extremely extremely low like if you like if you construct it so that the probability so yeah yeah like randomly they are less than two to the minus 200 or something like that it's like so low you cannot even like yeah think about it yeah x to the P minus X right yes yeah no you can't but that's fine I mean that the um somewhere correct but okay so we are always because we are limiting the degree of our polynomials right so our trusted setup will only go to a certain power for example to the power of twelve and inside that space there's only the zero point number yes yeah yeah so like if you have no limits on the polynomial decrease then it doesn't work but we always have a limit okay thank you thank you excellent excellent thank you dankrad for the math so okay so now we're gonna go into the bit more like kind of kind of like we're in the sky of math and we're kind of like tone it down into like the protocol stuff so I'm gonna start with a small like um explanation of how all this math stuff going to our protocol and how like you know all the extra bandwidth of 4844 travels around and gets verified then Proto is gonna take it and tone it even down into more practical stuff like how the l2s are gonna use the data and then answer is going to tone it even more down and basically explain how people pay for this data so okay so basically um this is a graph that shows how um like optim optimism and L2 uh what is its costs and you can see that like this blue stuff is the data fees like how much money they are paying for the like data they put on chain and the other like white stuff is some other stuff but you can see that the blue the data is dominating all the costs so basically what 4844 is it's like a mechanism that drastically increases the amount of data people can post on chain um and this is all it is right so okay so basically um what we want to do is we want to increase the amount of data so um on this very simple picture on the left side you can see uh our data which we call blob because it's a bunch of data that also corresponds to polynomial and on the right hand side you can see a small thing a commitment that represents that data commits to that data and the like graph idea is that you know commitment goes Unchained forever whereas The Blob is kind of like you know there for a bit and then disappears so this is like the high level strategy of how we increase bandwidth we commit to data we keep the commitment forever but the data is ephemeral in a way okay so let's talk a bit about what this data is what these blobs are how do polynomials enter this picture uh so okay this is a polynomial I think by now you're very familiar with it based on the last talk uh the question is like how do we put data into this polynomial and like the basic idea is you know you have these coefficients the A1 a2a whatever and each it you you can basically put data into this coefficient so you know if you have some data uh one four one six you can put it in the coefficient so you make this little polynomial on the bottom and that's like a very straightforward way to put data into polynomial so so right so think of like what like in in our case um let's see about these numbers one four one six how they can resemble real data so in our case uh the numbers are going to be finite field so they're going to be parts of a finite field which is going to be like a number between zero and this insanely huge prime number um and so each coefficient is going to be a number between these two things and that's about 254 bits that's about 31 bytes so a coefficient with a polynomial with like 4096 coefficients can store about 128 kilobytes by putting the stuff into the coefficient so you know now we know the way to store 128 kilobytes into polynomial and that's kind of interesting because like you know right now roll ups they don't even use close to that number like maybe they use one kilobyte so we're basically giving lots and lots of space maybe even uncomfortably lots of space to roll ups to put their stuff in but this is like the whole idea of 4844 of course in reality we don't put the data into the coefficient and we put them in the valuations and then we're doing interpolation but like whatever this is not so relevant for this case the idea is that like you know when code data into polynomial and we have polynomials that correspond to a big amount of data and that's a blob right and you know then we have kcg which is what dunkrat was explaining for the past 45 minutes which is basically like a black box where you give a polynomial to the black box and spits out the commitment and the commitment is Tiny and the data is Big so you end up with a situation where you know you end up posting a chain lots of data and then a small commitment and this is like the rough idea so just to talk a bit about like when this data travels what the network is supposed to do you know like when you see a commitment that corresponds to lots of data what the network needs to do is like they need to make sure that the data corresponds to that commitment and like the the basic thing to do there the basic strategy of the verifier to make sure that someone is not like you know fooling us and giving us a wrong commitment to other data which would be catastrophic is to you know like commit to P of X use this black box again commit to it and then check that the commitment that the verifier computed matches the commitment that the guy gave you so that that's basically a pretty straightforward way to to verify um that polynomial matches the commitment but you know then we have more data and more commitments you know in a transaction you can have lots of those in a blog you can have block you can have lots of those and and that starts being quite expensive so what we end up doing because you know like it's 50 milliseconds to do each of the commitments so and it scales linearly so that ends up being quite expensive especially you know for mempool and this kind of stuff so in the end what we're using we're using kcg proofs and this whole random evaluation trick that dankrad taught you before and basically um for it like data and commitment we also put a proof of a random evaluation so basically the proof is a helper that helps you um do this small verification and I don't have enough time to go into the details but like the idea is that you know like the proof tells you that the committed polynomial evaluates to Y at Z and then you can also evaluate the polynomial on the left side at Z and get some other number and if the y1 and Y2 matches you're certain that it's the same that the polynomial matches the commitment and this is much faster than doing the commitment manually um it's not my intention to go very deep into this I'm just giving you some idea of how kcg is used in the protocol so um I think I'm gonna stop here and and stop with the cryptography and pass it over to um protov who's gonna go a bit deeper into the actual system [Applause] foreign so let's talk about the blood pieces so with ep44 we're introducing a transaction type to make to confirm these blobs in the evm10 however something to note is that it's A New Concept here where we are having a transaction type with data outside of the transaction that's now responsibility of the consensus layer so it's like a regular Erp month of F9 transaction then the transaction contains some pointers or hashes ready let's then commits to the debloop data thank you this is the transaction in a little bit more detail something else to note is that it's not rlp but as you see so it mercolizes nicely it's better for layer too and then note here that we have these data hashes committing or to hashing the case D commitments which then commits to the film block data these data hashes are available in the evm so the blob content is unlike call data not available in evm eventually we can prune this blob data it's not a long-term commitment to store off this block data but rather we are introducing this blob data and just for the availability properties a layer 2 needs this data to help users sync the latest State permissionlessly without communicating directly with the sequencer or whichever operator exists on a rollup and then people can reconstruct the latest stats they can have a different solution for retrieving very old stats like a month ago or a week like two weeks ago so there's the separation of data and the transaction itself so this is what the life cycle looks like as a layer to user you submit a transaction then we have this bindling as a layer two we often combine the transactions so you can pack them compress them and so on this is task for develop operator and then as a rollup operator you publish your bundle to layer one with this new transaction type and then in the transaction pool we have both the transaction.bes the fee as well as the wrapper data with the actual blob content and then the layer bomb Beacon proposer creates a block and the blobs make their way from the transaction pool in the execution layer to the consensus layer a disparents the blobs don't get into the execution layer back it's just the responsibility of the consensus layer um pairs on the beacon that and they think the blobs bundle together with blobs from other transactions as a sidecar and then the execution payload stays on layer one whereas the blobs stay available for a sufficient amount of time to secure layer 2 but then can be pruned afterwards so blob data is bounded foreign this is what it looks like on the network level we have the layer 2 sequencer communicating with the transaction pool the execution engine communicating as the beacon proposer then Beacon notes syncing the blobs with each other and then there's the splits of the data where the other big notes they give the execution parallel to process the ECM and everything fees will be processed by everybody the Deep blobs they are they stay in the contents layer into a layer 2 Note retrieves them to reconstruct the layer 2 stands foreign so how do Roll-Ups work with this um thank God already explains the proof of equivalence trick so I'll give you just a simplified overview how we do this in the evm we introduced two new things in evm and opcodes and a pre-compile the up codes simply retrieves the data hash which is this this hash that is part of the transaction just like the hashes in the access list from the Berlin transaction type instrument can be retrieved through an up card pushed on the stack and then there's this pre-compile which you can provide as a proof to verify that a certain data at a certain position matches the deep Loop contents committed to by the data hash and in that case of zika rollup V2 so we use this pre-compile to do a randomly Federation and prove that the data that the rollup is importing is equivalent is equal to the data that develops this into the blob is introducing this pre-compile is versioned so we can change the commitment scheme and in the future I hope we can use it for other things perhaps Freckle tree verification then this is part two yes [Music] or red so going back deep proof all the inputs that pre-compile they're passed in as call data The Blob is completely separate it's not involved in any of this computation and so to just decode data that we're passing in um with a proof the index of the pointer trying to fit for about the commitment that hashes to the hash that we retrieve from the up cards and then the pre-compile will verify everything um similarly we have the zika state transition that needs to be verified this is also you can roll up specific um up to you to design this but with the data that's verified and the secret proof that's verified we can then get some outputs that we can persist and then use to enable withdrawals and then this is the version of the interactive optimistic roll-ups interactive optimistic rollups use this concept called a pre-image Oracle where we do not access all the data at the same time but rather we loads pre-images one at a time and by bisecting an execution Trace we only really have to do a proof for a single step a single execution um of a single like VM instruction and this might be loading some data so for example the start of a layer one block header hash then we retrieve the fill block header as a pre-image then we retrieve the transactions by digging into the Mercury commitment in the transactions hash and then we can get the data hashes from the transaction and then from the data hash we can get the case D commitment and then it's not a regular hash commitment anymore but there's a different type of commitment with the same Oracle very loads one point from The Blob that is committed to by the block transaction and so this way we can load all the data into the fraud proof VM thank you [Applause] thank you okay yeah hello everyone um a Man's Guide I'm going to talk a little bit about now that we hopefully in the future will have this functionality how how can you pay for it but also kind of conceptually basically um I mean the data invention is already you know kind of pushing its limit like where's the extra space and resource-wise for this basically where where does the efficiency gain here come from um and to understand that first we have to just look in general about like how do how does research resource pricing on ethereum work today so this is just um kind of my way of thinking about categorizing the different resources we have we have on ethereum so there's things like bandwidth compute State Access Memory State growth history growth right this is all the kind of things and this is a non-exhaustive list right but this is this is basically the kind of things that actually cause effort for nodes while they are processing a transaction and if you squinted this hard enough you you'll notice it and that that basically two different types um of resources here and um we we called those um there's the burst Limit and sustained limits and the best limits I think things where basically they they they cause costs or costs right at the moment that the the block the block is propagated right the bandwidth to to to to propagate a block the compute to actually verify it all of this the the the the the critical point there is that basically it has to be bounded um in order for blocks to still be propagated um in a timely manner um and in order for notes to be able to to to verify them at all right they might run out of resources and the sustained limits they don't matter so much block to block those are more things that accumulate over time so that state growth history grows these kind of things right like in a single block can't really make it like produce too much damage there but over time it just basically makes it more and more costly to to to to run a full node um as it turns out like if you if you look at this um there's some sort of of structure to this and you can you can actually reorder this a little bit and it turns out that usually there's a relatively good matching between like a specific burst Limit and a specific sustained limit so bandwidth and and history growth kind of they correspond right because the bigger block is the bigger like the more bandwidth you need to propagate but then also the more disk space you need to just you know keep it around forever for history purposes and similar with State access and state growth these kind of things um now specifically for 4844 right what we are introducing is this new type of data so uh kind of the the resources we're talking about here this this first kind of row so it's um it's on those burst Limit it's the bandwidth how how big can blocks get and then on the sustained limit it's just like how much resource do you need to store kind of the history of ethereum and and if you um uh if you put an engine and if you basically look into the AP a little bit you already know that like there is this this this limit um in terms of History growth we basically we introduced this new new um uh basically uh a mechanism where blobs are only stored for a single month and so this is basically why on the history course side we basically it it will it does mean that there will be some extra requirement for node operators um but it's quite bounded Because unless normal history that today is stored forever but even after this this nice erp444 basically even even after that it's still going to be stored for a year um blobs are only stored for a month so basically in terms of sustained limits it's it's not basically it it has like a very limited impact the more interesting and also more tricky side of this picture is the the burst limit so so bandwidth um and to kind of understand like what what the situation is and how 444 fits in we have to first remember that today on ethereum basically we only have a single gas price right whenever you send a transaction you don't actually specify how much bandwidth am I do I want to use how much compute how much memory you just spend said like one gas one gas limit and then also like how basically how how what kind of Base fee are you willing to pay for this right and it's all basically mapped down into what you think about it as like a single dimension for pricing and and that comes with um uh basically very real trade-offs in terms of a kind of resource efficiency so if you if you look at this kind of stylized picture of just looking at two different dimensions here there could be I don't know data and compute or data and memory or whatever right two different two different dimensions and basically the way the uh the the kind of the ethereum Gas Works today and that's purely for Simplicity right because it's very simple to for users to deal with one dimension basically but the way it works is basically that it is um basically that those two resources and compete for for for for usage in a block right so you could imagine um if you use if a block is very full of compute then there's very little room for to to put any data in it or the other way around and actually if you if you want to like open e-test ether scan for example they fight for every block for the detail page they actually give you the size of the blog and usually it's something like 50 kilobytes 100 kilobytes but like rarely more than that but if you look at what would a block look like if it was was like just full of call data which is where all the data comes from if you were we were all the way like say on the lower part of the diagram if recess B was was Data it could actually be up to one or two megabyte like well two megabytes basically um of of size right so so what that means is B basically determined in the past that two megabytes per block are kind of safe um and uh and the reasons would be there right it basically sitting there but an average block basically almost like you completely underutilizes data and that is again just just because it's simpler for us conceptually to price these things so most of the time we are like very far up up the slope there and where do we want to be like what would be like the most efficient way of handling resources well that would basically be be this picture so ideally you'd want to basically make these things be independently used consumable where you can basically consume the most amount of like the most the highest safe amount of data that we think you should be like the chain can can manage but then at the same time you should also be able to do you know do we still do state access to the biggest the highest amount possible or memory or whatever right there should not be this this kind of a competitive nature to it um and this is basically where four eight four four on the burst Limit side um gets gets it in efficiency right because full charting uh full length charting we'll hear about it a bit more um after this actually that's really clever things where people only sample the data so bandwidth constraints goes go quite down but for four eight for four there is no fancy trick right everyone still downloads all the data so it's very real bandwidth strain so the Innovation and the burst Limit side is purely trying to get to this upper right point trying to actually basically make it so that the existing resource we already have today is just more efficiently utilized and the way we do this is by going from as we're saying like right now today um pricing is one dimensional and so what what we introduced with point for voice basically we go 2D um and this is how that how that looks like um so this is this is an open PR right now it's not yet quite much but um you can have a look so like small details might still change but I think the general direction is is pretty sad and so the idea is we introduce what we call data gas and as you can kind of figure from the name it's not blobcast status the aspiration would be that like maybe in the future we can we can expand this to cover the entire data Dimension but for now it's it's only used for blobs and we we set it in a way where basically like one byte of of plop will cost one data gas um and this data gets importantly basically is completely independently priced for normal gas so it has its own 5059 style mechanism where and that's that's where basically where they use and I see Mary is not very happy about this because you know like he has to implement it and get it in of the day but this is really important for the EAP because other than that basically you wouldn't be able to get to this more efficient bandwidth usage so um what does it look like kind of how does how can you think about it well it's just you know similar to how 1559 already looks so the way to the this is the courtesy of Proto I stole the site and so every column here would be a separate slot so the first slot and in this case basically the the the target amount of blobs would be two the maximum allow it would be 400 block so the first block the first block comes in it has exactly two so nothing happens the next one has three right the red one it's basically one one two two many so the price would go up and then the next two kind of like are stable again and then then one misses the blob so the price goes back down so it's like a very you know like just like 1559 like you you you know and love it basically um it is a bit different or like basically it's it's under the hood it works a bit differently so here's kind of a bit more more look at the details here so first of all of course just we have the max data gas per block right just similar to 259 and the Target that that is half of that um transactions these block transactions they specify an additional Max fee per gate per data gas field so like how much are they willing maximally per data guess and to to to have their transaction included importantly you know that this introduced a little bit extra complexity for users but users in this case are not actually users those are like big Roll-Ups right so basically them having to specify one more value you know fine that shouldn't like if you can't do that maybe you shouldn't be in the world game basically and and so uh we just to to keep the complexity this year minimum though we did not uh opt for having a separate tip for this Dimension so we just reuse the the existing existing tip um and then we one thing that we where we deviate from 1509 a little bit in 1559 basically if the demand were to completely crash theoretically like one gas could could be could be I think valued as little as seven way which is just the minimum after which basically updates don't don't go lower anymore so the transactions would basically be free we don't quite want basically to make the the the lowest demand case of transactions here completely free so we set a minimum data gas price that's a that's kind of at least somewhat meaningful so that's like 10 to the minus five eighth per blob um so of course it's priced in late I guess but it comes down if you if you compute the for the cost of a full block to to the value and and the last thing and again this is very technical so like if if you just want to understand and conceptually this works don't don't care about this but but if you ever pull up the IP and you might stumble across this and you might be confused so so actually the way we we track this um in 1559 right now we we usually track the base fee directly and then we update it every every block and actually it turned out after we introduced it like looking at it it's slightly conceptually ugly because we always do these these kind of services basically there's some some properties in the upgrade updating we don't quite love it's it's a little path dependent and these kind of things so um so we moved to to just a conceptually simpler way of tracking for this Dimension where we track the excess excess data guess that has been basically been used over the existence of the EIP right so basically we just um we have some sort of Target that we want to be used and then every every block if it basically uses more than that we just add to this to this counter and then every block wave is basically uses less than that but we're still above zero in this counter we just reduce the counter uh yeah sure if you wanna foreign just like the base fee yeah so yeah yeah so this is one additional header field uh which good question actually also um uh I you can you can see that because I just wanted to give you like an impression of of what the kind of calculating the the the the the um cost looks like with this header field so um as you can see basically we have these kind of functions um if you want to to get the the feed that in third section actually has to pay it depends on on on on the head of the previous box similar to 1559 and so you first get the total data guess that the transaction consumes which is just you know data gas per blob times the number of blobs and then you calculate the basically the base fee but we don't call it basically because again there's no tips so it's kind of unnecessary to have the base fee tip distinction so we just call it data gas price um and so how would you basically for each block you once basically calculated its data gas price and you do that by um by basically taking in uh there's this excess um data guys and then we use this fake exponential function it's a little nice little tidbit I don't know maybe it's irrelevant but it's it's something time to talk about briefly So like um just because we want we want some so maybe I can already go to the next step to explain so basically this is kind of how the pricing develops it's it's like 1559 right so basically if you were to continue to just keep keep basically using up all the data space in a block not just the target it would basically be on an exponential curve and would be more and more and more expensive and you can see basically uh like a thousand a thousand excess plops that that's roughly I don't know something like 10 minutes or so so within 10 minutes you'd really like they have like super expensive blobs if it were to keep to keeping being fully used uh yeah Marius again uh that accumulate a lot of exercise um so the nice thing about this is that it's basically a pure function in um in [Music] excess data guess right so it doesn't really matter if it was accumulated at the beginning or at the end um there will probably be a different like in the beginning it will probably be relatively cheap to to use data guess because Roll-Ups are still kind of adopted in the process of adopting it so there's not that much that much demand so basically probably for the first first month or so we'd be like in the very zoomed in left part of this picture and then uh later on once once it's all basically fully adopted and people use it we'll be like a bit more towards the right in this picture but it's not like this is not a basically because it's so reactive it's it's similar to 59 so basically every block in it most they're doing 12.5 update so the difference here like basically you can come you can go from one of these paradigms to the other within five minutes of of uh of high usage or low usage blocks so it's not like it's not basically something where it matters immensely what and what what was done in the in the past basically a lot a high consumption in the past only means that like basically you have like five five minutes of of reduced Bob usage before you're back to your normal price level so it's not Yeah so basically there's no significant kind of accumulation effect or anything right sure no no but the thing is because so so the way that think about it is like because the the the the price is a pure function of the excess data guys so at any excess excess data guys I mean of course I put down excess plops just to think about it more easily but it's tracked in excess data I guess but once you reach something like say I don't know a thousand excess excess blobs that would mean that sending one block already costs 30 each and that doesn't matter whether the excess blobs were accumulated over one day or if they were accumulated over a year so basically once they access the data gas field reaches that value it would cost 30 years per block to send blob so we would expect of course like if robes are not willing to pay that much for blobs right so if for some weird reason there was some spike in demand and the excess would shoot up to that level it would quickly come back down and stay at some some kind of permanent level so the excess is not something that will continue to grow over time it'll just similar to the base fee the basically doesn't grow over time it just Finds Its equilibrium value and of course sometimes goes up and sometimes goes down temporarily but it hovers around some sort of you know 10 to 100 gray level ish and similarly it takes us blobs because that can can go back down right if a block uses less than a Target the number goes back down so it will just find some sort of equilibrium value that corresponds to to to um some sort of equilibrium price um and it'll just have around that um yeah I'll just keep for continued questions so anyway so basically this is how we how we make a flight to forward history growth not a big deal bandwidth we really need to put in work to make this work and this is kind of where the core innovation of the Erp for Now lies other than that it's what's compatible for full length sharding but for now this 2D fee Market is really why why we can do this and why we basically just utilize existing ethereum resources more efficiently and with that I think we are done with the the kind of the fourth for four part of today and we can move to full dang shading do we but that's not me talking about it so someone is missing up here I assume the man himself so let's see okay um yeah um cool so I will be taking on the math again so I don't know how much more capacity you have for that um sorry for that um uh uh kcg is what that should say but should we can reorder the names as well um um so okay I want to introduce um uh now to the two-dimensional kcg scheme which we will need um for full sharding sorry this is a big jump okay so when we do full sharding um why do we not take all the data that we want to encode and put it into one big kcd commitment um and the reason uh for that is that uh that is going to require a super node like some powerful node that you probably can't easily run at home unless you like um have a very good intensive connection and want to invest some money into it um so you will need this both to construct blocks where we're probably like kind of okay with that um but we will also need it to reconstruct the data in case there is a failure and this is an assumption that we want to avoid for validity So like um it's kind of um more acceptable if a failure leads to just not being able to contact blocks or maybe we have to make smaller blocks or we have to um make blocks without chartered data um but it would be really bad if the absence of the supernode could lead to the to a network split where some people think data is available and some people think it's not available this is what we want to avoid so what we want is a construction where yes like there will be a lot of data in the network and maybe like someone needs to be there specialized into Distributing that data but once they've done their job the very decentralized network of maybe your Raspberry Pi is at home can guarantee that it will always converge it will always be safe and so on okay um so uh what what if we um what if we just use um many many different uh kcg commitments just a list of KCT commitments um so if we do this naively we just take many commitments and we sample from each then we'll need a lot of samples because we before I had this number of samples for example say 30 samples now we need 30 samples per commitment okay that's that would be a lot of samples um but there's another much cooler way of doing this where we use read Solomon codes again and we will extend M commitments for like M actual payload blobs uh and extend them to 2m commitments so here's how this is going to work so we have our original data commitments um in this case um three commitments and what we'll do is we'll Define another four commitments that are an extension of these commitments um so they will be completely determined by the actual data commitments uh and yeah so here here's the math of how this works so what we'll do is we'll Define a two-dimensional polynomial um for the data and it works the same way as before so basically we will interpolate this polynomial we will Define it by this data region the original data that if that comes from many different transactions that include Charlotte data and what we'll say is for Simplicity I'll just take the row K will just be the evaluation of this polynomial where we set y equals to the number of this row y equals k so we evaluate the the polynomial at K and then we get a one-dimensional polynomial right so we get F K of x equals to this and like you can pull up all of this together and what you get is again an expression just in the these powers of x and then we can commit to those polynomials in our normal kcg way okay so we have FK of s equals to this now we replace the X by S and some complicated sum in there but overall we'll have like one elliptic curve element this black box evaluation and we call this C of K okay now the cool thing is if you look at this expression as a function of K then this is also polynomial right it's just a sum of terms uh of powers of K okay so this is very cool and what this means is that our commitments themselves um will be on a polynomial so if we see the commitments which are now elliptic curve points as a function of K they are on a polynomial so what we have is before we started with having each row being a polynomial that we commit to we also have that each row I mean this is a property of just a two-dimensional polynomial each column will be a polynomial but also the commitments themselves our upon are a polynomial in this case of degree three uh yeah because they're determined by these four commitments um so what we have is we'll the how the 2D commitment scheme will work as we'll have two uh 2m uh row commitments and um we can actually verify that this is the cool thing like a any anyone who validates these commitments can easily verify um uh that uh that they are on this polynomial using a random evaluation trig again which I introduced earlier so what do you what we'll do is we'll take the first M commitments divide them at a random point and we'll do the same for the second M commitments and if these two result in the same uh Point actually the point will be in this case an elliptic curve point then they are actually on a polynomial of degree n minus one for those who are interested there is a way to do something very similar um using uh 2D commitment so you can do one commitment to the whole thing but I won't go into the details here but um there are basically some downsides but which is why we're not choosing that way and so what what's what why are we doing this okay so like we have properties that we already know we can verify all samples directly against commitments um there are no fraud proofs required um but now we need a constant number of samples for all these commitments in order to get probabilistic data availability um and basically we get the property that if at least 75 of those samples are available then all the data is available and it can be reconstructed and that's the cool thing from validators or other and also only observe rows and columns so there's nobody nobody in the system will ever need or will be necessary I'm sure they will exist but it's not necessary that anyone watches the full square of samples in order to get these convergence properties um so what you'll notice is that this number is a bit higher than before so like if we only have one commitment um then we only need 50 of the samples to be available for the square we need 75 percent um so the number of samples you need to get this will be a bit higher cool and so um what we get with this um is that um I made a proposal I mean this is all still in discussions but like one of the ideas how how we could uh extend this to a full trading construction is that um basically the way validators um uh useless construction is that they will download um rows and columns they will each choose to run randomly of each and then what we get is um that if uh a block is unavailable it can't get more than 1 16th of attestations so automatically the consensus will never vote to unavailable blocks um and um and at the same time they can use these full rows and columns that they don't load um to reconstruct any incomplete rows or columns so if any samples are missing they can reconstruct this and because there will be some interactions like for each validator there will be if they do too like there will be these four intersections and they can see the orthogonal rows and columns with the samples that may be missing and so like as an example um I made a computation that basically with about 55 000 online validators you get a guaranteed reconstruction where basically every sample will always be reconstructed if like we initially had enough data available to do this um and the state in practice this number will be much smaller because most nodes don't run one valid data but uh tens and some even hundreds and data availability sampling yes basically just uh checking um like random uh samples on a square and what we want is again we want to get um that the probability that an unavailable block passes is less than 2 to the minus 30 and if you do the math you find that you need about 75 random samples to do that and so the bandwidth to do that in this example if we do 512 um byte samples would be 2.5 kilobytes per second which is really nice low number cool okay handing it to Denny [Applause] thank you I need this okay so there's a lot of math and there's an elegant construction assuming uh that we can do a constant time amount of work for a large amount of data to kind of layer it into it as similar to like a validity condition on our on our block tree we don't consider invalid blocks in our block tree we don't consider unavailable blocks in our block tree and so the math and the construction are very elegant but when the rubber meets the road um datability sampling on the networking layer is actually non-trivial problem that's the wrong way oh the arrow that goes right is so worn it doesn't look like an arrow anymore um okay so kind of stepping back why is this why are we making this problem hard for ourselves um everyone's seen this these things are it's not fundamental that they um cannot come together scalability security and decentralization all in one system but it is hard um and it's hard primarily because we want home nodes to be able to run we want standard computers to be able to validate the system uh to kind of have uh security and aggregate even against a malicious majority of our consensus participants again kind of in that validity condition of if there's an invalid block and all the validators or miners are saying that's a that's that what is that's what the head is you say well that's not even literally real because it is invalid um and so users in power uh kind of Define what the network is similarly we want to do that with um our bandwidth consideration with respect to um data availability so thus we need to focus on the bandwidth here um a lot of this is a quick recap we've been talking about this all day but we need to scale execution we need to scale data availability essentially Roll-Ups give us some sort of like compression algorithm for the execution of transactions whether it be from fraud proofs or validity proofs data availability we use disability sampling or we want to um data availability we've been talking about all day means no network adjustment including excluding super majority of full nodes has the ability to withhold data again this this it kind of makes data availability of validity condition um it is already today as we noted you have to download full blocks but once it's a lot of data and we want those home nodes it becomes very hard um right so again we want the amount of work to not really scale as those blocks become very large to scale the network so data availability Insurance the data is not withheld also Assurance the data was published real quick shout out dockerd made most of these slides of another talk and I'm just reusing them um important to note it's not data storage it's not continued availability there's a debate as to how long the network needs to have the data available so that people can check that it was made available some people say on the order of where are we at like 100 seconds some people say two weeks um you know it kind of depends on the use case and and it's a bit of a more of a ux debate it's kind of the online in this requirement of people to be able to get this security guarantee without trusting you know someone else so um is it important I don't think we need to get into this too much optimistic Roll-Ups and ZK Roll-Ups it's critically important and you know who knows the utility of uh solving this problem might extend beyond these two types of systems so networking and everything's hard and we probably are making it even harder on ourselves by some of our assumptions here so we could say okay we want we certainly want to make sure that block producer and consensus nodes we want to be able to um not be fooled by a malicious majority um but maybe we have a neutral PDP Network and we can just assume that P2P network is like healthy and gives us what we want this is certainly attractive it ensures that each node really can see that they get the statistical security but if we're assuming that the validators can be malicious it's very high amount of them at least you know maybe about two-thirds some people like to say 99 depends on probably the construction on what the real one is um then the Assumption then that the network network is neutral is probably not a realistic assumption so well maybe it's realistic in most scenarios but if we want to really be able to harden against that majority adversary we need to be thinking about um an attacker controlled P2P Network by some threshold defining whatever that is um again this is a lot of um kind of exposition of the problem rather than total Solutions of the problem so you know if I'm thinking about designing data availability sampling um I'm probably it's probably interesting to think about what's a good neutral Network solution but then I think when the rubber meets the road we need to think about what thresholds uh can we actually Harden against a very attacker-controlled PDP Network um so in this model certainly some nodes can be fooled and so it ends up being a collective guarantee again depending on the thresholds and how the system is tuned but rather than no node can be fooled it's probably going to end up looking like no above certain threshold of node can be fooled maybe for a certain period of time maybe until the network kind of resolves itself um but so this is likely correct model but it does make the problem harder so the B2B problem what are we trying to do here we want this like P2P distributed data structure that can reliably serve samples so that people can do their job of getting the samples we won low overhead on nodes from multiple perspectives one on nodes that are participating in pulling down samples but also potentially we want to leverage nodes that are not just validators not just builders in this distributed P2P structure so we want to also consider the the overhead of these nodes that are participating in the serving of the samples as well or in the dissemination of the samples other things I want to be robust against attacks I think one of the really really scary things here is liveness attacks um doses civil attacks Etc that happen on the network layer because if a majority of nodes are seeing data as unavailable either temporarily or permanently then they cannot follow the chain at all again we want this to be essentially a validity condition you know if there's an invalid transaction in this Branch I don't follow the branch if that branch is unavailable I don't follow the branch so that is a very important critical requirement but a very terrifying requirement meaning that like it is very important that this these PDP structures um are hardened and do we do understand kind of their failure modes we understand where they where they operate and we do understand how they um resolve maybe after an attack um and low latency on the order of seconds I have a um page of some deciderado I'm going to get into in a second um and there's some distinct challenges I think when you're kind of thinking about this problem dissemination into the P2P structure we have a lot of data how do you efficiently get it into this P2P structure without causing High load on the on the individual nodes of the PHP structure so if every node only needs you know 1 100th of the data but they had to touch 50 of the data to get it disseminating the structure it's we're kind of missing something there um similarly we want to support queries we've disseminated data sample for x amount of time which I can get into this Serrata again and validators certainly with their row and column kind of crypto economic Duty can identify and reconstruct missing data but we also probably want to consider should this PDP structure be able to identify and reconstruct missing data so there's two kinds of um potential reconstruction that we might want so validators are very incentivized out the gate you know if things are missing from the rows and columns to incentivize to to repair patch and make make things whole but if say the p2b structure is supposed to serve data availability sampling for one week then um are those validators the same people that will then identify and reconstruct missing data or is there some other more distributed and less timely required method to do so there's a handful of actors involved in datability sampling Francesca is going to talk about Builders and where they fit into kind of the consensus protocol but they're kind of the original source of the data they're highly incentivized to get it out but they're probably not one that you'd want to rely on uh in perpetuity validators highly these are you know crypto economically incentivized actors that we can try to leverage in this construction they do have the rows and columns they do also perform data availability sampling like a user node and then we have users users perform data availability sampling hopefully they can be leveraged in serving and making the whole P2P also more resilient itself some quick deciderata right now um you know if I were thinking about building datability sampling if I'm researching and uh doing stuff I'm these are kind of some Target numbers but I would also be sweeping these numbers and understanding where they uh where they work and where they don't so data size 32 megabytes per block that's per 12 seconds um or if the slot time were adjusted it might be per some other amount of seconds call it 16 or 20. um but with the 2D Erasure coding that ends up being 128 megabytes of data being inseminated into the network chunks I think we there's chunks and we sample the chunks or their samples and we sample the samples um but on the order of 250 000 you can make these larger but then you end up with you still need the same constant number of samples so you end up with more overhead samples he said 75 something on that order but essentially we want to drive that probability down um as we're doing the sampling latency validators really uh right now need to make decisions about what they see is the valid and available head on the order of four seconds that could be tuned depending on the constructions available to us but they if they could not regularly be able to do data availability sampling um then on the order four seconds we have a problem users you could have a potentially lack more LAX requirement on the order of 12 seconds on the order of a slot or you could even consider maybe they need to be doing it on the order of epochs and optimistically following the head as available and maybe there's some play in in the constructions there validator nodes 100K is pretty optimistic but we probably open the order of four thousand today so something on that order is kind of the the the Baseline and then user nodes on the order of 10 years especially if you start adding light lighter weight nodes with statelessness and like clients that might want to participate in this datability sampling um you know 100K to a million user notes you know so it's really if the user nodes cannot participate in the serving of samples then the load on if we only relied on say incentivized actors like validators then the load would actually scale as the the to serve as the user nodes serves so it's probably very important um to tie them into the data structure itself bandwidth assumption I don't know it's probably worth discussing the eth.org website suggests a minimum of 10 megabytes per seconds around a full node uh but but for good whatever 25 megabytes per second I don't know who came with that number maybe it's a good place to start the conversation and then persistence obviously like I said datability sampling is not for persistence it's to ensure the data was made available where how but you know data if data was made available for half a second like No One's Gonna necessarily be able to prove that to themselves that it was made available or a very small subset so is it two epochs is it two weeks um there's much debate here onsgar I think what was your recent number is he still here okay okay 10 minutes an hour whereas I think some are more like a week two weeks um and those that actually changes the requirements on nodes especially in terms of storage um my intuition here is that the online in this requirement for users that want to get their you know State transition changes from ZK Roll-Ups or policeman users that want to submit fraud proofs for um ZK for optimistic Roll-Ups you know this dictates their online in this requirement and so I'm I'd be like 10 min oh man I gotta get out of here um Okay cool so debate an hour seems short um P2P designs so uh one easy thing you could do is just say there's a bunch of super nodes in the network and if you connect to them you do dis and if they give you the samples that you want then things are available um this is I believe Celestia's current design although that statement I could claim is true a few months ago I'm not sure today um and you could potentially do something in similar ethereum whereas maybe instead of a uh each node meaning you have everything you could leverage uh ethereum validators the rows and columns that they custodied and it looks kind of similar um this is is nice um you know if you connect to one on a super node uh then you get what you need um but this doesn't really fit well under the node model especially if validators you know a node that's running on the order one two maybe three validators should be able to run on the order of you know home resources which is definitely not the case dhgs they all of a sudden DHT is a nice way to distribute data and attributed data structure across the network it's a nice way to find data and seems intuitively like a very good direction a very good start it fits really well in because each of these nodes can have very small amount of data and really nice scalability as you add more nodes to network you can depending on your redundancy Factor you can have you know similar or less data per node uh prone to lightness attacks it's really easy to assemble this thing uh naively uh you just make note IDs you fill the tables and if you're a malicious node you can just return uh entries from your table that are full of malicious nodes and one thing that's I think very promising is looking at secured dhts docker's been digging into Academia and I believe there may be some others in this room that have looked at some other papers about hardened dhcs and we do have you know we as long as you have a simple resistance set then you all of a sudden can have certain guarantees in these constructions so you can Leverage The validator set or maybe other types of crypto economic sets uh to have hardened dhts um so you could use standard open DHD for average case performance and maybe a secondary fallback DHT um leveraging the validator set for uh in case of attack you could also yeah there's some weirdness because then all of a sudden you're assuming that you have a certain amount of honest validators for this um so does that suffice under the malicious majority construction sure you can probably tune the numbers but you could also potentially layer other types of crypto economic sets um you know proof of humanity uh Spruce ID whatever the hell all sorts of stuff and could have layered dhts where they're ultimately just kind of fallbacks in the event that the the big main DHT starts failing um validator privacy and optionality and how they construct their node setups is probably very important I'm definitely over time okay cool great [Applause] oh and there are actually a handful of people in this room that have grants from the EF to dig into r d around data availability sampling if you are interested in this problem um there's a critical problem to solve and work on over the next 12 months uh talk to me or email grants program at the EF or something thank you hi I'm Francesco and I'll cover the last bit of this very large topic that we've kind of gone over today it's purpose of bullish operation expect probably most people will be somewhat familiar with the concept but this will be kind of a light introduction like it's not going to be um yeah it's not going to be too advanced it's going to be just for you to get a picture of how does it fit with dunk sharding and what does it have to do with it in general and also like uh kind of how does the roadmap of that fitting in the protocol look like um yeah so first of all what is PBS um it's oh sorry yeah so there's a let's start from the pieces uh we have DB and um NS so first of all uh block building the B uh is essentially this task of actually creating and distributing uh execution payloads mainly so we have Beacon blocks but then inside them there's execution payload which is the kind of the valuable part in some sense the part that actually changes the state uh in of the execution layer and this is the part that is kind of uh requires some specialization to deal with whereas the the beacon block part is more of a consensus part um and yeah so this is um the normally today we only think about the creating part like only basically putting together a new execution payload but uh the distribution part will also become critical uh especially in the well in the context of dunk charting um and yeah and also uh this uh distribution so the distribution will involve the data that is committed to uh which is going to be eventually very large so that's why it's it's kind of an important task eventually and so for these reasons and well later it will get a bit more into them it's a quite specialized activity that we don't really want normal validators to do because it would kind of increase their requirements too much for our for us to be comfortable with um and then there's proposing so this is just um you could think of today proposing includes both things both this kind of consensus part of making a beacon block and including all the consensus messages in it attestations and other things like slashing messages or anything that's kind of critical to the good function of the beacon chain um but then also the putting an execution payload in it so today it's still possible for anyone to do this by themselves and kind of have both the roles together but if we kind of ignore this execution payload part this is really not a particularly specialized role and we think that it's always going to be possible to or we really want this to always be possible with low requirements uh basically what we expect today validator to have um and yeah the separation is just that these two things are split up like we don't um the default it would not be any more that uh a validator does both things or the proposer which is a validator does both things but that uh the proposal does the beacon block relevant part the consensus messages uh part and uh some other kind of specialized actor comes in with the execution payload and the distribution of the data eventually um and yeah so why do we want to do this I've kind of already well hinted at it but yeah it's it's simply that if we Outsource the specialized stuff we can keep the simple stuff um basically decentralized we can keep the really consensus critical things um essentially done by a very decentralized validator set um which is a really important goal in ethereum in general so and I mean practically why you know what are these things that we want to Outsource um so that we've for all the whole day we've been talking about dunk sharding and um it's not there's nothing really I guess fundamental about starting that requires um this Outsourcing you could imagine other models I mean the I guess original starting model before the Dank part uh didn't require this Outsourcing um but it's really like a major simplification and um so I mean not just simplification also as I think like consequences for latency like um it just makes the it gives us this really tight coupling between uh the execution uh payload the blobs and kind of um yeah just streamlines the whole process um and so we in we're done charting if we do want these simplifications we kind of have to uh we start having something to Outsource because the proposer has to compute these commitments really quickly which is uh not easy to do for uh like normal hardware and also like probably the most prohibitive part is the uh basically distribution of the data to the network so that would require like really uh kind of uh not uh acceptable uh Upstream requirements for for validators like more than uh you know probably multiple gigabits um you know and so yeah we don't want we don't want to require this it's like or there's a magnitude more than what uh someone would need today um because basically the most you might need to distribute is 128 megabytes uh per block um and uh yeah but again this is not a kind of fundamental reason if there was no other reason that uh we needed the separation for we might be a bit more skeptical about dunkshardt we might think well you know we don't need these other actors why are we introducing the system just to get the simplification that's not kind of the ethos of ethereum like we really want everything to be as decentralized as possible as like resilient as possible these actors probably you know do introduce some complexities in this Vision but the issue is dunk charting isn't the reason why we introduce these actors the reason is med um and this kind of fundamental reason there's I don't think anyone that has looked into me uh enough thinks that there there's any other uh way essentially to go um and the issues is simply that as I said these execution payloads are really valuable and uh extracting value from them is a really sophisticated activity from many points of view algorithmic uh infrastructure like requires uh potentially very good Hardware very a very good connection like latency is really important so there's like all kinds of reasons oh and also like uh access to order flow so um you know today we can think that order flow is more or less so you know essentially access to mempool transactions is more or less available um to everyone publicly but that's it seems very naive to assume that that's going to be the case in the future and already it's not quite true that that's the case so maybe they're always going to be a public mempool for censorship resistance reasons for I mean other reasons but it's really naive to think that everyone is gonna have access to the same kind of raw material to build blocks like the transactions and this access order flow is is a huge part of being able to create valuable payloads so there's like all kinds of reasons why it's just not realistic to think that validators will be able to uh profitably make their own blocks and so there's this really like strong centralization pressures if we essentially don't provide them a way to do it you just go and you know have someone else to do it well which is the whole point of Separation but there's different ways in which it could happen some ways in which it could happen are for example just everyone's taking with pools because that that's the only way that they can extract value um although that's actually kind of not already it seems like a scenario that in some sense maybe we can avoid we already have PBS today like we usually say PBS and we mean um basically empirical PPS so where the protocol kind of knows about the separation like has a concept of a builder and in some sense like negotiates this um Outsourcing but today we basically FPS is just not in protocol it's called a map boost maybe probably a lot of you know it and essentially what it does is it introduces a trusted third party in between a builder and proposer which are these relayers um I don't think I have time to like go into the details of it but essentially you know we don't we want Builders to not trust proposers we want proposers to not trust Builders there's reasons for that um and yeah we just basically put like a trust to the third party in the middle which kind of negotiates the uh The Exchange um so you know the proposal wants something they're built from the Builder the Builder wants to get something to the proposer the third party makes sure that the exchange happens uh you know we where none of the two parties can cheat each other essentially and so this already exists today a lot of ethereum blocks are built in this way um so it's it's the reality that and it's not something that you know um the ethereum community I'm kind of made well it is something that digital Community made happen but it's in some sense inevitable like anyone could always build some infrastructure of this kind and people could use it if it's more profitable for them um so yeah so you know we already have this why do we care about potentially putting uh this separation protocol um so as I said relays are trusted to the parties we don't usually like to have these sort of entities in the protocol they're not critical in some sense well if things are set up properly um which I mean I think there's a lot of improvements to be done on the infrastructure that exists today it's very um you know young infrastructure um but either way there's always going to be some um kind of failure modes that we don't really like or some some requirements that we don't really like from having this um these parties so one is that you have to basically white list them because they're trusted so everyone has to kind of go and configure some list of these entities that they they're fine with essentially the distrust um and we don't care if Builders do that but we don't really like validators to do that um or well I don't know that's debatable but anyway um there's I think there's a future for relays to still exist and just have a full fall back in protocol that is not the default but that's a conversation from our time um but yeah another thing is that today we don't really have a kind of live monitoring for relays uh like locally people don't have a chance to uh observe interactions that really have had um with with other proposers and then disconnect for them if these interactions look suspicious essentially so that's something that we can include we can basically really improve the um you know the the resilience of this whole system because we can consider that people don't need to you know go on Twitter and find out oh this relay is malicious I'm going to disconnect from them but just maybe this can happen locally essentially um so this is there's a lot of improvement there um but still uh there's some kind of um I guess really um fundamental uh catastrophic scenario that that seems unavoidable to me if we keep having uh or rather if we only rely on these entities for this Outsourcing if we have kind of no fallback um so especially um we're done charting so today you can always have a fallback actually it's not so fundamental to this the state of things today you could always have this fallback which is uh essentially the characteristic scenarios like all relays that uh most people are connected to fail for some whatever reason they're malicious or they're attacked Anything Could Happen um they fail and now all of a sudden today it's fine you could you know once you manage to disconnect because you realize okay these people haven't given me blocks for you know however many times I've tried or if you have this monitoring system um that's fine you just fall back to you you building your own you know gather or whatever like other execution the clients you're running building your own blocks so now we're likeness is not really threatened maybe it's like a temporary thing um but with dunk sharding and also statelessness in some sense uh if you know let's say all the validators are still list they cannot build their own blocks um or with unchart in like you cannot distribute data then this becomes like a threat to know our liveness we dangsha did not exactly it's like you could make blocks you just cannot put a lot of data into them but you could argue well is that really liveness like if all the rollups can stop because they don't have access to data anymore that is not really what we want um yeah so this is maybe what it would well this is like the one of the current ideas of what it could look like to put it in protocol um I think well yeah I think it probably can't really go into it uh I don't think we have time to go into it in much detail but basically it looks like you know as I said before what are the relays they're just these kind of actors that negotiate the The Exchange um you know what do we do we want to remove these actors we basically have the protocol negotiate exchange and the protocol in this case is basically other validators so there's a proposer there's a builder and we have the whole rest of the validator set or some committee um more well likely um that basically kind of makes sure with their well they observe the exchange and with their attestations they sort of make sure that if the proposal tries to shoot the Builder um they they fail and vice versa essentially so it essentially gives us the property for example that if the proposer accepts some block and and or some some like a bid you could say from from a builder and we have good latency like things are fine from a fortress perspective from a network perspective then the proposal will get paid it doesn't matter what the Builder does if they reveal their block a good kind of this is a good case if they don't reveal their block they're really late you know tough luck for them they're going to repeat the validator and not even get their uh building opportunity and so this is one design there's this other design which is kind of interesting oh yeah also uh thanks to vitalik for all of the things that just took him from many of his uh research posts um but yeah like so this is basically you could say empirical map boost because it's really like um designed to look like my Boost again we have basically this like party in the middle this time more clearly than before um which is in this case a committee it also was before but anyway and this kind of um party again negotiates negotiates The Exchange we could think of the party as basically uh an availability Oracle so it's um basically its job is to ensure it's to give guarantees to the proposer that what the Builder sent is available so the proposal will accept um a header like a basically offer of you know I want to give you this blog pay you this much and the Builder will uh essentially erase your code so you know hopefully if you've followed the discussion you know whether each recording is by now um the um essentially the execution payload to the committee um like essentially encrypt well Azure code then encrypt and then basically split the parts to the committee so that if some threshold of the committee is honest and online uh they will be able to decrypt even if not all of the committee is um and basically the committee uh signs you know essentially the individual members of the committee will attest to the fact that they have their part so that if you see enough attestations and the committee is officially honest then you know that as a proposer that this thing will be able to be decrypted and uh you know the data will be there essentially um so it actually yeah this kind of fits in quite nicely with these data availability discussions like that is really the problem here that um the purposes accepting a bit but the Builder doesn't want to say what the bid is because that's the are kind of private like secret information and we want basically some guarantee that even if you don't know what it is it is going to be there once the time comes essentially um like once you've accepted a bid and it's ready to go in the chain um so that's yeah that's what it looks like looks like um and uh just last quickly I want to comment on um basically well so there's like sensory resistance questions about PBS um and I think they're not you know they're like fairly well understood there's a there's clearly like a way that PBS uh in a lot of protocol says it doesn't really depend this is like you know our questions also today um it does the great censorship resistance um but that we already know kind of how to deal with that there's this concept inclusion list there's like slight tweaks to that there's I mean there's like basically really wide design space of very like roughly said ways for validators or proposers but you could just say validators to basically uh make sure that transaction that should go in the chain eventually get in the chain even if Builders don't want that and this also by the way like a really important reason why we want decentralization of the validator set because if you don't have that then you just don't have this option like if you have 100 validators and they don't want some video on the Chain that's it there's no way well I mean there's ways like soft working or you know other reason other ways but there's no kind of automatic way to to do that whereas with a decentralized by leadership we can always do that um and yeah so inclusion lists are quite simple in some sense and there's like disagreements about how exactly they should work but they're super simple today so if we have like the property that it is easy for a validator to say this transaction is uh available and this transaction is valid um so the validity part becomes a bit harder with account abstraction so there's some questions there but it won't go into that that's not really relevant here uh the availability part that becomes a bit harder with dunk sharding because now all of a sudden you know there's all this like all these blobs floating around the network there's all this data that you're not supposed to uh no they're not supposed to essentially download all of um you're only supposed to sample what actually ends up uh okay yeah well I've just finished this phrase and then I guess that's it uh but yeah basically um with uh yeah so we're done charting the terminal availability becomes a bit harder so we would like to have uh some kind of Chardon mempool construction so that you can even for things that have not been included in a in a block yet you can still in some way determine that they're available uh without everyone having to download everything essentially um and at the point at this point uh and this might not need to be the default route that all transaction goes through and probably won't um for kind of some of the reasons that I've already hinted at before like it's it's unreasonable to expect that everything will go through a public mempool but this is kind of the fallback for censorship resistance always and so we want to basically have some kind of construction like this um and I think that's it we're out of time hopefully maybe we still have time for some questions for everyone um but otherwise that's it [Applause] okay so yeah obviously out of time but uh we can still use this room for another 20 minutes we have a special guest here of italics here to answer some questions around of it ah okay any questions thank you oh okay there's a question hello thank you how do you approach them the topic of multi-rely in this time sharing black ecosystem because there are many solutions because I heard in PBS but that it weakens them the topic of censorship but how do you how do you approach the um to improve the mempool with um multiple multiple relays so relays are a Concepts that exists in like math boost kind of out of protocol PBS right like it's not a concept that exists in in protocol PBS so the one right so the long-term solution is to effect to not need to rely on them hi the Erasure coding in show me a secret sharing scheme seemed very related they are they're the exact same math okay is is Network persistent uh for the blob is going to be dependent on finality because I would have expected this to be the case and therefore rule out completely these Notions of having them for only five minutes what do you mean by dependent on finality like that if we're not finalizing then we need to keep the blobs for longer no well oh I see it well so like if you're in the middle of an inactivity League that probably makes sense I mean I think like I personally favor blobs being around for long enough like you know I'm on like at least a month or so so that you know any like it's longer than any realistic inactivity a week that would happen but there's different approaches okay uh just on the setup was there like a consideration or is it even possible or what's the problem with actually making that like a separate system it reminds me a bit to like swarm as it was integrated into ethereum notes like wouldn't it be possible with pre-compiles and like the right new evmob codes actually make that an independent system or so the problem the reason why we need like data availability sampling in consensus and why it's like so different from you know ipfs and everything out there is because we want to have like actually have consensus on the fact that the data is available right like the yeah ipfs does not provide that right and there's ways to like upload files so that some people think it's available and other people think it's not available and for like regular file publishing that's fine because if it's if a file is half available you just publish it again but for Roll-Ups like it you need like exact like Global agreements on which data was published um on time in which data was not published on time because the Roll-Ups like in order to figure out the current state of our rollup you need to figure out like which data blobs to include and which ones to skip over yes and tightly coupled with the chain okay next question hi for the sort of data uh blob storing period are there any thoughts about like challenges for valve leaders that kind of keep the data or is that purely altruistic Behavior I mean there have been proof of class study designs that we've worked on over the years I think it's kind of it's on the sort of rhetorical back burner because we just like know that these techniques exist and like we know that when the time comes you know we can probably just stick them in to get extra security um yeah so I wanted to ask about the multi-dimensional fee Market quickly um we talked about this excess data gas field I was just wondering if uh the same would like in an Ideal World if we hadn't already done eip1559 with the same construction uh be wanted for the like original kind of gas yes yeah and but this can happen simply just could this happen yeah if you own 559 could be upgraded to that over time okay cool I think they even already thoughts in their directions I think over long over the medium term we would want to and one of the nice side benefits it would give us that it also makes other improvements to to a 50 59 like mechanism easier like for example like a Time based instead of a block-based um kind of throughput targeting so yeah we would probably want to homogenize this over time yeah so um I could be wrong in one of these assumptions but my understanding is that proposer Builder separation was motivated largely by the um centralizing effects of Mev and US wanting to keep the proposer set decentralized um but then kind of later with these designs like full sharding we realized that we could utilize the builders as kind of um with extra Hardware requirements because they'd be incentivized with the Mev that they're extracting to have these nodes but then there's also Research into completely mitigating Mev uh okay so maybe that's the wrong example okay there's multiple strands of research and some of them are definitely sort of covering for each other in case the other fails but and some of them are complementary right so there's a PBS which will allows proposers and validators to be more decentralized but at the cost of kind of Shifting that centralization to builders there's a separate strand of research on the topic of trying to make build like Builders themselves a decentralized internally um so like a some kind of protocol would plug into the market and make bids instead of being a single actor and then there's also research on making applications that are Mev minimized so like all three of those exist okay I guess the question is just simply like if we mitigate or very minimize Med then goes away it just means we do as much as possible to reduce it but there's no actually again like I think anyone that's thought about maybe for some time will come to the conclusion that it's just not possible to assume that there's not going to be an incentive to be a specialized actor like there's even if all transactions are encrypted there's always going to be some reason to be the first to to touch uh this state like you know so the idea is they'll still be incentivized to run these nodes yes there's always going to be I think a lot of money to be made by uh controlling a block I don't think that if ethereum is a platform with value flowing essentially right and just to brief mentioned it's not just dunk sharding where basically a PBS like architecture would help so once we move with vocal trees to some through a world where it's uh easier to run stateless nodes and then also with what PBS would get us would be that like that normal validators would basically all of a sudden have like way way um lower um storage requirements and we only we don't don't get that if they still have to create blocks because then they need the state but if you only actually validate but you don't create your own box you you get you you leave that to to a specialized energy then you can um as a validator turn stateless and that's kind of the one more of these benefits we would get out of this wouldn't it make sense to like uh charge for sampling or something like that to like motivate people to have the data and be able to collect the charges I mean I think that's definitely a possible construction you could like uh see a way like where you oops a specialized sample provider and you pay them um I think like the downside is that it makes it much harder to run a node because now you need to somehow set up this payment infrastructure so um I think it's not ideal that's possible hello will there be ways to um foreign for somebody who wants to make data available to provide a proof through a smart contract as well uh which is independent of this call data layer tools and so on which is a smart contract specific like it's a generic infrastructure for proving that the data was available I mean that's that's what this construction does okay like this is part of like you there will be a type of transaction that is called with the kgt commitment and with the guarantee that this data is available and then also being um does it need to be like a special opportunity to to run the proof then to will uh to check that it was actually provided because the data was like if you get that commitment it was provided full stop but if there's no extra check necessary yeah that it was provided but then if uh let's say somebody wants to have a check inside solidity if the data was available but it's a parameter it's just like you get this commitment and you know it's there it's like extra data check that the data is available behind a commitment you use a history proof to prove that document was any transactions okay questions so one way to understand the data available exemplary Network can we fix it basically interpret it as kind of like a dedicated ipf Fest but with the samples that distribute in the network and the validator samples and but definitely ipfs has ipfs is a very bad way to think about it yeah because what we're doing is not storage it's proof that data was not withheld um yes but I just like um kind of like for the network perspective like um yeah and I know ipfest is wouldn't want to go to civil attack yeah that's something that is going to be addressed I mean I think yeah from from the kind of network perspective of how the thing should be yeah implemented like this thing has much higher requirements in terms of Byzantine fault tolerance and in terms of real-time access and like real-time real-time being able to change what you're accessing yeah those are probably the biggest differences in requirements okay great next question there foreign it's been very fun thank you next thank you um something I was thinking about is that if so we um we assume that that movie exists and that people want to um you know sandwich other people's transactions and stuff like this so we we have this proposal Builder separation which makes a lot of sense and then we once we have this we kind of start to utilize it to do heavier work like thanks sharding and things like this one of the things I'd be concerned about is that um presently we have the ability for users to just simply not run Mev and they just let the transactions come in as they will and they you know lose a bit of money but they're kind of genuine people I'd just be interested to make sure that we don't rule out this person and we don't kind of glue together the role of like making really specialized fancy um sandwiching blocks and then also doing all of the tank shouting stuff I think it'd be nice if we can make sure that we keep a space for the home home user to continue to pack their own transactions and just be like you know a nice guy okay yeah yeah I know well no this is um actually one of those things that I yeah I think I wrote a new research post about last week right like basically yeah like can we uh push the autonomy in choosing a block contents uh back to the proposer and like that's a spectrum that potentially could go all the way up to the purpose of having an option to make everything and then one of the conclusions there was that if we want to have that kind of proposal autonomy property but also have the uh the property of like potential low proposal requirements we might need to have a third category of actor that does are kind of not and not Mev extraction that in basically the entire bundle of computationally expensive stuff so like uh witness Edition State Route calculation in the future ZK snarking and uh minimum um like figuring out polynomial commitments and proofs and broadcasting and so forth I mean I I just want to comment I think like people need to stop thinking of Mev only as a bad thing because you think of sandwiching and even without any front running any sandwiching we still have lots of Mev and it's actually a necessary part of the system like someone needs to do the Arbitrage on the exchanges someone needs to do the degradation someone need to submit the fraud proofs all of these are Maybe so don't think like is this we're incentivizing a bad thing here so it's it's a part of the systems we're building and but I mean you so if we I would say for example you can eliminate um the bad Mev using transaction encryption for example but you'll still have loads of mab left like yeah sorry I've thrown I was going to say I think well basically I think we're relying on ethical Builders I think there's like all kinds of techniques that were layering on top to limit before that Builders have to do like they get really nasty things a question on the PBS so uh here uh so we we like sort of apply some slashing mechanism once we have a separation between the builders and the proposers right sort of um different actions they probably have yeah so there are slashing mechanisms different slashing mechanisms that play right so like there's a swatching mechanism that slashes the proposer if they make two conflicting blocks in some of these partial block auction protocols we use eigen layer and that's like basically exposes the proposer to kind of extra swatching if they yeah so if they violate the rules of the partial block auction protocol uh Builders can get slashed in some some contacts and forget exactly yeah which ones but there's a definitely a few cases um so you know there's definitely like different uh forms of uh of swatching to make sure different the the different participants follow the rules of the protocol yeah thank you this question maybe last question so we have time to get to 12 30 stuff yeah okay okay I think you are good um thank you for joining us with uh for this 2 hours and 30 minutes foreign [Laughter] [Music] thank you [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] thank you [Music] thank you foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] wonderful foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] thank you foreign [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] thank you foreign [Laughter] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Laughter] [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign foreign foreign thank you potential s right you continue numbers of on okay check perfect foreign foreign [Music] thank you [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] foreign foreign [Music] [Music] [Music] thank you [Music] [Music] thank you [Music] [Music] [Music] [Music] thank you [Music] thank you foreign [Music] okay before we start um this is going to be an engine Workshop so you're going to have a couple of exercises to do um you can already like take a picture of the career code and close the reports that you will already have everything okay hey everyone so welcome to this workshop on how to build secure contract using fuzzing before we start just to get like the first one is using unit test the second one is using manual review and the two last technique are using fully automated or semi-automated technique I'm assuming everyone here is familiar with unit tests you should use any test and they are good usually to cover that the system is working as expected in the happy path something that we have learned over our audit and in other analysis that we have done is that there is no correlation between the quality and the quantity of the unit test and the likelihood of having high severity vulnerability and this is we actually have an academic paper on this where we have looked over like all the audits we have done and this is a correlation that we have not found and the reason why our intrusion tell us that when you are going to write any test you are going to try to cover happy path things that are supposed to do in the in the correct execution while vulnerability usually lies in the edge case in the thing that you have and the tool is going to tell you there is this type of work or not for example you might know slitter which is a static analyzer for solidity this type of techniques might give you first learning um but they are also really powerful because I might cut you know like uh critical bugs you oh yeah okay okay so for Twitter like the best technique is spend one hour like the first time you try it there is a triage mode so once you have three hours like the ways they won't show up like in the next execution and if it takes you like one hour and at the end of the day you might be able to catch critical vulnerability I would say it's worth going through the first result and like we are we have like a list of trophy for uh Twitter that demonstrates that we have found a lot of like actual bug using it so yeah there is like a false positive path for for false alarm but it's not going to take you so much time and it's going to provide you value for example we have a git abduction with Filter um when you can connect it to GitHub on every pull request commitment depending on how you are going to configure it it's going to run if to see if you are introducing new new vulnerability yeah perhaps sorry perfect perhaps next year we will do a slitter workshop on you and this is open source language free okay the last technique that you can use is using semi-automated analysis so these are going to be tools for which you are going to provide some information for which you are going to have a human uh intervention to explain to the tool what you are looking for and this is a bit more difficult to use because it requires like this interaction from you know from the user it's a Technique we are going to see today with property-based testing with echin now so what is property-based texting to understand how it works I have to introduce fuzzing so fading is a standard program analysis technique that is used a lot in traditional security the idea is basically you provide a one-time input to the program and you try to see what what's going to happen you try to stress this with random input the most trivial photo that you convert you just go on your keyboard and you know you patch from the bottom and you see what's going to happen on your program um again it's well established in traditional security we have a lot of tool if further go further and so on however most of the traditional feathers are going to look for memory corruption for crash in the program we don't have a lot of memory corruption on solidity there are some but they are not that common what we're going to try to look for is a property of the system that can be broken and this is why we call it property based testing basically the way it works is that the user is going to Define invariants the first one is going to uh explore London music program and it's going to try to save the invariant odd or not you can think of you of folding videos like unit tests on steroid where with unit test you try one specific value with the program while phasing is just going to try randomly a lot of different value I've been talking a lot about invariance what an invariant an invariant is something within your system that should always be true it's something that should never be first or that should never be not possible to check if it's actually holding so I'll talk also about ekina so echina is a further for smart contract interpensers uh we have been using it for like four or five years even now uh in all our audits you can see a list of uh match your code base actually using and I've integrated Echidna in their process for Aquino we are focusing on the you know ease of fuse so the invariant are going to be described in solidity we have a GitHub action similar to slitter and we support all the compilation of framework if you use Foundry r.warney travel whatever we're going to support it because we are using it in every of our audit and every now and then someone comes with a new compilation framework okay I was talking about invariant so let's say you have a token you have a near C20 token it has you know a balance you can transfer token what would be an invariance could be that if you have a total Supply no user in the system should have not talking to the total Supply right if you have 10 million of token if a user of 20 million yeah something is wrong foreign okay so the question was what are the benefits of using Echidna as a Foundry um so first I think echina has more features than from there at the moment they were developing Foundry for like six months we have been using a canine like four years we support any compilation framework so let's say you are using rdot because you want to do integration test and you need some complex setup using like typescript or whatever if you if you move to Foundry you're going to have issue because it's more difficult to you know create this type of test so you end up in a situation where you need to have a setup with two different compilation framework if you use some Advanced options then you have to need to have like both Advanced option in both compilation framework if they support it and it's a lot of you know like maintenance uh here we are like you know agnostic to the compilation framework in that sense we have we're going to talk about that later we also have like a couple of advanced features that the others I don't have for example something that you can do with Echidna is that instead of trying to find in there that are broken you can look for functions that consume most of the gas so you can list the further one and give you a summary of okay I can learn this function with this parameter and it's going to Output this amount of gas and if you are looking you know for this type of things uh it's very nice hey there uh Brock from Foundry here um I'm curious uh how do you go about benchmarking uh a fuzzer right so how do you um because it's something like for us it's just a black box yeah and yeah okay that's a really good question and even in like traditional Feathering you know like if you go like in the literature of whole feather or benchmark I would say that most of the benchmarks are poor or one of the issues that when someone does a benchmark to you know benchmark their own tool there's a bias all right so we we have Benchmark we have our own Benchmark to try to see you know like in our past audit and everything how it works and everything but obviously we have a bias like it works well for us because we are you know building the tool on our example um so I would say like the best place to have like a good Benchmark for fuzzing should not come from to level yeah yeah it's it's also an open question what is your benchmarking like if you're if you're if you're saying well this is faster than this sort of thing but it could be executing just always the same thing like you know calling a constant function over and over again is going to be faster than calling some deep deep part in the in the call uh on top of that you have like bugs what what about like finding bars how much bugs you found and there is there are a couple of academic papers saying that some people uh like to compare this but you have a like um like a plot saying how many Bucks you found and let's say that one faster is better than the other but you don't know if the the next hour you will have a a peak saying well these found a lot of things so there are all the things that you can do so you can use coverage but also coverage is not going to give you like uh uh it's not going to be the the ultimate answer so it's it's still a debate whole long you should run a faster for a benchmark or even for you know testing something it's also a debate what you what we should use for benchmarking uh should we use like complex defy applications like but how many of them we have like 10 or 20 we don't have thousands of different device so it's it's uh we we definitely are interested in a deeper discussion on how to have a good band merch set for for touring and we have the same problem for example with Rita where how do we Benchmark that our static analyzer provide good weather and it's tough um we usually tend to have a practical approach in the sense that if the truth provide value during our audit if it helps us you know to find bugs and we make us faster that's good enough for us yeah and at the end of the day also depends on the invariance if the if the if the developers don't know how to write good invariants then no tool is going to provide some magic um value so it's it's it's tough okay okay and I'm happy to discuss with with The Foundry team or any other team doing doing fasting we will will be here uh today so please let us know okay my other question is regarding like the tool is only for passing for its support like symbolic execution or something like that so yeah you want to yeah it's it's only for first thing we have another tool for symbolic execution which is called Mantika however and something actually we're going to discuss later I think in practice any formal based method approach is going to have a lower return on investment and further if you have two weeks three weeks to work on a project you know and you want to invest some resource to increase your confidence in the project first thing is the best solution yes and uh and also we found that so um Echidna is a tool that works with our static analysis leader that gets value so you have in some cases like let's say that you have a test that says if x equal to some some value some uh traditional fossil techniques have hard time to deal with this but what we what we do is constant mining so we scan all your code look for these magic values and we replay these magic values and some mutation of that from time to time so or fossil should be able to get inside the inside this if you if you if you have a test case that is not that is not uh working please let us know and we can try to to see it but in practice it seems like some of the typical use cases for symbolic execution in which you have constant magic values to to look for they can be replaced by uh constant mining extraction foreign how many of you have issue like installing Echidna or opening like the different exercise okay foreign blockchain and give you the answer so you don't need to connect into something uh so yeah going to the into the repository and it says like yeah sorry [Music] yeah so it's it's it has more specific yeah so so over there you said like if you're using Mac you can do that that or you can download okay yeah okay yeah so I wanted to highlight one one little feature that we are testing on Echidna that is that is also used in passing but instead of testing a property where we are doing minimization or maximization of some value so this is an a new thing that we are that we are testing it is not property based testing but it's it's something it's something that we are trying that we are trying to push so if you want to know if a user is it's capable of uh extracting tokens from your system without you to realize you can use that feature just saying uh Hey Echidna can you maximize this balance of this account so it will try to generate you the maximum uh sequence so it's it's it's a little bit outside this but uh it's something that we wanted to mention foreign we will show the solution um did anyone already solved it foreign foreign thank you thank you foreign foreign foreign foreign foreign foreign foreign foreign foreign thank you okay foreign foreign [Music] [Music] [Music] thank you thank you foreign [Music] foreign um foreign [Music] [Music] [Music] foreign foreign [Music] [Music] [Music] [Music] so for the people that are trying to install a Kida with growing style of the Wi-Fi is a bit slow hopefully it's going to get better foreign [Music] foreign foreign [Music] [Music] [Music] anyone else needs help with the installation [Music] [Music] okay I'm going to show the solution of the first exercise hopefully in a few minutes everyone will have a kidnap start but yeah we'll see okay so our Target here is a token um it has a transfer function like a classic Crosshair function enable it from a portable contract which is like a basic possible system and what we want to try to do here is to create the invariants such as no user should have a balance above the total Supply to test the tokens where we are going to do it is that we are going to inherit the token our Target we are going to create a contract test token we are going to initialize the balance of the colors of the first user to 10 000 and this is an initialization so you are creating a token there is 10 000 in one address and no the invariant is simply that um no user so the user echina color should not have more than ten thousand token again you deploy a token 10 000 token to one user this user should never have like 20 000 token all right and if you run this with a kidnap a Kina is going to tell you that this invariant is property on total Supply was broken it failed and it's going to tell you how and the answer is that it just call the function transfer with the destination address zero and 1093 token so what happened here this was compiled with solidity 0.7 so there is no overflow and underflow protection so there was an underflow problem here where if you try to send more more tokens that you have a new balance the balance is going to render flow and you know you have a really large balance something which is interesting here is that we Define the invariance you know without looking at the card without looking at the function we were not looking either any issue in the transfer function we just Define an invariance and by doing so we can realize that there is a bug in the transfer function um so this is a kind of a nice a way of trying to find bugs because you don't look at the individual function necessarily you can just Define invariant and the further is going to try to break the environment for you does that make sense any question yeah okay so okay so the question is does it execute a specific function or how does it know which function to curl the answer that is going to call everything so in this token if you look at like the word source code you have a transfer function possible function and like some additional function so the further is just going to call everything and everything external or public like everything that a user can call okay the question is if you have a very large token or very large contract you have a lot of function so here you can take different approach either you want the keynote to call everything and you just do nothing and you let the key now when uh which might work you know it depends on what it is if you know that some functions are more important and you want to Target you can change in the configuration of configuration file of echina and tell him call only this function or don't call this function so it depends what you are trying to look if you want to increase your confidence you should call everything if you think they might have an issue in a specific function and you want to focus on that you can you can Blacklist a white list special order okay so the question is can you define the order of call you can Define the order of the initialization but not after that I think there was another question no okay okay okay so the question is um can you have like a better log because obviously this is like a simple example and when you do random you know exploration you might call a lot of functions that are not necessary for what you are trying to to call right and the answer is yes so Echidna does what we call shrinking where once it found a way to break the invariant it's going to try to reduce the choice so it's going to continue to First more or less on the same you know iteration uh and trying to reduce like the size of the of the trace culture okay then we have um the second exercise friction so on the same repo just got exercise two it's on the same Target so you're going to try to have an invariant um on the same token the first invariant was that no user should have a balance above total Supply here as we kind of hinted before now this is a possible system so it's a system where the owner can pause or unpause the system and what we want to verify the environment we want to have is that if there is no or no and the system is pause can someone unpose a system and this is what we're going to try and yeah let's take 10 minutes for this one foreign foreign foreign foreign foreign [Music] foreign foreign foreign foreign foreign [Music] foreign foreign [Music] [Music] foreign foreign [Music] foreign foreign [Music] foreign foreign foreign [Music] foreign okay I'm going to show the solution for the second one so it's the same Target that for the first exercise but here we are going to focus on the contracts that were inherited by the token you have two token now you have two contracts right ownership and possible and here you have a system where your owner and you can pause or resumes as a contract and what we want to check is that if we drop the ownership and we pause the system is it possible to unpause it so here we have a bit of initialization to do right because we want to drop the ownership and we want to pull the system we are doing this in the Constructor so we are calling pause and oh no from now on the system as a contract is deployed it's pause there is no no the invariant is then just if the variable that you know tracks the possible state of the system is true and this should all right you pause no no ship it should be always post foreign count and common in old version of solidity there was no constrictor keywords and where you were doing the Constructor that you needed to have the function name which was a match with a contract name and here you have the contract ownership and a function owner and because of that the functional now is a public function and anyone can call it and become the owner this does not work anymore with more you know modern version of solidity but the type of bug that we are finding a bit more a bit too much in the past something which is interesting again is that you know we did not look at the ownership contract we did not look like at the implementation itself we just Define an invariant and without the one of the further one and you found the environment for us so now brings a question on how to define okay and um okay so the question is is defining invariant part of the auditor you know work uh yes like we are using a kidney in our audit and during our audit we are going to Define invariants and something we are going to do that we are going to discuss with a developer because the developer know you know better than us what the system is supposed to do so we are going to have this collaboration with them to understand what the super system is supposed to do and to Define this environment foreign how to define invariance because you know like if you have bad invariance it doesn't matter what you are doing you know if you are using further if you are using like formal method if your invariant are not good you're just going to check for some things that you know doesn't matter the best approach to writing variants is not to start with a tool it's not to start reading or writing down solidity invariant is to start with English open a file a markdown file or whatever I know a format you like and write in English what the system is supposed to do start simple start with environment that you know you know are true start with things that are not working once you have five or ten simple invariants buy them in solidity and when the further on top of them if the invariant are all holding then you can go back to thinking about the event of the system and you know go more deeper into into the environment themselves if something is broken then look if the environment is incorrect or if there is an actual bug and iterates go over um yeah in in our experience when we work with uh with um with clients when we ask them to do step one undefined invariants they are actually uh they they realize about bugs so it is already a good a very good thing to start thinking into that even if you don't if you're not testing yeah okay okay so if I understand correctly the question is that can we connect this to mainnet foreign okay how to use it with other contract so you can just in the Constructor deploy the contract uh some things that we are not going to cover here but we have a tool which is called itino which is basically going to take your unit test take like your sweet and we play them in Echidna so for example if you have like a complex integration with like you know you are deploying a new unit test 10 different contracts you are deploying like a mock-up unit swap or whatever you know you need and you can replay this in a keynote everything is going to be set up yes yes I I I think there's some a little bit something else there that you want to what if you want to Define an invariant on a unisot contract right that you that you are using that your contract is is is using so you will need to know how unisa uh Works in order to put it in your environment like if I'm swapping something then I'm getting something else right and in that case you need to realize that it's difficult to write invariance with other people call right despite despite this is working and everyone is is using it but every time that you use a third-party contract then you have you are importing some risk and you need to completely understand the other contract in order to know what is going to the effect in your own country so there is case was we have a special uh I'm from gearbox protocol and we work on composable Leverage and we have adapters because we just provide leverage for some other contracts so when you combine gearbox with uni swap you get immediately um and margin trading and in this case it has adapters and these adapters incorrectly parse path to make check after uni swap however they integrated and may call to existing uni Swap and guy who were on a modified problem he write a small test and this test show us that if you really add some additional part of call data it could be interpreted incorrectly so we have two different ones our system could be fooled check not Z balance which should be checked and in this case it was a fault of the system and the funds could be drained and in this case I think we could find some fuzzing testing to really provide any information but this test should work with uni swap because we behave in different way and we shouldn't cover that because we run with mocks and MOX of course was created with the same bug so MOX was okay but real implementation totally different and I definitely believe that fuzzing should found this mistakes yeah yeah definitely definitely when you are creating a mock you are assuming how everything works and if your assumption is not precise enough it will it will you won't be able to detect something and we as an Auditors is is common that we have two audit contracts that will interact with other other contract let's say company so when we go into compound we have all the documentation and says well compound word like this or like that when we look at the code and see some things that are not documented and we go back into the developers and look look if you if your contract is doing this so that it will revert and you're not testing for that so it's when you when you are using a third party uh contract you are importing the risk so either either you have really good tests or you even uh make sure that you understand everything otherwise it will be difficult to catch the uh but yeah I think this type of work can be found with fasting like the most like the difficulty is going to find an initialization that makes sense and an environment that makes sense um that's why that's why you know we are kind of in putting like the infances onto defining environment because this is like the key component of this technique yeah uh in terms in terms of speed is it okay to put all deployment script into Constructor because of course if you deploy such a huge system and you deploy some contracts from external repositories like uni Swap and so on it requires time and of course when you want to test million operation if you redeploy each time holds the system it could require hours or days I know yeah so so so when you use Echidna you deploy it only once and then the when your tests finish it will go back to the states after the contract is deployed there's no need to redeploy it and that is why we have we we asked the developer to have fixed amount of parameters in the deployment right on the on the on the Constructor otherwise we will know what what we should deploy yeah I think there is a question over there also um I'm just wondering as you uh once you've kind of defined your invariance um and I imagine you guys in your audits you you run through these and um basically I'm not trying to understand when you guys have confidence that yes the this is a good invariance uh from and and if there are any metrics that you guys use uh internally like I see it's outputting Unique instructions and unique code hashes those sorts of things that uh give you confidence in in what you've done so in practice you know you can look at the coverage but usually coverage is not a good indicator and In fairness like you know when we do that we do this in a time box manner so we have two or three weeks to do it and we are going to do our best in two or three weeks uh that's the best that we can do yeah it's it's tough we there is no a bullet for this um it we when we do a report we list the invariance that we test so it's it's clear what what we tested and what and everything else was was not tested with all so we will perhaps did Manual review or use all the techniques like Slither to check some some other things uh but yeah unfortunately there is no uh good way to to Define this uh but perhaps I I personally think that talking with the developer early on the invariance it's a really good thing it's usually the case that we think an invariant let's say that some some value cannot be zero and we go into the into the client and says is this an invariant we don't know because we have not designed the system and they don't know and if they don't know that's that's an issue right we should we should absolutely know what is the behavior of the system and and if we don't know if anybody should should if something should be an environment or not then we should go back and rediscus that and yeah Security in general is not binary it's not you know yes or no it's really a matter of how much resource you want to put into it and moreover social put more confidential thank you thanks I have a question actually more related to the earlier question which is a big uh class of bugs that's been occurring recently and for a while now are re-entrancy bugs right how do you deal with finding um violations of invariance that correspond to external contracts in that way okay so this is a really good question and in my opinion the best tool to find way on Transit is static analysis so the question is more to find which technique you should apply for which problem and for things like Korean translation static analysis is just going to be better you can use further you can create like green callback and things like that but in practice that you can reduce is just going to outperform any further this work uh that's why like for any any class of vulnerability which is kind of a pattern base you can use static analysis and it's going to be better in your opinion and uh one more question what's your addition for example we have a complex system and we want to make a classical fuzzing with the kidnap and it seems that to really cover many cases it requires a lot of computational power so maybe can you advise some cloud provider or how to do to run it maybe for a week with very powerful computer to get something achievable because of course this pretty simple contracts could be found on my MacBook but if we go a little bit further many contracts many setups maybe it requires more computational power um so do you want to talk about the kinapad the question was um if you want to run a key down the cloud or on a lot of you know a large like system how can you do it yeah yeah so so so the first thing that uh you should know is that uh uh it can it to have a very large contract it can take some amount of memory so first first thing get a good server with with a good amount of of memory and CPUs um so the second thing is uh every um we have a python companion tool called in a parade that will run echina in any number of uh any number of uh concurrent um uh instances so you can run 10 at a time uh but we're not only going to run it 10 of the time but we are going to randomly Shuffle parameters because in some cases there are some there are some issues that can be easily found with let's say three or ten transactions and some other issues are going to be more easily found with two 200 transactions in a in a in a row right so what we do is we run the tool in different with different random parameters and in different let's say Generations so we run we run akina for an hour 10 times then we save the Corpus and you can get you you can see all the all the all the code that was covered and then we started again but taking the the output of the previous generation so you iterate over and over again so you can see how your code is explored right or if there's some part of the code that is not explored with 10 different uh instances you can go back and say no I need I need to change this because it doesn't depend on the on the actual execution so we can we can give you the link uh for that it's just a python tool so it's it's it's easy to use um yeah it's also open source like everything we are doing is open source okay so yeah like it's really about spending time and thinking about our invariants and start simple like if the first invariant that you are writing leads to a bugs there is something wrong about your approach uh you should not have like simple invariants that you know are going to work the system so we'll start simple and iterate over them okay to give you some example let's say you have a now with magic libraries what environment can you have um you can have commutative properties a plus b is equal to B plus a you can have identity a 1 multiplied by two should be true our inverse if you add something by its opposite it should be zero this is not always true right but depending of what you are building this might be like the type of property you are looking for for token we already talked about the first one no no user should have a balance above total Supply let's say you want to look at the transfer function and patsync transfer function what does it do I'm transfering token to someone so at the end my balance should have decreased by the amount and the receiver should have see its balance increased by the amount and let's say you try to write something like that once you might quickly realized that what happened is the destination is myself so if I transfer token to myself my balance is not going to increase or decrease quite awfully so this is an example where you might try to Define an invariance on transfer it might seem simple you might write like the the thing in salinity and if you do that again is going to tell you that there is an edge case where if you transfer to yourself like the event is going to be to be broken and in this example if you go through this um it's not the code but which is bad it's an invariant that was bad so that's why having this iterative approach uh is really important because sometimes we are going to make assumption about your system and you might actually be wrong and as the system gain on complexity it's no more likely that it's going to be more difficult to refine the invariance something else which is also important to to consider is returning first uh reverting for example an Inverness you can have is that if you don't have enough phones such transfer function to the revert or written first depending on how you implemented the token once you have this list of invariants usually you can split invariant into two category function level invariant system level invariant function over invariant are usually stateless they are things that you can just you know look at a specific function and try to see if it holds so it may take you know environment and mention are stateless and our functional leveling variants here you can craft simple scenario just by calling the specific function then you have system level in the Hangouts system level environment are usually more complex but they're also more powerful and here you are stateful you are going to change the state of the contract and you are going to try to see the environment no matter the state and here is why it's important that the keynote is calling all the different function because it's actually what you want to try the balance being below or equal to the total Supply is an example of a system level invariant for functional value one thing that you can use is a different modern equivalent set of coding Akinator something we support a session so you can just create function put a session and try to see if it holds for SIM for system level environment as we we kind of already like discuss it might be more complex depending on the initialization of your system uh if it's a simple initialization you might be able to do everything in the Constructor if the Constructor is too large for like the bytical size or for whatever reason you might have to split it and here it's where you can use a kid now itchy no sorry okay all right so let's let's see this uh particular piece of code let's take um uh half a minute to read it um it's it's basically a buy function that will uh call an internal function valid by uh so what we are going to do is we're going to think uh what are the type of invariants that we can have here and what what each what will they are going to test and what type of guarantees we're going to get from this so let's let's take a few few seconds for this foreign yes so we have questions okay so the question is uh testing timestamp dependent so clearly not not the case here for timestamp depending code uh um I think now when it runs it automatically increase either the block number or the block timestamp inside some range right because it it happens that some code will fail uh when the timestamp is increased into a really really large number but yeah 100 years or like the end of the universe so we don't care if if the smart contract has a bug that can can only be triggered in the end of the universe will be the least of our problems um all right so yeah any any idea what are the type of things that we can test here the first one we can understand that how much token how many tokens we can get as a result follows our expectation when we're sending message value because as you can see it's a hard-coded rate is around 10 so basically it's a pretty simple formula we can change different values we put into the function we have totally prediction how much we can get and then we can try to verify that it works yes exactly so the the property is related with the amount that we can get even the number of uh weight that is sent so um yeah uh we can so the first thing is this this code will depend on the state uh we don't have the mean function so we don't know what is what is inside where we have the valid buy function that is actually abstracting the thing that we want to that we want to test so we will start with valid by which is a pure stateless function um yes so we were thinking about invariance here related with um the amount that we can get so this is these are very simple without going into specific uh this is a very simple invariant if the amount is the Wayside in C is zero then the user should receive no tokens at all right so so that's it's it's even simple and thinking how how much a user should receive but it's a Concrete case and it's it's kind of a corner case so it's it can be important to test all right so how we can test this so there are a couple of way to test it this is um this is one so we we can write a function that will take uh one parameter so again I will put any any number there however we're going to restrict the number uh the input of this function to be non-zero uh then we're going to execute valid by uh and then we want to know if echina can reach he can reach the um the statement after that because valid by Will revert if if the uh if the inputs are not uh the one expected um so we want to know if we can get if we can get tokens despite sending no um no value right and so perhaps you're wondering what if what if uh desire amount is zero uh then clearly this code will not do anything interesting so she was just is going to revert uh when you're writing tests you need to so you can put any any amount of requires or preconditions or people usually call it however if you put uh if the preconditions are too restrictive and your uh your function reverts most of the time it means like you're not going to get value from the execution so every execution reverted in a test in an invariant let's say that every every case that you don't uh that you don't use it's going to be an execution an execution that you waste so in this case you only waste one one execution in a range of in the full range of uh uins 256 so it's not a big deal but if you have if you put a lot of requires that only a very small small amount of uh values well satisfied and it will be difficult to get randomly or even even with the with the techniques that we use you will need a slightly different approach but yeah we will we will uh then go later into that so any any questions yes yeah so as you mentioned in this case that we are basically sacrificing only one uh one case which is when it's zero uh but is it gonna run over the whole range of un256 because that's a really large range and it doesn't make sense to test all of that in some cases yeah exactly so it won't there there will there is no tool in the world that can run for all the all the range uh it is always either either uh symbolic I mean you can do it symbolically but it's it's not it's not going to test all the values it's another thing um and uh fasting techniques are going to sample let's say randomly from the from the input or with hubs right um it's in the case of Echidna since we are going to compile this code and it will run to our static analyzer we will detect some interesting values in this case um 10 for instance 10 10 is an interesting value there it's a it's a constant and it's going to be used somewhere so definitely we want to we want to test with that constant okay so let's see what happens if we if you if you run so this is uh so echina will run a number of transactions it will eventually detect that a certain of free token has an assertion failure however this is going to be in the context of 100 of transactions random transactions that perhaps will do something that is completely unrelated but what we'll do is input minimization input minimization is a very old technique referred to testing in which you have a list of bytes that affects a bug you want to remove that bytes um uh one by one or in some random way in order to get uh um a list of bytes that will still trigger the bug but it's going to be minimal or or either local or a global minimal depending on the type of tool that you're that yours so in this case uh we can uh Echidna will try to minimize any any uh parameter here we have only one parameter and the parameter is actually is actually going to be useful triggering about if we have more parameters they are going to be minimized towards zero so if you have units uh then it will be reduced until zero so zero is the simplest value this is arbitrary defined on the on the code you can you can change it if you want and but in this case the parameter cannot be zero because if it's zero the test will pass right so in this case the minimal amount is one it's not guaranteed that you will always get the smallest uh set list of transactions to trigger uh this is an uh an MP complete problem it cannot be solved on on linear time so it's it's all going to be always a sample but in in practice uh even randomly sampling removing transactions or reducing the complexity of each uh value will will give you good answers all right so a little bit about ekina apis maybe we can just explain um yeah just explain why it's happening um yeah so yeah so the issue here is that if you send one desire token what's going to happen is that you do one divided by 10 and 1 divided by 10 is 0 because you are winding down and other results uh the required amount to be sent to zero so if you ask any number of tokens below 10 you are going to get them for free and this is again again an example where we Define an invariant we don't actually look at the formula we are not looking at how this formula works we just Define an invariant that if you don't send ETA you should receive not okay and by doing that echino can find you know how it matching issue and we actually evenly using a key now for bonding for you know mistake in the formula and and so on yeah so ah yeah yeah I gave that this function is for testing purposes but um in a real situation wage scent is not part of the of the signature function right you read that from the message yeah yeah yeah this is how we like like deeper into the code right right so how are you doing that okay you read that into the a certain function how you do that so yeah so uh uh if I understand it correctly so this this could be an internal part and you can have like a lot of code that put like gets the value from them from the message value and then do something else ah you can do that uh yeah I mean I mean is this depends on on your code here we are testing an internal function right using using some some defined yeah and if it was using message that value um yeah exactly so uh yeah so uh properties can also take value so and if you have a constant in your code saying message value should be uh 42 42 42 it will use that constant eventually so you should be able to hit that particular that particular case uh given the fact that this is random something of course but can you uh moving back to the previous slide to the function what I'm really wondering because we talk about this error however tokens less than is uh so way less than it's so small amount and the same problem which could be here is Overflow if I provide a huge amount of ease because we have a multiplication to decimals it could be done on the other side we all of us know that the quantity of s is limited you can't even take a flash loan and get more ease than it produce so what is the best practice follow this formal execution when you write fuzzing tests or take some real examples as limitations as there is no much a zeroom as it exists at the moment and we know it's a deflation so we can simply assume that in the future nobody can take so much to get overflow here yeah yeah exactly so yeah this is is an interesting question and it goes into the fact that what are your assumptions on the test right if you if your assumptions are like I have this token with this limited Supply that should never go over something then you you can just say I require that that the value sign cannot be more than the total supply of something right but in the case of interest is a bit more tricky and and in fact in in Echidna what we do is we have we have externally on accounts are simulated we loaded with eater every at every transaction because you can have a very large amount of feature you can take a flat loan uh you you probably cannot have as enough eater to overflow uh you into 256 because that will be a real issue for the AVM uh in in itself but um we can we can Define in the Kindle config which is the maximum amount of value that we send per transaction right so if you put like I don't care if the attacker has more than than 10 000 eater because that will mean that they can do other things then Echidna will happily uh take that limit and will never put something more however it's still the case that over several number of transactions the accumulate number of features can go over that that barrier so you should you should be careful with it and there is also no project you can take here so basically you are building an invariant and the invariant as you should be thought as like a value so either you start where the invariant has really limited Twitter like really with chain one and you try to see if it hurts if it's working with like a you know like you with one meter okay it's already working so you can continue like this if it's not breaking then you can increase this without from time to time or you can take the opposite approach if you define an invariant where the threshold is really large it's breaking because it's really large and then you decrease this result so that's how you start really limited and you know depending on the result you remove the limitation or you start without limitation and you reduce like up up to the point where you have a kind of like a value for which you feel comfortable with yeah yeah and and that is also really if we start with very large values we we could have false positives but and if we start with very small values we have a false negative but which are the ones that are going to cause you more trouble that is something to to think about it because if you miss one's false positive your your protocol can be you know destroyed and if you miss one one false negative then it could be okay all right hey there in terms of fuzzing mutation uh do you do any clever things like uh say this function has a constant of 10 so would you then uh see the constants in the the function and use that as input yeah we remove the constant forms of function and we will first R1 to constant also like if you see 10 we are going to use 10 we are going to use 9 11 you know like uh around you yes there are some there are some techniques I can show you a little bit after a few lines of the code of Akinator that shows all limitations we have mutation interesting mutations on list of transactions in which we Shuffle we do uh we do like splice as well so we take a list a list of Interest transaction and another one and it splice it on a random position so there are a couple of fun things to to look at but uh yeah I think we should move on a little bit um all right so as I was saying we have this feeling even if you don't understand what was what is the failure um well that is that is a different there is a different Beast sometimes you you you have you put your your um your invariant and your invariant fails and then you start the journey to understand why it fits so that's we are not going to talk about that you uh some people like to re-uh run We Run The failure into a unit test to make sure step by step what is what is going on but yeah that's that's a completely different uh type of Beast that is related with what happened after and how we can fix how we can fixation all right so a little bit about ekin apis this is um a topic that um it's uh it's it's still an open debate in some in some cases so what are the best way to test to create properties so Echidna supports a couple different ones it's about Boolean properties in which a function uh is executed um and then it will it should return a Boolean true or false and if the function reverts for any reason that is the same as returning false right so if we go back a little bit we can see over there error unrecognized op code that is going that is related with the assertion failure this that is how um on version of solidity used to uh have um this uh assertion failures but if you use Boolean you will just say return false right so you know exactly how how everybody failed uh or it could be a revert so you you say over there you see over there it reverts okay so either you do Boolean properties which are the classic way to define invariants and these come from uh uh some some very old techniques in particular a quick check which is which is a property based testing tool for Haskell and a couple of other languages uh which was an inspiration for for for this then you have assertion failure so every time you see uh every time uh the the assertion is called with false that is uh that will fail however if in the context of your function you see a revert that will not make the function the property fail here we can see that if valid by reverts then the this again I will not report that because we are using assertion mode five so you you should be careful if you are using if you if you care about reverse uh you should have to uh either use the um the type of um the Boolean type or what you can do is if you care about reverts invalid boy and if it's an external function you can do a try and catch and you can check which type of reverse and you can even fail in some type of reverse and not in other because you want the user to get a good message of of alert and perhaps other type of reverse you don't you you want to know so and finally we have the DAP and Foundry API in which um you will uh you will have um a function that if it reverts uh it's going to it's going to fail and otherwise it's not and I hope The Foundry team agree with us and it's uh is correctly implemented um all right so there is testing modes um in the in the um in a repository so we can um uh you can go there and it's explained a little bit more this is very high level overview so yeah yeah yeah so so so yeah again that's something returnable Boolean uh it's easy to Define uh no side effects that's that that is so interesting when you use Boolean properties all the side effects will be reverted uh before the execution of the actual invariant right but if you're using assert the side effects so everything you change in the blockchain will will remain so this can be really useful for testing some some complex code but yeah we're not going to well um yeah assertion is can be simple to to um Define um and it's it's it will also um it will also be easy to see on the code coverage if it's not if it's not covered or or not however some code especially some old piece of solitary code it's going to have assertion as required and that is a really bad thing you should not be doing it use require if you want to if you want to actually um uh have a precondition in your call and use assertion for for testing okay um yeah and finally we have the The Foundry and up uh compatibility the only thing well uh the thing that we don't support is pranks so we don't like to prank people so we don't we don't support pranks however we support some of the some of the um uh havm yeah the havm the original ones uh cheat codes you should be careful using it uh we uh we know that there are some uh catch with that especially related uh with what the solid compiler expects and what you're doing in your in your transaction so please be careful because you could have some uh um some some issues so we in we rarely use cheat codes uh we try to keep all our code as close at the solidity possible so you can easily Port it to another tool there's there's little again specific uh but yeah we are also open to discussion if if the community agrees that we need a specific cheat code or we need to avoid some specific chicken then we are open to discuss it all right um so exercise four we're going to deal with uh um one of the um one of the exercise for dumb vulnerable defy so how many of you know this uh um amazing uh CTF um ah yeah sorry you you yeah we're going to keep it up yeah yeah we can yeah we can skip it this is term so this exercise was exactly the same as the first one but instead of using function uh we were using assertion so it's exactly the same invariant exactly the same setup but with a different API just as an example yeah so we will go into a more interesting example but do that there uh something that you will need which is called the multi-avi mode um so we usually usually testing tools take a specific contract as your main contract to interact so in in the default mode akina will only target a specific contract that you can put in on on your command line or if you have only one contract you will use the first one right but there is something called multi-avi that we call every every contract that is deployed after the Constructor that you have API right so if you deploy something in bike in bico directly and you don't have Avi in the won't be able to call it because it doesn't know what is what is there but if you if you deployed a couple of tokens and several contracts and you use multi-abi again that will call any function in any deploy contract uh after after the the end of the Constructor so we will need this in order to deal with the next example because sometimes the the the the debug that you want to detect it doesn't depend on the state of one contract it depends on the state of many contracts and in that case you can you can be surprised by the fact that changing the state in another contract can break your your property and definitely we want to avoid that Okay so uh again how many of you know about them vulnerable defile okay a good number and did you actually well this is these are the first exercise the first the first two uh so I hope that uh people know so if you know this this how to solve it you should uh it's going to be even easier uh for you what we're going to do is we're going to uh take a look of this um sample so yeah I assume that we already you already have a code so um is the exercise the the naive receiver one um so what we want to do is we want to be able to drain um the the funds in Flash loan receiver right just to give like a bit of description of the of the challenge um here you have two contract you have the naive um we see the Orlando pool which is yeah yeah it's it's over there um I can allow you to take a Flash run for a fee and you have a second contract which is a user uh contract that is going to interact with a pool and the contract is going to be the user contract is going to be deployed with some font inside and the girl is going to try to see if it's possible for this specific contract from the user to be joined exactly so what we want uh we we want you to review the the um the exercise if you have already did it it requires you to send some some some some number of transactions so in this case we are going to prepare everything for Echidna to ReDiscover this without telling it what is what uh how how it can be uh uh sold so we will need two things we will need to initialize the um the code to have um too much what the what the initialization is it's um is actually showing us let's see okay so this is the The Flash loan uh the part of the or the flash loan so the the interesting part about this is we don't have to care about specific details in the code we want to we want to um give a kid now the same scenario that we have on the on the actual uh Challenge and we want to know if it can actually find a way to drain the contract so yeah we can uh we can see uh how the receiver function um works here but um yeah the interesting part perhaps is the initialization what we are going to do is we are going to deploy the Constructor of our um test we're going to deploy the the contracts that we have here and prepare everything and then we're going to use a suitable um uh invariant which should be really simple you don't have to overthink and we want you to with that in order to see if it can drain the contract uh with that of course with a suitable um uh with a suitable invariant so yeah we will take um 10 minutes yeah yeah we are running out of time but uh let's take 10 minutes so the first step is really like this is a initialization from the test case of the contract the first step is really just to replay this into a solidity Constructor and then to buy the environment yeah so happy to take questions or any technical issues foreign Ty [Music] yes there there is an alternative way to do it but the solidity approach is is easier to to avoid um because well the other tool is called atino that can replay this in a simulated blockchain like an ash and then send it to the to the tool so uh in that case you don't need to replace on solidity manually but you are still I still know still need to know where is everything deployed right so if you want to interrupt with the pool you need to know where is which is the address of the pool and you can you can then uh I can cannot actually call the pull functions uh automatically because it has everything but in this case we're going to go into simple route uh that is going to uh take us to uh rewrite it's just a couple of and this kind of much what we are doing now that we might look at the existing unit tests to understand initialization and we might translate them in solidity if needed to assign credit for the invariance so yeah the the previous exercises are on the easy side this is a bit more difficult but still shouldn't require more than a few lines um so yeah but please let us know if you if you need some some hints there are some hints in the in one of the one of the branches in the hint Branch but yeah hey there hello yeah sorry no worries um is there any thought or current support for magnet forking or um State forking of some sort not yet yes so not yet havm has support for that but that will that will need us to put uh that requires to do like input outputs on on transactions so we need to check if that will impact on the actual um on the actual um performance on the call um so yeah there will um yeah um I have kind of high level questions so I'm trying to think about what the limitations are in terms of expressing properties as invariance right so for example let's say we have a temporal property that we want to express like we have a wallet contract and we want to be able to say that any user who deposits their money is eventually able to withdraw it is that just like a fundamental limitation of Echidna or is there a way to so if you can so the notion of eventually it's you you need to have some definition on the or on the blockchain right So eventually cannot mean like in 100 years right so if you if you find say a will only allow increment of time up to some limit yes you can you can call it as a bounded version of that property but you cannot you cannot use it as a theoretical thing like you know I have a state which I don't know what it is and then I will transition to another state which I don't know which uh what it is and in that case you will need to know if the original state was actually possible and and things like that so uh a kind of works on the on a concrete on on concrete States so you always have concrete data and you need to put like boundaries on the things that you that you can do so if you say a user should eventually uh receive this amount and eventually means like in a number of transactions and a number of blocks or timestamp then yes otherwise it's it's more like a theoretical proof that you're doing and you probably need another type of tool yeah I guess I'm curious even if you took like the bounded case because that's fair that you're dealing with like concrete traces here it seems like that's not or it's not intuitive to me anyway how you would Express that as like one of these invariants so you you will you will do something like this you will put a function that says uh and and you will need a state to track down like uh you you do a deposit and you will need a user to eventually receive something right so you will need to have a state that tracks deposits like a mapping right and you have a function that is your invariant so if um and then you receive the address of a user and you're checking the mapping uh if if the time um between the last deposit is in this range then you were going to check something and if if the time in this in this other range you're going to check some something else so it will be like randomly checking in in transactions um and with that you will be able to cover uh given the fact that you're going to generate enough transactions and enough time times right so it seems like the answer is it's capable of doing it but it requires some like kind of manual adjustment of the code almost to add state of it so yeah you will you so if the if the property that you are testing requires to add State yes you will need to add whatever state is needed it won't be able to track state outside right the only thing that will be tracked outside is the increment between between the uh different blocks for instance so that that will be tracked uh outside a new akina will show you from in between this transaction and this transaction I have 10 blocks that will be tracked uh outside but everything else if you need to do a mapping between users and and the time between certain operation you will need to keep that in a different in installation in a different vial I see great thank you all right so we have some other questions hi um is there any feature that allow me to guide the future or the domain of values that I want to try um so in order to in order to guide the the faster into a particular State the easiest way to do it is to add um is is to add a small piece of code that will be auxiliary code in order to uh move the state from your contract into something else like for instance if you have if you have a contract uh the protocol that requires a deposit with a particular uh property let's say that uh you have three parameters and you need a deposit that has uh that has digital parameter in in in the same in the same number or in numbers in which in which are difficult to find you can have this piece of code but it's important to let the kidnap to explore freely at the same time that you are allowing information so it's it's actually important to know that as an auditor or as a developer you are adding information into into not just using it as a black box right so every state transition that is non-trivial to find or is really really important you you can have it and make sure that it can eventually execute it because it's going to be another another transaction to to execute but at the same time you want to allow the tool to explore things that you don't expect because if you just restrict says I only expect users to do this type of deposits then you can you can be surprised later because there there is a way to break your your property using things that you don't expect okay thanks and it comes down to the previous question where either you start with a lot of requirements and a lot of no restriction in what you are trying to explore and if it odds you are going to remove some of the restriction or you start developers interaction you don't have any restriction and you know you go more and more restricted but this is okay so did anyone manage to at least uh Still Still start to create a Constructor or even run it to have some some invariant or even think about the invariant that you need so in a couple of minutes we'll go over the conclusions and I will show the the solution um but yeah happy to take any additional questions foreign just from an implementation uh side I'm curious what um what actual like virtual machine do you use to deploy and execute contracts so we we use havm uh which is the Virtual Machine written in Haskell I know that it's in the process of uh improving and rewriting so it was moved from the adapt tool into the ethereum repository so we are eager to test new features but yeah um if you use the Tino companion tool to deploy your contract it's going to use ganache um and you will serialize into a Json file and you can then load it into into that okay okay yeah we're going to quickly go over the solution so we can have a couple of minutes for for this so the solution requires first to deploy have a contract with enough amount of heater uh to to match what is deployed um and then um you can see here uh in we we deployed all the contracts that we need and send um the amount of eater that every contract needs and then what is uh what we're going to use we're going to use a simple very simple property that is going to say that the balance of the receiver is at least 10 each year so we don't actually need to follow exactly what the exercise says about draining completely the contract if we have one transaction that allows you to reduce the balance of the receiver then then it's then something is wrong and uh definitely uh it will it will eventually be drained so yeah so quickly if you run it you will see something like this um in which well the uh the flashlight has a parameter that is the the borrower and we can control this to in order to um reduce the amount of balance all right uh so yeah you want to go into the conclusion okay um yeah we're the second exercise on demonable day five but uh we are running out of time this idea was the same we are defining an initialization which was a bit more complex and if you go through this exercise with something a bit more specific is that there is a callback from the contract to the caller so in the Kindle test you need to have like also the Callback to implement the flashlight but yeah okay um yeah so this is something we kind of touch on a bit during our discussion what about the other tools so there are a couple of other further out there there is Dub Brony Foundry at least this one open source this funnel might be a bit uh better for simple tests and for like you know ease of useful like the first invariant because they are integrated within the compilation framework but in the long run they might not be as powerful as a Kinder simply because we have you know use akina for a couple of years and we have tuned it in a way that it provides the best value that we can do I'll get finished okay sorry um yeah okay so I hope you enjoyed the workshop um we have more exercise in building secure contract something I would recommend for you is to try to write invariant you know in your next project and actually who is going to try again now on his next project nice and thank you [Applause] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign let me just set up the clock somewhere so I can control the time all right so uh today I'm gonna present how we can leverage storage screws to build cool applications maybe a bit of info before this is me I work on Herodotus um and yeah that's that's it so today we're going to talk about storage proofs I want to introduce this app yes uh I'm gonna present you storage proofs and explain why they're cool how to work with them why you need tooling to work with them and yeah a bunch of other things why is it even possible all the complexities behind the trade-offs and so on so a few words about storage Crews why I really believe that they are cool especially nowadays so might as this is that ethereum is pretty sharded nowadays and with storage truth we can essentially read the state in a almost synchronous manner which is a pretty pretty nice thing to do given the circumstances um yeah and maybe also let me explain why is it even possible so storage room is essentially this idea that the entire state is committed in a cryptographic manner using some data structure like Miracle trees Marco Patricia trees and so on and yeah we can essentially verify any specific piece of State at any point in time on any domain which is pretty nice and does introduce additional trust assumptions we just rely on the security of like the base chain so yeah that's like storage proofs tldr where they are cool now a bit of like sponsored section sponsored section so uh what we're doing at harvardus so our like goal is to make smart contracts software in a way by providing access to historical state uh we like I said might as this is that ethereum is pretty short that nowadays we want to unchart it by using storage Crews and we want to enable synchronous data reads because today we do not have really nice ways to make synchronous data access without introducing New State uh new trust assumptions so yeah that's we do and how we achieve that we achieved it by using obviously storage proofs we use snark Starks and then PC I will get why we even use all this tooling but first a few words about storage rules what these are and and so on it's so tricky actually I I need to be multitasking okay so uh what we're gonna cover in today's Workshop so all the basics required to like understand properly this primitive how to like work with it uh how you can generate these proofs why they're pretty useful and how actually you can access this commitments I'll get later what we call the commitment in a process manner and how we make smart contracts software and enable historical data reads cool so um it's pretty that's pretty tricky so about the background that I want you to have for this Workshop so we're gonna like start from the biggest Basics so what is the hashing function just a very quick reminder I hope it will take less than a minute uh like generalized blockchain Anatomy how I ethereum header looks like why ethereum they're not like pretty on only like ethereum focused however I think that for the sake of this Workshop it's the best to like present on this concrete example Miracle trees explain me like on five I will just quickly explain the idea how it works and what is America Patricia tree without really going too much into digitals um yeah finally no not finally uh the anatomy of the ethereum state it's pretty important to like deal with this uh with this primitive and finally how to deal with the storage layout cool so touching function essential is this idea essentially it's this idea that I can have a function that takes some input of any size and it always always return an output of a fixed size and now what's also important there is no input there are no two inputs that will generate the same output and you cannot reverse the hashing function so it means that given the output you don't know what is the input and this is not what we call like Collision resistance pretty useful primitive like used in blockchains uh I will and I think that's pretty much it I assume that everyone is like familiar with like yeah okay why is it important foreign so generalized blockchain Anatomy so why we call it a chain because we have a bunch of blocks mined together like linked together because each block contains the reference the parent hash and the previous header contains the reference of the parent hash which is pretty cool and let me remind what the hash the parent hash or the block hash of uh on ethereum is it's essentially the hash of the header uh pretty important to deal with this Primitives and make smart contracts solve for words so accessing six Oracle state does it just keep that in mind let's get to the next part so um no ah I think I'm missing one slide no it's the correct one okay so uh this is an ethereum block header uh as I said we're gonna go through the example of ethereum concretely so a bit of anatomy so to access State obviously we need the state route what is the state route is the root of the miracle Patricia tree of the ethereum state we also have the transactions rule which is pretty useful if you want to access historical transactions like their entire body and receive truth so it's very useful to access any events logs and and so on and all of these are like root of the American Patricia tree America Patricia 3's America 3 just think of it in that way and most importantly we have the parent hash and with the parent hash we can in a way go go backwards I think that's it let's get to Miracle tree so essentially it's this idea that I can take whatever amount of data and I can commit it in a cryptographic manner by using this data structure so on the left side we see a standard Merkle tree so essentially all the data goes to the bottom and we essentially hash it you know what the hashing function is then we combine these two hashes together with hash it and do we keep doing that till we get to essentially one hash and this is what we call the root mark up the Trisha three modified Miracle patricia2 to be exactly the data structure that we use in ethereum what you see here I hope you see on the top we have the state route and essentially the state route is the root of this tree and now how it works and how you should think of this of this it's a pretty complex data structure I don't want you to bother with it today but essentially we have three types of like notes we have Leaf nodes extension notes and Branch notes so Leaf nodes contain data Branch nodes contain data and extension nodes like on the high level just help us to like sort of navigate in that tree but to be honest to deal with storage truth you don't really need to understand this part but to like build on the low level as we do obviously we need to we need to deal pretty pretty a lot with that with that part okay so ethereum State how is it constructed most important takeaway it's a two level structure so I mentioned that the state route is the commitment of the entire state but it's not really true because ethereum is a does it still okay it works it's uh it's account based um and essentially the state route is the commitment of all the accounts that exist on ethereum and what an account is made of it's made of a balance like the if balance it's announced transaction counter storage route the storage route is like the root of another miracle Patricia tree and this miracle Patricia 3 contains the key value database that holds like the mapping from storage key to its actual value and finally we have the code it's essentially the hash of the of the bytecode so main takeaway first we access accounts and once we have the account storage root we can access is it's it's the okay um cool so to sum it up like the background so main takeaways given the block stay true to you can recreate any any state for this specific block on this network and give an initial trusted blockage you can essentially recreate all the previous headers which is pretty pretty cool and important to get the ideas that I will explain like pretty soon okay so as it's gonna be a workshop it's a short one so I won't let you code but I will show you some concrete examples so what I want you to like go through with me today is how we can prove the ownership of Alliance profile on another chain so a bit of background lens profiles are represented as nfts and lens is deployed on polygon I think that's it how do we get to this so first of all the question that we need to answer to ourselves is how does Polygon commit to ethereum L1 because if we want to like let's say prove the ownership of a lens profile on optimize we need to know the state route of polygon but there is pictorial one in the middle so how do we actually access this on each normal one primarily so uh polygon is a commitment commit commit chain and it commits to to ethereum a bunch of things every some amount of time and essentially on L1 we do not validate the entire State transition but we just verify the consensus of polygon and this checkpoints how they call it essentially contain uh State routes and so I mean not directly but we can access them and let's get to this to this part so this is taken from polygons documentation and this is how a checkpoint looks like so as you can see the checkpoint is made of a proposal so who proposed The Block start block and block give me a second I'll get to this and most importantly we have the root question so the root hush is essentially a Merkle tree not America Patricia tree that contains all the headers and which headers the headers in the range of start block and end block cool so now if we get back to the previous part we can essentially prove with this commitment that we know the valid state route of polygon first we even block okay A bit of Hands-On so we want to prove that I own a lens profile on polygon forever so number one we go to the contracts we see a contract we go for it and we see that essentially there is a bunch of logic on top of this erc721 this is like the basic erc71 as you can see it's an abstract contract and it's slightly modified instead of having like a standard mapping from like token ID to its owner we have like token ID to token data token data is a structuring this struct is 32 bytes in total 20 bytes is the actual owner and the remaining 12 bytes represent when the token was minted okay but how do I actually prove it oh and also very important thing when dealing with storage layout we have something that is called like slot indices so each variable has a given slot like in the some sort of meta layout I call it like that it's from the right way anyways this mapping it has like the slot index too I will get to this part in a second why it's two and we have a mapping from token ID so you win to 32 bytes of data represented as a strat Cloud just think of it as some bytes okay so uh I guess most of you use hard hot so I'm gonna present on on hard Hut there is a very very cool tool to deal with storage layouts it's called obviously hard hot storage layout this is how you install it it's literally yarn install hardcode storage layout you add one comment to your Hardware config you write a new script that contains literally eight lines of code you run the script and you get this weird table and what does it what does it really tell you and oh and by the way why this tool is pretty useful as you see this contract is abstract so some other contracts again does it store yeah some countries can inherit from it and obviously while we inherit the storage layout I mean this does this in synthesis can can get more trickier because it also okay so that's it's pretty hard to coordinate like one hand with another hand even though I'm Italian okay anyways um yeah we know this slot in the in the index and that's how we get it we have a column that is called storage slot and as you see underscore token data is marked as two and that's it okay but what do we do with how do we get this storage key and yeah that's that's it let me check the time okay um so a bit of Hands-On how do we get the actual surges it sounds scary and it's meant to be scary so we know the slot index the storage index I want to prove that it's like 0x35 owns with ID 3594 how do we get the storage key we essentially do this operation so we concatenate the slot I mean the key in the mapping which is 35.94 because this is the token ID as you know we have a mapping from token ID to token data token data contains the O okay so we concatenate this with the storage index we hash it all together this is the storage key that we have if you're interested how to deal with it for like more complex mappings than like layouts back the solidity documentation it's explained pretty well so now that's to make sure we got the proper storage key let's just check it how we can check it super easy let's just make a one liter PC call to get this storage at some specific key because the if get storage at so the parameters we want to access the storage of what of the lens Hub lens cap is a contract that's essentially is the representation of these profiles and its address is 0xdd4 and so on and the slot oh is it better oh it's much better and just slot does the storage key is 0x1 so essentially that's the hash the two we got and the result is zero X zero zero zero and we know that it's 32 bytes of data where we have 20 and 12. so let's split it into 12 and 20 bytes and what we have is some number like you can see Zero x a lot of zeros then 62 till D and this looks like a small number so apparently it is a timestamp and the second part is like 35 57 and is literally our address so we got it correct we have the proper storage key cool but how do we actually get to storage proofs so there are standardized method in like the Json or PC standard for ethereum clients and this method is called eth get proof which essentially given the contract address so I better call it account address in this specific case allows us to generate a state proof and the last argument I mean the sorry the second argument is an array that contains all this Storage storage keys that we want to prove uh there is another argument which is 0x1a it's essentially the block number for which we prove the state um yeah let's call this method oh by the way uh you might have a question how do we deal with this method on non-evm chains because for example on some specific Roll-Ups this method is like not supported actually it's not a big deal because if you think of it we just need the database and on top of this database we can literally build this this method we just need to know how the storage is constructed okay this is the proof it looks scary it is scary this entire object is four kilobytes of data and now I mentioned before that the state is like a two level structure first we have a proof for the account itself and then we have the proof for the storage I mean for the actual storage slot it is scary it's meant to be scary one proof is like more or less 600 bytes 700 bytes it really depends like bigger the storage is than bigger the proof is and also more accounts we have than bigger the account proof is so that's a lot of cool data if if you can imagine you can imagine uh and yeah that's that's pretty bad why because we need to pause this proof on the Chain so it's a lot of cool data but okay let's let's try what's going to be the cost on like an evm chain that's the cost it's like 600k of gas that's a lot that kills almost every single application that you want to build on top of this nice primitive so it's pretty bad and why is it that bad so I explained on the high level what Merkel trees are America Patricia trees are on ethereum we use Marco Patricia trees and essentially there is a trade-off that when using Miracle Patricia trees the proof is a slightly bigger it's like harder to decode it because actually we need to do some a bit of decoding there um but we need to do less hashing so this is a trade-off but depending where we actually verify this proof it might be more feasible to verify like prove that is based on Marco Patricia trees or Miracle trees okay but there is a solution and the solution is what if we snarkify such such a proof and we verify this proof inside the start why is it cool because we can like let's say that I'm gonna verify this proof inside the craft craft 16 circuit um and yeah the verification costs more or less like 210k gas the proof is like way less than 600 bytes so it's good so essentially get rid of the cold data because the proof itself can be the private input to the Circuit um yeah we can like use multiple proving system depending on the on the actual use case and now why is it like very very cool so first of all it removes call data second model it allows us to deal with very UNH unfriendly hashing functions or the evm these the ones that we don't have pre-compiles for like let's say Peterson um so it might be like super expensive to verify such a proof on the evm because first of all that's a lot of cool data and the hashing function is pretty like unfriendly but what if we can like do it inside the snark and just verify a snark and yeah so another benefits this really really helps in obstructing the way how we verify this proofs because you don't need to have like one generalized verifier for each type of of proof but you can essentially obstruct it behind behind the behind the snark which is which is great uh these numbers were taken from a very nice article written by a16z like a bunch of uh a few a few months ago um yeah and I think that's pretty much it let's get to the next slide so synchronous cross layer State access so how can actually a control deployed on some layer access the state of another L2 or L1 so I mentioned that we always need the state route but because all of these systems have a native messaging system we can send the small commitments like for example the blockage to like a one usually it goes off-road one and and yeah we can like enroll it or send the state through directly and also we don't need to rely on messaging but we can for example uh rely on the fact that polygon is like a commit chain and all these roll ups like commit from time to time they're like batches and and so on so this is like pretty important and we sort of can get the commitment from which we'll recreate the state directly on on another one and then send it to another um so if let's say polygon commits another one I can send this commitment then to start head and start and do the actual verification cool so now how do we actually do that so let's break the entire flow into like smallest pieces so the flow is the following we need to have access to the commitment which is either a block hash or a state route and again we can get it or I either by sending a message relying on the fact that is this chain commit so in a sense it's still a message we can relate an optimistic manner or we can go even more crazy and verify the entire consensus okay so this is Step number one we need to get the commitment step number two we need to somehow access the state route so the commitments of the state from like a previous block or the actual blog because keep in mind that these commitments are only block hashes and we block hashes we can recreate headers but we cannot access the state okay so once we have the state route we obviously need to verify this state storage groups okay and there are multiple features to do that all of them come with some trade-offs and let's go through all these approaches so approach number one messaging so I can send the message from let's say optimism to Eternal one I can get the opco I can get the blockers by just calling the proper op code and and I get it it takes some time but still I get it this is approach number one so we rely on the built-in messaging system which is I think Fair because the security of it is equal to the security of the rollup and if you're deploying an application of this rollup it's a fair assumption to do so um yeah it doesn't oh they don't know about the downsides so the message must be delivered so it introduces a significant delay especially when dealing with the withdrawal period in the in in the middle uh and it requires we it requires interacting with multiple layers so first you need to send a message and then actually you need to consume it so it's it's not ideal but the trust assumptions are pretty occasional another approach consensus validation by the way this like Gremlin is supposed to verify a bunch of PLS signatures I I hope it's self-explanatory uh okay so maybe a few a bit of an intro um right now we have POS as the native like consensus algorithm on ethereum which is pretty great because verifying the consensus is finally doable because before like verifying the hashing function eth hash which was used for proof of work was very memory intense so not possible to do inside the snark um on chain directly so it was almost impossible to do so um so now we also have this four Choice rule called lmdigos which is implementable but doing all of this like directly is pretty expensive so we need to ideally wrap inside the snark but there is another downside so a few words about the trust assumptions you well you verify the consensus directly so it's it's fine he do you introduce any trust assumptions not really but the biggest downside that generating the proof actually takes some time so to be honest this approach is feasible but comparing to messaging like quite often is like almost the same and you pay a lot of improving time and requires like having more advanced infrastructure okay last approach that we actually use is something that we call like an optimistic relator based on NPC MPC stands for multi-party computation maybe before I explain how it works let me explain the the image I hope it's self-explanatory so it's an NPC protocol we have multiple parties it's multiple parties attached something then we have an observer that can challenge it and then we have finally the commitment given to a specific chain in this case startnet once everything is fine how does it work so we have a set of trusted three layers validators however and they attach the specific commitment is valid so how does it work if we want to get the commitment AKA The Block hash of block number X on starknot then instead of sending a message that will be delayed with a like slightly delayed we can essentially make an uptrend call just get the latest one essentially relay this message directly to start but it comes with a few downsides because while we introduce some trust assumptions but still it's okay okay how does it work so it works in a way that we have a bunch of of chain actors who essentially make this calls and it works more or less like a multi-seek but the reason why we have NPC is because more validators you have than obviously more Securities but more validators you have in it like standard multi-sig approach you have more signatures so more in a way decentralized it is then it's more expensive to verify because you need to verify multiple signatures and you need to like pause the signatures it's a lot of call data such approach is not feasible on chains Oracle data is expensive so I want optimistic rollups and yeah okay so how does it work uh what is uh actually NPC part doing the MPC part is very simple it's essentially signing over like a specific curve some specific payload and the payload is the commitment itself and just say okay so this is how we actually attest but now how wide this approach is called optimistic and why it's still secure so first of all we just posted some something on the actual L2 and as you may know we can send messages from L1 to L2 and such a message can contain like the proper commitment so essentially even if the validator set will lie L1 will never lie so you can just challenge such a message and now to participate in verifying this validators it's super easy because literally two RPC calls one call is gonna check the actual commitment on the actual chain and the other one checks like what is the claimed Commitment if you disagree you just send a message it costs roughly 60 K of gas and that's it everyone can do that um and again the fraud proving window is pretty short because it's essentially how long it will take to generate like the proof of consensus if it's possible or how long does it take to deliver the message and what is pretty cool in this approach it's not gas intensive we verify just one signature so that's about this approach let's make a recap and let's identify the trade-offs so we have three approaches the first one is messaging the second one is validating the consensus and the third one is having this optimistic layer so I categorize it in four categories the first one is latency the second one is the gas cost the third one is trust and the last one is what is the off chain computation overhead why do I even list it because if we do some sort of proving then obviously it takes time because we need to generate the proof so messaging in terms of latency we are quite sad because well the message needs to get delivered so once the message gets delivered to some specific L2 L1 will be able to generate or write a new block so we don't have like access to the newest values in terms of gas cost it's not bad but it's not perfect because we need to interact with two chains at the same time so first we need to send the message and consume it in terms of trust we are pretty happy because we trust the rollup itself and it's a fair assumption option computation overhead we're very happy because there is no computation to do off-chain verifying the consensus so in terms of latency we are set because we need to generate the proof that we've done it it takes a bit of time in terms of gas cost we are I would say sad because we need to verify the actual ZK proof which is way more expensive than just consuming a message or verifying a signature in terms of trust we are happy because we verify the consensus itself and computation overhead it's significant right because we need to generate the proof Final Approach this optimistic layer so in terms of latency we're happy because we simply make a claim and we post it on the other chain that's it gas cost we're very happy because the well we just verify a signature in terms of trust well we are not that happy but also not that sad at the same time because it still can be challenged in an optimistic manner using a fraud proof computation option computation overhead are pretty happy because we participate like an MPC protocol so essentially the overhead comes mostly from communication not computation itself cool so this is part number one these are the three approaches obviously I'm not gonna say which one is the best because all of them come with some trade-offs um okay accessing the headers I hope it's self-explanatory because we literally unroll something from The Trusted input and The Trusted input is again a block hash for a specific block X and if you follow the initial slides that's essentially each block we've given a blockage you can recreate the block header and knowing the block header we can access the parent question by knowing the parent hash you can recreate the previous block header to essentially go to the Genesis block so given this very small input we can essentially unroll the state or whatever was present on the chain from this block till the Chinese is block okay so as I said I'm gonna explain everything on on the example of ethereum and today all the block headers together are like roughly seven gigabytes of data so it's quite a lot but okay this is how we actually do that this is the high level concept and what are the approaches so the first one we call it like on-train accumulation so essentially we do this procedure this computation directly on the Chain so we provide all these properly encoded block headers inside the call data and the blockers that we might receive as like The Trusted input by sending a message relaying it in optimistic manner or validating the consensus and yeah like recursively go through all these headers and and verify them but there are many many downsides because first of all it's vertical data intensive it's very computational intensive and now we can store all these headers on the actual chain but you know even storing on an L2 is throwing 7 gigabytes of data is still a significant cost because the state on an L2 is reflected as call data on L1 so it's still expensive either way but the cool thing is that I have direct access to like State router or anything that I want to access next approach is on chain compression so we can still use the same approaches previously so literally unroll it and process this seven gigabytes of data but instead of like storing then we can just update the miracle tree it's a nice approach but comes again with a few dump sites it's very computationally intense because if you have like millions of headers we need to perform millions of hashes on the Chain that's that's expensive but at least we we save on on storing data and also we need to update the miracle tree which is which is another cost um last downside is that we need to index all the headers that have been processed why we need to index them because if I want to up the access a specific block header I need to provide a miracle path because as we update America 3 and we just store the root in the contract itself then I need to know the path right so I need to index the data and essentially once I it's the moment that I want to access it I need to provide a miracle path this approach is okay it's I wouldn't say way better than the previous one but it's way cheaper last approach so there is a very cool primitive called Merkel mountain ranges love it and the idea is let's do the same that we do previously inside this Arc so we can provide this tremendous amount of data as a private input to the circuit and essentially do the same computation like unrolling inside this record itself and now we have a public input which is the blockage so essentially the commitment from which we unroll it so the trusted input the public input can be literally asserted when we do the on-train verification and why we unroll it we can accumulate inside the miracle tree or American mountain range why American mountain range is is cool because well let's imagine that you want to have like seven gigabytes of data processing once in our life the proving time is going to be horrible and why would you even like prove this commitment for like the entire history like do you really need that probably not so let's chunk it like into smaller pieces and Miracle mountain ranges are pretty cool primitive that allow to do this to do to do this to give you like a bit of intuition how how does it work it's essential think of it as a tree of trees um yeah so once we do all this proving like of chain we simply verify the proof on chain as you know like verifying the proof is is way cheaper than doing this directly on the chain and still we just provide a miracle path and that's it we essentially have access to any sort of data we want let's do a recap again so approach number one answer accumulation on chain compression option compression three categories prover overhead gas cost storage cost actually gas costs should be computational cost okay so prover overhead on-chain accumulation do we prove anything well not really so we are happy answering compression well we still like need to update the miracle tree I think actually there is uh there is an issue here so I'll just skip this part after in compression you're very very sad because well we need to prove actually significant computation so the proving time is significant okay now in terms of gas cause the third approach is horrible because it just costs a lot because we do the entire computation engine compression well we're a bit happy because we just do a bit of computation but still it's a lot of cool data a lot of computation but lost at least not so much storage storage cost oh sorry gas cost in the second approach while we just verify approved so it's cool um okay storage cost for the first approach well seven gigabytes of data it is horrible so we are very sad unchain compression uh sorry storage cost for unchain compression we just uh the root of the miracle tree so we are happy and in the second case we're even more happy because we again we just essentially keep updating a tree and we don't even need to post a lot post a lot of cool data because the cool data we passed is literally just the proof so we're very very happy but again I don't want to say that all of the one of these approaches is the best one because as you see there are trade-offs and yeah so this part is actually pretty easy so as you know as you may notice here I was explaining like the second step when it comes to toolingual storage groups and now there is the the last part which is essentially verifying the proof itself so approach number one is verifying the proof directly on the Chain approach number two let's verify the proof inside the star can then verify the snark approach number three let's verify multiple truths inside the snark and then verify the snark we can aggregate multiple sharks together and so on but obviously there are some trade-offs especially when it comes to proving time um and yeah so now why the first approach is visible on on ZK rollups for example on start medical data is very cheap and what we want to avoid in this specific processes called Data so this approach is for example feasible on starknet but for example if you want to verify like a proof an optimism or a colleague is very expensive you want to reduce it as much as possible so for that reason you might want to use a snark and finally if you have like many slots that you want to prove why can't you just verify them inside one snark you're gonna pay improver time but you just present one proof at the end so this approach is cheaper is the cheapest one but only if you have multiple actions to to take so there are trade-offs so let's identify them categories approval overhead latency verification cost so verifying the proof directly prover overhead doesn't exist latency doesn't exist because we don't need to prove anything verification costs well it is significant because we need to pass call data and we need and we need to do the actual computation so like going through the entire path and each step in the path is one hashing function oh and also let me get back to the previous slide I forgot this is very important why wrapping inside this wrapping inside this Arc is pretty important if you're like dealing with a storage layout that is using a specific hashing function let's say for example Peterson Peterson is not available like on on the EVN like you just need to implement it's not the pre-composed it's gonna be costly but if you do inside the scenario and Peterson is pretty smart friendly it's not friendly then well it just verifies anark on the one and you abstract it so it's going to be way way way cheaper but again there are trade-offs let me get back to this so I went through this the normal competition three snarky fight proof through the overhead it exists so we are not super happy latency we are also not happy because we actually need to spend time on improving this this thing verification cost we are happy because well we we just verify a proof so it's fine and it's an artifying multiple proofs the approver overhead is still there latency is still there it's even bigger because it takes a bit longer in improving time and verification because we are super happy because essentially we can mutualize the cost of verifying multiple proofs by just verifying one single stock proof okay went through quite a lot of things let's put this all together so let's imagine we have three chains and we want to have interoperability interoperability between them so we have chain Z chain X and chain y so it all starts with a message AKA commitment we send a message in order to get the commitment so let's say that we send a message from chain Z to chain X because on chain X we want to access the the state of changing so what do we do once we have the commitment we literally recreate all the headers using one of the three approaches and once we recreated the header is still the pond for which I want to prove the storage I just verify a proof and again for verifying approved there are multiple approaches but now let's say that on chain y I want to access the state of change Z and there is no direct communication between chain Y and chain Z so it must be routed through trainx by the way I'm like talking about this in a pretty abstract Way by Chain X I just mean ethereumulator one um yeah so from chain X I'm just gonna send again the commitment about change the as a message and then simply recreate all these all these headers as you may not notice it's pretty redundant because we perform the same computation on two different chains and we don't need to do that especially if you use like the third approach which is generating the proof on chain but now there is another problem how do you actually know what you should do like you need to be somehow aware of what is happening and for that reason we introduce an API we don't expect like developers to deal with all that complexities choosing the right approach for the direct thing essentially right now our apis optimizes cost wise uh soon we'll be able to optimize latency wise um and yeah and essentially that's it um just about our API I highly highly encourage you to check this out um and yeah like a few final words about the API it acts as a coordinator it optimizes the costs it optimizes the cost because we can batch multiple things and once the job is done you get a notification like via webhook VIA an event like whatever you want so essentially you're not you don't need to be like a infrastructure maintainer and you can just focus on essentially building on top of this primitive and I think that's it um questions so the API essentially is a rest API for now we also have a Json interface we have option entry points so we can request the data like by making enough jingle like calling a rest API or like calling a Json or PC method or if you're a smart contracts like wants to access this data then you just submit an event we're gonna catch the event and later on like after a bit of time fit this uh the specific data inside the smart contract so we have like a bunch of interfaces and by the way speaking of like the off-train entry points once the entire leg work is done on our side you can get a notification it can be like a web hook we can like send you a bit of information like using a websocket uh you can be essentially whatever whatever you want foreign oh yeah so uh that's actually a great question so different chains use a different like storage uh I would say architecture they might commit to America Patricia 3 Miracle 3 uh maybe even vertical tree and obviously like I said having a generalized verifier it's like pretty it's not a clean approach so we essentially abstract it but by using a snark and inside the snark itself we just do the proper work like you know we go for the for the tree like through the for the um through the elements of the proof and then we can like use a specific hashing function so for example now Poseidon is Poseidon is uh is is pretty popular um I think that scroll uses Poseidon and also SDK sync uses Poseidon on the avian like performing Poseidon will be pretty expensive so for that reason you cannot verify the proof directly but what you can do you can do the entire verification inside the snark and then on the one you don't really care what the Strunk is like doing you just just verify it so that's how we actually did it deal with it if we need to have it abstracted we have it abstracted if we don't then we just don't foreign oh yeah oh yeah that's uh that's actually a good question because I think I went super technical uh so actually what we do at Herodotus every two weeks we have some internal hackathons and right before the merge uh we built a proof of concept that we called merge Swap and essentially we allowed anyone to dump their proof of work if on proof of stake and the way how it works we literally build a bridge on top of this technology and the bridge Works in a way that you can lock your if proof of work inside a smart contract on if proof of under if proof of work chain you can prove that you've done it on ethereum proof of stake you can once you the proof is verified you can meet the erc20 token and you can do whatever you want with this token and then if you want to withdraw back to if you're in proof of work you just burn it you prove the fact that you burned on the other side and and yeah that's it also in terms of other use cases I think that cross-chain collateralization is pretty cool because this is the place where you want to avoid latency as much as possible and you want to be asynchronous as much as possible and essentially that's that's what we do here because our latency comes only from from the proving time but again using some optimistic approaches and so on there a lot of things we can do cure I hope it answers the question okay I think that's it I have like three minutes so I guess we can wrap it up and yeah thanks [Applause] thank you okay [Music] thank you [Music] [Music] thank you [Music] foreign [Music] [Music] [Music] thank you foreign [Music] thank you [Music] thank you foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign 