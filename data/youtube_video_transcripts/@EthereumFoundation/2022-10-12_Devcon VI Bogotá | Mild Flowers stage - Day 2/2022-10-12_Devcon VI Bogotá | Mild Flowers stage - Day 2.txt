[Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign everything [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign foreign [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign foreign good morning everyone how are you so let's start I'm Tom I work for semiotic labs and today I'll have the pleasure to present you our work on reinforcement learning in query pricing in the graph so first let me start with the outline I will tell a few words about semiatric what we are doing then I'll introduce what what's really we are doing in this area automated price Discovery I will formulate the problem I will add a few words up when it comes to modeling how we model that problem then I'll briefly went through reinforcement learning Basics let's say it's just a refresher for those ones of you who know or maybe do not know reinforcement learning then agent-based modeling is the the core of the talk and I will show how we're using that for testing system properties then and for testing well different behaviors when it comes to using single and multi-agent setups and then I will show that actually we deploy that that solution in production what are the results and finish with a small summary all right so let's start the first semiotic Semitic Labs was founded in 2020 by Ai and cryptography researchers uh we are focusing on applied research and we are one of the Core Labs of the of the graph protocol also we are developers of Autos the optimal text aggregator uh actually the the lead of that product is giving a talk at the very moment on a different stage so bad luck okay so our expertise we we combine the cryptography with AI That's our core expertise and of course there are software engineering involved and we focus mostly on building infrastructure we focus on crypto economics today's talks actually focusing on this on crypto economics um all right so what was the scenario so the scenario in here is that I don't know which one of you know the graph actually who knows the graph protocol all right okay 50 50. good so basically in this scenario we just focus on on on one part of the graph system right of the graph protocol but imagine that you've got some customers that are sending queries right and you've got indexers that are indexing blockchain and our clinical serving serving those queries so between those guys there is an entity called the gateway which is doing which is kind of like a acting as a query market right so depending on the price the price depending on the quality of service a given query sent by the customers when we would be distributed on this or or that indexer and indexers earn money by serving queries right so they can control the prices of the service queries and the core idea that that I'm going to talk about today is basically wanted to have Dynamic pricing based on query volume received by an indexer and we call that auto aggara and then I will explain the second why we are calling that Auto augura so this is how we model the the scenario right it's it's simplified from one point of view because as you can see ah as you can see there are no no let's say agents in here we get rid of the customers for the purpose of that flooding of those simulations instead we've got a traffic generator right something that just says okay at a given timestamp this is the query volume that you've that needs to be served right we also got query distributor so something that basically look at at the bits of the agents so the agents are putting their bids and then depending on those bits the queries will be distributed across those different agents and those bits are expressed in our domain specific language called Agora that's why that product we call it altag all right because it automates that that building otherwise the users will have to the index cells have to basically create those those price models manually which is kind of tricky right uh all right so some assumptions selected assumptions what we're discovering in here so this is only about how we generate the dot query volume so we've got in here so this is basically the shape that that we we model so the x-axis there is this price query price slash this is a budget of of a customer that customers are saying okay this is how much I can pay for that query this budget is unknown right that's important then it's not sharing that with the agents it's not sharing that with the indexers it's something that that is setting on its own so this is something that we want to discover right and the budget can move right from left to right which means that if the if the if the indexer was set its price its price bit higher than this well he won't be picked right also there is some kind of like a noise we we added annoys to that volume to that query volume and important is to know that there's there's a game happening right in here and the game is so agent's goal is to maximize the revenue but gateway's goal and basically protocol goal is actually quality of service right we want all the queries to be served you know uh five nines and so on right so there's again happening all right so if you know how model let's not now switch gears to reinforcement learning just one-on-one most classical thing so down there in the reinforcement learning we've got two main entities agents and environments agents interacts with environment by executing an action then agents actions change the state of the environment I just got reward in the observations after after oh okay and agent can up their its policy based on the received reward in short that's reinforcement learning so we're using different types of agents in our simulations uh in here I put two most important criterions one are trainable reinforcement learning online learning and the other ones are like rule based so there are some predefined behaviors and also we've got this second Criterion so Visions can be stochastic or deterministic in here I put some some simplified classification of of the reinforcement learning algorithms and I highlighted the two that basically we will be using in our simulation so we'll be using vanilla policy gradients and mostly most of our agents will be using PPL important there is one important type of ideas that actually we we are mostly focusing which are we call them gaussian Bandits so the idea that this is sustainable uh stochastic agent with policy that is represented with a gaussian and then when agent is is supposed to perform an action in something from that gaussian right so imagine that there is this distribution and when that when the digest is supposed to make an action it's just sampling right and in this case Okay the agent sample is zero point a and from that blue distribution second digit is sampled 5.24 right and why Bandit Bandit is because the agent is not building any an internal model an internal representation of the environment all right so going to the most important part so testing based testing properties with edges-based modeling so in here so I already kind of explain this but in the first set of experiments we focus on single agent simulation just testing the properties of an agent and this is a single gaussian Bandit with a modified PPO policy update Rule and the query distributor is like a super naive you know let's say version of that what could happen if here is that we've got access okay depending on the price business is just inverse inversely proportional and in the first step we've got a fixed customer budget with some noise of course right and what we are checking in here is the following we've got market conditions we've got two criterias for every experimental we've got some criterias and a property that we want to test whether the Bandit is fulfilling that that Criterium is is operating the way we want or not in this case the Criterion is customer budget discovery as I've highlighted before and the Agents do not know the budgets right but so we want to discover that so first so that's there will be lots of things happening right because we are tracking many things there are simulations so let me try to let's say unfold that and go step by step explaining what you will be seeing in a moment in in those animations in those videos that we captured during the experiments so first there are five plots right in a single video first one is the most important so what we've got in here is the is that budget right the query Volume Plus Plus customer budget the solid red line is the agent's current policy the dashed line is the initial policy that that agent is starting from second in the second plot from from the top this is basically query volume server by the agent that at that given time step so what happened in here is Agent actually didn't serve a query right didn't get any queries because it's he sampled somewhere in here right like few steps ago he sampled some like the the the price was too high right so the agent didn't get any queries so which which also is shown in here so this is the aggregated query volume served by a given agent in red and the blue ones shows basically how many queries were dropped weren't served at all right so you can see in here there's a tiny tiny step which is which actually is associated with this then in the next plot we've got agents revenue and aggregated agents Revenue over time all right so let's run our first experiment right so once again what we are testing in here what we are trying to discover is word up that where that that budget is right so as agencies is learning right so first the gaussian is was super wide right like it was like much much wider and once the agent as you can see it's something from time to time somewhere in here right above the budget so it's getting more and more sure that okay this is the max that I can get right that's the highest price doesn't that's the highest bid that I can make so as you can see the agency is dropping some of the queries right uh what is revenues is nicely nicely growing all right so first proper property test right that was the static environment so how about Dynamic environment right by by Dynamic I mean that the customer budget can change can vary over time right so in this case we are also testing the same property whether the customer budget can be discovered by the agent but in more difficult market conditions let's say right so once again we are starting agent is kind of a converging capturing the the right the price right which is kind of optimal and then the price changes so what happens is the agent is is not getting getting any queries right so it's saying okay I don't know where am I so it's press the gaussian and once the queries are once again captured it it moves to the left right so it's like okay I was above the budget let me let me go back right and that's happened that is happening over time right so there are plenty of queries queries dropped but still we are we are making some nice Revenue right and we are reacting to the to the dynamic dynamic Market cool second property check so in further experiment we said okay so there are some subgraphs that actually have no queries at all right there is a moment where nobody's is asking for queries so what will happen then so that's a different scenario in this case we call that the marking conditions basically there is no demand for your services right so we wonder how we should add so it will happen in a second right so there will be like three iterations we're recording few of those those videos but as you can see already there's a hint in the right hand corner that something is not right all right gaussian spread this is great and now time set 400 because every 200 steps were like you know you write it okay so what just happened there are no queries so according to the design the agent just said okay let me spread right so I can sample from my wider distribution but actually spread all across the the the the price Biz domain so we said okay actually this is not a good behavior right we if this is happening then the agent can can sample at the same probability like super some super small prices price bits and at the same super high moreover recovery from that will take a lot of time so we said okay can we improve that so we implemented something that we called graceful initial full which basically uh the idea is that okay there's this initial distribution that the agency is starting from this is something that the indexer will parameterize the agent when when when the agent is deployed and where when there are no queries it just slowly is pulled towards that initial distribution right and when the queries will appear once again it will get from there and once again start or something can happen I'm losing from time to time the voice I think okay cool so we pass that property so now let's move into multi-aging simulation so this was great at that point you know we aren't weren't competing with anyone up till that moment right now we are starting with with some we're competing with some deterministic agents the idea is that right now all the all the indexers have that the Agra models that is fixed right they are not changing the models so it can be seen as a rule base right so there's like okay that's my bid and I'm sticking with it no matter what right so this is what we are trying to model here competition with deterministic agents and the property that we are testing is called discover of prized bits of competitive agents right so we want to make sure that we we also will we can let's say compete with them and discover what are what were their bits without actually knowing them without having That explicit information so those three lines in here actually those are the three pre-competitive agents with deterministic policies that they're just set and the blue agent in this case is the one is the Bandit right and as you can see it after a while it's set up like a price just below the the the the cheapest agents which means that it's trying to once again snatch the whole Market if you will see a number of total queries right you can see clearly see a difference right that agent is dominating it right so I will say okay this is nice check cool so how about stochastic agents similar experiments we also want to discover the the price bits of other regions but their policies are stochastic so we model them in a similar way as gaussians those gaussian distributions right but they do not change right so we observe similar Behavior this is great right so the agent our agent right our band it moves somewhere in here and of course it's sampling you know that the red the red the red agent uh stochastic heuristic agent is also something from that distribution so it's changing but as we we can see the agent car moved slightly below and shrank once again to capture the market right all right right I think it's we check that property awesome so how about if we'll right now compete with like make a competition where we've got plenty of those bands deployed and we hope that actually this is the most inter that's the most interesting case right because we want Auto agara to be run by all the indexers in the graphical system so same property slightly different market conditions all right so what is happening in here sadly it's not good it's not good so the ages are acting actually as designed but as you can see they're like fighting fighting fighting going moving moving to the left hand side which means that they're lowering their prices all the time right and as you can see in here oh you can see the labels but this is this is the revenue that they're getting right at a given time step basically it's going it's converging to zero which means that after a while the digest will be basically serving for free right and so that's not a good situation for the indexers right because in here we are not even modeling the we are modeling the revenue not the not the no we are not modeling the costs right so at that point agents will be well indexes will be actually uh well paying for for that instead of making money all right so we call this this this uh phenomenon or this outcome raised to the bottom and we run many simulations we discussed that a lot and actually we realized that well this is the expected Behavior right in this setup so we've got some images that are basically trying to snatch the market right and they're purely driven by by the query volume right so if this is the case and the environment just negatively distributes you know the queries based on the the price bit rest of the bottom is the expected is the expected outcome right so it can be addressed in many ways but we look at the graph protocols what are the features right and down there and what are the assumptions so one assumption is that all the elixirs should have freedom with their pricing right there should be any limitations to that if they can if they want to serve queries superb that are super expensive or super cheap it's okay and all the indexers should be able to make any profits so the conclusion was that actually is the Gateway right that the thing that controls the market should implement this kind of anti-dominant policy and so happened it does it already does so in our next step what we did we wrapped the existing Isa right Isis index selection algorithm is one of the components of the of the of the Gateway right so we just wrapped it in here and using the simulation we are once again testing the same conditions right competing you know many versions many government is competing with each other and the property okay can we discover each other prices and maximize our profits so the outcome is totally different right we ready by changing changing the way the the queries are distributed actually you reach helical consensus right so that all the agents are discovering the markets right what was the market at the same Gateway is looking the ISA is looking at their their quality of service uh at their prices and trying to like feed everyone right not just okay you're the winner you're you're snatching all the queries right so this is really nice so please note that the gate will also leave some kind of budget right that is designed such a way that actually it's it's not enabling the agents to to consume all their budgets right so the budget is there is a portion that is left to the to the let's say to the customer okay so we're on some more simulations of course it's like okay free can we run that with 10 20 agents right will that still work it seems yes it does so we are testing the same properties right we are just a little bit changing the market conditions right this is looking really good if you look at look at the the query serve and the revenue it's like all the agents are you know are are making money which is the desired the desired Behavior right and they are rediscovering the market cool so what we also did uh we we started we wanted to see whether okay so how about different initial conditions right so we are something that different different in Xmas that okay that's my mean price that's my variance I want to maybe move a bit more expensive for like less expensive at the being wanted to see whether the the the the system will converge to that equilibrium and it seems that this is really happening right so disregarding the initial conditions this is good we also run some additional experiments with with frequently stronger policy updates so this one is our modified PPO agent as you can see it's much faster than okay the red agent is the vanilla policy gradients like parallel mental policy or Elementary update rule and the other one is just pure PPO right so our agent is kind of dominating and it's kind of interesting to observe that it's serving less queries right the blue or CN line is clearly below and this is the number of queries that are served but it's making bigger Revenue right so in short is making less work for more money so policy really matters right all right so having those results we are quite quite happy with with those we said okay so can we deploy that introduction and actually that's our solution you know on a Battlefield so once again there's there are many there are many things to unfold in here so let me try to focus on one thing at a time right so first this is the mean so this is going to go where the gaussian is on that on that x-axis right so what happens is that the initial value was too high so the mean was going down first but then steadily it was going up up up good so it was moving to the right at the same time variance was steadily going down which was which means that the gyrogen was more and more narrower which means that I just was like okay I'm I think I'm there I'm I'm good right I'm more confident that I should sample from that our smaller distribution so the outcome is that the reward goes up maybe you cannot see it in here like super nicely but that's the revenue right that's the total revenue and you can clearly see that it's it's going up there is this tendency right so we deploy that the results were great so I think we are well this is the month where I should start telling okay if you're an indexer please go ahead there's a repository to download play with it and we there are some additional materials okay but you know actually in a second okay so let me first summarize so agent-based modeling for crypto economics that was the core of that talk and in here is focus we focus on the diamond Dynamic pricing for the applied to the graph protocol to to automated price Discovery and we use reinforcement learning and and a multiple multi-agent simulation multi-agent modeling for for Revenue maximization so we can show how to use that for testing the properties of the protocol right we came with that framework that okay those are the market conditions those are the properties and systematically start the test right uh our our agents are solutions against against those and finally we have deployed out our in in in production and have shown that actually it makes sense so Future Works of course better policy better updates better policies I just have multiple rewards of taking quality of service into account we are not doing that right now just we are just looking at the at the query volume uh modeling and putting consumer agents into play right so that right hand side that we simplified actually putting that back in the simulation it will give us some more insight in what is happening right so that's kind of that will give a simulator with higher Fidelity and redesigning the game right so in this case there was no information the agents were like acting totally independently the questions if you redesign the game redesign a little bit the graph protocol would that kind of perfect information will help right there are some here are some additional resources so my friend Alex is he there is he wrote this blog post which describes more the technical side of deploying Auto algorithm production he also gave a talk around this about this during the last track episode in June I think so there's a recording on YouTube and of course this is open source so I encourage you go ahead play with it write all those visualizations are there basically you can reproduce that and try to deploy your your your your alt algorithm production right uh finally I wanted to highlight some of the of the other words that somebody is doing so their stock actually that's happened right now that that our friend Matt is giving his the product and research data for us he's giving it right now so that's uh that's that said and Savvy is giving a talk today about our work and our let's say our our our our our our our trip in the to the snarks world and we are focusing on on this is more like cryptography uh we are focusing on verifiable payments for the graph protocol as well so if you're interested in those those topics please come to us we've got plenty of positions opens we if you're interested please go ahead thank you for your information excuse me there's a gentleman hello and thank you for the talk I was wondering if the customer has a very very low budget will you try to match it and make a loss or will you shut down that's a great question actually I wasn't showing this in here right because it's tricky like I said the goal of graph is to have five nines so right now we are working with with the guys that are you know implementing the guide when updating that a little bit because for now if the customer will say okay I'm I want that for basically free the Gateway will still try to distribute that right it's not the best outcome I would say for the indexes right but we are changing that we are we've got a tool right now right so you can think about I was showing you that no we're working in this work we are focusing on the agents right but we developed a tool that enables us to test the properties and the outcome of the whole thing right when there are different players in the game building Gateway so great question it's a tricky right the tricky thing what do we want to achieve we want the indexer to make money or we want five nights right the quality of service to to be super high I have another question sorry uh maybe more technical when you show the experiments whenever the budget of the client was going down the spread of the ocean was was widening but the mean was things the same which means that you are still sampling to the right side where you know it's not the right price would it be more effective to move the gaussian to the left as you spread it yeah uh so what what what's the question actually because you just repeated the okay that's the the design Behavior right if we overshoot if we overshoot with the the the the price then we don't know whether the reason was there were no queries or we were too expensive right so we spread because we want to sample from wider distribution right all right I've if there are any other you know additional questions I'm I'm here I'm I'll be delighted to answer your questions but it seems that my time is up so thank you for your attention foreign [Music] [Music] foreign [Music] [Music] thank you foreign hello everyone I am Darren Langley and I work on Rocket pool which is a decentralized liquid staking protocol so at rocketpool we've spent the last six years building our liquid staking protocol so today I'm going to take you through some lessons that we've learned along the way so in this talk we're going to break down what is a liquid staking protocol what are some of the design choices uh what are the challenges of building a truly decentralized protocol and then what are the opportunities so liquid staking so liquid staking is built on ethereum's proof of stake system um a little tangent I couldn't be more excited that ethereum is now a proof of stake um chain and congratulations to all the researchers uh core developers and um sort of and coordinators that have made it happen it's fantastic so when you stake you are participating in ethereum's proof of stake consensus why would you want to do that so the most important thing is that you're contributing to the security of ethereum you also earn staking rewards for being a good node operator so ethereum has a couple of uh kind of barriers oh sorry ethereum mistaking has a couple of barriers first of all you need technical experience to run like a validating node you also need a fixed 32 eth per validator um the state eth is actually kind of naturally illiquid it exists on the beacon chain earning rewards but you can't really use it for anything else which is by Design but most of the 32 East that you stake is not actually at risk except under the kind of most dire of consequences or situations uh currently it's also a one-way thing so withdrawals are coming soon um but at the moment you you're staking and you're that's it so uh the rewards from staking East come in two juicy flavors the first one is consensus rewards um which comes from new eth inflation all right and then you get that by kind of Performing your node operator duties so that's a testing being part of a sync committee and proposing blocks as I said before um withdrawals aren't available at the moment but they will be um potentially after the Shanghai hard Fork uh execution the next type of uh reward is the execution rewards they come from users so they uh come from Priority fees which are the non-burnt part of a transaction fee and then potentially Mev if you're extracting Mev that's actually available today and pretty much in real time so how a staking protocol captures and then distributes those fees or those rewards is key to its design so how does liquid staking work a liquid Staker deposits any amount of eth into the protocol and in return they receive a liquid staking token and the ease that they deposited gets matched with the node operator the node operator interacts with the protocol and deposits the eth into into the beacon chain node operators then earn rewards by being good node operators and the liquid staking token accrues the value as yield set up to unstake a liquid stake it burns the liquid staking token for and gets back to their eighth or actually more East than they put in generally um and you you can either do it kind of in a primary mechanism or you can do it on a secondary Market so this is the very basic outline of how liquid staking works but as we'll see every step has some design choices and trade-offs to to be made so why liquid staking so it's kind of important to know why you're doing this what benefit does it serve so it turns out that there are significant benefits to ethereum as a whole so liquid staking encourages greater participation that in turn provides greater security and decentralization there's a little star there because it only contributes to decentralization if the validator set or the node operators are decentralized and not just one entity uh liquid staking facilitates unstaking through like a primary mechanism through the protocol itself um or on a secondary market so it reduces validator churn it also Fosters Innovation and capital efficiency and through the use of these liquid staking tokens in defy okay so on this section I'm going to focus on liquid token design but there are some equally big design spaces on the Node operator side and and some definitely some interesting challenges on that side as well so first thing you realize when you're designing a liquid staking token is that node operators earn rewards at different rates they may also be penalized and in worst case scenarios they can be slashed so there's a couple of ways of handling this the first way is to have a fungible token that shares rewards and losses across the entire protocol or you can have a non-fungible token where rewards and losses are specific to each node operator or each validator then you can kind of have this hybrid of the two um although you've got to be careful you don't end up in a worst of Both Worlds each of these approaches have some different trade-offs and you have to kind of weigh up um which ones so the next one is safety so there is eth backing you know backing the liquid staking token So What mechanisms are in place to protect that collateral ethereum's proof of stake system is a very forgiving protocol but it is possible to lose your your stake either partially or in like the extreme cases fully safety mechanisms built into the protocol need to account for things like slashing protection uh aligning incentives uh and general risk management so Rewards so how are rewards delivered to token holders how does the token reflect the yield that the protocol actually is producing so generally there are two approaches to this there's rebasing and non-rebacing so a rebasing token its exchange rate is like a notional one-to-one with eth but the quantity increases so the token increases in quantity over time that's how the yield uh represents itself or gets delivered with a non-rebacing token the exchange rate increases but the quantity is the same quantity stays constant so the the actual token increases in value over time um and there's some there's definitely some kind of pros and cons of each of these may need that so with the rebasing token it's very simple to understand you're literally getting more of the token uh but it's extremely hard to integrate so when the if the quantity is changing all the time that's not really compatible with most D5 protocols with a non-rebacing token uh it's it's harder to understand and if you go down that route you'll spend most of your life uh explaining to people what a non-rebasing token is um but it is much easier to integrate uh it's it's just a standard erc20 and it's supported by most D5 protocols and actually turns out that a non-rebasing token is more tax efficient as well so it depends on your jurisdiction um but within a rebasing token you have like a taxable event every single day whereas with a non-rebacing token you have a taxable event new stake and when you unstake so liquidity by tokenizing um steak teeth it can be traded on secondary markets such as exchanges this provides liquidity for people to unstake their eth do the price you pay on secondary markets is dictated by the market so it may present a liquidity discount or potentially a liquidity like a premium with large orders there can also be slippage and so staking protocols can offer like a if they can offer a primary mechanism for unstaking eth so that you can allow um a liquid staking token holders to actually swap back without the discount and without the slippage both of these mechanisms of undertaking kind of reduce the need for validator churn okay so now you've got your token out in the wild I mean everyone's going crazy about it um you need to start building utility these come in the form of Integrations with other D5 protocols so there's kind of like a Maslow's hierarchy of defy Integrations um it goes something like this uh you integrate into wallets and explorers first then you build up some liquidity this is the bit that takes time A lot of time to build up liquidity you need to get a breadth of liquidity and depth of liquidity that's important to kind of get to the next phases you also need to get good liquidity on layer twos because you want to build up um support in those in those ecosystems and also provide good ux for stakers so then the next layer is Oracles so once you once you've got some liquidity you get oracles the next one is D5 protocols particularly lending platforms but also kind of options and index platforms and fixed income products and all loads of things the last one is volts so volts are kind of like yield optimization and platforms and they sit over top with loads of different um D5 protocols and optimize that yield so a decentralized staking protocol is a set of smart contracts that mediates between depositors and node operators kind of escrows funds into the ethereum's proof of stake system and then back again on withdrawal the protocol transparently distributes those funds and ensures that each party receives what they expect in short withdrawals away of most of the action happens okay so it's at this point in the presentation I need to give a bit of a disclaimer uh the information on this slide is not final uh the capella specification is still being kind of drafted but at this point this is what it looks like so after the Shanghai hard Fork hopefully um consensus rewards uh will be fully withdrawable or partially withdrawable so fully withdrawn is a node operator initiated thing so a node operator submits an exit message they get processed by an exit the beacon chain execute they come into this withdrawable state and then their funds are returned to a withdrawal a withdrawal credential at that point they are no longer validating they're done as a validator okay that's full exit with partial withdrawn you're still validating and in fact this is like an automatic and ongoing process so um this is this essentially takes skims the rewards um off the top so you have 32 Heath is your kind of initial deposit and then anything else you make on top of that will be kind of continually skimmed and sent to your withdrawal credential automatically by the um the beacon chain and the consensus clients so withdrawal credentials are a key element of the ethereum spec um and they come in like two types there's a 0x0 which is this BLS signature credential and 0x01 which is an ethereum address that receives the withdrawn funds so most credentials in use today are 0x0 and that's because they were introduced first and then much later 0x01 came came ahead so in the current plan 0x0 credentials we need to be converted into a 0x01 to allow you to withdraw and there'll be like this special kind of migration process that'll that'll facilitate that so 0x01 is important because it facilitates the development of these uh non-custodial staking protocols um essentially because the zx01 address can be a smart contract and so that smart contract can you know take custody of funds it can deposit them into the um the beacon chain into the into the deposit contract and then when it withdraws it can then distribute the funds to all the parties so building a decentralized staking protocol is easy not so much there are significant challenges to developing a truly decentralized staking protocol so as for permissionless being an open and public piece of infrastructure is key to a protocol success and it's and it's also it's important for its alignment with ethereum being permissionless is a is a noble Pursuit and it's critical for ethereum to for retaining its credible neutrality so being permissionless subsequently leads to trustless designs which I believe are much more resilient in the long term like ethereum itself decentralized staking protocols have to rely on a combination of cryptography and crypto economics to balance incentives and penalties ensuring that the participants have aligned interests but it is challenging but personally this is what it makes it into this is what makes it interesting work so scaling being competitive market participant is important to ensure that ethereum remains decentralized scaling a decentralized staking protocol is much harder than scaling a centralized provider the two aren't even really comparable a decentralized staking protocol has to rely on Ingenuity to scale but never lose sight of its kind of core value a decentralized staking protocol is actually a community a community of node operators who are passionate about securing ethereum scaling is certainly a challenge but it is one that can be overcome so Reliance on oracles so semi-trusted oracles are essential for decentralized staking protocols today the consensus and execution layer are separate Concepts and they've only just been merged together so oracles are required to Aggregate and Report validator performance information to the protocol smart contracts there's actually an EIP that's being considered for Shanghai and it's eip4788 what it does is it adds access to the beacon chain state route to the execution layer so that smart contracts can verify proofs about the beacon chain state this is key to reducing the role of these semi-trusted Oracles in the decentralized staking protocols but it is important generally to combine those two concepts and allow Innovation to particularly around validator status and finality on the execution layer so what are the opportunities okay so this might be a bit counter-intuitive but ethereum doesn't need more stake it needs more individual node operators so I believe this quote is from superfizz so thanks Fizz if you're watching um decentralized staking protocols have an opportunity to redress the balance so by lowering the collateral requirement for node operators there are more potential node operators available by streamlining setting up a node and running a node it's easier to onboard new node operators and if you lower the barrier to entry for spinning up a staking business built on top of a decentralized staking protocol this allows a whole ecosystem of Niche staking businesses to compete with large providers more node operators the more node operators we have the more decentralized ethereum is being a node operator is not as hard as you think okay so execution rewards moving so execution rewards so the ones that come from like transaction fees and potentially Mev are extremely variable this is because Brock proposes are chosen at random so at this time if you have like one validator you'll receive approximately five proposals per year um on average but you could get two or you could get ten so this variability hits small node operators hard if I pop it harder actually hard the hardest um as a decentralized staking protocol There's an opportunity to provide a reward smoothing pool that participants pull their proposals to achieve a consistent return rather than this highly variable return so a smoothing pool actually levels the playing field the small node operators they can kind of compete with the larger node operators this is a particularly interesting when you think about Mev Mev you can have on average you can you can earn you know a decent amount from Mev but every now and again there'll be one of these Lottery blocks the highest block I think that's recorded was 100 East block um that doesn't happen very often uh but that hence why it's a lottery block but if you do get one of those blocks then this is what the smoothing Port helps with so I'm Darren Langley um I'm from rocketball you can catch me on Twitter and that's me thank you very much [Applause] hi my name is Benjamin with rubicon.finance I wanted to ask I understand rocket pull nodes are collateralized with RPL how do you protect against a tail risk scenario where if the price of RPL eth is is dropping the average collateralization of the network uh starts to fall and how can the protocol like uh become robust uh in light of basically uh you know relying on RPL youth price to guarantee security so in natural fact uh so RPL is used as a backstop so the the first thing that they we so with a with a rocket pool node uh the node operator supplies 16 each and the liquid stakers Supply 16 Heath um and then the node operator also supplies some RPL like it's minimum of 10 so it's about 1.68 and but the eth is hit first so if they if they get slashed or um if they you know if they're an absolutely terrible node operator um then essentially their ether it gets hit first and then RPO is used as a backstop because you can actually lose a little bit more than 16 in like a like the worst worst case scenario yeah multi-part question okay so first part um how hard is it to run a rocket pool node like do you need to know Unix or can you just run a script and it installs everything second part okay how many nodes can you run on one powerful PC right cool okay good question uh so running a rocker pool node you do need to know probably a little bit about um Unix maybe you can run it on different things we don't necessarily support Windows um but you can run it on on then a Linux box you do need to know a little bit about Linux uh speeding up a rocketball node is I think pretty much four commands um in your end there's four commands and you've you've spun up a validating node um so we've made it we've streamlined the process we have this thing called a smart note stack and it streamlines the process of actually spinning up a node we connect everything up for you we handle upgrades for you it's it's really easy um the other question the other part of that question was all right that's right all right that's yeah that's that is a hard question to answer so terminology wise a node is like the box that's running it and then you have validators so you can have a lot now whether now whether you whether that's a good idea or not is is another is another matter so what you would tend to do is you probably want to distribute across multiple nodes um if you had a lot um but you can run a lot um different clients different consensus clients and different execution clients have different performance profiles um but you know yeah you can run a lot on I don't think we've I don't think we've um we haven't modeled it uh but it is it didn't like the hundreds yeah it depending on it also depends on the box if it's a powerful machine then you can do that yeah cool yeah all right awesome thank you very much guys [Applause] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] thank you [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] thank you [Music] [Music] thank you [Music] [Music] foreign [Music] foreign thank you so hello everyone excited to be here talking today on the patreon of Zaragoza which is the pillar and I'm Alex binas I'm a marketing manager and cow protocol and today I'm going to talk to you about how thus Cloud protocol reviews Meb by optimizing transactions through patch auctions I've broken down the talk into four topics which is the first one Meb The Good The Bad and The Ugly then what is called protocol then the difference in the mindsets of maximization versus minimization and lastly where are we heading so maybe The Good the Bad and the Ugly will ethereum remain under this utopian dream of building decentralized Finance for fairness for all the users or will the dystopian site take over and as always will the bigger fish eat the smaller fish so first let's try to Define what is Meb and maybe was first coined as minor extractable value but now has been recoin as maximum extractable value and maximally extractable value is this hidden power that the block producers have and what this hidden power allows them to do is the ability to arbitrarily include exclude or reorder any transaction in the block of course this is a bit of a problem because right now with the introduction of Mev boost we're incentivizing the block producers to only care about the maximization of the profits and not care about like the overall healthiness of the network before block producers no miners only had the source of Revenue which was gas fees but with the introduction of Meb gas and also the boom of D5 summer then suddenly maximum extractable value became a thing and ethereum kind of like turned in the wrong direction and starting hurting what makes ethereum great which was their users but in reality where does Mev come from now Mev comes from two ways from either the execution layer and the application layer in the actual education layer it comes from the fact that as we just saw that the block producers have this sort of hidden power towards the transactions in the wrong wheel and basically like as we all know and we've seen data shows that they're constantly doing this but not only that it also comes in the application layer because it is important to realize that the different trading mechanisms that you use when interacting in ethereum will expose you to less or more Mev so be careful right there choosing your trading mechanism when you're executing your trade on ethereum not only that but it also comes from the fact that we're always going to live in a world that there's going to be difference in prices between the reality that's happening of chain and the reality that's happening on chain in other words there's always going to be latency Arbitrage kind of like this competition to get the opportunity to buy cheap in either a uni shop or whatever and kind of sell at a higher price in a centralized Exchange and lastly what is truly like a little bit the nonsense in here is that even though the transactions are truly atomically atomically happening in ethereum the prices are not we often see that we have blocks that have multiple transactions and they're happening atomically at the same time but if you kind of look carefully at the data you can see that there's a price disparity with the same token Pairs and even in the same token and even in the same trading venue this is a video of a big issue in finance because it's very nonsense that the transactions are happening at the same time but the prices are completely different and now that we know the definition and we know where Mev comes from let's see what you can do with the power of inserting transactions at your own will so there are three main forms of attack in mov they're well known the first one is front running and basically from running consists of any transaction that has any sort of value such as like a Liquidation on our Arbitrage we'll go from I get it to basically the block producer get it and they do that because they have the power to insert the transaction wherever they want but not only that the irony is that if I send the transaction to kind of get the liquidation through the mempool what the block producer will do will replicate the transaction put their transaction in front of mine and even like the ironic thing is that my transaction will also be included after so that they can earn the the gas fees that I've paid and and in the end my transaction gets reverted then we have back running that even though it's kind of notorious that people argue that back running is less harmful in the ecosystem one can argue that it's still back running is harmful because it is still taking away the opportunity from someone else and basically this is if you're like a large headphone and a whale or like you know a big Trader and you're you're doing like a let's say a million dollar trades against a single amm you're gonna like leave The the Reserve pool Reserve prices of that amm on balance in comparison to the other amms of course you know you're gonna do that because you know what you're doing and that means that your first trade is going to create kind of like an Arbitrage opportunity to rebalance the other pools and you can try to sequence the trades in a way that you are the ones that that's that opportunity but because you don't have the power to kind of sequin the trades in the order that you want mainly as soon as your transactions hit the main pool someone else is going to take that opportunity from you and lastly the most famous One the sandwich attack and the one that is more helpful to the users and the sandwich attacks happen because in reality you think that you're placing a market order when you're trading on an amm or or an aggregator but essentially what you're doing is the setting a limit order that is controlled by your slippage tolerance and this is how the mine how the block produces no take advantage from you because what they'll do is basically insert a transaction in front of yours that pushes the price up to your maximum slippery spot slip is taller as paintball then insert your transaction so it push the price even even higher to later on execute another transaction in the opposite direction and what this finalizes is that the miner has made has been able to make profit at your cost by basically buying low and selling high at the expense of you getting a very bad price but how damaging is Mev well according to Eagan 5 website which is a website that specializes D5 and Meb analytics throughout the last month of September in the last 30 days 90 of the top 10 exploratory contracts are either amm or aggregators not only that but 10 out of the 13 billions that Mev volume has moved in September have been have been used to perform sandwich attacks and as we just saw this is the most helpful attack for the users because it gives them the the worst price and and lastly a change sign analytics using the data from the flashboards theme has estimated that since 2020 there's been a total of 1.3 billion dollars of Meb extraction from the users now why are why are these numbers of programs these numbers are a problem because if ethereum truly wants to become the world com the world computer due to its ability to settle transactions in a decentralized open in an atomic manner we need to try to fix Mev and we need to try to fix Mev because I think we all want to and believe that ethereum will indeed become the world computer and absorb the all the transaction value of the world and then if that happens the logical thing to say is that at the current stage of things also mov is going to increase and if maybe increase this is going to poke The Regulators and basically The Regulators are gonna come in and kind of try to try to dictate how we should operate the market we've already seen that in several reports from the bank of international settlements how they try to establish like whether a front running or sandwiching is illegal activity or even more particularly in in Europe where I'm from and we can already see this in the Mika regulation that current like the current forms of from running or sandwich attacks will be deemed illegal under the asthma Market structures this of course is a problem because I think we all believe we we also want to have The Regulators to come in and dictate how the market should run but rather we want to just build Technical Solutions that like address this problem so that the regulator doesn't have to step in and kind of like ruin the party so what is comfortable the protocol is the trading mechanism that underpins hopefully now the famous UI culture of Exchange and it's similar to an amm or a aggregator but with a fundamental smart difference that we add a theme matching layer on top of them that allows us to execute um sorry that allows us to aggregate multiple traits together and execute them execute them in a single ethereum transaction not only patching allow us to do this sort of aggregation for transactions but it also allows us to give a structurally better prices via house which stands for Coincidence of ones and these give better prices because if the same they counter opposite orders are in the same batch we can match the user peer-to-peer and therefore they don't need to go to the liquidity pool and thus safe on transaction and transaction costs or liquidity provider fees and on price impacts batching also allows us to be the sort of metadex aggregator similar to scanner to Skyscanner we can query all the amms or all the liquidity on chain as well the all the other aggregators so that in a sense your kind of baseline's price is always going to be the price that the amm or the aggregator gives you but because patching offers certain benefits we all like we almost always improve it then it also offers nav protection because the one of the key reasons for batching is that the batch executors have to guarantee the price that you get so either you get your price or your your trade is not going to go through and those you're not going to be faced with failed transactions and lastly for all the if Maxes that we're all here it also allows us to do gustless swaps which basically allows you to pay the gas fees in your cell token and you don't need to eat actually for trading on on cow shop so how do we actually reduce the Mev on chain through batch actions well the goal is that through batch auctions we want to reduce the overall interactions with me amms and if we have to go to amms then at least we're going to try to diminish the level of exposures that that the trades are gonna have against Mev by applying certain rules to the batch auctions the main way to actually reduce mevs as we do so is executing Coincidence of ones and because you don't need to trade against a liquidity pool and the trades are much completely peer-to-peer therefore there is no matter in the order of those transactions because it's a pure simple transfer between two parties and and there is nothing like no one can get in the middle then second they if we have to go to the amms then we try to break order dependency by having a single price per asset per batch and basically this is achieved because every single batch settlement it happens in cup protocol has uniform clearing prices and uniform prices that means that like the people that are trading the same token pairs in the same parts are always going to get the same price unlike what is happening right now that we see a lot of price disparity and this is guaranteed because the executors of the Vats of the batch commit to a price Vector that regardless of the order of the trades are always going to give the same outcome in other words no the order of the factor does not does not alter the product and lastly batch auctions allows us to like re-aggregate a fragmented liquidity via ring trades this is something very interesting because it allows us to actually kind of match different users without the need to go through many like that they need to go through many liquidity pools and for example here we have a badge where we have four different users and where we have kind of like a coincidence of one and a ring trade where the four different users are trade are trading four different tokens and they're all they're all trading in the same batch and without having to touch an amm because each user is kind of providing the liquidity needed for the other user kind of like in the in the circle so in the first one it would be like in the in the oh sorry in the in the top now we have die against Aura and they actually the liquidity for the first one that's selling die goes to the other one that's trying to buy usdt and so on and like we make a circle now use the C USD and USBC against aura now how do we actually compete for Mev Mev minimization how protocol outsources the settlement of these batch auctions to a competition of third-party algorithms whoever gives whatever algorithm gives the best price Improvement for them for the user and thus that minimizes me with the most is the one that actually wins the right to settle the to settle the watch action not only they win the right to settle the batch auction but it also gets rewarded for doing so currently right now there are 12 different solvers competing against each other for kind of like minimizing the amount of Mev or in other words maximizing the value for the users and they can be grouped into four different types of solvers we will have single order servers that basically specialize on just settling one order in a batch then we will have patch caldex aggregator solvers that specialize on finding this Coincidence of ones amongst users then we will have mixed integer programming solvers which kind of are able to settle multiple trades against uh like unisoft P2 or UNICEF P3 style kind of pool and lastly we will have the quasi-linear solver or quasi model that specialize in settling trades of the batch auctions in more fancy pools like balancer so what are the difference between the maximization and minimization mindsets is really mov protection that is all there may be protection out there the same thing well of course not although one can argue that in my opinion no flash was technology and corporate protocol technology are kind of two sides of the same coin there are two sides of the same coin because both Technologies Outsource the settling of the transaction to a competition of third parties and in that competition the kind of the winner is the one that optimizes the bundle of transactions in the possible way for the users the difference here is like who is the monarch in Bosch systems in the case of flashbots no they're trying to maximize the value for the validators and those kind of like maximizing the attraction from the users but in the case of cow protocol we're trying to maximize the value for the users and thus minimize the value for the validators so whether flashback goes and and Mission well flashbacks is a collective group of individuals that have done a lot of work in kind of like highlighting the issues of Mev in the space and kind of educate a lot of people and and make us realize what are the consequences of building systems that are prone to Mev but from their website you can take that their goals is divided into three steps the first one is to illuminate the Dark Forest and they've done so via Mev aspect or Mev Explorer and basically this helps educate a lot of people within the ethereum community by putting a number and seeing how helpful nav is the second one is democratized instruction before mebkf or Mev boost the extraction was only available to a very few amount set of players but now with the democratization of their tool a lot more people can do it and while it's good there are a lot more people can actually like you know do extraction we need to try to realize that we're kind of democratizing a weapon that is harming the overall like ethereum users so it's kind of like a tricky situation here and then lastly Distributing the benefits this is still a not very clear part or very clear how they're gonna achieve it but right now one can say that like they're redistributing the benefits to the wrong people because the benefits right now that are coming from Mev extraction are going to the validators which are not the ones that are creating the opportunities because in the end the opportunities are created by the Traders are created by the Traders yeah so what are Cloud protocols goals and missions well how protocol goal is to actually achieve one single price per token per vast and we want to try to achieve that by basically executing one batch settlement per every block but if we are to compare as well into three main points we will have that the first one is protection from the Dark Forest and basically we do this by executing The Coincidence of ones where the user doesn't have to go against the liquidity pool and also through our delegating trading model execution where the user doesn't set in the transaction directly to the mempool but rather sends it to an offset an off-chain set of relayers that would be the ones that kind of like handle how they they transactions hits the man pool then the second one is if we have to go to the amms and if we have to go to the mempool we try to avoid extraction by enforcing certain rules to dispatch actions and the most important the most important rule is the uniform cleaning prices which in the end breaks the order dependency and thus makes those trades much more or less uneviable and lastly distribute the benefits if you're trading on Costa parade today you already know this if not I recommend you doing so but already today it's kind of like the price improvements that this so-called solvers or Searchers are able to find are given to the users instead of being kept for them and for example in this case we can see the kind of like the patch in the middle that there was like 18 different trades and on average there was a surplus of 2.5 percent for each order but to sum it up how are they different no in the case of cow protocol technology if we look at the key metric that evaluates the success of the technology is the overall reduction or overall reduction of the exposure of Mev versus in the case of flashback technology the key metric is how much Mev is being extracted looking at the world Democratic section no one can argue that how protocol technology is democratizing the tools for protecting users versus flashback technology that they're democratizing the tools for extracting value from the users or from attacking the users or hurting the users and if we had to look at how these two different Technologies interact with uh on chain the goal of cow protocol Technologies kind of reduce the amount of amm interactions that the users have because in the end like amms are the reason that the Mev exists but in the case of car in the case of flashbots they want to maximize the amount of MMA interactions because these ways they can maximize the slippage tolerance from your trades and cut off extract the most value from you and lastly if we look at how both systems kind of like play by the rules in the case of car protocol we see that the the more money to the users the less mov and the higher the chances you have for winning and the right to settle the batch and in the case of of flashbots the more money that goes to the validators the less money that goes to the user and therefore the more Mev the higher the chances you have of winning so where are we heading as I said before not All Is Lost like we believe in building Technical Solutions that can kind of address this type of problems and we think we're on the right path to kind of like reduce all the big chunk of nav that is happening right now on on a stadium so what is the future of of the access we believe that the future of Texas is one that focuses on fairness and cost Improvement for him for the users rather than just let them trading kind of like some optimal ways it is one also that offers Mev protections because as we said before if the transaction value in ethereum is going to increase me protection is going to become more and more important it does it's also one that puts the users at the center of it now in the end in our system the users are the monarchs because they are the ones that are creating the opportunities for different price improvements so they they are the ones that should actually be rewarded for it and not other parties in the system and lastly it's one that abstracts users from complicated technology it is unrealistic to expect that everyone is going to know what is an RPC or how to add an RPC or even stay up to date on all the different liquidity pools to trade your token or what is their like hottest new D5 like trading venue and how are we going to get there no what does the future of Finance look like well the future of Finance looks like the one that we have a cow patch Builder where this Builder commits to have the first transaction of the block to be a cow protocol settlement and this is important because if the first transaction of the block is account protocol settlement then we're gonna have uniform trading prices in all the trades that are happening within there and therefore there is no Mev that can happen with the trades then if that's the case and we have the first transaction then the transactions that are going to come after are gonna actually be like true intents of Arbitrage from the market and we're going to be able to like have the overall ethereum ecosystem in a sense focus on the real market efficiency because right now a lot of the Arbitrage that's happening is toxic Arbitrage and it's not arbitrus from the market but rather The Leftovers that the Mev attackers are kind of living living in there and lastly it would be like the combination of the utopian side of the flashbus technology with cow protocol so how does this look like well currently we have this this scenario right now in ethereum where we have the Maple and we have the different searches or private order flow that submit their bundles to the Builder and then it goes to the relay then and the validator but for us the key is to kind of break the the order and kind of transform Searchers into Cloud Builder and put cow protocol and Cal shop in the middle of it in a way that if we make if we convince Builders to focus on the utopian side and kind of shift the revenue mindset from hurting users to actually protecting non-maximizing to the value of the users then we're gonna have a much overall better outcome in within ethereum and Will the Market fix Mev well the question is that if Meb maximization takes over we will have not built the future of Finance but rather have just built the new wall system but just with new rails so help us build the future of Finance because we're hiring for a lot of positions and thank you very much questions foreign Alex ah sorry no problem um question so the batch auction process you're describing uh is one in which you have a bunch of orders from from Individual Traders and then a collection of solvers each submit a price Vector that can satisfy all those orders how do you get the individual traders to commit to an order without them knowing the final price well basically it's like they see in that like in a way worse is that when you go to Cloud swap you sign a message with your intent to trade and basically your your Baseline price is the quarter price that we give you so from there the solvers start working on trying to improve the price so you're always at least a base you're gonna get what we have showed you in the UI and from there if there are uh if there are price improvements that the solvers can find be a coincidence of one or some sort of batching then it's gonna be rewarded in the form of surplus from you so you're gonna get more how's the course price calculated by basically looking at their own chain but the the on-chained prices and from there like we we discount the gas okay here sorry thank you for the talk um uh the question is do you have any guarantee that the individual user is going to have its trade executed in a batch like the way it works is that you sign a message no and then if if if your prices get out of market then the the the trade is not going to execute but you're not going to get it uh you're not going to get charged for a failed transaction or anything because simply the trade doesn't execute so in a sense like the guarantee is that if you're if you're if your order is more or less within the market pricing then within a matter of 30 seconds to one one minute is of course gonna be included hi um I'm here sorry hi thank you so I was just wondering you can pair flashables in Cal swap but isn't Cal swap solving for like the subsets of the Mev that flashable Society software and also how do you feel about Mev maximization with Mev rebates being equal to Mev minimization can you repeat the question sorry thank you yeah um so you compare flash Wilson color swap but um how do you feel about uh to me it seems like telescope is solving for subsets of the Mev that flashbulse is trying to solve for as in you for example you're doing so for electric pairs you only saw for Mev that goes through amms but flash boss is not trying to solve for them maybe flash was is just making a tool that is making mbb extraction more excessively common for the for whatever user but in the end the goal of their action is that the higher the more Mev the more I'm gonna pay and the more likely I'm going to be included like my bundle is more likely going to be included on the on the next transaction and they do that because they're in the execution layer and the problem is that they are able to do that because the trading mechanism that people use like amms expose their users to Mev but what we try to do is kind of like fix the Mev at the application layer so that then flashbacks wouldn't have like a way to extract Mev so if like you know in the end if if the servers kind of like find the optimal prices and uniform clearing prices the Mev that will go to the block produces is non-existent because there's no like it doesn't matter the order of the transactions [Music] thank you [Music] foreign [Music] [Music] foreign [Music] [Music] thank you foreign foreign [Music] foreign [Music] thank you [Music] foreign Smith I'll be talking about does it make sense to Aggregate and average a fee recipient rewards uh uh commonly referred to as Mev using a smoothing pool and uh very glad to talk about a number of things I'm with a a firm of my own called nextblock Solutions but I'm also a rocket scientist with the rocket pull protocol and most importantly a member of the node operators Association so one of the things you can do is you could download a copy of this report that was published early summer there's a QR code here in the corner that you can grab but this this presentation will cover the highlights of that report or at least give you a good orientation to read it but I do encourage you to go there for more additional uh information that I can't cover in in 24 minutes and nine seconds here so to begin with let's let's spend just a short second talking about the taxonomy taxonomy of Mev right you've heard a lot of talks about it one of the first things you might be asking is why would it make sense to aggregate rewards in a smoothing pool this is something that kind of comes from the mining industry right where the chances of you actually solving a cryptographic hash and being rewarded is so rare that it makes sense to go in collectively with other individuals to have a constant income stream this here is a graph that shows the uh the probability over five years of receiving a block proposal right so if you have a single validator and sometimes I might refer to those as mini polls and Rocket pull terminology but validators over five years given four thousand two hundred four thousand uh 25 validators or a little bit more than that now on the on the blockchain uh you will receive on average about 30 proposals over that five-year period now again that's just an average right you can see in the the probability histogram you might be unlucky and might only receive 12 proposals over that five years you might also receive 50 or so right and it scales linearly if you have two validators you can see here that the average is going to be 60. we think that there's no advantage to having more validators uh in there if each each proposal was rewarded uniformly with the same amount of Beacon Rewards but obviously in addition to getting Beacon rewards you'll also get a chance at Mev right so let's just take a little bit at looking at Mev right you know the definition right I think one of the things we lose a little bit though is that it's viewed somewhat as a theoretical limit it's the maximum extractable value and I would argue and I think a lot of the data supports that we're not actually extracting that full amount right so the flashbots team has used this term I don't think it's quite as catchy called rev which is the actual amount that we're able to extract From the Block right and there's some amount of of earnings right that is not captured right it's just left on the blockchain for the the next block I'm just going to call that eulage until somebody comes up with a better name right there's some really great metrics coming coming uh out now uh post merge about if we look at this rev how much of it is actually going to the Searcher right how much of it is going to the block Builder right um what part of it may go to the relayer right now the relay players seem to be providing that as a as a no fee service right but there's a potential they might capture some revenue from there and most of it from the metrics we're seeing right now is going to the block proposer right that's the validator and so I've been I've been calling this term in my report here the proposer payment value it's the amount paid to the the validator to put those transactions into the block okay one of the things that we did is performed a Monte Carlo simulation looking at it pre-emerge we looked over a period of 60 days we looked at the amount of Bev that was extracted by miners now this was done in the kind of May June time period but Bev seems to be about uh pretty much stable in that time period it's not maybe declining slightly but pretty much in there and it is this long tail distribution okay uh most of the math this this line right here this is the median value right so 50 of the blocks received a value that's less than uh less than .05 eth in their block right 50 of the blocks had uh eth payments and Mev payments or proposer payment values that are greater than that the average is actually quite a bit higher than the median and the reason for that is of this long tail distribution in fact it's it's so much that you know in looking at some of these metrics if we just look at the top point one percent right uh let me get it here 30 percent of all the each values is in that top 0.1 percent of the blocks there right and it's because it's just so long tailed here I mean the ends right here at one each but it actually continues on right we heard earlier that you know some of the blocks would be you know near three digits e right tremendous value to it right so because of that long tail right even the top one percent have 45 of the proposer payment value in there right it it creates these these very rich um eth blocks right that we've been calling Lottery blocks right and if you win a lottery block it can be it could be very rewarding in terms of an income stream right so this is why it makes sense to start aggregating them into a smoothing pool so one of the things we did pre-merge is we started to looking at okay I think this was originally proposed by flashbots there was a Dev on the rocketful team that actually uh came up with an ideal and a mechanism to implement it uh Joe Joe campus um and so what we did is we we I created a um a Python program and you can get it from the GitHub that went ahead and it for and it went through a Monte Carlo simulation and so just briefly what it did is it said okay let's figure out some time period and that's important right uh because uh the way that this works is that if you validate for an indefinite period of time right you all approach the average okay but we're not going to validate for an indefinite period of time there's some investment right maybe you start staking you earn income but maybe it's a five year maybe it's a ten year or 20 year there's some limitation of time perhaps it's even much shorter with proposer Builder separation right maybe that will come in three years or four years right or as we heard in the other talk maybe there are other technologies that could prevent uh prevent a lot of this matter from occurring so there's some finite determination the software goes ahead and it performs a try it grabs a validator it makes a guesstimate about the number of proposals that it will get over that five-year period for most most of my graphs um it then for each of those proposals randomly assigns a um an amount of Med based upon that 60-day historical period okay because math seems to be somewhat random we don't know when the nfts are going to drop when the Arbitrage is going to happen Okay it then goes ahead and does that for a validator or a set of validators that you have it then also Compares another set like type you know set a set B set B is where that validator instead of running by itself something I call a solitarius mode is now running as part of a smoothing pool okay and the size that I chose for the swimming pool initially was about 3 000 validators that are working collectively sharing sharing their proposer payment values um and then it it it did that for the entire uh validating period And it simply compared it said okay would you in this case in this try did the solitarius mini pool earn more income and and proposer payment value than if it was in a smoothing pool right and so we've we've got some results that we could share about this it records it and then it repeats it a whole bunch of times and you can set that number for anything but you get enough power at about a thousand tries running through this thing here and then finally it produces a lot of a lot of knee graphs so let's go ahead let's take a look at those graphs okay so let me walk through it with you a couple of times let's just start with one we have one validator who over the course of five years produces on average about 60 blocks right and you can see here that it runs for a period of time it gets it gets a a block and it receives a small amount of MAV here it got it proposed again and received a larger amount and you can see as it steps up here it's not receiving any blocks it was never selected to be a block proposer here it's a block proposer and it went up and so it just goes up every period right and I had the periods kind of slice and in 28 day periods and at the end of it you can see over that five-year period this run of this single validator earned about 2.5 Ethan math okay um we could run it again and we could say given that same performance what about if it was in a group of 299 other validators and it so therefore the mini pool had 3 000 validators and we could see here that if I can if I put both of those slides together that it outperformed right it was much better to get a small constant share of a larger pool over time now this was just one run right okay maybe it just so happened I picked one that had outperformed it we need to repeat that a little bit more right but let's let's look at that a little bit more here right so again I said we'll try it 10 times right and you can see here back to the solitarius validator that maybe it only earned two eth maybe over here in this trial it earned about 60 right okay there's greater variance because of just the the long tail randomness of of the Mev but if it's part of a smoothing pool you could see here even at 10 tries of a smoothing pool the variance is much less because now we're averaging a share over 3 000 validators right and when you put them together on the same chart in the same axis is here you can see that at least in this this trial of 10 runs at nine nine times out of ten being in a smoothing pool you earned more income over that five-year period than if you had just ran independently only in one time you had a very lucky solitarius mini pool that had actually outperformed the smoothing pool right so um we this is just another chart that's looking at it where I normalized if you were by yourself and you could see that most of the runs you earned you know in this case about three eth more over that five-year period uh than if you had ran solitariously that's the red line okay well again 10 times is not a lot you need to start increasing that very large to get your orders of power in in terms of your ability to see resolution so here I ran it again all these lines start to overlap and it looks like from here again the blue lines here are the smoothing pool and you might say wow look at all these here that are up here earning hundreds of Eve over the smoothing pool right but it's it's a little bit of a of an optical illusion right because the majority of the lines are here in this very dark purple at the bottom right in fact how many of those are there we could sit we can start looking at it right and I'll just I'll just point to this one's part here if you had one mini pool of your own and you were enjoined a smoothing pool of only a hundred other mini pools 78 percent of the time in this run of a thousands of tries 78 of the time the smoothing pool outperformed a solitarius mini pool right we could show the same same chart here just shown on a line here and you can see that it will underperform in kind of a unique case when when you are the smoothing pool if you join a smoothing pool before you reach the 50 mode right so don't join a smoothing pool if you have one validator don't join until there's at least two or three validators in the pool right okay but as soon as you become the small fish in the pool right if you join a pool of let's say 3 000 mini pools and you only have 10 or 15 15 validators right you can join you could expect about 80 percent of the time that that that's that smoothing pool will outperform your solitarious mini pool so it does it does provide that um you know the kind of takeaway here is that a fractional share is going to outperform most of the time okay now uh another rocket scientist in rocket pole uh valderram I think looked at the report when it was when it was in the early phases and he said Ken that was that was pretty interesting stuff I love the code but he says I think I could show it in one graph okay and so he put together this graph I got to give him the full credit and what it shows here is it shows the probability of you earning an amount of eth given whether or not and we'll just look here at the blue line which is one validator versus joining the purple line of 3 000 validators and so this blue line you can see that there's some probability right that you might even get up to 10 each over the five years right but most of your probability is that you are going to get less than 5e okay versus if you're in a smoothing pool you're in the purple line right you're almost assured not to get these low values most of your probability is getting some value close to five or six eth over that period of time and then again it becomes very unlikely that you're going to win the lottery right because the pool won the lottery and you're just getting a share it's also shown down here lower below here which is that if you're in the pool right um You are you are nearly guaranteed to get at least one eighth two e three fourth five five E six e and then it starts dropping down to almost a negative probability right versus the Blue Line you can see that yes you know the chances are I'm going to get much less than five but I give up some chance of winning the lottery right so it it does provide um you know the takeaway here is that if you participate in a smoothing pool right you are more likely to receive large larger monthly eth rewards okay especially if it pays out on a on a monthly process then running your own mini Pools by yourself okay now what why is this important kind of post-emerge well let me let me share a couple of things post mortgage I mentioned that rocket pull already has a smoothing pool for its node operators they can opt in to it right uh you're not required to and there's a great uh a great tool again you can scan the QR code a great tool called Rocket scan by another rocket scientist um peteris that actually shows um the rocketful validators and I did this last night when I was putting these slides together and you can see what proposers or what blocks they proposed and if they have a little uh looks like a smoothie right they're in the smoothing pool okay and uh you can see here this person got pretty close to a lottery block right but they're in the smoothing pool so we all benefited from it but you can see most of them here are these just you know smaller Mev amounts and you can see wood MAV relay um you know won that block auction in there right um there's also another great dashboard here by by Genex another rocketpool member um who put together a dune dashboard that was showing since the smoothing pool launch which was a few days I think after the merge happened so about 30 days ago or so um that you know it is trending very close to our model predictions right uh we have um you know a median of about 0.6 e so if you were a Sagittarius that would that is what you would expect um in terms of of your rewards and then uh if you're in the smoothing pool you can see the performance enhancement from it right in the smoothing pool you get the average if you run by yourself you get the media it's basically kind of how it works and they're at they're adding about 1.5 percent if you're in the smoothing pool in terms of your APR uh to have a validator in in the smoothing pool because of that uh they're tracking it very much like our model predictions that we can see most of the Mev are these very small amounts that occur but you may not see it here but I'll just kind of Point there's one around four or one around five and the biggest Med block we've got so far in the last 30 days is about 8.4 e that was right after the right after the merge there was a very uh a fortunate uh block proposal that came in and got vetted but it models very close to our um our predictions so that's good to see now we only have 30 days of data it needs to go uh there's a great command here in the rocketpool Discord the invis bot it actually shows the number of Rocket pull nodes that have opted in it seems to be very popular with about 70 percent of the node operators in the network joining it we've been told or at least seen chance that one of the reasons some of the the larger nodes are not joining it is they do not require yet uh that Nev boost be running on these things but that is the plan to make it that if you're in this smoothing pool it's a it's a it's a med boost smoothing pool but already it's performing quite well and this is uh it pays out every 30 days this is only in the last 14 days we're about halfway through the reward cycle it's already generated 70 among these participating 4 000 mini polls that are in there right now obviously this is this is a model that kind of showed how it works it's an example from the rocket pool uh protocol about how a smoothing pool can work but I actually think it it's it's a value to other staking Services right I can certainly see a need for solo validators to perhaps aggregate into smoothing pull of their own I could see other protocols begin to create their own smoothing pools one of the interesting things is I know that the ethereum foundation and the research side is looking as one of the possible solutions to proposer Builder separations is the establishment of a protocol including smoothing pull that redirects those rewards and so this could be an excellent model for them to look at in terms of getting data and that was one of the reasons I wanted to present it here at Devcon because I think it does does offer some example on stuff last but not least so I have time for questions I would like to thank the note operators associations with their financial support that I was able to attend here and present this paper and shout out to all those in the rocketful trading Discord that gave me a lot of support to actually submit and present this and write the paper so with that I'll conclude my my prepared talk and open it to questions [Applause] uh hi there thank you great talk um simple question what's the right number of validators to have in a smoothing pool uh at what point at what point does the probability of rewards begin to approximate your 3 000 validators would you get the same effect at 30 300 yeah I think I I think the numbers you did on a slide maybe I can pull it back up here well maybe not uh but if you look at the slide that shows it you know even at 100 a mini pool of about 100 100 mini polls right um as I go back for it you you start to get it right even even at 3000 and so forth um the numbers maybe this is a better slide over here I mean look you're at 78 80 right which is pretty close to it does not take that much to start getting those rewards obviously the more you have in the pool the less variance you get and it becomes a very predictable stream of income foreign I'll wait for the mic to get back there well it does I'll add over here that you know certainly if you join where you're a small part and um you know you can see the numbers they start moving moving more and you know again it's Monte Carlo simulations so if you see little variances as to why this is lighter shade and not it's just you run it again on another thousand you get a you know a slightly different picture but it all it all looks this in my report there's a really nice one where I let it run you know over the weekend and you know generated a number of mining collar tries um just um is there anything special about rocket pool that makes it only work for them um a smoothing pool or could potentially other homesteakers join the rocket pool smoothing pool or could they have their own you need oracles for it to check participation of of the validators yeah yeah no the the math and the um uh the mechanism Works regardless of the protocol right so just if you get enough validators together working collectively to share the rewards the math all follows right now I understand it's actually come up a couple of times I understand from the rocketpool community if they would open the rocket Bowl smoothing pool right now they have it only to Rocket pull validators that are with their protocol but I know it's been talked about you'll have to ask some of the dev team in the in the front row about what their current thoughts are but it doesn't surprise me if if others start creating similar approaches foreign you mentioned that joining is voluntary and I was wondering of like the people that know about it how many people actually join would you say yeah I I think my slide said that there was uh about 70 of the uh existing node operators so a little bit of terminology here a node operator is an individual who has a a single piece of Hardware maybe I guess it would be a single piece of hardware and about them 70 of them have joined right so um out of the 1600 node operators in rocket pull uh just just a little bit more than 1100 have joined um we do think that from some of the the other node operators I think they are waiting for Med extraction to be required like right now I believe it's about 90 percent of those in the smoothing pool have Mev boost on but 10 don't and so you can kind of think of it if you know some somewhat as a strain on it right because they're not actually going after any MAV but they are getting a share of the rewards uh the plan in rocket pool is to make it mandatory as part of joining the pool uh but because of the merge and the new technology on Mev and the relays uh right now it's just truly an opt-in but most of the node operators are obviously profit seeking and they are turning on the the meth boost an excellent question by the way all right well I don't see any more questions so I thank you very much for allowing me to present the present here at Devcon [Applause] foreign foreign foreign [Music] foreign foreign [Music] [Music] foreign foreign foreign foreign foreign foreign [Music] [Music] foreign [Music] [Music] works okay hey folks um my name is vashilishapawalaf I work in Lido and what I want to talk about today is what I think will be the future of liquids taken I really love the crowd by the way there's so many of you um um so does it work yeah okay so um a bit about uh last us and about me why I'm talking about this I'm a depleted Lido which is a dial that headquartered on the Doom that makes liquid sticking protocols and it maintains the largest liquid sticking protocol in existence right on the denim which has a bit more than four million Heather in it currently the number two D5 protocol by qvl and I'm co-founder Tech lead so I guess I know a couple of things about staking so um um for folks who are not really familiar with the topic liquids taking is when a Staker gets transferred over Rocher when they look up the stake and has taken protocol so like uh it's not uh it can be used in defy it can be transferred sold both collateralized etc etc um and uh a liquids taken is a small but growing and pretty significant in size uh part of stake and economy you know like all the stake in the common area without 80 billions and liquids taken protocols that together are about 8 billions or so and if we add to the liquid sync protocols also liquids taken and exchanges which I think we actually should count among the consequent options uh it will be closer to 20 billions I think so uh large but not overwhelming part of staking economy um so um I'm going to like I'm not going to do any deep insights here like I'm uh don't pretend there is something I'm going to say that is not pretty obvious I think um but I'm going to say it anyway I'm going to make three predictions basically one is that like protocol based liquids taken could grow alongside the FICA system uh when the fire system growth a liquids taken protocols grow with it when it doesn't grow and stagnates that like liquid state will not uh protocols will not grow like there will be uh people will use sticky knock changes instead um there will be a lot of options for liquids taking but few of them will be Witness and the like who will be the winner who will be the winners uh will be determined by stakers they they will be the the one with who vote with their money about like what the future would look like um now in a bit of more details our area of this uh statement I'll uh talk about it so our experience at Lido building liquid sticking protocols many chains we have like one on ethereum one on salana on polygon on polkadot the thing is liquids taking protocol growth is driven by defy consistent growth whether when there is a low amount of defy penetration liquid sinking will also be pretty pretty small and when there is like a very strong economy like an ethereum liquid thinking would be will be very popular there are other factors but like that's primarily I think like when people want to use the other in defy they will rather use some kind of second uh stake and liquid second token in the file uh when they don't use uh want to use either and defy they don't care about State the rocket pull leather Etc so um and the main competition are the wallet guns of C5 like exchanges have liquids taken since forever uh it works very seamlessly it's very uh very easy for the user they like just deposit the uh tokens that they use used to if they are working with exchange and it's automatically sticked it can be often used as collateral for margin trading like it has most of the users that people want liquids taken for and if you don't care about being in decentralized finance if you don't care about like this being not your keys uh it's perfectly serviceable so [Music] um and the product is just better than regular stake in the um liquids taken is better for users than regular stickers the adoption barriers that like a major problem for liquids taking protocol growth is smart contract risk governance risks and sometimes tax implications um the smart contractors can governance risk on one hand they go down with time so like with time people starting to trust more their protocols for a good measure uh because they haven't been hacked they are like less likely to be hugged in the future uh same about governance and still uh it's not worth it like if you're not using to use the token and defy like if you if you're not using no not going to trade it or collateralized it you don't want to take a traditional risk um and uh there are like a multiple options for building liquid sticking protocols uh and like like the second in general not just protocols that are um uh going like exist already or will be existing going forward and um to talk about them we need to understand what is liquids taken as a product like who are the users what do they want and there are three kind of users like three kind of stakeholders for like it's taking one is the stakers they are most important here and they once taken Awards they want security they want liquidity and usability in uh in finance basically protocols Community want the best solidity set for the protocol that is decentralized sensorship resistant and not operators in protocols they want to run a stable stake in business if they are professional and if they're hobbyists they want to be like I don't know respected they're not rational they just do it for the fun and for the feeling of uh contribute contributing to destroyalization which is great so um and the options that are available will be available in the future are broadly um it's custodial option uh The Exchange based liquids taken broadcast that you basically it's taking uh it's protocol that are based on risk management it's protocol that are based on uh bonds for security they're hyper compliant protocols and Marketplace types type protocols um so Castor yeah I'm listing them in the order of adoption custodial Liquors taken like a change liquids taken is uh like the largest one uh if you combine like even on ethereum but if you combine together all the uh exchange section options it's like a lot of uh market share they [Music] um uh exchange for custody based very simple to use for users because like centralized solution are like very good for you for user experience usually you can do a lot to make it simpler uh they have no additional risk if you already draw The Exchange in question with with your capital um often have included uh uh their options for margin training Landing money markets Etc um and because the operator of custodial liquids taken like a change or something they are double dipping so they they're getting staking rewards and like fiance taking rewards and they are also getting fion trading on maybe custody fees or something like that um they can offer very competitive rates compared to others like the the this option will likely win on profit liability going forward it's not the case right now because like coinbase takes a lot of um a lot of fees but they have much more uh options to go all over on fees because they can double deep in vertical Integrations um they have usually subver value data sets so like the don't provide a like that much value for the like as much value as it could for the protocol uh because they select few operators um uh with little diversity in jurisdiction's physical locations etc etc um and it's not transparent and they are suspectable to regulatory capture because like C5 solution uh like extremely regulated [Music] um risk management based protocols they are non-custodial they manage slation risks protocol wide by creating creative value data sets that like minimize slash no risks they therefore they are usually Capital efficient so they're easy to grow they're easier to uh to accept stake and for like for the competitive advantage on user side they rely on the fire system so like when the FICA system is better than uh any any single wallet Garden of exchange they can offer a good uh uh that alternative to centralized exchanges uh valid data set they have are like very different in in design because uh and in quality they can be big they can be small they can be like consists of one operator which is like the same the protocol operators basically um and they um that's because like the managed protocol set is the valid data set is basically the major part of the product here and that's where they differ very much um it can be good it can be bad it like depends on the protocol uh bonded protocol uh also non-custodial they manage slash the risk by acquiring validator bonds uh that makes them Capital efficient because validators like the good value like the uh not operators Community they don't usually have a lot of money and when they are required to provide bonds this is limited by basically the amount of capital of operators or the amount of debt they are willing to take from outside sources um and uh the valid data set management is left to the market like basically if you have Capital uh you can participate as an uh operator you can provide your own validation and that's the only thing that is required which I think at scale delivers centralized for data sets because capital is centralized uh like if you take a look at uh Genie coefficient of ethereum for example which is like one of the best ones it's it's really high like most of the wells have worldwide like a few dozens of whales um at when the protocol is at scale that they're either providing a loan or their operating cells at scale and uh take the majority of the validator said um this option of hyper compliant one is not uh um very different than Tech side from risk management it's different approach because like it's uh seven point and Main feature is that the valid data set is extensively vacated and um certified and regulated and they are likely to not deliver well it just said that the protocol Community want and I don't think there will be much liquid usable because like usability and defy requires uh like they they made clients uh people who I have like very scary or regulations I think that's like selling through lawyers uh type of things um uh that are averse to participate in in much of the device so and the last type is Marketplace is when there is there are multiple options for staking in one place uh different risk profiles features and costs uh and when they do a liquid sticking token they have to fund it uh in some way like uh risk either they do risk management on options like to to to make a fungible basket of uh different uh options in the marketplace or uh do the bond type or just do a naive like assuming every risk profile is the same um current state on ethereum is that risk management is the largest protocol like it's Lido its risk management type uh the second by size is custodial but it will very quickly jump to one by one when withdrawals are possible and every other on every any exchange can be staked more safely than right now and the third type is bonded it's rocket Polo the third third position uh and by growth speed right now coinbase uh liquids taken token grows the fastest um lidoor has the second place and the rocket pole is the third so this is the current trend um I think that the only non-custodial trust minimized options can be designed with the condo evil principle in mind so um liquid sticking protocols that um are sufficiently limited uh in what they can do though like those taxes or valid data set selection to uh two force or like uh entice operators to operate in certain way when they are limited in that uh they can be entirely harmless for the protocol and not a threat uh custodial Solutions can do that because like they can act together like with their values but they are extremely susceptible to uh to regulations which can be not aligned with a company's value um I think that only risk management based protocols can deliver good value data set at scale because at scale the like the main thing that you deliver for a protocol is a good quality data set and if you don't have an opinionated uh selection algorithm that says that like the Goodwill data set should be defined like survey diverse should be diverse in jurisdiction in um should be pretty flat and distribution of stake should be diverse geographically and like um in in cloud and on-premise operations etc etc [Music] that's that's essentially what like Risk Management is your monetary for the protocol as well um so I think there are two possible outcomes for the future of idiom is one is that like most of the stake is in the risk management protocol that provides a good quality asset um minority isn't custodial and one that is like the uh on the third place and the bad outcome is vice versa when most of the stake is custodial and the rest is in protocols um um so anyway whatever I think on that uh it doesn't matter much because I'm not the one who decides who will win and like who who will lose and like what will be the best uh the the the largest options it's a stakers who decide that outcome uh like not not operators uh they are agents uh they operate like either very small amounts of their own stake or like large amounts of other people's money they do not decide like who who is going to be the winner it's not stake aggregators like protocols or custodies because users stakers actually decide which uh which aggregated to use uh it's not protocol researchers or developers because they design like a neutral protocol that can be used by other people which has takers to to have the desired outcome it's not uh voices on crypto krita Twitter or like protocol governance it's it's going to be less takers Uh current state of ethereum stake increase the direct result of stakes making the choice in the past like they why is lighter big because people sticks with lighter why is like a coinbase big because people decide to stick with coinbase uh the future will be a result of stakers making the decision going forward so what I'm asking you folks is to select the best state based on your ethos your needs your capabilities like if you can if you want and you can run your own note run your own node if you see a good option for staking the protocol like stick with the protocol or with the not operator you trust or something like that um because like status will be deciding the future of uh validator settings like how the engine will be operated in the future um at the bonus thing I uh like I I made a few predictions but there are a few code curveballs that can invalidate all of that uh very like very unexpectedly um one thing that like is obviously can be can change the campus very much is uh regulations uh like Regulators can force most of the stakers and most of like a lot of ethereum is like in regulated uh jurisdictions a lot of ADA is hold a sign regulated addictions to comply to local Rex and if vocal Rex say them to stake in a specific way they will likely to comply uh the funds the like the cost studies etc etc they will have to um the second curveball is uh risk taken uh shout out to Angela and it's basically using the same stake on multiple protocols um um the the main idea is like you can stake either in uh in their own protocol and then can use the vowel chat of the text Data like stacked at the token or like a erase token or something as a stake in a different protocol uh that does something different and it's subject to to additional slashings if something goes wrong that leads to a combinator explosion in potential risk and rewards profiles so right now it's pretty easily it's pretty easy to funch than fungible the staking positions like you manage the risk and rewards and that you get can get liquid token out of it like oh make it bonded or something when there is a lot of options for getting additional rewards for taking additional risks which is going to happen if NCLEX takes off uh that makes uh the the space very very diverse and for possibilities uh of using your other in different ways and it's become becoming much harder to funch than fungible um and it makes risk management based protocol harder to design and marketplaces uh more uh more suitable to the to the state when there is a lot of differentiation in different options for staking marketplaces are better suited to that than uh risk management and the Elephant of the room in the room is the second order effects of Mev uh because like that's a very uh field with emergent rules uh we like we don't understand how it works until it works in specific way usually we can predict some things but not all things uh and it's hard to see the future clearly and it clearly has a lot of requirement for latency and benefits a lot from included deals which can force centralization somewhere in some place and it can happen that it will force centralization like on protocol management like on Pro non-stake Management level or like on stake and layer or um like wherever we don't know yet like at least I don't know I'm uh I I I I don't I don't quite understand how it will end up and I think that we we will see it when we'll see it not before um I think the best way to counter that on the staking level will be to transform validators and protocol to thumb pipes that don't make any decisions basically or make as as like uh can opt out of making decisions at least uh something like PBS design we see a list for example um um but like as I said it's curable I I'm not completely sure with how it's going to play out um so that's it thank you for coming I like I'm really hyped with scenes that's such a good crowd any questions how is this related to the insurance of light or in relation to the TBL I don't quite get the question uh I'm going to answer as best as I can so uh Lido is a protocol that takes uh stake as ever and it distributes to a valid data set in a specific way this valid data set is selected in a way to the risk the stake that holders so like are you slashing risks and to make the validator set of ethereum beta as well so like we try to receive it in a way that will be uh increase the diversity of validators and like the stake distribution make better stack distribution um we don't require operators to bond the capital to to operate and that makes uh scaling for lighter really easy because all the operators need to do is to upload some more keys into lidoor and like accept mistake and that's why when there was a lot of demand was taken uh in in the ecosystem it went to Lido and not say to coinbase when the growth is limited by by not operators having enough capital or having enough like cheap launch options or something like that uh it's not like it's not being able to uh to ride these waves of like staking demands uh so I guess like the key value of Lido is a direct uh results of the design I can't hear you the mic doesn't smoke okay I was wondering um like one of the curveballs that maybe could happen is what if it becomes dramatically easier to operate like it's pretty complex to like spin up your own like Beacon currently but what if it was much much easier how would that impact the overall Market Opinion um so I think that um it's not going to become dramatically easy for a long time the reason is like before protocol classification it's really hard to maintain the the notes in like in a good State because there are a lot of grades and like hot fixes and stuff but imagine it it is becoming dramatically easier than um on like on liquid stake and I think it will result will be that there will be more solo operators in it like every product like every protocol will that that will be a big enough will conversion like on some hybrid model of having most of the stake and risk management and less taken bonded solution for permissionless apparatus uh just to promote the uh solo stickers and um the removing of barriers will lead to more solo stickers and like a bit more importance for uh door to hand this feature that's my guess hello it's really nice to meet you I have two questions why actually first of all first one I would like to understand what you see as the biggest risk for the legalist taking industry are they is taking industry as a whole right now and second one what do you see as the biggest risk for later right now um I think the answer to both is smart contract risk uh uh like defy protocols uh are like very robust I think it's the most like the most box free for code in the world like right now like better than Aerospace better than medical code etc etc uh but it's like existing in a very toxic environment where any like any mistakes gets exploited like very fast um so smart contractors because probably them they're like the the major risk and uh on a more like economical side I think uh like I'm really uh uh thinking that uh exchanges wallet got lack of sify are really a like a very strong competitor to uh protocol based liquids taking because centralization is uh more like it's cheaper and people care a lot about APR so so uh I have some statistics um on the light of the tokens emission it's kind of like outdated but it's um something to me nice so the token incentives that the emissions in the last seven days is five millions in terms of tvl while the rifle news it's only 600k do you think it's that people actually value the governance of Lido itself as a liquid sticker or it's kind of like being overvalued do you see any attack factors or governance if they refer to like it's kind of like go down if people don't value it as much as right now um oh let me rephrase and say that if I understood it right so you're saying that we have a lot of incentives and Lidos tvl is probably because of incentives and not because of like people because of the fact that people wants it right okay while the token incentive is 5 million uh so the incentives are not that Beacon uh in the overall uh protocol um like compared to overall protocol Revenue and they are needed to maintain a good liquid pools for all liquidations to happen um that's the only reason they exist the staking rewards are the main uh reason people people have staked it there is like uh of over like four million of weather in Lido I think about uh between two hundred thousand and six hundred thousand of stake that is locked in uh maybe 700 I don't know is locked in in in into in centralized pools uh so I think that when withdrawals I had like are in uh that's will be completely unnecessary and uh will change everything like uh the incentives will be slow but the protocol will be just as useful thank you [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] thank you foreign foreign foreign foreign foreign [Music] [Music] hi everyone um welcome to the flowers room and we're very excited we have a great lineup today our first speaker is John Stevens from veridice and he's going to be asking us a question at the back of all our minds um are your zero knowledge proofs correct John all right foreign okay as was previously mentioned I'm John uh I'm also the CTO and the co-founder of verdice and so today I just want to ask you a question are you zero knowledge proves correct why don't we find out so just a little bit about Paradise all of us come from an Academia background so on the slide right here you can see some of the main members from Paradise so on the top those are the co-founders including myself and on the bottom you can see some of the other people that we have here at Defcon so if you have any questions please come and find us ask us questions they're also right there in the third row uh but yeah so just a little bit more about us uh we are a security company so we provide Audits and we also prefer or provide Tooling in order to find vulnerabilities in all aspects of the blockchain and so basically we built tools to help ourselves and other people find bugs and defy applications blockchains and what we're going to be talking about today zero knowledge circuits and yeah we're very interested in making our tooling available and so today I'm going to talk about specifically our open source tooling and not about the tooling that we're providing as in our security as a service Suite so uh to begin with I think that would be useful to talk about what sort of bugs we actually see and zero knowledge circuits and so the reason why we want to do this is because these circuits are extremely prevalent especially in an L2 blockchains and so that means that you can see these circuits integrated into smart contracts themselves like tornado cache and semaphore and blockchains and also there are a number of libraries for circuits as well like the the circon lib and so one of the interesting things that we've seen is these circuits present new challenges especially for you know people who are interested in security and the reason why we want to do this is because we've seen a number of exploits in a or in all aspects of the blockchain from Smart contracts to the blockchain itself and these zero knowledge circuits are a relatively new domain and so while we haven't seen very many uh hacks in the circuits themselves other than luckily the the ones that tornado cash found before it was exploited we want to make sure that people are thinking about the security of these circuits as they develop them and so the way that we want to do this is by our for varadise we want to provide tools and so I think that the best way to start is to talk about some general types of bugs that we see and to do this I need to kind of establish like the model of the zero knowledge circuit that we're assuming here just because these circuits can be fairly complex and so here's a simplified model of the circuit so basically what happens is there's going to be a witness generator this could be a blockchain or a smart contract or something like that and it's going to take in a set of public and private inputs it's going to generate a witness and this witness is going to be provided to the prover along with some additional information which will generate a proof and then finally that will go to the verifier and then the verifier is going to say yeah it's correct or no it's not and so when someone wants to create one of these circuits there's a couple of ways that they can do so and the simplest model they simply write a single application so this would be their circuit application written in something like circon or Cairo and then that will generate a the witness so your smart contract for example and it will also generate a set of finite field constraints which is later used to generate the prover and the verifier and so the interesting thing that we see here is you have a single application which is used to generate uh two different or two different applications which do slightly different things and more interestingly they can be out of sync and so one type of bug we see here obviously is if someone doesn't Implement their circuit correctly so if that happens then we have a functional correctness violation and it basically just means you know someone needs to go back and rewrite their circuit in a way that's correct the more interesting bugs though are when the witness and the constraints are not in sync with each other and so this happens because generally these languages provide ways for you to um put specific logic in your witness and put specific constraints in your finite field constraints and so if it turns out that your constraints are less permissive than your generator you could reject valid interactions with the application so for example if you have a blockchain it's possible that in this case you could have a denial of service where the blockchain wouldn't be able to process transactions which you know wouldn't be great the worst case or the more dangerous case though I should say under constrained is an under constrained circuit so what happens with an under constrained circuit is the the constraints are more permissive than the witness itself and so the reason why this is more dangerous is because while the witness might be correct and it might provide all of the interactions you want and they're all accepted by the constraints everything seems great however an attacker could then come in and they could provide an alternative witness which might be used in order to get some alternative transaction or logic accepted by your constraints and then prove it and this would necessarily be bad because let's say one of the constraints which uh didn't end up in your finite field constraints was equivalent to uh the sender is equal to the admin right obviously if someone was able to bypass that then bad things could really happen and so the way that we deal with this in Paradise is by using a um a field in computer science called formal methods and so formal methods provide a way of automatically reasoning about software and so it does this by uh understanding and understanding the semantics of the underlying program and iterating over that program in order to determine information about it and so basically what we're going to do is we're going to use formal methods in order to find bugs and create proofs that the software is correct and So within formal methods there's basically three different fields so there's automated testing so if those of you are for those of you have heard about fuzzing that falls into this category and so basically in that case someone is just going to run the program on a bunch of inputs and so this is useful for finding bugs next we have static analysis we'll see a static analyzer in this talk which is going to analyze your source code for a particular class of bugs and the important thing about these static analyzers is if they can't find the bug they it has proven that the bug does not exist in your program however you know the more interesting case is if it does find a bug and then last we have formal verification some of you might have heard about this there are multiple ways to perform formal verification but at Paradise we're particularly interested in automated formal verification and so basically what this is going to do is it's going to analyze your software and make sure that a specification where a specification is essentially a way of expressing the intended behavior of your application it'll make sure that that specification holds for the application and so that means that after using a formal verifier one you can verify custom correctness properties and two you will have proven that that property holds or that your implementation is correct and so as we go across the Spectrum uh it takes more effort in order to uh both build and run the tool but it also provides much stronger guarantees and so we're going to be looking at some of the open source tools provided by veridice that fall into these categories and so first we're going to talk about a static analyzer which performs common vulnerability detection on Cairo or circon programs so basically what I'm going to describe this is the type of bug that is found by a a static analyzer that we created called pikis and so basically what it's going to look for is uniqueness bugs and so what a uniqueness bug is is it happens or it occurs when a constraint allows a single input to map to multiple outputs and so what I mean by this is if you look at this circon program over here you don't have to understand what it does but basically there's a bug that's circled in red the reason why that's circled is because it constrains all of the outputs except for one very specific output so decoder is supposed to Simply uh zero out all of the bits except for a single bit that is specified by imp and so what happens here is if you look over on the right you'll see that the constraint specifies that that multiplication has to be equal to zero so there's two ways that it could be equal to zero it could be equal to zero because the output is equal to zero or it could be the case that imp minus I is equal to zero and so all of the cases where uh imp and I are not equal to each other our constraint but in the case where imp is equal to I we find an unconstrained bit and so what does this mean this means that an attacker gets to decide what the value of out at imp is so it could be the desired value which is one or it could be zero and so why does this matter well because of the reason that I told you about earlier it means that depending on how this circuit is used someone might be able to leverage this uncertainty in order to violate some assumptions that you have made and hack your protocol so what can we do about this well I already told you that we're going to be using static analysis and so there's two types of ways that we can solve this problem one is we can just perform straight static analysis and so what this is going to do is basically we'll just look at the inputs and outputs and we'll say that if the output is a linear combination of the inputs then it must be constrained so that works however it also produces a lot of false positives which is undesirable because then you have to go and you have to determine is this real or is this not so instead you could use something called T solver so what an smt solver is is it's simply a a program that can solve mathematical formulas and so because you have constraints uh you can encode this mathematical formula that states whether or not a particular output is constrained the problem is normally there are many constraints and so if you tried to send that to an smt solver it would quickly become it would quickly bog down and it most likely would not be able to solve uh uniqueness for large circuits so basically we see that static analysis is scalable but not precise and we see that the smt solver is precise but not scalable and so what we have done is we've developed picus and so pikis is going to take in your constraints and it's going to Output one of three things so it's going to Output yes this is uh this particular circuit is entirely constrained no it's under constrained or I don't know but the way that it does this is by using a combination of static analysis and smt and so what I mean by that is we're going to have two phases that are going to iterate and so basically in the first phase what we're going to do is we're going to take those constraints and a set of signals which we've already proven are constrained and then we're going to compute a new set of constrained signals and so this will give us a set of constraint signals that we we already know are constrained and so if we end up finding that all of the output signals are constrained great we're done otherwise if there are some output signals that we haven't been able to determine if they're constrained we can go to the next step and so the next step is the smt phase so what happens in the smt phase is we take these signals which we have already proven are constrained and then we again take the set of constraints and we use an smt solver in order to try to find additional output signals that are constrained and so if we happen to find that all the output signals are constrained again great we solve a problem if we find an output signal that is unconstrained because we can prove that in this case then we say all right you have an under constrained signal and is specifically this one and then otherwise if the smt solver fails for some reason so basically that happens if K Pi Prime is equal to K then that means that we've basically stalled and we'll instead just return I'm not sure so it I mean the last case is if we ended up making progress then the important part is we can go back to the static analysis case because we have now proved more cases are uh constrained and so that means that the static analysis phase might be able to make further progress and so basically we'll do this in a loop and we find that in this case picus is able to scale much better than the previous attempts that we saw so basically we evaluated this on a set of circuits and uh picus was able to solve I believe that comes out to 98 percent of the circuits that we threw it at whereas if we look at static analysis and smt static analysis returned a bunch of false positives and smt timed out on a lot of the benchmarks and so this is the first tool that we look at so the important part is you provided a circuit and then Pikes does all of the work for you you don't have to provide it any additional inputs and it will give you a list of possible problems the next thing that we're going to look at is a verifier so this performs automated verification so what I'm going to talk about next is a tool called Magi so what Magi does is it performs automated verification on Cairo programs and so the way that it works is you will provide Magi with your source code and a specification and as I stated before the specification just specifies the intended behavior of your Cairo contract and then Magi will spit out either yes or more interestingly it'll spit out no and it will tell you why and what I mean by why is it will provide you concrete evidence of why uh your specification does not hold and so the way that it does this is it's going to take your specification your program it's first going to translate it into a symbolic representation it's what I mean by a symbolic representation is rather than having you can think of a symbolic representation as being unconstrained so basically any value in your program could map to any other value in your program and so then once it does that it's just going to run this through it's it's going to run your program symbolically and I'll show what that means in a little bit and more importantly it's going to interact with an smt solver in order to generate proofs that your uh any or as it runs your symbolic program is going to use the smt solver to prove that your behaviors are consistent with the specification so let me get a little bit more concrete about this in order to do this you need a little bit of information about symbolic execution so basically the way symbolic execution works is looking at the very top you can see just a set of variables and so we start with these variables being mapped to any possible value so unlike testing where you say like in this case U is equal to 5 we say U is equal to some symbolic value so it's going to map to any value within the range and then when we symbolically execute the program what we're going to do is we're going to essentially just run through your code and generate case statements so for those of you who remember you know your your proofs class basically we do case splits whenever we come to like conditionals and so here what you can see is starting at the very top we have everything unconstrained but then once we enter that first if statement that you see you'll see on the left hand side that U is equal to 5. but then we also need to consider what happens if you don't enter that if statement and in that case U is still some unconstrained value and the important part is at the very end of this after executing the entire function we end up getting a set of symbolic equations which we can then send to an smt solver to make sure that it's consistent with your specification so some of you may have noticed that it seems like there's a fairly large spread so if we look back at this it you kind of get this tree-like structure with a lot of breadth and so in order to make this actually scale what we do is we have to perform a basically intelligent merging so essentially what you can consider is after we execute an if statement once control flow joins back we can also join those States and so if you don't understand what this means that that's okay basically all I'm saying here is that in order to improve the scalability of this tool we push some of the work off onto the smt solver and we you basically have to balance the work that your symbolic execution engine is doing and the work that this that the smt solver is doing so let's look at an example of how this actually works so here you can see the move function in maker Dow's Cairo implementation and so the specification that we're trying to check is basically that move transfers funds correctly and so the specification that you see on the right hand side basically says that after the move function has executed the uh the die balance of the destination should be greater than or equal to the die balance at the beginning and then the die balance of the source should be decreased by an appropriate amount and so the interesting thing here is Magi reports that this does not that this is not sound this specification does not hold for the code on the right hand side and specifically it says that it does not hold when rad which is you know the amount that's being transferred is equal to some specific value and so the reason why it reports this is because in Cairo you have this interesting case where because of the size of the large primes they are going to be less than the size of a uint-256 and so Cairo ends up splitting your unit 256 into two parts the lower 128 bits and the upper 128 bits and so importantly the reason why Magi reports there's a violation is because the maker dial developers here forgot to constrain the value of rad and so the way that you do this is in Cairo they provide a specific function in order to make sure a uint-256 is properly constrained and if you leave that out that means that your uh un-256 is not well formed and so once we add that check back in and then we send this back to Magi Magi will report yeah everything's right the the specification holds and so in this case we have properly guaranteed that this move function transfers funds correctly and so this was just a very simple case however we've been working with the makerdale people in order to verify their Cairo implementation of uh their protocol and so as we've done this we've ended up finding I think at least one bug that uh the maker Dow people have merged into their um their implementation so that's all I have for you today if you are interested in learning more or seeing demos of these tools uh in about an hour or so at three o'clock we will be down at the ZK Community Hub just giving a demo of these tools and a couple of other ones also on the left and right you can find links to the repositories for picus and Magi so if you're interested in trying them out uh please do and um there's also a Twitter link so if you want to follow us on Twitter that would be fantastic [Applause] okay foreign thanks for your talk uh just a question when you're talking about uh these Cairo contracts and could you say if I didn't have access to the source code and I just had this circuit that's like how easy is it in practice to exploit these uh you know kind of arithmetic errors or constraints so if you're an attacker and you don't have access to the source code how easy is it to figure this out um yeah that's a good question uh you know attackers are very good at looking at low level code and figuring out how to exploit it and so I would say that you know if you were trying to get security through obscurity that's really not going to work for you here because generally your code is going to be public somehow and so if someone is able to gain access to even the low-level implementation of your code than they could possibly figure out how to exploit it and on top of that we've seen cases where even if someone doesn't have access to your code but they're able to query it they can also find potential exploits foreign I think there's one over there what I want to know what kind of bugs or issues cannot be found by those two tools so what kind of bugs cannot be found by these two tools okay so uh for picus because it's not performing functional correctness checking it can only find under constrained bugs and so that means that it won't be able to find like if your program is written incorrectly for Magi because it is verifying functional correctness it relies on the quality of your specification however if we assume that you have specified the behavior of your program properly it should be able to find any potential bug in your program so for example one thing that we see is if you write a specification that says let's say the die is transferred correctly but then it turns out that due to an under constrained bug or an under constraint bug or a uniqueness bug it doesn't end up being transferred correctly Magi will point that out however it depends on the quality of your specification and um yeah and also won't find bugs in anything other than your Cairo contract so if you perform interactions let's say with other with other applications that Magi doesn't have access to their source code Magi won't necessarily be able to check and make sure that everything is correct it has to make simplifying assumptions basically okay [Music] foreign we'll be starting the next session in four minutes so please be seated bye then thank you thank you [Music] foreign [Music] [Music] thank you foreign [Music] [Music] foreign welcome back so our next speaker is Brian Wilkes and he'll be comparing the performance and security characteristics of some zero knowledge proofs that are commonly deployed in the ethereum ecosystem so Brian hello wow that's loud so who are my data scientists in the room raise hands very few devs devs out there lots of devs nice investors d-gens yes we'll put it down um how about expertise um who's a beginner in zero knowledge just got the basics cool um who knows what's going on but has not written a circuit before who's a true ZK expert knows a ton about ZK you perfect all right um well um thanks for having me my name is Brian um I am a grantee with the ethereum foundation on privacy scaling exploration team so um today I'm going to be talking about um ZK proof performance and security characteristics so um so what's in the docket for the day search for you to go over some background so we'll just give some background on what zkps are I'm try and work through what they actually mean for on-chain data then we're going to look at how to actually find those Dom zero knowledge proofs and verifiers on circuits on and off chain then we'll look at some of the characteristics behind those and what they mean so unfortunately Waldo's not going to be with us today so this is the second best piece of art I've ever done so I hope you appreciate it everyone so what are zkps of our high level first we'll go through this quick and then move on but um we have Alice Alice wants a drink she's a prover we have Bob Bob is the bartender is a verifier um Alice does not want to give all of her information to Bob and so therefore she can put in her birthday the circuit will output a proof Bob can verify that proof is correct that she is old enough to drink and the cerveza will flow so um what's important for this is you have approver you have inputs you have the circuit itself you have the proof popping out and then Bob decides if it's true or false and then does an action based on whether that um verification is true so um why do we need zero knowledge proofs in ethereum so the first thing that we struggle with is scalability so right now we can do about on main chain 13 transactions per second um and we really need to get into that 13k transactions per second if not significantly higher to get in the Visa range so um how is your knowledge proofs help us they can separate they can take fully separate chains um and they can impart the full security of ethereum to those chains so privacy as well on is something that's a hotbed issue to me uh ethereum's public if you're a data scientist um as a few of you are you can go online or you can go on ethereum and see anyone ever everyone ever does um so I I don't want to live in a world where you have scalability and everything sits on that platform without having the Privacy behind it so let's talk about on chain so this is what it looks like um you have your layer 2 blockchain here um so uh they have the blocks going you aggregate some of the transactions together um and then you batch them into a ZK circuit so you take those transactions they go into the circuit the circuit produces a proof this is all off chain so then you have your ethereum main chain on where things are expensive um so you're able to take your proof and post that to the verifier smart contract um and then the layer two State source of Truth can get updated so that's what happens with the scalability side of things now let's talk privacy um it's very similar I won't have reveals for you I'll just go right to it there's a secret there's the ZK circuit um you put the secret through it produces a proof and then that proof goes on chain to the verifier smart contract and funds are released so that's very similar that's going to be your mixers your channel caches different things um what's important for both of these two is that you have um off chain and on chain and on chain is where the verifier in the proof gets posted I mean off chain is with the circuit and the proofs are so this is the best piece of artwork I've ever done so I appreciate it while you can um so let's find some ZK proofs on chain and figure out where they're at so um this is called the search for verifiers so why are we searching for verifiers the important characteristics of a zero knowledge proof are in the circuit so the circuit's going to generate um they're going to have the different characteristics that are important to us let's look at a particular DK sync um transaction flow for instance so between September 26th and September or September 20th and September 26th there were two thousand one hundred and seventy one two thousand one hundred and seventy blocks 265 000 transactions um which were batched together into 214 inputs proofs brought on chain so what you want the ZK circuit did not change but those inputs in the proofs did um so each circuit has a unique verifier on chain so for each circuit or circuit systems they have their own unique verifier that's there and the um if you want to know about circuits and you want to use on-chain data you need to go through the verifier so going through the verifier um you use the L1 on chain data so let's talk about how to find verifiers so the first section is um if you guys haven't used bigquery on from Google I highly recommend it um it's a great tool it allows you to search through lots more data than things like Doom do as much as I love Dune so um different queries that time out within 30 minutes for Dune will go in about two seconds for bigquery so check it out um the three tables that I used um for this project were contracts traces and transactions since you're almost all devs I won't go over traces versus transactions but start versus end of things so okay um so the first way to try and search through bytecode um is to go for constants constants will show up in byte code as a big hex so if you can get a big enough number that's unique enough for what you're looking for you can take that in search with very simple SQL queries um in through the byte code so this is both a big and unique value it's the finite field um so the finite field is the range at which you modulate over when you're doing different cryptography cryptographic things for once your knowledge so this is it in HEX form um it's massive so the first way that I look through the data was to search for this finite field um unfortunately it didn't exactly give me all the verifiers so it gave me a lot of different noise there's 558 contracts um that have the finite field constant um and after checking every one of them manually most of them are not verifiers so I moved on to trying to get into method IDs so the second tool that I think is really important and apparently it's new according to etherscan is is the transaction decoder so the transaction decoder what it does if you look through it is if you go through and look at a transaction it will tell you the exact flow of what happened on through that transaction so looking at this here is another ZK sync example um first it proves the blocks after going through some proxies and then in the verifier contract it does it calls a function called verify aggregate block proof so after doing some modular Edition down below it outputs true and the state gets updated so method IDs the method IDs are the first 10 characters or four bytes that come through with any input or transaction call for traces or transactions so just going through them briefly um you can see that this function here is for startware um and it's a fry verifier so this is what is actually verifying the proofs um so if you want to know what the method ID is for this particular function here then you take you take out all the chaff if you will from inside and then you run the kcat 256 and you take the first four bytes plus the indicator that it's hex so that there the ox E85 A6 a28 is the method ID it'll be the first 10 characters in any call that when you look through the data so a great resource again um I'd like to show the resources that I get to use um the four byte.directory is fantastic um you can input any method ID and it will tell you what the text signature for what it should be is so um now what do we find essentially there's about 65 confirmed verifiers on main chain now there's probably a few more on maybe 30 more that are not on verified public contracts so um in those 65 confirmed verifiers have done over 600 000 verifications so um of those there are about 16 that are active still that's active according to me and some of them are very minor projects if you're trying to squint it's because I made it small so you'd have to go on the GitHub at some point so sorry um I was surprised by how few verifiers there are actually on main chain but moving on to the characteristics so we'll look at proving systems um so you're proving systems here that are actually being verified on main chain um you have your your Classics I'm sure you guys have all heard of these but your growth 16 from 2016. um that runs your tornado cache your Hermes your Loop ring um and a bunch of other uh different uh privacy functions you have plonk um which is asset connect ZK sync um then you have turboplak which is the original Aztec um Aztec connect uses turboplank before it gets on chain but then it uses Planck on chain if that makes any sense then you have Halo 2 which is coming out zcash uses it um and then also uh scroll is going to deploy with it um most people I talk to are really moving towards that Halo 2 move it seems like it's a really good proving system so um it's not on main chain yet that I've seen so then you have your Starks on and that it includes all your starboard projects so Starks versus snarks snarks the succinct non-interactive argument of knowledge and that then there's Starks which are scalable transparent arguments of knowledge and the major difference between the two there's a bunch of them they're these are the base level for what happens but Quantum Computing is probably going to be able to take down snarks or but Starks will probably be resistant to it so what is quantum Computing when will it come I have no idea but um if it does this is what will happen so looking at these different proving systems you have graph 16 which is in blue really taking off that's on the back of tornado cache as well as bluepring and then you have down below you have your Stark um fries that are coming through those are the starkware projects um they have really taken off dydx posts a lot of data to chain often and then you have your plonks and unknowns are probably blocks um but I didn't wasn't quite confident enough to say that so um okay let's talk about trusted setups so a trusted setup is where on the elliptic curve um you should evaluate the different ZK proof so it's it's kind of an agreement in a spot that um approver and a verifier can kind of verify through um what that snark's going to look like so I'd like to think of it like a trustful how many people have done trust Falls before in their life a couple yeah it's scary um so you get up there on a big standing and then you fall backwards and as long as one strong person below you is there to catch you then you should be good so as long as the trusted setup is kind of similar if everybody's toxic waste it gets collected together if everyone gives their secret out I mean doesn't want to catch you or is distracted then um the snark can be um then the trust is set up um will not work and someone will be able to write proofs that will um be false positives and they'll be able to take money from you if that makes sense so um this requires one honest participant from the mo from the MPC ceremony I also like to think of it like Thanos um if you have enough Affinity Stones if you have them all you can destroy the proof and it'll be good so all right bad actually okay so um what are the trusted setups um we have um growth 16 over there on the end um it has a two-phase trusted setup um and then the universal setups came in and that's Planck and turbo plonk um and that only requires one so whereas Roth 16 needs a new ZK circuit or needs a new Pro needs a new entrusted ceremony for the um each circuit that it produces um Planck and turbo Planck have a universal setup and that allows them to only have one total ceremony so if anyone here wanted to create a plonk you could use the universal ceremony that's already been created um and already been done and then you have Halo 2 and Starks um they do not require trusted setups so that's obviously you want the least amount of trusted setups as you can so moving on on the actual projects themselves you have um graph 16s are Loop ring polygon Hermes their withdrawal and then tornado cash um those are the major ones that are currently being in operation so the number these are trusted setups they use the Perpetual powers of Tau um which is um basically an on-chain on step one for the different uh trusted setups then they had a step two that step two these are the number of participants that were in those ceremonies so if you can get to 16 people from the loop ring that did it if they all compromised then that will not be a valid just the setup and then those funds will be able to be taken so um you have 16 for Loop ring you have six participants in the polygon hermesis then tornado cache had a whopping 1114. um moving on to plaques um so like I said there's you only need one trusted setup ceremony to be correct and then anyone can use it so they're all using the Aztec clock Universal setup um it is all these you can see asset stations um on GitHub to see how they worked but they had 176 participants on in that so that's all your your ZK sync your ZK space asks to connect Aztec um all the way down the line and then you have your Starks um which do not have um any trusted setups um we'll start by recursion this is the other major thing that um people are looking for for performance so as you go through performance a recursion is the ability to take you know a big computation and break it up into a bunch of little ones and do them um together and then recursively proof back to the firm back to the top so um what this allows you to do is recruit is parallelization of proof generation so you can decentralize and have a bunch approvers doing this in parallel um so get out your Asics because you just got rid of them for the merge but uh and then it also lets you do proof chaining um so you can have the output of one proof be the input for the next proof um so what provides recursion pretty much everything but growth 16 is now doing recursion and they're doing it in better ways I guess as you move forward so um that was the other the why would anyone use growth 16 if you can't do recursion and you have a big trusted setup you it's faster um and there's it's cheaper to verify so it's the OG it's been there for a long time um but people are moving on because of those two problems mostly so now I guess we can talk about my favorite topic which is data availability as a data scientist um so uh we'll talk this is going to be about l1s and l2s so for um for the purists out there l2s would only be Roll-Ups but I'll use both back and forth so CK validiums validiums um they have their state route is on the bridge contract um and then they have their state reconstruction that's off chain it goes through a data availability committee to go on chain to be able to make it so you have that data but it is permissioned on that side of things then the alternative which is in supposed to be gold it kind of looks like bronze so forget that um is you have your state reconstruction and your state route um for the ZK Roll-Ups um down on the bridge roll up or down on the bridge contract so that allows you to um always have that data to be able to generate a proof um in case you do Escape so um why would you do a validium over a roll-up it's many more transactions per second so your imxs and your different Stark X projects are able to be a lot cheaper um so moving forward um let's look at your validiums that are currently operating you have Stark X which is on immutable X um you have randify um so rare Apex um other ones as they're coming out you have um ziki Porter almost coming out from ZK sync um and then your roll ups I'll run through these because you guys probably all know them but um you have to idx Loop ring ZK sync V1 usdk space um starknet so I'll pause for the camera then moving on there's a couple more that are um the newer ones that are coming up for Roll-Ups that are exciting um you have ZK sync V2 um which I think is under 30 days now which is exciting to mainnet um you have scroll um which is going for a higher level of evm compatibility um and then uh let's see what's next then you have all the polygons projects um which are all exciting in their own right so um let's talk about what it means to be a roll-up so everyone talks constantly about how the full security of ethereum is being inherited by these Roll-Ups so what does it mean to have the full security of ethereum you need to have the right to be able to force exit from that chain so if you have on your Layer Two if they have um decide to be against you you need to be able to submit your transactions directly on layer one to the ethereum bridge contract and get your funds so right now you have your happy user down here um that's from Walmart it's kind of creepy actually the more you look at it so don't stare at it too long um but the transaction goes to this uh the happy sequencers here they put the transaction into the layer two those transactions are batched on into the ZK circuit it produces the proof and it goes to the ethereum bridge contract as as we talked about before so um this is how it's supposed to function so let's say that the sequencers become evil and start smoking cigars um you get unhappy so your transactions can't make it to Layer Two and your funds are frozen so what do you do you take your transaction and you go directly onto layer one to the bridge contract so when you take your layer to transaction go directly to it then the validators have a certain amount of time to actually include your transaction on into the layer twos so usually it's between 7 and 14 days so if they don't do it within that amount of time then Hell Freezes Over and the chain freezes so your Layer Two cannot update State on layer one and the the zero knowledge circuit um then you as a user can produce a proof from the circuit then that proof will provide you the ability to get your funds back um so that sounds great but it's also kind of difficult how does the average user especially when these proofs are off chain um in usually sometimes proprietary how do you generate that proof um that's one of there's a few things that you have to really do to inherit full ethereum security so let's go through them to have the available data on layer one so if you don't have that available data then you can't reconstruct the state proof and therefore you won't be able to properly get your funds back from the L1 contract um secondly you need to have functional and accessible Force exits so those Force exits have to be in code and they have to be accessible for the average person they need to be able to generate those proofs they need to be able to take that data um and Pull and directly send it to that L1 contract um that for a lot of devs it's not a big deal that um there's the the front ends are all centralized but we need decentralized front ends to allow people to escape um in the case of one of the sequencers um going evil so and then thirdly um you need time delays for the L1 contracts so if the layer 1 contracts aren't have time delays or if they're immutable then it gives people the chance to actually exit from that later one but if they don't have time delays then they can just change the contract and everyone's funds will be frozen from there so um and then late then last is um this is a nuanced one but the the amount of money that you can secure from L1 is really based upon the amount of locked funds on layer one so if if everyone Force exits then the layer one contract which is the bridge contract with all the assets um they will impart all their assets to the people but if there's more total locked value above on Layer Two then somebody's going to be out of luck so the best way to think about this is a lot of the l2s are starting to get traction with direct transfers to them um they might even have their own state from different contracts and Native tokens if you have a native token on an L2 and you don't have assets backing it up on the layer 1 Bridge contract then the ability for you to inherit ethereum security and take your assets off um isn't there so it's really important that um as we move forward that any state that is native to the L2 is um is known as for that risk that it has so now um let's get in I guess this is the end of my talk so I'm trying to implore you to get into the data directly on don't be like Waldo um and be sad because other people tell you what's true and what's not on so we have um I I tell you I I'm very impressed with these projects I think you guys should all check them out they're all open you have bigquery you have Dune um the dunes getting better and better um and then the bigquery is a public database that has all of ethereum layer one um posted there they'll give you 300 credits to go um query through it um and you can really differentiate yourself especially as a data scientist um you have the the project that I actually completed um please check it out on just the QR code for it I'll have it at the end too so um if you miss it it's no big deal but um please go check out the data tables all the SQL queries on any of the different things um please uh feel free to let me know what I screwed up and it'll be good um and then L2 beat is just such a great resource I'm sure everyone's heard of them and I'm sure you've been to their Booth yesterday but um they categorize all a lot of risks and a lot of specific things happening within these different projects and I just can't recommend them enough to check them out they're doing Bridges now too which is exciting um and then the layer twos um I haven't got to the data on layer twos so I I it would be really great for everyone to check out the different we're going to a different layer modular blockchain where you have layer ones layer twos and layer threes and these layer threes are going to be deployed on layer twos in the Privacy things already are for like semaphore so um I highly recommend that I really like to see somebody want to check out layer twos and be able to do the same data type projects for them so um I have some thank yous I'd like to get get ahead of um so I'd like to thank the daily Discord Channel and and podcast for being so great um that's my favorite Discord Channel there is um if you need stud to know things there's so many helpful people um there's the zero knowledge podcast um it's so good for all the new pressing things I highly recommend them as well um l2b I'll show them again and then uh the ethereum on Foundation privacy scaling exploration team um that's not their symbol but I think that there's their new logo for Defcon is really cool so I put it up there and then I'd like to thank Justin Martin um for all of his help and getting me into this um and I'd also like to thank Mark for all your help getting me through this stuff and really brainstorming through it all and I'm trying to get through all those issues so um so I got thank you very much uh I will take quite yes please I'm sure yeah they'll have a mic for you unless you're really loud and then you can just go for it I was wondering if you comment a little bit more on the security considerations when dealing with Native assets on l2s like I get that you can't withdraw the funds on the L1 but can you double spend or can eat out there like Forge a balance or things like that given the stage rolled up no I you know I I don't think that those are concerns um to do double spends and things but I do think um because your state is the state source of Truth is really sitting on layer one so um you're not going to be able to uh one of the reasons they do some of the delays and things I believe is to stop double spend from happening um but the um and that's why they freeze the state route on layer once um but I I don't think that um yeah if you understand I won't go back over the ideas of but if you have tokens that are here um and the values more are there than on the bridge contract then you won't be able to get your money back off of there so if that makes sense good enough oh sorry hello so when you're talking about State on layer one have you heard any thoughts in regards to upcoming State expiry not yet but I I would love that I'd love to hear your thoughts on at some point I'm kind of practical on the side of things as far as what that should look like and what hello believing yeah please go ahead sorry um yeah I don't have any particular thoughts just that I think current plans are to expire state after a year or so you'd need um you know some other way to get the state I my understanding is there will be I mean zero knowledge proofs that we'll be able to construct that and prove that that's true um but then you'll also have um so you're kind of using the same different different technology but I'm not going to speak to out of turn on what I don't know it's not about it's a great question though I'll definitely I can look into it and get in touch with you about it I think there's one over here if he still has this question please I think it's too late thank you very much guys [Applause] it's Brian um our next session starts in two minutes so thank you very much I really find your seats foreign foreign [Music] [Music] foreign our next speaker's chance Hudson and he'll be talking about how to build an identity ecosystem on uni rep and unirp itself is an anonymous reputation system so chance thank you all right so thank you all for being here um oops I'm excited to talk to you guys today about a protocol I work on called unirp so before we get into it this is a quick roadmap of how this presentation is going to go first I'm going to give a semi-technical overview of what you interopter protocol is then I'll talk a little bit about improving the user experience of ZK and blockchain applications in general and then I'll talk a little bit about how we can scale ZK on the blockchain and sort of where we're at with the capacity right now right so let's dive into it uh unirp is short for Universal reputation and you can think of it as two different things first it's an identity system that gives you anonymity and it does this by creating public keys that change over time the second component is this attestation system within the system we have a testers which you can also think of as applications or just smart contracts and these adjusters give reputation to users and you can think of an attest or to reputation like erc20 contracts to tokens they create the system to find how the reputation is distributed and spent and destroyed and everything like that and so we Define reputation as two different integers positive and negative reputation and we do this so we can represent negative reputation like net negative reputation in CK proofs and in smart contracts without having to deal with assigned integers or wrapping around unsigned integers or anything like that so that's one component of the user State the other is this graffiti value which the uh the a tester can use for uh anything in anything they want to within the application and this is just 32 bytes uh and yeah the a tester can use it as they like and so one example use case of this is and a tester can allow users to register a username and so the user requests a username and then the tester attests giving the hash of the username as graffiti to the user now when the user takes an action or makes a proof they can prove the pre-image of the graffiti and move from anonymity to pseudonymity so that's one example use case that's relatively simple a more complex example would be storing the state route of a Merkel tree inside of this graffiti and then the tester can extend the ZK proof system to prove things about the contents of that tree that's in the graffiti so for example they could use an incremental Merkle tree to give achievements to the user or to track actions the user has taken anything like that so very extensible system so the two main properties of unirp are anonymity and non-confidentiality that means we can see everything that's happening inside of the system how much reputation is being transferred and whatnot but we don't know who's doing what so let's talk a little bit about the unirp identity system we build on top of a system called semaphore which is also developed by the privacy and scalability team uh and with some before we have this public private Key System that's the most simple explanation it has two Secrets attractor and a nullifier and we Define a public key as the hash of the hash of those two secrets we also call this an identity commitment in semaphore talk and we use the Poseidon hash function to calculate these values which makes it a ZK friendly protocol and that means that you can extend it to do arbitrary things so so in this example we have just a public private key system which isn't particularly useful but using a ZK proof you could extend it to do signatures for example by writing a proof that proves the secret values as well as the hash of some data that you want to assign uh yes you can extend ZK proofs in our in arbitrary ways like this another way you can extend it is by building something like unirap so we have these public keys that change over time and we call these Epoch keys because they're valid for the length of one Epoch uh and this is some amount of time that is set by the tester in question and so the epoch key is the hash of the nullifier the tester ID Epoch and nods and so as you can see because the epoch is part of the hash it changes uh every epoch we also have this nonce value which is a value between like one and or zero and two by default and it allows us to give the user multiple Epoch keys for a single Epoch so if a user wants to commit an action and then wants to give it another action but doesn't want to link their identity between uh the two keys they can use different epochies uh and still be able to prove the same amount of reputation or whatnot and just like some of where this system is ZK friendly and therefore extensible so if you want to make a signature system using these epochies you would use the exact same approach where you prove the proof control of the nullifier and the public signals and then you pass in whatever you want to hash and it all gets output yeah so now I want to talk a little bit about data structures that we use in Europe so we have like this identity system but how do we assign reputation and graffiti to users in the system and then like continue to prove it so we really have two structures um we have this state tree first which controls uh or measures whether or not a user is a member of the current Epoch and as you can see it's an incremental Miracle tree that we store completely on chain and the the leaves themselves are the hash of this private identity nullifier uh the adjuster in Epoch and then the reputation the user has at the start of the epoch so this tree control this tree has leaves that a user inserts into when they join the current Epoch so for example when a user signs up we insert a leaf or the the user makes a ZK proof and then creates a leaf with zero positive and negative reputation because they're just joining the system so they start with zero the other structure we have is what we call an epoxy so the state tree tracks the starting ballots for the epoch and the epoch tree uh tracks the amount of reputation that was received during the epoch and as you can see it's a sparse Miracle tree and we store only the root of this tree on chain and we use ZK proof's off chain to insert and update leaves in this tree and then we just post the new root along with the ZK proof to the chain and you can see that the leaf values in this tree are the hash of the total reputation owned by this Epoch key and so we determined the index of the user's Leaf in the tree in the tree using the epoch key so in the previous slide you saw that the epoch key is just smash of some arbitrary data and we're able to uh apply or we'll we're able to determine a leaf in the tree by taking the modulus of that hash uh across the the number of leaves in the tree so if we use this exact tree we would take the hash modulus two to the third because it's only three levels deep but of course if we did that we would have lots of collisions so when we operate this tree in production we use a depth of 128 so we take the hash modulus 2 to the 128th and now everyone gets a unique epochy and we have 128 bits of collision resistance so the whole idea behind unirp is users have these identities um for that are valid for an Epoch and then at the end of that Epoch they pack up their reputation and they move to a new identity and the next Epoch by inserting a leaf into the new state tree uh I didn't say this before but we have a copy of these trees every Epoch that we that we use right so let's change gears a little bit and talk about the user experience for uh zero knowledge applications so we have these proofs with unrep and we want the user to be able to uh make these proofs inside of like their browser for example on a computer so here we have a graph of the proving time for proofs of various sizes on a few different devices and yeah you can read it pretty well uh so you can see that there's a sweet spot below 30 000 constraints where any proof you make is going to take less than five seconds on most modern devices that purple line at the top is uh an iPhone from like 2016. so like older mobile devices can still do that in less than 10 seconds which is pretty acceptable performance so where's unirup where are our proofs on this graph the first proof we have is a sign up proof it's very small it's 700 constraints and all it does is output a hash value so less than one second a little bit bigger we have an Epoch keep proof this proves control of an Epoch key and also proves a leaf in the state tree so it's a little bit bigger 3000 constraints still less than one second on most devices and way over here we have this user State transition proof and in this proof we add up the value in the state tree leaf as well as the values in all of the epoch tree leaves and output a new state tree leaf with the sum of the reputation the user owns so we have to do multiple inclusion inclusion proofs over trees that are quite large but we still end up with about 29 000 constraints so less than five seconds on Modern devices and we can also execute this proof in the background so the user doesn't have to know about it and doesn't have to wait right so I think a good goal for zika applications and for blockchain in general is users not being aware that they're using the blockchain while they're using the blockchain so a lot of you probably use websites or applications like Spotify or Twitter or Reddit or stack Overflow uh just raise your hand if you know what kind of database those applications are using right no one raised okay one person raised their hand we have at least one nerd in the crowd but for the most part users don't really care about the data structures that are backing the applications they're using and blockchains are really the same they're just databases with different properties and so the user shouldn't have to be aware that they're using the blockchain so this is the the architecture of a traditional dap and as an engineer I really like this architecture because it's simple and it's sort of unprecedented before blockchain but for the reasons that I like it I think that users kind of hate it because they have to learn about the blockchain and then they have to learn about wallets like metamask and transactions and gas and gas prices and way and ether and they have to get ether and it's a whole thing and it's it's a lot for them to learn just to use a single dab so luckily if we use ZK identities instead of wallets we can build this more traditional three-tier architecture where we introduce a relay uh that that bundles the transaction and sends it so the flow would be the web app generates a ZK proof gives it to the relay the relay creates a transaction and then post it to the blockchain and the economics of why the relay would do this are sort of there are a lot of different schemes you could build like subscription models or just like free trial models uh all sorts of different things you could do and so one one note about like this architecture the relay is not a trusted entity uh the Relay can censor transactions that can go offline it doesn't matter uh the web applicant or the user can only send their ZK proof with a different relay or broadcast it to the blockchain themselves uh the relay also can't compromise the ZK proof itself because if they change anything about it the proof itself will be invalid and the proof determines what the user wants to do on chain so I think a good goal for this would be from a user experience perspective a user clicks a button and then in less than five seconds we show a loading animation and we generate a ZK proof give it to the relayer the relayer then packages it into a transaction and gives it to an L2 node who then returns an instant finality guarantee and at that point we can stop the loading animation on the front end and say okay your action is complete and even if we get a weak economic guarantee from this L2 node we can still do this on the front end and then have an alternate code path where if the sequencer doesn't include the transaction we show a notification saying oh this action failed or do you want to try again or not and hopefully that code path is relatively cold and sequencers uh include transactions as they say they're going to right so how can we build an ecosystem using like ZK proofs in this sort of abstracted architecture uh one approach I think is sort of detailed in this diagram you can see there are three different attestra applications and a user's browser and each tester application is managing a unique identity for the user inside of the browser local storage and the advantage to this is we can treat these identities more like web 2 authentication tokens and less like Bitcoin or ethereum private keys so instead of prompting the user for permission to do a signature or make a transaction we just in the background make these ZK proofs and give them to the relayers and at the same time when we use different identities for each of these websites if one of the identities is compromised because the website injects some malicious JavaScript or does something like this the damage is contained to that single a tester but at the same time we want a testers to be interoperable so if for example this review a tester wanted to get a proof of from the Cyber resistance tester how could we do that and the answer is basically oauth for ZK so for so in this example flow I'm trying to sign up for this review a tester and the reviewer tester wants a proof that I am a human being in the form of a reputation proof from this cyber resistance attester and so in this flow I create the sine of proof from the reviewer tester and then I get redirected to the Cyber resistance to tester and I get prompted to make a proof from this identity and in this case we shouldn't operate silently we should definitely request that the user Make This proof because we're going to hand it to a different third party and so if the user says yes then we prove that we have like the reputation and we sign the hash of the sign up proof to prove that we're the same person and then we get redirected back to the the original application and we can continue sign up so using this flow we can get ZK proofs from different applications into different sort of Origins okay so this is part three of the presentation um how can we scale ZK so first I want to talk about uh like where we're at right now and the limitations of our current infrastructure so when we talk about scaling ZK there's two limitations one is the the call data itself and two is actually executing the verification on chain so let's talk about call data first so for graph 16 we have about 130 bytes per proof and for plonk we have about half a kilobyte per proof so assume that we're talking about EIP like a post eip4844 worlds and we have two megabytes per block that we can use for blob data so at that point we're able to do 1300 across 16 fruits per second and 330 Planck Perth per second uh that's not not terrible so let's open door number two and look at the verification costs so across 16 of Planck both cost about 250 000 gas to verify this isn't totally true cross 16 is a little bit cheaper and they both scale up with the number of public signals in the proof but for this example we're just going to say they both cost 250 000. so the ethereum Mana is doing two and a half million gas per second right now they're doing uh 30 million gas every 12 seconds which comes down to this so we get to verify 10 proofs per second on ethereum Main net so on arbitrum and L2 for example they're doing 7 million gas per second so that number is bumped to 24 proofs per second so we see an obvious bottleneck here it's the verification cost on chain and these numbers are also extreme upper bounds this assumes that we're filling entire uh The Blob blocks with two megabytes of ZK proofs and we're filling entire blocks with verification of ZK proofs and not even factoring call of data all right so how can we scale unit up and how can we scale ZK proofs in general given those bottlenecks so this is uh a proof from the unit up system where we're generating a user State transition and as you can see there's I think seven public signals and for every user that wants to join a new Epoch we make one of these and put it on the blockchain and verify it so how can we make this a little bit more efficient we can do recursive proofs so we have the users make a proof and then they get sent to an aggregator and then the aggregator proves that four proofs are valid and outputs one proof and so we're able to reduce it by in this case a factor of four there are other things we can do like to reduce the public signals as well like cue the ZK proofs on chain and then form a hash chain of the public signals then the in the proof stuff uh to reduce it so that we don't have to Output all these public signals at once which will also reduce the verification cost and so recursive proving is really important because it changes the approach from scaling the throughput of decentralized network to instead scaling off-chain computational power which we're much more able to do like Intel and AMD do this every year we can also build Asics to make proof of it make proofs very quickly and then we're able to see the sort of uh improvements that we want to see like if we're able to bundle 10 proofs at a time we get a 10x Improvement 100x Etc and and this does introduce a little bit of complexity because we have to deal with a proof aggregator and now the user potentially has to like wait on that or potentially optimistically evaluate the aggregator but this is a decent approach to scaling right so now um that's most of the presentation I'm just going to talk about like some of my tester ideas that I think are kind of cool that we can build with you in our app or the other Universe protocol so the first is just like ZK Dows we can keep the balance uh hidden or the balance controlled by a user hidden within the Dow we could also do things like vote for proposals anonymously um a lot of interesting idea a lot of interesting things that can be done with this the next is anticipal reputation I use this as an example in one of the previous slides but what you could do is basically take like proofs that you have a web 2 identity and give your reputation for that or potentially use like bright ID or or approve that you have like poapps for example and just get reputation and then use that to sign up for other testers the third is like really simple and generic which is a recommendation system as a web app everyone has things that they use or that they would want to recommend to other people you having like reputation for good recommendations seems like a good use of the system and then the final one is one that I thought of when I was given the Devcon poapp I I really hate claiming co-ops on chain because I'm just giving people a history of like the places I've been in the real world which I think is kind of weird uh so we can use unirap to like claim co-ops anonymously and then we could make a proof that you have like a co-op and a set like for example I could prove that I have two pull-ups from the set of all Devcon UPS something like that uh and then just some nice to have things for sort of the ecosystem like infrastructure wise the first is like a ZK directory this just a directory of the hashes of proofs and human readable descriptions of what the proofs do and this would be really important if we're going to oauth between different applications and request ZK proofs because they're potentially requesting just random proofs that are specific to their application and so applications should have a place where they can request information about a proof and then return it to the user the next thing is planck we already have this but I just want to talk about a little bit more um the most important part of Planck to me at least is we don't have the space to use trusted setup that is circuit specific so with unirp we want people to extend hit proofs that we've written and write their own to build their own functionality and with gross 16 that's very difficult to do because they have to run a trusted setup ceremony for the circuits that they build and this is a huge amount of coordination and effort can't really expect most developers to reasonably do so with Planck we cut all the head out we get to just use some phase one trusted setup made by some trusted density or some trusted entities and then you have secure proofs and the last thing is easier browser proofs so we have proofs in the browser using snark.js but you have to like configure webpack really specifically and it also uses like Z key and webassembly file and a second web assembly file for a curve and it would be much easier if we had a tool that just sort of bundles all of that into a single web assembly that we can run in the browser just pass in signals and get the proof back this would give us like free asynchronous operation as well um yeah it would just be a nice thing to have uh yeah that's pretty much the end of the talk we have a few events happening related to unirap so first we have a unwrap workshop on Friday at 10 30 a.m that's going to be on the first floor at the ZK Community Hub we also have a demo at Thursday at three o'clock in the same place a big thank you to Xerox park for putting on those events and inviting us to participate uh and then if you want more information about unirap you can go to github.com unit or you can scan this QR and it'll take you to our organization homepage where you can find documentation links uh links to our Discord and a link to a demo application that we have running on this protocol uh yeah that's it uh any questions [Applause] um first of all cool cool presentation thank you for all the information uh I was wondering from a product standpoint or maybe from the user perspective how do you explain um the need to sequentially create new identities in order to remain anonymous and also if there's a way to apps like that maybe save it in the session in the browser you know abstract the user away from such a involved mechanism of creating like upsado anonymous name every every Epoch sure yeah so well the reason we do this is because they need to have uh they need to be able to prove a leaf in the epoch tree to claim reputation uh but I would say it's not as involved as it might seem we can do this silently in the background in the browser so like the user enters the web page and then the web page checks if they need to generate a new identity and if they do they generate a ZK proof in the background and submit it to a relayer and it's done um yeah there's not a way to make that less manual without changing the architecture of the system like pretty substantially though hello so how often do you see the state transition happen is it going to be peripoc or maybe there can be some um tricky there to maybe avoid this Kevin load in the computation uh yeah so the state transition happens anytime you want to move to a new Epoch so if you are participating consistently then yeah it's every Epoch we can adjust or testers can set the epoch length themselves though so this could be like pretty short maybe one hour or it could be like a week or it could be a month or anything like that and we're also planning to make it so a testers can set the max knots value to change like the number of keys the user has for Epoch so that would sort of make it so that the power key proof is longer but you also have like longer epochs so there's sort of a lot of tuning we can do with that uh any thoughts about using dads and VCC standards in this implementation uh any thoughts about using what uh decentralized identifiers in verifiable credentials from the w3c no um no I have to look into that I'm not aware of actually that but thank you thank you uh quick question you talked about grout 16 verification after eip4844 I would be curious to understand if you have like more detailed thoughts in particular two questions I was thinking of was like one uh if you don't have your uh I guess proof in call data Nissa did some The Blob you would have to verify there's some batch proof right opening your cutting polynomial Insider snark and then doing recursive proofs or I guess what is the exact setup you're thinking of uh do you mean gross 16 or do you mean Planck either one I guess right so sorry the mic's a little bit hard to hear but it's just like a question about how do we aggregate these proofs or how do we sequence them for aggregation in a post eip44 world right yeah right way yeah so I sort of touched on it a little bit but I didn't talk about it much but one approach is users can't send the public signals of the proof and then the hash of the proof onto the blockchain and then we form a hash chain on chain and then the aggregator is able to make a recursive proof using that data and only put a house chain on chain um but this still involves like the aggregator receiving the full proof and then like calculating the hash um we honestly we haven't gotten uh we're not really close to doing that yet like we're proving especially for like solidity approvers are a little bit far out um but yeah it's probably going to be some sort of optimistic system so the user experience is pretty good cool thank you thank you all [Applause] [Music] foreign [Music] [Music] thank you [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] thank you foreign [Music] [Music] foreign [Music] [Music] and the public goods and experiments that um they've experienced um in the course of developing ZK Opera uh hello everyone um thank you so much uh for uh joining today's talk uh today's talk is about mostly about the public goods and experiments so it is uh very very furry beginners I think you don't need any knowledges about the zkp or very some um typical map so I think you can just uh let's my talk very uh in a easily okay so let me start the talk so today a lot of our PSA team members are having presentations uh at Defcon uh I've been in this team from 2019 and actually today I want to share my experience uh during the team uh I'm sure that we've been keeping our own ethos pretty well from 2019 uh I can say like our ethers I mean PSE teams either is focusing on public goods and doing experiments uh let me rephrase it we do public goods that others will never do if we don't do and we do experiments because sometimes experiments are expensive and they are not it is not affordable for some other teams but actually it was pretty hard to keep the faith I mean because there are so many fun things in the ethereum world we had gone through the defy summer nft boom in maybe if you are some technical guy you can launch a layer something layer Network something so our team has uh has been working with a lot of talented and outstanding people but during those summer seasons we could see that many of our friends close friends becoming millionaires some of them like billionaires and you know actually what we are doing in the team is kind of a very difficult thing and also one of the most Advanced Technologies so maybe a lot of the teammates uh could just a very big reverse if they go to market so many people left the team uh so in the team they are left only some types of people someone who really loves the cryptography research and the achievements or someone who truly loves the public goods and maybe someone like me who has a strongly aligned value with the team so today I want to share on my case share about my case actually this is kind of a personal stories I just found it a startup in 2012 with my friends and actually it was acquired by another another a little bit bigger startup in 2014 and I swapped all my equities to the new one and after the question I tried to work very hard there because I stopped all my equities the new one but I thought most of other team members uh were not like me and I just found that oh yeah I think there is some incentive problem because the founders have like a more than 50 shares and just imagine how much you can give to the 10th member if you're a Founder it's like I guess less than one person usually like five point uh five percent then actually it's kind of very 100 times it's kind of exponential smaller incentive for the teammates so yeah actually that was what I found and just to disguise this instantly problem Founders usually should have to do some lots of sweet talks to motivate the team members and I for me this kind of situation were pretty exhausting so yeah actually I'm not saying that uh Founders are greedy because Founders should take the older responsibility to build a product and to build a company actually it pretty works well for some rocket speed Venture companies but for me it was very exhausted so from that time I just started to explore open sources and getting deeply uh into that culture so and interestingly what was very inspiring to me was the contributors are giving up their commercial incentives of their of their source code and you know in our society Source codes are the most powerful means of production doesn't it sound like very communistic like don't worry I'm from South Korea not the north so yeah by the way there is a very very bad difference that is freedom in the open source Society the contributors give up the commercial incentives uh of The Source Code by their own freedom and we do that to run the software and to study the software and modify the software and share the software by our own freedom so this value-driven community was so fascinating for me and uh because it was very very totally different with the startup community that I was being before so and I just started to get to know about the value of sharing and then finally actually everyone knows which Community is the most crazy value driven community yes we are yes we are ethereum is the world of Freedom by the etherean people where we can do everything by the code I just so I just dived into this world and just just enjoy and explore the freedom with this lovely community as a result throughout a few years of Journey now I am doing the public goods and experiments for the freedom in the team yeah maybe now is the time to talk about the copra yeah by the way then uh what kind of Freedom I'm talking about here now let me give you another story um once I traveled with my friends before I just sent stable coins to my friends to split the bill and actually my friend just added my ethereum address to the watch list of the either skin and when my maker dial position got liquidated he just messaged me oh my God where's my freedom for the privacy so actually this was the start of the zico pro project so I started to write a privacy protocol to keep my right to be free from unwarranted publicity at first I just tried to implement some member protocol in ethereum just the same as a layer to protocol using optimistic rollup signature application but because of that it couldn't hide the transaction graph and also it had a some problems like it needs an interactive transaction building process so from 2019 uh with Barry I started to write a new specification to implement Z cash on ethereum uh just using the optimistic approach so we achieved pretty low gas cost cheap transaction and also the compliance compatibility using the spending key and viewing key of the zika's scheme and a lot of uh people and teammates contributed to this protocol very chance regia Kobe or Jeff a lot of people contributed this to this protocol so finally we could ship this protocol in 2020 our first version and we could ship the second version in 2021. during the development we had a lot of discussions about the growth strategy the first option for us was about just spinning out just like he and his team but when we thought about our philosophy that we do public goods and researches what others will never do uh and what what else never do if you don't do in this point of view actually spinning out didn't make sense that much because already many teams like Aztec tornado uh polygonite ball and a lot other teams were already trying to ship the private protocol so then what's the thing that uh others will never do so because other teams have their own shareholders and because of the many things should be related to the business school business stuff uh therefore sometimes the source code also becomes the means to put means to maximize The Profit only for some people so what we've decided to do is shipping the reference we ship the reference specification and we ship the reference implementation and also we ship the reference example applications on top of our protocol and what we do is encouraging you to Fork our reference and ship your own thing for your own Target Customer because we believe the power of Open Source and we are very ready to help you everyone who wants to work and achieve your own product this is how we think about the public goods and our own events to be more detailed actually there are some reasons uh good reasons that you need to Fork zika Pro uh the first thing is like the Cooper's goal is uh to seeing everyone uses crypto payment in our daily lives like when you pay at a restaurant but when you pay at a restaurant if we have the private transaction the government cannot tax you so we need a compliance compatibility so if you ship this protocol for your own country then actually you can ship a great crypto payment Network for your own country which is pretty big business right um however because the current version 2 is pretty outdated compared to the recent advancement we are going to write new specification with everyone so we are looking for participants who will join this journey together so what we are going to do to write the new specification is like the first for the new specification we're going to include a recursion scheme and a better membership proof scheme also we're going to support various types of talking and also for the consensus part we're going to use hybrid finalization scheme using zkp and also optimistic approach and then we're gonna ship the reference implementation if many parties join this journey then maybe PS team might do only some coordination just like ethereum 2.0 I mean the consensus layer and we're going to ship the uh zico Pro to the public and we want that other commercial party do this together because we won't do the commercialization yeah so it's almost end of the talk uh today I am very very happy to share our PSC teams only those and our own culture video all so I just want to Define because this is my personal opinion I just want to Define PSC teams only those as we do public goods that others will never do if we don't do and we do the experiments uh which is not affordable for many teams and also we work for value of e3 personally which is freedom okay thank you for thank you everyone this was my stuff thank you so much any questions is this working there we go thank you for the talk um not super familiar with zko Pro but when you say it's intended to be forkable do you envision I guess I'm curious what you see as like a project potentially forking it like is it an application specific um or is it like a potential Layer Two project looking to like build off your stack as like a foundation for their L2 or L3 or Beyond yeah actually we are trying to focus on the examples like the first one is the actually we are trying too many experiments the next talk from takamiche is about the experiment we are doing which is kind of Private Exchange so because using the private exchange we can prevent the MAV actually we are not sure that it can be usable for they'll like just like uniswa but we are just doing this experiments if others want to do wants to use it and also the another the other example that we want to really want to see is the merchant service the shipping the crypto people or shipping the crypto Shopify on top of the Corporal because the copper has a private Atomic soft feature so you can exchange an nft which is a digital good with your token so this is the examples uh that we want to see and maybe we are targeting these uses all right great talk uh just one quick question I was curious if you have any thoughts on polygon Nightfall or other sort of more commercialized versions of C corpu yeah um actually we are we were very very happy that when Nightfall team launched their project because they gave us a credit they just said they Inspire from our product and project so actually that was kind of our first achievement in terms of our ethos like because if we want to ship the reference and actually not full team inspired by us and ship the project so we were very very happy and actually including polygon Nightfall team and any other team if any any team wants to work together then we are really happy to ship the reference specification and implementation together great talk I just want to ask is hmm how do you achieve a balance between experimenting and not wasting Resources by while still preserving those resources and trying to maximize The Innovation that comes out from that and thank you yeah maybe I think that um our super designer Rachel might be a very good person who can answer this one because we were trying to do a lot of some like you know that we can do some experiments without shipping it we can do some experiments with some papers right so we are trying to do some methodologies like that and our designers are doing that really well so I'm really happy for the okay thank you everyone [Music] foreign we'll be starting up again in 10 minutes so see you back here [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] foreign foreign [Music] foreign [Music] foreign foreign everything [Music] [Music] foreign [Music] foreign foreign [Music] foreign foreign [Music] foreign [Music] [Music] foreign [Music] [Music] and he's gonna tell us about a private exchange application that was built on top of ZK Opera thank you for coming um my name is takamichi today I'm going to talk about the Private Exchange yes I'm software engineer at the privacy and scaling Explorations team at Ethan Foundation and yep today I want to explore a privacy preserving decentralized exchange on gkopro for this project we are aiming for more private peer-to-peer exchange system and private exchange consists of three different zero knowledge protocols to protect users privacy and I in this talk I'm not going very deep on the technical details of the protocol but I will show how those protocols works and how the protocols are used in private exchange all right before diving right into the private exchange let me start with one question so why do we need privacy I think there are many reasons for many different people and for many different use cases so I'd like to share my opinion on this today the Privacy is important because it can contribute to censorship resistance like own public blockchain like ethereum like the transactions on the all the transactions on the blockchain are all visible to everyone so it can be censored mostly like in a smaller Networks because the number of the validators and the block Builders are there to be relatively small to the censorship likely to be happen more often for example if you want to spin up minor roll up on ethereum is is the case and for second it also can mitigate the Mev or front running if the privacy of the transactions can be kept at work to prevent the mov and front running but most importantly it is a human rights the Privacy is a human rights there is a quote from the universal Declaration of the human rights and article 12 so my opinion is we don't need any reason to work on protecting privacy we need privacy because it's human rights and we should care when we're building an application and today in this talk I will walk you through how we approach to protect privacy when building a decentralized application like private exchange all right let's get into the interesting part what is private exchange and how it works so what we are trying to achieve with the Private Exchange is to make exchange process more private yes very simple and don't let users to expose their information or intentions that's our goal here so what exchanges do exchange let users exchange their tokens in this diagram Ali sends 1.0 if to exchange and gets uh 1050 500 die in exchange there are more functionalities but I'd like to focus this one function today and there are two popular constructions in uh decentralized exchanges today the first one is automated Market maker and this kind of Exchange users can typically users can exchange their tokens without a counterparty this means that at least just interacts with a smart contract and doesn't need to anyone else doing an opposite trade the price is calculated automatically in the smart contract and Alex can exchange tokens on that price the second approach is an order book in order book Alice puts something called an order to order book and the order contains information such as token pair amount and a price in this example Alice wants to die in exchange of Eid an amount she wants is one thousand five five hundred die and the price is one thousand and five hundred die per it and if there happens to be some other order by Bob who wants to do the opposite trade attending one thousand five hundred die 4 1 8. then the order book attempts to match the orders and execute the swap transaction by sending transaction on chain and one thing to note is that order books can be on chain or of chain if the order book is on chain at least post an order to the smart contract order book then Bob finds the order on the smart contract and takes it by sending transactions and if the order book is of Jane there is typically some intermediary order book provider that collects all the orders and trying to match the orders but in either way the order is rope and Alice sense includes the information that uh order book provider or smart contracts can find a pair okay now it's time to think how we can execute the process more privately in order to make exchange process uh private we come up with a relatively simple peer-to-peer exchange system and it is some somewhat similar to an order book construction here but it tries to keep information private as much as possible so let's look at the Autobook diagram again here we can see the parts that are not private in this diagram the contents of the orders Alice and Bob sends to the order book are not private and the transaction that Autobook sends to the blockchain is not private so how do we make this more private here we use three zero knowledge protocols blindfind and socialist millionaire problem and zko Brew okay firstly let us think of how we can make orders more private in the order book system order book is the one who collects all the orders from users and make matches so Alice and Bob needs to send the order data to the Autobook provider but if they can compare prices and make a much a more P2P way by directing directly sending messages to each other then they don't need to reveal the data to the order provider at all oh wait how does Bob Reach Out Alice in peer-to-peer Network we use land find protocol here land find is a network that peers have neighborhood lists and Bob can look up Alice and prove that he can reach to Alice in the network but importantly Bob can search for Alice in the network but without revealing that Bob is the one we're searching for this so the neighbors that receives a message from Bob will never learn that Bob is the one who is searching for Alice so in this way uh Bob have like some privacy of that key um have some intention to take the Alice's order in the public aboard now a bob can reach out to Alice to execute the peer-to-peer order matching thanks to the Socialist millionaire problem here we can do the private peer-to-peer order matching socialist millionaire protocol is used to check the equality of two values but without revealing the actual values so the idea is that they have some numbers in mind let's say Bob have X in mind and Alice have y in mind then they don't directly um sending the original values but instead they derive some values by by calculating um some mathematics and also they use some random values and exchanging messages then after the exchange uh complete they can test that if the original values that Bob has in mind and Alice has in mind are same or not but here Pope cannot calculate back uh using the received value to y and Alice cannot calculate X from the derived messaging values foreign and we use this protocol to check if the price is that Bob wants and Alice wants much or not and here's how things work in private exchange so firstly Alice creates information to public board but without a price we call this an advertisement and Bob finds the advertisement on public board and search Alice in the blindfight network price matches they proceed to create transactions in this way we could omit the intermediary who acts as comparing the price and making making a match and preventing posting prices they want to public now the order is partially private because they can post an advertisement without price and now we should think about how we can make these transactions right hand part private so here comes TKO prove DK Oprah is an abbreviation of zero knowledge optimistic roll up we use ticket proofs or privacy and optimistic rollup technology as a layer to technology protocol and in essence we can make secret transfers with cheap fees we can prove that those transfers are valid using GK proofs but without revealing the contents of the transfers but there is another thing a secret Atomic swap so there is a feature called Atomic swap on zko Brew and let's say that Alice and Bob agreed on Sam Price using socialist million problem and they can create they want to create the transactions called Atomic swap to make swap transactions or a pair of transactions and has to be included in the same block so in this uh case Bob send Alice 105 500 die and Alice sends 1.0 is to Bob the two transactions have to be included in the same block and if not it's not valid transactions anymore and of course the transactions contents are secret even block Builders called coordinators cannot see the contents of the transactions so using the secret Atomic swap the transactions sent on the blockchain is private now so here is a diagram complete diagram of the Private Exchange all right so let's go over the private exchange flow again so firstly Alice and Bob joins the network called Blind fine so that they can reach to each other but without revealing that they are finding someone in the network next Alice posed an advertisement on the board but without a price and next Bob finds the advertisement and Bob search for Alice and a blind find if they successfully Bob finds the Alice he start executing socialist media on a problem to compare the price that they want and if the price are same they can create Atomic sub transaction and send it to ZK Oprah in this process advertisement is partially private and price matching is done privately and peer finding is done privately and transactions contents is also private so what I've talked today is one simple example of a privacy application and what I talked to us firstly I started with a what does this application do so in our case it is an exchange so exchange let users to exchange their tokens and in that process which part of the exchange process is should be private in our case it is an orders and transactions sent on chain and lastly I talked about how to make those informations private we use three general knowledge protocols line find and socialist millionaire problem and TK approve and as a result the private exchange does not reveal transactions content and it does not reveal price of the orders to public and it also does not reveal the peer-to-peer routing in the network but at the same time there are challenges in this constructions firstly socialist maybe on a problem can check the equality of the values therefore it's not easy to compare for example to use a range to create a match so in the application it's harder to find the counterparty elect Bob if the SMP can work only this way secondly and advertisers needs to stay online while waiting incoming social estimate on a problem messages so Alice after sending advertisement on public board she has to stay online until Bob shows up and finally users need to join blindfind Network before starting the process and also run the blind find process file in the application so this other challenges and it directly affects you the user experience of application but some of the challenges can be mitigated on the application layers but some other needs Improvement of the cryptographic protocol layer so in privacy and scaling Explorations team we do many privacy projects like applications and also cryptographic protocols to improve privacy and this blockchain space if you're interested in those Projects please take a look at our home page and Discord and I want to give a shout out to the TK Opera team once up Jane Jimmy Geo chance jelly Rachel thank you [Applause] um questions please um first quick question is the SNP uh interactive or non-interactive as interactive so you so you have to basically so like if there's a list of advertisers a user will have to communicate with every Advertiser right yes so this doesn't really scale well as it increases it's N squared like complexity for yes okay so that's that's uh yes that's also uh our ux challenges as well yeah sort of on the same vein um one question I had was um because you know you still have to do the SMP to find the price um you sort of know a general idea of the price because you have other markets that you can measure the price from and so um is there any prevention mechanism for like a mass de-anonymization of the of the price the ads uh in a way that somebody could just keep trying to uh discover the price and build up an order book that sort of matches the expectation of the ads because there's no obligation to actually trade on any of these ads right so you could just try to Brute Force um figure out the actual order yeah yeah yeah um so for current implementation we don't have like such features but as you said it needs like the uh many times of SMP trial to find a the price matches and also we don't have like the prevented to prevent such Dittos like types of attacks right now so yep we need to work on that ux Improvement as well thanks thank you foreign [Music] uh we'll be starting back up in six minutes see you back here in six minutes [Music] yeah [Music] do you know I will give it a life [Music] [Applause] [Music] foreign [Music] thank you thank you welcome back um next up we have URI kerstein and he'll be telling us about the different ways um formal verification proofs can go bad so early [Applause] hello everyone I am worry from sartora and today we're going to talk about bad proofs in formal verification many times people think of formal verification as the Holy Grail or highest insurance against bugs and yet sometimes we still see that even projects that have been formally verified still contain bugs for example this is a high profile case where uh we in sartora have formally verified a specific thing about one of our customers and it still contained a bug in deployment in this lecture we're going to see how that is possible how it happens and how can we prevent that in general the lecture is going to be divided into a few different parts at first we will say what is formal verification what are the proof that you get out of it and what does it mean for a proof to be bad then we will showcase two different types of bad proofs and we'll also show you how you can sometimes tell if a proof is bad or not and at the end we'll show you a real life example the same example I've shown you on the previous slide we'll delve into it deeply and understand it let's go formal verification is the process where we take a piece of software and try to see if it behaves According to some pre-fined set of of rules we call those um security Properties or rules we call them specifications and they're one of the two inputs along with the code that we want to check that we need to feed into the the software that that executes the formal verification in this case for example Lister approver and then after the prover gets those two inputs it can give us one out of three possible outcomes one a proof that means that in all different cases ways behaviors or States the program behaves as intended another is that it doesn't always behave as intended and then we get what we call a counter example which is um often showing us something that is hard to find and very often a bug in our program an unintended Behavior and also the tool can time out and then in that case we don't know if the software is behaving well or not let's show that with a simple example of a facility pseudocode this is a transfer function we're driving we're transferring some amount of tokens from one address to the other and an example of a specification that we can give to to the software that does the formal verification is this invariant that says that the total supply of the token is equal to the sum of the balances of all addresses that hold a token which makes sense if we feed those two inputs to the prover we will get a bug we get a counter example A specific scenario where that invariant doesn't hold anymore here we can see that when we pass an amount of 18 from the address of Alice to the address of Alice we get a violation we see that Alice's Alice's balance grew by 18 tokens which it shouldn't have now the invariant is broken and that's why it's a bug however if we try and fix that solidity code let's say to this implementation which doesn't contain bug will actually get a proof and what does that proof mean it means that the sum of the balances of the token for all addresses before and after the transfer is equal and then we can be sure that it's true no matter which addresses we chose and which amount we have chosen and that's one of the key advantages of formal verification it is exhaustiveness we're not checking for a specific set of inputs but instead we're covering the entire input space that way we're able to find bugs that are often or sometimes missed by humans another big Advan is that whenever we get a violation it's very easy to verify it we get specific numbers specific addresses and we can see and run it ourselves and see if we get a violation or not and try to track and see exactly where the problem lies and also we can get a proof of correctness which is something great that we can't really get through many other security methods for smart contracts however those proofs are very hard to verify obviously we cannot go ourselves through all the different inputs and see that it's true or not else would have done that instead of trying to prove it right and actually sometimes our proofs don't actually mean what we think they do sometimes we're proving something completely different from what we've intended and those proofs are bad because they can give us a false sense of security and if we have a false sense of security we might upload code prematurely that might contain bugs let's look deeper into those specifications I've been telling you about um this code that you see on the screen behind me is written in a specific language called the circular verification language used for writing specifications but um the same principles hold no matter how you represent your property the general Anatomy is dividing into three different parts the first is the precondition then we have an operation and then we have a post condition and if we look at this specific example here we check that the transfer function behaves as intended the way that we do that is that at first our precondition we check um the balance of Bob for a specific token that's our precondition and we keep that number then we do the operation in this case it's the transfer function and after the operation we check the post condition we check the balance of the tokens of Bob after the transfer and see if it is indeed equal to the balance Bob head before the transfer plus the amount of tokens we have transferred to Bob let's look at it more visually and when we Define a property we Define a starting state by the different constraints that we give and that's the circle the arrow represents the operation and then we want to land within one of the desired States one of the states that satisfy or in which the assert expression that we had holds okay so the desired behavior is that every state that is within the starting States if we do the operation if we draw the arrow we end up within the green circle and all of them live within the space of possibilities when we get a violation we get into a counter example that means that we started from one of the starting States we did an operation we have an arrow but that Arrow doesn't land within the desired States that's a counter example that's a violation that's what um might be a bug and the interesting thing to note is that if the tool cannot find a counter example for the specification or the property that we provided it then it will output us a proof and that is because in in logic we Define a false statement as a statement to which we can give a counter example that part covered on the first section of this lecture what are the proofs and what is formal verification and now we look at two different types of bad proofs the first step is what we call a vacuous rule or vacuous proof vacuous is something empty or mini glass or insignificant and this is better shown by a real life example I am 29 years old and I don't have any children yet and I'm claiming the following statement if I let my children drink Colombian coffee they will sleep better at night is that statement true or false so if we want to say that the statement is false we need to provide a counter example so we need to start from one of the starting States meaning you need to choose one of my children then you need to do the operation meaning let them drink Colombian coffee hopefully not too hot not to injure them and then you want to see that they actually do not fall asleep at night can you do that you cannot do that since I don't have any children and therefore there is no counter example to that claim if we don't have a counter example to a claim the claim is true here it's true vacuously we don't have a starting state so it's true however I can also say something that apparently seems completely contradictory I can say that if I let my children drink Colombian coffee they will not sleep at night sounds completely the opposite yet the same principles still hold you still cannot provide a counter example here I still don't have a child and therefore that statement is also true if you look at it visually we just don't have any starting States and we don't have any arrows and therefore we cannot have a counter example let's look at a code example this balance of function belongs to a token of open Zeppelin and the interesting part about it is that we require that the address that we check the balance of cannot be addressed zero one of Sir torah's employees was working on open Zeppelin contract as well as writing a rule of property that he thought should hold on the tokens of open Zeppelin however that rule that specification contained an error you see in the middle of the rule we have this requirement that the balance of the adverse zero for a given token must be zero that can never happen because every call to the function balance off for the address 0 will revert and if it reverts in particular it never returns back the value zero therefore nothing can satisfy this requirement if nothing can satisfy this requirement we don't have any starting States this is a vacuous rule and therefore it doesn't actually matter what appears later in the rule after that requirement it will be true no matter what we do and in fact I could try and insert something completely ridiculous I can assert that 0 is greater than one and that would still be true because you cannot still give me any counter examples and when I rule is vacuous we can prove anything we want unfortunately vacuous rules are not just an academic concern for modification has been used a lot on Hardware along the years and Studies have found that about 20 percent of specifications written on Hardware on the first time are actually vacuous and whenever they are vacuous they hint at a real problem either at the code you're trying to verify or your specification so this is a pressing problem but fortunately we have some ways to try and catch them and one of the ways that we use in sartora is is doing what we call a reachability check what we do is that we take the same specification as before exactly as it is and add something at the end we add an assertion of false obviously assertion of false is always false so we want so we expect the rule to fail we expect to get a counter example right because because nothing can can satisfy false however if we do not get a counter example if the rule is proven that that means we didn't actually reach that last line of the specification we didn't reach that last line of the specification because that requirement um erased out all the possible starting States it doesn't matter we never reach that last line and that's how we check that we we say something absurd at the end and we see if we fail or not if we don't fail that's the problem that's when we know that a rule is vacuous for completeness I will say that usually when we have vacuous rules it's not due to one um One precondition One requirement that is never satisfiable it's usually due to a combination that um their uh intersection is actually empty and not satisfiable but each part by its own makes sense for example here if you require that X is smaller than y maybe we can we can prove the property let's say we add another requirement y smaller than Z now we limit the starting state only to be the intersection and if we add this third requirement that Z is smaller than x then we have no intersection anymore here by transitivity we say that X is smaller than itself and that's never true for any number therefore we don't have any starting States and this rule is vacuous but note that some combination of those requirements could make for a sensible rule moving on to the next type of bad proofs we're going to talk about tautologies a tautology is something that is always true um and therefore is actually not telling us anything useful about the code that we're trying to to prove things on for example let's look at uh this Rule and this rule tries to check again um the Integrity of the transfer function we check that the balance of a recipient is zero then we transfer some positive amounts of tokens to that recipient then we assert that the balance of tokens for that recipient actually grew however this rule on the screen is wrong because we didn't check the balance of the user before and after the transfer we checked that the balance of the of the user after the transfer is not smaller than itself and this is something that's always true for any number and that's a problem because in this case we didn't actually check what the transfer function is doing we're checking something that is always true the transfer function could burn all the tokens could send all the tokens to me or it could do something entirely different and not move any tokens around we don't know we didn't actually check that if we look at it visually at autology something is always true so it encompasses the entire space of possibilities so here the arrow representing the operation lands somewhere within the space of possibilities and therefore is true the problem is that we could have drawn any Arrow we could have done any operation and it would still be true so we didn't check anything specifically about our code at all the way and one of the ways that we find dotology in the insert Aura is that we remove all the preconditions and all the operations just live the record the assertion at the end and see if it still passes or not if it passes it means that it it does not depend on the requirements and the operation it's something that is just always true and therefore it's a deutology so here in this example we just remove all the lines but the last two and in this case this rule would pass it would always be true and that hints that it's a tautology and now it's time to delve into the real life example I've shown you briefly at the beginning of the lecture before that I need to introduce you some new notion a notion with an invariant an invariant is something that is always true in particular it's something that is kept after doing some operations in our case is it's calling functions of a smart contract the way with the reproving variants is by induction and you might be familiar with it from school it's similar to proving by induction things about natural numbers we have the base of the induction we're trying to see if the condition is true right after calling the Constructor and then we're doing the step of the induction that is we assume that the invariant holds we operate some function of the contract that must be public or external and then we check that uh that it's still true after the same condition is true after if we do that we get that the invariance is true the same when the induction proved that things are true the interesting thing here is that we check for every possible function of the contract in this example belongs to notional which is one of the tourist customers and they were using our tool and writing a specification and we'll go over it it's a more complex example because it's a real life example what they tried to to prove here is that an asset in their system and their system allows lending and borrowing at fixed rates by the way an asset cannot be counted both as a bitmap asset and as an active asset and here we see that they try to reach that conclusion at the end the highlighted line at the end says that the same currency can be both active and bitmap the first line here is just a natural requirement on the index of the asset and is not very interesting and the rest is where it starts to get a bit more complicated here we see we have one statement that says that the Vietnam currency cannot be zero and that will always have to be true then we have another statement that has an or sign and if we look at the first part of it we see that we require that the active on mass currency is equal to zero so if you look just at that part of the or statement we see that the Vietnam currency is not zero and the active currency is zero so zero and non-zero are always different so the conclusion is true trivially true what happens if you look at the other part of the or statement here we require that the active Mass currency is zero emotional system assets are represented using 14 bits plus two more bits for a mask so the mass impasses all 16 bits and the unmasked encompasses the lower 14. but if the mass currency is all zeros means that the and mass currency must also all be zeros therefore it follows that it is zero itself and we have actually reached the same statement as we've reached before meaning if the bitmap currency is not zero and the active currency is zero then they must be different and that's always true because zero is always different than non-zero and in total we get that this entire statement although complex is there a statutology hard to see and you can understand why uh the person that wrote it could make that mistake it's not apparent but this is indeed a tautology and unfortunately the code was deployed uh thinking that it cannot contain that bug and unfortunately it did the bug light in this specific function enable bitmap currency and it's a bit hard to see the bug itself so I'm just going to show you the exploit scenario the exploit scenarios goes like this we enable the bitum currency for an account let's say if we deposit a second currency for that account uh let's say die and then we try to enable die now as the bitmap currency due to the bug now dye will be counted both as an active asset and as a bitmap asset that means it will be counted twice it means that the user collateral will be larger than it really is and therefore we could borrow more money than we should and therefore we can drain funds from the system fortunately this bug was found by a white hat so no real damage was done um notional had to pay the maximal uh Bounty amount of 1 million USD but no real damage was done the interesting thing is that these invariants I've shown before is pretty close to the correct one the correct one the fixed invariant will look something like this actually looks simpler in this case and the interesting thing to see is that if we run this invariant on the buggy version of the code we get the exact same counter example where you set one token as a bit of currency deposit another token then move the Bitcoin currency to be the other active currency we get this exact counter example notional used some of the top Auditors in the field and form of vacation and missed it however it's interesting to see that if they use funnel verification correctly they could have caught this critical bug so not only can we catch the bug with the fixed rule version we can also verify the fix right because um the specification doesn't change when the code changes so we can run it again and more importantly this incident was a catalyst for Satora to invest in ways to catch bad proofs and ensure quality of specifications automatically and today you could have caught that um the buggy invariant was a tautology using the method that I've shown you before meaning that we just take it and require them we require this condition without having the precondition without having any operation we would have seen that this would hold because it's a dotology and therefore this mistake would not be possible with our tool today and we're still working on more ways to ensuring qualities of good specification and catching more possible bugs and and bad types of proofs to sum it up specifications are written by humans just like code is and they're equally or even harder to write this is something you have to keep in mind you shouldn't expect your specifications to be perfect like you don't expect your code to be perfect therefore you should always check your spec it's good this other another person reviews it if an expert reviews it and also even better to use some automatic checks like we insert or provide you you should always suspect what the tool gives you like you would any other security tool when you get a bug it's always a good result because you can always check it and see if it's good or not however when you get a proof you don't have a good way to verify it and the best course of action is to be suspicious of it don't take it blindly and just upload your code and obviously we could have seen that writing the correct specifications you can still find bugs that are worth millions or billions of dollars thank you [Applause] I would like to take some questions thank you so great presentation um so it seems from from what you explained that um when you have a bad proof uh that is uh vacuous you can just detect it just with a coverage report let's say in which part of your of your specification was never reached so it's it's it's like immediate but when you have a tautology it's more difficult because you will need to remove let's say preconditions and removing precondition you can have like 10 preconditions you don't know which one perhaps a combination of this uh so what is what is your view on this is is it is it correct that is easier to detect vacuums than tautologies okay so one thing I want to emphasize um the checks that I have shown you don't catch all types of acuities and not all types of tautologies just catches only the simplest types and I've chosen to present them to you because they're the easiest to explain in a 20 minute slot they're actually more involved checks that try to to to cover more and more cases we don't cover all of them currently it's an ongoing effort in sartora or in full verification at Large maybe it was not clear so I'm just gonna say something simple the removal of all the preconditional operations is done automatically by Satora it's an automatic check so in that sense it's not harder so if I understand the question correctly you say how can I fix it which part of this statement causes it to be a tautology and that it indeed sometimes more difficult but it's something that you can do systematically say um your expression is an or expression you can check each part of the or independently and see which branch of the or is something that is always correct if something is always correct it means the entire statement is always correct therefore this thing causes the Acuity and this is actually a feature that's very close to production in sartora maybe I'm wrong it's already in production maybe I'm not so so keen on the details but it's really close to production if you have an implication same thing you can see that um maybe um the premise that's the part before the arrow that comes before the conclusion if it's something that's always false you will have a rule that's vacuous so um depending on the structure of your conditions you can learn more intelligent things about them that can guide you it's still not perfect but this is something that we're actively working on there is time to another so thank you all foreign good all right hello everybody so my name is Jonathan Alexander I am CTO at open Zeppelin I'm the co-founder of forta and in this talk we're going to be I'll be covering research and development on decentralized threat detection Bots there are a number of um contributors to this presentation whose work I'll be citing here um a lot of the information and research is been conducted by these individuals it includes Christian Seifert who is researcher in Residence at forta who is lead machine learning engineer at open Zeppelin Dario laboulio who is security researcher at open Zeppelin we have a number of independent researchers and we also have teams at nethermind and lime chain who have contributed to some of the techniques that we'll be discussing here and so I'm going to start with a little bit of background we are starting to see increasing Acceptance in the space that security needs to be end to end and that part of end end is secure part of end-end security includes runtime monitoring and threat detection we have multiple leading security audit firms uh open Zeppelin chain security hellborn mixed bytes who are beginning to make recommendations along with their audits for a smart contract monitoring that can be done post deployment the recommendations include covering protocol assumptions and invariants critical protocol variables uh known protocol risks that have been considered acceptable the use of the use of privileged functions and the transfer of privileges on chain and cross-chain synchronization and oracles and keeping track of the state of data that's meant to be kept in sync across these different environments and also external contracts and protocols that a that your protocol May rely on as well as identified attacks that that may follow certain patterns that are already known and this monitoring is meant to cover benones the things like specific attack patterns that we might see so that's one thing when we want to we want to monitor the known unknowns which would be places in the protocol where you know you're not sure what might happen and you might want to monitor that and then very importantly the unknown unknowns so we might have things in our protocols that we think are invariant and will not change and yet if something could happen that was unexpected we would certainly want to know about that so the challenge though with runtime monitoring is if you take that whole list it's very difficult to implement the full set of monitoring that you might want especially when you get into some of these areas that are not the responsibility of the protocol team themselves such as other protocol implementations and their risks and updates to those as well as things like the different attack patterns that have been seen in the ecosystem but also maybe new and emerging and we see new attack patterns emerging all the time so we uh at open Zeppelin we started working on post deployment tools and specifically monitoring a few years ago and this led us to the realization of the challenges here and we decided to build and along with a number of Partners now have launched a decentralized network for runtime threat detection it's called forta I'm going to be citing some of it in this uh in this presentation and a lot of the research that's here has been done by people working in in and around Florida Florida you can go check it out it is it's a decentralized network of scan nodes there's thousands of scan notes running now across a number of uh number of main Nets and the way it works is that you build Bots threat detection Bots they're deployed as containers onto the network they're run on multiple scan nodes and it is those results then that are brought together to detect threats so um so and the idea here is that for protocol teams we um I mean sorry am I like literally standing right in front of the slides the um the uh for protocol teams back to the prior Slide the the challenge of trying to do sufficient and real end-to-end monitoring and threat detection now is not just the risk the team doesn't just have to do it themselves they can take advantage of a community of threat detection we can work together to have a real complete solution so um first of all I'd like to share with you some observations from research that's been done on threat detection that hopefully you'll find a little bit interesting the first thing is that as we look at the history of attacks we begin to see a very common pattern of a set of stages that an attacker goes through and this starts with funding obtaining funds that will be used to carry out the attack um it moves to preparation and there's a number of steps that are common in different kinds of attacks it may involve deploying a smart contract which is going to be used to carry out the attack it may involve token impersonation uh it may in uh involve the transfer of privileges or making use of privileged functions and then there are different kinds of fraud techniques that we see being carried out to prepare for stealing funds and and we'll get into some of that a little bit later the next stage is exploitation that's where the actual attack occurs in this stage we may see flash loans we may see private transactions we may see re-entrancy minting anomalous transfers anomalous functions large balance changes things like that and finally we have the phase of laundering which is where the attacker takes the funds that they've obtained and they move them to try to uh you know hide the trail and uh and exit with funds one of the other interesting things in reese in the research is that looking at a quite a large number over 180 attacks over the last three years specifically in defy we see that over half of the attacks are not have been non-atomic and what that means is that in the exploitation phase it is it is more than one transaction and typically many transactions that's used to carry out the completion of the attack and why that's important is it means there is a time frame over which the exploit occurs and therefore early detection could be useful to taking action and possibly mitigating the full effect of the exploit so um so that's another another interesting fact another observation is that attackers often use more than one account this may be obvious but as researchers have looked at this and you may have seen this in some other presentations we can use heuristics to associate attacker accounts uh through and then by associating attacker accounts into clusters we are able to track and attack through those stages as I just talked about even if many accounts are being used as part of that we have examples of using heuristic-based approaches such as a connected component graph algorithm which would allow us to kind of graph together the these connected accounts and then track them as a cluster another observation in over 40 percent of the attacks the attacker deploys a smart contract to execute the exploit these attack contracts differ from benign contracts in very clear ways and here I'm on on this slide I'm citing um a uh some research conducted where over ten thousand smart contracts were sourced from Lua base 155 external accounts were identified as tagged as having an exploit in ether scan and then those accounts were associated to a set of smart contracts that those accounts deployed so we had a set of benign contracts and we had a set of um uh malicious contracts using a classification of the top 100 op codes in each of those smart contracts we see incredible success in being able to identify contracts that are benign so in this case of the benign contracts night with 98 accuracy the classification algorithm is able to say that is not a malicious contract and in the case of the set of malicious contracts in this case 21 malicious contracts the classification algorithm was ident was able to identify 17 of those as suspicious or likely malicious so this shows promise in the ability to just identify among a smart contract if it falls into the category of potentially malicious taking that a step further we also see that when we get into the details not not just kind of like which uh op codes are used but we look at the patterns of the op codes in use in these smart contracts that are used for attacks we see that they follow very similar patterns and so if we look deeper into the patterns in this case taking a even more advanced kind of approach to classification where using techniques from natural language processing grouping together op codes into groups collections in order and then analyzing the use of those different groups again with classification in this case the researchers looked at over 12 000 benign contracts over a hundred malicious contracts um and then using the uh the tfidff TF IDF um NLP technique of grouping feeding it into the classification now we get even a higher level of precision in this case 88 precision where the technique and the classification would predict that a contract was malicious when it predicted that it had a fairly small rate of error so 88 accuracy and also in this case the researchers went back to a number of known attacks that were that have been historically analyzed and carried out via um smart contracts and some of them are cited here the wintermute audience inverse Finance Finance exploits and in that case this technique was able to predict in almost 60 percent of the cases that the attacking contract was malicious so again promising areas of of research that we can identify malicious contracts um the last observation I want to share with you relates to fraud uh fraud is an attack um that is usually carried out with social engineering or some kind of web to attack where um in the case of as the examples given here ice fishing uh an attacker will get users will trick users into signing approval transactions that give the attacker control over their tokens these also produce detectable on-chain patterns and so in this case you can we can use heuristic based approaches just to look for these patterns of you know accumulation or repeated kind of actions that might be carried out by a single account or again a cluster of connected accounts that are um that are carrying out some kind of fraud attack on a lot of users and then and then you know going to steal those users tokens they might do it we've seen this happen in in certain cases we've seen attackers who do all the preparation up front and then they carry out the attack all at once we've seen some cases where the attack just continues over time but in this case using a heuristic based technique um in and then using public data about ice fishing attacks that occurred over a one week period in the month of September this technique was used was able to with 95 percent Precision predict that a uh that ice fishing and an ice fishing attack was underway so in this case it made 12 predictions that there was a nice fishing attack and or I'm sorry it may have I probably have the data runs probably like 13 and um but it was 95 percent accurate it's a very very low number of false positives and of the 21 ice fishing attacks that were known publicly to avert that we could see historically had occurred that week it actually predicted 12 of those so again these are promising kind of this is kind of the research that's going on and some of the things we're seeing so now what I'd like to do is share with you some threat detection bot techniques that can be used to detect some of the various kinds of threats that we're seeing um there actually will be a workshop tomorrow on photobot development if you're interested I think it's tomorrow afternoon I don't have the exact details but it'll be useful for you to know just up front that because I'm going to reference it in a couple cases that photobots you implement handlers you can Implement in JavaScript or python one of the things you can do is initialize you have your own environment it's a Docker container and then you you kind of subscribe to either handle transaction or handle block and you can pretty much do whatever you want you can also look up other alerts that have been emitted by other Bots and ultimately what a bot does is it if it has a finding if it if it finds some kind of threat or something that one alerts on it shares that which then gets published and again other Bots can take advantage of that or users can take advantage the first technique that I want to discuss and on I have three techniques I'm going to talk about and in each of these I'm providing some examples of bots these examples if you go to the documentation on forta you'll be able to find references or if you go to the Ford Explorer you can also look up Bots by name and they most of them are open source and you can link there are templates that mostly are what I'm covering here that have been provided by either the Florida Network or nethermind or lime chain so the first technique I want to talk about is multiple Bots working together in a group to Track Attack stages and then to with increasing confidence as an attack goes through stages be able to to express that there is potentially attack underway going all the way to complete confidence that an attack has occurred this is done with having a set of bots that atomically detect some of those things that occur in funding in preparation in exploit in laundering you can have individual Bots looking for individual things like suspicious contract creation contract spoofing ice fishing large transfers money laundering you can also have Bots that identify clusters of accounts and we have there are example Bots now available where you can see the source and how to do that and finally once you have these Bots who are each kind of making an individual Discovery then you put that together with a pattern matching with pattern matching on the various alerts and being able to then declare with some level of confidence this is what's going on this is the possible attacker and eventually this is the party or parties under attack and so um this there were there's some Bots that you would be able to find the technique in the Florida Community has been referred to as a combiner bot the second technique that I want to discuss is simulation in this case you can have a bot that literally while it's executing forks mainnet with ganache with something else you can fork mainnet and then run simulations to detect issues there's kind of two techniques that have been used here one is if you have a known protocol and you know that there are if if certain things happen on the protocol then you would know that there must be something wrong perhaps an attack underway you could you could execute those in simulation after every single block that's been mined and alert if the if the transactions failed an example here is a liquidity tester this is a bot that's been written to take a defy system identify the top 10 current liquidity holders and simply execute their ability to exit their position after each block and in the case if any of them were not able to exit their their position it would alert the other technique here is to test contracts and we have a couple of Bot examples one that uses fuzzing one that doesn't but in this case the simulation is to take a deployed contract Fork the network and then basically try to execute every single method knowing nothing about the the contract itself just detecting it through code and trying to execute it and looking to see if any execution results in let's say a large funds transfer or some other kind of unusually anomalous occurrence that would say there is something this this contract is trying to do something or it seems to result in a big a big transfer of funds and therefore is suspicious the final technique I want to talk about is the use of machine learning in bots in the case in this case you can take a machine learning model you can serialize it to a file in the case of Florida you can deploy models with the bot and you can use the initialize function to load a model and then in your Bot you can execute the model this has been used successfully already in a number of bots that do anomaly detection uh and also a malicious contract detection so in this case for example there is a smart change detector bot that I've referred to here and a Time series analyzer that they use um they use machine learning algorithms I know one of them is a library that actually The Meta team developed that is very good at detecting outliers uh and um D deviation with a limited amount of false positives so it's pretty good at saying you know this is a real outlier and this is a you know there's a price change that occurs here if we're looking at on chain prices or there's uh if if you're looking at let's say the balances or certain uh values in your smart contract variables you know it can detect over over a period of time an outlier that is uh that is perhaps meaningful in something you should look at and then in the case of the um the machine learning for smart contracts there's a bot called the contract contract Destructor and some other Bots that do similar things where they literally uh take a deployed smart contract to address and they um they decompile it to get the op codes for the smart contract and then they use these machine learning models kind of that I referred to before to pattern to analyze and classify that smart contract and and therefore predict whether the smart contract is malicious or benign so these are all techniques that are already now in use and further research is underway on how to improve these um these kinds of bots so I want to wrap up just with a couple of challenges um and you might have been thinking about some of these as I've been speaking there are challenges here that we're all dealing with um Atomic attacks are obviously a challenge so I said that well we have history to tell us that more than half the attacks are non-atomic but of course that means that 40 to 45 percent of the attacks are Atomic also private transactions can be used and so this is a challenge for threat detection and monitoring another challenge is that there are teams who want to monitor in secret that would prefer not to monitor in public and and they may have good reason for that for example if there is a known vulnerability and you want to you're you're in process of trying to deal with the known vulnerability you may want to do that privately and not on a public system and then finally we have the challenge of response latency with go which goes to how quickly can we alert but also can we respond and is there even anything we're able to do once we've detected um these are things that Ford is working on there are already some things available for private but these are also areas for future research um uh latency by the way in the Fortin network is about 40 seconds from Mind Block to alert but things that are being looked at are possibility of trusted private scan pools for people who want to monitor in secret pre-submission or mempool transaction scanning on-chain alerts which would provide a possibility for on-chain reaction and other systems that can be used for automated action and follow-up open Zeppelins working on these things for the community and many of the contributors I mentioned are starting to work into these things and um and to wrap up I would say if you are interested bot Developers security researchers data scientists I please go to florida.org and you can learn more and find ways to get involved and there's various grants and incentives available for people to get involved and with that undone and if there's time I'll take questions [Applause] thank you for that sorry thank you for the talk today um I find the Bots incredibly useful for teams but what I'm curious about is what what happens once it's once the bot alerts a team what does the team have to do to them like you know make a fast impact right yeah that's well I mean in this conver in this presentation I wasn't so much getting into that but that's obviously a big topic of conversation for everybody um I think we are starting to see um well well we have seen many attacks in the wild where teams have paused um their smart contracts and we even saw a chain pause the whole chain um we are seeing more discussion about that because I think we're we're definitely headed down the road where emergency response capabilities may be an acceptable thing to have on your protocol but it needs to be contained it needs to be limited we need to kind of have some level of Delegation to a set of responders so that they can maybe do fast response but with limited control right so we're starting to see protocols implement this and teams talk about it publicly um you know I think you may have heard some of the leading D5 Protocols are starting to discuss this so pausing and then having some capability to uh you know deal with the attack is is kind of a base point but there's a lot to get into there because you know even pausing depending on the nature of the attack may not solve the problem right and and I think the important thing here is that we all have to start thinking about this because if you go back to the you know the unknown unknowns it's just impossible that we're going to catch everything up front and so yeah but but it's not solved that's that's the true answer yeah right um thanks for your talk Jonathan um speaking of things that you do after the attack uh what is open sibling doing today with that information how does uh open sibling harness that information to transform that into knowledge that is the first question and the second question um is there anything to do besides a smart contract audit in terms of things that you can do before that attack happens like scanning the the mempool or or having some realistic in there well a couple two parts to your question um the first first one is that um uh open Zeppelin is in our audit practice we are now when we do an audit we typically also make recommendations because about what should be monitored because the Auditors have good insight into well we're training on our Auditors more and more in how monitoring can help so then the monitors also the Auditors also understand the protocol and they can say to a team you know these are things you should think about monitoring and production because if our audit didn't catch anything or these may be just live risks you're going to be exposed to open Zeppelin is also working on tooling to automate the creation of monitoring and we have like if you've seen our contracts wizard we're going to be starting to build in things that if you use open Zeppelin contracts will auto-generate monitoring templates for you and then we are working we have a product we have a product platform called Defender we're we're working on how to automate those responses that we were talking about like how you could at least get to where you could pause as quickly as possible as an example so we're doing all those things and we're collaborating I know Forte has many multiple I mentioned chain Security in halborn and mixed bytes are all doing some similar things I think yeah yeah we're good thank you foreign [Music] awesome you might want to turn down the volume slightly because I'm quite loud ah is it good yeah this is a lot better I'm not gonna blow your ears out sorry guys um my name's Heidi I'm a part of the special investigations team at coinbase and I am super excited and also very honored to be able to present to you guys at Devcon rug life using blockchain analytics not only to detect illicit activity but also to track stolen funds and hopefully stay safe and I really like the previous presentation actually right before mine because it was all about sort of detection right but this is about once your funds kind of get stolen what do you actually do so quick tldr I know we all don't have time to be sitting around a panel all day so if you've got places to go tldr is be paranoid be paranoid that is literally the end of this presentation the rest of this presentation is going to be full-on paranoia mode you're going to want to lock down everything throughout your computer and just give up on things but if you want to stick around for the rest of it let's go for it so first and foremost we need to find what illicit actually is right there are a lot of people who say oh scammers are illicit dark markets are bad thefts are also Bad true Traders you know who do Market manipulations they're also bad oh fact entities more recently also bad like we've got a whole like swath of different Terrible Bad actors out there but today we're only going to be concentrating on one which is theft I personally believe that thefts are like the worst thing that can happen with this ecosystem right first and foremost you see millions and millions of dollars like just last night we saw 100 million dollars being drained out of mango Finance right that's horrible for the entire ecosystem doesn't matter of what chain it happened on right not only that but it also drives more regulator scrutiny do we really necessarily want that we want to be able to develop right we want to be able to build without having to even consider things like that so this is why we need to start thinking about things like how do we prevent this kind of stuff now I don't want to spend that long on this I think we need to talk about different theft typologies out there now I largely bucket into three different sort of categories first and foremost you have the dev slash team initiated thefts right AKA we all know these Rooks right what does a rug actually look like first and foremost you've got some token dumping you've got unlimiting or even limiting usually of functions as well as cashing out of any and all proceeds possible now sometimes with rugs what will also happen is websites will 404 and obviously social media accounts will go totally dark now the biggest bucket though are third-party thefts right and those I actually put into two different categories first and foremost you've got hacks right third parties hack into places all the time right for example let's think of these different Sexes that have been hacked in the past couple of months right and also you've got another thing called Market manipulation as we all know like with mitt what happened with mango last night now what exactly do we Define as a hack well a hack is very similar actually to a rug right the only different I would say In This Very generalized sort of rule of thumb is that with a hack you usually don't have the website of the team so I don't know I think it's not working um this video of the team also going down um so you don't have that actually happen with a hack now um but what you do see is obviously the cashing out and of proceeds extremely extremely rapidly and obviously The Dumping of funds now in terms of Market manipulation what usually happens there is a is usually a flash loan will be taken out nine times out of ten and what will happen is there's a certain pool is manipulated right where one token's price is arbitrarily pushed up for example what happened last night with mango right price automatically gets pushed up then suddenly a dumping of that mango right and then in exchange for tokens that you know actually have value and finally there's another bucket of thefts right and I'm not going to go too much into these but these are sort of more third-party service attacks so think of for example when you you're in a Discord right you're managing a Discord you probably have a whole bunch of different Discord Bots out there right they're doing a whole bunch of different things but sometimes those Discord Bots get exploited meaning that you know people who are investing in your projects sometimes fall for those exploit and then unfortunately you get their funds siphoned off of them and then they blame you which isn't really fair but it definitely happens now I actually want to go into detection methods like how do we actually detect this kind of stuff how do we find this stuff right and so there are a lot of ways to do detection Forda just did an amazing presentation on how they go about doing detection but for as a protocol what you want to do is you want to actually just set up alerting for large flows of funds that's actually something that the Florida does another thing you can do as well is use various different platforms that have listed on here to actually start you know alerting you for when sort of any sort of strange activity happens finally another thing you might consider doing is following a whole bunch of Auditors online because chances are on Twitter the Auditors will alert you probably before you know you know even your most Avid Twitter followers alert you of something strange happening now unfortunately this is all fine and great but once a transaction is broadcast to the blockchain you're screwed right you're done for so this is great we can detect this all day every day but we need to actually prevent this stuff right and so what we need to actually do is we need to start detecting this sort of weird kind of activity before any money starts to move right and so if you're a Dev there are a couple things you can do first off you want to constantly audit right your debits and credits and I know this sounds super effing oh I just got one out again um I know it sounds super boring but for example Nomad they would have been able to detect some strange stuff happening going all the way back to July and they could have probably prevented what then ended up happening at the beginning of August now another thing you might want to do too is Monitor who's transacting with your contract what are they actually doing with their funds right and finally another thing too is you want to be monitoring your site's back end but also your sites front it for example seller Bridge that's actually how they were exploited more recently right is their front end was attacked now this is all great but if you're an investor right this means nothing to you like euron could have been debiting and crediting a particular like protocols smart contracts that's not your job right so what should you be doing well one thing you can do is make sure you know what contracts you're actually interacting with right so when you're on metamask and you are trying to do a transaction before you click that button make sure that the contract is indeed the contract you want to be transacting with finally um or next rather revoke cash so for example when you interact with a contract right you before you interact with that thing to send your tokens anywhere you have to allow those tokens to be moved right now when you allow those tokens to be moved with that contract you you're granting that contract license to move them you want to revoke those every once in a while because chances are those contracts you interacted with back in 2021 or even 2020 they might not even exist anymore right you don't want to have anything to do with them right so revoke those I would say just clean them out probably every once a month or so probably and last but certainly not least is you want to monitor the social medias of your projects right and as well as all of the wallets that you hold I know this is exhausting and annoying but it's also one way to also stay a little bit more safe now I want to get to the most fun part of this presentation which is actually tracking the funds so let's say your Project's wrecked right like suddenly all the funds have been siphoned off or let's say you're an investor and you invest actually in a project and oh damn their money is all gone what happens like can I even do anything right so I'm not going to be boring you guys with many more bullet points on slides don't worry this is the last one but um first and foremost you want to learn how to read etherscan right there's no point in like pretending to track funds if you can't read etherscan right and I know etherscan is painful to read as arbiscan is as snow Trace all the other ones are but it's super super important to learn how to read a block Explorer another thing too is once you learn how to read a block Explorer you're going to be much more Savvy with blockchain analytics tools like Dune and bloxy and whatnot and they're going to be able to help you track on those funds further now there are a whole bunch of other tools out there that I haven't listed on this slide but yeah these at least at the very least are free and open source um now another thing you can also do as well is leverage Twitter investigators but I say that with a massive caveat so there are a lot of really amazing Twitter investigators out there right and I think we probably all know their names right but there are also some overzealous researchers as well and I saw this in fact actually last night with a mango exploit there were several people who called out several different addresses but were indeed not related to the mango Finance exploit um so yeah it's kind of frustrating at times for people like me when I'm going through this and I get pings from all sorts of people about things um so yeah leverage them but do it cautiously the biggest point on here though that I want to make is if you do have a project that gets hacked right or if you're a part of a project that gets hacked what you want to do is immediately reach out to law enforcement so ic3 is with the FBI and you can file a complaint with them um and they're they're quite good at tracking down funds um this is regardless of whether or not you're us-based because chances are you probably have an investor somewhere in there in your project who might be us-based so I would go ahead and file it with them as well as obviously whoever your local law enforcement is now the next biggest point on here is if you are a Dev and your guys's project gets hacked you want to communicate communicate communicate because the worst thing is you're silent right and then all of your investors are sitting there like what the hell is going on oh my God oh my God oh my God and then immediately the flood starts and immediately people are like oh my God the project got rugged and then all the rumors the rumor mill begins right so you want to communicate as much as possible as much as you possibly can and I know you're going to probably hire lawyers and whatnot but try to communicate as best you possibly can now I want to get you guys prepared for my favorite thing which is story time because I want to actually give you guys a good case study and unfortunately this is not going to be a nice story time this is going to be a nightmare story time we're going to be talking about the Ronin theft that happened earlier this year I'm sure we all know about it but about 600 million dollars was stolen from the Ronin bridge and what effectively happened was these guys stole all this um eth um and as well as usdc from the Ronin bridge I think it was like 173 000 and they had all this eth right what did they do with it first and foremost what they did with it is they started cashing it out at various different Sexes now this was also around the same day where the Ronin Bridge came out and they finally announced that they had been exploited and what do you think the attackers did they had to immediately stop because the Sexes then caught on like oh dear God we're being used to cash out of these terrible proceeds we gotta stop this and of course the attackers then also pause too right and then two weeks later they start mixing funds through tornado cash right and all of this ethos flows through tornado cash now one thing I have to mention is this was a ton of eth right I just mentioned it's 173 000 each this is a ton of money I had never seen this much money flow through tornado cash so the sheer volume was already unprecedented but the other thing that was also crazy too was these guys were rinsing and repeating the exact same methodology over and over and over again right so what they would do is they would mix the money through tornado cash and then they would they would then swap it using the same decks over and over again to Ren BTC which is a Bitcoin proxy right and then after that what they would do is they would burn that brand BTC in exchange for Bitcoin and this happened Time After Time After Time day after day and I mean we're all human right these attackers are human right and so we like a good pattern and it's one thing that you'll notice in this presentation is that patterns get repeated all the time and that's the part of doing analytics and tracing funds right is you you start to notice these typologies over time now what happened with this Bitcoin this Bitcoin was then sent on to chipmixer and you guys might know chipmixer chip mixer is a Bitcoin mixing service now when I look at ship mixer I think to myself oh my God that's so 2019 no one uses chip mixer anymore like there are so many better Services out there right and of course again why am I saying that so 2019 right well what I'm essentially saying by that is there's no liquidity in there right you want a deep liquidity pool when you actually care about privacy right because you don't want to be found out you don't want people like me to go looking for you right and of course these guys had a ton of money that was flowing through chip mixer right so what effectively happened was is unfortunately we could trace through it and a lot of it went to an APAC based sex but not all of it so not all of the funds actually were mixed this way this was only their pattern in May and by the way we're going to skip all of the patterns that happened in June and July and we're going to go straight to the pattern that started in late August and is continuing to today so some of that Bitcoin that they had what they started actually doing and this is late August beginning of September they started splitting it out into these weird increments of rounded five six seven and eight BTC right they split out all these funds what do they do with it send directly to Ren mint some Ren BTC great right this is a very very very easy pattern to detect on chain right and from there what they did was they used the decks they swapped it into usdt and then they got a little confused Midway they wanted some eth go back into usdt and they ended up cashing it out at otc's and bunch of sexes and they continue doing this pattern right through September it was the I mean it was very very obvious right even if you were to just look at Ren BTC based mints you'd be able to find this stuff what they also did is they got lazy in between and just cashed out directly from tether now one thing that you'll notice is that I have usdt and eth written on here right and for those of you that are Savvy you're like hey wait a second Heidi what about gas fees did you did you get anything out of gas fees Yes the gas fees actually originated from those Sexes right here right so it's a very roundabout pattern that we end up seeing so what also ended up happening as well is they had this large sum of Bitcoin and they got super super lazy and they started just cashing it out directly to Sexes and then they decided to get unlazy because unfortunately some people found them and they decided to go back to the ren BTC route and then do the same thing over and over again right now what am I trying to take away with this case study that was a lot right this was overwhelming now what I'm trying to tell you with this is blockchain analytics ain't easy it's quite complicated and trying to recoup stolen funds is no joke right it's not only extremely costly it's very very very time consuming right I've been tracking these guys literally or Yeah March right and not only that but the amount of funds that you know the whole like crypto Community has been able to recoup has been so limited right so you don't want this to happen now luckily we do have the transparency of the blockchain right now which allows us obviously to trace funds right the fact that it's permanent it's immutable right and it's fully transparent it's awesome right but it doesn't change the fact that tracing this money is a lot more work down the line than actually trying to prevent the bad activity now let's go into actually what we can do about this right I hope I scared you guys enough um now we've got the crazy slide that I actually want to take a pause on and I want you guys to actually take a picture of because I'm not going to be talking about it because I want us to actually talk through case study examples so on this slide we've got lots of different recommendations in terms of General stuff blockchain stuff social media email stuff and doing your basic due diligence I.E d-y-o-r cool we all got our photos awesome okay worst case um I can share the slides with you after the fact now I want to share a couple examples um with you guys and I want to share an example of being impulsive and not checking the contracts that you're interacting with before you interact with them now back in April um board API club's Instagram was hacked I'm sure you guys remember this right their Instagram was hacked suddenly there was like this announcement of like oh we're giving away this land all you have to do is if you hold an ape this mint this land token and of course people fell for this right they didn't actually look at the fact that they were interacted with and what happened well sure enough you can imagine what happened all of their most prized nfts were ripped from them right and one thing that was very noticeable when looking at this initial contract is that there was this set approval for all function that kept on being called by these addresses that had been fished over and over and over again rapidly and when you see this on chain run run from that contract right because what that effectively that contract is being run by some sort of phishing a person who's doing fishing now these nfts were immediately sold for Ethan a whole bunch of different marketplaces right and then after that the eath was cashed out a bunch of sexes immediately right really good then also it was sent to the Ukrainian Armed Forces that's interesting maybe that tells us about where the threat actor might be we'll see in just a second and then we also see that they're sending money to a karting shop so they're clearly buying fake IDs right probably using those fake ID to sign up for those Sexes interesting right now remember how I talked about those gas fees those are something you want to look at when you're initially interacting with a contract right so you can see where did they initial get will you get their funds from and if you take a look at that eth right the original like that paid those topped up that address to initially get those gas fees you'll actually find that that address indeed also did a whole bunch of different fishing and made a ton of money right great what else can we find out about this address one thing that I found that was really interesting with this particular dress it also cashed out a whole bunch of sexes as well was that it also sent money to this very interesting social nft marketing page right so essentially what you could do is you could pay this money to Market your nft project on various different social media websites interesting I don't think that's Ukrainian right there right and my perspective actually is is these probably is guys probably actually just donated to the you in armed forces in order to pay probably throw people like me off of their tracks right and so this is one very very easy way to do it because as we all know that address for the Ukrainian Armed Forces is tagged on etherscan right so it's very very obvious so this is an example of being impulsive and this is what you don't want to do so take those two minutes literally two minutes right and look at that address before you interact with it right look at what those other addresses that are sending funds are actually doing right or they actually like are they actually getting something for it are they actually minting any land are they getting nothing out of it right now I also want to talk about an example of social engineering this has nothing to do with blockchain analytics yet but it will right so here's a great example from Poly play and they were amazingly transparent about their exploit that happened last year this is actually on what from one of their tweets um as you can see here uh the finance team reached out to Poly play in order to pretty much list their token right and as you can see is this is looks pretty you know legit in terms of a you know LinkedIn page he's got like 400 some odd followers this seems like a pretty legit email or whatever it is but there are a couple funky things with it right first off they're asking they're saying hey you know please donate to the binance charity Foundation that's a little weird right like and they're asking donation in Bitcoin too I don't know that's a little sus right um so it's one thing that you want to kind of look at right when you're looking at um people who reach out to you is verify indeed their identities right for example if you look up Teddy Lynn right now on LinkedIn I'm sure this isn't the only Teddy Lin of Finance you're gonna find right and so it's one thing that you're going to want to do you're going to want to verify with these teams before you know you start interacting with them right because the worst thing is is you get down the line they send you over documents to agree to you download that document and that document includes a whole bunch of malicious code and your whole project gets screwed over right and what's interesting about this particular social engineering incident is that Poly play ended up getting hacked right because of all of this and what happened well if you looked on chain you'll actually find that the Poly play hack is related to the bzx hack that happened back in November and also a link to the mgnr.io hack as well and if you look backs up what you'll end up finding is all of them fell for very similar social engineering tactics so this is a common one used by many nation states actually one in particular and you guys can guess who that might be now one thing we talked about before is what's the difference between a hack and a rug right but I actually want to about what is the difference between an actual Dev team cash out and a rug because a lot of times Dev teams I mean a lot of times Dev teams need to cash out of their funds right they make money they need to be able to move their money around and do things that maybe not not isn't crypto right how do they actually do that and I want to give you guys an example of how legit team looks like when they're Cashing Out versus an illegit team because I think the the comparison is quite Stark so in this example what we have here is we have 10 ktf which is a very large nft project they get proceeds right from their nft sales and you can see that on chain and all of those are then sent to the 10K TF deployer address that deployer address then sends on to their gnosis multi-sig fine that seems fair right another thing that we also see too is renga deployer they also send funds onto anosis multi-sig so if you weren't following projects which I wasn't I had no idea these projects were intertwined actually until I was looking at their gnosis multi-sig and actually found oh wow they're clearly a part of the same ecosystem right and if you take a look at their gnosis multisigan by the way this is a great practice right you want to see a project using a multi-zig and keeping their funds locked up there they sometimes will actually Cash Out directly to FTX right but they do this over time right their website is still active their social media is still active these guys are still clearly involved with the community right so nothing weird is going on here this is the kind of activity you want to be seeing when you're analyzing on chains seeing where devs are cashing out at now what do you not want to see this is exactly what you don't want to see so I'm sure you guys heard about the squid Game Rug poll last year but essentially the squid team token launched right on the BSC Network and um what effectively happened was you couldn't trade this squid token so you could only trade it uh you could only buy it with BNB because it was on the pan it was on pancake swap but you couldn't sell it so effectively what happened was the price was driven up to like twenty five hundred dollars right and suddenly one day the website 404s all social medias are gone all social medias are deleted right and suddenly then the squid token is swapped for B and B and fully dumped right suddenly the squid token now is valueless now one thing the devs did right here though is they decided to unlimit that contract so that suddenly you could actually sell your squid but of course it was worthless so who cared at that point in time right and so anyways you had this B and B right these guys then decided to rapidly send that BNB through tornado right and again this was a very high volume going through tornado at that point in time so it was kind of traceable now there's one thing to note about privacy protocols privacy Protocols are incredibly important I'm not trying to be a proponent here in this presentation to say oh you know we can demix everything you know or whatever but what I am trying to say about privacy Protocols are that unfortunately illicit actors while they do use them is quite easily traceable to see where they're moving funds to because usually they move a ton of money at once and why do they do that because they're paranoid they don't trust anyone with that money but themselves so the sooner they can get that money out the better right privacy Protocols are incredibly important and I think we're going to still continue seeing in the them in the space in fact I've been super excited to see all the ZK Stark discussion here right but effectively all these funds then went through tornado cash and again these guys rinsed and repeated the exact same transaction patterns because once they got that B and B out what they immediately did and this is all in one single transaction every single time was they would swap that BNB for die right and this is still on BSC and they would bridge that die through any swap then into each base die or on other stable coins right and this was a very very very particular pattern so it was super super obvious now once they had this die in this east-based diet what they end up doing with it well this eat-based diet they ended up swapping for ease and then they sent it through tornado one more time on ethereum right and they could not be more paranoid right but of course they're moving this money through super super fast again super obvious now one thing I've noticed actually about uh these attackers more recently is and especially in the squid game situation right as you guys remember this particular rub was super super known right this was all over mainstream media right so this guy was Super Hyper paranoid right so what he decided was this oh I'm just gonna sit on this money for a while but he just didn't want to sit on that Heath no no no no he wanted to earn some yield so he decided to swap that money into usdt and lend it to the usdte's pool and then he also entered out the compound and from there he sat pretty for a while and then slowly started cashing out to uh two different APAC based Sexes right and this is actually a pattern of activity that I've continued seeing quite often unfortunately now what it's an unfortunate but also fortunate right because once they're sitting in these pools we can still see that that's the beauty of the blockchain right and so because of that is once they exit out right it's kind of great because we can also potentially recoup all the interests that they've earned so it's one thing to also think about now I want to leave you guys with a couple predictions because I only have a couple minutes left is and by the way I wrote this slide deck a week ago this was before the BNB Bridge attack this was also before mango Finance but unfortunately this is now reality we expect we hacks and scams continue even in the bear market right D5 protocols particularly those with high tvl right will continue to be perceived unfortunately as honey pots right and threat actors as I was talking about before they're mainly going to continue sitting on funds right that's actually what we're seeing with the BNB Bridge attacker right now he's just sitting on the funds we see that actually with quite a few different attackers at the moment right and to possibly earn yield right for example if you take a look at mango Finance guy what do you think he's investing in right now I'm not going to say you guys look it up anyways some quick break predictions right first and foremost be paranoid right hopefully this has been a lesson in paranoia here and you want to protect yourself before something happens right you don't want stuff to be broadcast to the chain right that is the like that is the ultimate like thing that you do not want to have happen because once it's broadcast to chain it's done right money is gone right and of course you want to do your own research that should be obvious right do not rely on other randos to do research for you and just believe them because they're anons on Twitter uh that you know what they're saying is correct you want to verify and last but certainly not least definitely learn how to read a block Explorer block explorers are extremely powerful and all of this research I actually mostly did using Dune analytics and just etherscan as well as BSC scan a couple others and don't forget to set up monitoring and uh that's all thanks guys [Applause] [Music] [Music] foreign [Music] foreign guys hear me perfect okay all right so I'll be talking today about time lot recovery recovery factors um it's going to be kind of interactive um we'll do some thought experiments and we're going to start with like a toy problem and then a toy solution and let's see whether we get something that works so what are we really talking about what are we trying to solve right the main problem we're trying to solve is that c phrases are bad right and C phrases contribute to loss TEF and even terrorism and sea phrases are like a very long password right they're easy to use uh easy to lose sorry easy to compromise and basically we write it down it's like you know um writing a Post-It note that's you basically when you run out of C freeze and what we should really be using is multi-factor authentication right NPC or multi-6 and why we should use it is the outcome of Decades of evolution and user authentication right since 1986 people have been trying to push um multi-factor authentication and it works it provides better security prevents phishing attacks and is relatively easy to set up so hopefully you also use this after this talk and let's just talk a bit about NPC and multi-6 so um most of you guys are probably familiar multi-6 and maybe a bit about NPC and both of them have very similar properties right um they're both better than C phrase they are more hack resistant you have to compromise multiple factors before you lose your account there's better redundancy so if you lose one factor you're still okay and that's also the possibility of additional checks right like a daily spending limits for example but there are some differences so NPC is off chain multi-6 usually on chain as a smart contract um so multi-six can support much more complex access structures think like organizations right thousands of people trying to vote for something um whereas NPC usually costs less to deploy and operate because of chain so there's no gas multi-6 also trustless access to on-chain state for example um pricing from dexes as well as like limits based on block time um whereas NPCs cross chain but actually you can use both right um one is probably more useful in like a large setup and one's probably more useful for personal setup so NPC is probably more useful for personal setup so I'll talk a bit very briefly about MPC because we need to understand this before we talk about um even multi-6 and how to use recovery factors so first we need to understand how to share a secret key right imagine you drew a line and the y-intercept represents your private key right now you draw three points in a line and then you erase the line right and with one point if you only have one of the points you can't redraw the line right it's not possible any number of lines could pass through it on by two you can you can redraw the line and recover your key this is basically xiaomi's secret sharing um which splits a private key into multiple shares and by tying each share to some user Factor we can get multi-factor authentication on private keys and where does NPC come in NPC does all this except there's no line drawing and basically it avoids a single point of failure because when you're generating a key if you're if your device is compromised at a point in time the key could be stolen whereas NPC just does that in a distributed way but today we'll just limit the scope to personal key management but of course you can use NPC for other things as well managing it in like an organization if you wanted so the Crux of the problem and what my talk is mainly about is there's this binary dilemma right between non-custodiality and custodiality what do I mean right we say a key is non-custodial if a user has full control of the majority of shares so let's say you had three shares right and you require two of them to reconstruct a key as long as two of them are fully owned by the user it's considered non-custodial since the user can easily move his key as well as use it but no other person can control the key whereas a key is custodial of third parties which we call custodians right have control over a majority of the shares and it's pretty obvious that the above are exclusive properties right you kind of like two majorities so either get non-custodiality or you get full cast reality and there's some special cases like if the user owns a share Enterprise share but let's not think about that for now but realistically wouldn't it be nice if we have something better ideally right what I really want is that my private key should be non-custodial all the time until I accidentally lost my factors or you know touch food I die then it's somehow you know transitions to become custodial right so the at least macro to assets can be retrieved sounds impossible right okay let's try and solve The Impossible time loss secret shares so firstly before we get into that right what is a Time long um it's basically something that takes a long time to decrypt the multiple variants right if you guys have been following the merge um they use verifiable delete functions uh and all that but we have Jackson is the most basic one which is repeated squaring and realistically it really doesn't matter what you use just think it's locked for a long time that's what matters right and and you know what let's just do this right let's just have an extra share right we have three just now we require two to redraw the line let's just have one more right one more this time log and let's say your house caught fire you know and you've lost both your laptop and your phone so you lost two factors uh they're now Ash and but luckily right you've prepared ahead of time you generated a four share the time lock for like one month and you put it somewhere other than your house so it didn't catch on fire you put it in public um in a month's time it unlocks and with your other share which is like an email share you can get back your key perfect not really that doesn't work so let's see why right the problem is that now you're just one compromise Factor away from losing a key so let's say there's a patient attacker adversary just waiting right he just Waits a month well your time of share will eventually unlock right after a month and remember it's in the public domain because it's not your house someone could get access to it and then they just need to steal any wonderful factors right to get a key so basically you've got for 2fa to really one fa so that's no good solution two let's improve that right let's do time log shares with key refresh so once again new content right what is key refresh um for those who don't know basically a key refresh is a way to refresh the shares of a SQL share secret sharing while keeping the private key the same the private key here is the star right this is the star so that's a private qcm and the way we do it is kind of you just draw another line that goes through the same point the same y-intercept which is a key and then you get rid of the old shares basically all the shares are different but your keys are sale and the point here is that as long as you don't lose two factors on the same line right you don't lose two factors before you did a kiwi fresh your key is safe right because even if they compromise these factors they can't redraw the line it's not possible and the two lines are completely unrelated so that's what key refresh does um there are a lot of protocols that do that today but please don't do it like this there's a lot of other stuff you can do please don't do that in production so now we have some idea what key refresh does what if we just keep refresh before the time locks up right so we do a key refresh whenever you use a wallet maybe once a week the time loss one is like a month right so as long as you use it once a week you're fine so when you use it you do a key refresh you give out the shares even when the time log share unlocks right the attacker can't get anything even if it steals another Factor right because he can't get back um the line so you can't get back the same line so it seems like a safe right and this seems to work right the setup seems to have a lot of properties that we want during normal operation attacker can solve the time log but I cannot use the other shares since key Refreshers occurred it's on a different line there's an interesting property here so if you guys have been following time timeline research and like verifiable delay encryption um you'll know that there are some research being done there's some research being done on homomorphic Tunnel puzzles and all that because it makes it cheaper but in our case actually it's good that it's expensive it's a feature we want it to be expensive because we don't want attackers to be getting through the timelock easily or even people do it parallely and in the case when you lose your key you won't mind doing the work so it doesn't matter the expense is not wfdo um so during normal operation you're fine but if you have catastrophic loss right you lose all your user factors you I don't know somehow lose all your user factors then if the timeline share is reviewed and um share refresh won't happen right because you didn't have your shares so you can't share refresh then you can use the custodial share with the email share and the time log share to reconstruct the key so it's kind of like a Deadman switch of a smart contract wallet except it's done in NPC so we've created a non-custodial setup that degrades in the custodial setup on loss so that's it thank you no I'm kidding this this kid this is for a horse there's no way that's gonna work right so firstly what happens if a phone share is stolen then the timelot shot unlocks let's consider that right so your phone chair is there it's been stolen right someone just stole your phone well what are you going to do well you're going to key refresh right that's the only thing you have protecting your shares you're going to keep refreshed you bought a new phone so your new shirt on that phone perfect but the attacker and the hacker who like in food and tea who stole your phone is not going to actually delete the shares and when a month passes and a timeline expires he's going to be able to steer key gone terrible okay maybe what we can do is stop making more shares right the reason why this person was able to get a key was because he had more cryptographic material so we're going to do is instead of having more shares the time block share itself is just going to be my phone share right so this person stole my phone and then he gets the timeline share which is also my phone share so he doesn't really get anything new so I'm safe uh not really um if you store your laptop right then the timeline unlocks you're running the same problem so it looks like we're in a dilemma here right what really can we do um a new solution right let's split the device share into a time lock share and then a custodial subshare right so how does that look like how does that look like so what we're going to do is we have this initial setup right two out of three factors you have an email login a phone uh and a laptop share so that they actually start based on these factors and we're going to split the phone factor into two sub shares which is explaining to and one will be time locked and the other will be encrypted in your email right so this way let's say if someone's used any of the factors and they get a timeline share they still have to compromise another factor which is your email right so key recovery still works if you lose everything you still have let's say you lose both your phone and your laptop it caught fire in the house right um you still have this time lock share and the email share which you can use to reconstruct your phone share and you already have emotions so you can redraw the line right so you're still safe from catastrophic loss but adversary can't actually get your key unless it also compromises another Factor so you still get the two fa properties perfect but it's also another problem the email provider might not delete shares during key refresh let's see why that's a problem what happens if let's say the email provider doesn't delete shares right so let's go back to the scenario right you do key refresh right and the email provider instead of doing key refresh properly and during key refresh you're supposed to discard the old shares he doesn't he just keeps the email share after all why not why shouldn't he keep it well after a man passes right so let's go back a bit after a month passes right this is going to unlock right the time launcher is going to unlock and it's going to reconstruct the phone share on the previous secret sharing polynomial and reconstruct a key so even though you refresh your shares this is still a problem as long as the custodian doesn't delete the share right you still have this problem where when the time log expires the custodian can get your key thank you solution right a student's bad no custodial factors let's just get rid of all the custodians and third parties there's a problem there though we're trying to solve for the disaster case when you lose all your user own factors do you really want all your factors to be lost because if they're all user owned and something happens to you such that all factors go away there's going to be no way to recover a key and this problem is really less of a problem than a statement it's not possible to get recovery from catastrophic loss without third parties having at least threshold minus one shares let's see why right um fundamentally because students only control let's say threshold minus two Which is less than 30 minus one um with a Time log shared that unlocks after one month they will only have threshold minus one right so threshold minus two plus one so we only have pressure minus one shares which is not sufficient it's not going to meet a threshold so because it doesn't mean the threshold you need some other Factor you need some user owned factor and that's impossible in this scenario since you're assuming that the users lost all their shares so they can't access anything so we haven't really solved a non-dilution problem remember this solution of removing custodians was to get rid of the non-delision problem but it's a non-solution so we haven't really solved the non-delision problem right um we still need third parties to hold threshold minus one of the shares but we can't guarantee deletion so in one month the custodian might be malicious and it'll still the key we have a final solution for now one hour and deletion so there are many forms of trust right and the best kind of trust is like trustless right zero of n but there are also other trust factors like U of N and one of N and maybe so like half you know majority of n um and one way to do that is maybe there's a way to make it such that we can delete the factor if at least one of the end of the parties follow the rules so what we could do is right we could have the custodial software be further split across a hundred independent custodians like Guardians and trust that one of them will delete it during the refresh as long as they do the key material is gone this isn't ideal right one or n is not as good as zero of n trustless but it's close enough and the cell gets bigger one or a thousand is actually pretty close to trustless and also doing NL and SQL sharing is very fast it's quite practical so this is what a setup looks like right we have the two or three setting we split it into two one is time locked like you saw earlier and then this one is split in amongst a bunch of Guardians right however many you want and during normal operation right if you do key refresh and at least one of these Guardians deletes the share you're not going to have enough pieces to reconstruct the phone share so it's good enough so adversary can get it and neither can the custodial provider um but during catastrophic loss when all user factors are gone so your phone factor and your Plateau factor is gone at least there's some way to recover it because your email factor is still there the tunnel Factor will unlock and the Guardians should in this scenario return the shares to you so you can reconstruct reconstruct your key there a bit more complex but much better than what we started with so I have a demo but it doesn't work very well on still internet so you guys can check it out on the slides below or the link there um and if you don't like the solution you can also just use a Smart contract wallet with the events which is much simpler not everything is MVC really so some open problems um The Gardens can really grief you so that's your catastrophic loss the gardens can just refuse to return their shares to you unless you pay them exorbitant fee there's also a problem of how should the Guardians know when what's a key and return the shares to you so these are some open problems we're exploring so that's all thank you very much for coming to my talk thank you [Applause] hello I think a bit early so um does anyone have any questions yes go ahead hello oh hi um so the time locked piece is a bit of a magic box that just works are there any like practical algorithms yeah just that I've um I need the oh he's taken away but anyway if you go check out slides online this is just uh it's implemented in JavaScript so you're running a recipe even faster but is the most um far the most common and simple algorithm is a repeated squaring uh algorithm so you can just look it up it's quite popular and and it works in this case yeah it was for just this use case you don't need anything very complex yeah sorry uh yes it's Asic resistant in a sense that it requires sequential steps you can't paralyze it so um you could maybe build an Asic for squaring but it's not possible to paralyze the computation so it is sequential yeah any other questions yes ah okay so we're considering the scenario the whole point of this setup right is complex but the whole point of this is to protect against catastrophic loss right so during catastrophic loss you're going to have this issue where you've lost basically all your user own factors like phone and laptop the email we're just assuming it's custodial there's always some way a subpoena the guy you know there's always some way to get it but what what might happen right is um the time not show unlocked but the Guardians knowing that you didn't refresh your shares have now detected that you've lost your factors and now they're gonna grieve you right they're gonna be like if you don't pay me you know half of what's owned by this account I'm not going to give you back the shares and and this is extra extra challenging here because you only need one Garden to grieve since there is no redundancy here yeah so there's one one problem there's an open problem to solve any other questions go ahead yeah um someone not deleting a share and someone keeping the share um the problem here is that the the actors here are Dynamic they're not starting so do I will delete my share if I'm pretty sure you're refreshed right then you're not going to be you haven't lost your account so my share is actually worthless but when I do detect right maybe you have no on-check activity for like um like three weeks right a little suspicious three weeks um then I'm gonna modify my behavior in accordance to that so it's kind of hard to model it that way I I think a much more comprehensive way to think about a threat model is to think about the different types of attacks because the number of ways you can attack is uh limited since there isn't much interaction between the parties is really just about factors yeah any other questions all right thank you very much [Applause] foreign [Music] foreign [Music] 