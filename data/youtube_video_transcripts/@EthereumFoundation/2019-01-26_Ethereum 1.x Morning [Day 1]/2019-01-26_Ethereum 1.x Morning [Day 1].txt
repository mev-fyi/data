first and talked about the simulation team yep that's it sure we're gonna get set up huh oh he's got it we don't hit the breakout we are kind of late so the breakouts gonna be shorter than it would be but oh there's not six working groups there's four isn't there got it whoo if you're planning on presenting raise your hand okay so for these first set of presentations only four but then tomorrow it might be different or this afternoon it might be different Aleksey you're on point today thank you yeah totally all right cool we're just getting set up up here who's and who's not in the all core devs channel pretty much everyone here should be in the all core devs channel on get er it should be good or dot I am slash aetherium slash all core devs all one word if you're not in there join it there's a lot of good discussion in there that's where the core developers meet up for a theory amande if there's you know something major going on we meet up in there we announce meetings in there if you're interested in coming to the meetings if you feel you can contribute something then let me know and I can definitely help with that great to go all right thanks everybody sha Han pronounced to hunt actually so sorry so very short presentation just to give everyone an intro into the simulation working group so the simulation working group started in Prague as part of the youth when X stuff and the goal is to do some analysis that will help the community develop the roadmap one of the first things to do is talk with people find out what analyses people have done and then start exploring ways to do those analyses that requires collaboration communication openness because we need datasets we need code we need different groups to coordinate what they're doing so currently we're working on the main simulation that we're working on right now that's being run by Vanessa and Nico is the unclear rate simulation basically there was a question that came up in Prague about why our uncle rates dropping what can we do to help ensure understand what uncle rates represent on that main node and how they impact clients and network protocols so along the way there are several steps collect datasets develop models and also test plans determine how these potential understandings are simulated and emulated and share those results with a community so we have a few data data sets that we've collected a lot of you has provided us with some Eve stats thanks Hudson for sharing stats from each sets ether scan those are those public charts that are available and Zack will talk about the nodes on the network proposal essentially launching no nodes on the main that to collect some statistics the live network so there are two simulation emulation frameworks being used and explored new clan Vanessa are working on the wittgenstein simulator which you can model different consensus algorithms and at the same time because uncle rates are related to Network protocols it's being used for that as well and Zach will be able to talk about the testing platform which is essentially a platform that's being designed in order to reduce some of the overhead that goes into network protocol analysis usually associate with say latency and low level network access so while there are a lot of simulations that deal with say just modeling or clients that you can launch an Amazon the platform will enable people to do deeper dive into the network protocols so the roadmap for the working group is collecting data doing analyses working on the code bases are one of our goals here this weekend is to figure out who else we can talk to who else wants to get involved and communicate communicate with other working groups even though we're working on uncle right simulations right now we believe that the working group can but help address other simulations and statistics or analyses that need to be done state rent or state pruning there are a lot of statistics analyses that have to be done and we want to find a way to do that perhaps more generically with tools that can be reused rather than just each group having to do that so kind of join forces and help each other as well some of the other areas that we're gonna explore our gas costs because things like state rent all of these analytics deal with the cost of transacting on the network and we need to better understand how what what impacts each of these components have these things so in the side conferences watching conferences will review the analyses will invite others to join so if there are people from other working groups that are requesting datasets or things like that please come talk to us and that's mine any questions okay um shall I invite the next group up shall I invite he was him sure who wants to go next can someone help me with the computer my slides hello we're excited to be here I'm Paulin I'm on the e1 team and we have a list of our names up there we have you as an working group proposal that we released a document a few months ago and I'm just gonna speak very very briefly today just a introduction and a reminder of what that was first two problems the first problem webassembly is great but it's not it's designed to execute arbitrary code and at native speeds but it's not designed for consensus but it's pretty close it's close enough that we think we can choose a subset of it that is designed for consensus and that we can have guarantees of what's going to execute so we can avoid consensus bugs and so that's the first problem with that webassembly is great but it's not quite there and with engines yet we have to modify things we have to choose subsets of it the other problem is pre compiles are a burden we haven't had a lot of pre compile for a long time because implementing auditing is done by each client awkward gas metering testing who's gonna do all this work so people are reluctant to keep on adding these pre compiles so the idea was one precompile called awasum and then that's gonna be the last pre-compiled then we can launch things on this II was in pre compile and so these are the two problems so the solution is replace the current pre-compiled infrastructure with web assembly and this is a first step in a journey for general user deploy to us of contracts the advantage of pre compiles is that we have a subset of e wasum we can start with a with a first step a small step where we're sure we know what the code were executing we can check things and we can hopefully audit and even verify things before we even starts and the deliverables are listed I think there's some there's some of it is being included okay there it is so we have we're gonna deliver Azam modules for existing pre-compose and a bunch of other pre-compile the specifications for the interface and the implementation of these specifications are a lot of work in death client parity all of these all these things take a lot of work the specification is a lot of work gas metering is a big thing that we'll talk about it's really interesting actually if we can find some way to automate metering of pre compiles and there are some options and the goal of this first step is to have a streamlined process for deploying pre-compile where someone can give us a module and we can we can deploy it and maybe eventually will lose users deploy their own pre compiles everything automated so that's the ultimate goal but we're taking a first step we're just exploring ideas and where we're building experiments nothing is sort of finalized yet so we're still open and we're still discussing but this is sort of the state we're at right now and there are discussions perhaps we'll just go straight to use or deploy dae-hwa's and contracts perhaps we'll start with pre-compile but and there's a lot of other questions we'll talk about it this week a lot so really exciting stuff and we hope to hear from everyone so there are some discussion channels that were available on okay head up Lexi absolutely we got the live stream up when we got 14 people watching so I'm gonna go straight into the not going going straight into the stay trend yet I just I think it makes sense to okay so I think it makes sense to do a little introduction into this is my my point of view on the serum 1x and of course a lot of people can object but yeah I think it's useful to explain what binds us together why these are four groups they came up not the others and some of it is sort of accident that some of it is not so let's look at the short his prehistory I would say of how this whole thing came about from my point of view is that first of all in Cancun and DEFCON 3 vitalic had his talk about modest proposal for cerium 2.0 in which we sort of hear this proposition that that a theorem existing in theorem 1.0 should stay safe and conservative and all the kind of just sort of the break in innovation should go into the theorem 2.0 in the shards so that it included he was a man I think this is the first time when I heard that he was am is probably gonna be shifted towards 2.0 and other things as well so we basically realizing that the current chain is too valuable to sort of do the breaking changes and so so forth and so people largely agreed with that sentiment so what happened next is that I mean again from my personal perspective I was at that bad corn and Vitalik was talking about do you want to become Casper validator which sort of signal signal the Casper is very near it was Casper effigy at that point and people got really excited about it and it was one thing in particular which was very exciting is that the slashing condition was changed so that if your equivocation like if you're you're voting on multiple blocks in the same height and that sort of didn't cause too much trouble then your slashin is going to be less so that means that there will be less risk of participating in in in in validating pools and and then another thing I remember that essentially enabled people to validate it from our laptops which was really cool so but then June 2018 something changes so one of the accord that meeting number four which I have a link there but basically there was a pivot announced by the by the research team so essentially like initial idea was to have caspere to have a contract on a cerium 1.0 which would be validating the votes and doing other things and the sharding had was a different research group and they realized that their researching pretty much like very similar things and they really decided that it's gonna be very challenging to validate all the votes and the contract simply because there was a too many signatures to verify and so the decision was made to join together the the Keira Casper and sharding research groups and not do contract based caspere and this is where we start hearing things like beacon chain and Jasper which is the sort of the hybrid of Casper and shouting and and so then we're if you look at what happened in October November which actually the events which led to the creation of this our experiment Pony X is that we we came to Prague and we kind of realized that well these is going to take a bit more than we thought before the what I mean by this means that serenity is a 12.0 and and so when I talk about this serenity I don't mean the beacon chain is gonna be alive it might be even alive at the end of this year but actually to functionally supersede the theorem 1.0 it might take longer because a current plan is to do phase 1 phase 2 phase 3 and the phase 3 is or maybe phase 2 they call it if you count from zero is where you have execution engine and everything starts working and also that I think somebody says that well also then you need to go through the trial by fire which means that survived a few attacks and things like this and until people become comfortable using this chain so we're talking about optimistically three years per semester CLE five years maybe even more so as I was joking sieve italic I will retire by then and so why did this whole start so it is the in Prague when when I came to Prague there was a kind of sort of cognitive dissonance for me around the around the rooms I saw lots of excited people who were building stuff on aetherium they were really kind of cheering cheering lis excited but I saw some different people who were kind of going around and thinking like the sink is taking too long there's too much data to go around the nose because it take more and more space and stuff like this and so there was a sort of the people who I talked to I started hearing the sentiment and then eventually we just start talking to each other about this and this is how the serum on Phonics came out again this is my personal perspective please correct me if I'm wrong but the main point of this Union point one point X from my point you used to develop and design some changes that we can make into the existing existing network so that it survives until the the Sirians 2.0 supersedes functionally and people do migrate so which we don't know how long it's going to take but there the danger of not doing any changes is to basically we don't gonna have any intermediate thing between 1.0 and 2.0 so so there was otherwise there gonna be some kind of rupture and so another important thing which I sort of started talking about emphasizing that we are we're concentrating on solving this problem because as we saw before that the bandwidth of changes in the Sirian protocol was really low I mean it takes months to figure out how and what we're gonna change and given that we might not be able to afford to introduce more and more features which we kind of randomly collect along the way we have really needed a focused attempt and here this is why I see it wasn't as a part of this combination is that it allows you to to deploy the meta feature rather than a special specific features and I think for deliverable again this is what could change as well so but out of the three working groups so there's two things which definitely might require will require hard forks if they ever going to be accepted and storage pruning at the moment we're thinking about it might not require the hard Forks but it might if we if we find out there's something we have to do to make this whole thing better it might actually require hard work and at the moment we're thinking about emulation and simulation groups as a kind of way to help others to to research things but that's pretty much it yes so I think I'm not gonna go over it wasn't because this presentation contains a short information about all the groups but I think I will just let other people to do that so this is the end of my short presentation I will do the statement in the next round or maybe later this round see Thank You Alexi next we have you Azam right or Fred wait we just yep Todd all right Fred everyone keep me on my toes all right I don't really have a presentation I'm just gonna talk through Peters proposal because this is originally Peter Shivaji's proposal on chain pruning and I'm been involved in the discussion for a long time as well so I think it's good to get a bit of history and sort of what chain pruning is all about so we all know that a full node takes a lot of space it takes about 140 gigs or so to run a full node and it's not really reasonable expectation for someone to run it on their laptop anymore and that's something that we want to address and so this you know has been a long discussion you know Bitcoin introduced this a long time ago prune nodes there are pretty common and Rob had a BRABUS one of the main consensus developers a parody he had a talk at DEFCON 3 and 2017 on basically trying to shard up the block history across multiple clients based on like the first byte and in the price in the key in the node key something like that so basically breaking up this storage that was met - pretty cold reception I would say it's been a discussion for a long time but no one is really willing to take action and now we're starting to really feel the pains because the number of clients in the network have reduced drastically over the cover of 2018 I think we need to do something to encourage people running more clients and this is one of those approaches so just going over the proposal from from Peter he highlights that cross client coordination here is important this is like chain pruning is something that any client can implement today if they want to it's and it's not really that big of a problem but if parody implemented chain pruning by putting all the Box on ipfs and gets implemented it by putting it all on bit torrents then we can't really sing from each other anymore and that's a bit of a problem but furthermore if then Trinity wants to join the network and they have neither of these these methods to bring historic blocks then they can't join the network anymore because they would need to implement one of these methods to even get history so that's the thing if every client is pushed towards having this as default behavior we get into those sort of problems but if we do it optionally there's a little bit more leniency Peter presents a lot of good data on what we're actually talking about in terms of size we have blocked bodies the header chain will always have to be downloaded and maintained by every client but just the block bodies is on the order of 100 gigs then we have logs that can be deleted as well and then of course the Associated indexes with all these things take up some space as well so the server requirements of a system like this is that we need to have data retention we need to ensure that we have data availability in some way and so we could either say there's a probabilistic availability some number of people will always run a non-punitive mode but it's unsatisfactory solution to just say it price is probably fine so we would like to have some stronger guarantees and that could be doing things like putting the block bodies on IP FS and then it's discoverable and you want to download it it's freely available we can say that you know the Internet Archive parody and the foundation will always pin all of this content so we have multiple replicas of it you know there's methods like that if we start delving into actually incentivized like Unchained incentivized methods of ensuring availability it becomes much more complicated and so I don't think Peters proposal does not cover that and I think that goes for beyond what we should be dealing with today but I'm interested in if people have ideas about that so obviously pruning history makes syncing different like a parody warp sync right now doesn't rely on history actually downloads the snapshot in back Villas history fat fast sync is a little bit different but it would also work well but a regular sync becomes much more complicated if it's hard to find historic blocks so basically we would be crippling regular sync to some extent but a regular sing-sing takes weeks anyway so the question is if anyone is actually doing they're not and I think just covering a little bit of what I hope to get out of today I think the chain pruning proposal is the least controversial it's the easiest one to talk about easiest one to implement it's something that we can have a very minimal version of like next week if we wanted to but requires some coordination and agreement between clients and as it relates to other working groups here I think it's chain pruning will become important if the other proposals go through and we can actually increase the scale of the chain like if we start processing five times as many transactions then a full node will will just balloon out of reasonable size like it's still to the point where you can buy an SSD to put a full note on but if we go 10x on on the main net then that's not going to be possible very soon so if the other proposals go through then this is necessary if they don't go through and this is a nice to have so I'd say the mission is to get to a point during these couple of days where we agree what we should put forth to client implementers as this is what we wanted and part of the goals of that is sort of establish what problems and concerns exists especially from like deaf developers and and other people who who use history in some way and not just take this perspective of someone trying to run a full note on their laptop and to figure out a bit of a roadmap so as I said the simplest possible solution is something that we can do next week what is the roadmap to you know what other stages of this exists and I have some ideas here and sort of map out what are the different stages and when should we be trying to implement them okay so next on the agenda was a breakout or actually did anyone else have presentations are we all good that's probably a good idea so people can get an idea that it's Alexi again oh and from now on if you have any questions from the audience I'm gonna be passing around mics so just raise your hand if you have a question after we're not taking questions right now but just for the future if we if we do stuff will do mics cuz it's kind of hard to hear okay so before we start going into so this is gonna be about state rent and before we going into the proposal for the state rent which I would encourage people to come up with the alternatives to what I was gonna present because it's not you shouldn't just be me writing these proposals so I would like to present the framework which is that my attempt to to set out the questions which proposal should be able to answer and sort of the problems that it has to go into so that when we look at the proposals we don't have to do a lot of big interrogations about what about this what about that so and so as the I start with a list of questions about like how do we go from from beginning to the to the actual proposal and so the the we're gonna go through every question in in in turn so but we'll start with with this one so again this is my personal perspective and I would encourage people to come and challenge me on this if they if they see that something I've missed here or something that they disagree with this is exactly the reason why we're here I want to hear how this needs to be changed so I'm claiming that this is there's a two main values that the the state is providing to the to the applications or to people so first of all it's the claims so you can keep to some sort of claims in a blockchain and these claims are automatically available and provable essentially the whole network works really hard on trying to make keep your claims available for a very long time and secondly that the state in Syria allows the this kind of synergy between different smart contracts this is the sort of like a higher level higher level value and I when I was writing the the the first proposal I probably more thought about the from number one but I didn't think a lot about number two because the number one is this is where the beneficiaries of this valley could be the people who are holding the claims but the number two is not as that clear so it would be like the users were actually can for example trade died on the decentralized exchange would be those who sort of benefit from this energy so then why is this so again this is my claim which I need to sort of run through everybody and so depending on the answers to these questions we either say ok we do need this measures to control the size or we say no we don't need this measure so there's no actual problem that we're solving and so the important bit here is that I do depart from the so initially when I read the historical proposals for the state rent I most people were trying to solve the problem of cost of storage like how much does it cost to buy the this 4 terabyte hard drive how much does it cost to do this and this I completely ignore this question here because I think it needs to be solved in a different way and also it doesn't allow me to constructively discuss this so I only concentrate on performance impact and so if we do not constrain the state sighs what is going to happen to serum in terms of the performance degradation and so there's four things I'm discussing is that transaction processing block sealing snapshot synchronization and the again the the snapshot synchronization but a different aspect and so I'm trying to figure out what is the function of of these performance degradation depending on the size of the state so number one is when when you when you process a transaction you very often get the transaction reading the part of the state and and because if the state is stored as it as a try or in getting parity then you would need a logarithmic time in in the size of the state to reach the node that you want and if it's a trooper guess for example it's probably going to be some sort of linear but most of the times because the state is now a bit larger than your usual memory most of the time the state will be some on a disk so it will be organized in a database which normally has some sort of index which also provides the regular logarithmic time when you search the entry so that's why I stayed here so for most implementation is a square of logarithm time of access and so you need to multiply this by number of unique items which is accessed so and when you think about it if you believe that this is the function basically number of uniquely accessed items multiply but Square logarithm of the state size then you can say okay we either constrain the state size or we reduce the number of unique items processed in transaction because the other way of solving this program is just basically say well we're just going to make sure that there's a fewer transaction going on then we don't care because that's actually linear dependency so then the next thing is that when you actually seal the block then you need to recompute so if you change anything in the state then when you come to the seal the block and recompute the state root you need to actually access some sibling in the Merkle tree of of your items and usually that is you know 480 bytes from each level because there's a 15 siblings and again there's a this is kind of a lot of reasoning but you can imagine it is still square logarithm of the state size but multiplied by number of items modified but here it looks like the same function but the actual coefficient is much larger because it's probably going to be yeah so you basically end up reading much more data because you're not just reading the the bit that you're read there you are accessing but you also have to read the siblings in order to agree redo the miracle tree and here people might say okay what about caching so can you just cache things so yes you can but this is actually not explored yet I think so when if the client uses the caching for to cache the state they normally use are all you caching restless wrists recently used at least LRU anyway so but we haven't seen this attacks yet but essentially what you can do is that if you know the way that a client's cashing things you can specifically construct transactions which generate hash cache misses and and if you want to be resistant to this attacks then you need to the only caching strategy you can implement is the randomized caching essentially you just hold the random sample of the state and the the sample is going to be slightly different in all the clients but essentially that leads me to believe that the function is simply logarithmic so the third number is that when so currently the state I think with the most compact representation is around maybe twelve gigabytes or something like that it depends but I think the most efficient snapshot sync is when you're basically a node join in network instead of reading all the blocks and trying to re execute them from the Genesis you can do the snapshot sink this is what most people do these days so the most effective one is to do a parity warp sync and that already takes you quite a long time because depending on your bandwidth the syncing twelve gigabytes of data is it takes a while and you can probably see that if the bandwidth doesn't grow that much so then duration of the sync is linear to the size of the state so you can see this is actually a bigger bigger deal than previous to potentially and then the number four which is also very well we were very little known but probably is known to some of the client developers is that as the state size grows more nodes start more aggressively pruning the state history so when you're surrounded by the nodes when your node is surrounded by the nodes who very aggressively pruning the state history then it could be that it takes you longer to sync then it takes them to prune away the state your sinking for example you started right now you start thinking the the state which is current now and then it takes you two hours to sink and in this two hours the the chain progressed to five hundred two blocks or something and then it could happen that all the peers that you're thinking from already prune that history because for them it's too old so they have nothing to give you anymore well you might have already halfway beef halfway through but the question is that do you gonna start from the beginning and try again or are you gonna be patchy patching up the holes and I think Fred had and I think pizza also had some proposals to deal with that but and this is where I just recently had an idea this is where we can involve the simulation simulation group to help us with essentially to figuring out what is the actual relationship between between the syncing between the pruning threshold and bandwidth in this particular rate of success of the sync so basically to simulate these failures sync failures like how often does this thing fail if you have this bandwidth and this pruning threshold yeah so Fred said that most nodes sometimes can't produce the snapshot in time because it just yeah so they hit the pruning pressure before they can actually produce this warp sing snapshot okay so the these four things that I've described this is the the real reasons why I think we need to manage the state size and I think this is up to discussion but I would like to not talk about cost of storage because this is very distracting conversation and it needs to be solved in a different way and so here before we start going into solution maybe we can mitigate this problem somehow without too radical things maybe we shouldn't just do rent but so we could do a little bit of mitigation and if you know about other mitigations please let me know because we can add them to the list so my two mitigations I could think of is that we could improve the latency of blog post processing by sort of pre-warm in the caches so we can look at for example we could look at the transaction pool or we get the miners to pre-announce the blocks and we can figure out like with a certain probability like what part of the state these transactions are going to hit and so we can pre-warm them in a caches right so by the time the block comes we already have the warm cache in and it will reduce some of this latency but important to note it doesn't improve the latency but not a throughput because if you're looking for because you you basically you if you're so if you hit in the point where process in the block takes almost as much as the block propagation sorry interblock time then this is not gonna help you so we were still hitting the limit so if our if processing of blocks takes longer and longer for example now if it's takes 500 millisecond or 400 millisecond and if we keep the state growing it will then take 1 second it will take 1.5 seconds and then there we know that the block time is only 15 seconds so we probably have to increase the block time or something like that if we don't want to you know hit some promise so even you do if you do this latency mitigation it's not gonna affect so the the problem of throughput and another thing we can do this is where we've been talking to Fred about to make some sort of clever syncing algorithms which allow you to improve the success rate for a sink and things like this so okay so I'm gonna try to go faster from here so how could we manage the state so here I'm into you probably familiar with this stuff I just took it from the book about feedback feedback control system so there's this forward control and there's a feedback control you probably familiar with us think about thermostat it's a feedback control so if you think about EDM then the gas schedule is the 4-bit feed forward control so we figuring out how much each instruction could should cost and that should allow us to limit the the number of computations we which will the nodes will will make an obvious example of the feedback control is the minor difficulties so which we can change after each block so here is the there's a choice so in any proposal we can either try to do the feedback control or we can try to do the feed forward control so if we want to do the feedback control which then we need to have a state size observable within the protocol and then somehow change the parameters to make sure that we are within the bounds and if you want to use the feed forward control we're just like using some sort of Markin yeah using some benchmarking for example with the help of our simulation group just figure out okay what are the like let's say we want to fix the rent or fix other things just do some simulations okay this is sounds like a good value and just put it in and hope for the best hope this is gonna work or maybe later on we can we can think we can change it by a hard work so let me just stop there I don't want to I think this is enough granularity for this particular presentation I just we're gonna walk you through this very quickly to to just to show you the Quai are the questions that we want to answer and maybe we could address the other bits in a different presentation so first of all so if we both if you think about both feedback control or feed forward control so what we need to think about is that what is our input and now what is our output what do we want to regulate in our case do you want to regulate the actual state size or do we want to regulate the rate of growth so they could have come up with two different mechanisms and then we also need to define define whether those metrics have to be on state in the state itself or out of the state and obviously if we do the feed-forward feed forward control they don't have to be in the state because we just come up with some values if we want to do feedback back control they have to be in the state because we need to be able to verify that the state the control is executed correctly because most of the clients now sync with a snapshot without history so they need to be able to in the using only the data inside the state to verify that the control has been applied correctly so then another thing we need to think about what for what what are we gonna change to do our control and in this case we know that there are like six different operations which can increase the state or decrease the state size and probably we want to somehow affect those operations and then I also suggest that we can do three different things we can in the cost of those actions the ones that increase the state we can we can make the actions which decrease the state more rewarding or we can directly reduce the state size which is what state rent is doing so there's like three approaches which could be combined and so one of the the obvious thing to increase the cost of the actions is do some sort of increase the blower gas cost of the let's say of a store or something like that let's say from four 20,000 to 40,000 and so it has two effects what I would say is first as scarcity and another one is cost by scarcity I mean is if we don't change the block size block gas limit but simply increase the stores cost and means that there could be theoretically fewer stores that you can do in a block which will already reduce this theoretical maximum expansion from this operation and the second intended effect of this is to make it harder or less appealing for the people to actually do this operations because it cost more but I will say that ii ii ii effect might actually be somehow circumvented by the by a some private agreement with the miners or with the mining pools because they you can you know you because because in this sort of economic transaction there's only two parties this transaction center and the miner so they can do some sort of agreement and they can even make it appear that they don't have an agreement by doing some sort of kickbacks or rebates and the question open question is that do we think that the first effect is enough so that we can kind of ignore the the the fact that people can circumvent the second effect maybe it is another thing about i want to talk about is about the gas refunds at the moment the one idea to reward the state decreasing actions is do gas refund unfortunately this mechanism is a bit is a fraught with some drawbacks because there is a cap there you cannot get more refunded a half the gusty already spent which kind of makes it a bit tricky to actually use the refund so if you have a transactions but that doesn't really spend anything but there's a lot of clearing then all the refund is unfortunately lost because it will be less it supposed to be much more than a half of the spent and that there are reasons why why this cap was introduced but we potentially can come up with a solution to make this reward much more straightforward and effective so yeah I will be look I'm not going to talk about the lock ups right now because it's in in detail described in second proposals but this is one of the mechanism to make the refund simpler if you require everybody to lock up some Easter when they expand the state then you can actually return that almost in full or in full when you're clear estate and that that refund should come not from the minors but from the protocol itself and that that it means that you don't have to have a cap yes oh and then we are this third way is to directly reduce the state state size which is the rent and so there's a decisions you need to make about what do we go in what we're gonna evict and when we're going to evict this and yeah intuitively it's like we need to just evict something that nobody cares about some old stuff and there's also different ways of doing it which I'm talking about here but I think I'm going to cut it short so yeah and this is the important point that I think any proposal should this explore about if you are in produced any control you need to sound think about whether the system can actually try to evade this control like were sometimes I you know I talk of a lot about the hoarding problem is that when you increase the increase the cost of some old action and then you have to announce it in in advance and people can try to sort of prepare for that by hoarding some of the state and then reselling it or just keeping it for themselves so in our other example of how the system can try to evade the control is basically this private agreement with the miners to try to to avoid the in question of the cost so that's it for this but again to repeat for what I said before this is not actually finished so this is just an attempt to come up with these set of questions and things that every proposal have has to look at at least and explain what they do about it and does this problem exist and stuff like this so that we can have a proposal from pretty much anybody they want but we every one of them have to have this sort of quenches answer questions answered so we don't have to spend the time of pulling this information out so then we can just put the proposals next to each other and we can compare them like based on this criteria and if somebody one knows about more things to to cat please let me know that's it thanks so much Alexi I think maybe it's enough presentations or is it anybody wants anybody else wants one even want to jump up there oh yeah you want to go out Amir yeah so you're saying break out sir I don't think there's any more presentations okay great so should we do the thing everyone gets in like a corner with the chairs or something cool first thing can we stop the livestream thank you okay 