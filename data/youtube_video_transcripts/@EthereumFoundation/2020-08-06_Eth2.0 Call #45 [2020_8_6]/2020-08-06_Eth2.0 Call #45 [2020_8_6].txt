[Music] [Music] [Applause] [Music] so [Music] [Music] [Music] so [Music] so yeah herman i said that the other day and someone corrected me i know it's a local pronunciation thing so but thank you um it has been wake so fun for me yeah the the double l is that people say ja sha yeah like it's all it's all over the place um okay cool welcome everyone um congratulations on the modasha launch launch um however whatever your local pronunciation says um lots of fun the public call was fun uh thank you for the sticker guys for running that and uh you know we had some more participation out the gate but things are looking stable i also know that we have ongoing issues being worked through by various client teams and a lot of collaboration going on underneath the hood so that's good the first thing that i wanted to do and let me share the agenda is to just talk through um kind of enumerate what happened leading up to launch to better understand some of the things that went wrong and so things went well um just so we can keep our eyes uh moving forward and then based off of this call i'll circle back and fill in some more details in a document and if there's uh just for reference and maybe highlight a few points um sticking points that we should help check double check next time we go through this okay so i did write up a few of the things i'm gonna go through this thing this document that i just shared um pretty much we have pre-launch and post launch um the week before the launch we definitely had a number of issues with a launch launch pad primarily around installation and usage of python there was an effort to get binaries prepared but that was not completed in time for this uh this round i think that would have solved a lot of issues and there's probably some debate as to whether python was reasonable choice at all um but lots of feedback has been heard um and there's a number of kind of rapid changes and iteration going on based on that feedback um and expect some some better ux around this um genesis info was found at 1727 august 2nd uh clients agreed it's good but after genesimpo was found that's when clients start appearing and we did uh we noticed a range of issues at that point um this is what i remembered and could find in my documents but the lighthouse boot notes were down uh the prism boot node enr's had the wrong ports and so cannot be contacted the prism release until august 3rd at around 13 30 did not actually have boot nodes set and so those piers that had turned their notes on early were had zero peers and teku included i believe that included by teku was just quite simply the wrong those issues led to a mildly stressful sunday um for some scrambling to try to debug and figure out peering and also led to um you know a number of required updates by node operators to be able to appear before genesis obviously the ideal is you have your node running genesempo is found network appears and we're good so we didn't hit that ideal and had to work through a lot of stuff there some of this is pretty obvious getting the boot notes correctly including the boot notes and there was a prison teku meta data issue i don't know exactly what was going on here i know there were a lot of errors in the logs but i don't know if it was a critical issue but that was also fixed in that time span and i also i know there was plenty of other small changes going on last minute things does anybody want have anything to add to the pre-launch discussion or discussion of any of those points and one thing that i noted was some people were making deposits that were 64e for 96 eve um i guess there's maybe a bit more education to be made that you you know if you make a large deposit that doesn't mean you'll have multiple validators right i think that um well to justin's point um we should find those um and prism does a pretty good job at least if you go and look at their faucet channel we can often track down users so sometimes we can we can figure out why they made that mistake you know was it a tool that helped to do that or was it a misunderstanding um so let's let's make a note of that and try to track down some of those users then yeah at least in my case and one other i know of uh there was a bug in launch pad which made a double deposit for a validator which was a bit as bit of a surprise gotcha so that was too discreet it seems that from all this i would be very keen to hear about that so that was two discrete positive deposits rather than um like a 64 degrees correct yeah two two uh separate 32 each deposits yeah yeah i loved it at the time um okay the launch pad will allow you to re-upload the same deposit data yeah i i did it once i had 64 i was doing 64 deposits at once so i had 64 metamask pop-ups to work through which took 45 minutes and 65 of them must have appeared i didn't count because when i checked i had 65 deposits of 32 eth with one validator having two deposits so i followed the workflow exactly as designed ben i think that was wasn't metamask because i had the same thing happened to me uh it was like maybe a transaction didn't get confirmed and then got sent twice or something okay um that can't really happen yeah i mean because of the way the knot works it that's like a double signal to metamask to create two transactions right i mean would be one thing could we talk to metamasks i mean to fix that ui bug that it opens like all the windows at once so one thing we're trying to we're trying to address on the launch pad side now is rather to wait until the the callback comes uh comes back from your whatever you've done the meta mask side before initiating the next transaction so it will just be sort of linear once you click confirm or reject it or just load up the next transaction for you let's um table thank you ben for the potential bug or ux issue that's definitely it's definitely concerning um let's table the rest of launchpad discussions um maybe carl where's the best place to have these discussions um or we could uh we could have a call to discuss some other stuff but i don't want to belabor the launch pad too much right now uh yeah either hit me up directly or issues on the uh the reaper the best places to address this cool thanks other pre-launch things that i missed it seems like um from my experience um i also feel like genesis play of two days may be short i'm not sure if anyone feel the same way i thought that increasing geneticity late to three days may not be a bad idea i've been thinking 96 hours four days minimum for for uh may not i i've said that a few times in different channels but not made that like abundantly clear but i i think for the v1 release which i think will primarily just have uh some main net parameter changes um but configuration changes at 96. you could argue a week um yeah i'd agree with that yeah another really nice thing and i think most clients are doing that is cutting a release after genesis parameters are known with genesis state so users most if not all validators are actually following an eth1 chain but there's also a class of users that aren't validating and don't necessarily need to follow these one chain and so it's it's nice to just have them have the state uh it's nice to not anyone that comes online two hours before main net have to like do all process through all the log so there's a lot of benefits there other pre-launch items that we learned okay great so then leading up to the launch thanks again east taker guys um that was really well organized call and uh doing something like that for amanda i think it's gonna make a lot of sense um i posted some just quick things that uh we would be looking for on that call very kind of user uh end user things that people can watch for um watching for early blocks watching for participation is kind of the main things um the initial epochs came out uh we got early blocks we definitely had some uh sparseness in those blocks but like we weren't tracking that to the t we were just kind of seeing okay there's some skip blocks here and there without knowing the exact percentage um but it became obvious after epoch zero ransom i think maybe the block explorer shows this but we also ran against some of the apis that we had i think maybe 57 on the initial epoch but during epochs zero through four so the first five epochs we had somewhere between 52 and 58 so after a couple of epochs became clear that we were not going to have justification of canality out the gate at which point um we began scrambling in many different directions i believe cayman on the call noted that gossip was overwhelming glodestar at least that was the declaration of the time i'm sure there's been uh much more investigation since um zahari also said uh that we're seeing i think in an internal chat problems with our nodes that might be contributing to low participation rate investigating um and at 27 after the hour marin uh said he's running validar node um successfully but lacking piers so only one attestation got included so um we had some client errors on both load sir nimbus which accounted for at least 10 percent of the network uh just from the client teams each client team is running about five percent of validators uh so that there's some of it um but we also began to uh look through the gourley madaya madasha whale deposits which we had tracked before and noticed that there were a handful of approximately five percent users that were offline um registration began to start climbing it was 62 percent at the end of epoch 5. i was able to reach out and contact one of the 5 users due to the prison faucet tracking they responded with just started them up should be going now slight time zone hiccup on my part i at the time noted that i suspect maybe he had prism running uh but it didn't appear to the lack of boot nodes from a couple of days ago but i have not investigated that theory um at the end of epoch six we had participation above two-thirds of the first time uh so we justified sixty-nine percent and then i believe at the end of epoch nine or the end at the end of epoch eight we finalized epoch 6. so we had finality and another thing i noted was that at 14 30 came and confirmed that the load star validators moved over to a lighthouse node contributing to an additional percentage we saw definitely over the first handful hours some waves and participation um getting up into the low 80 i think is about the highest that we've done and over the night um i think many lighthouse users had uh went offline and so we saw early in the morning in the us there was some drop in um percentage likely due to lighthouse failures um i believe these are still under investigation and at the same time there were around that morning the second day um a number of prison nodes had a sequence of maybe six to ten um epochs of no attestations and i'm not certain uh i believe there were a couple of critical fixes introduced by uh prison that morning i don't know if they've had any issues since then uh there's ongoing investigations with nimbus and lodestar the network generally looks healthy um i i know there's a ton of stuff going on that's not immediately noted here but i just primarily noted the things that affect us early on in this launch generally things look stable congratulations all around we're learning a lot and there's definitely some things to work through anything to add to just that sequence of events or the discussion around them right so honestly i think there's probably a little bit more takeaways from the pre-launch uh the post launch their uh the technical issues are just quite frankly things that they need to work through and we need to find stability um in the next couple of weeks um i am considering between now and mainnet launch doing one or two uh launch production launch simulations or full production launches but on test nets that have a lower minimum heat requirement and a stated end of life of approximately three days so that we can give users a chance to have another dress rehearsal and have us work through any emergent issues in the genesis process that might come up um i think doing at least one of these makes sense any comments or feedback on that yeah that sounds great to me i mean one minor pre-launch thing that i forgot to mention is like um sometimes it like it it it feels like people are making the assumption that um you know the the minimum genesis whatever this fantastic date as opposed to that being predicated on there being sufficient validators so maybe there needs to be like a little bit more education on highlighting that possibility i guess yeah agreed i mean we we're definitely even internally kind of planning on okay this is very likely the date i think we all know that it isn't necessarily the date um but even giving a four to seven day lead time also helps with the communication around that too um also it would be good to have um a trinity and uh never mind um for the next uh launch since they didn't participate in this one so i suppose they also want to try their hands at being an early validator i can't speak for those teams i know that they're both in a yes but certainly welcome to join if and when they're ready um joseph chow i believe that it is uh that the deposit contract does match the formerly verified byte code um but i can double check that off useful we made sure to use the same compiler and flags okay uh does anyone have anything to add just in general about medasha testnets cool i mean one one thing that i that we also i guess was that um you know prism is like dominating the network it seems um at least maybe according to the ethernodes.org like like significantly over 50 and i guess you know maybe worth putting more effort to try and reduce this dominance just for the the health of the network yeah agreed um as i think a lot of people are trying to communicate on twitter uh get the word out a bit of different clients and as um some of the other clients stabilize in the next couple weeks i will do the same i mean the other thing that i kind of is a question for for people is do do you feel when there were issues during during launch that you had the sufficient debugging tools to investigate them and understand where they came from sufficiently fast or do we still have significant work to do there as well like could you clarify what you mean by debugging is it like finding the bug easily or just what insight into the network and the clients right the tools that we use you know api endpoints uh peer store dumps metrics anything that we use and this is kind of from my understanding it's definitely ever growing um and as we have issues these tools are kind of expanding but um does anybody have insight on that were you planning to develop a similar tool in germany for quite some time and i think there's a lot of it's true that the clans have specific data structures which are internal but they also have some common structures like it would be valuable to visualize in real time how the vaults are building up with from other stations and blocks and if there is a common tool something like a webpage a dashboard that is using the rpc interface and displays this information in real time it will be helpful yeah i agree it seems from my understanding that the most opacity we have is on uh the gossip and probably specifically on once my node says i've sent an attestation what is actually happening you know because um frequently i think once basic stabilities happen uh there's there's been the question of okay why is my client not performing optimally um we really don't have much i don't think we have a good a good way of doing that right now um a tool that has been really helpful from the blockchain explorer is that you map your validator id to your ethernet address which also maps to your username which i am not advocating this for mainnet of course because of bell data privacy but that has been helpful just to finding out which validators are offline and then who to talk to and stuff yep and being able to trace back um faucets to users has been very very useful in the past four days okay ready to move on great um testing release updates i don't think we have a lot here um there was a v0122 release maybe right after the last call i think most of you are very aware of that um there are a number of gossip sub v11 params that are related primarily to pure scoring that have defaults i think generally built into the protocol but we are investigating these parameters so uh if there are any changes then a v0123 would be in order in the next week or so so keep your eye on okay client updates let's start with loadstar uh hey uh so we're um basically we're working through um some of the issues we saw in the but um on madosha launch uh namely gossip i think that actually the issue is gossip validation um and peering so uh we're kind of seeing some issues where we don't get very many peers and maybe some of uh some of that may be because we're not starting gossip immediately and peers are kicking us because we're not supporting that protocol but at least out of the gate but um we're basically working through these issues some stability issues with sync and we're thinking we'll be right we'll be back at at the test net in a few weeks uh when we nail nail everything down cool thanks so you're seeing high load maybe from uh attestation gossip sub verifications so like those conditions that you decide before you forward things no i think there's actually just um bugs in our block verification that we're like things were silently failing so it seemed like we weren't getting blocks maybe when we were or and then also that we didn't have very many peers so we weren't getting very much gossip in the first place so it's kind of the opposite of being choked it was being like we're thirsty for gossip gotcha okay thank you canon nimbus hi um so we've been working on medaya we have we identified the four issues there might be more uh we fixed two so the first one is uh that um the increased number of attestations uh revealed limits uh in in the glue code between lip p2p and obico node uh in particular uh in the period of non-finality we're doing the same work multiple times when we received uh the same attestations or the same blocks multiple time so we fixed this this was a source of high slowness another one is an attestation processing bug we wanted to optimize database loading by delaying the decilisation of public keys and signatures and we explored two approaches and unfortunately the first one that we merged had a significant impact on sync it was reverted and we are now using the second approach which gives the expected benefits but has no impact on the rest of the stack so this is fixed too as two other bugs are still being investigated so what number one is media blocks are often filled to the maximum of 128 attestations and we are reaching the limits of milagros so we will switch to blst uh next week and uh otherwise we also have a race condition uh on incoming and outgoing requests from the same peer which led to issues for example with lighthouse and also we have multiple reports of low peer counts but we expect it is due to the long time spent on processing all those at the stations and this makes us a node less responsive to network and then we get kicked by peers so this is uh for media and otherwise on the audit side the auditors finished the first phase of audit which was focused on networking we are currently also fixing the issues raised during the audit and the second phase which will be on the become node will start in 10 days thank you okay that is me so we finished integrating the blast bls library and merged it in uh is showing some very useful speed up over our old milagro jvm version which was particularly slow so we're getting about a seven times speed up we're going to keep the jvm version around just in case we come across architectures that the the blast library doesn't support but uh one day we might deprecate it um basic slashing protection is done now at the individual validator level uh internally we're working on splitting out the validator process from the beacon node process so that that work continues well and as we go on with that we're implementing the new api endpoints as as they become relevant we've been migrating a lot of our storage apis from synchronous to asynchronous um and improving the robustness of the networking in the face of the johnny attack that is handling denial of service attempts more gracefully and just dealing with a whole bunch of bugs usability bumps general overall improvements everywhere and finally we're just closing out our security assessment rfp we've got a good range of responses and we're interviewing vendors this week with a view to appointing somebody beginning of next week that's all from turku great thank you prism hey guys it's me parents so um over the last two weeks um before genesis we shipped accounts management v2 which contains implementation of direct derived and remote key managers we also integrated that with the launchpad steps and make sure this is documented and the well tested and after genesis we have mostly just been fixing um bugs and us issues that came from that we've also had a few big note related bug fixes since since some genesis as well we found a few the the the efficiency around that part um after genesis we have seen a surge of users joining our discord and asking questions and most of the questions regarding um what to do when you run validators and how and how can you tell if if the validator is running successfully and uh so therefore we have updated our dots portals on there as well to document all the steps on like what to check for when things are running correctly and yeah that's all from me good thanks trinity hey everyone uh the main thing for us has been looking at sync performance for our beacon node uh we were working with altona and of course namadasha uh we sped up things by like i don't know a couple orders of magnitude uh mainly with updates to the state transition which was really nice uh bottleneck now is bls performance so we're looking at integrating blst and then otherwise just a bunch of miscellaneous networking things and pilot p2p we've seen while working to sync these different test nuts lighthouse hello um so like i think a lot of people were just working on um fixing the things that we found during medela um stability issues that we're working on are going to cut a new release soon that should increase attestation inclusion at least remove it an edge case where it might not be included um we've got a slasher functioning in isolation looking at turning that on real networks soon hopefully in the coming weeks we've been working with the blst people to ensure that um bestie matches consensus um and also that um if we can try and make it a bit more portable so it can run on like we can build it for docker and can run on many machines so we're still in talks about that but we're running blst on medela um and it's doing pretty well um we're starting on testing and working for week subjectivity starts we have someone on that full time now um we're progressing with our validator user interface we have a basic visual identity down for lighthouse hopefully we can go public with some of that soon i think that's it for me thanks well another mind okay i think that's everyone uh moving on any research updates i mean no research updates per se but um stuff that i've been doing for uh zero is just you know pushing forward the the blast library um you know trying to resolve the edge cases that have been found and also pushing for for more optimizations and the form of verification is also is also in the works um you know very pleased with all the the results so far the other thing that i'm working on right now is trying to build a security team for ef2 i mean to start with basically finding bugs in in the e2 client implementations so the idea here is that ev2 security is a public good and it kind of makes sense for the eef to provide resources there for all the clients i've had about 40 applications so far and we're going to be doing interviews in the coming days and weeks if you have folks you think you know we should consider please please send an email to if to security that's if you're not working not just um any others is it fair yeah justin is very cool uh for the security team because uh i expect your own security team at status will be eager to participate or even help you screen some of the uh applicants and uh other teams would be interested as well uh to uh collaborate and scope up the role of this and how it interacts with all clients [Music] so you know more than happy to to to get people in involved i mean at this stage is like it's very early days and it was you know the the message that was sent out was very broad and you know we're looking for all sorts of different uh types of people so i guess um you know we'll do a first round of filtering uh in internally um and then um you know when it comes to to final hiring decisions we'll be you know in integrating interfacing with the the existing um security team at a different foundation that's currently focusing on ef-1 e4 and yeah it also makes sense to to focus with the the security folks at um you know the e2 teams you know i've also been chatting with just security researchers in in the broader ethereum ecosystem for example people that have been doing um [Music] that or this and things like that um so yeah there's there's definitely lots of people to get feedback from and and and different folks have different opinions um but yeah the plan is definitely to to provide more transparency um as as the the funnel kind of um becomes more more refined but yes feel free to shoot me um a message on telegram to chat more about that other research updates oh i don't know if that was mentioned last time but um there's basically been a a pretty significant uh breakthrough for the uh secret single leader election so basically the the the most promising approach which was the approach by dan bonay um which is very elegant very simple and and quite effective um basically had had two ways to be instantiated it could either be associated with zero night proof so we have four proofs the cleaner way forward is with the zero knowledge proofs but you know zero knowledge proof infrastructure is still somewhat nascent and then there's questions about trusted setups and things like that um it turns out that we believe that there's there's basically a tailored zero knowledge proof for the very specific um statement so we don't need general purpose um zero knowledge technology and the specific you know proof is built using um you know like the the discrete log assumptions so there's there's no there's no trusted setup as with parent groups um so this is you know an effort i guess mostly from uh from uh mary a different foundation and it's making uh you know secret single leader election really really really promising potentially even for inclusion in phase one if things go well thank you okay maybe on um secret shared validators um so we had um an introductory call a couple of weeks ago and um we're currently thinking about how to implement this i think like one of the questions i have is which of the clients teams might be interested in pushing ahead and implementing the api which hasn't been specified yet but i think like um we'll try that in the next uh couple of weeks to it should be only like a small some small changes to the validator api um but it would be nice to have one client that actually implements it so that we can get a prototype out there so yeah if you're interested then please contact me i think it shouldn't be too much effort i expect it to be like a couple of days if you have that sort of capacity i think it would be like really great if someone could do that yeah cool keep your eye on the secret share validator channel on discord for updates okay uh networking anything to discuss in the ptp spec or any issues we've run into on test sets that we'll discuss john yeah i had a i had a comment slash question so there's been some um discussion around like client distribution um and i think in general people are curious um what clients are being run what version the software is being run um i know you opened an issue about potentially disabling the identified um protocol and so i'm curious what the motivation for that is is it um are we concerned about actually leaking what version of the client and what you know what type of client is actually being run if not um is it yeah i mean the reason i threw that up there is that it's just an explicit leak of information that does not immediately provide a lot of utility to the network um it obviously provides a lot of utility to us in debugging um which is utility right right um i wonder if it actually like i mean in practice i would assume that building a scanner that can identify which client is running is not that hard right you find some characteristic of each client and you yeah like looking are we actually really gaining anything by not just providing that information right like you can look at these words you can look at how they you know the particulars of sync like you can i i agree you can figure it out so maybe we just leave it out there because it feels like this is security by obscurity and we're just removing one nice channel that we have to like understand a bit more about the network and for example do things like correct for um client imbalances and things like that john you've discussed that you think the identify protocol is a dos vector you want to talk about that um it's just sort of speculation it's not um and i think when we were talking you made a good point like it's not any necessarily any more of a dos vector than um you know then maybe other requests but um but yeah like i can i can repeatedly like uh you know i can use like rumor for example to like generate a peer id connect you know hit hit youth and identify request you know disconnect region another peer id um and it's possible i don't even need to continuously generate your ids so the question is one like um we want to disable it um for that reason um if that's the case uh should we throw it and with that information when the agent string is useful should we put it in the enr um if not like i'd say in general maybe how far do we want to go with dos protection in clients um with these repeated requests do we want to do some sort of like do we want to recommend some sort of rate limiting by ip um is that out of scope for clients like how far does the identified protocol is deserve to be any more of a dos vector than a metadata request or a status request well i guess i mean it seems like it's implemented like in lib p2p right so so at least with metadata and status can't our our clients could like specifically like catch it right and apply some sort of rate limiting um you know in the client logic okay so the identify protocol doesn't actually make any sort of like call back into the application software so we don't have much visibility there is that the main thing i mean i'm not an expert that's just sort of my assumption maybe someone else can speak to that um sorry do you have a alternate setup are you going through your computer instead of your headphones or anything like that i'll give you a minute well he's while he's uh getting set up i wanted to say that the one interesting thing that identify gives is the sign peer records and i don't know if anyone's using this but gossip sub 1.1 has this like alternate discovery mechanism where if you prune appear you can send all of the signed peer records from other peers that you've collected and so the the pruned peer will have an opportunity to connect to other peers that they may not know about and those signed records are sent in this identi uh identity protocol i see does the gossip v11 use that by default or is that a configuration flag i think it's like it's configurable to be on or off um so i don't know if it's actually being used okay well we should consider making a recommendation on that being on or off and that can be part of the decision on the identified protocol it's part so this that information is given in the identify protocol yeah i'm optionally yeah okay um i mean i'm relatively convinced that it's like not really buying as much by turning it off um i think we could put it as a may in the spec um but i think looking into how it's used in gossip sub and identifying if some of these protocols are uh oh yasik said so one way i'd use identified if faulty versions is to find faulty versions of clients and dos them specifically yeah absolutely that's definitely the concern there is if it makes it any significantly easier to identify clients i suppose the identify protocol could be overwritten to provide less specific information about version yeah we could just change the agent string you know and just say like prison lighthouse you know teku node star whatever nimbus yeah i mean then you know the version so that that in particular would be fairly easy to fix but we're concerned about knowing what you know what the actual client is all right and knowing the version is kind of nice but as you also says it does provide more explicit information like if i haven't upgraded my node in five months and there's another issue then somebody can go and find me and correct my own issue likely i could potentially find that through some other type of um information leak but maybe not maybe not as uh because uh even versions are pegged to different clients uh doesn't seem like a big issue for them what was that where is the information uh is there a specific problem like uh in currently in eat one uh you can identify peers uh by their relative versions right so it doesn't seem like a big issue like they're currently in each one well nonetheless like i think that maybe we're asking the question the wrong way um [Music] should we have a bar where we say that anything goes except the things that we haven't identified a way to break or is it rather that our bar should be that this has this particular use which motivates adding it and if we look at it that way well providing a graph on the web page with science that's i don't know if that's really a strong enough motivation to add something which um makes it cheaper at least to attack the network well i mean i think i disagree i think this has much bigger benefits in terms of knowing what the distribution of clients on the network is for security reasons so i think we gain more than an attacker from it because an attacker can always make the investment if they're willing to break a client they they can be they can um invest enough to identify clients as well i just don't think it's difficult i think like anyone could build something like that in a few days something to notice like and johnny brought this up default ports just totally leak the information um at this point because the default ports are different uh so we kind of already have that information there the other thing is is you know josek you make a good point about the you know the version information you know and being able to identify like what clients are running old versions and being able to attack them but i would also argue that us having that information basically allows us to raise awareness so like we see that like you know a lot of people are still running an old version of the client then we need to get the word out like hey this needs to be updated um so you know in some ways it kind of balances it out yeah i think i think the asymmetry is in our favor if we have that information i mean there is there are some arguments to removing it in the sense that it gives more power to an attacker right because it's it's trivial to forge these things and so an attacker could potentially forge them and kind of give the network would have a false sense of transparency and maybe people might become complacent and not develop the the scanning tools that thank you were mentioning because they rely on these on this information which you know could trivially be forged i would also say that like we should be making the assumptions that that our users are competent and and invested in their nodes and that that includes like keeping yourself up to date and having a healthy network setup right but but one of the things we want is have a diversity of clients how how do our new users know that they are not running the same client as everyone else if we remove that information right but at the same time we can if it's trivial for an attacker to figure this information out we can probably figure it out too uh for example default port scans on discovery call um and the second the second thing is that like even philosophically like if we're entrusting our clients to validate we should also be entrusting them to uh make this decision whether they believe that the multi-client setup is reasonable right i mean it's still a choice that they make and we give them this power to make a choice whether they run client c or d and the distribution that falls out of that we kind of believe that that is a good distribution by trusting no not if they not if they're lacking the information well we give them the information that they should be diversifying their nodes right and how can i do that if i don't know what the other clients are what the other nodes are running um should we maybe move that to a specific call dedicated to privacy yeah i think we've made our points and we're trying to if i can think about it let's yeah let's let's take a breath on this one um i do want to better understand how gossip w1 is using identify um it does strange that it does feel strange that the identified protocol is kind of overloaded in that sense so i want to better understand that um i i do think that we triple trivially link client information so that should be under consideration both in the for and against identify protocol um usage and i do hear you and i greet yasic just because it was default there doesn't mean we should you know be considering turning off we should be considering like do do we want this at all um i'm gonna i don't i don't i need to think about this one i think everyone uh let's take a breath and think about it um it's not critical that we decide today and the conversation can continue on that issue any last words okay other networking items uh yes so um aside from midocia there are new networks as well uh attack maps you can talk about later in the call and a new devnet after the somewhere data rock it's just the thin smaller does not used to debug the php communication between nimbus and lighthouse it's already live the devs are already looped in if anyone is interested in helping out deeper php programs or once we join the network they can ping us that's good other networking items okay uh preston brought up uh concerns around eip 2334. i believe in this eip the same seed the same mnemonic is used to drive using an hd wallet both withdrawal credentials and active signing keys this is certainly a convenience um but depending on how you choose to recover your withdrawal credentials you know if you're not using doing that on an air gap machine and then transferring key stores out of it uh then you are exposing secrets uh with raw credential secrets on a hot machine um and there i certainly see the debate here and see um kind of the niceness of going each path carl you wrote the eip do you want to chime in yeah i mean largely in favor of the as it stands now from a simplicity standpoint but i do appreciate that um having separate mnemonics does present other safety things uh i think there's a strong argument to be made for not rederiving keys on a hot system if you are going to be yeah if you are going to be doing that i think yeah i really think you're trying to avoid that but i'm curious to hear the counter arguments so the i think there is demand for having an alternative where uh your withdrawal credentials are fully isolated from your active sign-in keys for example i believe even just using hardware wallets and this type of setup is going to require these two things being decoupled my understanding is that there is no way to export private keys from a hardware wallet nor will that i think i don't even think that could be programmed into hardware while it's hybrid wallets are actually secure um well i suppose you're using but at that point if you use your mnemonic outside of the hardware well you're not even i mean what you could theoretically do is ask hardware wallet providers to be able to export the [Music] yes for sure i mean like this is all logic right no but the the private information is on the screen which prevents you from accessing it at all well i mean they can't they can [Music] they can provide signatures right so there's some software that's running on there and if it allows to export a certain derivative of the key then so i have no idea how they currently implement it but i'm sure if they are already currently implementing new bls software then this is 100 possible as a functionality to implement it in there if that should be done it's another question so the other wallet they give you the mnemonic to recover your key but that's all that's that's all well i'm simply saying there's a software running on the wallet that can create a bls signature if you modify that software so that it can also export a validator key then that would work there's nothing like actually um i don't know enough about secure enclaves to go more to make a strong argument well i mean my my understanding is that the software for that is not yet finished so it needs to be in that software that runs on the secure enclave for sure but if you put it in there there's no reason why it couldn't do that of course typically that's not what you want like typically all of them the software right now is programmed in a way so that it would never export the private key or any private keys and my my intuition on that would be that that smells funny and trying to convert hardware wallets export private information is intuitively seems like the wrong path um another comment here is that hardware wallets these their current instantiations i'm ninety percent on any percent sure they're not powerful enough to implement uh the required uh password hardening as an script or pbkdf too so they couldn't ex they couldn't export the current keystore formats interesting even if they wanted to i mean i'm also unconvinced that it's the right way to do it um i think because like for example another question is secret shared validators you'd want to create those secrets on the on the actual validator machines like separately you'd never want to create one single secret so um to me it just feels like there should be a way to support two separate um mnemonics and but but on a secret scheme you're not going to know your you're not going to know what your keys are or where they exist in a tree anyway not relative to a mnemonic i mean your keys are entirely dependent on what other people choose as their keys there's another other other portions so that then talking about the monarchs literally makes sense in that context okay i don't know yeah so i think that we should do some at least some thought and due diligence on how this eip or another erp might be modified to allow the optionality here my understanding is that especially some of the more sophisticated users maybe staking providers are going to be having their cold storage keys decoupled from their hotkeys and in that case it's probably best to have a standard that can support that i don't let me also say that i was actually very surprised when i did it through the launchpad and the cli that there was only one password and password that was like wait which one is it yeah but that's that's a semi-related or unrelated topic we don't give withdrawal keys um so i did update the the wording on the um the eip to say that if you have a sufficiently good reason not to be following this exact process you may or if you can't use the keys at this location due to some other reason then you may not you may you may deviate from it but you should try to stick to stick to it to whatever is reasonable like so there's some scope for for for doing this kind of thing if you really would like to how does that manifest though does that make it easier like does that allowance but not a specific specification does that lead to issues in actually developing software to do this particular use case well i think part of the argument would be that if you derive a specification specifically for having two mnemonics then you're only solving one small problem and that there's still more other different key usage scenarios that aren't envisioned that aren't then afforded by whatever this new standard is yeah i suppose the reason for the standard is to capture enough of the use cases without overly complicating things i'd also argue that by the time you start introducing to mnemonics that's a lot of room for confusion within further confusion as to how the system works i agree from a security standpoint it's it's worthwhile but i would definitely like for this not to be the standard flow okay i'm going to suggest we take this into the discord um there is a key management channel um does anybody have any further comment i don't think we have a solution today but i think that it does what further investigation cool um it's back discussion ongoing work on phase one uh just fix a bug yesterday and added some tests metallic you have some comments yeah well i've been working on the my my own annotated spec for some time and it's uh moved to the point where it's it's sitting in a repo now so if people want to look at that i welcome feedback um no there's link okay other spec items great um during the madness of madasha uh proto launched a new multi-client attack net that has tekku prism and lighthouse on it 4 500 validators and 15 nodes with increase and some more interesting and targeted boundaries these single client attack nets still exist and if you go and look at the rules or to be used for particular types of boundaries very single client things i don't think anything's fallen out of that yet but people of the internet go break them okay anything else today open discussion closing remarks great well happy debugging congratulations on the dasha launch and um i'll talk to you all very soon take care everyone take care thanks honey thanks all bye [Music] [Music] [Music] so [Music] [Music] [Music] [Applause] [Music] you 