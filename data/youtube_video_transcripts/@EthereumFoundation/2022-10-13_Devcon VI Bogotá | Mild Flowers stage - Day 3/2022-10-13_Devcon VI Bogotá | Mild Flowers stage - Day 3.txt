foreign foreign foreign [Music] foreign foreign foreign foreign foreign foreign thank you foreign [Music] foreign foreign foreign [Music] thank you [Music] [Music] [Music] thank you foreign [Music] foreign foreign [Music] [Music] thank you [Music] [Music] thank you [Music] foreign foreign [Music] foreign foreign thank you foreign foreign foreign [Music] foreign foreign foreign foreign foreign foreign foreign [Music] [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign foreign [Music] good morning good morning good morning [Music] hi everybody uh my name is morelian I work on security protocol security at optimism and um I'd like to say good morning to all of you who got a great night's sleep last night who didn't sleep and maybe went on a hike um okay well good for you uh and so now I'd like to introduce my colleague nor swap who will be speaking to you about roll-up shards and fractals the dream of atomically composable horizontal scaling let's give it up from North swap foreign hello everybody can I just get this started we'll see so um the titles are doozy it's very long but what we're going to be talking about essentially is scaling and there's some lag but basically there's two kinds of scaling vertical scaling which is when you have a huge machine and horizontal scaling when you spread the load between multiple machines I think my thing's not working all right there's just some lag fractals you guys see that so yeah scaling so basically I mean I can wing it without the slides by the time they come by so in this talk well it goes great um I want to conceptualize um you know vertical and horizontal scaling I want to show you a cool new idea I have for a shorting scheme which has Atomic growth short transactions and we're going to explain exactly what that means and but more importantly what I want to do is demonstrate some research process so like hey can we do this there's a problem there's a solution maybe it's at the end then we need to go in our Direction so can I restrict the process and generally what I want is to Foster interest in this kind of research and I want to I want people to collaborate with and and to work with me and with us at optimism on this stuff because this is the kind of stuff we care about this is not something that's really on the roadmap it's a complete side quest and so like I said this is a nurse knife like this is supposed to be brain candy super interesting stuff but if you want to work on it you should talk to me so like I said vertical skating big machines horizontal scaling many machines spread the load and in terms of ethereum vertical scaling has been done by Roll-Ups right roll-up enable vertical scaling because if you're validator you can validate the Roll Up by validating The Atrium main chain there's only one extra assumption is that there is one honest L2 valid errors that can also Supply data for the rollout but the question is okay we can scale vertically and we can probably scale ethereum 10 times maybe 100 times if we start pushing some buttons like skilled database and things like that what if we need extra scalability um and and there's multiple reasons we might want that so one is that sure we could like require a data center for L2 sequencers but we want to have a healthy um very Data Network and not everybody can get a data center not even not even every company or every Dao so we might want to spread the load so that you can validate a subset really of the chain other thing is that we might want many different Roll-Ups this might be Roll-Ups with different security assumptions and different parameters like different fee different pay tokens in a different pay fees the different token increase the throughoutput change the value rules maybe use a different VM all that stuff you might want to do but you might still want to have multiple Roll-Ups talking to one another okay so there's two main approaches to horizontal ability one is parallelization and that approach you can have a big blockchain lots of state and you can spread the road between machines two options to do that optimistic you just try to do things in parallel and you hope they don't touch the same state if they do you just throw away one of them and you start again if you do this the pitfall is that you're going to increase the throughput because um basically you can charge more for transactions are not paralyzable that's because you don't have a clear criteria of what is parallelizable and what is not our option is to use a strict access list some fancy people call this utxo but I think strict asset list is clearer and so basically that's shipping every transaction with a list of either all the contract that it touches so you can have a lock on the contract or all the storage slots that touches so you can lock this specific storage shots uh the polarization approach still assume that you have one entity that's why it is everything just now it's spreading things like across cores and across machines so it's still high cost and validators if you want to reduce the the loads and validators you may want to do sharding and then valuers can value a single chart so a short is basically a small sub blockchain the annoying thing is that that apps no need to think about on which chart they want to deploy which might not be great but the great thing is that we can do it eterogeneous roll up so like I said you can customize roll ups to have different VMS different V models and things like that and they can all talk to each other uh the problem with sharding is that if there is no way for charts to talk to one another in a way that is effective that's a bad solution so I'll explain what you know an effective way means but as you guessed it from title this is about atomic composability so this is an example I will use for a cross short transaction it's a transaction that swaps Bitcoin for either on chart a then Bridges The Ether to a short B and short B will buy nft for either so this is specifying a single transaction that you send either to a system or to short a depending on on your architecture so one very desirable property is atomicity if any part of this reverts everything reverts and in this case in particular if you come by your nft for either then you also don't want to swap your Bitcoin for either sometimes you can achieve this via application Level atomicity but in this case it doesn't work because sure if you you know if you can't buy your nft for either you could swap back your user for Bitcoin but now you've paid two swap fees and you've been exposed to bitcoin either volatility but there are sometimes cases where you can revert the action on chart a and if you can do that then you can do oh just oh okay I was standing in the wrong area so um yeah so in some cases you can do application Level automaticity if you can roll back the thing on chart a um so what do we have today today we don't have Atomic composability but between pretty much any pair of chains you can do um eventual delivery of messages okay so imagine you have a chain a that chain B you can use systems like layer 0 I think Wormhole does this Nomad to exchange messages between chains and what you you have is that the guarantee that if you do the action on chain a eventually you'll do the action on chain B the action on chain B can fail and there's nothing you can do about reversing the action option A that's what we have today uh that's implemented ideally with light plans and zq proofs but maybe also a good old multi-sig uh that happens so getting to the fractal part of the talk we can do a little bit better with roll ups and especially with Zeke Roll-Ups we can at least ensure a bound on this right this was eventual delivery could mean if I go back to the previous slide it could mean many blocks uh hmm sorry didn't mean to do that you could go back like many blocks in the future go you know forward main blocks what you could do is say well we're going to do this on the next block okay one scheme to do this use the ZK Roll-Ups and so basically you have this architecture where uh you have a apparent roll up here you can roll up one it has child drops you give up 11 and 12. and basically it's a fractal structure because the roll up one rolls up to the layer one but the layer 11 and 12 0 up to the uh to the role of one and if you assume that there is Ezekiel proof boosted every single block then now what you can do is do um instant delivery transaction between say this one 11 and one so for instance you could so do your swap here and then buy your nft here it's still not Atomic because you can't revert but now at least you know it's going to be done like in the next block or one block apart that's good but that's still not what we want so what about uh crush or atomicity so I'm going to propose a solution for this and we're going to assume that we're slicing our system into one blockchain block which is going to comprise one block for every charge so every chart Advance at the same rate and we want to have some form of atomic crochet transactions to enable this sure need to be able to send messages to other shards so let's just take the most nice possible idea which is eager enter chart blocking so you just process all your transaction at some point you hit onto a transaction I want to send a message to another charge you just literally call that chart you wait for the answer and then you continue so this is less crazy than it sounds and it's somewhat equivalent to what strict access list do because at do in the worst case you have the same throughput as a synchronous blockchain in theory but you can charge fees because you know exactly which transactions are crossing the shards the great thing is you don't need to specify the access list but the really really bad thing in practice not in theory is that you're assuming that all these shorts are being validated by different uh entities and so the latency for one charge to talk to another is really high and so in reality this is not feasible because the latency will just kill any throughput that you might have so let's try to instead reduce the number of exchanges between charts right let's try to bound this very tightly and so the idea would be to divide the the block time into multiple slots in the first slot every Shard executes its own transactions and also collects messages to send to other shards in the second slot all the shards extends the messages and then they run the messages that they receive optionally you can keep this going so you can have many many slots you could say all messages can send our messages etc etc I think in practice you don't really need this you can have a lot of expressivity with just like one hop just one short talking to one to another shark so this is still a bounded message delivery though you can't revert uh you can't revert part A If Part B fails and that's because other transactions rely on the result of the a part okay so this is very unclear so there's a graphic here so say I'm saying a I'm doing my local transactions I'm doing the a part of a cross chart transaction and let's say that's you know that's a swap so both of these will be swapping and then on video will be buying an ft so this transaction here the a port depends on the a part of the first one because the first one swapped uh say Bitcoin for either so it moved the price so this one will use the price from from that was moved here so there is a dependency link and because of this dependency link you can just revert uh you can just revert this transaction if this one fails because this one uses the result so this is the problem that we're dealing with and the general properties that atomicity requires synchronicity basically local transactions can still be processed separately but we would like all the crocheting transaction to be executed as though it was a single person executing them right um so we have a problem which is we don't want to chart talk to each other too much solution to that is to create a special chart I'm going to call it the atomic chart that will execute all the cross-shar transactions and we have a second problem is that to execute all the Crosshair transaction you need to state from all the charts right and this is this removes one of the big points of shorting which is that you can separate the the state between all the shards so uh the solution to that is to make the execution in the atomic short stateless and that means you will supply all the state that's being accessed by the the transactions for the atomic short so uh This sounds too good so where's the poop the poop is that you need to simulate transactions on on each chart individually so in general I need VM given that you're not sure the state it's impossible to collect all the storage slots accessed by a transaction you can approximate and the easiest way to approximate is just to run a transaction against the current state and just assume that the transaction before it will not revert so you know here it's easy you just run the state of the first transaction against the the state after all the local transactions for this one you just assume this wall this one will revert and execute and simulate but maybe it will revert so maybe your simulation will be incorrect other thing you can do with anything so you just um tell The Shard well um this transaction is going to touch this slot or you can give him some logic to find all the slots are being touched and there's many ways to do this that can be explored it could just be some evm code that just touches the slots could be some kind of infrological information or even out of protocol information that is not being validated but that's being sent to the to the sequencers um so I want to make things a little bit more clear so here's a Crosshair transaction very abstract it computes things be on some State and this state might change right we're not exactly sure what it is then it's going to send a message to short B and this message is going to depend on the result of the computation uh that message will have a result when I compute something based on that result and we're also going to send the results to sort C and then do some more computation so this shows you like sort of the dependency we have right we can depend on the results from one chart we can send to our short things that have been computed and so this will be important to understand later so to rephrase what I'm proposing before we dig deeper into the problems like I've showed you only a small part of the poop a lot more poop coming up um but the scheme is this phase one uh the short execute the local transaction and the simulate the cross-star transaction when they do this they will collect all the cross shot messages and all the access storage slots in phase two we do the exchange and the short stimulate the messages and similarly they collect all the storage shots in phase three all this oh sorry if history all the stuff that we collected here and there we ship it to the atomic charge and this will automatically execute everything and normally you should have um all the stated needs and if it doesn't it will just uh basically fail to execute transactions so the big problem with this scheme is that the simulation restrict the expressivity right so we can only safely Express a transaction where the simulation will be deterministic in some ways because otherwise you you know the state might change and you might not be able to execute things so you might sometimes take the risk um that transaction won't work say well I'm gonna try if the sales doesn't change it's gonna succeed but if the state changes it's not going to succeed uh we'll see that sometimes that doesn't work by the way this is exactly the same problem as building strict access list or you take so for slots same thing you need to collect all the the storage slots that have been accessed so this problem is uh is pretty similar so the problem you know when it gets worse is that crucial messages may depend on uncertain States and we need to derive all the messages during simulation and that's why we want to be deterministic um we can also want an answer from our chart and so we absolutely need to have a hint for the answer so let me illustrate because this is very abstract so what I was saying was that this parameter here x depends on the result of computation so this we need this to be always the same otherwise we just asking the bishar to compute something that might not be what we want so we absolutely need X to be uh to be known and you know that the message sends an answer and we're going to send this answer actually directly to C so we absolutely need to know the answer already also um I've actually Illustrated these constraints so here we need the computation to be deterministic in a storage class that accesses here we actually need the computation to be deterministic so to always return the same result here if we use y just in a computation we need to hence the result approximately just to get across Russia but if we send it to another chart then we need to know exactly what Y is like no no guessing otherwise we're just asking C to do some and we don't want that um so open questions is this reasonable is this a good idea are these restriction um feasible do they give us a powerful model that's useful in practice our question is can we aesthetically guard against some some cases of the non-deterministic execution that we want to avoid or do we just say well it's a user responsibility and maybe some people will build some tools to detect some of these cases this is for sure a foot gun right like if you're gonna make a crucial transaction and you don't expect it you expect it to be deterministic but isn't there's definitely there are a lot of possibility to make bugs but is it worth the cost our question is what is the correct abstraction level for all this if we don't enshrine static checks we can simply add like a cross short call upcoded evm and then just on the back end it will do what I explained but from the point of view of the VM there's just a new up code and later we can change the implementation to be something that maybe handles more cases now this is kind of janky because you have this up code and it's only going to be valid in cases where we can actually execute it right in case where the computation is deterministic at least if you accept if you accept a cost to expressivity um there's some similarity with the problem of building strict access list for storage Lots uh you know open questions is this worth it is it not worth it that's where you come in and you do some research and you talk to me um and yeah this is exactly this is this interesting let's talk let's do something about it uh feel free to reach out I'm North swap pretty much everywhere Instagram twitter.io and uh this has been my presentation [Applause] we have five minutes for questions I'm sure some of you have burning questions you should have a lot of questions because this is very questionable yes um yeah do you think sharding is maybe just fundamentally kind of impossible and like kind of like when you're trying to deal it's like you have all these different systems it's almost like they shouldn't be able to interact in this way it's a really dumb question um I mean in a certain way you're right like we're yeah the whole idea is like you can paralyze what you can but then sometimes you don't want to paralyze and you're just trying to build this hybrid like asynchronous synchronous system uh so yes and no someone else someone come to comes from some wacky idea there's another dumb question yeah in your model the regarding the new OP code that is aware of sending message cross charging does that mean like smart contracts need to be aware of calling another smart contract that is in another Shard yes how do we envision those things and also in charting without the New Hope code our smart contracts being aware of calling something that is in another chart because that brings friction to Smart contract Developers so yeah the model I've been assuming is the model of shorting as like small sub blockchains um there are some proposals of shorting that I would classify under parallelizations you just pretend you have a big blockchain and then on the back end you just put things in short and you move things around but for the user they don't see that here the model is explicitly you have multiple sub blockchains and they talk to each other um this I think simplifies the implementation a lot but not everybody agrees uh actually my good colleague car does not entirely agree with that um but that's at least my point of view and there are other advantages which is that you can have multiple Roll-Ups that do things slightly differently as I mentioned um so that's an advantage does it okay so regarding the the shutting in itself is it possible for instance to imagine a system where the accounts who dynamically move across the shards to try to reduce as much as can be the internship exchanges and so to simplify the whole process or is it like completely deterior use it's possible I know there's research on this I don't really know anything about it but yeah I think this was more related to the models where you just like rebalance The Shard and like you would you necessarily be aware of the charts um yeah it exists it's definitely feasible it wasn't really thought through here in this simple model um yeah I think this sort of needs some like like a balance chain where you have all your coins that will probably uh or at least have your balances be accessible by every Shard that would support you simplify design a lot and in practice when we see things like Avalanche they have like a special like chain for like payments because it's so special and so important um so that might be one part of the answer hey um my other question was um could we use things such as like AI or sort of predictive behavior in combination with this to sort of like just automatically optimize or is somebody working on on stuff like that to to sort of predict the interactions between the smart contracts and how to kind of optimize across charts uh I'm sure some people are working on it I know I know there's a bunch of research with that even on payment chains so things like Bitcoin and stuff I don't know why it was very hot Academy like academics are like lagging behind uh the stuff we do a lot and they've been like very like I've been to a workshop this year and they're very interested in exactly this problem and I'm like does this change the smart contract and we're like no I'm like What's even the point like just put it on a big server and be done with it um but yeah you could do it I sort of didn't assume that model here but it's doable okay thank you very much foreign [Music] foreign [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign foreign [Music] okay welcome everybody to flowers I'm morelian your MC for the morning uh and I work on security at optimism you may have noticed that the lights went out during the last session so I've been told that we are running on generator power that may happen again if it does stay cool it'll come back on in a couple well about as long as it took before okay uh and so for our next talk we have bobbin threadbare from polygon and he's going to be talking about using a hybrid utxo and and account based State model in a ZK roll-up okay give it up for bobbin thank you hello everyone um excited to be here so um in my talk let's see yeah so in my talk today I'm going to be talking for the first time about um uh how we've designed a polygon Martin roll up and specifically how we use this hybrid utx or an account based model to achieve some interesting properties so just to set the context at first the goal that we have in mind is we want to build a scalable decentralized roll-up with privacy enabling architecture and what I mean by that is that our immediate goal is to achieve scaling but we want to design the roll up in such a way that when we want to turn on privacy it will not require a complete architecture overhaul it should be very easy to achieve that um and I'm sure a lot of you here are already familiar with this but just to set a context of what is a decentralized roll up we have users we have rollup operators um and we have ethereum L1 and in this model users and transactions to The Operators operators aggregate those transactions into blocks and then they submit kind of the state Delta in a context of SDK roll up with a ZK proof to A3 ml1 and then um uh what we get and this is not specific to a decentralized role of this is a true for any roll-up we inherit security from ethereum um what is specific for decentralized rollup is that a roll-up is has its own L2 chain and it's um its own consensus mechanism because the operators need to agree on the state of the of the chain and then we want to have these operators to be the set of operators to be permissionless meaning anybody can join and leave the set as they please um now compared to a centralized roll-up where we have only one operator a decentralized rollup has a number of challenges and the most important ones are um you know you need to have a separate consensus mechanism um you need you also have this execution bloat problem and I'll explain what it is uh in a couple of seconds and then you have a state load problem so in this uh talk in this talk I'm going to talk about specifically execution bloat and stable load and so let's get into it so what is execution bloat an execution load basically means that the network needs to execute all the transactions and more specifically a block producer needs to execute on the transactions in the block but also everybody else in the network needs to re-execute the transactions to make sure that block is valid and you know that leads to a lot of executing the same code over and over again um what a state load I'm sure a lot of you are familiar with that this is basically means that state size grows with time the more accounts there are the more tokens accounts hold and all of that um you know the state size increases and the reason why we can't do much with about that is that uh nodes or operators need to be able need to hold the full state to be able to validate uh the blocks and nodes need the full stage to be able to produce new blocks now why are these things bad like I said there are challenges so why exactly are they challenges so the first thing is if you have state load and execution below do you need powerful machines too like let's say we have thousands and transactions per second you need a powerful powerful machine to process that if you have a large you know terabyte size State you need a large machine to hold that in memory and that leads to essentialization and if you don't have good solutions to this problem you might as well just build a centralized rollout uh the other one because everyone sees everything and everybody needs to re-execute transactions and have the full State um there is inherently less privacy uh in this uh in the setup and last one is especially specific to State load this is not sustainable if the state grows you can you know scale the role up only as fast as the hardware scales you can you can go like a Hardware in a single machine or something like that um so what do we want to achieve what is the ideal solution to this so the first thing we want to minimize execution below and that means we want to execute transaction only once and also we want to make sure that it doesn't have to be executed by the same party so it's not the same block producer that needs to execute all transactions we want to have distinct actors in a network that can execute transactions um we also want to minimize the state below it and that means um we don't want to enforce the condition where you need to know the full state to validate blocks and we also don't want to enforce the condition where you need to know the full state to produce new blocks zkps can give us you know these two upper properties if you have zkps you can you know produce a proof of execution for example and you don't need to re-execute the same transaction over and over again but to achieve the other two properties you need something else zkp is alone are not enough you need what I call a concurrent uh State model and before I get into the concurrent State model let's talk about like the popular approaches to what that you know the popular state models right now so we usually have an account based date and a utxo based date and if we look at pros and cons of each let's say for account based date um you know it's great for expressive smart contracts this is what we love about ethereum we can write very cool applications you have a lot of freedom and they will interact with each other very well uh it's not great for concurrent execution it is possible to achieve but it is not very easy and you know has a lot of issues and it is also bad for anonymity because if you want to if you have accounts and you know which account participation which transaction it's very difficult to hide kind of this transaction graph so to say you take So based model is kind of opposite of that it's great for concurrent execution because in a utxo model transactions are logically separate from each other it's actually a very good tool for anonymity like if you want to achieve anonymity you almost have to use a utxo model it's not the only thing that you have to use but it is one of the kind of basic building blocks but it is not great for expressive smart contracts you can kind of get smart contracts with new hxominal but it's not easy and you know the more expressive they are the more it starts to look like an account based model so what we want to do is combine the nice properties of each of this into a single model and I call this like basically account based model each Excel based model and combine that with EK proofs and we'll get something that I call is the actor based model with concurrent of chain State and I'll get into what all of those terms mean in a in the course of this presentation so first thing that I want to explain of how this works is how do transactions work in this model and what is an actor model specifically and how we think of transactions in that model so just to take a step back and explain what is an actual model it's a concept from distributed uh kind of systems where you have actors which are kind of state machines within boxes and actors communicate by sending messages to each other and the important property is that the messages are asynchronous so an actor can produce a message and then a different doctor can consume this message at a later point in time the way we apply this action model to a blockchain is that in our context in context of Maiden actors are counts an account holds a state and exposes an interface an interface is just a collection of methods which you know every of those methods is the Mind program and might and VM is a you know fully touring complete SDK VM so you can think about it as very you know expressive functions that you can write for the account interface accounts communicate with each other by sending notes to each other and nodes can carry assets and the node also has this pen script which needs to be executed to be able to consume a node and one important property is that in this models actually takes two transactions to move assets from one account to another so in traditional kind of ethereum model for example you usually have just one transaction that moves assets from one account to another in this model you have to have two uh transactions because the first transactions create a node and the second transactions consume a node now let's talk about transactions a bit in a bit more detail so what is the transaction in the context of Maiden a transaction always involves only one account the transaction does not involve more account and you know as a in the course of a transaction the state of the account gets updated um a transaction can consume zero more nodes and a transaction can produce zero more nodes so in the previous example for example there was a one transaction that produced One node and one transaction that consumed one node and we can have also transactions that produce and consume nodes in the same transactions um now the execution graph of how like let's say nodes get consumed um kind of to explain how this whole process works is let's say we have a transaction that wants to consume two nodes in a context of one account so the way it would start is like we have this product and epilogue that do some bookkeeping to make sure that like let's say sum of inputs is equal to sum of outputs uh and nothing kind of no new assets get created in a processor transaction but then we go into this execution stage where the first thing that happens is we call execute we execute a script of the node of the first node in this transaction and then this uh execute script can call any number of methods on the account interface so you know in this case let's say there is a receive method that receives assets on the account so a node can pass assets to their account through this receive method and one important thing is that account methods are the only ones that have access to account State and node cannot modify the state of the account directly it needs to call a method on the account interface to modify an account and then the account interface methods can create other nodes that's how you can for example create new nodes in the process of a transaction and then you know if we have another node we do the same thing sequentially execute the second node in the context of the same account and that node can again call the same or different method on the account to have different effects and so forth now in our context the because we can kind of execute and nodes only touch a single account what we do is we execute a transaction and immediately produce a proof for it so in our case we use the start proving system so might and VM is a start based VM so whenever a transaction is executed we immediately produce a proof of execution and because again I mentioned the transactions are logically distinct they only touch each account separately we can produce many transaction proofs in parallel so we actually produce all the all the transaction groups in parallel and then what we do is once we have a bunch of these transaction proofs uh we recursively aggregate them into the into batches and this batches then recursively get aggregated into uh block proofs and then these blog proofs get further aggregated into like Epoch proofs and that's what gets submitted to ethereum now it's important to know that all of this recursive aggregation can also be done in parallel so as I mentioned all transactions have been can be proved in parallel but also all batches can be proved in parallel the only thing that doesn't get through the parallel is the final kind of tip of this block proof and then there is another interesting property is that we can prove transactions locally and I'll get into that in a second of what exactly it means um but then these aggregation steps need to be done by the network for example a block producer or a block producer can delegate this uh kind of aggregation to someone else some other actors now let's talk a little bit more about what this concept of local versus Network execution so in a traditional kind of uh model okay when we execute a transaction we have you know a step that prepares some inputs for the transaction signs the transaction and so forth then we execute it then in the context of SDK system we generate a proof for this transaction and finally we get this transaction proof that you know according to the previous slide gets aggregated into batches and finally end UPS in the block now in a network model the block producer so that the user prepares the transaction sends it to the um you know the network and then the block producer would execute this transaction generate a proof and then you know aggregate this proof as I described on the previous slide in a local context the user can actually do all of this so the user can both prepare the transaction execute it and generate a transaction proof and then what gets sent to the network is actually just the transaction proof itself and then the blog producer doesn't actually need to execute the transaction and doesn't need to generate the proof for it it just the block producer just needs to aggregate it but it was other transactions for which it has generated the proofs one important thing to note is how do we handle shared State because you know it works what I described works very nice when you have like transactions which go and don't touch multiple accounts or like when you have nodes that go to different accounts and so forth but let's say we have something like a uni swap situation where we want we have several accounts that want to send nodes in exchange let's say assets for some other assets uh using uni swap account so in the way we would do it is that first we would have you know each account generates its own transaction to create a node that targets a uni swap account um this would be two separate kind of logically separate transactions then the block producer would generate a third transaction that would consume the first two nodes in a single transaction and also as a result of this consumption of this node it would generate other two nodes that would kind of Target back create like the exchange tokens back to the original accounts and then we would um and then we would have the you know additional transactions that uh the accounts the users of accounts one and two would execute to consume kind of this nodes back into the uh back into the their respective accounts so basically in this model we still have this ability to interact with a contract or account with a shared State just in this case the transaction that interacts with the account of the shared State needs to be a network transactions it's not a locally executed transaction must be executed by the network or the block producer because the block producer needs to sequence the nodes according to whatever logic they want to do and then execute all of the nodes against the same account now just to kind of summarize this you know pros and cons of local versus Network execution so if we want to have a shared State kind of an accomplish air State we cannot use local transactions but we can use Network transactions now if we use a local transaction we can have privacy because nobody actually on a network needs to execute those transactions um we cannot have privacy with network transactions because obviously somebody needs to execute them now generating proofs is a fairly computationally intensive process so the client Hardware requirements might be high for local transactions um but on the flip side because you generated the proof locally there is much less work than a Blog producer needs to do they don't need to generate the proof for the transaction they don't need to execute the transactions so the fees for such transactions for local transactions would be lower than for the ones that uh requested for the network to execute um now the next thing I want to talk about is what kind of a state model do we need to support this type of transaction model and this is where the uatxo and account based model kind of comes together so my little Royal Upstate is actually described by three databases usually you have a single database you have usually an account database or you know in each Excel context you have a kind of hxo database but in our context actually it's three separate databases there is an account database there is a nodes database and there is a nullifier database and I'll explain why you know why all of them are needed and then in our case updates to all of this three database so like when you have a Blog a Blog contains information that updates all of the three databases and you know takes the state of the network from you know stay tuned to extend n plus one um account database account database holds all of the you know current states of the accounts and we use the sparse Merkle tree to kind of uh as a data structure that holds this information uh and the sparse Michael tree Maps account IDs to account hashes but we have one kind of twist to this we have two different types of two different modes of storing accounts in this database the first one is on chain state which is basically the same as what you would get was ethereum where for each hash the node store also all the associated data for the accounts such as like storage code you know nodes and so forth but there is also an option to do just an off-chain state where what the node store adjusts the hash of the account and uh the the user himself or herself is responsible for storing the actual state of the account so network not Network don't do not store the actual account state um let's go to the nodes database next the nodes database stores all nodes that have been ever created and for this we use a miracle mountain range which is a append only accumulator uh and and leave in this market range is basically just a set of nodes that were created in a specific block and one of the reasons we chose this uh there are a number of reasons why we chose the miracle mountain region it's very convenient for a number of purposes but one of this is that um you can extend or add new nodes to this accumulator without actually knowing most of the previous nodes so you can discard the big part of the uh of the nodes database and still be able to add new nodes to it without problem the other property that is very important is EK context because we need to prove the inclusion of um you need we need to prove that we're spending a node that has been you know has been created at some point in the past is that the witness kind of inclusion witness does not become stale so you know if you have a mortal path it actually just needs to be extended from time to time very infrequently but it doesn't become stale and that means that hdk proof that you generate does not become absolute very quickly this is a very important SDK context uh and then lastly we have the nullify database and the reason why we need this nullifier database is that um you know we have the account database which stores states of accounts we have the nodes database that stores all the all the nodes ever created but we do not remove nodes from the uh nodes database because we want to have this nice property of append only accumulator therefore we need another data structure that will tell us which nodes have been consumed so the nullify database is something that keeps track of nodes that have been consumed and for this we also use a sparse Merkel tree where we basically map a node hash with a zero one a zero indicates that the node hasn't been consumed one indicator the nodes has been consumed so whenever we generate a proof for a block the the proof must include that you know this node existed in a accounts database and it did not exist in a nullifier database um we actually have a slightly more sophisticated data structure where there are multiple epochs and you know there was a time periods and for each airport you have a separate nullifier tree uh and then um you know nodes are expected to keep the last two epochs but can discard the nullifiers for the prior epochs now we have two we have this different databases and there are very different growth drivers for each of these databases so An accounts database grows primarily with the state of with a number of Public Accounts um or the accounts that have on chain State because if you you know it does grow within a total number of accounts but if you only have to store a single hash for an account that you know that's almost negligible like you can store a billion accounts and it's going to be only 64 gigabytes uh and also we can dynamically kind of prune this we can you know for accounts for example that haven't been used in a while we can just remove all the data and store the hash for that account nodes can choose to do that if they wish to uh the notes database grows with the number of unconsumed nodes so as soon as the node is consumed it can be safely discarded you don't need to store it anymore uh so unconsumed nodes is what drives the um the size of the database but also you can have this pruning where you can you can remove some of the notes and just keep the hash then finally we have the nullifier database and this one is a different one because you can't easily prune nullifiers to be able to create new blocks you actually need to keep all the nullifiers and the nullifier database depends on the throughput so like the more transactions per second you have the more nullifiers you need to keep in for a given Epoch we can make Epoch smaller but you know there are some downsides to that so overall you know if we look at kind of like what sizes of these databases could be that nullifier database is going to be by far the ones that drive the size of the overall State it's going to be larger than the nodes or account databases combined now I have a few slides to wrap up the talk to say well what did we achieve first we have this concept of different models of execution so the network is execution and local execution and we have this concept of on-chain data and off-chain data and the combination of this gives us you know different nice properties so for example if we have on-chain data and network execution this is a typical you know public transactions something that happens on ethereum right now we can also have stateless transactions if we have off-chain data but Network execution where um you know the network doesn't store the state of the accounts for example but the user needs to provide the state of the account with every transaction so the network can execute the transaction and the next thing we can do if the data is off chain and local execution is happening we can have private transactions where the network is not aware not only of what code was executed necessarily but also is not aware of the data that is in the account and we can also hide the transaction graph using utxos um it's I'm not going to get into that right now but it's a bit slightly more complicated but we can do that as well and then finally for completeness there is this uh you know local execution and on chain data I personally don't know which use cases that would cover but maybe people will come up with something now how did we address execution load with those models so first we achieved no re-execution so all transactions are executed only once uh second we have concurrent processing where transactions can be processed in parallel uh on Independent machines and you can almost scale this thing horizontally by adding more and more machines to generate proofs and finally we have this local execution where transactions can be executed by the users that are involved in those transactions and the nice property here is the more locally proven transactions you have the less burden computation loaded in the network has to encounter because let's say 90 of transactions are something that has proven locally there is very little work that the block producer needs to do to uh to they don't need to execute them they don't need to prove them they just aggregate them into blocks and then regarding State load we have kind of this Dynamic pruning where we can collapse accounts and nodes into their hashes um we can have very light verifying notes if you only want to verify State Transitions and you don't want to create new blocks you actually don't need to maintain the nullifier database at all and in that case as I mentioned the notify database is the biggest part of the state so you can actually discard the biggest part of the state and we have this nice thing where because the nullifier database dominates this overall State size the oral State size really depends on GPS so the higher the GPS the higher the nullifier uh the bigger the state but it doesn't vary with the number of accounts for example as much or number of nodes in the system and last thing that I want to leave you with is that this is what we're trying to achieve where the more privacy there is in a network the more scalable it is the more scalable it is the more private it is and this is our goal with the maiden roll-up thank you [Applause] happy to take questions yes I think the microphone is coming thanks so how would the network result when two accounts try to spend the same utxo like in attacks or something like that so um if two accounts are trying to spend the same utx oh that's a conflict you can't spend the same gxo twice um so I think the problem um so it's not really a problem in that case like if you are trying to spend like if you if you have utxo and I have utxo and we submit transactions for whatever reason that both of us can consume the blog producer will need to decide which of those transaction goes through because um you know you can't you can't execute both of those transactions simultaneously because there will be you know one of them will produce a nullifier that the second transaction will not secure because the nullifier for this utxo has already been created everyone so like in the uni swap example you can send so like you can send a note that says I want to swap token a for token B right at this price and somebody else can do the same thing and those are two different requests but then the block producer will aggregate those requests sequence them in a single transaction and execute them and there will be no conflict on that because the the state of the uni swap contract gets updated sequentially after each uh consumed node so you're not consuming the same utxo you're applying the different nodes to the same account but yes that cannot be done locally that needs to be done by a block producer yes um it's an optimization the idea is that if you want to have an um if like let's say your node was created in a prior Epoch and the nullifier was created then you will need to provide the path that proves that it was it hasn't been consumed yourself the node the nodes are not responsible for that so the notes are meant to be like a short-lived object so they are not meant to stay in a state for a long time and if for whatever reason you decided to keep the state there for uh quite long that's your responsibility to be able to provide this proof to the network it's not Network's responsibility to keep it for more than let's say six months or so thank you [Music] thank you good luck [Music] foreign so for our next talk we have yei Zhang talking about um oh shoot I don't have the title in front of me this time using a high no yeah Zhang's going to talk to you about ZK foreign should I start now or okay um so hello everyone my name is yeah I'm the co-founder of scroll today I'd like to introduce growth architecture and our pre-alpha test net upgrade um before diving into more detail for those who are not familiar with who you are so scroll is a general purpose screen solution for ethereum so in short he's just making ethereum cheaper faster with a higher throughput and more specifically we are building an evm equipment they can grow up so technically speaking is a liquid up solution which is considered to be the most secure screening solution with shortage of analogy based on math and we are also evm equivalent by saying that I mean like in our digital app it's by code level equivalent which means developer can reuse everything that they use on eum layer one are coolings including like hard hat and also develop development tooling and we can achieve like native by code level compatible which means you can make the code from layer 1 to layer 2 seamlessly and uh so in the rest of the talk it will be divided into into two parts in the first half I will talk about the architecture of scroll and uh like how your transaction is being processed down scroll and in the second half I will talk about our important upgrade for our test net and the roadmap like in a filler future so now let's to take a look at the architecture of scroll so before diving into more detail to give you a better sense of how screw work let's take out the traditional like architecture for liquid op so the idea of thickerope is that instead of sending all the transactions to layer one you send all your transactions to a layer to node and then layer to node will run something on our proof algorithm and a generator proof so the proof will be verified on Smart contract layer one and so firefighting the proof is mathematically equivalent to executing all the transactions so that's how you you get the scalability because for example if you only get 10 TPS it can so but but each transaction is verifying some proof which is equivalent to executing 100 transactions then you can scale your your network massively so intuitively uh like the architectural scroll look like this so you need some sequencer which is sequencing the transaction after receiving that and the challenge layer two blocks and then you also need some relayer to relay message between layer 1 and layer 2. for example like there are some deposit from there went directly through the bridge back and your relay need to relay this message from layer 1 to layer 2. and also there are some deposits where like sequencer I need to send this message to the relayer and after sequencer sequencing the transaction the catching layer two blocks it wasn't to approver and the prover will run some algorithms like we want to proof algorithm and a generative proof and the relayer will submit the proof necessary data and a unique feature of screw is that we are not running this poor in a centralized way but instead we have a decentralized proven Network for changing the proof so in our architecture we have a coordinator which will receive blocks from the sequencer and the generator execution trees it will dispatch the execution trace for different blocks to different products in our Network and the proof we call them rulers in our Network to distinguish from from miners they were around the KVM and the generative proof and then we'll send back group to the coordinator the coordinator will then send to the relayer and the release up to the layer one so uh so the magic thing actually happens on the ruler side where you are running some Wiki evm and generating proof for the validity of all the transactions inside the block so now let's take a look at the what's happening inside the roller so after receiving this execution Trace from the coordinator of a certain block uh the the dollar will around the KVM so what exactly evm so the KVM is composed of several circuits so the the circuit means so for so one circuit can verify certain functionalities for certain parts for example evm circuit can verify that your evm like State machine moves correctly from for example like push to pop and to the next next by by next up code you are executing and then Ram circuit is useful to prove that your read and write for this virtual machine is consistent for example you you previously write to some place and then you read so this Ram circuit can can prove that those are consistent and there is also a story circuit which means when you are updating the storage you are you are doing things correctly and there are some other circuits to prove some other like function entities for for evm including like ecd as a circuit for Signature and some bicycle circuits and some cat check circuits for other functionalities and you need a circuit input builder in between to translate your execution Trace directly fetch from gas to the Circuit specific weakness um and then like so intuitively the KVM should have multiple proofs right because it need to have approved for evm circuit I have a proof for Ram circus so but so all those proofs need to be verified on layer one efficient so what we do here is that we build another aggregation circuit so this aggregation circuit is used for proving that the proof is correct so for example like you know your evm the aggregation circuit is saying that event even prove the correct run proof of the correct and other circuit proof are also correct so this is your aggregation circuit and in the end you will only have one block proof for for the whole for the whole block uh to prove that your execution Trace is correct and uh moreover like note it was to notice that our our coordinator will dispatch block to different Proverbs so those dollars were General proof in parallel for different blocks they are not competing for the same block which will like have a better utility for the poor networking in our Network in our system because all the programs are doing something useful they are not doing something redundant um and now let's take a look at how your transaction is being processed on scroll and the workflow of scroll from a timeline perspective so from so let's start with the workflow they grow up so uh if I'm layer one because you need a consensus so you generally block very slowly and on Layer Two you can generally block much faster and with a higher throughput so you generate multiple blocks and then after a period of time you roll up your transaction data and uh and the journey to let it prove to prove that all the transactions are correct and send that to ECM layer one but worth to notice that this block data doesn't really rely on validity proof it's used for data availability so what you can do here is that also part of course design that we separate this block data with validity proof so you will like submit the block data first on chain to get some committed version which for example users can see their their transaction on chain uh like without even without the proof and the and then like you wait for some proof generation to to finally finalize your your transaction so accordingly like you have three different features for your layer 2 transaction one is called pre-committed which means your transaction is sent to a sequencer and the sequencer has already included your transaction in layer 2 block so it will send back a pre-confirmation which is just maybe three seconds and something like that so you get this free confirmation from the from our sequencer and the next state is called committed which means we already wrote up your data on chain and which may which usually takes minutes and so users can this is a much stronger confirmation because users can see their data and they even like replace the data by themselves and finally it's finalized which indicates that uh you are regarding the proof and the proof got verified on layer one so that's the final state where you get the final confirmation on layer one because your proof is generated and verified and uh so let's take a look at the like from a timeline perspective so you send your connection to uh to a sequencer and the sequencer has included your connection in the block so the the orange one it means the block is pre-confirmed and then like the the sequencer will upload your uh like data and with some proof to layer one row of contract and then like your your your block gets committed and then the sequencer will dispatch this block to the coordinator and coordinator will find will improver inside our Network for proof generation like to to generate proof for this block and the similarly for the next block sequencer will also like after committing this this block or also like coordinator to find the final ruler in the in the in the whole system and similarly you can do the same thing for for block three and block four uh and uh after after those proof generation the the proverb will send back the proof to the coordinator and uh the coordinator received multiple proof and then we do another like dispatch to dispatch those those proofs to another proofer and let the prover do some aggregation to further reduce the the verification cost because you can actually aggregate multiple block proofs inside one inside using one proof and then after this proof aggregation you finally get one proof which can prove that uh P1 P2 P3 are correct which means uh they they block one block two block three the transaction inside are valid and then you submit this block it's the Mrs proof on chain for a vacation and uh the the the rough contract will use the previously input as some big input and they've proved to verify that it is correct and then like finally your blog get finalized so that's a final stage for uh for a transaction and we have built a special raw arpeg floor to show the block status so for example you you have like a few seconds ago you have pre-committed blog which is the orange one and minutes ago you have multiple committed blog and there is a commit transaction hash where you can find which connection is committing your data and you can find your data Unchained um and uh there is also like finalized transaction hash which means for example your your proof gets verified and there are the finalized connection has here for showing like which transaction content is proof and we like when you get verified so this is a special Explorer like built by us for letting users to know that what's happening like inside and now uh like after some like talking about the technical uh background uh I will introduce our Scrolls pre-alpha test net and where we are so three months ago we have released our test net our pre-alpha test net um that version is mostly for the community users where we can get user feedback uh like they can they can play without pre-deployed applications for example a photograph unit Swap and also through their familiar like wallet like metamask so it's all for users and users can also Bridge the assets between layer one and Layer Two like for example they can experience the deposit and withdraw they can also see their transaction status through this real active Explorer so that's that's where we are like it's all for collecting feedback from the community to improve our UI and ux and also fix some bug ahead of time and uh we'd like to thank the our community for their helpful feedback so that we fix a lot of like bugs on the UI side and they improve our uh our front end a lot and we have onboarded over 10 10 000 users to test our bridge and depth and at the meantime we are still scaling our power pulling infrastructure to support 100 000 users hour wait list so the reason for that is that we don't open enough Proverbs for the for this test net so once we open this like decentralized poor Network for everyone we can scale out the users or the transactions throughput like massively um so and uh a few days ago we make a very big announcement which is the upgrade version for our pre-alpha test net so it's a very important milestone for us where so the most important upgrade for our test net is that we are not only a test net only for users and for pre-deployed contracts but we it's a test net for for the developers where uh developers can support can deploy arbitrary smart contracts on us so it's very important because it's like you know it's not only like interaction between users but you can actually you know Deploy on things on that and you can experience a seamless migration without any need to change any line of your code you can just directly copy paste your code from layer one and directly Deploy on layer 2. um and we also support all the tools around because we are natively evm compatible native and even you couldn't say on the background level we can support remix hard hat and the unit Foundry and all the tools around and we like days ago we have a hexagon that is global for liking high class to to register to our test net and deploy things on us we have also done some live demo at at East Global and also like yesterday at the DK Community session where like we let the community to deploy smart contract on us and we have opened this register to all the developers so if you want to become an early tester or the contributor Fab at scroll.io early Dev and like you can you can experience how how easy it is that is to to deploy things on earth um now just a quick summary for for you and developers um so the developer experience will be exactly the same as the ECM layer one uh and uh so for the concrete performance so layer to block generation takes less than sets three seconds which means for example like for for users you can get your your pre-confirmation like within three seconds it can be even further as we move move to like multi-block aggregation it can be even bring down to like one second and your experience will be pretty good and the deposit usually takes two minutes because you need to wait for six layer one block so it's not because us but you need to wait for like layer one blocks block confirmation and withdraw takes uh around like six minutes or more depending on your concrete like how many proofers you have in your network and what's the throughput so URL this takes like two minutes to to one hour but the first improve our generation already like for one block is six minutes so it's very fast and uh yeah so that's for for our pre-off test net and now let's talk a little bit about our roadmap and uh and where we are and what we we plan to do so from a high level our roadmap look like this so in phase one we have a pre-alpha test net for user and the developers so users can interact and developer can deploy arbitrary contracts through uh they are they registered and uh in fifth two we will move to Alpha test net which we will move to Let's very very soon which is a permissionless version and like anyone can do I could use that without any any permission and the developer can deploy like any contract without like register and so that's for our test set we are moving to that very soon and in phase three we open this layer to prove ourselves into the to the plural Community or you relate has a large overlap with the minor community so which means in this three we will open proof generation for anyone to be the approver and they can run their their poor machine and be one of our proven nodes to generally prove for us so that's in phase three and then we will move to field 4 which is our main net so the distance between like those like before minute is that one is that uh because the evm contains many lines of code which has metallic indicates that it won't be bug free for quite a long time so we need very rich secure auditing for our dkvm to be really confident that we we can we can reach the state of magnet and also we need to wrap up the sum of the rest leaky circuit to make that more sound and also improve our performance massively like through pre-oximization and circular optimization and in the fifth file we will apply some research results which we are we are doing like during the like in parallel with the development is that for example the decentralized sequencer to make the sequencer more censorship resistant and also like we we can take we can we can we are doing some survey for some video on the virtual machine and see if there are some interesting part to improve our DQ events efficiency and so that's our like high level roadmap and the one thing which we hear really like a lot of things from the the community that people usually ask about our decentral approver and what's the requirement for running such a poor node and what's our uh like plan for for for Hardware so like so I will tell a little bit more about our plan for this Hardware acceleration so we have three stages so in stage one uh we will build a private League event like GPU cluster for running this this proverb so or we have already built a very fast GPU solution to generate proof our liquid and circuits so the current performance is really good like for example one million gas only takes six minutes to generate proof like people usually think uh like VK generating they can prove takes uh has a such overhead and it's unaffordable but you know like it's actually very fast on on our GPU approver and besides that besides the GPU solution we have built we have also built a private GPU cluster to provide the very stable computation power for are testing at this stage and so it's already there and it's already live there and meanwhile we are collaborating with several large companies which are aiming at like making the owner proof faster they are Decay Hardware companies and they build more customized solutions for uh for making improval faster for example they are building some ipg solutions Asic solution and the GPU solution so that's in stage one like we we started this collaboration we already built a cluster there and then in stage two we will give access to our Hardware Partners to run our approver so they can test their approvers and a general proof for us but as stage two is still for large Partners which they are committed to General proof for us and uh something like that and we believe that using even more customized approvers can shorten the financial time and massively improve the user experience because you get cheaper approver and with even faster finality and so that's day two and in C3 we will finally move to this permissionless tour where I call that layer to proof Outsourcing where you are letting the external parties to run run the approver and uh we will open source our GPU approver with a permissionless license for everyone to use so even now like our CPU foreign totally open source you can already run the CPU approver uh if you want but just the CPU GPU proverb will ask you like you know uh improving the performance and will be open source later and anyone can be can run our approver and the product that will be permissionless and they can anyone can generate proof at home for us and we can also buy some customized Hardware from those companies or even stick to use because there are some companies are providing some provider service so you can stick there and use their service to generate proof for us so that's basically our our plan for this Hardware acceleration um and uh one last thing is that so we have very solid and decentralized Tech Team so we have four directions one is the infrastructure team which is building out our uh the whole infrastructure making that more more robust um and to support the permissionless test net and it's usually based in Asia and Europe and we have Decay team which building the VK circuits and some crypto parts and for example optimizing the approval performance um so those two are like engineer teams and we have we are across like six or seven time zones uh it's totally decentralized and also besides the engineering team we also have an in-house security team which makes things really special because the security team because we really care about user security right there are so many like Bridges or platforms get hacked so we have this security team which composed of several experts uh in like expertise in blockchain security smart contract auditing and crypto cryptography so they will be like in charge of our our security of the whole system and also like collaborate with external hackers and auditor uh to to make our system more secure and finally we have a research team exploring very like multiple research reactions for example how how to design a sequencer and how to upgrade the next generation's uh proof system and doing a lot of interesting research like that and also around ethereum like we we are actually contributing to to a lot of eips and uh yeah so that's part of the research team and our vision is that we want to onboard the next billion of users for ethereum because we think you know making the transactions really cheap and uh and your confirmation really fast will make more users going to ethereum ecosystem and everything we build is totally open and especially for the they came in part we are copied with the with a large community for example the privacy and scaling exploration team for example foundation and several other community members and we want to find for decentralization across different levels like starting from designation of the approver so if you are a vision aligned and you really like what we are building and uh we are still hiring and check out our hiring page and uh I think yes that's it and thank you [Applause] uh any questions or um yeah hi um so obviously you have this like kind of cool infrastructure with like the prover and the sequencer could you talk about how like gas fees will work in scroll how do you like price transactions yes so the gas state currently we hardcore that to be exactly the same as ECM layer one but it's my subject to change if it doesn't match the proving cost but it will be minor mostly targeting some pre-compiled very expensive pre-compiled which are not leaky friendly but most OP codes will be the same and right now it's exactly the same so hi can I know the data availability strategy for scroll yeah so that's a good question so currently we are direct like directly submitting the row transaction data on chain as part of the data availability and now you do believe that dunk charting and other like cheaper like Data Solutions coming very soon and also like by submitting the the low transaction data like users can replace the transaction when you are in the committing stage so you don't need to wait you know wait for the proof generation time to get a stronger confirmation ahead of time and yeah hey thanks for the talk um what's the impact of New York's on ethereum on the components like the sequencer coordinator and the prover uh so you're asking like how for uh how do you handle reorgs on layer one reorgs the organizations or blocks uh you mean like one they are one block are not confirmed or yeah if blocks get reorganized yeah yeah yeah so so basically when your transaction is within Layer Two uh it can be confirmed really fast so this regard like the the it will only influence your deposit so for now like we just wait for six blocks and uh but yeah in the future it might have to change if you think it's not so safe enough but for now like we just wait for six blocks foreign I have two questions one of them is about the hardware component how do you make sure that there's like a decentralized network of like the people who are provers if you're like working with specific companies like how do you make sure that the approvers are a decentralized network versus being like centralized to one or two specific like fpga companies or GPU companies that become very large stakers and then my second question is so this this this process for decentralizing this the prover can you talk about some of the differences for challenges in decentralizing the sequencer like how does those two processes differ and like what are some of like the the different considerations for decentralizing a sequencer versus the prover thank you yeah that's a very cool question so for the first one as I mentioned like we will have two versions firstly that as we are collaborating with the external companies we will also open source a permissionless license GPU approver so anyone can directly use a GPU Pro if they don't want to use fpga or some other companies and we are not incentivizing the fast-power because for example like even if someone has an Asic approver or someone has a RPG approver uh they don't necessarily like you can beat you because so the strategy is that we will have a time period for submitting the proof as far as you can like submit the proofing time like you can be like incentivized so it doesn't necessarily you have to generate like you know one minutes you can always beat the the other Brewers so it's more like for parallelization and how you are like making use of the computation power across the whole network in parallel it's not like so even if you have those Hardware partners for like companies you can still choose like whether you just want to run the independent layer using GPU approver or you need their service and so that's for for question one and for question two so so what we are when we are thinking of this is that so the program is easier to be decentralized because for example we are having for now like at this stage we are having a centralized coordinator so you can still have some like you know for example verifying improved and doing something like that and so when we are thinking of design the program sequencer because it's actually two communities because the poorer Community requires specialized Hardware but the sequencer might be like just some some level of like uh decentralization and when you're making the the sequencer decentralized there are some like problems like for example like uh you if you want to do some like fourth grade so and like all the interactions they are if if it's much much harder there than like using a centralized sequencer so that's my part of the the problems and also like how you incentivize between like sequencer and approver and how to balance those those incentives that's also part of the the challenging problem we face and how to make your the whole system more efficient because you you still need some consensus there among those sequencers and uh yeah if that um yeah [Music] [Music] [Music] okay everybody uh so we're running kind of ahead of schedule so we're going to start the next talk on the half hour so we've got about eight minutes foreign [Music] foreign [Music] [Music] [Music] come on [Music] foreign [Music] [Applause] [Music] [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] awesome thank you foreign [Music] foreign [Music] everybody um I'm morelian security and protocol Dev at optimism uh and this morning I'm happy to introduce you to oh hang on we have Franklin and Olivier from consensus who will be speaking to you about proving evm byte code execution in the ZK evm okay take it away guys hello [Applause] hello everybody thank you for coming um so yeah we are both from the Z sorry from consensus r d we've been working on a ZK VM pretty much for about a year and in this talk we want to talk to you about um it's everything today arithmetization how we implemented it and we'll have an announcement at the end all right uh so first of all why ZK Roll-Ups um the screen is not working for me okay yes so um well in terms of scaling ethereum one of the bottlenecks that is addressed by a ZK roll-up is that of the state so in order to validate the state transition which is um you need to basically execute the transactions within a block and the state is a big object and this validation is a resource intensive operation so the promise of a ZK roll-up is to basically alleviate that workload from most of the nodes on the network so um when you have a ZK roll up you have basically this powerful node which is an operator which provides proofs of State transitions and these proofs typically verify the following the validity of the transactions in the batch the fact that the internal logic of the roll-up is respected Roll-Ups are usually application specific and the fact that they induce the proposed state transition and here and throughout when we talk about proof we talk about proofs of computational Integrity which in practice uh implemented as zero knowledge proofs so a ZK evm is a particular kind of a ZK roll-up um the big distinction or the the thing that makes it a zkvm is the logic part the logic that is being executed or and proven in the proof is the execution of the ethereum virtual machine it also means that the transactions that roll in pretty much obey the same format as standard L1 and instruct transactions sorry Okay so um how does that work in concrete or semi-concrete terms well you have your L2 which has a state and a bunch of transactions roll in and they inducer or executing them and uses a transition of state and the ZK VM basically plugs itself at this point it extracts the required data from the previous state it takes into account the transactions and basically the diff of the new state and it does its magic and it produces traces which it passes on to approver and the prover then produces a proof which ends up on mainnet and in this way you bundle all of these L2 transactions into a single L1 transaction so um why would one want to build a ZK evm there's two parts on this there's the ZK and the evm part so the evm part um basically the advantages or the interesting Parts is that it allows you to reuse or to use all the existing tooling which has been developed for L1 you can basically write in a ZK VM write and deploy smart contracts on this on L2 just as though you were doing it on L1 furthermore you can redeploy already deployed by code onto L2 and the ZK part and the zkvm well it gives you the basically the scalability Boost and also finality faster finality just because of the fact that there's a proof Associated to it so that makes it interesting as well okay when we set out on this project we had a few goals so we wanted to be able to in our zqvm prove the execution of unadulterated native bytecode and respect the logic which is specified in the ethereum yellow paper we wanted a full coverage of all the op codes where we allowed ourselves to deviate is in terms of the representation of the state and so for instance we will not be using ketchup we are building a type 2z KVM in the sense of the classification put forward by vitalik so this project I'm sure your web presents a lot of challenges there's a lot of complexity that comes from the evm itself which is composed of many parts which are tightly coupled and have complex interactions there's a lot of intricacies that are really specificities of the evm you have families of op codes that have slight variations in the execution uh it's slightly non-uniformity there's a completely different kind of challenge which is that of auditability so Frank Lang will touch upon that in his portion of the talk and the main challenge with challenge which everybody faces today is that of performance efficient proving schemes um yes and this is something we will communicate on at a further point today is really about the arithmetization okay so here's basically the basic setup um of how it's going to work um you will have a modified ethereum node and execution client which receives transactions sent by users by dapps we plug ourselves into this execution client and extract some data which we use to fill some traces I'll be talking more about traces later and these preliminary traces are fed into this tool which we call corset which does many things among them it is responsible for producing a constraint system and it also expands and computes the remaining parts of the trace all of this constraints and expanded traces are then fed into our in improved system um the inner proof system we use is not compatible with ethereum per se so we have to feed it into a verifier which is a circuit of a bn254 this is where we plug it into with canarc and not then produces the outer proof which is then posted on mainnet okay let's talk about a bit about the arithmetization so first of all um on Monday we published an updated and expanded version of the spec which is now a pretty hefty document and its contents are basically the arithmetization whoops of the evm so when we talk about arithmetization in this talk at least we mean to basically construct or write down a system of polynomial equations the simultaneous satisfaction of which perfectly encapsulates or captures a particular computation performed on particular set of inputs for us the computations of Interest are valid executions of the ethereum virtual machine given a set of transactions and an initial state okay so since the evm is a Beast of complexity it's a big thing we basically need or it's it pays off to try to decouple as many of these components as we can to work in a modular fashion and to sort of concentrate the complexity in different places so this is the general architecture we have we have the central piece which we call the Hub which is basically our stack and our call stack and then we have plenty of smaller modules that are tasked with doing specific kinds of operation such as arithmetic or binary operations or storage and uh I don't know if you can see it but uh uh there's also the memory Parts okay it doesn't work the memory which is this mmu and mmio modules foreign what do you get you get these traces that I was telling you about which are these large matrices which contain data represented as field elements there's one such Trace per module and each Trace obeys its own internal constraint system so on the previous slide you had these arrows which pointed from one module to another and this is basically connections pillar cap connections which allow us to transport data from one place to another um and uh the other kinds of constraints are basically the internal constraints so for instance when you update the program counter you expect something particularly to happen but you also have another kind of constraint which is sort of global constraints Global consistency constraints which range over the entirety of the block rather than two or three consecutive rows and that for instance May Express properties such as that well when you re-enter an execution context and you load something from Ram you get what you last put there so let's zoom in a bit on this Central piece The Hub which is our as I said our stack in our call stack um it gets its instruction from the ROM and what it does when it's once it has an instruction is to basically dispatch this instruction wherever it makes sense um so once it has an instruction that it loads from ROM it first does some preliminary decomposition of that instruction it extracts some parameters which are hard-coded and it decides on certain things such as how to interact with the stack how much data to excavate from the stack and where to put it in the in the trace the layout basically of the data it also raises some module Flags Etc and the next step is then to dispatch the instruction but before we can go there we actually have to deal with potential exceptions and the The Hub also deals with the exceptions it's basically some of them it can detect others it imports from other modules and if an opcode makes it past this hurdle then you have the instruction dispatching per se which um kicks in and you have these flags these activation bits that light up and you have some stamps that are updated because you need to keep track of temporality and yeah at this point these activation Flags well well they tell you what will be active so for instance in when you do a create you will be touching Ram so you'll be touching those two modules mmu and mmio you will also be touching ROM because you'll be deploying initialization code and you will touch gas and memory expansion and same thing for crate 2 but since there is a larger hash involved for the initialization code you tap into some hash modules okay so let's talk about about the next big piece which is Ram so Ram is probably the most complex piece in our optimization by the way all the figures that I put here are available in the document and some of the well one reason why it is so complex is that it has all these um data stores which with which it can interact um and the different instructions which interact with ram actually have different sources and targets so that there's already some first complexity the next source of complexity comes from the fact that operations which are Atomic from the point of view of the evm such as returns or creates have to be broken down into smaller Elementary operations in the zkevm and so the first Target task is to basically do a lot of offset computation and deciding when some padding has to be done this sort of stuff and once all of this has been decided well you can start writing instructions and this is still just writing them without executing anything you just have this sort of workflow that tells you in this case I need to do this and that some xoramped slide chunk or something and once you're at this point you are at the phase where you can actually start doing something so this is the the work of the mmio which is the the actual Ram in a sense this is the the component that touches data and that actually does um a sort of byte decompositions recombinations slicing dicing surgeries Etc and then you have these consistency checks that I told you earlier about which is basically finishing the the memory part basically what you've written last is what you should retrieve next time so I'll stop here for the arithmetization and I'll hand it off to Franklin so thank you for the this very extensive description of what is a constraint system on the arithmetization and now I'm going to talk to you about this comeback thank you I'm going to talk to you about how to go from this uh well let's say conceptual data to how we actually implement the whole thing so there is a lot of challenges when we want to go from the specification to the implementation and the most the biggest one is that there are three moving Parts on the one on we have the actual specification 250 Pages then we have the implementation of the specification and then finally we have the proof system of the verification of the of the traces so all of this stuff is developed by different people working on different teams we still need to maintain 100 Conformity between all of these pieces so of course if the approver is proving something else that's what the spec is describing which is it's itself something else that what is actually implemented nothing will work and finally he start to audit three diverging code bases or three diverting sources of data so what kind of solution did we find we developed a formalized single source of Truth which is then exported to multiple targets so what happened is that we have in some formats that I will show you in a few seconds a description of all of this constraint system and from this single source of data we are able to produce first uh go Library defining the constraints on the data that will be in the that will then be used by the approver to to actually prove stuff then we have another good library that is used by the ziki VM implementation to ensure Conformity with the specification and finally we can generate latex data for integration within the final specification document on the 215 pages of PDF so here is how it looks so you can see that this is a very clearly lesbians very inspired languages with a lot of parentheses all that kind of stuff and this is a very simplified example of what you could find for example in all of the mmu so what do we have in this first we have the columns definition those are like The Columns of the matrices that already here show you a few minutes ago and you can see that as it can either be normal like the one at the top or they can be we have a very rough typing system type systems that is used for some optimization and finally for the pure pure ease of implementation and ease of use we have some kind of very simple array that kind of stuff so yeah afterward you have helper fractions which are functions that can be work defined like any other list function to act on this data and for instance here you have two functions the first one is checking the two arrays of length eight are actually element twice equal one to zero zero on the other one is Computing the we're checking that an array of eight elements is actually a by decomposition of a given value so so do these do not really exist per se but they are just like smaller syntactic sugar for the ease of implementation and finally we have the the meat of the the meat of the data which is the constraints themselves so here we have an example of two three constraints so the first one is doing a battery composition of some data the other one is checking the memory is aligned and so on and so on and from this we will run it through courses at um odia evoked a few minutes ago on cross a we will do quite a few things and among other it will for instance generate a lot of go-kart for the proofer that because you can see here and you can imagine now writing this by hand would be a living nightmare or we can also generate latex so here you are for instance a piece of a piece of latex card on the PDF rendering that is ready to be incorporated in the spec so this whole stuff that we call corset is really a Cornerstone in our work workflow on the our implementation of the zkvm now I will talk to you about some results that we have reached for now from the specification to the implementation to the actual results so the first thing we want to Benchmark our implementation against is the evm test Suite of course the event Suite is a golden Center for ethereum clients including but not traffic not restricted to the evm itself so we have tested our current implementation of the zkvm which is well Advanced but not yet finished so for now you see here the list of modules that are ready so we have the Hub the mmu the ROM they'll use the binary on some comparator functions and uh on over 17 000 tests we run our evm on this and uh 16 000 are success so it means that it Trends on the traces are validated zero are failing so it means that we do not have any problem handling this this test and finally we have 1 303 which are heating functions that are not implemented in the in the area in the ZK VM namely in this case the pre-compass contracts on the self-destructure operation so for now we have a 92.6 percent success rate on the evm test Suite which is we believe a good start another way to test and to Benchmark or implementation is of course to work on real data right so we have quite a few real-world examples but the most striking one we are we each is I believe the successful execution variation of proof of exemptions using the units of contract and the successful execution and validation of random magnet block so what we do is that we run our ethereum client on the main net and we generate traces for some blocks there on there and then we validate all of the constraints so I will show you a little bit about this work so here we are going to check the verifications of the constraint of the on the traces we generated for this block 0x35b is even in it blah blah blah this block was created yesterday in the afternoon so let us start on the while it is working I will show you a little bit of what is in this block so here is the data on network scan of this block as you can see it is like a 2023 house all we have quite a lot of different transaction in it so we have a basic transfer transaction we have some wrap stuff we have some multi-call we have failing transaction we have successful transaction we have uni swap we have teaser so it's quite a nice example it's not a very big block it has only 52 transactions if you remember correctly and it's only using uh 2 million guys on the half but it is still quite uh quite inclusive in what it uh what it provides if we take a quick look at the traces that we generate for this block so here you have a decomposition of all the data that we generate for this block so on the very big red stuff at the top of the screen you can see that in the end we generate certain let's say 13 13 and a half million of cells so which is like the actual content of the traces you can see that the biggest one are the binary with 2.5 million of cells then you have the ROM sorry the ROM first with five Millions then the binary with two million and a half the Hub which is also before the binary sorry with three millions on all of that is culminating into 13 million of cells that have to be proved unchecked on then cryptography checked so here we can see that corset is actually doing a very knife check of all of this because it will basically just run all of the numeric numeric constraints that we defined on all of the lines on rows of the Matrix one by one which is obviously very suboptimal absolutely not cryptographic but this is only debugging tools that I show you for to to prove you that our staff is still working and as you can see the validation is successful on the Block 0x35be is actually validated with all our constraints on our evm we'll be able to give all of this data to the proverb on the profile will be able to generate the proof on put it on the main chain now going back to the to the presentation so sorry I can't play because no Wi-fi so you will have the development version so in conclusion the complexity of the VM implementation has been partially solved with modules were completely solved with modules the intricacies of the evm may be Olivia you want to say something about this okay sorry um yeah so in terms of um basically finalizing the arithmetization we have two big two big chunks that are still left to be done um but we're quite confident that we'll be done um basically for the test net sorry regarding the auditability we are we have we don't have yet no data a formal verification of what we have done but thanks a single thought of shows mechanism we have laid the groundwork to actually be able to work on that and prove in a single strike all of the successes all of the the three components with a single single audit and finally regarding performances we are now connected to the program system and there will be more more information regarding that soon on the in a inner coming paper so thank you for our attention we will be launching a testness soon so if you want to join a please just scan the stuff or just go to the to the URL and if you have any questions and we will be happy to take them can you just give a high level overview of of the differences between your implementation and the current other ZK EVMS such as from scroll and and polygon Etc I don't know exactly the inner workings of Jemez and skull I know that we share some design principles with both I know that our approver will be different arithmetization is also going to be very different and um I think this probably represents actually a strength overall not for us in particular but for the ZK evm ecosystem as a whole keeping in line with this multi ckv improver future that vitalik has been talking about um in terms of concrete details I'm I'm not quite aware regarding control details the big difference with scroll is uh let's say the arithmetician method that the the web the both of us are using different methods and we will see which one will sustain the test of time on regarding the difference with polygon Hermes the main difference is that a polygon contrary to us doesn't directly works on evm bytecode but the first translate the evm byte code into another byte code that is running not on the evm but on adock register machine which is then approved so there is the suprematory step that neither scored no we do do have foreign you may be surprised but call data load has been horrible you would expect if you do if you can do a m load you can do a call data load but in the way that we arithmetize things it's actually quite difficult because of the provenance of the data and the fact that there's some padding involved um but in terms of the real complexity the real complexity is actually for anything that involves writing a whole lot of data with padding potentially so anything such as code copy xcode copy I don't know um yeah basically these kinds of op codes have been the most complex and if you look in the arithmetization about mmio there's literally Pages upon pages of um I'm defining in we're defining nibble nibble seven and bit eight and they have to interact in some complex way um memory has been the worst [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] thank you hello everybody Welcome to flowers best room at Devcon and we've got a great talk coming up now uh uh we'll have Martin durka speaking to you about the blockchain bridge that you dream about I know you all dream about blockchain bridges don't you he's going to tell you about it [Applause] all right uh hi everybody thank you for coming my name is Martin I'm from Quan stamp I'm a senior research engineer and the head of new initiatives and I will be telling you about the properties of bridges that I really like so before I do that uh we should all understand what a bridge really is right so let me do that so when I say a bridge I kind of sort of want you to understand this type of an architecture we have two chains my mother chain or you know all intent and purposes this will be ethereum and then we have another chain and we want to get our assets from one chain to the other chain so for this purpose we have something that I call a custodian this is where me will lock some assets some ether then we have an off chain component called a communicator that is going to be watching what I have locked in the custodian and is going to instruct some liquidity pool on the other chain to uh send me the assets that I want to have breached the way how this really happens differs Case by case some bridges issue some debt tokens some bridges work with real liquidity pool on the other chain and just send me the assets then this was the deposit on the withdrawal the process is exactly the opposite so the communicator is actually going to be watching the debt issuer where I will be burning my tokens the data tokens and that then is going to instruct the custodian to send me back the eth that I want to have a bridge back okay so this is the mental model that I want you to maintain when I speak about bridges so now I'm going to give you the properties the menu of the properties that I like uh about bridges when they have them so first of all we develop Bridges because we have other chains many of the other chains are sort of trying to speed up ethereum right but you know if the bridge takes ages to bridge my assets from one chain to the other one then this is not really the user experience that I want to have so what I really want from a bridge is the speed then the next thing that I really want is finality that means that once the assets are bridged from one side to the other one or from the other side back this is final nobody is going to reorg nobody is going to bring those assets back they are just not going to disappear the next next thing that I really appreciate is atomicity so that would mean that there is no moment in time where I do not hold the assets I have them on one chain and then as soon as this bridging happens atomically I suddenly have them on the other chain there's nothing in between where the world could stop where an asteroid could hit the earth and I could lose my asses because you know I have nobody to call or something like that but this is actually really hard there is something called Atomic swaps but you know that's a really really complicated protocol so you know Thomas city is something that I long for but it's really hard to accomplish then you know very obviously I want my purchase to be secure because nobody really likes losing assets if you do please talk to me after the talk you must be a very special kind of a human being but we usually want the bridges to be secure then what do we long for in general in the web 3 space is censorship resistance that means that anybody should be able to use the bridge then we certainly also want availability meaning that it doesn't matter whether it's 10 AM or 10 pm or 1am or whatever I should be able to use the bridge right now if I wish to do so then I also want lightness liveness means that if I instruct the bridge to move the asses from one chain to the other chain at some point this is going to happen the bridging will never get stuck on the other hand sometimes we also want possibility because if one of the chains is in troubles you know there is a hack there or the bridge itself has problems then sometimes it really helps when we can stop the time when we can freeze everything and do some crisis resolution or something like that right next thing that I really want is liquidity so you know I might be a very rich person I might choose to today Bridge 100 million dollars and if I choose to do so I would really like to be able to do so and that can happen only if the bridge is sufficiently liquid uh the next thing that I like to have is what I call expressive power so I want to be able to bridge my native assets that means ether but I also want to be able to bridge a bunch of stable coins and other tokens right maybe nfts so all the erc20s erc721s ERC 1155s what I'm really saying here I want the bridge to actually be able to perform cross-chain smart contract calls then I want my bridges to be cost efficient because you know I don't like to spend money on gas right but sometimes we don't really have the option to do anything about that because we're interact interacting with two chains the transactions have to be paid for on both the chains right but it would be nice if the bridges were not adding any additional cost overhead to that then privacy is also often desired imagine a mixer uh where you know I send money in on one chain and they magically show up on the other chain in a different account and nobody can connect the transactions together and then last thing that I really like is transparency and on disability that means that if I'm watching the bridge at any point of time I can convince myself that the bridge is actually functioning properly that it is not you know sending money around without any reason without it being instructed to do so so this was the menu now the main point of this lightning talk is this is actually really hard there are always conflicts between all these properties that we would like to have for example speed and finality they are conflicting if I want my bridge to be final or the bridging process to be final then finally has to be reached on Two Chains independently right uh but that's probably not going to be fast similarly if I'm talking about availability and pausing well once a bridge gets paused it's not really available right it's not accepting any transactions I cannot bridge at this moment same thing with security and liquidity sometimes uh you know bridges are really liquid but we don't really like to have all the assets sitting in some smart contract that potentially could be exploited on one or the other chain right so therefore Bridge operators very often limit the liquidity they take the assets they move them into some escrows cold wallets and similar things so there are always a bunch of trade-offs you have to pick and choose what you are going to implement this slide is actually you know screaming with QR codes so this QR code leads to our talk from each Denver about security incidents in bridges and this particles uh QR code leads to a paper about security hacks and incidents and bridges that's everything I had to say uh if you want to talk to me uh during Defcon or anytime later this is my time thank you [Applause] [Music] [Music] [Music] foreign guys [Music] hello everyone welcome to my talk today I'm going to talk about a different way to design automated market makers in a way that hopefully is going to capture the Mev that currently is being lost to Miners and other parties and I'm a smart control developer at Cal swap housewob by the way is a an online chain exchange that protects you against Med so what is Mev or maximum extractable value currently block Builders have the responsibility of updating the state of the blockchain so how is this done by including excluding ordering transactions in a block and for this they get paid these transaction fees and Rewards however there are other ways where that that you can use to get money out of the block building process so one very common attack is called the sandwich attack in which user transaction is exploited to extract Mev so let's say that a user is creating a transaction once included in a block to sell on an automated Market maker 100it or 100 000 USBC so as is usually customary there is an extra slippage added in case the price moves between when the when the transaction is created and executed and normally it shouldn't really matter the user should take the expected price however the point of sandwiching is actually using the slippage against the user so how does it work the block Builder creates two transactions one before and one after the first transactions this first transaction buys usdc and moves the price of the amm to the minimum that would be accepted by the user the user trades anyway because this is according to this leverage still valid and then finally the block Builder can sell back the usdc at the new price basically this is a safe bet for the block Builder because the state update is controlled by the by them but that's not everything there is to it so now we'll talk about an issue for the use for a user transaction but also we don't need to have a transaction to have Mev one new mechanism of tracking value is called loss versus versus rebalancing and it works more or less as follows so let's say that we have an amm on chain and its state is currently well we have a market price what we need is more or less equivalent to 1400 die this could be any market price not necessarily on chain so it could be for example a centralized exchange anywhere and let's say that at the same time an amm is willing to give a 1400 die for one each the same price sometime passes no transactions occur on chain however the market price change unclear why could be some centralized Marketplace that change the price so suddenly there is an Arbitrage opportunity that can be exploited so the amm doesn't know anything about the market price but if you can execute the transaction and you're guaranteed that it's going to be finalized you can take this Arbitrage opportunity and this is more or less the essence of loss versus rebalancing the difference in from different information that the amm and the market has so if the amm had perfect information the IMM would be very happy to use the current market price however this is not the case and this money loss is basically the cost of providing liquidity on an amm currently so important to notice is that also that this is not impermanent loss so it doesn't really depend on how on the relative change in value of the two of Assets in the amm this was by the way part of recent research by using rough Garden so there are related papers eventually I'm going to give you more information but that's it how can we avoid this so let's actually try to build an amm that avoid that captures Meb so we start from any amm let's say uh constant product amm like uni Swap and on top of it we create a mechanism where we can auction the right of executing the first trade on the on the exchange so there is going to be a lead also called lead Searcher which is the highest bidder that is going to take the right to execute the first transaction as we see this is a good way to capture this lvr and the important thing here is that the proceeds don't go too well go to the liquidity providers and this is the Mev that otherwise will be extracted by minus so how can this actually work so to do this we simply we simply say that no user can trade until their lead Searcher has actually traded first so you can see if the list if the researcher actually trades then all the transactions go through otherwise if the block is really organized differently then the trade reverts naturally this requires cooperation with the block Builder so the idea is that the block Builder is aware of the structure of the of the cons of the concept of the amm and orders everything accordingly however uh there is an incentive that is needed and the incentive would be the gas used by transaction if the transaction reverts immediately it's going to use less gas and in particular this would help the miner having an advantage the the block Builder having an advantage on uh MVP on the Mev auction so let's analyze some costs so first profits the profits are estimated to be nine dollars per block how are they estimated so there is the Eden network is a network that works in a fairly similar way so it also auctions off the right to the first lot in a block and this is more or less the the uh more or less the bloody profit that comes from the hidden network which is comparable to our structure the costs are estimated to be three dollars per block which is basically the extra gas cost to enforce the rules that we just decided and not that obvious this is based on 40G wave gas price however it would be much lower so already like with this simple scheme we would have uh theoretically captured Mev of six uh dollars per block so I also want to point out that even if the extracted profit now for the amm is six dollars in principle it could be much higher so in this graph I wanted to show the average block reward comparing flashbots which extracts Mev and other parties which don't so you can see in the last two months it's actually fairly consistently 0.1 each reward more which is actually about more than one hundred dollars per block so this is actually a fairly large amount and it could also be extracted with different mechanisms or the mechanisms of amms okay so in conclusion we have that amms have hidden fees because of Med both for users and for liquidity providers in currently in Current Designs well the these fees go to arbitrary and block builders however we can do better and we can try to design our amms in a way that distributes fees differently and this is what I hope to transmit with this stock so all of this is work in progress is being worked on I would invite you if you're interested to join the discussion and please any opinion is welcome if you are building an amm yes extract all the Mev and after this thank you and thank you again [Applause] [Music] [Music] [Music] thank you [Music] [Music] thank you [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] I'm here to talk to you about a topic that I'm really interested in and I think a lot of people in the security field sort of should be as well and that's Escape patches for Roll-Ups you've heard a lot about Roll-Ups so let's give a brief introduction about what they are and then we'll talk about the specific feature I care about and then we'll talk about some features that those features should have so what a good roll-up might want so this will be sort of similar to my colleague's talk from earlier except not Bridges but roll ups okay so what is a roll-up well if you've you know attended a lot of these talks here at dovecon or elsewhere you know you see a bunch of more or less you know mutually consistent definitions but essentially it's some sort of Layer Two scaling solution or maybe layer three or layer four if you're really Forward Thinking and it's some system that just takes transactions and executes them off chain and then batches them and compresses them and does something to get you some sort of savings and pushes some data onto layer one and so essentially it's a system that's replicating your layer one blockchain like ethereum but it's doing it elsewhere with some nice advantage that gives you you know scalability in in some sense or sales and so how does the system work well usually you get some transactions and then your system design is very dependent on the implementation but typically there's something called a sequencer that gives an order to the transactions that are taken and then this order of ordered set of transactions is executed in some State transition function the state is updated and something gets pushed to your layer one chain where it's you know eventually considered final but uh this is great when everything works but what happens when things don't work in particular if your sequencer goes offline or some other component possibly but typically these Escape patches are discussed in the context of sequence or failures what happens because if you had a bunch of money or assets or tokens locked up in one of these Roll-Ups and suddenly the transaction that says give me my money back is no longer sequenced you're kind of screwed so the functionality of an escape hatch is some way to get program state or digital assets because you know tokens are basically just program State off of this roll-up system even when your sequencer or some other component has failed now obviously there's some sort of caveat some things probably can't fail and always work with an escape hatch right if you have a bug in your smart contract maybe no Escape patch functionality is going to work because maybe that's what's broken but we'll talk about that uh and and some people have actually started to look at this uh this this list thanks uh to l2b but Rewritten through for our paper um they'll suggest that that various roll-up projects right now are thinking about this and uh trying to implement these Solutions now some of them have the none there we're not going to talk about the specifics and I'm also unfortunately not I don't have enough time to talk about even one of these approaches uh but if you are interested let me know or go contact the project and say Hey how do you do this uh but even when they're implemented they're often not exactly done in some cases design is not well understood in some cases the code is literally messed up and it says you know hey it's going to come later um but now that the rest of the roll-up functionality is done you know like ZK parts are all coming together and everyone's got roll up State working it's time to start thinking about this feature that you sort of kept pushing off and you know into the future because ultimately while I hope this feature is never used by any roll-up ever we would like to see it exist so that if someone does have any issues you know you get your funds back and things work out in the user's favor and then you know ideally you fix it and your wallet gets back up or whatever so it's being done but maybe not done all the time or entirely correctly and so I'm going to discuss you know what I think some nice features would be in a roll-up now uh these would vary and just like in the cases for bridges for features some functionality is probably contradictory and you can't necessarily do all of these at the same time but there's some things that probably should be nice and obvious first um you know they should be well engineered components of the system uh they that usually means you know very modular and in case you need to upgrade your roll up you can upgrade part of it or all of it or just this part maybe this is the only part that's concerning because if you saw on that list some people just force a transaction whatever that means others suggest proposing new blocks whatever whatever that means um but you know this should be a full-fledged feature not an afterthought second it should be secure or you know big we always think about security at Quantum and certainly you know a roll-up is essentially a bridge that's even more complicated and we don't want to have to you know deal with exploits that arise for the roll-up because of this functionality if you do this do it right and you know make sure you test it and added it and all that and finally it should be correcting you shouldn't really have to use um consecutive Escape patches this is a bit confusing but I have heard some people say uh well our plan for a roll-up um Escape patch is to just migrate the state to another copy of our roll up and then everything will work but as a user am I going to be you know really trusting of the same type of roll-up that just went down on me I'm not so sure that that's going to be the case so ideally something that gets you sort of out of the system maybe crafting is not the best name uh is something that I'd like to see personally but beyond that we can think a little bit deeper about what it means to escape from a roll head rolling um in particular if you're taking assets out that's pretty well understood and a lot of them do have functionality for forcing transactions for erc20 tokens for example but what do you actually really want to escape maybe you want to escape more than just erc20 tokens uh you know the obvious case is what if you have an nft minted on your roll up obviously you probably want that back too without having to go through extra Hoops uh this also very much depends on the actual bridging that's being done but you know maybe there's some other things you want that are valuable to you they should be built in if you have uh adapt that you have on L1 you should sort of migrate it to L2 and sort of expect minimal extra friction introduced by this feature so in particular if that dap is supposed to escape some some State as part of this functionality it should come for free in air quotes because uh doing extra work sucks and finally it should be transaction efficient on the layer underneath if you escape from a layer or two roll-up and everyone else is trying to do this there's going to be a gas war and things are going to get expensive and things are going to get messy if you can do this in a nice way that Aggregates you know many user state or does something clever I think that's that's really the way to go now I'm not saying I have solutions for these but that's just what I want to see uh some other properties would be really nice be Global uh so in particular um if you call an escape hatch functionality you don't have to do it for every dap that you've deployed on that L2 so if I'm using uniswap and something else on some roll up I want to just say Escape everything not Escape unit Swap and then also Escape whatever the other damp is that would be ideal I'd make my life a lot easier and it should be automatic and live ideally it's always available because who knows when the system could go down hopefully never but you know if it does we don't know when it'll be and it should be triggered hopefully automatically so we're not depending too much on social consensus I mean it might be unavoidable but it should be you know very clear when you can trigger this functionality and when you want to do that it should be available to you or at least the community should be able to make the decision but automatic would be you know ideal so that's what I think they should do have I missed anything please let me know do you disagree please also let me know you can contact me um email or Twitter there find me around here and we do have a write-up of this at um a non-blockchain conference uh so I can send you some papers about that thank you [Applause] foreign [Music] foreign [Music] [Music] [Music] thank you [Music] thank you [Music] foreign [Music] foreign thank you foreign [Music] [Music] [Music] foreign [Music] [Music] I don't need [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] Mana [Music] foreign foreign [Music] [Music] doesn't matter [Music] foreign [Music] are you here with me [Music] where you can't get what you want but you can't get me foreign [Music] [Music] [Music] if we're looking out on the day [Music] if you can't get what you want [Music] me just looking out for the day [Music] [Music] [Music] [Music] foreign [Music] [Music] Million Miles Away your signaling the distance to whom it may concern think I lost my way every time [Music] [Music] can't you see I'm waiting [Music] to remember the days can't you feel it growing stronger [Music] [Music] [Music] [Music] [Music] always [Music] together foreign [Music] [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] keep you keep you in the dark inside foreign foreign [Music] [Music] foreign foreign you have to face future [Music] [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] yeah naked [Music] [Music] Ness [Music] stuff [Music] [Music] yeah yeah [Music] [Applause] foreign [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Music] [Music] what if I wanted to break laugh at all the face what would you do [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] foreign [Music] [Music] flowers [Music] [Music] okay [Music] [Music] foreign [Music] yes I would imagine foreign on the side [Music] I'm just caught up I was a creator [Music] foreign [Music] love life soon [Music] foreign [Music] download fast almost [Music] Terror you know I wish I had a dime for every single time I've gotten stared down for being in the wrong side of town ain't no rich man I'd be if I had that kind of chips lately I wanna smack the miles of these races Gringos rancheros Mexico on your side of that River don't call me Gringo you finger mommy has been a Mr foreign [Music] thank you foreign thank you foreign foreign foreign [Music] [Music] foreign [Music] thank you [Music] foreign foreign [Music] [Music] [Music] [Music] foreign [Music] [Applause] [Music] foreign [Music] foreign [Music] [Music] [Music] foreign foreign [Music] hello hello hope you have a good time on lunch we are going to have a talk from Maxim he's coming he's a software engineer from Aztec and he's going to talk about private value transferring 10 lines this is a session for questions and answers so q a is going to be at the end big round of applause for maxim hello all right hello hello everybody I'm gonna just hello everybody I'm Maxine vesnov and I'm a software engineer slash language designer here at Aztec Network and I'm going to be introducing you to Noir today Noir is our rust-based domain-specific language for writing zero knowledge circuits I'm going to walk you through a basic circuit and then also show you how to go about proving and verifying that circuit in typescript so first I want to talk about what actually makes Noir unique Noir aims to be more flexible in its design than other domain-specific languages that currently exist out there and we do this by rather than compiling to a specific MP complete language we're compiling down to an intermediate representation we call this intermediate representation the abstract circuit intermediate representation or the Acer as I'm going to refer to it throughout this talk this IR can then be compiled down to a rank one constraint system or planckish languages whichever you so choose this is analogous to the llvm IR but rather than converting down the IR to specific instruction set architecture we're compiling down to the constraint system of a given ZK proof construction this actually enables us to essentially decouple the back-end proof system and the language we currently have one fully integrated back-end system and that's Aztecs brettenberg library and we also have plans for Integrations with The Arc Works proving systems such as Marlin and growth 16 but anyone if you can compile it down to whatever you'd like this to our knowledge is the only DSL that currently has fully integrated proving system optimizations so rather than just having an interface for language system optimizations we actually have it on the proving system as well but like where do we actually get from this so we now different use cases require different kinds of systems and now we actually have a language that separates the middle end of front end middle end and back end of a language and developers can then Converge on a common architecture standard the fact that Noir has the ability to support proving system optimizations enables us to have the addition of optimized Black Box functions a few that we have are listed right there but if a back-end system has integrated that specific Black Box function the front end can then access it the ultimate goal here is to lower the barrier to Circuit development by incorporating cryptographic safety into the language itself while maintaining performance so I'm mainly going to be talking about how to prove and verify in typescript this very high level compilation overview also shows our package manager nargo that I specifically want to note that allows for approving and verifying circuits this is very similar to Cargo so you can build your circuits using this but because to actually approve you have to require requires a proof system you see that line on the top going all the way over to the proof system as well in order to compile nargo you need all integrated so now I'm actually going to take you through what a Noir circuit looks like and I'm going to use one of the most common examples where ZK proofs are used to enable private transfers and rather than you know simply showing you the entire circuit at once I'm going to walk through kind of each part of the circuit and this is to highlight noir's unique features it's for and also for those who may be unfamiliar with how to write a private transfer circuit how that may be done if you're already kind of familiar with it you may have deduced from the inputs that the circuit essentially boils down to a Merkel membership proof similar to how noir's rust-based the syntax is rust-like as well but the main thing to note here is the usage of the pub keyword all inputs to the Noir circuit are going to be private by default and that keyword allows you to then say this also has to be supplied to the verifier as well currently anything outputted from the function must also be public and to take note of our native data type that's called the field this is in the Aztec breitenberg back end a field on the BN 254 curve but we also support smaller data types such as u32 u8 you can even have u3 but these are less efficient as they require range constraints and but that's all to the needs of the circuit developer if you want to constrain using our data types rather than directly in the circuit yourself now we can actually get into the logic of the circuit and we're going to start off by using two of our standard Library functions the Schnur fixed space scalar multiplication method so we can construct a private public key from the private key that was supplied to the circuit and then also a Pederson hash function which is going to take in a list of field elements and return a point on the bn254 Curve we first you we use the random input secret that was shown in the previous slide to keep our note commitment private and not associated with any one account once again this is using the Aztec brettenberg back end but if a back end supports these functions then the front end can access it and now we actually have the the meat of the circuit where we're first going to generate a nullifier and this nullifier has to be unique and this is used to prevent double spends and the circuit the contract that integrates the circuit is going to have to make sure make sure of that where we provide the no commitment that we just calculated in the previous in the previous Slide the index of this no commitment in the Merkle tree and then the private key to act as that identity differentiator and then finally we have a check membership function that's going to take in the hash path the index that no commitment and a note root it's going to calculate a note root using the hash path and commitment provided and if it matches the the note root there that we provided from the circuit which is public we then know okay that this that there's no commitment that was calculated within the circuit is actually the person proving it does possess that no commitment and finally we return our nullifier and our recipient this uh is actually constraining these two values with the outputs it's a little syntactic sugar that we have for our return values you could instead for example not have the nullifier be returned as public output you could instill Supply it as public input and you would have to then write constrain like nullify or hash equals nullify or index 0 there but this is happening implicitly when we return it from the circuit right there as the circuit as the title of this the presentation is it's Noir uh private transfer in 10 lines so we don't really show a lot of what the Noir syntax looks like so I wanted to quickly highlight what else also exists in Noir we do have compound data types such as arrays tuples and structs as you can see right there and you can then we also have sub modules so you can easily separate your functions as you as you see fit also just recently added Global consts which you can also import as you would in any other programming language such as rust or control flow we have for loops and if statements and we're looking to add recursion soon as well we also have recently added generics which is highlighted right here which is a great benefit to the developer experience just to recap we try and be high level as we want to make this the development experience easier but also abstract away some of that cryptographic safety that you might be familiar with in other ZK circuit languages or where you have to directly specify essentially every constraint and that's the ultimate goal of Noir to have safety and the a greater development experience now I'm actually going to kind of walk through how you would use our wrapper to prove and verify this circuit in typescript and then ultimately in the browser so that clients can you can prove on the client side we have a Noir JS package which has been created to allow a developer to you can directly compile your circuit in typescript as you see there but in this in this uh package you can also read in from the Acer from file that would be generated by the nargo package manager that's a choice of the developer but in this example we just uh we just show you how to compile it directly in typescript so after we have an object representing our Acer we then just have to specify our ABI and ABI is what the prover is going to be providing to ultimately construct our proof which has our private and public inputs and that's going to be used to generate the witness and ultimately generate the proof so because Noir is back in agnostic though we have a separate wrapper around the Aztec brettenberg back end and that's any other backend that's gets integrated is going to have to do the same thing as well uh because this backend has been made to match the Acer interface in order to set up the prover and verifier we simply pass in the Acer and as you can see right there and then that ABI that we specified earlier to construct our proof and then we just pass it along to our verifier one thing you might note though is that you don't see any public inputs being passed to the verifier even though we have we had that in our circuit currently the proof that gets spit out by brettenberg is pre-pending those as 32 byte hex string inputs but we plan to separate that as developers might like to have those inputs separated for their own development purposes but that's just to take note of why you don't see any public inputs being passed to the verifier there so we can this it's cool that we can actually verify in typescript but we want to be able to also enable this in smart contracts as well so Aztec's brettenberg backend does allow you to compile from a Noir program to an ethereum contract improving systems looking to integrate Noir are going to have to provide their own implementation of how to generate a solidity verifier or if you choose to use any other smart contract platform you're going to have to provide that implementation yourself if you're integrating a back end this script here shows you how to generate a solidity verifier using that Aztec brettenberg back end and it's important to take note though if you change your circuit and don't change your you don't regenerate your solidity verifier with this script your circuit is ultimately going to fail as you're going to have a new Acer and you're going to have a you're going to need to regenerate that verifier in order to have accurate verification and finally we can actually have our ta-da moment and verify our circuit and solidity so using your favorite deployment method you can deploy your turbo verifier as you would any other smart contract and then you can take that proof that was spit out from brettenberg in the exact same way with the pre with the public inputs prepended pass it to the verifier and if your public inputs are correct and your specified your circuit correctly it should it should ultimately return true in the case of our private transfer we're going to be checking is that nullifier hash been used before is the root uh the correct route and upon having a verified proof we can actually then perform a private a withdrawal and that that note that will be entirely private so that's that's it there and you can see a more full example with the uh simple Shield but ultimately here the benefit is that we can enable autonomous execution in contracts based off verification of Noir circuits and I want to also discuss some of the future work that we're going to be doing in or one of the main things that we want to add is verify proof this will actually enable us to have recursive snarks inside of Noir itself which enables many more use cases than you can currently have uh the wrapper itself is pretty new and we want to improve this development signifi these development tooling significantly to make it easier for anyone looking to integrate with Noir we also want to integrate it with Rebels such as DK Rebel if you're familiar with that online IDE Integrations other debugging tools a language server lots of development tools to add and in the medium to long term we're going to be adding Noir contracts and no more contracts are going to be the next stage of Aztec's platform where we'll actually have public and private state which is has its own challenges which you can attend Mike Connors talk at 2 30 from Aztec who's going to be discussing that infrastructure and how we actually are going to enable Smart contracts in Noir if you have any questions uh please shoot and thank you [Applause] thank you so much Maxim so we have two volunteers there you go is our balloon two coming hey good afternoon uh very impressive um right now how does the how does your tooling work for this uh you have to compile the Noir contract and then deploy it into into the layer 2 into the layer 2 or the layer 2 test net or something not right now that's that's the next iteration of Aztec's platform when we add the Noir contracts this is a purely a zkevm DSL right now where Aztec can generate that solidity verifier and you're going to be proving circuits on a evm platform you can you can gener like I showed in that example you can compile your circuit in typescript but you can also use our package manager which I briefly mentioned to generate that intermediate representation that intermediate representation can be used the same way that that that it was used there you just have to read it in you're reading it in from file and using our same our wrapper method so you don't have to do anything extra for serialization that's handled that's handled by our wrapper yeah right now but you can also you know verif you can verify without the evm as well you can just verify using typescript and you don't have to necessarily have a verification using a solidity contract that depends on your needs but a lot of people like to have that autonomous execution we have time for more questions one over there in the left side then we have two more here hey hi uh it was amazing uh I wanted to ask let's say as a developer I have the choice between sarcom and Noir now uh both as an end user feature provide proof systems with Planck and growth 16. it's exactly the same value proposition so why would you say that ah we should experiment with Nair instead of just going with sarcom sorry to put you in a position no no it's okay uh they do serve slightly different use cases circum is definitely a much more low level you're gonna have to be specifying a lot of the constraints individually while in the language we specify those constraints for you a basic example there when I talked about if statements you can constrain your your on if statements you don't have to use a multiplexer and things like that that make it just easier for the developer and kind of lower that barrier from someone who's maybe moving from who's never coded ZK circuits before and you can also constrain on data types outside of just our native type because they everything that we have ultimately translates to that and we do have uh I know you can currently do Planck but I believe right now Noir is the only language that actually has truly fully integrated Black Box functionality where the proving system is specifying the custom Gates and making those functions faster hi um why base Noir off rust are there certain properties of rust that make it you know a better language for this well we want to the one of the main focuses of noir's safety and Noah and rust that same thing in Russ right and rust is also just a nice language clear it up thanks a lot for for the talk uh I was wondering do you have some tools where am I looking sorry I'm here oh hey hello thanks for the for the talk uh do you have some tools for uh testing the logic of the circuits uh right now it's essentially you're gonna have to write your tests in typescript we hope to add debugging tools that's one of the main things that need to be added to make the tooling more effective uh yeah you're gonna have to essentially test it either in typescript you can test it in Rust too and use the nargo package manager that's up to you but I only highlighted typescript here because most contract developers are probably going to lean towards that okay thanks we still have time for more questions five minutes more there you go on the right thank you uh just a quick question on the first uh like code sample you had uh I could see the STD the STD package was under a depth namespace and I was wondering where that is that's just our I guess native kind of dependencies and that's how we do it like we you can also import from from GitHub or you can have modules as you would normally in Rust within your uh where it would be like mod something and you can also import packages from within a crate as well where it would be like create this uh and that would be the import that's just how we import our standard Library yeah like rust crates uh our our nargo kind of it it's not like a copy of cargo but it kind of mocks that functionality with crates and modules so you can have you can import modules from within your crate using use crate and then whatever else you'd like it's a very nice talk uh in the next phase of of Aztec 3. can you talk about the process a process of converting existing circuits to Noir what do you mean existing circuits you mean contracts like the uh so in Aztec 3 you're planning on converting existing circuits to Noir I assume so that's actually probably you'd want to go to Mike's talk to see more it's very interesting how the infrastructure works essentially though each function in the contract is going to be a circuit but how that actually gets integrated into the system you're going to have to attend Mike's talk to get more info on that it's very interesting foreign we have someone from Team here as well you want to complement anything like to add on top of the conversation hello I just want to look so with Aztec free we're not actually trying to be ZK evm compatible right it's a separate no right yeah just making sure cool there's one more over there this side oh yeah that was my question again um that was he said he they made a preamble to the question so that means that Noir the since Noir is going to Target to EVMS uh Arctic 3 is not going to be using Noir for this for for its inter for the circuits so we're going to be using Noir for circuits but you don't necessarily have to Target the evm there we just Aztec's backhand does allow you to compile an evm verifier from a Noir circuit and that functionality will still exist but no R contracts and Aztecs platform are not going to be zkbm they're gonna they're gonna be Noir so yeah so that means that you also have the your go you right now have the target for Aztec or you are going to have or you're going to build it yet sorry can you say it again if the compilation Target right now it's only ebm but you are planning to to be able to migrate to to be able to compile as well to Aztec in the future yes yeah so currently Noir is just a language for creating circuits um and you can create a solidity verifier that allows you to verify those proofs uh on the on the ethereum uh once we extend extend the syntax to have a contract terminology uh Aztec free will be built and it will have its own VM and you can compile those contracts to the Aztec free VM not the ZK evm no I wait hi uh well I am now in nor right now it's so new for me and I would like to know whether the main applications that they are using now right now the main applications using Noir right now yeah uh well there hasn't been many uh mostly the examples you can find on my GitHub or inside the Noir repo testing the language um but there's anything you can think of where you would need to verify a proof and do something based off that ZK proof you could theoretically do it Noir so that can be quite a lot of things synonymous proof of membership uh private games on on public blockchains lots of different things all right big round of applause to Maxim from Aztec [Music] all right [Music] foreign foreign foreign foreign foreign [Music] foreign foreign [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] thank you thank you [Music] [Music] foreign [Music] [Music] thank you [Music] all right so we have azus from sorry I don't forgot you can introduce yourself the very the what is the prayer you are represented but common stage big round of applause please your knowledge airdrop in which you enable Anonymous governance where people's votes aren't impacted by other people's votes uh where people can vote independently and they're only revealed once you want to tally the votes or once the voting period is over in order to do this to do a zero knowledge airdrop you need a way to avoid double claiming that is people can anonymously claim their tokens to an unlinked account but still there's some identifier that represents them that prevents them from claiming the same airdrop twice or maybe we want to do an anonymous Message Board in which we know uh that each of the people cannot be identified but we know for instance that maybe the first poster is the same as the third poster or imagine we want to do Anonymous voting within a large anonymity set such that the very first person to vote has the full power of every single person in this anonymity set or really any civil resistant Anonymous application so we thought about what we could use to be this kind of tool that stops us from double spending and is kind of unique identifier of someone on chain and after talking a bunch on Discord we decided to create a new Zeke nullifier scheme on ecdsa this is Joint work with Persona Labs geometry research and originally done at Xerox Park in MIT foreign so to kind of clarify what exactly this nullifier is let's try to maybe go through some properties so if you want to avoid people from double spending and we want each person to claim only once then we need this property this this identifier outputted by this account to be unique that is everyone should have a different one we also want this value to be deterministic that is if I have some ethereum public private key pair I should only be able to generate one of these signals finally the signal I generate should be verifiable without access to my secret key this is because in a ledger wallet or in a secure Enclave it's not possible to even read the secret key or take it out of the secure Enclave we also don't want users copy pasting their private Keys all over the place and finally we want this number this scheme to be completely non-interactive if you've heard of nullifiers before you might have heard of something like tornado cache or semaphore and the way that works is that as people sign up the anonymity set goes bigger and bigger and bigger however the first person to use tomato cache or the first person to sign up for a semaphore group has zero anonymity until more people join but if we want the full power of the entire anonymity set for the very first person we need the scheme to be non-interactive that is there shouldn't be two phases like a sign up phase and a utility phase so to understand these properties a little bit more one useful exercise is to go through some examples of schemes that may work or may not work so imagine when we wanted someone to post this deterministic value that identified themselves that stopped them from double claiming or stop them from double voting that they use the hash of a deterministic ecdsa signature now you might guess that this has all the properties we want it's Unique if you're assigning a message a deterministic signature there should only be one um it's it's non-interactive anyone could generate a signature and blend into the anonymity set you don't need people to to interact and sign up for the scheme first however the issue of deterministic ecdsa signatures is that you cannot verify that it deterministic ecdsa signature was generated with the correct deterministic Randomness unless you have access to the private key that is deterministic ecdc signatures are identical to non-deterministic ecdsa signatures because the randomness is like the hash of the private key and so if you're trying to verify this on chain or in a ZK snark you would need the private key so it doesn't quite achieve what we want it to achieve a second idea is this idea called a verifiable unpredictable function or a unique signature now the vast majority of the schemes for these are on pairing friendly curves and ethereum which is on ecgsa on SEC B 256 K1 is not a pairing friendly curve so the vast majority of this literature is fairly inaccessible to us we could try something like hash of message come a public key you'd imagine this is unique every account every public key has a different value of this hash it's deterministic if you have a public key and there's some message say the app ID then this hash is deterministic it's verifiable without your private key and it's not interactive you can give this hash without signing up in advance now the issue with this is that it's a little bit subtle but it's actually possible to de-anonymize people who are using this for anonymity the way you do that is you can take the list of all 200 400 million private keys on ethereum calculate the hash values for every single one and match it up with this nullifier value on chain and boom you've deanonymize a person who tried to use this as their unique identifier maybe kind of getting inspired by this we can try something like hash of message comma secret key you'd imagine this is similar to the the public key case except you can't take everyone's secret key on chain because you don't know everyone's secret key so it's not possible to anonymize people in the same way but the issue is to verify something that's behind a hash you have to calculate the entire hash function inside the zero knowledge proof or inside whatever and any primitive you're using and we don't want to use this copy around the secret key and so this scheme isn't quite it so finally we resolved on this scheme we use a hash the message from a public key to the power of secret key it's kind of similar to the other schemes but it turns out it has this nice property that's a ddh vrf or a decisional diffie-hellman verifiable random function and specifically what this means is that we can verify that this is actually calculated correctly without access to the secret key so to maybe summarize like what exactly the properties we are that we want so far is that we want this deterministic function of a user's secret key that can be verified with only their public key and keeps them anonymous and the way we do this is we Define a new deterministic signature scheme on ethereum so I'm going to try to go through what exactly the signature scheme is and so this is going to get a little bit messy and I've tried to slightly color code the numbers that are the same so you can kind of correlate them across and although the math is a little bit difficult to understand in first glance I'll try to give you an intuition for why it achieves the properties that we wanted to achieve so remember our nullifier was the hash of message come a public key to the power of secret key and so this value will be public it will be calculated inside the user's wallet whether it's their metamask or their Ledger or anything else and this value will stay public the entire time it will be posted on chain eventually for everyone to see these private outputs will be created by the wallet and given to the user these outputs if they were seen by the external world could de-anonymize a user for instance if you saw their public key you would obviously know who they are um and so the idea is that these signals are kept to the user and the user can use them to generate a ZK snark so what the user does is once they have these signals you can think of this as splitting the signature scheme into a deterministic part the nullifier and the non-deterministic part the private part the verifier simply checks these values are calculated correctly now it's a little bit hard to drive through all this math so I'm going to try to give like a high level overview of why we have this kind of C value floating around now the reason we have this like C value floating around is that when you're taking a hash it's impossible to Brute Force the output of the hash so you must know all the inputs that went into the hash this means that the person you can kind of think of this C commitment as a hash commitment to these values of G to the r to our nullifier to our public key then we calculate this value which is s which is based on the C value which we know must have been calculated after c um and notice that this s value does not reveal the user's secret key although it has secret key in it it's blinded by a completely random number the check in the snark you'll also notice has no secret keys in it we only pass in these signals none of which you can use to derive the user's original secret key and the check that we do is effectively did the user calculate C in the correct order did they commit to their secret key their public key and calculate the signature in a way that would have actually proved that they owned that secret key and finally the ZK snark apart from checking that the nullifier was calculated correctly in a German succession um on the correct curve we also check whatever public key check we want to do this allows us to verify that the public key is in fact acting correctly and has some unique identifier but now I have to verify that the public key is in fact allowed to interact with this protocol maybe they're a numerical proof maybe they submit a Merkel try Patricia proof that they are they satisfy some condition on chain and the idea that we would also lump this in with the verifier check in the ZK snark so to summarize the wallet generates these signals the user plugs them into a ZK snark and posts the ZK snark with the public nullifier output on chain proving that they are allowed to interact with this protocol and a unique user who is still going to be kept anonymous now interestingly when we were doing this work we discovered this Quantum secrecy interactivity trade-off now it turns out that semaphore is extremely Quantum secure because it uses hashes but it's not it is interactive meaning that you have two phases and the first people interact they interact with the protocol have no anonymity our scheme on the other hand is fully non-interactive the very first person to use a nullifier will have the full anonymity of the entire anonymity set however it is not Quantum secure and the way that you can kind of think about Quantum adversaries in the situation is that a Quantum adversary can effectively take discrete log and prime factorizations but it can't undo hashes you can imagine that when we have a quantum computer with 2000 signal qubits that can break all of ethereum's uh public keys and derive their private keys in the actual situation of the chain continuing this is completely fine users can sign their new key with their old key before Quantum Supremacy occurs and the entire chain can move seamlessly to a new Quantum resistance system however if you can derive all the user secret Keys then if you remember the trick we did in the very beginning where we calculate the half of meshes come a public key for all the users and de-anonymize people we can do the exact same thing with this Quantum adversary calculated secret key Quantum adversary simply calculates every secret key for every private for every public key on ethereum calculates another Fires for every single one of these keys and matches them up with the anonymous signal and so interestingly any future Quantum adversary can break past anonymity um and the issue is that actually any deterministic nullifier scheme that has the properties that I defined in the beginning has its vulnerability because if you Source Randomness from only the secret key that means the quantum adversary can derive it as well and so you have to Source Randomness beyond the secret key which requires the user to remember that value which means you need to have a password so to kind of get a better intuition for like whether this actually matters or not there is the the predictions of quantum researchers very extremely widely some researchers have mentioned they think it's going to come as soon as 30 years signal qubits that can break ecdsa some researchers think somewhere around 100 years and some think that because of the noise problem it's theoretically impossible and it will never happen um it's a little unclear as of right now we have about 20 single qubits and we need over 2000 to break ecdsa so I've written a short mental model that you can read once you look at the link on the slides which describes kind of how I think about this Quantum mental model of how these systems are going to evolve and whether or not we should be worried about it but I think for now the decision that we made is that it makes more sense to not be worried about Quantum oracles because as long as the protocols are transparent that you will be genomized in something between 30 and 100 years the idea is that most people probably won't care because by that time for instance even governments declassify their secret documents in this kind of time frame so to understand more how this another five scheme is going and if you can use it yourself we have a paper in which we've proved all the security properties of this nullifier scheme um we have a blog post that's going to be coming up on the Persona Labs blog soon and we also have a proof of concept uh in Rust and in a metamask map finished um the next step is to put it into a ton of other wallets into Ledger into burner wallets into metamask core create an ERC standard for people to use this scheme so we can have unique anonymity across the board and to potentially ride an appendix to an ITF RFC to make it a complete standard across all of cryptography if you're interested in seeing more about the code this link will link you to our GitHub repo where we have all the open source code we'll be linking to all the slides here as well along with a Quantum mental model um and our Twitter if you want to learn more um you guys are interested in this uh want to use the scheme or are curious about uh student I'd be happy to talk and it would be really fun to see where the scheme can go if anyone has any questions about understanding the scheme or thoughts on where it can or can't be used I'd be happy to take any questions for the next five or ten minutes cool test okay how is this scheme called so right now the scheme is called just a ZK nullifier because we think this is the kind of most straightforward way to describe what is going on but I'm open to any other name suggestions I think uh there's a question in the front foreign probably maybe using an example how uh like this non-interactive uh is like is like seen in practice like in terms of like uh using the scheme like maybe like an airdrop like can you explain like how that system would work how like user would interact and like kind of maintain that property yeah that's a great question I can maybe go through the kind of the whole airdrop scheme and try to describe why it wouldn't be practical with something like that before but why it could be practical with something like the nullifier so if you imagine the semaphore scheme for the airdrop the way it would look is that people would first have to sign up to receive the airdrop and this is so people have to submit their semaphore commitment to some uh either like an on-chain directory or an off chain list and the issue with this is that people can end stable attack at this airdrop very easily because they know that an Arab is going to be coming and this is a way to gain eligibility for it and so because they have this two-step process people can spam whatever mechanism it is that they're using to determine who is legible and who is not resulting as being a somewhat impractical solution you can also imagine that if you didn't have a two-phase process where instead people could sign up for December Four Keys whenever and claim whenever and the very first person to claim the airdrop but sign up for this time before key you would see only one summer for key in the list of all summer Four Keys and then that person would clean their airdrop and she would know that the person who posted that proof must have been the person who claimed the airdrop and so there's all the small anonymity set is is not ideal if you're trying to maintain perfect anonymity for everyone in the set the way they know the fire scheme would work is that when you claim the airdrop you prove that you have another fire for a message like I am claiming the ZK airdrop and the verifier would prove that you in fact signed that message that you created your deterministic nullifier correctly and would allow you to claim the airdrop and then add that nullifier to a block list from ever claiming again and so what would happen is the very first person to claim no one would know who they are out of the entire set of all the keys that are legible for the airdrop do you think that sufficiently answered the question where does the like nft eventually end like it's an ethereum address versus uh like is it like the things are happening on chain or it's like an off chain process or like uh just trying to understand that yeah that's a great question uh for those of you at the beginning the question was basically how is this EK airdrop actually implemented and the idea is that what you would do is you would prove that you own an account that is in the Merkle tree of eligible addresses for the SDK airdrop you would generate a summer for another fire for that scheme uh in the original case and then you would claim it to a completely unlinked unused account that has never had any transaction on ethereum um and then all the fire scheme you do a similar thing it would prove you have an account in this Merkel tree you would prove ownership of that account you would prove the nullifier via the server makes another fire scheme and similarly the tokens would go to a completely new fresh ethereum account and there's no history on it and it's effectively anonymous that's a great question though yeah uh so just to expand on the previous question like a concrete example so you wanted to like airdrop an NF an nft to anyone who has a crypto Punk and but you want people to be able to claim it without revealing which Punk they are um within semaphore or anything that's like a two-step your anonymity set is only going to be the amount of people who like sign up for the semaphore and with this the anonymity even if it's just two people that end up claiming this nft they have the full anonymity set of like all Punk all Punk holders if that makes sense and it's going to like a new address not to the one that has the punk yeah it's a great collaboration on the engine turned on hello all right uh so I'm wondering uh can you um expand a bit upon if you if you use this scheme for a sort of a multi-step protocol where it's not just maybe a tornado cache deposit and withdrawal or a claim scheme but you continue where you maybe have some sort of transfer of private assets within the anonymity set do you keep that anonymity and and how does that work yeah so the the way dark pool transfers would kind of work with its anonymity with this nullifier is that the way most people enter a dark pose they send their assets to like a specific smart contract and so you already know that the engine reset is limited to everyone who has ever interacted with a smart contract and one thing you could do is you could use something like semaphore and if you interact with the smart contract the guarantee that you have to sign up for a semaphore key so because the anime set is already limited by something like an on-chain interaction the nullifier wouldn't add additional anonymity because it's already limited by the chain interactions however what it could do is prevent you from having to memorize secret notes so for instance with tornado cache you have to download a string and remember it forever um and if you lose that string then you lose your money but what you can do with this scheme is that instead of having to memorize a Toyota cache note you just remember the amount of money you got and then you could say okay I got one eighth so I'm going to sign a message saying here to cash one eth and I know that the nullify that it creates will be a valid note for my deposit and the contract can simply validate that and so it can help uh saving memory or losing strings for anonymous money but it wouldn't add significantly additional anonymity if the original scheme was implemented uh with the dark pool correctly ascending to the contract yeah that's a great question though hey hi uh okay uh I'm pretty sure like many of us would have read about nullifiers for the first time in vitalik's article the one with like privacy applications now and like I did not understand it completely but I thought it was vitalik who wrote it so it must be true so so can you like just uh give a brief difference between that nullifier technique and your nullified technique for us to compare right uh so the another fire technique that is classically referred to in mostly ZK snark posts um is the nullify technique that exists which is uh kind of why Halo 2 defined a number of years ago and it's kind of evolved into being this semaphore type scheme in which another fire just represents that you can't double claim or double interact with a specific protocol because another fire nullifies their ability to do so that is a good added to a Smart contract or something and then when you interact with that in the future it would notice that you've already interact with it before and then stop the transaction from going through um so the scheme that likely is being referred to that in that post is likely a semaphore type scheme or a sign up type scheme in which the solidifier is like a hash or some string which again would have to be interactively generated um and so when I when I say this kind of semaphore side or this like interactive side that's larger than the classical nullify that I'm referring to yep that's a great question foreign thank you for the talk um I had a general question um kind of about coming back to the example of the token gated airdrop so if you either use like the Zeke and Oliver or semaphore to generate a proof um that an address is eligible for an address based on um owning an nft in a set for example how does that respond to updates for example when you transfer the nft afterwards since the nullifier is only tied to the address could you then chance for that nft to different address and create another another file and again qualify for their job yes that's a great question so the way that you would solve this kind of multiple qualifying for the airdrop problem is that when you define when you release this airdrop you define like a strict set of keys that are eligible for this airdrop it can be say like people who owned a certain nft at a certain Block in which case there's a finite number of addresses that can claim that or it can be so anything you could prove in the ethereum storage uh Merkel producer try or it could be like it's just a list of addresses in a miracle tree like a standard airdrop is done so the idea is that even the the reason that nullifiers uses it would only suffice for this claiming step because once you've claimed the nullifier to an account and you've claimed this nft or airdrop or whatever to this account that account can do whatever it wants with that information now it's it's come from a completely anonymous source and so it can it can go wherever and it'll continue to maintain the anonymity so the you can continue to transact that nft or eoc20 but you just wouldn't be interacting with the original airdrop contract ever again so you wouldn't need a nullifier in the future awesome yeah okay good so this is probably a silly question when people redeem it right they have to provide some gas so the guests must be coming from some account so even even though we have all these anonymity is probably hide the the address that's going to receive in it but when people provide gas this transaction actually links the account that's receiving nnp versus the account having the gas so it's still revealing like some information yeah that's a great question the question was basically how do you pay for the gas um without being anonymizing yourself and there's kind of two main ways to do this one is uh the airdrop contract itself will pay for the gas the way this would work is when you claim uh the airdrop contract would record the amount of gas being used in the transaction there would be a certain Bank inside the contract to pay for that gas plus a little bit extra and so the idea is that you could send your claim transaction on chain uh or uh to a private mempool and Mev Searcher would actually be incentivized to claim this transaction act as your relayer and decentralized manner because they're getting compensated for it because it contract is calculating and reimbursing them more than they would have paid for the gas that's one way to do it second way to do it with something like account abstraction where you can again have a third party pay for the gas usually out of the contract this can happen with say the contract doing an atomic swap with say the token that it's giving you and each to pay for the gas um similarly reimbursing the the front running relayer effectively um or you could send it to like uh something like keeper Network which would incentivize which is basically a place where Mev Searchers can post or people can post maybe ideas and maybe Searchers can take them you could use a mempool or you could use a kind of a private centralized or decentralized method to do that yeah that's a good transaction that's a great question though thank you so much bigger Plus [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] foreign foreign [Music] foreign [Music] [Music] ER from Aztec he's going to talk about public private composability pick a round of applause for Mike whoo come on excitement thank you uh okay yeah I'm Mike uh and I'm going to talk to you about public private composability and what that really means is I'm going to talk about Aztecs private smart contract architecture so where we spec it all out and we're going to be spending the next year maybe more uh building a general private smart contract platform for ethereum uh so what were our aims when designing this private smart contract platform well firstly private States and what private states are are values known to only one person so a private state has an owner and no one else in the world knows the value of that state okay and then private function execution and this is really cool this is the ability to execute a function without revealing to anyone in the world which function you've just executed so similar to like the sexy protocol and we want people to have a similar experience to ethereum so we want people to be able to deploy contracts permissionlessly so just any time you want to deploy a contract to the system you can just deploy it and again similarly to ethereum experience we want composability so we want someone to be able to call a function and that function might make a call to another function of another contract and that function might make another call to another function of a different contract and so on so that's really important and we want a different kind of composability as well we want composability between private and public functions which I'll talk about more as we get into it um and all together we just want intuitive transaction semantics so we want we don't want a developer to have to think about the underlying cryptography or any of that we just want them to have as close to a Smart contract experience that they have with ethereum okay so I'll take a step back what fundamentally is a smart contract well at its core it's just a collection of State variables and a collection of functions which may edit those State variables and here we can see a naughty example of an erc20 contract balances is a state variable and there's a function called transfer which has rules which allow people to edit the balances state and there are two to the 160 such balances States in this example but what would an Aztec smart contract look like well people will still be able to write functions and states in for layer one so for ethereum they'll also similarly to all the other Roll-Ups and existence the ZK Roll-Ups and the optimistic Roll-Ups they'll be able to deploy functions and have persistent State on a layer 2 just to save on costs but the key differentiator with Aztec contracts is there will also be private functions and private States and crucially you can see here these will be on a user's device rather than executed by ethereum node or roll-up sequencer okay so what do I mean my private State let's look into it a bit more get everyone up to speed on some background so a public State it's just a value in a Merkel tree it's the same in ethereum it'll be the same on our Layer Two what's a private State well it's just a commitment to a value in a tree and it has an owner so here we can see an example of a commitment it's got an owner public key it's got some value that you want to hide and store persistently in our layer 2 storage and it's got assault which is something random to give the commitment hiding properties and maybe some other stuff depending on the application that someone deploys foreign public States will follow an account model just like States and ethereum smart contracts and with the account model a state lives at one position in a Merkle tree one leaf and we'll occupy that leaf forever so if someone ever wants to edit that state the person executing the contract will have to go to that exactly position modify the value but that's no good with private States because if you have to tell the whole world I'm editing this particular Leaf of this tree please update your node accordingly you've revealed which state is being edited so that doesn't work which is why we adopt a utxo model and in a utxo model once you add one of these commitments one of these hashes of a private value to the tree it never changes if you ever want to edit that state similar to the zcash model you have to nullify that state which allows you to change it or prevent it from being used again without revealing and you have to add a new commitment to the right hand side of the tree so you're always appending new values uh so if someone comes along and they want to edit your state they will create a new commitment representing maybe the add end that they want to add to the state or something like that and just to continue with the background and carry on with this Z cash-like protocol description or indeed our current Aztec connect protocol description uh let's say someone inputs a commitment into their circuit and they want to modify the value contained within that commitment they pass it into the circuit the first thing they would do is prove that this commitment does indeed exist somewhere in the Merkel tree of notes and in Aztec connect the miracle tree is 2 to the 32 with this private smart contract platform it might be quite quite a lot bigger because apps will probably create a lot more commitments okay the next thing you do is you open the commitment and you would hash the values to prove that you indeed know the pre-image of the commitment and Alice in this example is the owner of this state and so for her to prove that she is indeed allowed to modify the state she has to prove knowledge of her secret key and she can do that by just hashing it however or following however the secret key has arrived and then she produces another fire in the previous talk was quite helpful because it explained nullifies so I don't have to um and then they prove it doesn't yet exist in the nullifier tree and the nullify tree at the moment is a sparse uh Merkle tree it allows you to prove non-membership um but we're actually going to update this to a linked list tree which allows you to prove much more efficiently that something doesn't exist in the tree and suppose Alice wants to perform some simple operation on a state she the circuit she'll add five to it and you can see we've incremented the value that Alice is hiding we've changed the salt to re-randomize the commitment and she adds it into the tree so this is quite a general way of just modifying state within a public protocol and crucially this step has to happen on a user's device or at least in the most basic systems this has to happen on the user's device you can do some delegating schemes but we won't talk about that here and so this gives our rollup something different from other Roll-Ups there is a layer where if someone wants to modify a private State they have to do it on their own device before then passing it to some sequencer who will deal with all the public State changes and indeed because it has to happen on a user's device possibly in the browser we need extremely efficient proving systems which you know we've been working on and we're continuing to improve so on an L2 functions are slightly different from a solidity function they're actually ZK snark circuits so similar to other ZK Roll-Ups that you might have read about except our role that will need to be actual ZK so provide hiding properties so if someone begins with a circuit suppose they write a circuit in some circuit language like Noir um they write the rules of their function you know it might include State changes computations stuff like that and using something like Noir they can take that circuit and compute a proven key in a verification key and a user will come along and they want to execute this function or rather prove that they have executed the function correctly and that function will take some secret inputs and some public inputs and they'll pass it into Noir a big black box and it will generate a proof out will come the proof and you can send the proof and the public inputs to some verifier who already has the verification key and they can verify that you did indeed execute the function correctly for the given set of public inputs one thing I want to point out in this slide and that we'll need going forward is that the verification key can be used as a unique identifier for the circuit because it was derived from the circuit and the hash of the verification key is like a unique single number that can completely represent the circuit and yeah this this can all be handled by Noir so if a verification key can represent a circuit and if a smart contract is a collection of functions then we can create uh well we can represent our set of functions as just a set of verification keys and if we want to prove one of our objectives that we have executed a function without revealing which function we have actually executed we can just miracleize all our verification Keys into a big tree and we can add our contract or our set of functions to a giant contract tree which contains all functions that have ever been deployed to this layer 2 smart contract platform and in doing so if this computation a Merkel membership check is done within a circuit you can hide which function someone has executed oops uh so it's actually already came up with a system which was really good a few years ago uh it's a good paper if you haven't read it uh for hiding which function has been executed and this hash here represents a commitment that someone might add to a tree that we talked about earlier and within the commitment they had a nice idea that you would put in a birth predicate I.E details of the function which created this commitment and a death Predator a function which the only function which may nullify this commitment but if people are familiar with solidity smart contract development it's not quite good enough to just have one function a modified State you might want multiple functions to be able to modify a state which is why in the previous slide we've kind of collected all possible functions of the smart contract and added them to a contract tree and so instead what we do in our smart contract spec is we include a contract address and that represents just the set of all functions which may edit this particular state and again this is how zexy proposed to hide the function being called so that no one knows really what's happening in the system you know all observers see is that some function got executed According to some set of rules for some smart contract uh you take the private circuit and generate a proof and if we stop there the world would see which function got executed so we passed the proof into this kernel circuit and the kernel circuit verifies the earlier proof and kind of masks from the world which function got executed which is really cool and we take this and we take it a step further which I'll talk about in a second I'll talk about it here one of the things we said one of our aims is the ability for functions to make calls to other functions and so on and so on and so on and so on so how are we going to do that well first why would we want to do that here's a really simple reason if you consider a decentralized Exchange just on layer one at the moment if Alice wants to swap tokens A and B she must first call The Exchange contract and then the exchange contract has to call the erc20 contract for token a and then she and then the exchange contract has to call the ESC 20 contract for token B so the most basic kind of applications on ethereum require nested function calls and we want to mimic that with our private programming uh smart contract language what does a function call look like well in ethereum it's a contract address some identifier for where people can locate the function and a set of arguments and so similarly for Aztec it will be the contract address the VK index as we saw in the previous Slide the verification Keys all get added to some contract tree and a hash of the public inputs to the Circuit which represents a function so let's work through an example and you don't need to read the code for this or you need to really look at is the colors so we've got to imagine someone calls the red function over there function1 and the red function is making call to an orange function in a different contract and in turn that orange function is calling a blue function in a different contract it's returning some values and then it's calling a green function in this green function is a public function so what I'm going to try and demonstrate is how functions can core functions of the smart contracts and can also you can have functions from private world to the public world well what we first have to do is each of those individual functions is an individual circuit and so you would have to generate six distinct proofs one for each function that you want to prove you have executed correctly but then the question is how do we prove that they relate to each other in a way that the code wants us them to relate so how do we prove that they were executed in order and the parameters that were sent to functions and the return values all intertwined correctly well we we use call Stacks so say we want to execute function one first well the public inputs to every single function on our smart contract platform will have to adhere to a very rigid static API and one of the fields in that API will be call Stacks so because function one makes a call to the orange and yellow functions this circuit will expose through its private inputs the call arguments and the function signatures of the orange and the yellow functions and similarly the orange function is making a call to the green function so oh and the blue function so it will also expose calls to the blue function of the green function in its public inputs and notice the green function is labeled as public and so because public functions get executed by the sequencer and private functions get executed by the user they have to be separated in this way and on we go the green function is making a call to the purple function so we add that to the call stack and here's the really meaty cool bit and that's kind of extending the the sexy example we saw before where if you pass a function into a kernel circuit it hides which function function has been executed we add some external uh logic to our kernel circuit to allow these call Stacks to be interpreted and to check that the parameters being passed between all these functions makes sense and are according to the rules of the smart contract that somebody wrote or smart contracts plural that some people wrote so we begin with Function One we pass it into the private kernel circuit and the private kernel circuit will read the public inputs of function one and push onto its own call stack the two items that function one makes a call to um and then we will carry on with our recursion the private kernel circuit will spit out of proof and we will pop the next call from the call stack and we'll pass it in to another iteration of the private kernel circuit so each of these arrows is a proof and within the private kernel circuit two proofs are being verified each time and so we can see the cool stack has shrunk a bit and then we pop call number two uh the orange call off it and pass that into the private kernel circuit and we do recurse until the call Stacks are empty and then we know we're done except the orange function is making calls to another two so we'll just keep going until okay finally the private call stack is empty and therein ends the work that needs to happen on a user's device so because the user only executes private functions public functions are executed by the roller provider this proof here will be sent to the roller provider what will the parole a provider see well they'll just see that somebody executed some function of some smart contract and changed some kind of States but the role provider will have no idea which um which is really really cool what they will see if the private function is making a call to a public function they will be able to see the public functions which need to be called there's no way around that so seeing the cool stack as it is the role a provider will take this function and execute it and pass it into a public kernel circuit which has instead of hiding logic it just executes States out in the open similar to I guess other ZK roll-ups and this function's making a call and we're almost done we pass that into the kernel circuit and hooray the call Stacks are empty we finished executing our function trace or our stack trace and this can then be sent to the roll-up circuit this entire thing represents a single user transaction on our smart contract platform and it can get rolled up with all the other transactions uh you don't need to read this all this is saying is that the in order for the kernel circuit yeah in order for the kernel circuit to be able to operate on any kind of function we need one giant static interface which it accommodates as many possible function permutations as possible and that's what these interfaces are there's one for private circuits and you can see there's a lot of arrays to allow for various sizes and various permutations of stuff that an app might wish to do or a function of an app might wish to do and we've got one for public circuits similarly the private inputs and public inputs to the kernel circuit there's loads of them and there's a lot happening um there is so much computation happening in these kernel circuits that we've been working on a better proof system more efficient proof system for ZK snarks and it's actually similar to hyperplunk which espresso systems have been working on um it's really cool idea um and I guess there'll be more on that at some point in the future so we have this transaction and I mentioned when we get to this stage it gets sent to the roll-up what does the roll-up topology look like well first I want to point out that that example transaction was doing some private computation for the user and some public computation but it doesn't need to be that way it can just be this transaction is just public computation um similar to other ZK Roll-Ups or it can be completely private computation you can think zcash transactions are completely private so you could deploy an app which does zcash like transactions on this smart contract platform and I won't go into it today but there's also the ability to deploy contracts and you can even have similar to ethereum functions deploying contracts for you from a smart contract Okay so we've got these transactions and they get sent to the sequencer or roll-up provider and this sequencer is going to compress compress compress compress until we're left with one proof and the way you do that is by verifying proofs within circuits so this base roll-up circuit verifies two proofs and this one verifies two proofs and then we go to the next level and we verify two more proofs and then we end up with one proof and because the proof system we use here ends up being quite expensive to verify and ethereum we'll squish it down to a cheaper proof like plonk or gross16 and then send it to the smart contract whoa right and then within the smart contract this proof will be verified and the state trees of all the various trees we're managing will be will be updated or rather the roots of the trees will be updated okay and I think this is the final slide so what I want to point out here is that there's always this Arrow of time with these transactions so you begin with a private function executions or proving you've executed private functions always on the user's device and once you've done that step then you can go send your transaction or your call Stacks to the roll-up provider who will execute their stuff and we'll ignore the contract deployment step and then it gets sent to the roll-up contract and from there similar to Aztec connect if you've looked into it you can then make calls to any smart contract in ethereum and interact with any D5 contract that already exists but suppose your private function wants to make a call to a public function or to a layer one function and receive return values Well it can't receive return values immediately because of this Arrow of time and the order things need to happen excuse me so you'll have to wait until the next Roll-Up and so what we have and I don't have time to go into it here is if a private function is making a call to another layer of this stack it will have to record callback functions similar to if you're working with JavaScript and those record of those callback functions will be added to a tree and then in the next roll up the user will have to prove that they are executing the correct sess of success or failure callback function in order to carry on with the execution of their function um so there's a lot of complexity there um we've written a lot of documentation which I think we're going to publish um which kind of explains all of this complexity um but yeah that's everything I wanted to talk about today uh oh and I guess I do have time to just briefly say well the kernel circuit does it does tons um and my throat's hurting so much that um I'm not going to through go through this but uh feel free to take a picture and uh ask me questions afterwards uh yeah thanks very much we still have time for one question maybe if someone wants to do a question we have our volunteers with the mics there you go here to the left and maybe one there Depending on time um one thing that you didn't touch it on the presentation was about the participants so you mentioned in the beginning that you might have public state with participants um so for example Google which I'm sure you are aware there are platforms that allow you to have public and private State contracts they have different ways to tackle it so best of those like groups you define your group and that group will share a private State and go quadum which is a photograph with modification for private State allow you to like select for each transaction which participants will see those changes and that means that you can have the versions on the state so like you might have a transaction that you change one smart contract and you change and you apply that state change for two participants but then in the next transaction you change the same value and only one botism will see it not the other how do you solve that in your platform uh I didn't fully follow the long question um maybe we can chat afterwards sorry about that yeah you can chat outside that's the great place to do networking and they get to know more and we have one more question there please send to that Mike hello um just one quick question uh is is this private call for contracts already available in NASDAQ or it's going to be in the next release uh so we'll be releasing the specs but we we haven't built this um we're kind of we've begun building it um but it's we're probably maybe a year away from testnet um so yeah it's a it's a big beast bigger enough Applause once again for Mike Connor Thank you [Music] [Music] foreign [Music] [Music] [Music] foreign foreign foreign [Music] foreign [Music] foreign foreign [Music] all right so our next speakers can come from Argentina Federica and Herman they're going to talk about Cairo BM so super excited to have you guys here big round of applause hi thank you well excited to be here thanks all for coming today and in this talk we are going to be talking about how we improve the performance of provable computations using rust and specifically how we implemented the carrier VM using rust so okay let's go first I want to give a little worse on who we are and we are like a class Lambda class is a lesson base software company and it's in the industry more head interior years and yes basically we love solving difficult problems so if you if you have any problem just give us a call and okay let's go straight to to the talk and the first thing I want to start with is to give a little bit of context and talk a little bit about ethereum and as you all may know ethereum is a decentralized network meaning that when you run some suction the transaction is not run in only one computer but in every computer that forms part of ethereum of the network so that's really cool because the verification of the transaction does not depend on only one computer on a centralized entity but on every computer inside the network but it comes with a downside and the downside is that the computation capacity of the network depends of on your slow snow so we have in a situation that we have a high demand of range transactions and our computation capacity is limited by our slowest low so how can we solve this problem and here is where starnet comes into play starnet is a secret roll up and what rollapse allows us is to run transactions of chain then make a batch of intersection and send to ethereum the update State and what CK adds to roll up is that we can run the construction of chain generate a proof of the Integrity of the execution of the transactions and then we send to ethereum they update the state and the proof of the computational integrity so then on chain the proof is verified by a verifier and that verification is running all the computers inside ethereum so well and the coolest thing about this verification is that the verification cost of the proof growth logarithmically with respect the number of transactions inside the batch so as the the cost of verification is distributed between all the transactions including the batch as we add more transactions in the in the batch the cost per transaction the average cost per transactions goes to zero so ah yeah because it's a slide that I was supposed to to say that because of verification grows logarithmically with respect to numbers and sections and yeah if we tend to Infinity if you take this one extreme and we tend to Infinity the number of transactions inside the batch because the average cost per transactions go to zero so okay we say that in stagnet we can generate a proof on the execution of our transaction but what kind of proof well it's a series proof and specifically a stock and to your knowledge allows us to prove the veracity of a statement without rebuilding anything beyond the fact that the statement the statement is true and in this specific case Starks allow us to prove their computation Integrity of the transaction that would run off chain we are having to run it all again so this is kind of a game changer because before serial knowledge the only way we had to to prove the acid the computation Integrity of a computation was to run that computation ourselves but with your knowledge and stacks now and untrusted entity can run transactions a generator proof on the Integrity of that execution and then the proof have to be verified and we can be sure that everything is okay so okay how do we write a program that is uh provable meaning that its execution can be can be proven the Integrity of the execution okay we have Cairo and Cairo is a private language a specific design to doing this when we we write a program in Cairo it's run in the Cairo VM um that Cairo VM executes a program and also generate a trace of the execution and that Trace is then is going to be sent to approver and generated the start proof and that that's that probe is going to be verified by a verifier so okay how does the Caribbean works and North America will guide us through the internals of the Caribbean so fariga soldiers well now let's talk about about how the Caribbean works well you already know how a virtual machine works basically you write your source code you compile it and then you use the virtual machine to interpret that compile code and execute the instructions but what sets apart this current Bureau machine is that it also generates this Trace that can be used to generate to proof so let's go a bit into the architecture of the career virtual machine well first we have the memory model it is the right one's reads only memory that is divided into different segments these segments have a known size at runtime and once we finish the VM run we these segments go for a relocation process and we end up with a continuous memory so well the segments consists of the program segment which contains the code of the program the execution segment which gets filled up as the VM runs the built-in segment which we'll talk about later and the user segment which contains structures defined by the users such as arrays or dictionaries well let's explain a bit about this relocation process the first step is to assign a size to each segment for example the segment serial has five elements so it has size five and then we will use decisis to assign a base to each segment what we say with base is that is the first address of each segment in the relocated memory so for example for segment one we have the previous segment base is one because this is the first segment and we add five which is the size and we get six now with this space is calculated we'll proceed to relocate all the addresses we'll do this by adding the base of the segment with the offset of the address so for example here we have the segment the address 0 0 the base of the segment 0 is 1 and we add 0 and get one we'll do this with all the addresses and also with the addresses that are contained inside the memory for example you see here that we have two zero and three zero and they both relocate to nine as the base of the segment two and segment three is nine well and this is how the relocated memory looks like it's a set of continuous addresses with elements well now let's talk about the registers Cairo has free general purpose registers the first one is the program counter which iterates over the program segments and points to the next instruction to be executed then we have the frame pointer and allocation pointer in execution segment the allocation pointer will point to the next and use memory cell while the frame pointer points to the current frame what this means is that when we want to execute a chiro function the frame pointer takes the value of the allocation pointer and remains constant while the allocation pointer keeps increasing as we add elements into memory and when we exceed this function the frame pointer will change its value well this is how the VM operates this is the main execution Loop we first get the next instruction we decode it we compute the operands then we add the current register values to a trace and then we update the registers so okay this is how the trace looks like it keeps track of how the registers change for the execution of the program and it also goes through the same relocation process as memory well as we can see the size of this Trace will depend on the amount of steps that a chirop program takes so if chiral program takes a lot of steps this Trace will get very big and will take more time to generate the proof so sometimes there are some computations which are expensive take a lot of steps but maybe the information provided by each of the steps is not that relevant for example if we want to compute a Peterson hash this will take many many steps so in order to solve this what we have is oh sorry we have built-ins built-ins are low-level optimizations that are integrated in the core Loop of the VM and they allow otherwise expensive computation to be performed more efficiently and while each of these segments each of the built-ins have a segment in memory okay but where do they come into play well we see here that we have our main execution Loop and we have this the GS operands added and what this means is that sometimes we might want to compute an operand and we can't really compute it directly and we'll rely on some deduction rules example here we have the patterns and built-ins auto reaction wall what it does is it takes the two previous values in memory and computes the patterns and hash of them this rule comes into play when the address of the operand that we cannot compute belongs to the built-in segment and that built-in has the you know to the action rule so what that's what this allows us to do is for example here we want to compute the hash of X and Y so what we do is insert X and y's memory and then we ask for the next memory location as we haven't inserted anything into this memory education we won't be able to compute the operand and we will fall back on this deduction rule that will be the one Computing the hash so we computed a person hash but there is no sign of the computation in the chiral memory or the steps or the trace well another feature that Cairo offers are hints these are blocks of python code embedded into a career program and they allow a python context to access the BM State and modify it also access local carrier variables and can communicate between each other for execution Scopes what we see on screen is the unlock function from the common Library this function is commonly used to generate arrays and what it does is it creates a new segment and it inserts it into memory so we can use it from Cairo well we mentioned execution skills these Scopes are a stack of dictionaries that can hold variables that are created inside a hands these variables are not seen by Cairo just by the hands and this can be created and removed inside hints and multiple hints can access the same code for example this code is a bit long but we could just focus on for example we first enter the scope with an N variable then we use that and variable we modify it for various iterations and then we exit the scope deleting this and variable well so now let's talk about how we implemented hints in Cairo RS that is our implementation of the VM well first why we chose rust we chose Frost because it offers very good performance memory safety guarantees and it also has an amazing community well our first iteration of hints in Cairo RS was basically to implement Ros functions that would imitate the behavior of hands and match these blocks of python code to our sense for example the constants we see here are the blocks of python code and this is again the example of the alac function this is the block of python code and this is how we replace it with a rough native function well this first iteration had pros and cons on the cross side we had that it was very easy to implement as we didn't need any new tools and it also offered better performance as calling functions within the BM was much faster than compiling on running code during the bm's run but on the con side is that we needed to watch out for changes we first applied this to the common Library so if a common Library hand were to change we needed to change our implementation in order to keep supporting it and it also wasn't really extensible as if we wanted to support more hints we'd had to code it inside our BM so as we wanted to support user defined hints and we also wanted to integrate our VM with the current python infrastructure we decided to go with the next iteration that consisted of integrating python with RBM well in order to achieve this we use the create Pi u3 it is a query that provides rust bindings for Python and it allowed us to share our BM state with the python context I know that python context to modify RBM and also allow us to define a strict interface between our VM and python by using pi classes and Pi methods Pi classes are brush structs that can be interpreted as python objects and Pi methods are Rus methods that can be called from within python so let's take a look at the code and well this is how we allowed hints to modify memory we created this Pi memory object and when we want to get an item what actually happens is that we call our VMS memory and we use our rust get function and the same happens when we want to set an item well how did we allow it to modify Cairo local variables and carry local variables can be accessed inside Hands by writing ideas dot the name of the variable so what we did was override this IDs objects set attributes and get attribute methods so when we want to get an attribute we are actually calculating the value of that attribute inside during the hints run and we want to modify a local variable we calculate its address and insert the new value into memory well the next step was execution Scopes in order to implement this we took advantage of the local python variables so what we did was take our execution scope variables convert them to python objects and inject them into the python locals and when the hint was done running we extracted the python objects from the locals and inserted them into our execution scopes well this is how executing a hint currently looks like we first take this our local variables and inject them into the python locals we create these Pi classes that will allow hints to interact with our VM and insert them into globals and then we again retrieve these values from python locals and update our execution scopes but well how does this all look like we we created a separate grade for this called Cairo RS Pi in which we instantiate the VM and we execute the Cairo methods debians methods such as initialization running instructions and when a hint cannot be run by our VM we fall back to this python hit execution so with this approach we were able to maintain the rust functions that we created in the first iteration and also add this python integration so when you use the common Library hints you can still enjoy the boosting performance from the plus native functions okay awesome thanks for that oh that's difficult sorry okay okay um well in the name of our talk is how we improve the performance of proper computation so showing some Benchmark or some math and here we can see how our implementation of Cairo Cairo RS is about a 100 100 times faster than the Caribbean using the C python interpreter and about 20 times faster than the Caribbean using the Pi Pi interpreter and the coolest thing is that the our implementation is a 100 times faster than a car VM using python but also consumes half the memory and our implementation is 20 times faster than the Pi Pi version but consumes 12th time less memory than Pi Pi Pi so yeah it's really cool and yeah okay I hope that standard development developers Embrace a Kairos and that helps building and accelerating the ecosystem so yeah that's it thanks for being here questions we still have time for some questions so we have our Volunteers in the room just raise your hand if you have any questions compliments for the great presentation um hello hello hello um well I've seen you before um that's the my question is how stable is this is this available to use right now uh or it if it's available for F2 is in right now yeah yes if we are still developing the integration with python but yes cairos is a is open source and you can use it but we are finishing I think in maybe two weeks and they implement the integration with python so after that we are going to start a testing it and making a faster so we can be sure that everything is okay and then where it's going to be start using it in production so we are really excited more questions we let them speechless about the representation well maybe big round of applause thank you so much for the weekend Herman foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] [Music] [Music] [Music] foreign [Music] stop [Music] [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] thank you thank you [Music] thank you [Music] foreign [Music] foreign [Music] thank you foreign [Music] foreign [Music] foreign [Music] title Defy is missing a primitive privacy so big round of applause to John from Aztec his facility engineer John please [Applause] so hello I'm Jan and the stock was original is supposed to be held by our CEO but he decided to escape through this building is up to me and I will talk how D5 is missing a key primitive which is privacy and how Aztec connect fixes it fixes it so many of you probably remember that last year there was this GameStop Saga and basically what has happened is that this information about The Big Short position of one hedge fund leaked and then since internet is kind of random like people decided that it would be fun to just ape into the stock and pump the price and the fund got liquidated and this was caused by the fact that like the information leaked and so the situation in web 3 is even worse like basically anyone who is able to use a web browser and can go to etherscan and search like what anyone is doing and this is made even worse by people's use of ens so we can see quite commonly that people use their ens names with trading because they apparently have no idea that is not private and to drive this point home like basically there is this ensname docs.eth and if I want to spend some time on it I can just like basically figure out that the user has units web position I have a position and how amount of USD available for collateral and you might be asking why this is bad because like plenty of the times it's considered the transparency is like a nice feature but when we are talking about financial information it can be bad because like it allows basically anyone to assess solvency of any entity on ethereum and this information can be used by adversarial parties like Mev Bots and then there is also of course a risk of doxing and consequences in the physical world this can be as innocent as your friends shaming you for your bad trades but it can also make you a target of crime so the solution at Aztec we think that the solution are privacy preserving roll-ups and our privacy preserving group is called Aztec connect it is live on mainnet and we like to call it VPN for ethereum and I think this is quite a nice mental model because essentially what VPN is doing in the webto world is that it replaces your IP address with the IP address of the VPN provider and essentially what we do is that we replace your ethereum address with if ethereum address of Aztec so I wanted to show some demo but of course it's not very possible here so I will just show some screenshot so this is our website ZK money and when you come there you can click Shield and then you can choose an alias this is like an analogy to ens name on ethereum mainnet then we will create your account this will take a while because there are some complex computation happening behind the scene and then you deposit the amounts of funds and behind the scene that actually happened quite a few things that happened two transactions on ethereum one deposits the funds into a royal processor contract and under approved this approves this hash and then there is like a generation of cryptographic note on L2 and this cryptographic node allows you to spend those funds so once you deposit this is what it looks like it's like a normal wallet and then there are opportunities so you can deposit into an arbitrary Bridge we've implemented so for example if I deposit to Euler I have this screen and there are quite a few interesting things here which I want to explain first one is a batch so there is quite an interesting feature to us that connect and that is that we allow for this thing called batching and basically what it means is that when there are multiple users which want to do the same interaction on ethereum L1 we basically do only one interaction on L1 and on the inputs of the interaction is like a sum of those token amounts so basically when there are like 10 users which want to deposit each to Lido and let's say for Simplicity each user has one if we only call the stake function on Lido contract once with the 10 ether and then later on WE distribute the results to the users and in this case like the results would be the output tokens which is staked yeah I would like to briefly pick what is behind the scenes and we use utxo model it was talked about by quite a lot in the Aztec 3 presentation before but basically what is a utxo module is that you can imagine when you have banknotes and you want to go to a merchant and you want to pay with the banknote so what happens is that you can like split this banknote but you give the merchant the banknote and he gives you the money back and this is essentially what we do as well and we do it with this thing join split circuit and essentially what this allows is that you have two possible nodes on the input which get destroyed in the process and then there are two nodes on the output and for example in the example of of the bridge interact of the D5 deposit to Lido uh on the input would be for example 0.5 if and in case the original value held in your account was one note with the one if this node would get destroyed and on the output would be this claim node and another volume node that value node would hold the remaining each balance so it would be 0.5 if and that claim node would allow you to later on claim the funds a resulting return from the defy interaction so you might be asking how is this possible that like this is private when the settlement like the interaction actually happens on ethereum and this is a good point and uh here in the left bottom corner we can see what it looks like like what token transfers there are and this looks pretty not private but the thing is that thanks to zero knowledge proofs nobody can distinguish like who is actually originated this introduction and also there is this aspect that there can be multiple users per interaction and so these balances get even more obfuscated and currently there are more than 50 000 unique users last time I checked it was like 51 000 something and it means that like the amount of people you get height with is pretty large you might be wondering does it scale roll up is quite a common password and it does scale and one reason why it scales is because there is that 101 transaction per multiple users but of course everything is always more complicated and there are overhead costs to roll ups and discuss our cost of posting chlorolog and per transaction cost of call data luckily this cost of posting a roll up gets split between many users so when there are many users in developed this goes down and the cost of call data is fortunately going to down a lot with this ethereum Improvement proposal for 844 so hopefully like in a not so distant future the cost of interacting privately with D5 will actually get very close to the gas cost of that one L1 call divided by the number of users uh now you might be wondering like how do we integrate is there like some integration needed and unfortunately yes but it's not so complicated and we have this thing called Bridges but Bridges nowadays have quite a bad connotation and in our case the bridge is just a simple smart contract so there is no multisig risk and the smart contacts tend to be fairly simple so here on the left we see the ID vibration interface and there we basically just Define input and output tokens then we have total input value which is the sum of those users input balances and aux data which is arbitrary data which can be used to pass to the bridge contract and on the bottom right corner we can see the implementation of the con of the function itself and in this case it's an implementation of an erc4626 bridge and we see that it is like relative we basically just call deposit if deposit flow is executed or we call Red beam in case withdrawal flow is executed now you might be asking if there are some limitations and there are quite a few I would say the biggest one are the longer settlement time and that is because when you you we have to wait for the batch to get submitted to to ethereum like luckily you can pay if you are really in a hurry so you can pay for the full roll-up block and in that case you could get the instant settlement time but most users don't want to do it because it's fairly expensive and this causes like some problems with slippage so for example you can imagine when you trade on uniswap you define the slippage parameter which basically protects you from Mev Bots exploiting you or in general it can protect you from Market movements and the longer settlement time can also cause issues with liquidations because you can imagine if we are close to being click related and you need like fast to exit the position or something in the that is problematic but always you can pay for the instant settlement then there is the limitation by the amount of data which can be on the input like most of the time this works quite fine because it's 64 bits of data and with like a bit of smart encoding we can actually put a lot of information there but it can be problematic when we for example want to pass in ethereum address because that is 160 bits then another limitation might be that the system doesn't work that well with esoteric tokens because those esoteric tokens might have insufficient anonymity sets and basically what that means is that there is unlike one esoteric token you user and he would deposit into Aztec and he would be the only one like working with the D5 protocols it would be pretty obvious that is that one user but in practice I think this is not that big of a deal because usually users want to use the high liquid tokens and I don't feel personally that is such a big limitation then another lamentation might be that some Protocols are specifically designed to work with message sender so for example when when those protocols just work with tokens you can imagine that there is like token on the input and token on the output which is for example the case of Lido like this works perfectly because there is no limitation we just have tokens on the input and then we distribute the results but when the protocols work specifically with message sender which can for example can be when we are borrowing LUSD from liquite uh it's pretty complicated to do the implementation and how we've solved it is that we deploy these multiple Bridges and each of the bridge has fixed collateral issue and like if the bridge was like close to being liquidated user can't add more collateral but the user can always repay the depth and exit so even though there are limitations usually there is like some way how to work around them now I would like to talk about advantages and I think those advantages are pretty huge because Aztec connect doesn't need redeployment of smart contracts on L2 because we work directly with smart contracts on L1 and this means that for example liquidity is not fractured and I did my own personal experiment when it was blue market and I checked like how much it makes sense to do for example swap on L2 and it turned out that for not so large amounts like over five thousand dollars even with the high gas fees it still made sense to use ethereum L1 and that is because the liquidity was just so much higher so the slippage would be just so much slower that like the piece would be worth it and another Advantage is a lower risk for integrated protocols and that is since we don't need to the Target protocols don't need to like get redeploy again on on our L2 uh there is no risk for them like for example the risk of fracturing liquidity then I would like to mention that there was of course 95 use cases but the fire is like a nice password so we mentioned it here and I think very cool use case could be private doll voting and that is because in all the voting systems privacy is very important and another interesting use case might be NFD minting yeah thank you for the attention [Applause] thank you John we still have questions time for questions so here to the right Nicole ah yeah I can I can put that I can let you know but I must point it out presentation yeah um just a quick question uh how expensive is here here sorry you're looking for me how expensive is is each operation because it seems like you are doing complicated cryptography and solidity uh so on on ethereum L1 it was like the same as I mentioned before like the color itself and then there is the cost of call data and processing the roll up uh so is does it answer the question like I have to admit I'm not a cryptographer I'm just like a basic solidity engineer well as well so yeah um so it's not very expensive a second can this work on on alternative chains like uh like other in other l2s like having when it has taken okay so our contract is implemented in solidity so we can deploy it to any evm chain and like our L2 infrastructure just uses generic Json RPC so like there is no fundamental issue why that would not be possible but like an interesting note is that it might not make sense because usually those other l2s have also like have to post the call data on ethereum L1 so it means that if you for example were running over optimism we actually would not save much of gas we have one there on the left side then there and then the third one uh thanks for the talk um you mentioned you have over 50 000 users um on Aztec network network um a couple questions like who has integrated besides Euler um like who's on the docket and like what are those 50 000 users using it for currently yeah so we have quite a few Integrations I think the most important are for example Lido Oiler than we have for yearn um we are working on unisfab click with the and yeah we have a grants program where we basically give some developers money to build the Integrations and yeah that's it and what are the users are using it for currently mainly uh I think that like in general the farming vaults are quite popular so we have quite a few users of Ian and we've also built this thing called DCA Bridge which allows like users to DCA into eth or die and it's also becoming quite popular pci's dollar cost averaging for those who you don't know um I got the mic ready uh so uh one of my my question was around what you're doing to try to deal with this settlement like waiting times and stuff um and I was wondering if like as you integrate with other protocols if that's like a breaks complete or brings in composability challenges and kind of how you're thinking about solving that in the long run yeah so there are no composability challenges because it's running on ethereum L1 like the calls and like the best how we can deal with the settlement times is basically having enough users that it the that it makes that it's profitable to submit the roll-up blocks frequently but as we said if there is like some user with your origin plan needs it to be settled he can always like pay for those instant settlement much does it usually cost if it's just like one user or it depends I mean when it's badged like go and play it costs like one dollar to deposit into Euler um thanks for the awesome talk um one example you gave at the end was like a private nft purchase um does that mean the nft will be owned by the Aztec address yeah exactly it would be on by the Aztec address but there is this thing called we don't currently like support nfts directly in our UI but we have this thing called virtual assets which basically allows like a representation of any ownership so like on ethereum of course it would like stay within ether within the bridge it will stay within the bridge contract but yeah the user could then in interact with it by using the virtual assets virtual assets basically what it does is that it like doesn't exist on ethereum L1 but we create this another value note on our L2 which contains the information which can then be passed to the bridge and worked with so yeah though it would stay within the contract on ethereum my question is uh can you do scripting Logic on the L2 itself don't like not on a bridge but within the L2 or I mean it's not accessible to like developers but it's possible to like build circuits but I mean it's not the goal of Aztec connect it will be the like we will have generalized smart contract once Aztec 3 is available which will be in a year or more so like things like multi-6 on the L2 itself that's not really possible yet attack actually there is an implementation of multisig it works with like those account cryptography so yeah we have multi-6 awesome thanks more questions and thank you test test up so the the always a fun question about regulatory risk um obviously this has like a sounds like a mixing component similar tornado um what are your thoughts about that and uh that that that risk yeah so currently how we approach it is that we limit the deposit side size and basically like our goal is to avoid the use of hackers avoid users hackers using our platform and like the big issue with tornado cash was that like 70 percent of the money there was elicit so basically we are just trying to make it impractical for hackers to use our platform what's the limit I think currently it's like 10 each per deposit but we might increase it in the future okay thank you can you share a little bit more um I'm over here on the front right you're 50 000 users can you give us a sense of like before Aztec what were they doing are they doing the same thing just totally in public or as a result of Aztec are they now doing things that they previously weren't doing oh I'm not so sure how to answer but I would say it were like General like defy users and they just wanted that privacy so they deposited to Aztec but I actually didn't do the analysis we have four minutes more if you have any questions you have John has been really Brave to substitute the CEO congrats big round of applause just prepare this thank you thank you [Music] all right [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] foreign foreign [Music] foreign [Music] [Music] foreign [Music] foreign [Music] I will give it a love God [Music] [Music] [Applause] [Music] foreign [Music] [Music] our next speakers are going to talk about CK application landscape let's give a big round of applause to lakshman and Yi [Applause] oh I want to make sure this works cool uh so our talk was originally ZK application design patterns but ye and I got together and started talking we realized a lot of what we really wanted to talk about was like what is the landscape of ZK applications uh ZK or zkp or snark is quickly becoming like a big data-esque buzzword and very jargony it's starting to mean a lot of different things um and so we hoping to talk to sort of like talk through some of the different application classes we're seeing immersion emerging today and in in the process sort of like plot what we think the next six months to one year of like new ideas coming out look like um and if the course core thesis or stated in a single statement it's basically um maybe unsurprisingly to many people who work in Decay that succinctness and privacy are are kind of like the two sort of features of ZK that are interesting and understanding what succinct private succinct apps require and what private apps require like independent of one another might help us understand how like these two applications exist mentioned we really think of ZK as this Matrix where your proof can be either succinct or not succinct and your proof can either be private or not private so we think that each of these quadrants is useful in a different type of application so in the top left if you have a private proof that's not succinct well it's going to be hard to verify it on chain but you can still use it in some off-chain applications in the bottom left if you have a succinct proof that's not private well that's perfect for on-chain infrastructure applications but not so good if you're trying to hide information and of course you can have the best of both worlds in the top right where you have both succinctness and privacy but as we'll talk about a bit later in the talk that's extremely challenging to do so we think that almost all applications right now fit into one of the left two quadrants so just to talk through some of them on the on the top left we have uh more social type explorations of what you can do when you can hide information about what groups you belong to and what statements you want to make with partial revelation of information on the top left there are in the bottom left there are a lot of projects surrounding making blockchains more scalable or giving additional capabilities to decentralized applications using the succinctness property but where everything's public already so we don't care too much about privacy and finally in the top right there are a very small number of applications that have managed to both achieve succinctness and privacy and you're probably probably everyone is familiar with those shown okay so we're now going to give an overview of where we are today on each of these factors succinctness and privacy so in the bottom left corner if we ask for a succinctness but not privacy the general theme is that zkps are used to scale trustless off-chain compute and the theoretical principle behind this is that if you run code on ethereum today every ethereum full node has to rerun that code to validate that the execution of your transaction was correct so that's about 100K at times overhead and it has very high duplication the new capability that succinct proofs give us is that only one party needs to execute the transaction and generate a validity proof of that transaction everyone else simply needs to validate that proof so this removes the duplication but just incurs a very high overhead on the prover and so examples about applications that use this pattern today are all of the ZK Roll-Ups as well as the sort of app specific Roll-Ups like dydx or Loop ring so another capability that we see ZK succinct proof springing is cryptographic interoperability so one thing that ZK allows you to do is to take anything from the grab bag of crypto cryptographic Primitives on the left and allow you to wrap it in a uniform format namely a ZK snark so while each of these Primitives is useful for a different thing they are designed in a somewhat single purpose way and that makes it very difficult to aggregate or compose these things so ZK provides an interoperability layer that turns all of these into a snark proof that is arbitrarily composable using recursion and while that snark proof adds some overhead it makes this open interoperability possible okay so let's talk about now what's necessary today to make this happen the first thing is that because we're only asking for a succinct proof we can really scale the prover regular we can first not prove in the browser we can prove on the user's bare metal CPU so that gives us a really high 5 to 10x Improvement secondly we can Outsource the prover to the cloud so we can send the user can send a request for a snark proof of something that's already going to be public and then the cloud-based server can run a big large AWS instance it can run a GPU it can run an fpga or eventually an Asic and that can give another 5 to 10x speed up in the proving time and finally I wanted to talk about the last constraint when you use succinctness on chain and this is a pretty heavy one which is that you have to verify all of your zero knowledge proofs on chain and in this case the gas cost of verification really differs from the CPU cost it's going to depend on the proving system you used the choice of curve for that proving system and also some proving system specific implementation choices the most restrictive of these is the choice of curve so on ethereum there are pre-compiles for the bn-254 elliptic curve and those operations are much cheaper unfortunately other curves are somewhat prohibitive to do directly in evm hence the need for a pre-compile in the first place one way that is commonly used right now to get around this for example in the zkevm is the operation known as aggregation the strategy here is that if you want to use a snark a very big one that's not natively compatible for evm verification you first generate your snark a and then you produce a recursive snark B which proves the claim that you know a large snark a that proves the statement you claimed in this case you can choose B to come from a different proving system than your original snark and that proving system could be very cheap to verify on ethereum in this way you can sort of transmute a very large narc from an arbitrary proving system to an on-chain verifiable snark from something that's compatible with ethereum the only caveat here is that you of course incur the overhead over this recursive proof cool thank you um yeah now I'll talk about the the other kind of interesting cluster we're seeing in the top left which is uh application classes that seem to require privacy or pseudonymity of some sort but don't necessarily require six sickness um this is kind of like a more nascent uh ecosystem of things um many of them don't require chains and so uh I think relative to maybe a lot of the things that you talked about they're perhaps less familiar to the average ethereum conference attendee but what we're seeing is that they seem to they seem to Cluster around social type things um this doesn't mean that this is all it is in the future but kind of want to talk about what we're seeing today um and so in thinking about this quadrant I spend a lot of time thinking about sort of this spectrum um there's a Spectra so let's say think about those arrows or zero knowledge proofs being made the computers are like say like an end user computer ethereum there obviously is a decentralized state machine with a high-ish block or a low low ish relative to congestion block gas limit um those those bottom two icons are sort of uh how proofs are being consumed and the top two icons or how proofs are being produced it feels like there's a spectrum of applications and how we think about what is consumed is it being consumed by a human is it being consumed by a state machine or a decentralized date machine um if it's being consumed by a decentralized state machine like ethereum hence like on-chain verification you need things to be considerably more succinct if it's being interpreted by a human maybe you don't maybe it can be like a much larger piece of information that is like human interpretable or has like some kind of like interesting graphical representation and so in thinking about this spectrum and again this is this is not including a lot of things in the middle so things in the middle might be things like uh like being consumed by like an app chain or a layer two or a a lower congestion layer one or a short-lived chain or something like that but these two ends of the spectrum I think we all quite understand like the chain side of applications like things that that use your knowledge proofs that are that requires tend to benefit from being very composable creating things that are canonical on the other hand if we're talking about like humans interpreting proofs it feels like it's the applications want to be higher velocity or can benefit from being higher velocity because uh you don't need a chain verifying every proof uh things are more ephemeral maybe you know humans don't necessarily have like persistent memory themselves right like we see a lot of things on Twitter and sort of like form a vague understanding of what's going on um another mental model I think about is um zkps on the on the Privacy not just inside is uh like zkp sort of creating a new content type which is like all these places where people create content online today um what if they now were enriched with the zkp of some sort it kind of means something a little bit different which which is quite cool um so I didn't really talk about I realized I probably should have had a slide here presenting some applications but um some examples are uh like um actually let me just pull back go back to this slide so it's done before which is a project of the PSE which is kind of group uh group membership in like proving private group membership so you can prove that you are one of the people in a group and so you can use this to do like private like uh synonymous message board or something like this um things like CK email like proving that you know mail servers turns out many of them uh like sign uh sign the email body so uh you can you can do a zero knowledge proof that you received an email with certain Properties or something like that which is kind of cool and and do things like it's not obvious that that's your knowledge proof in either these cases needs to be interpreted by a chain um and so the requirements in this quadrant are quite a bit different um than and then the the quantity was talking about and that like verification complexity doesn't matter as much verification complexity matters its thickness because um or matters when you're verifying on chain because you don't want the chain to have to spend a lot of gas to verify your proof but if a human's verifying it like whatever right like uh it can be done somewhere else uh like off chain and or like you know it can use like some like even consumer Hardware is more powerful than a chain um proving complexity tends to matter a lot more because we are operating usually on a consumer device for proving so consumer device proving friendliness as well um this is something we think about a lot is how do we make things work in a web browser how do we make things work in a mobile device another piece is you know like given that we care about privacy in the Squadron is respect for sensitive user information so if anyone was around for um you should talk earlier today uh where where he's sort of been developing this new deterministic nullifier scheme um that came from realizing that we just couldn't do a lot of things with private key uh in in uh in a client device because it would we would need to pass this very sensitive piece of information around different parts of application memory um and so we have to think of new kind of like mechanisms for not having to do that oh this is still me oh cool all right so some of the some of the some of the challenges um that we see uh technical technical challenges in the next six months um so as I mentioned earlier like we need to be very friendly on like for privacy not succinct uh applications in in research constraint environments like mobile devices like like uh web browsers I think a lot of like um a lot of the ZK space is assuming that most vka proofs will be reduced on like very large servers or with fpgas or gpus you know there's some like debate around which of those two things wins of course um but if users are making proofs to other or humans are making you proofs to other humans uh we perhaps can't quite make that assumption so we spent a lot of time thinking about performance and research concerned environments um I so specific like you know the specific example that I gave earlier earlier with ayush uh we need to think a lot about um non-private key nullifiers like non-private key uniqueness uh of identity um because we don't want to have to deal with private key we don't want to have to create like an environment or a platform or anything where we we assume private key is passed for map to app and then we also spent a lot of time thinking about um representing more crypto systems where identity matters um in snarks so you know ethereum we all interact with ethereum that's quite obvious there's a lot of interesting things you can say about ethereum stuff but there are other now like networks in the world where uh you know cryptographic signatures are made like like email mail service like I mentioned earlier so um we're constantly looking around to understand where new networks are forming and how to like put those operations inside snarks so lock swans talk a bit about some of the challenges ahead for privacy and a lot of that was dominated by finding new ways to improve the user experience and find applications I think on the sickness side uh the split is the other way where what you want to build is somewhat more clear but you need to have good enough performance to actually build it so in that sense what what I view the challenge ahead as really optimizing the performance of our proving systems and our architectures so One Direction is to really weaponize this operation of aggregation and recursion that I mentioned earlier and the reason here is that if you want a very rich infrastructure application you need a proof for a much larger circuit than is even possible than the large in the largest circuits today and so there are a couple ideas here that are only beginning to be exploited by projects right now one is to maximize what's called a prover verifier trade-off there's a fundamental trade-off between the proving time and the complexity of verifying a proof If you feed in more compute you can get an easier to verify proof um so we can do things like starting with a very fast to prove uh system and then wrapping it in an aggregation layer with which transforms it to a cheaper to verify system one form of that is already in use with basic aggregation but we can push it much further using multiple aggregation layers and the things here to really optimize are non-native arithmetic and elliptic curve operations for these snark-based systems and a second direction is that even if we really optimize aggregation it's likely that the most interesting operation statements to make are going to require multiple circuits to prove and so in this schema you divide up a large computation with into multiple pieces and then you verify each piece in a snark and then you verify those snarks recursively so with the ZK VM and other ZK rollup VMS we're seeing the beginnings of building virtual machines in this way and I think that VMS for transaction execution are just the beginning of this trend in a separate Direction in the last five years a lot of the progress in ZK has been driven by the emergence of many new types of proof systems they've really pushed the envelope on what we can do inside a ZK circuit so Justin as recently as 2016 really the most viable ZK proof system we had access to was Groth 16. but since then we've added many capabilities like custom Gates lookup arguments and we've been able to remove some aspects of The Trusted setup in newer systems like Planck Halo 2 and in Starks in the landscape today there are actually a number of New Primitives emerging which may be equally or more exciting um so systems like Nova allow for much more efficient accumulation some check based systems like gkr or hyperplunk allow for fast and faster uh but larger proofs which could get then be recursively aggregated in other proof systems that we already control very well and finally there's the potential for much more efficient lookups and new systems such as caulk and so each of these represent a pretty big shift in the way that we can design ZK circuits and what type of programs we can express efficiently in them and so it'll be exciting to see how fast these things can come to production and what new applications they can enable oh yeah um and so so finally um and to talk a little bit about where we see kind of these two things converging into third quadrant um this is probably what the future will look like in the sense that so so the the left side is the kind of uh privacy side and the right side is sort of the succinct proving side and the idea being that hopefully we reduce the surface area of the private to public ish proof to something quite small that could then be recursively included in a larger proof uh that is that you know that that is in the kind of like succinct but not private uh kind of like um technological requirement land and then that is interpreted on chain and then this way we get all you know all of the nice benefits the Privacy answering thickness and on-chain applications with pseudonymity and cool stuff like that um yeah that's it do you see we have five minutes for questions so please raise your hand we have our volunteers here sorry maybe a silly question for the audience but um can you explain a bit more like what actually sing means like what's that probably um you know in the Layman Layman layman's terms yeah yes succinct this is a property of zero knowledge proofs which says that the size of the proof can be asymptotically smaller than the size of the computation you're able to do so depending on the proof system either the size of the proof could be constant size in the size of the computation or it could be logarithmic but either way the point is that you can verify this proof in much less compute than doing the actual computation itself hey I'm perhaps there's another distinct slide thing which is um many many of many I mean most of these proof systems do have constant proof sizes but also like the the lower the lower the constant proof size the better for uh something that is interpreted on chain obviously um but yeah use answer is better than mine would have been I was just going to say small Hey so uh when we recursively prove stuff so over here sorry when we recursively prove so there is a proof and then you are proving the proof right so uh there are multiple layers of uh Integrity checks right that that is the assurance that you get so uh and these circuits are super complex and there could be like a million arithmeticians in in there right so what about the security of uh these proofs because if you have a single bug in a circuit right that is gonna Traverse down to like this very small proof that you will post on chain so what are your thoughts about that how is the security landscape around verifying CK systems glad to hear that yeah definitely I would say on the second side definitely recursion and aggregation make it substantially more difficult to verify that your circuits are correct so we have obviously traditional unit testing we have randomized fuzzing and we have some emerging formal verification approaches for ensuring circuits are correct where recursion makes these more complicated is that you may need to encode assumptions slightly external to your circuit itself to check things I'm sure locksman has some views on how that affects privacy as well oh I mean I like I I think this is actually something we need to think about very seriously it feels like I mean if any of you are familiar with like supply chain attacks and the node.js ecosystem it feels like we could easily that that is like I don't want to get too pessimistic but that is like a very bad worst case here because it's like it's very hard for the the recursing proof to make strong assumptions around the soundness of like proof that it is recursing on um I I don't have any optimistic takes unfortunately but I think it's something we need to think about there are lots of great people like especially in the Xerox Park ecosystem kind of thinking a lot about about like uh formal verification and things like this and maybe like yeah there's formal verification of the dependency graph type things needs to needs to start happening I don't know hello uh what measures can we take uh to make our proofs Quantum resistance for the long term so I think of the different proof systems most of the ones which are based on elliptic curve cryptography are not going to be Quantum resistant since they will require at least a discrete log assumption but we actually do already know proof systems which are quantum resistant like Starks which only require a random Oracle hash assumption um so if you want something to be really standing the test of time you probably want to use one of those Quantum resistant systems um I just want to add to the 16 minutes part there's a difference between space succinctness and time succinctness um and usually six things will mean like both um a question on calc um what do you mean by being more efficient than um the easiest lookups so it from all it seems to me that you can actually be applied to like look of arguments in circuits or in the Mercury setting where you want to have you know zero knowledge membership roofs because that the there's a cause like there's a linear overheads when maintaining this you know pre-processor proofs definitely like I would say we're being a bit sloppy with the use of succinctness here in most of the systems that are deployed today you get both uh verifier and size succinctness uh with regards to call I'm more trying to gesture towards the idea that there are people working on enabling lookup arguments to be more efficient and that that would really transform uh how we write these circuits I totally agree that they're not caulk is not like today going to enable much more efficient lookups well the music is coming I guess we're coming to an end uh thank you so much big round of applause to lakshman and Yi [Music] foreign [Music] [Music] thank you [Music] foreign foreign [Music] foreign [Music] foreign [Music] foreign [Music] [Music] all right I guess you get tired of after all this like nfts we are going to suggest like to bring them down so our next speaker is sador who's going to talk about Anonymous signal in ethereum big round of applause hi I'm Cesar and I work as a software engineer with a privacy and scanning Explorations team so I'm going to talk about Anonymous signaling on ethereum and I would like to start with an important question why anonymity is important I don't know why people are so Keen to put the details of different life in public they forget that invisibility is a superpower we cannot be sure if this quote is from Banksy but even if it was Anonymous the message is still very important invisibility or anonymity is a superpower because intercouples an idea from the identity of its author because it allows flowers to be thrown without the identity of the thrower being known so all talk ideas are intrinsically connected to the hotel sometimes the message is more important than the messenger and sometimes preserving the identity of the messenger is even more important so anonymity can therefore become an important tool because it limits power those with a lot of power derive much of that power from Knowledge from information the more someone knows about us the more power they can have over us keeping our data or our identity private decentralized that power anonymity promotes freedom of speech the first step in censoring an idea is to attack its auditor and the fear of suffering repercussions for what we say or write also limits our thinking so knowing that our data and identity are safe encourages us to think freely anonymity safeguards reputation it is important that people are held accountable for their actions and especially for their mistakes particularly when these people have a lot of responsibility when they have an important role but it is also important that our ideas are not judged based on who we are part either on what we have to say anonymity allows an idea to be shared regardless of its author's reputation which allows people to consider and evaluate ideas for a more objective perspective so most of us are aware of the importance of anonymity and privacy but what about our challenges what are the main drawbacks right now and how can we solve them there are probably many reasons but two of them deserve particular attention the first one is complexity of Technology in recent years we have seen a constant growth in the development and use of cryptographic Technologies data knowledge proves for example are now being used in many applications proving you have a certain information without revealing the information itself is a really powerful concept by the way these Technologies work is not trivial for people without a background in mathematics or cryptography in addition the availability of simple and practical development tools is still Limited well a lot of progress has been made actually but I believe one more step is needed in order to further abstract some technical Concepts so the second drawback is the indifference of people one of the reasons why some people don't care enough about privacy and anonymity might be that there is not enough awareness and education about how our data is used and why anonymity is important in ielt society so what can we do then there are many solutions but I would like to focus on free Solutions the first one is privacy by default the right to privacy is mentioned in most natural constitutions but it has often been neglected in the name of National Security issues or to make the applications we use every day smarter and more efficient privacy and cryptography should be the backbone of the internet infrastructure protocols and applications should be supported all user data by default and in addition users should be free to change the settings not the opposite another point is education explaining to people why privacy is important and how we can build privacy focused applications are very valuable challenges it is important to choose the right words build effective designs and convey information as simply as possible however simplify things is not enough making people aware of the technological and social complexity of the work we live in is equally important because we don't just want users to enjoy an experience we want new friends helping us build a new web free so the fear point is the developer experience the growth of our community and the adoption of a technology depends heavily on how clear the goals are and how easily the main values and Concepts can be conveyed however another extremely important aspect is the quality of the code the call should be tested documented formatted with the atmosphere it is extremely important to use the right patterns and to follow the best practices like dry and kiss so developers need to be able to rely on robust easy to use tools that allow applications to be built in a short time while also allowing them to be extended customized and adapted for their specific uses so developer experience and education is what we have focused on in the last few months of work with some of her but many of you may be wondering when December for his so semaphore is a zero knowledge protocol that allow people to join a group and then to send signals such as messages endorsements our votes without revealing the original identity in addition it also provides a mechanism to prevent double signaling which basically means you cannot create the same proof twice or the proof cannot be verified twice so what makes semaphroa powerful tool is the Simplicity of the protocol and the flexibility it can be used to build many applications like private voting applications whistle borrower applications or the knowledge dials semaphore is made of three different parts they can see liquids solidity contracts and JavaScript libraries my work has focused more JavaScript JavaScript libraries and solidity contracts so I would like to show you the main sum of our Concepts and how our libraries and contracts works and how they can be used to build private applications so creating a consumer for identity is the first step to interact with the protocol each identity is made up of two secret values trapdoor and nullifier and one public value commitment so deposit a hash of the identity nullifier and fractor is called the identity secret and the ash of the identity secret is the identity commitment so although in this case we are not using asymmetric encryption an identity commitment can be considered something like an ethereum address it is a public value use it to represent people in the protocol and in particular commitments are used in some of our groups to represent the identity of a group member the secret values are like ethereum private keys and they are used to generate semaphore zero logic proofs and authenticate signals so some of our identities can be generated off chain without a JavaScript library and quite simple and you can generate it in two ways randomly or deterministically you can generate a random identity without passing any parameters and a deterministic identity passing a secret message like a password or a silent message groups are an important concept when we speak about privacy and zero knowledge Technologies they can be fought as anonymity sets and they are a way to establish necessary trust among participants in semaphore a group can be people who have an account on some web tube platforms with high reputation employees of a specific company voters in an election or people with specific sbts or nfts essentially any set of individuals who are eligible to participate participate in something so groups can can use several types of data structures but Miracle trees are particularly efficient since since they can prove that a leaf belongs to a tree by using just one portion of that tree so semaphore uses binary medical trees in which the leaves are the identity commitments and all the other nodes in the three hard dashes of their child nodes so when a user join a group their public identity is added to that group's Miracle 3 which is therefore used to allow users to generate proof of membership and then send anonymous signals so groups can be created off chain with our JavaScript library or on chain with our with our semaphore Contours when a group is very large and is updated very frequently enough chain group might be available alternative resolution this solution is cheaper faster but tended to be more centralized on the other hand if you need correct execution or censorship resistance groups can also be created on chain for exchange groups there is an admin which can be an ethereum address a smart contract or a modific world for example if you want to create a fully decentralized on chain group you could create a group um a group and send your contract as admin or ua3 models which can which code used to allow people to join that group only if they own specific nfts or sbts so after creating the identity and joining a group users can anonymously prove that they are members of that group and send signals such as voice endorsements or any message signals are part of a zero knowledge proof which must be generated by users preferably in their home device as they need to use their private identity so secret values and it's something like client-side encryption in this case and it allows proofs to be generated in a protected environment locally so that proof can then be made public and verified by everyone unchain or off chain to generate a valid proof we also need an external amplifier and the hash of this value and the identity nullifier is the nullifier ash which can be used to avoid double signaling so imagine you have a group you are using to vote on some proposals and you want all members of that group to vote only once you can use the ID of the group as an externalifier and when a user was the first time you can save the ash of their identity nullifier and your PD and afterwards block any double votes by checking if that Ash already exists also in this case we have a JavaScript library which can be used to generate proofs proofs must be generated of chain and those proofs can be verified on chain or off chain off chain with our JavaScript library the same library and on chain with our smart contract also in this case the way you use them depends on your application on your use case so verifying kind of chain proofs allows the verifier your server or you to be certain of the validity of the proof so it's more centralized and verifying it on chain on the other end allows everyone to consider that proof value because the smart contract is public so semaphore is already being used by some projects like unirp unirap is a protocol or a reputation system built on top of semaphore where users can anonymously give receive and prove reputation it is a great way to allow people to build reputation over time while giving them total control over how much they reveal about themselves in a given interaction so another interesting project is the kitter which is an anonymous social network where people can post and chat without losing their real life reputation and um temporary Anonymous zone is like an experiment um we have a booth on the first floor you can try our applications and there is like there are like two applications you can use where you can answer questions and ask questions anonymously with one application and make hard anonymously with the other application Collective art it's like an experiment to allow people to learn more about our Technologies and about semaphore so stamaphor is still in its early stage and some of the ideas for improving it are the following create an infrastructure to manage groups the idea is to have a cloud or self-hosted dashboard where people can create groups add members remove members or update members and they they can create many kind of groups like permission at groups or permissionless decentralized groups and also reputation groups similar to Europe so another point is create attestations contracts for decentralized groups the idea here is quite simple uh it is to create uh on chain contracts the smart contracts where people can join groups directly without any external approval based on unchain attributes like sbts or nfts or just the amount of money you have so we will investigate our zero knowledge Technologies and public systems of course and we will continue improving the developer experience and our documentation website the idea is to create like an educational website and last but not least we want to create a strong community we have main ideas here one of them is our guns round so we are launching a semaphore Grand round um you can scan the QR code on the left and uh yeah if you are interested in building privacy preserving applications with semaphore um you can scan that QR code we will we will be really excited to know more about your ideas and projects whereas if you want more information about semaphore or if you have any questions please visit our website where you can also find our this the link of our Discord server or come come to our thoughts booth on the first floor so high of my talk has been helpful and I really want to thank you all for listening thank you we have eight minutes for any question that you have for sedor no no questions okay so you can go to those QR and continue the conversation on the website and the grants so much foreign [Music] [Music] foreign [Music] [Music] [Music] foreign [Music] thank you foreign [Music] foreign foreign foreign [Music] thank you foreign [Music] foreign foreign [Music] foreign foreign [Music] [Music] [Music] thank you [Music] foreign [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] foreign [Music] foreign [Music] all right [Music] so we are going to start our lighting talks serious starting with Tyler from the PSE privacy and scheduling Explorations from the EF big run of a plus please to Tyler all right can you hear me okay thank you yep so uh I'm Tyler I'm at heart engineer on the internet uh and I'm going to be talking about rate limiting nullifier uh all right there we go so uh our land has been mentioned quite a few times throughout presentations this week and uh I wanted to kind of talk about how it worked like what's under the hood in a way that's like accessible to most people uh so the key part is the rate limiting so it's basically controlling like how fast you can interact with the system so an example would be like interacting with uh Discord like if you're in a big Discord there's like a slow down mode where you can only send one message every like 15 seconds or so it's very similar so it is made up of a couple of things so semaphore which is the talk before this which is basically just uh proving you're part of an anonymity set without revealing Who You Are samir's secret sharing scheme which is the majority of this talk uh and some time or event delineation which I'll get into in a minute there's kind of three stages to how Arlen works so there's a registration process so like joining uh which is basically just a Merkle tree and signaling which is your actual sending of messages then there is the slashing or secret recovery so yeah so like I just said the registration process is just the Merkle tree so this is basically what semaphore is so it's just proving you're part of and and an anonymity Set uh and this can be done on or off chain so the signaling part this is kind of the the meat of all of this is uh so it's a circum circuit and the first public input is the epoch which is like a time or it could be like a post on Reddit or something like that um just some delineation in a time or an event and then the actual message and then a secret key and out of that comes proof that your Epoch and your message are accurate and you know verified with uh what you put into it it proves that you belong to that that Merkle tree that membership group and it spits out a secret chair so this secret chair is one of the most important parts of of how rln Works um basically if you have a a secret uh you can plot that on a graph and on the y-axis you can you know this is basically how Superior secret sharing Works um the random so you have to have a random point to construct your uh shares and that is derived from the epoch and these Shares are generated from the message hash so uh the shares basically the the whole idea behind Shamir secret sharing scheme is you have you have a secret you break it up into parts and you distribute those parts publicly and if you have enough of them you can reconstruct the secret so with these two shares you can reconstruct where that line is so I'm going to go through how that works so if you have one share you really don't know where that line is right if you have two shares then you can see where it crosses the y-axis and you can determine what the secret is so if someone basically submits too many messages too fast like uh you know within a 15 second time period or whatever your your rate limiting is then someone can reconstruct your your secret your private key basically figure out uh your identity commitment and withdraw uh you know a steak or something like that um and this can be split up into multiple pieces it doesn't just have to be a linear function this could be a polynomial so you can have like multiple shares that you need to be able to recover so you can say like you know a person can send 15 messages per day but that's it so this is the actual circuit I know this looks a little confusing but uh if you kind of just look and see how all this is assembled it's really not that bad so uh you have the epoch the message and the secret key going in and the center part here I have a little laser pointer this center part is the Shamir secret sharing scheme and the right side this is basically just the construction of the identity commitment so it's your like public identifier for being anonymous and uh the left side so this is just the this is uh nullifier so it's basically the equivalent of like a nonce and some use cases for this uh that we thought of as you know you could do like auctions where you can have only so many uh bids per item or whatever bits per user um you can have a bulletin board system like Reddit where you can only have you know you could only comment once per post or you know once per level of post uh this we've also thought about using this for preventing denial of service attacks for something like Cloud Player or we can do that anonymously and decentralized and then uh so this is also used in a couple applications that the PSE has developed like uh skidder so like we have a we have ZK chat in there so it's Anonymous chat where it's rate Limited and so we have a JavaScript library for this we're working on a rush library in collaboration with the VAC team and we're probably gonna write a python version and a couple other bindings for languages um yeah so if you want to look at the documentation or the code there's the QR code and if you have any questions you can join our Discord again I'm at heart engineer and yeah thank you [Applause] thank you so much we have still one minute and a half maybe one question the audience some q a any question raise your hand here hello uh how does this compare to suppose if you like use Zaxby's approach where you keep one notes and you keep countering the notes and you keep including encounter and you prove that it counters lesson um some some number hmm um I don't know I'll have to talk to you about that offline I'll have to think about that for a minute okay okay any other questions all right right Plus thank you [Music] [Music] hearts [Music] [Music] foreign thank you foreign [Music] talk please let's Welcome Way Dai from research part it's a research partner from Bain Capital crypto he's going to talk about thresholds f h e for blockchains whoo [Applause] mic check awesome all right so my name is way Dai I'm a research partner at bank cover crypto uh we're a VC funds crypto native uh anyways um Okay so what I'm talking about today is uh not about zero dollar proofs and let me open by a question raise your hand if you think it's your knowledge solve all the Privacy problems in the blockchain space I see one hand I see two three all right well if you didn't raise your hands maybe um well maybe you know better than I do but I don't think zero dollars of all the Privacy problems so let me explain why so um let's think about blockchains so what are blockchains and computation right uh blockchains applications uh everything is a computation and a blockchain right so ethereum is a public State machine that's replicated uh we have some consensus mechanism and so what is it what is the same machine it's same machine is something that uh keeps a state and has a transition function and transition States through transition function right and more computer same machines blockchains and smart contracts right so everything on ethereum all the smart contracts are State machines okay what if we want to do the same machine transitions uh with privacy well let's try to use your knowledge how are we going to actually go to sleep machines or knowledge well we're going to execute the off chain and only prove the execution that is correct on chain right so I'm not going to show you the details here but more or less is going to look like this picture where every single States you know it's a different color and your zero knowledge update is going to essentially link one color to another color right so here on chain will have three different states as opposed to the user want to interact with the state machine and uh they each independently enjoy a different proof right linking state two to some state three now if one proof is processed on chain let's say the the left one the blue one here what happens is that the second transaction of this of the second user is now invalid right because it's the proof um is going from the same like state two but now you need to link to state three right so that's the problem with executing General State machines in general knowledge blockchain you range of race conditions okay so this problem is actually being observed over the years in you know many different ways but basically um you know the the store is at either you do a transparent engine computation which you know that we do on ethereum so you have no privacy but you have the you know benefit of a share state that everybody can have access to or on the other hand you have a zero knowledge of chain execution uh and you kind of have the flipped uh you know uh benefits and and kind of downsides which is that you have private inputs but to State that's not shared between anyone okay so uh we're basically someone like us with you know these two properties so we can't really get both and my talk is about how to get both properties and I call that um a confidential share state so how do we get confidential share State on the blockchain and the answer is fully homorphic encryption or fhe for short okay so what is homophone encryption um this is you know brief intro to cryptography uh so um so this thing is here is kind of community diagram okay so suppose there's a message and fix a circuit see here we can compute the circuit on a message to get C of M so that's this error over here okay um fhc scheme allows you to kind of do this thing but with three different steps okay uh and so first let's take our message instead of computing the function I'm going to encrypt it okay so everything that's done here is encrypted you can now see what's inside right so uh you know this is what kind of go through in your TLS channels and stuff like that right you first encrypt it and then you kind of run this evaluation function which takes in the circuit right and this stepper text to get another set for text okay um we'll have you down here well we haven't really done anything useful right but let's actually try to decrypt this thing again right uh if a PhD skin allows you to essentially so if you decrypt this and you know and these things it's you know it actually works it means that you actually get back to the resultant computation okay so what have we done here instead of doing computation in the clear we can first encrypt do the computation and then decrypt Okay so so this thing is like really really sick right because um it allows you to do computation encrypted data it's like you know the Holy Grail of you know Computing uh and privacy right um so you know this thing has been discussed ever since like the 80s um but like you know recently we like only within the past about 15 years do we know constructions of this thing okay so there's still a single point of failure here which is the decryption key right so that anyone holds a decryption key can get the data so the idea is we're going to like move everything Unchained to be encrypted and stuff right so but we don't want a single person to build decrypt so what do we do we're going to add um thresholds cryptography we're going to SQL share the description key to every single validator okay so now I'm gonna have to guarantee that you can only reconstruct what's inside if um if k out of the N validators say so okay all right so all right so what can we do with that uh you know with such a virtual Effigy scheme uh it turns out we can basically uh you know do confidential same machine replication on chain and release any information uh you know that we want to the public so let me kind of wrap up by telling you about the work of Pesca uh so Pesca it's uh well short for privacy enhancing smart contract architecture which is kind of a blueprint to unify the three types of computation right so we have transparent online computation zero knowledge option computation and this on chain replicated computation computation let's go it's a framework that I try to unite the three things now putting the three together um so very briefly on what kind of what the challenge is here so the challenge is that if you want to make this in Practical uh you have to really really think about efficiency as it so it turns out FH is really really slow and uh is way slower than ZK even so you want to push computation to zero knowledge as much as possible while doing you know the bare minimum amount of like shared computation in fhg okay and uh I'm going to kind of connect the two components using virtual decryption so this is going to look at look like this where on the left hand side we'll have some type of like DK shooter tool which is three standards we're going to also use zero dollars to kind of encode inputs into this apigee computation module on chain and then we're going to decrypt stuff and then put it back into the you know through the pool um so this is actually inspired by the penumbra State model I encourage you to kind of look it up it's it's actually one of the cooler projects that's you know being implemented right now that have that that's a private X okay anyways um and in the work we also extend this to uh Tucson Publications uh one of which is uh essentially a dark pool a privacy preserving constant function Market maker and the other one is uh first first price privacy preserve auction okay so let me in by a short conclusion uh which is uh you know what are the three points I talk about today privacy um is you know privacy it's really important in blockchain space and zero dollars for solution and to get confidential share States we need something that's fancier and in this talk I presented one option using apigee and encourage you to go check out the work of best gun and the link to the paper is in this QR code foreign of applause may we have one question anyone that they didn't raise their hands everybody understood that already after the talk thank you so much wait once again [Music] all right [Music] boys [Music] all right Welcome to our next speaker I see so many names on the screen I have to guess is the one in blue uh please turn your mic on let's check on testing one one there you go okay yeah the last one so Mikhail that's how you pronounce it cool now try with my family name huh close close what's the proper way Zions okay so big Runner plus to Mikhail is going to talk about vampire a novel chip to verify seeking snark yeah hi guys I'm really excited to uh to present you uh our new music a snark uh it's uh it's a joint effort with uh with two other guys with helger lipma and janosim from simula so um we in our paper we presented a new Zeke snark uh called vampire it's uh it's a ZK snark for arithmetization called as r1cs light it's Universal and updatable that means that you can have like one trusted setup for for every circuits up to given size and if you don't trust that the trusted setup was generated correctly or non-maliciously you can always update the setup by your own uh it's it's based on Marlin another ZK snark but it's like really highly optimized I would say it's a Marlin on steroids uh it's uh it's so optimized that the proof is only four group elements and two field elements like compared to Marlin which has like seven group elements or Planck which also requires uh seven uh seven group elements so communication wise uh this ZK snark is really close to growth 16. so the difference in the proof size is about 500 bytes uh but graph 16 is neither Universal nor updatable uh and it uses only two subjects and why I want to talk about some checks is because this optimization was possible thanks to the fact that we uh we invented a new subject which we adapted count it's a univariate some check it has great communication efficiency because the proof for the subject is only one group element furthermore the complexity of of this subject is really uh really good it's almost linear what is much better than a standard subject we use in snarks called Aurora which which requires ffd okay so um let me tell you how to make uh snarks shorter like how it was possible to um to to do to deliver a snark that is only four group elements long um so snarks uh like a class of snarks we we are we were investigating uh has a number of of building blocks so so first there's a relation so we need to to pick What relation for snark we are proving and for example the relation could be r1cs it could be R1 CS light in our case long hair has its own relation so arithmetization matters the ultimatization also uh tells you well uh the proof size really highly depends on the arithmetization you pick then we have a subject so basically the question is how many subjects do you need to show your result then we have lean check so the question here is do we even need a lean check and the last but not least the famous polynomial commitment scheme which also like defines the the size of the proof so we need to tell which polynomials are we committing to and what polynomial what commitments do we do we open can we batch openings can we batch commitments so to compare to compare Marlin with our vampire so there are some several differences so first of all Marlin is for r1cs while vampire is for r1cs Lite and why it matters I will say uh in a bit in a few uh in a few slides then we have a subject both Marlin and vampire need two subjects but but Marlin uses Aurora some check and the vampire uses count and Aurora uh regarding clean check martinin acquires it we don't then there's a batching of of kcg a cut a polynomial commitment scheme so in case of marlin it is batched but we managed to batch it better okay so let me tell you about the difference between r1cs and r1c as light so we start with arithmetic circuit as usual and and we we have some left input gates in the circuit we have some right input Gates we have some output Gates and these gates are encoded into three matrices so Matrix a corresponds to left input Gates uh B corresponds to to the right inputs and C corresponds to outputs and then we have Z dot corresponds to evaluations on these Gates okay and that's r1cs light and the fact that we have three matrices basically means that we need to send free polynomial commitments to each of these multiplications here while in a r1cs slide the situation is a bit different because we are able to encode the whole circuit into a single Matrix so we have only one Matrix that need to be need to be sent and we want to show that that the vector with evaluations times this Matrix gives gives zero okay so uh let's look at this uh for a moment so so um W Times Z equals zero basically means that for all X's uh sum over y of w x y so the elements of the W Matrix times element of a z Vector equals zero and with some mathematical trickery we can say Okay so let's not talk about vectors let's talk about polynomials um and let's talk about some of some polynomials some over some y for some polynomial f and here comes the subject so count is a new subject uh so so subject argument basically states that if you have a polynomial f and you evaluate it at some elements age belonging to a set H and to sum over all these elements then you get some value V and compared to Aurora as I mentioned I Aurora needs two group elements we need only one group element so I'm not going into details of of this subject maybe maybe some other time so let me only say about batching kcg very quickly so there are like two uh two ways of batching polynomial commitments first of all we can batch openings of multiple polynomials evaluated at the same point and that's what Planck is doing that's also what what was done in Marlin however due to some new results we can also batch openings of multiple polynomials at multiple points if you want to learn more please check our paper and now I can take some questions thank you so much a lot of data and formula over there but if people can talk to you after the talk thank you so much [Music] foreign [Music] foreign Network please speaker enough Applause to Yannick this loving hack thank you hi everyone thank you all for coming um yeah my name is Yannick I'm the shutter Network and I'm going to talk about today how we can use threshold encryption and apply it to shielded voting um I was not supposed to do this uh actually my colleague ULU was supposed to have this talk because he's actually done all the work but unfortunately he got covet so I had to jump in I hope I can do him Justice um I want to start very abstract to make it a little bit fun uh what does shutter do it basically allows users to safely reveal private information that they have to some sort of system meaning that netteca who gets hold of that information can't abuse it or can't use it for anything malicious um so basically we want to go from a world like this where we have monsters that look at uh in monsters in the forest but look at your private information and do bad stuff with it to something like this like a more Heavenly View where everything is safe and uh nice how do we do that we use threshold encryption which allows uh basically social encryption we have a set of nodes that provides an encryption key and then a user can now encrypt their message using that that key and then it's protected from everyone that no one can abuse the data and only when it's safe to reveal that information then this network of nodes will produce the decryption key um and then everyone can see the message um what is safe of course depends highly on on the application and here we want to talk about voting and governance and this sort of stuff basically what we want to do here is we want to encrypt votes until the voting period is over so that voters don't see how others how other people voted until yeah until they can't vote anymore it's very interesting because in Dallas this is at the moment not the case in endow voting usually you see all the other votes and they can influence your decision whereas in traditional votes like if you vote for a parliament or in an election this is not the case and there you only start counting when no one is allowed to submit votes anymore this is very good reasons because otherwise you get some sort of lots of undesirable behaviors for example just three here voter apathy if you see that um right in the beginning a big voter uh submitted lots of votes for candidate a then maybe lots of people small people with only a small voting power that would want to vote and be on candidate B might not do this because basically um it's probably already over um we've lost but if they would not have had this information everyone would have voted then they might have actually won against this one big player and this is actually a big problem right now in Dallas in our voting another example that we might want to prevent is strategic voting so if you for example if you have a vote were two candidate that's convince and your primary candidate that you want to win already has like lots of votes then you votes then you might want to vote on your secondary candidate which is not what the voting mechanism is intended to encourage so that would we also prevent if you can't see what voters do and last example buying of votes it gets much harder if you don't know how many votes you have to buy in order to win out of scope is privacy because after though it's over everything will be public foreign yeah we're very um proud to collaborate on this with snapshot um if you don't know snapshot is this big Dow voting platform basically all dollars use it and they're pretty cool and since we are starting today basically they support now shutter as a private voting system so that was that set up in votes they can use a shutter to make their votes more secure their elections more secure how does it work the life cycle of a vote the shutter system starts by generating a long-lived eon key it does that once and then if a user wants to vote on a proposal they use that yonkey and the idea of the proposal and derive an encryption key from these two pieces of data for this no communication is required it can be done purely locally once the proposal is finished um basically then yeah the user sent encrypt the votes send it to snapshot and once the proposal is finished the snapshot will send a decryption request to the shutter system shutter will then generate the decryption key because when it is safe to reveal the the votes because no one can vote anymore and using that decryption key shutter snapshot can decrypt and count everything um some screenshots here oh um how it would look like basically if you set up a vote you can select shutter it's purely optional if you do that during voting you see nothing basically you only see the number of votes who who already a number of people who submitted votes already you don't see how many votes LS got you don't see how many votes Bob got and only in the end when no one can vote anymore you will see that in this example Alice is one and Bob didn't get any votes um yeah and as I said this is starting starting from today it's live you can try it out um we do have some other things at uh at Chata we do using this mechanism I hinted it but it's quite General and it's mostly about mbva related and since I have one and a half minutes left I want to talk briefly about this basically we use this Russia encryption mechanism to prevent front running and malicious Mev our main focus here is is roll ups so we our product is called rolling shutter that can integrate into Roll-Ups as a kind of a plugin but we're also looking at Beacon chain at once there we have for example a beacon chain proposal but one the research side um yeah that's basically it um please try it out it's uh it's still of course early there might be bugs but it should work um it's live um you can scan this QR code if you want to um uh try it uh follow um us on Twitter project shutter also snapshot Labs or collaborators and ulupi who's the guy who was supposed to be here and my name is another Yannick um yeah and of course this is uh all part of brainpot so the company that does a shutter is called rainbow and we have lots of other projects including Beamer rain trust lines how damp lots of people here um yeah thank you we still have time so if you have any questions remember that lighting talks are like seven minutes and we have three minutes for questions so we have our volunteers there you go please one second because for the streaming thank you Nicole is it is it obligatory to uh to have the the results shown at the end of the voting with with snapshot like on a per voter basis or do you just show the aggregate results so in this mechanism to be honest I don't really know what what snapshot shows but in principle all votes will be public afterwards because we encrypted votes are public and the decryption is public so you can find out who voted for what that's part of what why I said privacy is not in in scope of this for this you would need other mechanisms if you want that one short question mark yeah just follow up are there plans to introduce privacy into the setup um not with this I think threshold encryption basically ends there I don't think it's it's possible for this you need some heavier cryptography thank you very cool all right big round of applause for Yannick thank you so much thank you [Music] [Music] thank you [Music] well we're getting to the end of the day three please welcome to Gabriela Chang from ethic Hub he's going to talk about Leon Bank what a compelling title a real blue ocean opportunity for crypto people hello everyone I'm Gabriela Chang CSO and co-founder of Epic Hub I'm an industrial and product designer and artist and writer and I work for the Secretary of economic development in Chiapas Mexico and I was also an organic coffee producer I'm thrilled to be here to talk with you about the amazing opportunity arising for using web tree to solve one of the most pressing problems in this world the unbanked farmers I'm here to remind all of you that blockchain initials promise was to enable a financial system where no one was left outside excluded shocking data according to the World Bank a quarter of the world population is excluded from the traditional Financial system so this means that the only source for working capitals for nearly 2 billion people is shark launch and they often have to pay uh over 100 interest for the little money they can borrow within their communities bad news from these two billion people 70 percent are smallholder farmers living in poverty despite producing a third of the worst Food our food they live and work in very fragile ecosystems we all need to protect for our own sake money for these tiny Farmers is scarce and outrageously expensive preventing them from keeping a benefit after paying the debt also the annual Financial need accounts for almost one trillion dollars annually they are not eligible for microfinancials micro credits because they are not able to pay every week or every month this is known as the poverty cycle they are good payers there's no record to prove it um so quite often risk is perceived way higher than it really is for an example the global Benchmark for grouper loans grupal solidarity groups is the the default is less than three percent there should be people queuing to invest in these farmers this enormous dysfunction is also a blue ocean opportunity to build a better economic system leveraging on blockchain and crypto as Catalyst for True development the global Paradox of the economy inspired us to connect complementary economic regions to benefit both parties in a win-win Arbitrage where lenders and Farmers become partners that no longer pay abusive interest for loans or negative rates for the savings through our landing and staking platform almost anyone in the world can invest into cure projects and earn a return while generating triple impact social economic and environmental fintech approach student Banks usually is focus on data to do the credit scoring or in in assets to Atlas collateral but small farmers live in areas with no connectivity and their small lands are of no commercial interest so for them it have designed crowd collateral connecting D5 to the productive economy of the real world building a most resilient model where Jill doesn't come from derivative uh sorry this is welcome Financial derivatives but from real Agriculture and this is Jill Farmers back in the launch of actual farmers the key is to leverage on crypto to develop the incentives to create the proper incentive Loops of the goal we built an International Community where all stakeholders benefit from interacting our mission is to improve the living standard of the smallholder farmers through their own productivity why because there's no future without sustainability and it's a huge business opportunity to leverage on blockchain on web3 to replace and pray internet and inefficient supply chain transforming scarcity and inequality into abundance and through development we are one of the most awarded and recognized projects within the verify the regenerative Finance space they say web true it's about competition and web 3 about collaboration we need to collaborate in order to solve the huge problem the planet is facing that most remember this most of the greenest areas of the world is where the unbanked lived so we need to secure their well-being to secure ours we need to stop thinking Financial inclusion it's about solving others problems we need to work together using blockchain and crypto to bring back balance to humanity join us it's a good business to do good thank you thank you for the optimistic magic at the end we still have two minutes for questions so please raise your hand our volunteers will bring the mic for you now we need that for the streaming sorry oh hello hello hey um I have a question what's what's the biggest roadblock to specifically ethic Hub scaling massively I know you've been around for a while so I'm just wondering just you know how do we scale ethic Hub and projects like it to reach more Farmers all over the world right and obviously sort of with nuances Etc but what are the roadblocks true collaboration we started in wearing this space since 2017 we launched the platform in 2018 and it's very hard to grow with the available resources the way we start from from zero from scratch and the challenge is to bring together all the actors we are already working we are raised attending over 500 families we are in three countries and we are growing but the challenge is to bring more actors to the ecosystem we were by Links of trust by Circles of trust and theory of games so we are working with web trust here and we are actually having the partnership with Ika which is a an Institutional entity The inter-american Institute for cooperation for agriculture and we are already already doing some other Partnerships uh in order to access the farmers that are already selected by these trusted entities and from there go down the Pyramid of the social population thank you shout out to Gabriela gracias if you have more questions you can contact me later thank you very much thank you foreign [Music] foreign [Music] we are coming to the end of the day three the good thing about this last talk is that we can have more time for Q a so please welcome Pablo largia from Sensei Nook of father and CEO he's going to talk about client diversity why it matters welcome Paulo okay thank you hello how are you everyone thank you for joining us um so I'm Pablo as Juan David said and I'm going to talk about client diversity so um this is the clearest image of the universe uh that it was taken by Telecom web like three months ago so imagine if instead of this the universe will be like this with a lot of black hole doesn't make a lot of sense right well that's that's exactly how we humans collaborating in the ethereum validation of the network are working together like uh most of the notes uh worldwide are in Europe and in the US and very few in other regions so for geographically decentralization is super important from all of us to to make sure that we collaborate and make a better Network together if if you go to ethernotes.org you can see that in the top 20 countries you know number one is the US and there's no single country in Latin America or in or in Africa uh as a stop as top 20 in in notes in the in their regions then we have a second centralization point which is with client as you know most of the notes are hosted with a get and prism in execution and consensus clients which is also a kind of centralization so first we have geographically centralization secondly uh client centralization and then we have the Amazon centralization most of the 50 of the nodes were wide are hosted in Amazon web services so again at the end of the day we are building a decentralization network super centralized in this case in Amazon so it is the third centralization point and then we have the fourth one which is uh the providers of staking worldwide as you know most of them are concentrated um in Lido and centralized exchanges like like uh coinbase Kraken binance so again uh we think there's an opportunity to to change all of this together um so this is a little bit what is going on right now um in terms of of low deployment worldwide so um my talk today is to bring awareness and consciousness of how important is to really think when we stake uh to take to take care of these four different pain points and what we can do is make conscious choices like make sure where you stay your eat you know if you if you can do it with a non-custodial solution a centralized solution where are the notes of that node operator hosted um Diversified jurisdiction is also super important as you know what happened with tornado cash a couple of months ago in the U.S so again diversity is so important here also with hosting providers and execution um and consensus layers so what we are doing uh last minute what we are doing from Sensei node we are the first and only node operator in Latin America so we just launched a smart contract uh a non-custodial solution where anyone can deploy a node in the region so basically you just connect with your metamask wallet and you can send 32 it and have your node uh and in Latin America helping diversity of geographically diversity also data center diversity in Latin America the data center industry is super fragment which means that there are a lot of local and Regional providers so we work with all of them to make sure we promote decentralization so with that smart contract a certify of deposit we are giving an nft where you have your 32 each plus all the rewards generated and it's also the liquidity that can be selling any nft Marketplace so this is our approach to decentralization and how we want to help um so yeah so now thank you very much and I'm happy to be here thank you Paolo we have three to four minutes for any questions why is important diversity client diversity geographically and in general uh good day I only wanted to know that you can't you unite all the validators to the light or coinbase because light is different validators and coinbase uh uh don't share who is the sub workers for uh validate the essay on two notes so on your slice it slide it's like 29 percent mostly it's like more than 30 different validates there inside light or you say it's not the light it's like Unity of independent validators yes they uh that's correct but you're talking about live in particular that they have 28 different node providers yes and coinbase have the same situation only they don't have uh clear information on the public sites yeah in the case of Lido yes they work with different no providers but there's a single point of failure in the smart contract potentially yeah but you okay okay but I think that this screenshot that is on your presentation is not correct that is only my you know which one this one yes uh is taken from huh dude sorry so well you can yeah you can tell Julian that it's not a correct slide but it's over there hello oh nice hi thanks so much for uh this talk I wanted to ask you who are the sorts of people you're finding in Latin America that are beginning to run nodes for the first time are beginning to stake and what sort of um is this in any way helping with adoption or is this mostly like for technical people um and I asked this question because I'm trying to do something similar in the Middle East thank you you're welcome um so nowadays as you know worldwide there are many node operators but we are the the only one deploying notes in the region and also uh for centralized exchanges in Latin America we have a lemon bits or repeal and they do not offer staking Services yet okay so that's also a constraint because most of Latin American users are on those exchanges and they start the second service is not in in their in their wallet or exchanges so I think this will change in the next six months eight months probably when when the when the the exchanges that concentrated most of the Latin American uh people uh will be available to stake one more question we still have time those are the periods to be the last one they can go a big upper the clock here hi and I would like to I would like to know um how every time if you're a smart Contra being outdated for security yeah we have been through two audits one of them with coinspect and yeah and we are we are launching next week this is smart contract so we are just starting with it yeah coins back this year our auditor foreign s okay all right Pablo thank you so much we need to improve that map to have more points below absolutely thank you you're welcome big round of applause for Pablo thank you thank you [Music] [Music] all right [Music] [Music] [Music] foreign [Music] 