[Music] [Music] [Music] [Music] [Music] [Music] [Applause] [Music] so [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] so [Music] [Music] so [Music] [Music] [Applause] [Music] so okay great i believe we are live and i'm asking people on youtube if you're streaming in uh to just send something in the chat that says you can hear us i'm sharing the agenda both in zoom and on the youtube um and this agenda is largely divided into two elements um first we're going to go over just general sharding design and some of the awesome cryptographic primitives that are used in this design with dark grid and vitalik then we'll have a short break and then we're going to go over kind of the current vision state of prototyping um and separation of concerns and the eth2 merge um i'll give a little bit of an intro there then mikhail and guillaume are gonna take it over uh there's a few goals of our workshop the first and foremost is to educate on these two kind of large upgrades of the beacon chain that we expect to happen over the next couple of years to facilitate more collaboration and development on it pretty much the beacon chain is launched there are a couple of big things that we want to do with it one is for it to become the new home for eth1 ethereum and to facilitate some massive scale through what we call sharding uh both of those are um kind of on the cusp of uh our r d entering into the d side uh it's definitely time to get more engineers more minds to refine specifications get prototypes out and ultimately bring these things to mainnet so that's our goal today is to kind of expand who's uh aware of these things who's involved these things and facilitate a really exciting year collaborating on both i need to give shall we host permissions give me one second great so that is done um so this is going to be a mix of presentations to um that people prepared to help educate but in the nature of like a workshop and the nature of an educational thing rather than like some sort of conference um i hope and have encouraged speakers to pause ask if there's questions at various points throughout so that we can more dynamically engage with the information and um part way through some of the sessions and definitely after each of the sessions um opened up for for more general conversation um but don't be afraid to like raise your hand drop a question in the chat or um wait until one of those moments when the speakers ask if there's any questions and if there's not any questions i'll probably ask questions to keep this thing moving um cool so i think that's all in terms of the the structure today it'll be a mix of presentations some dynamic discussion um and otherwise thank you everyone for coming uh i'm sorry that we again are all hanging out on the internet rather than in person but it's really really good to see everyone show up and see all these nice faces um with that i'm going to turn it over to our first um part one of today the sharding design and the cryptographic primitives i believe donkrid you're going to start us off right okay turn it over to docker thank you all right um thanks danny right can you all see my screen yes yes okay great um thanks danny um so yeah so i am uh mainly going to talk about um kct or kate commitments um because that is an essential ingredient on how we're going to design charting in easter and um basically um the way to think about it is i guess gave the subtitle um how to hash polynomials um right so this is the outline of what i'm going to talk about um i'll start with some motivation um why in data availability sampling we we need polynomial commitments why they are useful and then i will quickly go over finite fields just so that yeah anyone who is not completely familiar with it kind of knows what we're talking about and then i will talk about basically a simple construction that you could think about to hash polynomials um that is broken in some way but very naturally leads us into kcg commitments and proofs and then finally i'll give a quick overview of like where you can go from here and continue and um please feel free to ask questions at any time this is like for your benefit to understand it so if anything's not clear um please just ask ask about it all right let's start with the motivation so data availability sampling the basic idea is you have got this blob of data that would be a block on an eth2 chart and you want to be sure that the data is actually available without downloading the whole data obviously like when you can download data then you know it's available but um that's a bit too much work so we want to do this by only sampling some part of the data by doing random sampling of data chunks basically and the the advantage of doing this is that this does not depend on the honest majority so for example an alternative data availability solution is you send a committee to download the full data vote on whether it's available that's actually what we're going to do as well but the problem is that only works is that the majority of that committee is honest and that again only works if as as a very minimum the majority of validators is honest and data availability even works like data will be sampling even works when that is not the case in order to be able to do this we need to do some encoding of the data so-called erasure coding because if you just sampled the data itself the random samples then it's if if an attacker is say just hiding one of the chunks it's very unlikely that you're gonna hit that one so it's easy to hide small amounts of the data and we want to avoid that okay so how does erasure coding work um we say we extend the data using a read solomon code which is a fancy name for polynomial interpolation and what we do is we take the original data so i like port four chunks i've drawn here and then you you find a polynomial well the polynomial that goes through all those um chunks of minimal degree so in this case since there's four chunks that would be a polynomial of um degree three so ignore the five here that's somehow from an old slide um so the polynomial would be of degree three and in this case um since yeah we know that we can always find with with any four samples of or points of a polynomial of degree three we can always find the polynomial we know that any 50 of the chunks are going to be enough to reconstruct the whole data so it doesn't matter which which one we have we can have two original data to extension or oil extension anything all of them no matter what what you get in your sample if you have 50 you can reconstruct it and so now we have changed the game like basically now we only need to be sure that 50 of the data is available rather than that that everything's available because we know if it's 50 if 50 of the extended data is available then that's enough right and now that problem you can solve using random sampling you've converted it into a problem that random sampling can do because if you take 30 samples um and if all of them are available then the probability that less than 50 percent is available is very valuable right um fun fact erasure coding is how they do uh make cds skip resistant yep that's right um okay so so let's now think about like if we just committed um to the merkle root of this um data and extension right um then there's one problem like that would be the natural thing to do right um that's how we usually commit to large amounts of data we commute the merkel route um but the problem is how do we know that the data and the extension are on the same polynomial of degree 3 in this case right so here we illustrate what happens we have like the original data and the polynomial extension but this attacker they just make up anything for the extension right and then you don't like you can't reconstruct the data from the extension because it's a completely different polynomial or like the whole thing is not any sort of low degree polynomial that can be extended from a small number of samples um so what we would need is if if we if we did this using merkel roots we would need to introduce a fraud proof in case it's incorrect so someone would have to collect enough data and send it to everyone to show that someone distributed the incorrect route and the the downside of that is basically that fraud proofs add complexity and up to the protocol obviously but also more annoyingly they add these um synchronicity conditions that um yeah like you you if you have poor networking or anything like you might miss the workproof and therefore think that the wrong chain is valid um so the question is what if we could find the kind of commitment that always commits to a polynomial that would be great then we wouldn't have that problem okay then now i'm gonna talk a bit about finite fields because we we need them and um i'm not sure if everyone is completely familiar with them here maybe are there any questions on the um previous stuff before we move on yep sure happy to take any questions could you use a um a zero knowledge proof to prove that it was a polynomial extension right so this would be another possibility um we could construct the zero noise proof so we could compute the smoker route and then we could afterwards since this is just the computation right the input is all the data and extension elements and um and for example in addition you could give the polynomial like the coefficient of the polynomial that they are on and then inside the their knowledge proof compute this root so that's possible um it's just that it's currently computationally infeasible like it's very it would be a lot of work that just realistically can't be done in the space of time right in a few years it'll get easier once we have more established arithmetically friendly hash functions so like i'm definitely expecting that in the longer term it will be easier to have uh snark silver merkel roots and eventually and eventually stark silver miracle roots and like when once quantum computers come out that's something we'll have to switch to anyway i mean one thing i was wondering about was um what's the thinking with regards to uh an interactive protocol versus just broadcasting the information for example some others that we broadcast stations oh you mean an interactive protocol for data availability well it's it's very very difficult to do uh well it's impossible actually because the problem is you cannot can never prove that data is not available like you can always whenever someone challenges you hey this data is not available you can just show it and say like it's here all the time it was here all of the time you can't prove that it wasn't there basically and if it was in terms of the um like dynamic requests for sampling versus sampling on broadcast uh vitalik is i think going to cover that in his portion right but that is that is a good uh point yeah like i mean you can always like it's basically probably like very intuitive that you think oh it's just like this challenge response thing but there is actually not really any solution in the space for data availability um i i think for an intro maybe you can explain why we need this for ephraim 2 and not we don't have this for ephraim 1. we'll do the explanation for well the explanation for that was going to be the first slide of what i'm presenting on then let's do that great cool okay let's get into final fields right so i could give um a very mathematical definition but i don't think that's very useful i think the best way to think about finite fields is you all know the rational numbers um usually denoted by this double bar q or the real numbers and the complex numbers and we all know that we can do basic math operations with them like we can add them we can divide subtract them we can multiply them and we can divide except by zero right there's always one element and exactly one that we can't divide by and that's it well otherwise we can always do this and then they are like the basic laws associative commutative and distributed and i don't need to go into these like just think about like you you have the same laws as if you were using rational numbers um the difference is that they are finite so um unlike the rational numbers and all the others they all have an infinite number of elements and finite fields only have a finite number of elements and that that is important because that means we can actually represent them all using the same number of bits like if the yeah um so for example for the modulus we're using in the uh in the elliptic curve in bls 12 381 it's always 255 bits whereas a rational number for example could grow grow very very large um if the numerator and denominator they can both be any integer and we know that we can't represent all integers using a finite or the same finite number of bits they might have to grow and and then the best way i think is like to just look and add an example so i'm going to look at f5 which is basically the finite field that consists of the numbers 0 1 2 3 and 4. and the idea is that for every operation that we've talked about before [Music] we to to compute the result of the operation we first just compute it using integers and then we take the remainder after division by five and this remainder is the result of our computation so for example if we do something like um three plus four right then that's seven and seven divided by five um is one with remainder two so we say that that's two like in in our fields we say the result of that is two and then do another example like three times three the result is 9 and modulo 5 the remainder modulo 5 is 4 so we say that that is 4. and now there's one thing missing um [Music] if you divide obviously you can't you can't do division in the integers generally um and the way you do that is basically um i've made a little table here that shows that for each um for each element zero one two three four um you can find an inverse an inverse means that if you multiply it by that element so for example for two it's three then you get one so two times three is six um and six modulo five is one um and um and the division basically just works by multiplying by that by that inverse so for example now if i wanted to divide um 4 by 3 then i would do 4 times the inverse of 3 which is 2 which is 8 and so the outcome would be 3. and that works because 5 is a prime number basically if it's a prime number then we can prove that there's always an inverse um like each each of the elements will have an inverse otherwise that's not true right and we can we can do this for any prime so this is just an example to show how it works in a small field but in practice our primes are going to be huge i'm going to have hundreds of bits okay based on this i'm now going to talk about um hashtags of polynomials first a quick reminder um about polynomials so a polynomial is an expression of this form so it's the sum of some f i which we call coefficients times x to the power of i um and we call this fi as i've just said the coefficients and the degree is basically the highest coefficient that we have in the polynomial so um n um and each polynomial defines a polynomial function that's basically the function that's defined when you replace this x by by values in the field and then just some some facts from basic math so for any k point so if you're giving given k pairs of values um x and y uh where all the x are different then we can always find a polynomial of degree k minus one or lower that goes through all these k points um and any polynomial of degree n that is not constant has at most n zeros okay these are some basics about polynomials and what we would like now is to have some kind of hash functions for polynomials so it's it takes it's a hash function that takes as an input a volume with f um and what would be great if it has some extra functionality so what would be nice if is if for each z so that is some element from our finite field you could construct a proof p of f and that that proves that h of that is equal to y and the proof as well as the hash itself would ideally be small and here's an idea for such for such a hash function which i call random evaluation so let's choose some random number let's say so we're still working in f5 right now just as an example and let's say our random number is 3. and now i say to hash a polynomial i evaluate it at this random number so i've i've given two examples here like i take f of x which is x squared plus two x plus four um i compute what that is if i evaluate that modulo five and you get that at four in this case and that's another example for another polynomial um so basically you just you just compute the evaluation and now i mean okay so like i mean we only have four possible outcomes in f5 but if we if we said the modulus had 256 bits then it's actually super unlikely that two polynomials would have the same hash right and because there are so many possible numbers um like 2 to the 256 is so many it's very very unlikely and this this trivial function has actually some very nice properties so for example here i get like and the first one h of f so if i add two hashes h of f plus h of g that's the same as if i first add the two polynomials using normal polynomial addition um and so that that has the same hash um and that's because if we just take f at the at three and add g of three like three is just my example here uh for for a random number um then that's the same as if i add f plus g and then evaluate that polynomial at three it's really like a completely trivial statement when you think about it and and exactly the same is true for multiplication right if we like multiplying two polynomials is again exactly defined in such a way when you write it out i'm just doing the term by term multiplication so that h of f equals times h of g equals oh that should be an h here h of f times g and that's again because when you just write it out what it means to actually evaluate a polynomial at a at a point then f of 3 times g of 3 is obviously f times g evaluated at 3. okay so it looks like this idea has some fairly interesting properties so that's nice but the problem with it is that you can very easily create a collision um if you know the random number like if you know that i'm going to evaluate it at a certain number then if it's adversarially like before we just assumed like oh it's just random polynomials and it works very well problem is um if it's adversarial if there is an adversary who tries to create a collision then they can very easily do that because like they can just modify for example the constant term so that the two polynomials have the same the same hash in this case so what we actually want is some way of instead doing this inside a black box what would be nice if we have we can put a secret number which i call s here as for secret and we could put it into a black box so i i denote this black box by the square brackets here and um and let's add also into black boxes the powers of that number so s squared s to the power 3 and so on and [Music] in such a way ideally like this is all kind of uh just imagining how nice it would be if we could do that um so that we can multiply ply it this should be by by a field element by a scalar um and adds so so i'll quickly add a correction on the side multiply it with another number so that should be possible and add two of them um but what we don't want is that you can multiply two numbers in the black box so quickly so what you want is for example that you can do some a times s which would go into the block black box so that that's just kind of um a s and what would be nice if you can do like s plus s squared [Music] so that you can you can create another black box that has s plus s squared so those are good what would be bad is if we could do something like take two black boxes s times s squared and compute another one and the reason for that why that is bad is basically that um that if that were the case then you could com compute also the inverse of s then you could compute in some ways using many many operations you could get to compute s to the power of -1 and once you have that then we're good because then you can use this not just for polynomials but also for fractions and we don't want fractions of polynomials we only want polynomials okay so that would be wrong yeah i was just going to say it's actually even worse because i believe if you did use fancy elliptic curve algorithms to discover what f is if you can uh multiply right i i probably yes cool so that's that's that's kind of what we would like to have and the great thing is that very surprisingly elliptic curves are the rescue here they provide exactly that so this is actually how you should think in the future of an elliptic curve an elliptic curve is a black box for one of these finite field elements each elliptic curve has a curve order so i call it g1 the reason for the one here will be apparent a bit later because we will need another group g2 and we say there's a generator which is basically one of the elements in there and the order is p um order means that p times g g1 [Music] is basically is zero [Music] and and to represent any field element inside um in fp you multiply the generator with x so x times g1 and um you can then you can you can multiply um so g is now here the g uh it's not d1 this is like any any elements in in the group and so is h so you can compute x times g for any element you can add two different elements like if g is an element and h estimate you can add g plus h and you can you can compute these linear combinations x times g plus y times h but this is the thing you can't do you cannot compute g times h you can't multiply two of them um and we introduced this notation um where we inspired by the black box so square brackets we just add the one here um which means that it's in this g1 for x times g1 and that's our black box so basically elliptic curves are just black box final fields okay and then we are ready to introduce kcg commitments okay so here um here's how a kcg or kartik commitment works so we assume that we have this trusted setup so we have the the secret number to the power of i for different i zero one if it's zero then it's just one so and it's just a one um uh up to some large number that's that's the length of the trusses setup when you have this in one and in the second elliptic curve which i'll come to a bit later um and we take this which we have a polynomial f of x and now here's how we compute the kcg equivalent we take the coefficients of the polynomial and we basically if you look at this formula we just replace x um by by s to the power pi so x will have i is replaced by s to the power of i in the square bracket so in the in the elliptic curve and now um when we when we multiply out this formula we see we can move those coefficients inside the box right we state before like um we want to be able to multiply something like in the box a times x equals um ax you can multiply by a scalar and then we have we have the sum we basically just use that x plus y equals x plus one um and so we can also move the square brackets outside the sum and then from there we see that actually this commitment that we can compute like this is just s at the point s inside the elliptic curve that's all it is it's just the evaluation of the polynomial at some secret random point but inside the elliptic curve and nobody knows what the actual point is and that's what we call the cutting commitment so basically we've now transformed our random our random evaluation scheme that we had before and that's completely broken that obviously doesn't work when there's an adversary and by moving it into this black box world where we can compute with black box numbers in elliptic curves we've turned it into something that at least gives us a hash for a polynomial at this point okay so that's cool now we need to do in order to do something with that we need to do a quick recap on pairings okay so um it's a pairing as a function um that we usually denote within with an e um that takes as an input two elliptic um curve elements from g1 and g2 and now here is the reason why we have to talk about this um g2 because um most pairings or like the pairings we work with nowadays aren't symmetric so we need two different elliptic curves for the first and the second element that you put into the pairing and the output of the pairing the output of this function um is in a target group gt so gt is a different kind of group but essentially you can still think about it as um as like some some group of the um of the order of the final field so it's basically just another yet another version like g1 is some version of the black box of the final field g2 is and jt is a third one unfortunately they all have to be different i guess like um well in some ways it would be nice if they were the same but actually would break down if gt were the same so we need we need all these different groups um okay and the important property of this pairing is that it is bilinear okay what what does bilinear mean it's actually it's very yeah it's it's it's kind of bilinear if you've ever done like a linear algebra course it's actually exactly what it means the same as bilinear only that the notation is very strange because in the target group at least we still denote everything in the exponents and so um so it all looks a bit weird but actually it is exactly what you have learned as bilinear and say linear algebra and so what it means we have this pairing of a times x so we've multiplied times z and we then we can take out this a and in this case we put it in the exponent so that's very confusing right what you would expect um traditionally from bilinear is that it would be here then we would all agree this is bilinear but don't get confused by this is actu it's actually still bilinear it's just a strange notation and that we use for the target group and the same is true in the second component well that's why it's bilinear in both components so like b times the you can again take the b out and put it into the exponent um and then the same is true for addition so we if we add two numbers x plus y uh and z and paired with c then we can we get um the pairing of x with c um um actually sorry i messed up my notation here this should be times in this case should be the product of the two pairings okay all right so this uh that is how pairings in elliptic curves work is basically it's about by linear map onto an elliptic curve okay right so the cool thing is when you when you when you look at that then um pairings allow you to do multiplications so we have this we take e of x in a black box and y in a black box and then just by applying our two equations so we take first the x out and what remains is a one because we can always say x equals x times one so we can take an x out put it in the exponent and then we do the same with the y and now we have computed this x times y in the exponent here um and so now we add the notation for the target group we add another of this black box notation and we just whatever is in the exponents e of uh one and one in the black box um that that that we yeah denote by this by this square bracket t t for the target group and then we get the nice x equation um we don't have to write the strange stuff in the exponent anymore where we get e of x y equals x times y so we just multiplied x and y essentially inside the black box uh we should probably i should very quickly mention i said earlier we don't uh want much that is possible to multiply like you might wonder now haven't we just introduced something where you can multiply two of these s right um where i earlier said uh we we don't want to be able to multiply two numbers in a black box um it actually turns out to be okay here because the output is a different kind of number that's why we need this target group we don't get a result in g1 or g2 but we get something completely new and that's why it's okay all right so let's assume we have two polynomials f and x and g of x and we committed to both of them using cutting commitments so we have a [Music] copman mittment 2f in the first [Music] elliptic group and in g2 we have a commitment so g of s to g of s become uh commit in g2 and so we notice that we can using the pairing we can compute the product of these of these two polynomials so using the pairing we can multiply two polynomials cool [Music] oh sorry same slide okay okay so why why is this useful well we need one more missing piece then we've got what carter proofs works how cardi proofs work so if we have that f of x is a polynomial and y and that are two field elements then by the so-called factor theorem it's actually a fancy name for a result that you can very very easily prove using long polynomial division if you want to check it google it it's it's a very easy proof um but basically it says if is this quotient f of x minus y divided by x minus z is a polynomial right i mean it looks like it doesn't look like a polynomial at first it's a rational function right it's a division of two polynomials which we call a rational function and it's a polynomial if f of that equals y okay and you can restate that another way like just multiply this equation by x minus z to bring it on the other side then there exists some polynomial q of x that fulfills the equation q of x times x minus z is equal to f of x minus y if and only if f of that equals y cool and that's how we get to the kcg proof so we if we want to prove that um f of the z equals y here f of z equals y then we compute this quotient q of x f of x minus y divided by x minus z which is a polynomial as if we are an honest prover that once that that wants to computer honest proof so we know there's a polynomial and so we compute um this quotient and then we send the commitments to that quotient and the verifier then checks that um that's e of f of f s minus y paired with one equals e of q of s so that's the the um commitment to this quotient s minus z if you look at this is that's a commitment to the denominator here right x becomes s that's the commitment to the polynomial to the denominator this is the equation that that the verifier checks okay and why is this true well so if you look at it then the right hand side it turns out is a multiplication um it multiplies these two polynomials as we've seen earlier right using the pairing we can multiply two polynomials so we multiply these two polynomials um so what we get is q of s times s minus z so that is a commitment to the polynomial right if we write this as polynomials again q of x times x minus z but that's uh that's just f of x minus c right so if this equation is true then that's f of x minus c uh minus y of course okay so that's if the equation is true then that is f of s minus y and and that is obviously the same as the very first pairing okay i think so this is this is really like the core of everything so i'll pause here if anyone wants to ask some questions i don't know that's clear but so the core of the argument is just um if if this like it's it's clear that if we can compute q of x then this proof holds right so we can we can just do the algebra as i just demonstrated and um and this thing two things the two pairings should be equal and the other way is like if if it doesn't hold then the problem is that somehow if the prover wanted this pairing to to be fulfilled they would have to commit a commitment here on the left-hand side to something that's not a polynomial and they can't do that because we have only given them um the positive powers of s so they just cannot commit to the quotient if it's not a polynomial can i just ask a quick question yes um what do the points in the target group look like because i know that the ones in g2 are like a field extension of degree 12 or something right no they are actually a fear the extension of degree 2 and the gt are the extension of degree 12. so the the second one is an elliptic curve um of the over a field extension of degree two right that's g2 in b i'm talking about bls 12381 here and the gt group is a is not an elliptic curve it's it's simply it is itself a field extension of degree 12. okay cool um okay i i plea please please ask any question if anything is done here this is really like the core the if you understand this then like that that's that's amazing because then you've completely understood kcg commitment and if not you should ask questions now are we the are we the first project to try to use kzg in production um well i would say i mean like at least many snacks um do use kzg in some way or like and i think uh the i think zk sync or doesn't it already use splunk which is based on keystone g yeah so plonk is definitely based on kcg um one i mean you could even argue that in in a way even even got 16 proofs like it's still kind of based on kcg although it doesn't use kcg explicitly like any yeah all prover schemes that use a trusted setup over elliptic curves essentially are in some sort kinda based on kcg so to be clear t is that element that you need to do the chosen setup for s s gotcha that's what's in the black box so the these elements so the trusted setup are these elements oops where's my pen again i'll just write s right s squared and so on um and t t the dt is just that's just a group that's just like that that's not that doesn't need a trusted setup okay great all right um if there are no further questions then so from a practical standpoint to bring these into production what are what are the things that teams need to be thinking about doing or kind of some expertise that might need to be built around here right so i mean i think like um conceptually like the the carti commitment themselves aren't very difficult like i mean once you have really understood this part then doing the basics um just uh checking say one cutter proof as it's done here or computing a commitment um like that that once you have the elliptic curve library which is the hard part um that that is very simple and anyone can do that um there are some more things that will need to be built out in the future but they are more relevant so actually like um technically speaking for the very first version of this um the only thing you will need to be able to do is to compute and check its commitment um so before we do data availability sampling which we might only do in like in a future version in an upgrade um you don't actually need to do anything elaborate uh for for full data available sampling you need to do like many more things which is like computing these multi proofs and stuff like that um and uh i think like we should yeah we should probably have another presentation about about that how to do this because that is a lot more technical and um and yeah would you mind for those that might be treating this more as like a black box and has certain engineering properties uh would you mind just like running over those real quick like the size of the commitment the size of the proof does the size of the proof scale with the amount of data they're trying to prove against it that kind of stuff um so i just i mean this is basically the summary slide so let's go all go there right so let's i'll just quickly go over it so um just this is just a recap so yeah you commit in the kst commitment scheme you commit to any polynomial using a single d1 element what's the g1 elements um so that that's that's like this um if you serialize it it's a 48 byte [Music] blob basically that and to take it back to the beginning commitment to a polynomial was commitment to block data divided up into points right right yeah um and uh and so when you open the the the commitment so points um as well as the value right these are both field elements so these would be in the fields um so that's uh fp and p has 255 bits so that that's like a 32 bytes 32 bytes and then the proof for that um is again just just one uh d1 element so again 48 bytes so you do not actually need like there there's no need to ever send any d2 elements for this and finally to check this proof you'd use this pairing equation so it's a very simple one-line equation e of f of s minus y paired with one is e of the quotient uh with uh paired with s minus c cool so we have a 48 by commitment to a block essentially or an extension of a block and then those that block is composed of points 32 byte points um and i can prove the inclusion of any of those points against that 48 byte root with a 48 byte proof yeah correct any of the points you can for any of the points on that so you don't need like a merkle root like to get to a point you need this whole branch right all the siblings somewhere here in a tree and you need to give all all the siblings in that tree all right and in the context of if i want to prove many points that we're committed to do i need many 48 byte proofs or can i just use one uh no so there are schemes and i didn't go into that because there are many many different themes um and i don't wanna and yeah i mean i think like understanding the basics is the most important here but there are school there are schemes where you can take any number of points down here and obviously you will need all the points and their values but once you have that then the actual proof is still only one element okay any other questions for donkey i think we're about to move on to vitalik and as i mentioned in the chat we're going from math heavy in the direction of engineering heavy uh so if that if your eyes glazed over on that uh i think you'll be able to bite into this next thing and if you enjoyed that um reach out to don great i think doncard and proto have a small group of working on some of the practicalities of implementing some of these things and optimizing them so if somebody on your team wants to dig in deeper uh reach out to donkey and proto okay and i've collected quickly some reading materials here but you can i'll just share those slides later so you can just take it from there thank you cool thanks cool um okay then i guess i will um start my presentation uh does everyone hear me and see the slide okay yep yep okay um great so to start off uh some key goals of sharding and this is just motivation for why we need to do uh kind of any fancy stuff at all and that used to one we don't need any tens of stuff in each one so basically the goal of sharding is that we want to create a system that can process far more transactions and specifically a system that can process so many transactions that you do not want to require any single node to process all of the data so one important nuance is that at layer 1 we're doing data sharding only so which basically means that the sharding is only just coming to consensus on blobs of data and on the facts and blobs of data and specifically about 1.5 megabytes per second they're worth blobs of data is available and it's not doing any kind of execution kind of of transactions inside of shards directly right now you might want to ask and what is the point of having this a data space and then there's a few applications like you can store cat pictures on chin uh there's enterprise applications that might require some kind of on-chain contra data storage of different kinds and then you would just have a kind of separate scripts that would they're going to walk over that data and finally a rollups right so uh basically with rollups what happens is that you need some kind of space where data gets published in order to get around the data availability issues and just make sure that the data actually is there so that some other nodes can come in and generate state or generate fraud or uh generate fraud proof um and you know i i've talked about this a lot in other discussions and other contexts and basically like even though there's no execution charting on layer one with rollups you kind of get execution charting on way or two and if we want we can add an execution charting on layer one at some point later but that's like not just not part of this phase of the roadmap right so here what we're just trying to do is we're just trying to come to consensus on uh well during each slots or during every 12 seconds blobs of data that get 64 blobs of data where each blob of data has a size of somewhere between a zero and a half megabytes so we have 64 shards and it's probably best to just think of these as indices so that is just numbers from 0 to 63 during each slot for every shard so for every index from 0 to 63 a proposer gets assigned the proposer has the right to to create the shard block at that chart for that or or at that slot for that shard and and a tester committee is um assigned so basically these set of values that are testing at a particular slot are themselves split into 64 groups and randomly assigned to these different charts um if there's not enough validators then there's uh the number of uh shards that actually i get a block during us while i get reduced but that's it you guys you say oh don't need to think about that for now so basically the what's going on here is that you have these beacon blocks and then you have these shard blocks uh and a shard block has a block body which is just a blob of data um up to uh sixteen thousand vision eighty four field elements in size so that just means up to half a megabyte um there's there is an eip fifteen fifty nine like mechanism in place uh so it would be targeted so that on average these things are half full uh but basically their size can be anywhere from zero to about half a megabyte but actually like this is not bytes this is field elements right so it says a list of numbers from zero to uh some really big long number whose digits i yeah i don't even remember but it's somewhere between two to the 254 into the 255 or or 253 i forget which one so block body is basically just a list of these the field elements the list of these numbers from zero to between zero and p um increments on the left side not the right side and um the header is a cape commitment to the blob um or sorry that's the cat is that the g commitment to the blob um yeah proof of size so just a proof uh that proves um how exactly what the size of uh the blob is which you need for the eapu 1559 pricing mechanism and the signature from the proposal right so it's a yeah fairly simple structure for a block you have the body which has the data and then you have a header which commits to the data and then also just proves how big the data is and adds a signature from the proposer and the block header the or the shard block header can get included in the next speaking block or in the later beat box now so shard block structure so this is the structure of the shard block as it gets sent over the wire um so well over the wire you would probably realise that we just send the the first end samples and that you would send all the proofs but basically you have a commitment which is that has they commitments to the entire block then you have the actual contents and and you can if you have the original contents then you can derive the remaining contents basically by just recovering the polynomial and evaluating the polynomial at some set of standardized coordinates and if you have any any of these um samples then you can use that to recover the full data and then here we have uh we also have this set of proofs and these are basically just the quotients uh that zek red talks about so uh you have one uh now one kind of implementation detail is that instead of having one proof for each uh single evaluation you have one proof for each set of like either eight or sixteen evaluations and like you can do um you can make uh kind of multi-um evaluation proofs uh fairly easily as i think red mentioned uh so the thing that would get sent over the wire is probably basically just the first n samples and from the first 10 samples you can theoretically recover everything else right you could from the first n samples you can recover what the polynomial was that i gave these outputs at those coordinates and then you can keep the commitment and then you can verify or and then you can generate all the proofs though realistically for efficiency purposes it might be better to just also provide the proofs as well because they do take a little longer to generate than everything else so this would get sent over the wire and then the a tester committee would download this data and they would verify it uh and the the attester committee basically their job is to vote on that of which proposal to include right so if there are more if there are two proposals or more which only happens if the proposer is bullishes then they get vote and they choose one you know if there's zero proposals then they just vote for the default zero proposal or if all the proposals that were published or are unavailable uh which basically means that like but when you when we see a proposal is unavailable what we mean is that like the proposal header exists but the proposal body just can't be found then they choose the default segment of zero proposal which just means that they just say there's nothing there and that's always an option that the tester committee has right so short blog proposer makes a proposal publishes it um on a subnet or publishes the body on the subnet from which is the header on the global net the header gets included into the next beacon block basically as soon as there's an honest proposer that hears about it and the attester committee uh is voting on the uh the fork choice rule uh kind of lmd related stuff and the ffd related stuff for the beacon chain and on short proposals simultaneously uh so we're kind of adding one more responsibility to what these uh tester committees are doing um also we have a kind of virtuous rule change which is this concept of site coupling right so basically we have this concept of a beacon chain confirming a shard block uh so the the concept of confirming basically is uh that this is kind of the first layer of defense against uh un uh unavailable proposals being included um so basically if we have a malicious proposer here and they published a header but they don't publish the body then the attacker committee would vote against that proposal and so normally it would not get included but if the proposal actually is available so if the full body can be downloaded then the intestine committee would vote for it and the critical thing is that the proposal is or the shard block is considered a canonical part of the chain only when like first first the proposal is actually included um in the beacon or the header is actually included in the beacon chain and second when um you get votes for that proposal from at least two-thirds of the investor committee to confirm immediately uh or uh you need a kind of majority or a plurality of votes to include it after like about one and a half e bucks um so once the gas shard block is confirmed in the beacon chain then you have this work choice mechanism where basically the shard block becomes a dependency of the beacon block right so if it turns out that the shard block actually is not uh canonical or is not available then that beacon block is now just not able to be not canonical so it's not part of the the chain that people should follow even if uh it's uh like you have all of these committee boats and this provides an important safety guarantee so basically if you wait for a shard block to be confirmed that you do something on chain based on the knowledge that the shard block is confirmed like do some operation in a roll-up you publish uh some um kind of fraud proof or whatever then if it turns out that the shard that that committee was bad and the shard block actually is not available sorry this should say available instead of confirmed if it turns out that the shard block is not available then well the chain will get reverted and so whatever you did based on the assumption that the shard blockers available but will also get reports right so you you have this kind of atomicity guarantee now the next and natural question is well okay so we have this rule that says that if the beacon chain confirms so not just includes but also confirms which means it also includes enough votes for a shard block that's unavailable then theoretically you're not supposed to follow that beacon block but what's the point of that rule when nodes are not supposed to be actually downloading all of these shard blocks personally and this is where a data availability sampling is going to come in um but first uh just a bit of a kind of motivation for why we need data availability sampling so why we can't use adjust the committee so the first kind of concern is well what if one third of sum committee is corrupted right so basically there's there's always the possibility that an attacker actually does uh gain um access to a fairly large portion of all of the speaking participants and we want to have second lawyers of defense to uh take care of this possibility and because if we don't then there are some nasty things that can happen right like basically the problem is that or one of the problems is that even though you theoretically need one-third of all validators to be willing to participate as you push an unavailable proposal through um it only require it only requires the actual committee of the 128 validators to actually sign off right to actually publish the signature that says like yes we think this proposal is uh available when actually it's not and so there is there isn't really a way to kind of even with a hard fork even socially like just no way at all to significantly penalize the attack and then the second kind of new one says well what happens if one third is offline right so uh in general like in east 2 we have this goal of trying to thread the needle between um being both uh kind of a having both asynchronous safety as much as possible and liveness even if a large portions of the network are offline as much as possible right so this is why like for example we have the fortress rule um and and and not just uh relying on the two-thirds finalization and and so it would be a shame if we kind of lost that ability to um be able to get some things done even if more than one third or off are offline uh for these uh shard shard blocks right so basically if we just have a two-thirds threshold then if more than one-third are offline nothing gets committed and but then there's this other option which basically says you have no fixed threshold and either you have a dynamic threshold that adjusts based on what portion is uh is online or you just need more votes from a committee in favor of a walkman against it and the problem is if you take either of these trade-offs then an attacker could potentially get kind of a knock uh find some way to knock many nodes offline and uh they could uh and basically reduce uh this threshold from one third to or the melting into attack from one-third to something even smaller like if they knock half of the nodes off um offline then it goes down to a one over six now we do choose option two right so we choose the trade-off of being uh more vulnerable to attackers instead of being more vulnerable to just the long periods of uh the chain uh completely halting or or at least the chain's ability to like confirm shard watch completely whole thing um and hey vitalik before date availability sampling do maybe see if there's any questions about the committee mechanism because these these two are kind of like additive and one the date availability enhances the base of the committee yes okay i have a question so for option two should we still respect the honors team minority model or not what do we mean by respecting the honest minority model do we still um i think the last time i checked is that we need some like 14 notes to be online so i guess for option two let's say there's like only yeah then what do we do oh i see it um right so if even the honest minority assumption which is like that there is a total of like some small number of nodes uh so some small number of clients so not even validators just clients i'm checking every shard like if that assumption is broken then basically we're screwed like there does come some point that which we are screwed but as long as there are clients on the network then um like my their opinion is that the chance that we have an actual like on that we have so few clients checking the shards that even the honest minority assumptions not satisfied is very small any other questions on committee i have yeah i have one i think it's about fork choice um so assuming we have a beacon node that has no validators it's just sitting there listening how does the fork choice know that the block is available please great so this is what i'm about to talk about right so data availability sampling so basically this is this kind of second layer of defense to prevent unavailable blocks right so the idea is that we don't want to just rely on committees we want nodes to be able to personally get at least like some medium level of guarantee about whether or not a shard block is of it is actually available uh and we want nodes to be able to make this check for every shard every swap now there's a trivial way of doing this which is to download everything but if they download everything that basically means that every one of these nodes would have to download 1.5 megabytes a second and over time we expect the specs on the chain to increase so it might go up like over 10 megabytes a second and then you have to multiply that by peer-to-peer overhead and so it just becomes like unrealistic for all the most powerful nodes um so here we have this clever mechanism where instead of the nodes and downloading all of the data they basically each client randomly chooses some set of positions and they try to download only the chunks at those positions um right and the client accepts only when they download the chunks at all of the positions that they asked for successfully so um digrad went over this briefly but the basic idea is that if let's say you make 20 samples then in the case where less than 50 of the data was available then with probability of like one minus one over a million at least one of your uh sample checks was gonna is going to end up failing and so if all of the checks uh do succeed then you know that with a probability of at least one minus one over a million at least 50 of the data actually is available and if at least 50 of the data actually is available then you can use that to recover the entire data and and so like that's just because of uh how the erasure voting works and so random sampling actually does prove uh availability of uh the of the block uh so the key guarantee is that sampling provides so this is the kind of honest minority assumption right so if there's a d is the number of positions that you need to recover the full block um so now like i mentioned before right that a block is going to be up to 500 kilobytes or more specifically up to 16 384 points um but uh we are made doing uh kind of eight points per sample and this is just the efficiency optimization um so we actually have 2048 samples um so and then k samples per validator looks like that's about 20 um so if you have about 100 um honest uh or and then right times uh times two so if you have at least the kind of 200 honest clients except then with high probability the block is available all other honest clients will accept so you have the same kind of convergence guarantee that you have with uh just regular full block downloading uh and the now if the attacker can theoretically fool a few clients right the way that they fool a few clients is if the attacker publishes and none of the body and then they just wait for clients to kind of have requests and they only satisfy those requests but the attacker would only be able to fool a small percentage of them and only with a very small probability before they would basically need to publish enough data that when the clients rebroadcast that there would be enough data to just recover the entire block by which point they'll just well they'll have published the block and if you're in a state of being fooled as a client um what are you using to decide that it's not just not available and i should keep working is it like heuristics i keep seeing attestations come in you know what are your thoughts there right oh i see it so one important nuance here right is that there's no time limit on dna sampling um there's so it's similar to that like it's not the sort of protocol where you run this procedure and then you wait 30 seconds and if nothing comes back after 30 seconds you decide the block is not there and that's your decision it's more like you accept the blog the shard block has available only once you get back all of the queries that you made at the beginning right so if you make a bunch of queries and you only get 19 of them after say 10 seconds then you just wait wait wait and then if the 20th comes after two minutes well that's fine then uh the shard block was only available available to you after two minutes uh so the um and this is kind of similar to how just downloading a block works right like you accept a blog in a non-charted blockchain you accept a block as available only once you've downloaded and there's no like there's no if you receive a block there's no time limit for when you uh after which you decide that the parent is unavailable and you throw it away forever it's just like you know you just keep the block around and then whenever you receive the parents and then you know you um uh accept the parents and then you ex and then you would accept that block as well so the blocks will should be available like as some a long time i think would be the answer uh but to give some more concrete answers um the um so the block someone would get uh kind of would get re-broadcasted through the uh through these uh uh through some subnets um and i'll talk about the subnet structure later um and then no the nodes are gonna kind of bounce between different sub different subnets and so like the mechanism for just the kind of grabbing the data from the subnet might work for potentially a few minutes um but then we would also have some longer term dht structure and the clients can obviously also try to kind of poke in and make requests to the dht and blocks that could be available in the dht theoretically like months or years or like however like the only constraint is basically nodes about unknown storage capabilities like if that if everyone had super big hard drives we could make it be forever one thing is in the committee construction uh there's a proof of custody construction which has a tunable parameter for how long they're expected to keep it or make a crypto economic commitment that they have it or can find it and so that number kind of provides at least the baseline amount you'd expect this network to be able to serve before it's maybe goes into secondary sources outside of the immediate network what would be the minimal time then that parameter in the spec is like 80 days right now but depending on the storage requirements that we'd want to have on consensus nodes and and per validator that could be tuned okay and and to produce a beacon chain block how long is this additional block meters i mean theoretically the entire kind of cycle of uh producing to attesting to confirming under optimal conditions could be finished in a single slot one thing i'm worried about with data availability sampling is um right now we have uh lots of disk traffic and we also want to use pruning but if we need to keep the block open for 18 days and we only have a slot to reply um that means that pruning is difficult so i guess one thing to be clear about one thing is that this is only for validators and it's not for um like not non-validating clients don't really have any responsibility to like keep data um oh another thing i need to mention is that like there's no like there's no requirements the response of someone else's message will be a single spot right like or the well again like you do get rewarded for um including and uh kind of frost attesting to and processing a short vodka within a single slot but that's like we do things within a single squat already that's just like the progress of the beacon chain the um but ultimately like if you are a validator then yes you would have a responsibility to store um basically whatever number of e-books are in um 80 days um times on average 256 gigabytes of data proto you have the numbers on that don't you do you remember what it was like the total storage requirement per validator on like an 80 day window hold on one each is to 20 48 epochs um and then 80 days um i think it's 72 days so it's like eight weeks um so that's 16 384 and then multiply by a quarter meg that's four gigabytes right and so from a consensus perspective that's all per validator kind of requirement which scales linearly at least for a while um and pruning from a consensus perspective could also happen after that uh right window and another thing right another thing is that it gets uh and it is perfectly okay to have the short block data be on like a hard drive or even hdd or like whatever kind of a relatively high latency medium like basically if you're sampling for one of these like if you're sampling in real time like in real time then we want to try to optimize the real-time sampling to make it fast but if you're sampling after the facts like through the dht then like you know if it takes a minute to respond that's not that bad because you're like your hours are days late anyway right and you're gonna go over that uh optimization how to do live sampling right yes cool that's uh um two slides from now but uh just one quick slide is this this is just going briefly going over polynomials again so basically the way that you do this recovery thing is that in the header you have a polynomial commitment so this is what amp the polynomial commitment is a commitment to a degree deep or less than 16 384 polynomial and but what we do is we generate evaluations and proofs for evaluations at more than d points so it's um two times the max degree point so three two seven six eight and any d plus one of these points can be used to recover the entire polynomial uh and the kind of the special case of this you probably learned in high school is that like the d equals one case a degree one polynomial some align um any two points on a line can be used to recover the line and then for high a degree d greater than one polynomial said z plus one points can recover it uh and so that right and also as i mentioned before um there's a uh there's also a pro a proof that proves kind of a balance on what the unwanted degree is and you can think of that proof it's actually also like just uh one more i think it's uh one more g1 point so it's just another 48 bytes um and what else did i want to say about this right and just like the number of uh kind of what the size of a block is it just is also part of the header and so like basically if the if a block is full size you need 50 of it to recover the whole thing if a block is like quarter size you only need one eighth of it to recover the whole thing so uh starting by subnet so this is where we get to how this is all supposed to look from a peer-to-peer point of view so we have two kinds of subnets some i call them horizontal subnets and vertical subnets and so you it it's good to think of them as being a grid um so this diagram is a little bit wrong like actually like ideally we wouldn't just have 64 horizontal subnets that really would have 2048 horizontal subnets that's the one for every um kind of pair of uh slot and epoch so like whether it's the zero swath first one second slot all the way up to the 31st squad and the debug and the shard from zero to 63 and the horizontal subnets just like store the uh and broadcast the full body of the shard blocks corresponding so that's the watch or the pair and then you have vertical subnets and vertical subnets store the ice chunk of every chart block so the third vertical subnet just stores a chunk three of every shard block and vertical subnets are how we make data availability sampling concretely kind of simple for validators um basically what happens um is that a validator chooses a set of indices and then they just join the vertical subnets that correspond to those indices so that's and basically when they're on those vertical submits they just would just receive the eyes chunk for just every shard block that there is [Music] and so they'll be able to just like well basically they they're able to do the data availability sampling completely passively right and they're listening to that global subnet that has the shard block headers so that they have the basic information to verify those against yes and the shard block headers yes they get published in the global subnet and they also get included in beacon blocks very quickly um how is the vertical subnet shuffled or is it shuffled at all today my preference is to just not bother with shuffling i don't really see much reason to do it but if we want to we can i guess one right it's because blood could be different size right well right but like it's not the case like but the sh but the subnets don't have different load right like i see so the polynomials have different degree um but every block has the same number of samples right so basically like if you if let's say we imagine hypothetically that there's a block that has like only one only two points in it um so the block itself has like 64 bytes then that would be a degree one polynomial and so like if uh just in reality it's a bit more complicated but basically you would have like if the coordinates were just sequential then over here like you would have the number three here you'd have number five here we'd have the number seven here we have the number nine here we have the 11 number 11 and so forth right so every one of these uh squares does get filled um it's just that when a block is smaller it has a higher degree of redundancy a quick question from previous slides it seems like with polynomial commitment a marker roots are disappearing are we removing them there are yes there are no merkle roots to sharp to shard bullock's data there are the moral roots that are already part of the protocol are still seen for now so ssc still exists and the mercurization there happens but then at a certain point as you dig down into the shard blocks you have a polynomial commitment instead of extending into the tree so um one one longer term thing we may want to do is just to do any more thorough replacements of polynomial commitments with uh um with or sorry of a replacement of verbal trees with kathy commitments um i don't know there's been some research on virgo trees that digrad has been uh working on uh and also there's been a lot of uh work or i like i've also suggested that uh cat day commitments can be used in ease one point x uh for uh codemoralization and the reason why i think they're really good in that use case is because then it could work like code verbalization the witness for the code needed to process a particular transaction just tends to kind of randomly jump all over the place right because contracts aren't really designed to optimize for kind of execution being a contiguous and with code mercalization you just need because all of the the code is separate for every chunk of code you access you need a kind of separate branch going to it so there's a lot of data overhead but uh for cat day commitments you don't really care about that and and you just need a single 48 byte proof for any arbitrary subset of the code regardless of where it like where those chunks are are located um so like basically there's reasons uh there's reasons to move toward by using catholic events more and more in other contexts as well okay so broadcasting procedure so there's uh i think two broadcasting well there's three broadcasting games that we need to i think worry about i might i forgot to mention the third one here so one of them is that the initial broadcast game so uh when a proposer publishes a shard block they generate the blah a shard block including all of the proofs um then they push the entire shard block to the corresponding horizontal subnet and then they uh basically push the chunk eye to the eyes vertical subnet you know this third step is a bit more involved because we don't want the proposer to have to be on every vertical subnet um instead what happens is basically the propos the proposer just publishes to the horizontal subnet the testers pick it up on the horizontal subnet the attesters then um like that's already about a hundred of them um and then the attesters themselves have a lot of peers and then they ask like which vertical submits their peers are on and so between all of the attesters and all of the appears of the attestors there's just like a lot of nodes and and they are able to kind of collectively cover uh re-broadcasting each chunk to be a corresponding vertical subnet now recovery so recovery is important um this is this uh kind of case where you have a malicious proposal where they let's say yes they don't publish to the horizontal subnet instead they just publish um some chunks of vertical subnets and maybe the only published chunks to some of the vertical subnets so they publish enough data to um get most of the known most of the clients to accept the block and they publish enough data to recover the block but you we actually need to do the recovering and so here what happens is it's like first we run step three here in reverse order uh so we try to kind of pull like the nodes in the hor the committee in the horizontal subnet tries to pull the chunk eye from the eyes vertical stub that in some cases they might succeed in some cases they might fail but like all together they would succeed and then this would and then they would just re-broadcast those chunks in the horizontal subnet then in the horizontal subnet there would be enough chunks to recover the rest of the blob all right and then just someone anyone uh just like recovers the entire data generates all the proofs and then they just re-brought re-broadcast the same way they were if they were a proposer and then the third game is basically a vote for longer term access we have this dht and the data needs to be published onto the dht and the clients needs to be able to kind of grab like specific shard headers or a specific interface from the vh2 on the recovery that's primarily this is a failure mode right like alright it didn't quite fail but it almost failed and now i need to recover and usually wouldn't have to happen yes correct so the recovery is only required when there is a either a dishonest proposer or if you might occasionally be required when if there's like some network pick up and let's say yeah like the chunks get broadcasted onto every vertical subnet except for one and then the adjusters that are look or the clients that are looking for that one last sample are going to be kind of spinning their wheels and well they have to actually kind of get that data back and so you would need to regenerate it and even practically in that standpoint if i'm on a horizontal subnet and i'm listening to one of the eye subnets and i don't see it i can just send that proof instead of having to go recover because i saw the full right this is true so it's like right it's the backup of the backup probably in many cases right right now this is true i mean if the uh right if the proposer is honest then then the thing on the horizontal sub that does exist and so right are there some thoughts about how we might coordinate recovery so that everyone doesn't do it at once um um i think good also a good question uh so i think this is one of those kind of step-by-step things uh so the pulling from uh um the pulling from vertical subnet thing is i think just the collaborative effort that everyone should participate in in the case where the uh data on the horizontal subnet is missing and then as far as like who recover does the recovery and the rebroadcasting it's definitely a good idea to kind of nominate a limited set of actors saying if we want we could just like reuse the mechanism that we use for at the station aggregation here but i don't know think about it right and i do want to mention on that third item uh chuck it into a dht uh there's a lot of thoughts i think primarily led by proto and a little bit of conversation with felix on how you actually do go and find or brought or share the fact that you have um more historic black data rather than this live stuff called in the last few epochs um it's there's probably there's many things in its design space uh but it's i wouldn't say we have 100 the design that we want to use here so um definitely some open questions and some like trade-offs in this space still to explore so if you're interested in that problem uh that's something to dig into as well no okay um so um i guess any questions on sampling before i just this was a the last slide so practically it looks like i'm in a subnet a horizontal subnet and i'm listening to shardblocks sometimes as a validator i get assigned to one here there um and then all the time i'm listening to a subset of the vertical subnets and checking those against the all the shard block headers that i saw come in if i see what i think is unavailable data get confirmed into the beacon chain by a committee then i actually use my fork choice to not follow that beacon chain because i think it's unavailable if at some point i actually see that data is available i bring that big back into my my fork choice this would allow if the majority of validators are honest in doing this they're not going to finalize something that's unavailable okay so my question is very simple uh won't it be easier for the next short blog proposer just to include the blob of data rather than falling back to the recovery mode i mean no one has like a choice between proposing and recovering right like if you're the proposer then you should propose um oh right um one other thing i forgot to mention is that like there's no linking between one shard block and the the the previous shard block of the from the same chart at least like right now right so there's uh and even if there wasn't linking that would only be the header so like it's not like um so like basically the jobs of the different proposers are kind of independent of each other right so and in general like the people who would be doing the recovering and the people who would be doing proposing are different so it means that the same blob is not possible to be included like in the one block and include it then next to the another one correct because the header already contains like what the slot and the shard is so the header can only be included in one position now obviously you can have two multiple headers in multiple positions that just happens they can have the exact same context you know that's fine okay um i did have no question sorry it was about the the fork choice one perhaps it was poorly worded but i'm so informed choice there's the uh tight coupling if say a non-validating node is just following the beacon chain does it make does it need to just like actually query for data availability does it just make like assumptions based on the pending shard bit fields um in my opinion nodes should you should do data availability sampling if they have the capacity for it wouldn't that make it kind of uh like oh n again like you need to be watching all of the shards um no because for sampling you only needs to be processing a um of one data per chart right so you still have to know like basically you're like for just a little bit right so like the uh the the total date amount of data in the system is o of n squared um the beacon chain is already of n um and the availability sampling on all shards is also all of them so you can follow all shards all the time and know that the data is available you could do date available data availability sampling or you can follow some shards and then do date availability sampling for the rest of the shards um and still know that the data with extremely high probabilistic guarantees is available you could then not follow any full shards do date available stamping on everything and get those same guarantees uh you could then you can certainly not follow just follow the beacon chain pretty lightly um even as a light client and kind of rely on attestations and finality as your signal um and maybe you could do data availability sampling on a subset although the way that the vertical subnets are constructed you would generally just be seeing the points for everything on that that indices so i mean there are there it is certainly a design you can design certain types of nodes that don't do sampling but it's definitely a good thing when users are right sampling yeah and the one class of node that would not do sampling because it's relying on like on the majorities anyway is the the light clients that are using sync committees right and i guess no i mean even though it's like in the long run i think it would be possible to do it to have a node that does that and does the sampling got it thanks and i had just one more uh regarding publishing multi publishing the shard block header and then a bunch of proofs about it i'm just concerned about um publishing lots of related messages all at once might make it uh might jeopardize validated privacy um okay so validators let's see they get up they publish uh right so a validator would publish the uh shard block which is the header and the body and that's just like similar like that that's not really different from how beacon block publishing works then you have the broadcasting game and [Music] the broadcasting game the one right okay so the one piece of like significant information that the broadcasting game might reveal is like which vertical subnet you're currently in which uh reveals like which has and if the more you can know about which vertical sum that someone's in the more you can potentially kind of start like target and try to attack them um and so okay so you do need like and for that reason we do have uh validators like shuffling which that which of vertical subnets they're on uh and i think the current approach is to just like switch out one of those every squad right so there's the kind of random shuffling here i think it is a valid point that when you add more duties and add more consensus messages and more things that people are listening to um you're certainly leaking more information um and things like secret leader election and other like nice to have maybe ended up it might end up critical but nice to have from our perspective right now upgrades are really valuable there um agree yeah i just think yeah i think there's something about publishing multiple related messages that that makes it more obvious because you can you can start to reason about like if you got lots of these messages from the same person they're probably closer to the source of it there may be ways to try to like anonymize the uh rebroadcasting games more right well one thing to well and i don't know if this makes privacy better or worse but um if they're you're not operating on the assumption that the chart proposes necessarily on all the vertical subnets and so there is like a rebroadcasting that's likely to be happen from the others that are in the shard committee to their own eye subnets um which might help obfuscate or might actually help leak information i need to think about it more that's your assumption metallic right is that the sharp proposer isn't necessarily hitting all those i subnets or every single subnet correct this entire proposal was a design is designed around everyone needed each validator when we needed to be on of one subnet okay uh so last slide um this is just the kind of an aside on how applications would actually benefit from this data so basically in the beacon chain state we just have a merkle tree of the confirmed commitments so just the tree where you can look up like what the adverb commitment is at a particular slot at a particular chart if there is one and the way so the way that a roll-up currently works right is that like roll-ups basically um you kind of work over like you refer to uh blobs or batches of data by their hash but you do have to actually prove that the blob is available and so when you submit a batch you provide the call data and then like as part of a transaction then within the transaction you hash the call data and then you just like save the hash root of the call data in the list of data roots in your storage and then when you make a fraud proof the data route gets saved and then you just once again you hash the portion of the call data that's providing the blob or the batch data and you just check that it equals the data root so that's how rollups work today a roll-up using es2 data basically the call data will to publish a batch the call data will not go in the transaction contents because well there's only o event space for a transaction content but there's no events where it's based and the shards and so you would want to just use them the shard data directly and so basically you would just kind of poke into um like you would use some method the get recent confirmed data route that would accept the slot and to short us an argument and after the merge uh the beacon chain state could be accessed from each one from the ethereum execution in exactly the same way as the east one state campus um so you just get the recent confirmed data route from the yes just by looking at the state and then in a fraud proof you actually would take the data and you would include it in a you would include it in your transaction uh and then you would just uh like make the cat day commitments kind of inside the evm and you would verify that that cassini commitment you've generated actually is the same as to the egg commitment that got saved all right one thing to note and definitely if you're a rollup provider listening to this uh the use of content commitments uh is going to change how you do things in roll-ups uh on an optimistic side optimistic roll-ups not terribly much i think the commitments need to change and some of the proving there but a lot of the core mechanics stay the same whereas in the zk land it's a different cryptographic construction and so there's probably some like more fundamental things to rework and stuff so as this becomes more reality uh keep your eye on it any other questions for vitalik um so with the ratio codes you can also produce an arbitrary number of encoded blocks or samples um right now you are proposing to do a hundred percent of the real sample data which means that every byte of data that is produced in the network is double with this proposal is um is this uh already completely 100 fixed or is possible to explore other um sampling techniques with ratio code like just 80 or 20 because there is this trade-off between um data ability and storage of course right uh it's definitely possible to explore other points on the trade-off space um though one thing to keep in mind is that if we try to make the balance tighter then the number of sam of data availability samples that you need to get the same security guarantee increases quickly um so like for example if uh 50 a 50 full block requires 20 samples then a yes 70 block will uh a full block will require or a block where the data needed to recover is 70 of the entire block would require 40 samples and then in a block where the data needed to recover is um 84 of the blocks of the entire data would require 80 samples so basically the inefficiency on that side would shoot up quickly uh so well actually to be clear our current this proposal says 50 is for maximally full blocks and then 25 for average full blocks what determines what goes into which chart uh um the short proposes too each short proposer decided just decides what uh data to put in so the blocks that they can create the mechanics below that would be some sort of data fee market and likely some sort of data transaction format and likely a data mempool transaction mempool related to shards that potential proposers for that chart are listening to and know that it's valuable to include such data into their blocks and i mentioned in the chat i'll see you i can find the links uh there's a couple of different um shard data feed market proposals uh from the past like six seven months on eth research uh that go into maybe the mechanics on how fees can be paid here um if later we were to add execution in sharks with this approach um how difficult or like easy would that be um basically we would have to just extend the shard header structure to add a uh a state route and then we would need to decide what mechanism we will like what kind of backup mechanism we would want to use to make sure state routes are correct like would we either have a fraud proof scheme that could reverse the state routes if it turns out that someone computed them incorrectly uh but would it be uh or would the statements just be ziki proven there's also there's no independent chaining of these um chart chains and so that these sharp blocks so they have to be kind of like confirmed independently um but with execution you kind of want more of a strong notion of like chainedness and a chain being able to build be built without necessarily being confirmed into the beacon chain every slot and there's actually a reasonable amount of consensus complexity that appears there and my intuition would be if we build shard data and it is a valuable resource to the ecosystem um to keep them as shard shards that are just for data and then like maybe have additional execution shards so you have kind of like two different zones of of things that are happening in consensus that's uh there's a lot of different design tradeoffs there though okay we're at our two hour mark any last questions for vitalik thank you appreciate it with alec um thank you so next up on the agenda is uh we're shifting from like i said we have the beacon chain the being chain is a bootstrapped proof of state consensus um and there are two major things that we want to do with it one is to come to consensus on eth1 on ethereum and the other is to facilitate more scale in the ethereum ecosystem through sharding which was previously discussed um i'm listed as a speaker i'm not i'm just gonna give the intro um mikhail and guillaume and a number of others in a small working group have spent um i don't know the past eight maybe more months um kind of digging into the design landscape of what an eth one eth2 merge looks like and there are something that are beginning to look like specs and there are uh prototypes merge test nets and some other fun stuff there's a lot of things to think about there's a lot of like design trade-offs here there's a lot of things to think about like what actually happens at the point of the merge what are the security considerations there um other things like what has to change for users and what can be largely stable and from our understanding a lot of it can be largely stable a lot of the same apis same endpoints that kind of stuff um to set the stage i just wanted to talk about what how i see heath 1 how i see e2 and what's kind of been happening in the different zones over the past couple of years essentially the work that's gone into what we call e2 which i think is a terrible name uh was to build and bootstrap a proof-of-stake consensus mechanism a consensus engine to supplant ethereum's existing proof of work consensus engine which if you've worked on an uh eth1 client uh likely you got that proof of work component stable and haven't really done much with it since because it's small isolated and it drives things but you don't think about it much what i see the merge of eth1 into e2 of e2 of this like proof of stake beacon chain uh that was built and facilitated in the what we call an e2 client um and if you've read anything that i've written recently or uh seen some of the posts we've put out um you know the unification this is kind of like a a beautiful uh unification of layers um e2 clients are very sophisticated uh engineering teams that can build out proof of stake consensus mechanisms and ethon clients do incredible work in the kind of the user layer in state and execution transaction management and everything kind of user user land and so what mikhail and guillaume have worked on is kind of the marriage of these two um defining a communication protocol between an e2 client kind of the driver of consensus and an eth1 engine which is a modified geth which says okay i'm not listening to proof of work anymore or i'm not even i'm not listening to clique instead i'm listening to this uh locally defined rpc uh that tells me you know there's a new block hey i'm thinking about producing a new block can you give me a bundle of transactions that kind of stuff so really i think this is pretty exciting in that we have uh people that are very sophisticated in this one layer of proof say consensus and people that are very sophisticated as other layer kind of like user land execution and i think that through this merge we get to leverage uh the sophistication of software on both ends of the stack and i'm very excited about it and on that i will give it to mikhail who's going to talk about things from the the eth2 uh kind of like eath to a driver consensus side um and then we'll pass it on to guillaume who's going to talk about catalyst which is a modification of geth to be this one engine that lives adjunct with an e2 client so i'll turn it over to you thanks guys um thanks danny for this nice introduction um can you see my screen can you see the presentation cool so let's get started so um in a very simple understanding and a very simple version and according to the most recent proposal uh the merge is a tight coupling of the beacon chain with the ethereum so um what's affected on the two side is like in terms of this pack all the parts of the stack are affected um i will just go through them through the change step by step we'll stop at each slide for questions but feel free to stop me at any point in time to ask the question if you need to so uh are there any questions that form okay let's move on um so the first part of tight coupling is that the beacon block body gets a new field which is called executable data which is probably not not the perfect name um and yeah any any inputs on like the new name is very welcome so let me just turn off my my earphones um can you hear me now yep great sorry okay um okay so um the executable data if we take a closer look into it it's just um several fields almost all fields of uh basically it's one block executable data doesn't include the fields that are related to the eth1 consensus it also doesn't include an extra data field um so we we also can see uh some leftovers like difficulty here it's gonna be removed and don't pay much attention to it also there are parent hash and the block hash they are if one styled block hashes and we also can get rid of them of these two but it's a bit of an exercise because you need to somehow identify the uh block uh the eth1 block that you are building on and that you are inserting upon and at some edge case at some cases it introduces the um circular dependency if we for example try to use the beacon block route so it's it's a bit of an exercise but it's doable uh so the transaction the transaction structure is just a copy of what we have um in this one currently um so the other affected structures is like the beacon state um but it's not much of affected uh and since we use like the more advanced deposit processing flow after the merge we are able to duplicate the eth one data pole which is nice so we don't need it one data votes anymore but we will still use use the ethon data which is like considering to be removed at some point in the future um so i will talk about deposit processing later on a couple of slides later in terms of size currently the eth1 block is like 40 kilobytes on average this is uh according to the current guest limit which is uh only only three main net and uh for the record the beacon block is like uh a hundred kilobytes or even more in average so it's not like a big gain uh in terms of size uh for the beacon block it's like thirty percent something like that uh so that's pretty much it regarding the structures does anyone want to ask anything here so something i meant to mention at the beginning is um this particular proposal right now which has the ethon integrated essentially as a payload and eth2's consensus um ends up looking like early proof-of-stake uh designs like the casper ffg proposal and things where essentially you're taking eth1 and you're adding a bunch of consent proof of state consensus messages um you know to facilitate the them living kind of in the same chain and this instead of attaching it on to eth1 was bootstrap that consensus independently and then slot eth1 is such that in the you know the payload the consensus payloads and the uh execution and user layer payloads live in the same place so if you're familiar with previous designs this is kind of a roundabout way to get to uh where we were going a couple years ago okay one question i would have would be like take the gas price for example um how does that relate to the balance you need to or say that you know the deposit or whichever piece of balance like data you take um it's not related to the balances on it too so uh gas prices and the throne balances stays within the is one part um of the merged merged chain so if two balances are used by heavily used by the system by the consensus layer and they will be like they will withdrawals and deposits in the same fashion we are discussing them um as of today and as mikhail's going to get into a lot of these like essentially you have all of the e2 consensus validations and but then you also have like a call to eth1 validations which are going to validate this transaction list validate the gas you use the gas limit that kind of stuff in a very similar context to what's validated on either one today another question um so what is the coinbase used for yes so likely the um the actual block reward within like e1 execution would be zero uh but if the coin base which i believe is uh used to collect transaction fees from uh transactions that would go in there so if i'm a beacon block proposer and i propose this transaction list for eth1 execution um i'm also going to specify where i want my transaction fees to go to in that coinbase yep and the this coin base opens like a path to the eth1 engine market like if you can expose your like github engine instance to produce a block for you for your for someone or once validator who don't want to run the heavy eth1 node this is one of the options to confirm so we'll now have two sets of validations like these two validations in the one x validation and the e1 validations and that second set will happen through a bunch of rpc calls so like it'll call into each one client yeah yeah i'll talk i'll i'll talk about it just like starting from the next slide practically that ends up being a design detail because you could write an aggregate client but we do have these like largely clients that are separated in terms of concern so practically yes the rpc would facilitate that uh validation okay so let's go the next one um so the validator uh the validator will have to obtain yet another version of the executable data how does it do it um uh it was mentioned that the json rbc is used so uh the validator just prepares some parameters um and makes a call to ethon engine asks it to create another portion of executable data if one engine goes to the transaction pools gets all the transactions that it finds reasonable to include in this portion of data runs them assembles a block and returns it to the validator so um a couple of things about the parameters uh first of all we want to use the most recent randomix for the sake of better randomness so we pre-computed here with the recent trend hour wheel that will be included in the same beacon block and we expose beacon block routes to the evm and we also expose the slot to the evm to make it possible for the uh to access the beacon state outside of the evm so we get this passed to the phone engine and it exposes it uh in the same fashion that as uh block hashes are exposed today right so a lot of that ends up are nice to haves and exposing extra information to users about like the eth2 consensus um like for example you don't have to put the random x in there but uh if you expose it to transactions if we an op code it's a nice source of potential source of randomness um something to also note is this this ends up uh looking a lot like i think there's a get work rpc call on eth1 clients today which essentially doing the work of like assembling a block uh for mining and this is similarly like assembling a block for the proof of stake proposer what are the latency requirements here i mean if a data is calling e1 and then waiting for a response before it produces a block what what's the timing structure like um yeah it's it's a synchronous goal obviously um and i guess that requirements that with regard to latency that we can gain them from uh the timeline of the of the slot so we need to i'll speak about it later later on but uh what we need is uh to for the network to be able to produce uh propagate and uh process the block like it is now but with the in the including all these latencies are can be included like in the processing in the proposed time in the proposal time i i don't think if if they are located like in the same data center it should not be like big latency if there are if these two parts of the client are located on the same machine it's like just few milliseconds i guess i mean the latency in transmitting the data of course producing a block takes the all the time that the execution of transactions of the block takes yeah because i'm thinking about it from the point of view that right now it's one kind of just feeds into these two it's a one-way street but here we create round trip right um yeah we can implement it like i mean we can like have this uh server side push or whatever it's called to push the block whenever it's ready if you're meaning that assuming get work is works today the latency here i don't think is incredibly high um i think if you're located on the same machine which is generally what we're designing for that it's probably not much of a concern i think if you're actually dealing with remote machines and round-trip communications uh yeah you might run into an issue on block production times well the the curiosity here is that we provide the e2 data to each one at the same time as we ask if one to produce the executable data and i'm curious why these two necessarily have to or do they necessarily have to be linked to each other like this synchronously or could you think of them as two separate feeds so if you're going to have eth1 transactions and things that's the payload in e2 then you have to have them tightly coupled but in terms of what some of these uh variables are doing or providing um essentially read access in eth1 to some of these consensus layer items like the randomix recent beacon block roots and that kind of stuff uh thus you do need that kind of two-way street but it you could also have eth one that never learned about the beacon chain um and couldn't really communicate with it in such a way or read from it um and you wouldn't have to necessarily provide those but then you're limiting a lot of functionality and likely some of this ends up being necessary for handling deposits and withdrawals transition we add the process executable data function to the end of the process block routine what it does it actually prepares like the same uh almost the same parameters as the validator does and ask it asks uh sends these parameters uh alongside with the executable data patch to the formation and the it one engine inserts this block into its um one chain it's uh producing and returns the flag of whether the execution is succeeded or not yeah then the um these two side verifies validates that the execution has succeeded um otherwise if it's not then the beacon block is considered invalid that's like yet another call to the phone engine looks pretty similar pretty tiny the um so it's also a synchronous goal in terms of a slot so we need to process a beacon block and we need to process if one block within the same slot and some like option for optimization we'll talk a bit later so that's it that doesn't look like a rocket science okay regarding deposits uh what's on the left uh you can see like the ideal deposit processing flow which is uh the proposer of a slot takes the deposits made in the previous right in the previous slot takes the new deposits builds like the proof that is linked to the previous beacon block route and include this batch into the we can block that it is pregnant um so why do we not going straight to this uh flow it's because of the if one state route is the miracle patricia tree root and there is a plan to change it to the binary tree or even replace it with the polynomial commitment so um this is why we for the moment we reuse pretty much the same logic of deposit processing with my one major change we used the eth1 data produced by the previous previous e1 block because of the type coupling if one execution to the beacon chain we can do this and we process deposits in the same fashion but to in order to make sure that if one data that published in the previous block i will lead we add this uh check for it1 data correctness into the folk choice on block routine um this is possible to do because uh when we tied couple the execution to the beacon chain these one data produced by the previous block becomes deterministic so we can uh exactly verify them and see whether they are related or not so that's how it's proposed to do in the current version of the spec like the schema on the right and the state transition function updates the if one data in the state in the beacon state like for each slot and it allows for like inducting deposits made in the previous slot right in the next one which is great anything to ask here okay cool so the um a little bit of speculative flight now this is like an attempt to reason about how if one block processing time how it won't block processing effects impacts on the slot processing time and why speculative because these numbers are not probably not that accurate but the main what's the what is to highlight here is that it's highly likely that each one persistent time will be will differ from the beacon block process and time by an order of magnitude so let's just assume that these numbers are correct we have like three to six times increase in the producing propagating and processing the block by the entire network um and what's what's uh to pay attention here is that we should be we have to process we have to do all these these three parts of the uh block uh dissemination like uh within the three within the first third of a slot to allow testers to attach to the most recent produced block and if we take these numbers we see like 30 percent increa 30 increase uh 25 increase uh of this time but it's just a 30 of the whole uh four seconds which are dedicated to this first part of the network activity in each slot and uh even if we say that it one block processing will take one second and beacon block processing will take like 100 milliseconds and we increase the propagation time up to like 500 milliseconds which should be pretty fine and we will have like uh slightly more slightly less than a half of uh this portion this first third of a slot time in advance so um what's to improve here probably and the potential options the option is to produce is to process if one block and we can block operations in parallel but this is not of a much gain here and it's up to client optimizat it's up to client to decide on this kind of optimization um and probably it will make more sense as when the relation between numbers will be changed so something important to note on the so the first 500 milliseconds you have in there is the block production right yeah so as a proposer um i can probably in most cases actually opportunistically compute and process my block uh prior to the start of my slot and broadcast immediately at the start of the slot um so i think we can with the right optimizations which i think nimbus and others have been doing such optimizations kind of in the context of epoch processing i think you can probably cut this in half in most cases yeah right always if it's as long as the previous block is available right right and even with the fork choice fix that we're putting in a mid-year with like kind of the adherence to the vote of emptiness it even becomes better there because you don't get that late block throwing you off like right before the beginning of your slot what happens if we exceed this limit if it takes more than four seconds likely attesters are not going to see it on time and they're going to say nothing happened and you're going to be orphaned yeah sounds pretty bad yeah i mean it's something that you would expect to happen from time to time but that you would uh is like a huge optimization target uh something that if you're writing a client you don't really want to see um yeah by the way i have like a quick question to client implementers on this one side what is like the iphone block processing time at the moment on average machine you have an intuition then 100 200 milliseconds nice cool okay the other aspect of sorry the other aspect of this is the network propagation in terms of what validations you have to do before you gossip on a block um in each one you just check proof of work you don't execute the transactions are we able to do the same kind of thing with the beacon chain basically propagate the each one block assuming it's valid yep i think we'll just do what we do currently on the beacon chain just verify the signature the proposer is this what it does yeah yeah the the proposer signature kind of does a similar thing is the proof of work in that it's expensive to create a proposer signature and definitely to create multiple of them um and so you assume that there's validity behind that and just propagated on similarly yeah that makes it work i just wanted to ask um a question this wasn't entirely clear to me with the uh eth1 data being propagated as part of the beacon chain and then with this execution happening for you know validators am i correct to believe that all beacon chain validators which means all these two validators will now have to um propagate as well as execute the eth1 blocks or is it is there some subset self selection or something um in this design it's like as you said everyone will every validator will execute with one block and propagate it um into within the um we can block is just actually part of the beacon block it becomes a part so then it's i should think of the eth1 data as being 64 times more expensive because uh versus shar data just in terms of the fact that it's not just being done on one shard is being done by all validators um i don't actually get this um yeah i don't know i mean maybe the the supply and demand of what is essentially like the eth1 payload versus sharded data those would be independent so you might expect data to be very much cheaper on those shards rather than where you're competing for execution and shard and state kind of an eth one land but i don't know if i don't have an intuition for whether there'd be like this like kind of 64 uh mapping because there's less amount of validators doing work on the other uh i don't know does anyone else get an intuition there yeah sorry the way i asked that made it sound more like a financial question i originally was actually just thinking about it from uh computation bandwidth storage those types of like resource yeah it's being done by all the validators and which presumably is not i know some people are using infuriate but presumably not like an increase in load over what is today um obviously if you get stateless ethereum um and if you do the design of moving this out into a shard which i think most decisions are being made in the design so that that is a that you can facilitate such a movement um then you could have reduced the requirements there but also i mentioned stateless and statelessness isn't required necessarily to move it out to a shard but it's required to then have multiple instances of state and execution uh because if you don't have statelessness now you have the burden of two states or in states however many shards you make well i would claim it's a fake shot if it's not stateless because everyone has to validate it the whole outside shot right right you could check it in but you can't you could check it in as like the data was available but you couldn't actually do the execution you're right sorry even that would require being able to construct four proofs okay yeah so i mean this design pretty much elevates what we call eth1 to be a first-class citizen live right next to the consensus and facilitate an early merge um given how sharding develops and how it's used inside of this like what each one and statelessness and other moving parts you could imagine moving it out from the beacon chain and kind of reducing the load on consensus participants but those are all there's a lot of different moving parts there okay cool let's just move to the last slide which summarizes the 202 communication protocol um there is like the third method here which notifies the if one engine about the block is being finalized and and allows it one engine to do to do the garbage collection at this point um so that's all uh with regard to the communication on the system layer um and yeah there will be definitely some extensions in the user level json rpc but it's like decided on it on the concrete design later um so uh unless there are any more questions for the youth part i would like to turn over to guillaume who will proceed with the ethon side of the deal i did have just a thought about how during the i guess per slot per block processing we have to call out to s1 it seems like during syncing that the latency times would start to be pretty significant there um i wonder if some sort of batch verification would be useful what latency do you exactly refer to so just like um because before say process block there was this pure function and we've all been optimizing it shaving milliseconds off it and then process executable data is now a json rpc call there's going to be a bit of latency there uh you mean the latency for the the call itself or the like the whole persistent cycle i guess the the computation is going to be a lag as well but it seems like a necessary lag um but the the just the like the http call there um seems like it's gonna add add some slowness to it so i'm thinking perhaps it'd be nice if we could say like here's 64 or 128 blocks you go and verify them and then come back to me when you're done oh yeah see what it means so it's like deferred execution right i mean so you process the beacon block but you're deferring the eth1 execution for like some time right yeah yeah that's right you can kind of like use it in parallel in a batch and remove like divide the the um the http overhead by like 64 128. yeah yeah i see but since the uh if one execution can invalidate the biggest block like it's probably not the best strategy to do it that way yeah i mean i guess it it's not a good strategy if you're on a network that's like serving you bad finalized blocks but i haven't seen that one before on f2 i think being optimistic there is probably a big win yeah and this is probably a good strategy for like the sync process so you just execute the beacon block you you get to the top of the chain and then uh like the you can process like if one blocks in parallel and you finalize like this um yeah probably probably start processing uh if one blocks from not not from like from like some finalized checkpoint only that's been discussed just skip some uh some execution in during the same process yeah but if you're like uh thinking uh if we are uh catching the head and staying with the in sync with stainline stainless with other with the network you probably want to like execute y one or why one by one sequential order i agree yeah the process of fasting probably comes in here when you're cooking finalized you don't want to have to do a full sink of the entire main net of ethereum because it'll take you days to catch up um so you you're probably looking at just just ignoring processing you know these one engine blocks and letting it fast sync the world state up to something close to head and then you hopefully just have the non-finalized um and hopefully something faster than fasting by the time we get around to there i know there's a few different proposals but but i mean the requirement the the requirements for synchronous execution here also implies that the outcome of the one block production call depends on the data to pass in there so when you're thinking basically you need to execute these one block in that historical context right and i would say often times you're maybe not actually executing that um ethon context and more of like doing some sort of fast sync with with respect to the eth1 state and and then doing a snapshot snap sync or something like that from there but there's definitely a lot of trade-offs in sync speeds and computation and what's optimistic and paralyzed and things okay thank you mikhail let's turn it over to guillaume thank you all right let me thanks share my screen so i'm split between two computers uh can you see the screen yeah okay yep cool uh yeah so i've been asked to uh to talk a bit about the changes i i made to guess to create this this thing called catalyst which is an es1 engine and try to explain to you a bit what changes are involved uh in order to build the either to interface with it or to build something similar um yeah so it's not so much to talk about what i built as a talk about what i broke um the idea is we took this this very um yeah well uh closely like designed very well designed piece of software and uh and we like i just ripped the part of the all the bits that were not needed so uh like the goal was to take something that works and try to to build a prototype to to see where we are targeting uh yeah try to understand where we were going and and build it um so uh yeah like like mikhail said we we have this structure and you can see that most of the components in there are basically what you would find in a block so when you ask a block to to be produced that is what block to be produced this is this is what you get and there are things like the difficulty which are still currently remain and that's basically uh what this talk is is really about there are there's a lot of stuff left in um in catalyst that still inherit from gas and that will slowly be be phased out but the idea is to just uh yeah get something um pretty pragmatic get something to work done is better than perfect and then slowly uh yeah make it more streamlined more release like production ready uh so the changes i've made so far into into gas to obtain catalyst is to deactivate block reception at the protocol level so catalyst is still connected to an eth1 network so a devp2p ethernetwork to receive transactions but we don't want to receive blocks blocks are passed from the from the east to into json rpc interface um and then i added some uh well the rpc calls for the further for the connection so this is how we were able to build the the the demo that mikhail did uh a few a few months back and i added some instruction i mean at least currently just one um to to get the root of the beacon block so like you can get the uh up to 20 20 sorry 256 blocks uh past beacon route beacon block routes in order to to try to to access the proofs and uh and fields of of the beacon route from um from the from the yeah from an evm contract uh so the block production on the block production side yeah i mean it's been covered by mikhail so i'm not gonna i'm not gonna dwell too much into this but the idea is that you just get uh the parent hash so you know where you want to insert your block so it doesn't have to be on the canonical chain there's a bit of a problem with this that i will uh cover later um but otherwise it's uh yeah it's all the the the info you need to to produce a block and then you return uh like the call returns executable data and then you have the the when when the block is produced it's then propagated on the eastern network received unpacked and and the validators call their own version of catalyst with the same executable data so compared to the to the block production the parent hash has been lost because it's currently in the executable data but otherwise all those informations could be accessed from an evm contract so they need to be they need to be accessible um like they need to be the same as when they got produced so i i currently i did this instruction that i set in the same space as all the other uh let's say execution context instructions like like guest limit like uh like block hash and um i arbitrarily decided to to give it the the same cost as block hash this is not this is not something that was uh really discussed um this is a choice i made because the code for that looks a lot like that of block hash so i just gave it uh give it the same the same cost um and yeah this is a temporary decision it's not the set in stone the idea is that um yeah the block hash will probably be replaced by a beacon block route in the future um yeah so just to to summarize this first part um there were a few challenges um in in creating this this thing that people who want to make their own east one engine might be interested in and the it's basically the fact that when you insert a block you might not insert it on the canonical branch um so at least from the view of of catalyst so there can be a reorg like all the difficulty management is still present in there at the moment and uh so there might be a reorg and so far we we try to make sure it's not uh it's not like it's contained and it will be phased out but yeah that's something we uh like i kept around for the moment because this is a very very intricate code uh that has been honed over the years and we don't want to to touch it before knowing where we're going and to make sure that we we can proceed um yeah another important thing is that guest doesn't really support uh the ability to have block numbers that have spaces like missing block numbers like the parent of a block needs to be uh the same uh the same block number minus one um so if you miss slots if slots are missed the block uh it like the the east 2 engine needs to needs to make sure that whatever reference to the block number will be will be continuous and uh otherwise the like catalyst still has all the the same um all the features or problems uh connected to difficulty so the difficulty bomb still still exists um there's um if you set the wrong kind of difficulty uh it will try to adapt so the the the timing uh the timing might be might be off so there's a there's a few gotchas to to to be careful with at the moment um if you if you want to play with catalyst or if you if you want to just uh do your own version of it um so yeah that's just wanted to to make sure um yeah is there um yeah i was i was told to ask if there were questions so if you have questions otherwise okay i'll proceed um right so the upcoming changes there are a few things like they are going to come to each one that are necessary like the ability to read the bls signatures to to handle the binary trees uh so all this thing uh are are on the way now the question is are they going to happen before the the merge or not um so that's that's a big question question mark we're we're trying to uh to not count too much on uh on on that so it's it could be that there will still be uh exit retrieves after after the merge but uh i think bls is kind of a requirement so there's uh there's there are some discussions uh for that um so yeah we would have to to wait for that to happen um right so so far the the tests i've written are are mostly uh go like simple google test test uh we have written in the go test framework um we want to to use uh some something called hive uh to to do those tests so that's that's what's gonna keep me busy in the next few weeks and of course there's testing the merge so the merge has been tested um technically in in those in those unit tests but yeah i think we should uh before we start building a whole test net uh i would like to to validate more uh more worst case scenarios more more uh more questions that i have and and this is what uh what hike is gonna be that's the actual point of the merge swapping for improve of work at one instant to the beacon chain of the other right yes so um yeah i mean okay uh exactly but that's that's uh what i i think should be should be tested more at least that's where i'm not so comfortable giving the giving the go for for like if you ask me is the merge happening tomorrow i would say well maybe uh maybe first uh try uh try to test it a bit more so um so yeah that's that's what i was getting at um and then there's the question of the sink so the sink is how do you synchronize a catalyst client after the merge happen a lot of things are going to change so it's going to be a bit difficult to uh at least it's not straightforward to to to synchronize uh so that's something that's still being discussed um and uh yeah that's uh i mean i'm gonna go over those things so uh so hive was uh uh it's it's a test environment we've got uh so it's it's nice because it's docker based so if you want to to play with uh with catalyst you can uh you can just create your own image um so hive at the moment doesn't really support these two clients but this is something uh proto wanted to i'm told wanted to to look into so um yeah we're we're going to to work on that i uh i so in our team it's uh it's renee and felix who work on that so it's nice to see some um some interest on this side of of the easter of the research team and we're gonna try to to make those um those two things work together so that we can we can test uh test the merge and get uh and get some proper uh some proper data and and find the the obvious bugs before we we started test net um so when it comes to the merge code itself um well it's basically like any other fork you wait for your uh for a given block and after this block you you start activating like the you change the behavior of of geth or catalyst um the only problem i would say is that um it's i mean it's not exactly a problem but you're going to have to disable difficulty and block ceiling and those two things are are quite um subtle and quite um like difficult to to to handle so um like we're pretty touchy about changing the the consensus for example so this is something we're going to have to look into fairly carefully and yeah the last the last part is about the the synchronous synchronization so there are really two methods that i can envision it's still something that we we are thinking of in uh in in the guest team so we're like we're still uh we're still i mean at least i'm still thinking about it uh but there are two points there are two possibilities either uh you ask the easter client to to provide you with uh with the actual blocks so you let you let the client figure out which blocks are correct and insert them so that's uh that's nice for me because i don't really have too much work to do the problem is that you don't really understand like you're not able to verify because you have this this finalization and presumably after the finalization everything that comes before is garbage collected is destroyed um so you can't really have this property that you can synchronize all the way from from the beginning and verify and re-perform the merge afterwards and check everything everything went went well um so the an idea that peter proposed was since we know what is the the last like the the last uh finalized block you could download the headers chain backward so you you download the block you check where was the parent you asked for this one because you know this one this finalized block is official so or at least it's finalized so you can uh you can find all your way all the way to the genesis and then you can download the blog buddies either from ipfs or from other clients um that's that's something that is going to require at least that's what peter was saying uh some significant tweaks to the downloader so it's not it's not easy as easy but it's got this nice property that you can actually uh check from the start and the other aspect of the sync is of course how do you sync the state so there's this problem with that exists already in um with the fast sync you you download the state but by the time you're down you're done downloading the state all that uh data is um um like the what what the guest client does it only keeps if i remember correctly 256 uh like states like the the state for the last 256 blocks and then afterward this gets uh this gets deleted um so you find yourself having to download the data and then by the time you're done you're done downloading or in fact before you're done downloading the the the machine log the and the node you're downloading from has already moved on and deleted all its data so you find yourself having to to catch up and that can last actually a lot of time um so there's this new uh synchronization algorithm that that mostly peter and martin have been working on which is called the snapsync and provides much better much better results and much better performance so we want to uh to try that and at least to download the data so that's uh that's what we're thinking of uh at the moment and uh yeah if you have any questions if you want to play with catalyst uh you can ping me on discord and i'm i mean most uh i'm on most servers and yeah this is the name of the branch that contains the i would say the most not the most up-to-date version because there will be a new one published in the in the coming weeks once i'm done playing with hive but yeah this one is the one that was built for the demo so if you want to have a look if you have questions this is this is where where to look and yeah that's pretty much it for me i have a small uh not necessarily question maybe addition and question um so with regard to synchronization uh theo mentioned that there have two approaches one of them is to just rely on the him to client to figure everything out and then just feed the blocks to these one client this is super simple from our perspective unfortunately the reason i don't think this can this is feasible is because uh well first thing first uh do the pruning in east do the chain pruning but the bigger problem is that uh and if the client wasn't really i mean these two client doesn't have the necessary code to to streamline the data that we want in our little ideal conditions so for example depending on on whether we're doing fasting we or or normal things we do a lot of things concurrently we download headers in advance with gaps in them and then backfill them we can currently download block receipts block bodies we store some of them in level db we store other others in flat files and essentially have we have this super complex logic where we try to somehow get everything into their right places on disk and the problem is that if ethereum two we rely on theorem 2 to feed us this data then i mean ethereum 2 won't implement all this in same complexity just because guest wants it so it will be a huge step back performance fasting performance wise that is why we're saying that we would i i personally i only see it working at least downloading historical states only if guests can download it from other guest notes without needing to reach out to e2 and in theory we can do that if if we know the head header i mean the hash of the head block then based on the hash we can download everything backwards because everything is cryptographically proven so we say the catch which i realized today is that if we want to include these extra contexts or fields from ethereum to like uh these randall numbers or or whatever else so everything that we need to introduce that we want to expose from ethereum to into ethereum one those need to be packaged up in the ethereum one block i mean it somehow needs to be retrievable by the guest notes during sync otherwise otherwise again relies on these two to have them so this is a interesting limitation i would say that it's probably not that hard to do if we allow each one blocks to be extended with the next necessary extra context and i mean each one already has a timestamp which is just a random number uh it's fine to add granddau and other random numbers too just saying that in my opinion probably relying on on the e1 clients to to synchronize these one blocks is the way to go yeah i see that and i yeah i see how you could take some of those things and add them essentially to like the header payload or something else as the payload so it's kind of embedded in that eth1 hash for consensus um i do wonder what sync can look like in the context of finality and weak subjectivity assuming a user shows up with some finalized route which would presumably have the finalized like eth1 block hash in there what historic sync is requisite there other than just saying give me the state maybe in some fast sync or snaps up sync and let me compute from there forward what is record i know it might be nice to have in backfilling uh everything but what is requisite there while first up if you say that hey give me the state for this finalized block that won't ever happen so ethereum knows currently prone blocks older than 128 which is approximately 15 minutes so anything that's older any state that's older than 50 minutes you can forget about it so the way synchronization can work is that if you give me for example your current head block or the finality block no actually i need your current headlock so so currently what the hero1 chain does is that we request the head and we retrieve the state from the head minus 64 give or take block which is still available in the network and then when we finished up syncing the state we have a fairly recent block but not very very recent and then we start computing from there and the reason why we prefer this almost recent block is because if there are is many reorgs then then we still have the state to go back to and and calculate on a different form now in theorem two i'm not exactly sure how how many blocks um so one finality occurs how many blocks we have to process until it hits so we can definitely play around with this 128 number but in general it's a state in ethereum consumes a lot of memory so the more state you want to maintain in memory the more memory you need kind of so if so two two opportunities if say i give something to the ethon engine that's two weeks old the eth1 engine then could go forward in time you don't need to go back in time from there right you can go forward in time to block sync header sync and then state sync from there correct yes so the so the thing is that as long as uh data is available in the e2 client and i can always ask for it so you can give me that hey this the is of the finalized block and then backwards i can just use the geth network to retrieve all the historical state or however much i want and then i can go forward but every time so in ethereum one the only way i can go forward is by having checking the proof of work so kind of the proof of work is the thing that allows me to go forward now if we nuke the proof of work and we replace it with the proof of stake then it means that i need to ask somebody whether a blog that i received from the network is correct or not so i need big theorem to client to tell me that yes this is the next stock yes this is the next block yes this is the best one right so that way it is perfectly possible to go forward yes so another and then the alternative is the ethereum two client tells you essentially the uh the eth1 head what can you do from that so if i have the ethernet i can download everything else backwards as long as all the contacts is available so that's why i was saying that we would need to add all these all these two fields that you want to expose into the evm those would somehow need to be bubbled into the if1 header so that those become part of the e4 history um do we need to download anything else than the most recent 256 blocks to serve the block hash of code i mean in terms of the history well the state yeah yeah i mean the history the history box but presuming if you have the heads we don't even need the full blocks only the hunters the 256 colors oh yeah yeah right so i need the head state and the 256 headers right yeah yeah so here there's an interesting uh so here we're kind of going into a theoretical problem so practically no you don't need anything else the theoretical problem is that currently uh ethereum guarantees that you have the chain and you can always rerun the entire thing now if we want to do a merge from ethereum laundry theorem too the question is do we want to immediately break this promise and just look out for past history or not uh i would say that it's a bit strong to look it out immediately my personal suggestion would be to maybe launch uh have e1 as one chart and have multiple other evm charts which don't have historical states so there you could say that you're willing to nuke out all states older than say 256 blocks and eventually when people are used to to this model then maybe then you could remove the history from each one but i guess my my main issues i i always mentioned this to guillaume that if you're willing if you're going to merge east one into e2 and simon simultaneously just nuke all the history then that will be a fairly large i would say there would be a very large opposition from the current why why would what do you mean by nuking the history well if i'm not maintaining all the blocks them to uh blocks ordered at 256 then essentially that disappears from the network because the the key thing there is that those block headers link to the the um log events as well so when when contracts emit logs um there's a lot of daps that then depend on that history and having the full chain history there is that that right yes that's i haven't thought about that but yes that is also right so you're you're actually concerned specifically about the blocks and and just that replayability like the reverb re-verifying the chain trust the data but why would we why would we ever lose that i mean it's a pure like we have we have to have the capability to do like a weak subjectivity thing but you can always replay all the blocks from the start yeah but you have to store them somewhere and if you flush them out from the network then where are you going to get them back from wait but does the east one history like aside from the depth in the log use case is there any like is it just like a museum essentially like like why would regular consensus need needs to care no that's what i'm saying they don't need to care but that's a current invariant of the system that we need to be we can break it there's no technical reason why we shouldn't break it it's just social reasons right right consideration there but i mean you can just i mean anyone can just store this this history forever like i the having the peer-to-peer network have to store it seems like a very useless requirement well i mean anyone can store the entire history of the internet yet the internet archive has a fairly hard task at doing it so the thing is no i get it like it says that like just running continuing to run the current equivalence of an archive node if uh we defined that as being archiving the premier uh a premier g1 would just would be like would would require explicit explicit work if we want that functionality to continue existing do we for no i mean you can always do it right i mean you can always be an archive note the question is does i understood as does the peer-to-peer network support you getting all the historical exactly exactly coming in archive notes right right and that's the question that that i'm i really wonder if that is really required and why not use bittorrent for it instead one so one one interesting issue here is that with an archive note currently if you want to run an archive note then you can rebuild it from the network from the full nodes but if we were to stop the ethereum network from maintaining the chain history then you will not be able to rebuild it from the ethereum network you would need to find some third-party source again this is from ethereum's network's health perspective this is fine it doesn't you don't need the history it's just a social promise that we can break yes we need to be perfectly clear that we're breaking it break it with intention if it is to be broken there was this idea to store uh sorry there was this idea to store the the blocks in inside the the storage tree a try is that uh would that solve the problem why would that be a good thing well okay it would be a bad thing because we would increase the size of the state but at the same time if you go back in in history you have uh you have all the previous blocks stored in uh in your state yeah well i would might as well just keep it stored in the way as they are yeah and you essentially have that just through the nature of it being block chained it's all uh everything's committed to at least in a similar way to state maybe another comment on this uh like does the network provide history thing isn't it already the case that yes like it's it happens to be default behavior but nobody is actually incentivized to actually have the history like i could easily change my note to not have the history right now and i would suffer no downsides right yeah and there are clients that do that uh ditching the history is generally probably viable but you might have issues maintaining certain connections if your node is unable to respond to requests right right you lose the peers that are currently thinking which aren't doing much for you anyway is that well the thing here is that uh currently [Music] most nodes assume that most other nodes play nice and there's no reason to add extra protective mechanisms but deleting chain history is essentially your you become a leecher of the network you just gather the data you don't give anything back and if more and more people start using that then probably clients will start adding protective mechanisms to try to filter that out and it just boils down to this war between clients of trying to somehow get away with leeching as much data as possible right but i disagree here because the node is still a full peer-to-peer node in terms of relaying blocks um providing recent history even it's only you're essentially a lifetime okay not like that you do have the state but you're not the if you don't have the history then nobody else can sing from you so yeah so you try to help others get onto this this is the philosophical question where that actually is important to think from if if that's how users expect to be able to sync which might in fact begin to change in this notion of like having finality uh in a weak subjectivity but definitely some like uh sensitive things to consider when we're dealing with thinking about the still relatively undefined component of how to do sync in this context um we're 15 minutes over i think we're gonna i think we're gonna end it here i really really appreciate everyone showing up today thank you all the speakers um that was super informative i'm glad this is on youtube so people can can learn uh and and begin to collaborate more as you know uh there's if you took anything away from here there's plenty of work to do um there are there's definitely some like concrete foundations all around uh there's beginning to be things that look like specs but there is a lot of work to do and there's been a kind of small group working on these things as the beacon chain was launched and as always eth1x is maintained but it's time to dig in deep on all these elements so um as always generally collaborating in the uh ethernet discord um is the e2 call although that's more like kind of just us checking in on things we might think about how to restructure that in the context of some of these r d things or have some uh smaller breakout calls and stuff um other than that thank you thank you shawway for organizing this and again thank you everyone for taking the time and it was a little bit long thank you thank you thanks see you all in person one day thank you promise yeah all right was a good place [Music] [Music] so [Music] [Music] [Music] [Applause] so [Music] so [Music] so [Music] [Music] [Music] [Music] [Music] [Music] [Music] so [Music] [Music] [Music] [Music] [Music] [Applause] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Music] [Applause] [Music] [Music] [Music] [Music] you 