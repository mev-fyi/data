so I'll talk today about modern methods of factoring well unfortunately I know very modern because the main advances in this direction were in 80s and beginning of scientists and just the recent factorization records that we have seen largest application of these methods to increase in computer power a built-in power that is in hands of researchers Academy and so on so I'll try to give some overview without going deep into math just a bit deep so that if you're later would like to learn a bit more this material may serve like basis but I will also publish some additional material on Google Drive with some simple explanations of number field sieve and similar methods which can be helpful if we in the future decide to explore the harder perspective of these things more details ok so feel free to ask questions and I'm between so what's actually the problem of factoring that's formulated and it is suppose we have a big number capital n which has n bits small N and the form factor is defined it's prime decomposition so find it after addition into prime powers and this factorization for integers is known to be unique up to the order of factors but there cannot be different the compositions this is actually the property that factorization is unique is important because it doesn't exist everywhere and for example in fields it's of course not the case because in field every element is divisible by anything else and in field every field element can be present and have like multiple infinite number of representations but it's property of drink of integers or think of polynomials and of some other so-called Dedekind domains which includes some funny algebraic fields which are basically fields where you accession fields where you add some roots of certain animals or some square roots of negative numbers and so on there are some fancy sets where fertilization is unique and this is used some extent and bacterial nets but not everywhere that's because and because of that mainly well even though for example discrete logarithm problem is related but in the sets where there is no unique factorization many methods do not really apply there ok let's go on so if the number cannot be factored it's called prime number and it's really easier so it's there exists remains the coefficients which are polynomial and the number of bits that determines if number n is prime factoring is of course an NP problem because you can quickly check that factorization is correct but it's not known to be np-hard and I think as far as I said many people build it's not only hard and due to that algorithms faster than just pure exponential exists but another had purely polynomials do not exist so we don't know where where it really is factoring problem oh how can we simply a factor so we just go over all numbers up to the square root of N and if numbers prime we test if it divides and and of course when we exhaust all the numbers up to the square roots we exhaust all the all the numbers all the factors and what what remains it can be bigger than square root of n but it it must be prime not they single prime that remains and complete city of this is about square root of em that's true down F it's a bit more interesting method that related to methods I will discuss is the following so suppose you generate a two numbers x and y and then you test if X square equals y square model the number that even a factor Y this equation interesting because if this happens if this holds then if we like food twice queer to the left side and apply the simple formula then we see that there is a product of numbers that equals 0 modulo the modulo n some of the numbers in try to factor and there is certain chance actually quite high chance that if we that either of this is actually not in itself but some factor of n and if we compute the GCD of X minus y and n or X plus y entender it's very likely that we actually get a factor of n otherwise so if this doesn't hold then we go to step on and generate a new numbers but in general if if this holds then we almost win will usually with probability more than one half so if this numbers are small enough and by proper arranging x and y we can make the complexity about square root of n again why is this important it's important because I will show how we can generate x and y in much more interesting way and that's actually how factoring becomes easier than square root how exactly ok so we generate x and y so the equation is more likely to hold and this is in the advanced factory methods now there is some interesting stuff suppose that for some number X X I bigger than the square root it happens that if we compute the square of X and take it model M and we have some numbers that then it factors into powers of two three and five just two three and five so some two to the some power of a a I three to some power of VI and five to some power of CI so we can actually factor that so we try for every set that we compute this way if it factors into this into two three and five and what's interesting interesting that if we collect sufficiently many of such numbers and if you have three factors then actually yeah then I need for such relations why is that because if I collect four different relations like this with sufficiently big X bigger than square root then what happens if I have four such relations I can compose a matrix we are the powers of these prime factors are in the metrics and then I seek for linear combination mode to model or to Y modulo two because there's basically means if some for example if some column if we sum up for example some rows and there is 0 modulo 2 this basically means that this numbers some to some even number and this basically means that if I multiply this that I the J tau together if I multiply them up but then this exponents they will add up this will be even number and this means that this will become a square and the same will happen for 3 so if this we collect sufficiently many elements in this matrix then big chance there is for souvenir dependence in modulo 2 and there exist some rows with alpha beta gamma and Delta zero or one so that this linear combination gives zero zero zero model look to if this happens then for example yeah this alpha beta gamma delta then we know that if we take X e to the Alpha X J to the beta X gamma X K to the gamma except to the beta Delta then the square root of these guys that the square of this product if we put this here then it will translate to some when everything thumbs up so in this vector will get so some you to here to you to here to you three here to u5 basically here and if model to the salt gives all zeros and then this means that we have now the product of squares into some power so instead of having two to the something through the some five to the south will not have four to something and so on and this basically means that this is the square and this is the square both are squares and we get this V and W this way and we get this equation that we were looking for so there for example for example suppose that we would like to factor number 143 anti-tick square root of 143 which is smaller than 12 and I compute powers of this number size slightly increase and computer fortune square 15 square and 16 square they do not they're not divisible by two three and five but 17 square is just three nine ten square 75 21 square is if I take all this model 133 twenty-one square is four for one equal 12 which is 2 times 3 square and so on and if we note then so if you look at this then if we multiply 17 by 19 then there are squares for this course will happen that will have 3 by 3 and 5 squared 2 so 17 times 19 squared equal to 3 squared 5 squared which basically means that 30 if we take this modulo 143 we have that 37 squared equal 15 squared we put this to the left side apply a simple formula and see that there is of idiot equals equals zero model 153 we compute GCD into half 11 and 11 is the factor of 1 but if you are any questions so far oh this is super cool because super simple one question I have is is it still the case that the probability that that the vnw kind of won't be equal or like the GCD just doesn't give you any Prime yes there is a chance that it doesn't give you but in practice it's it's quite low so I think with probabilities 1/2 if you get this like equation with random V and W if they are both sufficiently small then I think with with very high probability you have you get an extra factor and all these like provable things or like heuristics yeah I think this approval I think for for sufficiently small B&W for which this happens you can rigorously prove that the probability is very high and what do you mean by sufficiently small very small I think smaller than them really they're like close to square root or if one of them is closer to square root then I think you can bound this I've seen some estimate how you can compute it I think it's widely assumed that the permute is about 1/2 that you get correcting because actually it may happen that this you don't get you don't get the answer only if only if one of this is bigger is multiple of M right so if it emerges M or if it's 2n or 3m but if x and y above smaller then then probability of this is small right okay so this method is very nice but the problem is that the number the direction of numbers divisible by 2 3 & 5 is very small and we have to adjust this method somehow to to benefit from this so the matrix part that we like looked for linear dependency is very simple because some of the three factors but we have tested so if you see that I for example tested twenty numbers and only in four cases I got nice decomposition so the smart and method of factoring I introduced what's called smoothness and we call number zero be smooth if all its vectors are smaller than B or like smaller or equal to B and in in our case if we say that it's like smaller than B then clearly the previous case when you consider only two three and five we talk about six smoothness but of course we can consider bigger smoothness value so how this is how this works we take some X out there later how we take this X we compute X square model is that and test if it's be smooth so we select some number B and test if it's be small if yes we add it to our table and this is called seeding if we find about B well B over blogger if not be good to be formally then if you find about be such X and then we have a matrix with so we become so if it's be smooth then in our table of exponents there are at most B columns because every mccollum correspond to exponent of some prime and there are at most B Prime's here listen through also in our metrics there will be big columns B over and if we find about be such X then after applying for example the ocean elimination to the metrics and we can find some linear dependency and basically it will see that pretty much the same way that some product of square so product of squares gives you another square and when we aggregate these guys and take it modular and then we have X square equal Y square and we are done so what's the complexity of this principle so basically the metric step we work with metrics B times B but interesting that we don't need B cube operations to find linear dependency to find basically an element of the kernel because the matrix is very sparse and interested there are special algorithms called block V demon or block lancers algorithms for sparse matrices that find elements of care of kernel of sparse matrix and usually so this algorithms works in about B Square times well then sparsity somehow so it's about this queer and this is one step that of course can be improved in hardware so so there will be improvement to see even but the metric step I can already tell that there are several suggestions how to make this this thing much faster and much more efficient and hardware so this can be considered independently of this even part there are very complex part of number field sieve but this thing is there it is very simple and can be explored I think relational easy how fast it can be on custom hardware to run the service or modifications of this elevations that are suitable for multiple cores on like this so what about seeding step the second step of course all depends on this numbers ad because the smoothness probability for a number it depends how big the number is because clearly if the number is smaller than B then probability that it's B smooth is 1 but when it increases there is a very big chance that it has a big prime factor and because of that more and more numbers are not be smooth so the probability decreases significantly with the girl as that grows and we have to take this into account so the recipients for Stephen we need B such numbers and too basically they're complex to find one number is the fraction of B smooth numbers amount our outputs for given that and multiplied by the complexity of test so how expensive it is to test that that is smooth but actually what's really interesting that the modern sieving methods they are quite fast in a smokes test and they may take into account the benefit the fact that we created Z in very special way and the amortized cost of smoothness test is very low so what's what's really really important here is this probability the probability that this number is smooth and the smaller we can make this number the bigger probability it is that is be smooth and of course since the metric step is about of this queer we would like that this B of B Square to so that both steps are balanced and if we make the compass complexity almost 1 or closed 1 then the trade of is where the smooth probability is 1 over B but it of course depends how because that is so the smaller that is the bigger we can take this be and the bigger numbers we can basically factor questions so far yeah so actually I like to stress that this smoothness property is very very important thing it's actually why factoring is much faster than brute force factor in that square root of n is because we can in integers there is this exist this notion of smoothness and there are numbers that factor that this way so in in other sets we don't have this notion and because of that problem similar to factoring like this could look are much harder there so this is the crucial property for factoring and what's a typical size of B that we're talking about if we want a factor like 800 bit number or something so you see that we make both we would like for trade off we would like that at the time of both steps is all be square so if you can spend two to the 62 today 60 time and then your B is two to the thirty clearly alright okay now I understand why the matrix espouse yes so numbers are like 30 to 30 cents 30 bits and of course [Music] they have don't have too many prime factors so they can have like every snubber I think in recent records like 50 or 100 or something not terribly mainly so compared to the size of the matrix the number of non-zeros is very small like one yeah you can mention like 100 over but at 30 very small so you mentioned that there was some suggestions to use a six for the matrix that but what about the seating stuff yeah so what about the sitting step I'll have to go how exactly ceiling works in some in two algorithms in quadratics event number field sieve and then I will explain so how hardware will help there so about the smoothness how we get actually this faster factoring that's faster than square root it's for example if B is constant like six we have used then probability that the number is be smooth is one over N to the logarithm of B but if if B is this big like exponent of square root of logarithm so which is not like logarithm by 1/2 but square root of lager then the probability the number smooth is one over square root of B and because of that if we defined be such numbers we spent B to the 3 over 2 time and the metric step is be square so by further tuning this number we can arrange that numbers are factored with about this complexity how I will be a bit more precise in the next algorithm which is called quadratic C so what's interesting about quadratics if it's it can be explained relatively trivial compared to the number of fields if so what's in the connecticut what we are doing so the first nice step is how we select our x if we select x close to square root of n so square root plus some epsilon so of course it's the integer part of square root then if we compute x square a modulo n then if we took and we're modeling so this will become almost zero and what remains is epsilon square and two epsilon times square root of n this makes our number Z Y of size about square root of N and because of that it's much more likely that it's be smooth compared to random that Mutulu n because number that is half of digits of n is much more likely to be be smooth so we're going to find all of B over what where can be such X we construct our equality of square reports and we are done but how to test smoothness and for smoothness test it's interesting so we basically try X and we have arithmetic progression shape so if it has be square then we take square root of n plus 1 square root of n plus 2 and so on plus B Square and how we work with that so why it's called quadratic sieve we test big smoothness as follows so suppose that let take some prime smaller than B then we solve an equation that X square minus M equals 0 modulo Q and 4x somewhat closer to square root and we can solve it because in if it's prime then such equations solved easily there are polynomial algorithms to do that and if we solve that then for any K we have that if we add to X Q or to Q or 3 Q then model or Q there will be all the same meaning that if Q of X divisible by Q that means that Q of X plus KQ is also divisible by Q and coming back to the previous so remember we constructed X square minus M of 4x which is arithmetic progression and this means that for some first element so how to find smooth numbers here we take one number one prime and we find here the root we find we solve for some X in the beginning of this equation X square minus n equals 0 mod lookyou we solve it and then so suppose this is true for this number and and then this means that if this is divisible by Q then we can add to the argument any multiple of Q and with like an arithmetic progression in equal gaps the outputs of our polynomial will be divisible by Q and we can divide it by Q like a radish and sieve and eventually if we repeat this many many times the different Q some numbers will boil down to 0 the one because we've factored out all the prime numbers of them so how this Civic works is that within in a very long curry we'll find some number to start with and then every cues number we divide by Q and we know it will be divisible and to repeat this for for all the prime numbers and eventually what remains are there must be about B numbers out of B Square let up once and these guys will be be smooth questions I wasn't really able to follow the last few steps okay so they want me to repeat or you want later to take a look into the final I think I get it I think the idea here is basically that instead of like just sequentially trying random numbers there are multiples of like some small medium size that smooth number and that way you have a higher chance of hitting something yes that's more or less the case so to summarize it shortly we find one number here that is divisible by prime and by solving an equation and then we know that we do that for each prime less than B yes for each prime less than B okay and then so we have the list we have the list of all these numbers that are squares of root n plus 1 up 2 root n plus B squared yes and among them some some of them are divisible by by a prime we know that we have found a one such number so we we solve this modulo Q and we find such X so this is easy to find such X this means that for any prime it's easy to find in this sequence which ones are divisible by this crime so yes it's it's the same veritas and stiff but butte in a bit different way so in there at us and see if you have like Prime 3 and you you cut out 3 6 9 and but here you got not 369 but you have to start from somewhere in between from like 47 for example you figure out that queue of 47 its visible by 7 and then you divide q47 then P 47 plus 75460 168 and so on so in equal gaps you find numbers that are divisible by this prime and you divide so you basically like overwrite and then you repeat with another prime it will give you another if metric progression so and you divide you divide until it's not divisible so if it's divisible by 49 you will divide by 49 yeah well 49 is not a prime so you it's right but if you if it's 7 right yes find all the ones but some of them might be divisible by 7 squared 7 cubed and so on yes I think they recommend to divide to try to divide it further okay so you indeed divide it and basically then you do that for all the primes and find all the numbers that you for you to reduce to 1 yes ok ok and because of that the amortized cost of tests in one number for smoothness is another high so how is it compared to the kind of trivial constructive method where you just try random Z it's much better because the these numbers are relatively small so this they are about square root of n right and because of that the probability for them being be smooth is much higher and this basically means that we can we can take this smoothness bound with the same smoothness bound as before we can attack much bigger numbers so because if in in regular method if said for example you can spend like two to the 60 time and then for random said the probability to be a smooth is 2 to the minus 30 for example and you can break I'd know 200 bit numbers with this complexity then since in the new method Zeb's are of size square root of n that basically means you cannot attack twice bigger numbers roughly kind of two tricks right one trick is to make sure that you see has half the bit size and then the other trick which is the sieving trick presumably you could keep the first one yes so the serum gives just and of Itamar ties is the test cost but I think even three really the test cost is not that high well if you do it like stupidly then you have like B numbers if you have like B smoothness you you have B square numbers and you test everyone for this movements you have to try B factors right you spent B fact be divisions trial dividends for this betrayal divisions for this and so on so eventually it stand like B cube so there's a time memory trade-off here right so like basically if you if you do what Justin did not do the second step by sieving but with the trivial way then you can probably do it with much less memory or more locally yes yes that's that's certain true there are even faster methods in the very end like an extra method called elliptic curve factoring that allows you to find small factors faster it allows you to like if you know that your factor is small then you can find it whatever it is faster than and by just trial division so you can optimize using this trick as well so there of course we quite many optimizations here and the fact that so how how you can exploit this in hardware so you recall what I said about arithmetic progression that we do this even using arithmetic progression so we access memory in a very predictable way so we we take this number then we step by this prime that we divide by it and again and again so all these steps are pretty predictable and it's possible to share this task among several cores and there this is quadratic sieve the modern method called will use what's called latest eve but they all share pretty much the same strategy and memory accesses here are very predictable and I think after you have generated this numbers you can properly share them among your smaller computer so if like everyone can do their tasks to filter out they to find smooth outputs doesn't make sense this is the second crucial point I wanted to make that the sealing step is very predictive and oh it requires quite certain amount of memory it's it's very predictable and you can even compute these guys on the fly and you can divide this into segments and do this process and of independently for different segments just you bit loose into the total complexity but you can save a lot of memory for every single core right ok so then so we proceed and yes interestingly so the optimal B is e to the 1/2 e to the square root of 1/2 logarithmic fan I will not computer directly right here so because of the square roots this all this complex estimates are a bit odd and not trivial to state but the thing is so since B is this way then the total complexity is B squared and this basically means that when you square this you have to instead of 0.5 under the square root because it's complex of quadratic C and the next advanced method is called number field sieve and number field sieve uses very sophisticated algebraic tricks to make this number even smaller so all the rest is almost the same but they managed to have this said even smaller than square root of n they make it like some root of n which is not like square root but something in between like two point half or cubic root it depends but after they if they can if they make it like smaller the smoothness probability greatly increases and because of that you can attack bigger numbers that's the core advantage of the number field sieve and how they do it so this unfortunately involves quite a bit of algebra let me just state the main points here and maybe later if you would like to understand a bit better you can return to this but what's important here is that algebraic part doesn't play almost any role into there by computing this and hardware so so the code doesn't change significantly from the number from the quadratic sieve and the properties of architecture needed to break the number with number field sieve it's almost the same as for the quadratic sieve so you can have understanding of quadratic sieve and built ASIC for quadratics if it will be quite efficient for number field sieve as well so how number field sieve works first they take some polynomial of small degree I conflict numbers about five or six so that it has some roots that we know modular m and this polynomial actually can be derived rather trivially so we can imagine so this number say if this is five then clearly this number should be about five fifth root of n and we can imagine that if we a digital representation of N and divide it into five segments and have this M every presentation of n then we can find such polynomials our ability to find and there are many of them suppose also that if we can see there are not modular and but just over integers that it has some root which we denote by alpha so by default it's probably not divisible I don't have any real roots and actually if it does then it probably we can find a factor of n trivially so we assume it doesn't and then what we can do is that if we have la anything or which are built over alpha as variable and if we substitute into this polynomial number M we actually get a homomorphism from the set of polynomials to integers and this nice homomorphism it has very nice properties that basically if if for some set of numbers a and B we consider simultaneously a I minus bi alpha where alpha is this root and we consider a minus BM so we try to find in B such that this thing is so this thing is smooth and because of that we can find that sum product of these guys equal Y square in parallel we do the same but in the algebraic number in the algebraic number field and we try to find smooth numbers of this kind and if they are smooth then this product is we can find by using the same linear algebra we can find it's equal to better Square and if it is equal to sum by the square that we know but we haven't computed it yet then it's possible so there are some sophisticated augers that compute the square root in Q of alpha but not for every number but only for numbers only for the elements that are squares so it's not like in the field where you can compute square root but it's in some bringing of algebraic integers where you can compute square root when it exists and if this two things hold then basically you can apply our home morphism to be the square and you will have X square and on the other hand if you apply how morphism to this product then you will have product of a I minus B em and if and we know that we have found that a and B so that they equal to Y square so that eventually have X square equal to Y square and what nice here is that well we have we search for a and B so that this number is smooth and this number is smooth as algebraic number and we can make them rather small so that's the main trick that if a and B are small enough then M is also small enough we remember that it's some root of M so this guy is small and this guy is also rather small so even when the even though we need that both of them are smooth this probability is much higher than for the zit that we served before the smooth and if both of them are smooth then impious efficient team finds officially minion that a and B then using the same linear algebra trick we find X square + y square so that's basically the core of number field sieve so to summarize even if you don't understand anything like I did many like first ten times I read about that so the main property is that we work in two areas note honestly in integers and in algebraic integers and we managed to have our numbers both like smooth in both places and interested about this irrational but yes I think that the moat I think this we can view them as actually that of alpha because well these guys are always integer in our case I believe so actually this so probably should be I think this Q should be so here the co morphism is works for Q but I think it's older further explanations they are over Z over alpha yeah thank you for that so basically we just to find smooth elements of this set indeed of Z over alpha is not not trivial because we cannot divide that easily in that field but what we can do is we can again map this number to integers using simple formula so we substitute this to the know that we have this is called the norm and fortunately the norm is multiplicative so if this guy is a square then so is the norm so what you basically do is we find a norm which is be smooth and after composing efficient sufficiently many norms we find the product of elements that is a square and because of that there is very high chance that the product of algebraic elements is also square oh yeah I think I will I'll skip this details yes there are some explanations why this numbers are small basically they are incurring D about like n to the zero point four and this section is sufficient to give us this increase in the length of the number we can factor if we go to some concrete complexity so we can translate this ideas into concrete complexity and there is a very well bit weird thing so if n is the number of bits then this is the formula then we have take the square the cubic root of the number of bits we also have to take logarithm of the number of bits and take power of one over point 1 point 5 multiply all this by 2 point 4 in the deck 16 and this gives you the complexity in the way that the square root of this is the smoothness bound so practically for recent factorization space is about like 2 to the 30 and this is about the 60 the 64th that's of course good question but if we translate it into core years using the recent accreditation records then we will see that this is about 50 so each element is about 50 CPU cycles so it's some kind of basic integer operation big integer operation which gives you like I think about TCP uses so this is the complexity of number field sieve but this is just the computation complexity the number of operations but this doesn't tell us so far what's the complexity to break it in dollars but that's actually of course very interesting for us what's the actual complexity break this thinkin dollars we know are from recent factorization records how many core years they have spent but this core years are very well but but weird core year so these are not GPU years these are mainly class three years and clusters of different sizes in different countries people around by different people and so on so these numbers suggest aggregation of something and this is not very precise but what's interesting that the architecture here is well I've been busy to the big extent and here just normal clusters what we can do is we try we can try to translate the cost of the very cost-effective this 900 careers into the electricity cost so how we do this we take a number of years and we calculate how many hours we calculate how much one core consumes the energy and we calculate how much one kilowatt hour costs and dimensionally what I get is that this seven seven hundred ninety five bit number it costs only only eight thousand dollars worth of electricity eight thousand dollars worth of electricity so electricity is not the dominating cost here but we know from the mining that actually for very big computational efforts electricity becomes to dominate and this actually means that these numbers from the electricity perspective this numbers are very small so from the electricity this cost like eight thousand dollars and this cost $30,000 electricity that's for from practical point of view is not it's not really big and of course we can expect that if proper amount of if proper funding is spent into that we could see like if you want decide to spend 1 million dollars then even on the regular clusters he can easily factor nine hundred bits or something and they're on proper hardware even probably more what I mean might be any questions so far yeah I mean I think this is very interesting maybe we should go through the numbers one by one so that we can properly understand them because like yeah ok so basically I'm I take number 900 this is two to the nine point eight then how many kilo hours in the year give hours mean thousands of hours it's eight because we have like 25 hours 24 hours per day 300 days and so on so it's like 80 hours in the year and then I take a rough estimate at one core in the cluster consumes about 30 50 baht there's of course not very precise so it can go up and down a bit like maybe factor of two or something and then I take that one kilowatt hour in some rich places well places where electricity is cheap for example like near hydro plants there are numbers published by some harder plants in the US that they can spend like four cents they can sell you kilowatt hours for cents by percentage and this basically means that for three that it's about 2 to minus five about like quantities of USD if you multiply this together you get this number yeah okay so how what we can expect from number field sieve and the cigars so there are two main importance here so one thing is about seeing that on 100 tests all of these query integers forbid smoothness oh and there are methods that do this using all B of memory and all of this queer time so these memories because we need we store resultant B numbers but fortunately we don't have to store much more than that so using a bit the furthest main processing segment by segment we can do this with all the memory but that memory access is not random can be paralyzed and memory is used predictably and that's why I think that and there are some kind of theoretical designs how circuits can be constructed for NFS the problem here is that the current algorithms current code for NFS may be rather sophisticated because there are like thumbs of improvements by a factor of two or three how exact if we go over this integers because there are pairs of them and and so on what we store but we do not stores of something optimized for class there and so on so there is a ton of bells and whistles here when we might have to like dig up to figure out how this can be properly implemented on hardware but I'm pretty sure that with a certain amount of research this can be figured out how you should do this harder problem and comment here like this I mean we know that basically the computations itself we can probably expect a factor of a thousand to a million reduction in the electricity right from mining and so on so yes so that means that what we should look at is actually not the power consumption of the CPU because that will go almost to zero but maybe just the power consumption of the memory and memory controllers yes and also another another thing is that we now have extrapolations from this amount of core years but the thing is the actual amount of computation so this is not the amount of computation day they have done is that how much they have spent but probably a big fraction of this core years were spent on IDE no cache misses or memory accesses or whatever so it's not it doesn't mean that the number of operations is exactly this number so it's maybe because of x86 architecture this number can be reduced significantly in terms of the number of operations that is being done so this complexity the estimate that is here is based on the assumption that this core years were spent entirely on computation so if like only 1,000 of them were spent for computation there there should be a subtraction of another 10 here and when we calculate the worth of electricity and so on yeah and then the second very important thing is in linear case so basically we find a kernel element per metrics with all been nonzero elements and currently we spend at ob squared time but the thing is and all of the memory but the thing is there are suggestions to have for like multi-core algorithms that using the same amount of memory can do this in much smaller time maybe with at the expense of more random memory exits but still if we can spend less time on the linear step this basically means that we can increase to balance these guys again we can say if this step becomes much cheaper for some reason then we can balance them by increasing the smoothness bound and so that oh and with increasing the smoothness bound we can break bigger integers even using the same algorithms just by proper balancing the things together do you see the point so currently both take be square but if this takes not be square by but be 21.5 then clearly we can balance them together and use some different B so that they again use the same so maybe we should test more integers so that the the smoothness probability is different so that they have they spent again the steps and the same where does the B to the 1.5 come from a bit of 1.5 comes from parallel algorithms that works with the sparse matrices so there are suggestions by DG beam by Daniel Bernstein who suggested some parallel fictions for this block me demon and local answers exist in algorithms which are not very much parallel but he has some suggestions they are mostly theoretical ones but it I think there are others which can be made practical so it's very right interesting but how would how would parallelism help though because if we say what we estimate is the amount of electricity then parallelism itself doesn't reduce that well there you see is currently the we spent B Square time and B memory so that the electricity here spent for this is more like B cube but if we spent the same memory but B to the 1.5 time the electricity will not be B cubed but B to the 2.5 you so he basically suggests replacing some memory with course and because of that reduce the overall computational time and thus reducing the overall electricity consumption wait so you're assuming that the memory is always turned on is that the assumption because I I think there's memories now where you basically only pair in the next accounts when you want to access them yes yes there are static and dynamic Ram and it's of course very right and for interesting questions so which one should be used here I think it depends on the algorithm which kind of memory use because some I think the one that you pay only for you can turn it on and off it's I think it's bigger and by itself it requires a bit more space and cheap and so on I mean SRAM is crazy expensive I don't expect anyone would use it yeah it's also expensive but if you run it for a year maybe it's worth it I mean my impression is you literally can't get gigabytes of SRAM at the moment it's so crazy like it's come it's several orders of magnitude in between several orders I heard a different chip I don't understand - okay well I think that there but I mean I don't think that's such a big deal because even even DRAM the cost when you don't use it as actually not that high you have to refresh it but I mean it is very low I mean like a good laptop doesn't really significantly drain its battery and standby and it still refreshes it's D right yeah I remember there it's like fraction of ten or something quick twist you need um much higher than that I think I think we don't much much more than that okay I'm totally not an expert here but I would really love to talk with some extra say about all this I mean one thing to consider maybe is like there's this memory from Intel called octane memory and from what I understand it doesn't need any refreshing so it's like persistent memory but works a bit like okay this but there's still a lot slower than Ram as well well it between SST and Rams right but it's not that much slower is like a small constant more more than 10 right that's more constant I thought was less than 10 well this is something like 5 but I need to check again okay I shall one maybe sure and one question I had you know you mentioned the extrapolation from the existing numbers yes how much variance is there in the run time I can you get super lucky and just completely algorithm really quickly oh I don't think so because basically you you have to collect quite many relations quite many smooth integers here and so your only chance is to find some sub matrix that is linearly dependent so like so that you actually does you don't need all the B numbers all the B prime numbers to give you the solution like what I had in my example in my example I had that I need only three and five so I didn't need to but the chance for for this to happen is not so high thinking in real numbers and there are actually it's much more likely that you will have some positive solutions or pursuit of solutions if you have to filter out so that as far as I know people even create numbers that mattresses with much more rows than needed because otherwise there are they get some solutions that they don't really not really work right okay so what else interesting that I made some estimates for example in one case as I just extrapolated from the current CPU utilization record and in other cases I I try to figure out what the maximum Ezek advantage could be so I got that if we talking about course across the course that spent like 30 but then the biggest advantage I think would be about about a thousand in electorate advantage I mean there are some more detailed calculations in my in my paper in my report I will send the link soon in it so I also computed so I for some conservative scenario I can I added some small advances like 2 to 2 to 3 into the algorithms improvement and somewhere into Moore's law and so on so they don't differ significantly but the rewards there is also super conservative scenario where I analyzed the case when we we have found this B to the 1.5 time algorithm for or for mattresses and then I try to estimate the security in this so how I did that I I took 80 BTS key and I try to figure out so if you take the current Bitcoin hash power how much it would cost if the same if we don't run a short 256 there but a yes how much it would cost in terms of electricity to break 80 BTS and actually got that you would have I'd spend $50,000 for this using current mine well before the fall - thousand five hundred thousand years but it was a Bitcoin price a month ago and then 128-bit security it's reasonably that it's cost of 150 bit recover key recovery Nasik which is about 67 USD which is of course beyond our capabilities and but what is 256 bit security 256 I say that the system has big security if it becomes 128-bit secure if you cut out half of the key or if all algorithms get quadratic speed-up that's actually the same so and in this circumstance I analyzed the perspective of different model sizes and here are the numbers so you see that in this metric if we stick to CPUs to the clusters on which the most recent numbers are broken then 80 bits acuity is 950 bit and 128-bit security is 2,850 bits but if you consider a conservative scenario where we have some advances in a6 bringing in 1000 reduction montaigne factor in the reduction of electricity cost then 80 bits is not 900 bit number but 1500 it and what country did the security is 1000 bits more for the number to be broken this is the RSA model they did have both prime numbers and super conservative scenario is if it found this advanced hardware algorithm for mattresses then we basically have all our modular sizes increased by a bit 30 20 % or something and also there's this table he is in the report I have two extras there is a slide about Whittaker factoring method and there is a slide about discrete logarithm basically in the else capability curse and for this cooperation is the principle is quite similar so what they basically do if if you want to find discrete logarithm of some number of some element of a prime field the prime field if you want to order from H with respect with base G then you generate many heat many exponents heat and to test if the number is bismuth with the same saving principle and that you find sufficient in many such beasts most numbers you can pose the same linear system and basically for every you you get discrete logarithm for every number in this factoring base for you you logarithm every prime number smaller than B and after you've done that so how to fact how to discrete logarithm this guy you take different how do you do that well you if you find B such guys then you have your the same metrics actually but you you call the columns so you compute your linear dependency not modulo 2 but modulo pi P minus 1 so if the prior to this P then basically means that all the exponents that they wrapped up in minus 1 and if you have different if you have all of BiBi smokes exponents with different heat then basically you can solve the linear system and well this linear system the unknowns will be actually these guys and you will find it by the ocean elimination ok so you compute the logarithms of all your primes you you you now that's actually the last light right you compute the logarithms of all the primes and then you just one randomizing age that's all you made me HP smooth and then you yes any questions on the entire oak I mean as Don had said I imagine the electricity costs for for a six will be somewhere between a thousand and a million a thousand being the kind of the the less conservative estimate as opposed to conservative mana yeah I mean I think one interesting exercise would be that we could do trivial me now and would be to just look at the consumption they like to see consumption of memory yes you can but I I think that will give you a factor of two to the five already because I suspect it's about one watt or something like that instead of two to the five foot so what I for example I have calculated myself that there is this for quick cash which requires four Z cash about two hundred megabytes of RAM and if you take a six for them and then again two hundred megabytes two hundred megabyte uh-huh yes and basically what you do with the RAM is sorting so you just and afraid exhort your your RAM several times that's how this in general works and the advantage in terms over I love regular CPU over laptop is about one thousand okay I'm the question is does it all have to be in the same memory like because the problem is if you need high bandwidth then you probably have quite a bit of power consumption if you can lower your bandwidth requirements that would be less what if you could split it across 1,000 different memories with all much lower bandwidth requirements and can merge it all together like for example do all the sieving steps for different primes on different hardware and merge the results together at a later step is that possible yeah that's possible I think that's what's actually being done by all this authorization teams because the case you can probably like lower your bandwidth requirements are extremely and go with much lower power requirements for memory and I think you will end up with something very low already like in normal computer's memory isn't actively cooled right by that you know it doesn't really consume that much power and in laptops already much less so I think yeah yes I'm in fact of thousands seems easily possible there and even outside of the the hardware question is like the algorithmic question it is very scary you know that maybe some sort of I receive algorithm comes out and has a slightly better asymptotic complexity and that completely changes everything oh yeah the good point I think as far as I understand in the fraction of integers not much progress has been made in the last 20 years but in discrete loggers and there have been many different approaches because there are these different algorithms for prime fields there are the other types of focus for extensions of profile of prime fields which are important for pairing Bay scripture there are algorithms for fields of characteristic 2 and so on so there are like several groups of these columns and they all follow this number should see if and one another guard so I think that if they have found some something really really important that would be translated probably to the number field safe but on the other hand this the algorithm is itself pretty complex I think it's the fact why it works it requires significant algebraic background so the code itself is not that that difficult but if I understand the authors they spent several years just to make all assumptions realistic so they they basically had to add a few more components to to the to the procedure before it started it started working for every integers and beginning it worked on before very specific types of integers like 2 to the 2 to the M plus 1 or something like this and to make it for all integers they have to go through sufficiently many obstacles so it's a really difficult problem and yeah I think no one can estimate if there is a breakthrough in the upcoming years regarding death when did this happen when was this kinda from 90 to 93 and one of the prominent guys in this research was Leonard add lemon you might remember that Elora say there are even very near and Ted Lehman and some people say that women did the minimal amount of work but still was included into the RSA at least of authors but I think as far as I said this has been very well compensated by by his contribution to do the factory people like very few people who made this NFS really possible so I I would also be really interested in your judgment as a cryptographer so we know that kinda it seems like the progress has slowed there and I guess there's the two different competing theories one is oh yeah we are kind of at the edge and there isn't that much more improvement possible and the other theory is like the bar by NFS a set so high that like it would be crazy for a young cryptographer now to go into factoring because like they would have to spend so much time just getting there and it wouldn't be that likely that they find a new algorithm I wonder what your attachment on that is yeah I think that seven it's not a cryptographer actually the program a problem in fact so I think it's much more mathematical problem because there is almost no cryptography involved in that and there are like some deep mathematics things involved like when you for example and you try to read about this imaginary quadratic groups it's pretty much similar so you would have to have like really deep understanding of algebra before you figure out how all this works and I think just prove that it's really an ambitious task and ii don't have that many mathematicians this days right so as far as i understand the number of working mathematicians is decreasing so many people go into more applied science whereas this factoring he is it's actually more theoretical so my gut feeling is we shouldn't expect real advances from the theoretical side but we definitely should expect some advances in practical side so when I see this as far as linear algorithms and this see being done on a regular CPUs I feel that it can be much much more sped up if so if if some person who is confident in numbers you'll see if talks the person confident in silicon design I think they within a few days they will quickly figure out how to make this you know on Hardware much much faster that's my okay interesting but the other question is basically so you said you don't expect huge progress right now but it sounds like that's not because you think this there is nothing out there to be discovered but there's nobody who's gonna work on it now yeah yeah something like this I think people who are closer to the discrete logarithm research could give a better better answer because there there are some advances there there have been some advances maybe not for in this particular case but in some in some like sister algorithms they were their worst advance maybe they can they can tell a bit better about like their unexplored area and some potential to use them in the face so one question I have is that you seem to focus a lot on the cost of electricity are you is the claim that this totally dominates related to the cost of hardware oh yeah I think so because well the hardware itself so creased if we talk about numbers like what was that three thousand four years so what's what's like if it's a month this means like I know thirty thirty thousand course and yeah so it's I know 30,000 cars what it is three million dollars maybe think even cheaper so of course the it's it's more expensive than this is course this cost of electricity but when we talk about custom hardware I think if we take cheap of some moderate say it sighs I'd know ten by ten centimeters or something and imagine construction I'd know million of these chips factoring I am pretty sure that their cost will be smaller than the cost of run in time if we talk about millions of dollars spent for that so the design will be I know several millions and the chips themselves maybe also several several millions but the amortized cost of the small I'm pretty sure and the electricity consumption can be really huge to talk about I know hundreds of millions of dollars Christy hmm I'm I don't know I'm not totally convinced because I mean for the the power consumption that can really go down a very significant amount let's say between a thousand and a million but then the area which is going to be your cost that it's less clear that it would go down significantly area will not go down but if we can live with the rather small chips then we can put significant amount of them on a die and then we can leave with the reasonable failure probability so if they're not like meter by meter then I think their production can be amortized significantly you but I'm I'm not an expert on that of course I mean I guess I mean I think asymptotically you are certainly right depending like if we assume that that you have a reasonable like assume you're okay with factoring it in one year I would say like if you want to factor it in one week or one month like ASAP and you only have one number to factor I would say very likely the course of hardware will actually dominate because you're basically only running all your hardware for that amount of time but like also because you won't be able to design proper harder within this timeframe right because if you know what number to factor you can optimize your hardware already for that number significantly think wait so that is our our situation so what is the speed up if you're ready if you design your hardware specifically for this number where do you get gains that's a good question I think it is oh well we all have this model reductions right so I think the model where this number is used in me model reductions and if model reduction circuits can be optimized given a particular number then we can win there so all this factorization records of course they knew which number two broke to break but I think they couldn't really exploit it because they just used a regular CPUs about and custom hardware I think if you can hard code the the number into the model reduction circuit you should get some benefit okay if it's only the modular multiplication then I think you get roughly a 2x advantage for hard coded versus programmability both in terms of area and in terms of consumption you know of course some constant factor yeah so wait but I mean the numbers you square out of a very specific form like it's like root and plus epsilon for quadratics if yes oh I see but for the negative there I think it'd be different not sure you square there I mean anyway I think that would not be my sort of main worry I mean another way to look at these security assumptions is just to because you know we talk in terms of dollars but but maybe like the bottleneck is actually just how much electricity the world could produce be cursed you know like how much is the world producing and if it was horn percent of the city dedicated to this problem how much time will it take to factor and is that more than ten years and if it's more than ten years then you know we definitely say well but I mean like please always these are always estimates like you always need right yeah of course yeah yeah no ten is not a enough of a margin of security I mean ten years and a factor of thousand I would say yes because the dollar amounts always very kind of difficult to reason about I guess well you know energy and time is more kind of physical okay so 21 trillion kilowatt-hours trillion 10 to the 12 so 2 times 10 to the 13 kilowatt hours per year this current worldwide electricity consumption okay anyway we can be the do these estimates as well later okay but anyway all of this is pointing me towards the 3,000 bits that's my god yeah I mean Thank You Dmitry this was very very well prepared and very understandable very good at least for me and I think I understand it much much better now yes in here yeah thank you for your time guys 