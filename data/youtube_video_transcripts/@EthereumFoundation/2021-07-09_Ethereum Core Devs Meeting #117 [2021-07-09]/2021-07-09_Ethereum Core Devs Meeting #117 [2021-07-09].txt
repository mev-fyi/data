[Music] thank you [Music] foreign [Music] [Music] [Music] something [Music] [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] [Music] [Music] foreign [Music] foreign [Music] [Music] foreign [Music] [Music] [Music] thank you [Music] [Music] thank you [Music] thank you [Music] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] [Applause] [Music] foreign pools being relatively stable uh so I think there are no big issues there yes the same for me it was fine yeah sure uh so one of the weaknesses that we found um after work is that a few hundred percent uh and the transactions we're still executing but there was about 700 which were so to say executable but miners kept rejecting it but we looked into the whole thing and essentially what was happening is that when you so get has this minor configuration parameter called the gas price you just set the minimum price that you're willing to accept which miners own every single Miner under it can be have set it to one week away and this flag is also interpreted towards London as the minimum tip that minor is willing to accept transactions with so the moment we switched over to London it became an instead of one gigabyte transaction we want the one gigabyte tip plus whatever the base fees now of course uh since post London the the base we immediately jumped up it meant that none of the transactions were actually executable anymore but they were nonetheless still in the transaction pool and they were just lingering there so um essentially I just reduced one of the miners gas price limit by 20 weight and that solved the whole thing and that's actually the problem the reason there were 700 transactions was because uh I think 400 of them was the faucet and probably a few more accounts that happened to block up the Cube I mean the first transaction was non-executable sorry obviously everything else was blocked on the queue so yeah that's kind of what we saw on mainnet I don't think this will be relevant because in general um it's not that transaction so transactions don't really reach the limit that miners set rather there are always a more expensive transaction the pool is always cool and the more expensive transactions always push out the cheaper ones so I don't think this would this can happen on on mainnet and even if it would happen it would sort itself out because more expensive transactions was just the biggest the cheaper ones so even if temporarily it would happen but I don't think what happened I haven't investigated a possible fix for this specifically because I don't think it's really an issue it's more like this weird transition I mean it happens once for the lifetime of the change so it's a not that big of a deal but probably before and then we will investigate the tiny bit closer just to make sure that it's fine got it thanks um sorry this is my bad I had us muted on the stream for the first Gordy part so uh just a quick recap uh when we had the Gordy Fork uh on on or the London fork and Gordy uh the get that basic team ran a spammer and everything was fine uh there was a small issue that we found on winkabi which Peter was just explaining with regards to the minimum price uh that a miner will accept for a transaction um and that was fixed by just changing the minor config so sorry for the people who missed kind of the first part of that um cool uh so yeah with the three testers basically having forked successfully uh we also set a block for mainnet this week uh so we did this I think but just to to document it and make sure it's clear uh the block is 12 million 965 000 on mainnet um yeah and I think most clients have already either merged that or have a PR open for it uh it should happen on August 4th roughly uh and we'll obviously no better the exact date once we get closer to the uh the actual Fork type um and I think the last kind of open issue with regards to London uh was what do we do uh with regards to the effective gas price field and the transaction receipts um like client you'd opened an issue for that on the Ethan specs which I saw you kind of updated yesterday um yeah do you want to maybe just give a quick update of like what what the issue is and and what you've kind of figured out yesterday sure yeah I made a mistake I uh when I was talking to the open ethereum team they were asking about the spec and wondering uh if these this effective gas price value should be returned pre London transactions in the receipt and I just quickly looked at the Geth code and I completely like misread it and it was thinking that Kath also did not return the effective gas pricing so it seemed like there was an issue with um the clients that movement is back but then I looked I saw it again yesterday I realized I had made a mistake but that's still uh leaves the question like do clients prefer to follow the spec because open ethereum seemed to signal that they would rather not uh provide effective gas price before London for transaction or for transaction before London I am like minorly against that but that's what the the issue is about so if anyone has thoughts or preferences that's yeah curious to hear foreign [Music] also states that after London the gas price field of the transaction should be dropped at least for 1559 transactions and uh currently what Gap does is that after transaction is buying the gas price field and there's it's not so not sorry not in the receipt but the gas price field in the transaction well it would be pure transaction for the RPC it is sent to the actual gas price state so if you become any fast transaction the gas price will be the the amount that these are paid for it and so essentially it exactly the same thing as the effective gas price back but part of the transaction and I don't think it's a good idea to drop this field just now because that is what everybody is relying on so we could say that the field is deprecated and maybe drop it in the future but I think it's it's a bit bit of a bold move to to just milk it out immediately on the four kids and switch it over to different location should we so should we have both the gas price be the effective gas price that's in the transaction object and then effective gas price as a element of the receipt so that is what get does what do other teams do right now I'm not sure uh are originally implemented it to transaction object not to contain gas price for the 5059 transaction but I'm aware that this discussion was mentioned a few weeks ago and not really sure about the current implementation I need to check yeah I believe we have effectively asked price in a receipt and no gas price in the transaction object I can check though the queen doing the same like bezel and open ethereum above the on the other hand I think it is good idea to return effective gas price so maybe we will do that so is there an argument against basically doing both um and if we go ahead with both is that it seems like it's a pretty simple thing to add basically keep the gas price in the transactions um and uh and keep the effective gas price in the in the in the receipt like is there any reason against doing both those things so the catch the only catch from keeping transactions outside of guest passing the transaction is that before time transaction is executed the gas price is you cannot calculate it because it's obviously depends on the base speed So currently gas sends it to the fee cap meaning that that's the maximum that you would pay and once the transaction is mine if it is update this to the amount you actually paid and we kind of found that this is something that the user experience life is what kind of makes sense but it's I think it's important to realize that there's this work yeah it's not just selling the value yeah so we tend to always want to it seems like we always want to overload these fields why don't we want to just not have the gas price for the 15 transaction and if you're curious about the effect of gas price just ask for the receipt foreign transaction needs to be changed and that's uh this is the environment in my opinion so that's that's why that's why we are suggesting to keep it around at least as a deprecation thing and remove it I don't know in a few months so I guess um the uh the other kind of uh concern here is this is like a non-consensus change and if we set a a fork block for mainnet uh we probably want to have releases from clients you know out ideally next week uh with kind of you know that are London compatible um so that people can start upgrading across the ecosystem because there's only going to be roughly like three weeks um I'm I I guess how important that people feel that this is like harmonized before their Like official London releases are out um yeah because I yeah do we want to just agree to kind of do both uh have the effective gas price uh in pre-lendon transactions and um also uh also have the the gas price field um the gas price field return the effective gas price um yeah or maybe I guess a better question is like how much delay does it add to client teams to do both right uh is it like a day that that's needed or does it push things back by a week and if it pushes things back by a week I think I would have a weak preference for us you know not necessarily harmonizing this before the fork and and maybe clients will just need to like document a bit better how they handle those two things Focus I think you just like that oh sorry we can't hear you very well um erect you kind of sound like a robot sorry sorry so for us it's it is like a day to fix it so okay I think it would be good to converge and do those things and it wouldn't be difficult to do but I also kind of like Peter's suggestion of potentially signaling that this will go away um yeah just because of the overloading semantics of the gas price and the transaction object I assume that's something we can just add as a comment in the Json RPC spec uh I'll update this spec right now I think it's really good to try and converge if at all possible before the fork I think that's something that would be really good just to keep in mind as we go forward the easier it is for people to just swap clients in and out I think is better for the ecosystem Yeah the more specific behavior rpcs have I think you know into a dangerous place so oh go ahead one other question is and this is something that I don't have enough experience with is how how often does it happen as somebody or how often do we expect it to happen that somebody is curious about the gas price paid without caring about other fields in the receipt because so if you if we put this so essentially the effective gas price can be calculated without the receipt so yeah every other thing that's going to be useless because I only need the feed cap the priority fee and the base fee that I can just calculate it now if we put it in the receipt then you just need to dig up the receipt and send it to the user even if they are only interested in in how on the final gas price was the question is is this a problem or not I feel like we had discussed on Discord like why it ended up in the receipt but I I forget why um yeah I mean theoretically it's a better place I don't think I quite followed what your argument was like are you saying um it people might only want the effective gas price so why download the whole receipt or are you saying the inverse or if something different yeah so essentially what I'm saying is that if I finally care about the gas price then I still need to pick up the receipt from the database just to calculate the gas price which doesn't need any of the receipt fields but the question is does it happen often that somebody would be interested in the cash price but not the status of the execution or what does it ever happen and this is something I cannot respond that's a good question I also don't know I I assume it's like a common use case where I'd like you imagine you go on etherscan it's like something you want to see in your transaction but then they also show you everything else right or like I don't know meta masks showing you like the amount you paid um you actually don't see it so like if I don't know I'm looking right now at metamask you actually uh you see the when you click on one of your transactions you see the gas price that you've paid yeah um I mean any such wallet or something will also show if the transaction fails or not and there you cannot meet the receipt so okay yeah if if you did the receipt to show whether a transaction has failed it feels like there's not a lot of use cases where you want to show the price paid but not the success status right yeah that's what I was also thinking about now yeah yeah um and like you said the right you can actually recalculate the effective gas price from uh the information in the transaction object itself so I think if a wallet wanted to do that optimization they could also like calculate it right yeah of course so it was just a question so I can also see that uh probably it is fine because um you care the effect you if you care about the affecting gas price you also care about the transaction sexy buildup fails so I guess yeah given that we can probably go with that obviously if somebody feels very strongly in the future otherwise they can like make a leap to change it and if we've announced if we announce that this field is being deprecated um I don't know before like the next hard fork or something somebody can also kind of step up and say like don't deprecate this it breaks my app for these very specific reasons um yeah does that make sense yep cool so I think yeah just to to summarize then we'll go with having the gas price field in 1559 style transactions which will return basically the effective gas price we also agreed to have the effective gas price uh in pre-eip 1559 blocks um and uh like client said he'll add a deprecation notice to the gas price field in the 1559 transaction format in the Json RPC spec so that people are aware um that this is happening and and also that the gas price Guild should return fee cap if the transaction's online oh yes yes that's right so it's basically yeah to be in mind yeah so just to clarify that even after this really sophisticated and removed in the future that would mean it's deprecated and removed for 1559 transactions but for legacy transactions we are still keeping it a lot yeah forever yeah yeah yeah that's right okay um and do I guess kind of the the next point on the agenda was like the client releases for London um do teams feel like they can get a release out like early next week given those changes is that still realistic we already released with the main Network but I assume you'll want me to take another right like basically the yeah the reason I'm asking is if there's a ethereum.org blog post that says download this release for base you you probably want the one with with that transaction fix right yeah of course okay cool yeah yeah um so yeah is that realistic to get like early next week yeah yeah I'll do it at a band release okay yeah so if if any client team feels they cannot have a release early next week now's kind of the time to speak up is it Monday early next week or Tuesday Monday or Tuesday ideally um and the reason I'm asking is uh uh yeah if it's Monday or Tuesday then we can have the blog post go out like uh Wednesday Europe um and that's like three I think that's yeah that's basically three weeks before the upgrade so it feels like it's you know enough time but if we have to wait if the release is only come out like Thursday or Friday then the blog post probably won't go up until like the week after that then that's only two weeks before the upgrade so yeah Monday or Tuesday would be ideal okay I think it's realistic for open ethereum although we don't have still fee history implemented uh completely but I think it will be yeah but the free history does not consensus so yeah I guess that's okay oh yeah but it's the same question as the previous one but ideally you want more clients to behave the same otherwise you can actually just swap it out but they do weird yeah it's not because that was critical but it it would be super nice to have them work the same way cool um okay so it seems like yeah there's no major objections um so yeah if if teams can have a release out uh early next week with those changes then I think we'll be all set for for London um one I guess related thing is I just want to make sure this is set on the call so with London happening uh in 1559 in it the miners will have to Target a block size that's twice as big as what they're currently targeting free London so if say they're targeting 15 million they're gonna have to bump that Target to 30 million um the three mine like mainnet compatible mainnet clients all have a Json RPC call to do that I've linked them all in the agenda for the call but get has added minor set guests limit open ethereum has parodies underscore set gas CL Target and base you has minor change Target gas limit um so just if you're running a minor it's really important that you basically well you basically need to call that and double the the the gas Target that you're aiming for otherwise after the fourth you're gonna start reducing the blocks um and just including less trends actions into them so yeah the links are in the agenda I think Getz doesn't have a doc link yet but it's it's merging the code and uh the last thing so uh I think yeah this came up Trent you maybe want to talk about it but as you were reaching out to like the ecosystem about London Readiness there's a bunch of questions that came up about uh some non-consensus behavior for clients and that it would be good to kind of clarify how different clients handle stuff um yeah do you want to kind of give some quick background on that then ask the questions that you've been asked yeah uh just a short summary of it um I think the two main things were how the different clients handle uh transaction pool sorting as well as um the other one Escapes Me but that was that was a big one oh yeah that's right the percent that's required to replace a transaction or the percent bump um so if the clients can just like mention what or go over what they've done for each of those that we could just have a record of it so um we can see what the differences are so from God's perspective we kind of um as far as our attempt was to keep the existing Behavior namely that um transactions in the pool are sorted by the gas stick so the amount of money that goes to the miner because kind of that's the important thing so I start including the transactions The Miner isn't really interested in how much is burnt rather how much do they get and uh as far as I know the replacement logic is similar to previously you need to pump by 10 percent the tip itself I mean the stuff that the miner gets that needs to be pumped by 10 to your replacement I think we do the same we are doing the same too just double check is it really that it's only the tip because I think we typed it went back and forth a couple times in the past but I think at least a glass version of the code that I saw required a bump of both the tip and the P caps I just wanted to confirm with whoever and get has seen the latest implementation there so just on our end um I think we check against the current base fee so it's essentially a 10 bump in the expected effect of gas price yeah I see that makes sense but but yeah basically the point is just that if um the fee cap is below um the current base fee then um but like a a bump in in the tip is basically free because because you don't actually end up paying that and so just just like a like a naive bump of only the tip I think has some my next break undesirable behaviors replacement but I I would ever recommend wallets and everything to usually just uh have that default Behavior like the 10 bumping both vcap and tip where it makes sense but like at least there you on the safe side but you you will kind of achieved replacement open ethereum does sorting and replacement based on the effective gas price and if you want to replace the transaction we don't care if you uh bump the max speed progress or Max priority progress you just need the calculated effective gas price based on that increased values to be pumped by 12 so again based on the effective gas price okay but a question there I mean transactions can only be replaced if they're pending a member transactions feel above the account includability like about the context people always just be included immediately I mean unless we have extreme congestions in all blocks are 2x but like usually so basically we're only talking about transactions that are not includable right now anyway so all of them will have an effective tip of zero so I'm just curious how do you bump an effective tip of zero I don't mentioned effective statement is that effective gas price on the stuff that gets paid at the end sorry what was you said your use when you say tip you're also you also mean like the effective tip right like what's paid you kind of broke up there Peter never mind I was just trying to clarify but it's it's just getting more complicated anymore lasting the details so it yeah I guess it seems like at a high level every client team sorts the transaction pool by basically the effective tip or what goes to the miner um and requires a bump of the effective tip by you know at at least 10 percent um and 12 for open ethereum is that right so I kind of got uh I mustn't I'm sure who said it but I kind of kind of got a bit uh unsure about it because maybe that also does it does the effective got it nice yes I'm here uh do you remember how to be in front of the election does it take into account only the gas page The Miner or or the global effective gasket when we figure when you have to figure out whether something is under Christ or not oh I don't quite understand the question uh so uh so the eviction rule yeah uh well yeah we have this this minimum for uh the effective tip like this this Global minimum and uh well we have the Dual priority queues but what uh are you asking about right now some transaction that already exists I mean the account mounts already exist than previously we had a 10 down Community pump up the fee to yeah oh the one pro yeah so right now the the price the replacement rule is like you have to increase both the uh the so so they did you have to increase the the parameters specified in the transaction by ten percent so like the max fee per guess and Max priority feed per gas parameters they both have to increase at least 10 percent then you can replace transaction so the current effective tip doesn't matter here I think yeah so and also just to say that I I think at least that that piece of the transaction pool might just be kind of the like the unchanged version that I implemented and and like if that is the case then indeed it the death requires the 10 bump in both fields yes okay uh Trent does that help yeah yeah I think that was uh that was good to go over great and it's also worth noting uh Aragon is working on a different transaction pool design I think Alexa has shared uh the write-up about it before um I'm not sure how final it is uh I don't know artem do you have any comments on that I'm not sure if you've been involved in that at all oh yeah I haven't been like very involved in it like we are we are we are still writing it I think we just finished the design of it um I I think we will have it ready by uh the London maintenance rollout but yeah I guess it's better to ask offline okay I just shared the link yeah to the the Wiki page in in the chat here if anyone wants to have a look oh and and scarcity you have your hand up yeah I just wanted to maybe I guess one last comment on like the transaction pool topic as a whole I just wanted to briefly find out that I think like right now the individual clients do have slightly different like sorting eviction logic so basically I think uh I don't think that's a problem but just like because right before 1559 because it was one-dimensional like basically everyone had the same Rule and so like the behavior was identical I mean up to of course the size of the transaction pool um and now this will will no longer be the case uh um I think that might won't be an issue because that just means that like slightly different transactions will be retained in but they're slightly different clients that actually kind of increases the global size of the mempool even um but it's something to just like keep in mind and put at least potentially watch for and see if that kind of creates any issues going forward I don't expect to but yeah that's a good point like we'll be able to see out my net I guess you know if there's some client that just has much less of transactions or yeah is like less efficient at relaying them um cooler that's all we had for London unless anybody else had anything they wanted to bring up I actually just want to mentioned that I did check the code just now and uh whether this is correct or not is up or debate or for debate but get currently requires either the Gatsby or the tip to go up 10 um are you sure they should they should they should it should require both to be bumped I'm pretty sure about that that's the correct Behavior yes that's what I'm saying it doesn't do the correct so if you have incorrect code master oh yes I just need a link and we can continue just for the reference since I mentioned it and let's just switch I think I think actually it looks like I'm looking at it right now on the master but it looks like only the comment is incorrect not the code itself but yeah we sure should that should be offline maybe because it rejects it if uh yeah if either is not met it rejects it so it looks like it's only the quotes to come into the gas pair programming later yeah okay uh so I guess yeah get team I should probably just triple check that is correct um yeah uh and yeah next awkward there so you can have the get PR programming session um cool anything else on London okay uh so I guess yeah next up um Mikhail put together uh merge Readiness checklist so basically trying to go over all the stuff that needs to happen uh on both the consensus and execution side uh so that we have like a high level kind of list of things to do um I think it probably makes sense for you Michael to like maybe share it and go over the the high level kind of details of it um it's something I think is going to be really helpful in in then figuring out do we think we can you know when do we think we can do the merge what do we think are the biggest problems or like open issues in that list that are going to require more attention um and also is is anything missing from this list that's that's that's important um yeah thanks then um before we go briefly through this list I just want to mention that thanks a lot Danny for compiling it um actually so yeah it's not final obviously because the reason I'm going r d and um and we'll be ongoing or already going implementation work so we'll keep this list updated um and as you mentioned it might not be full so um just want to encourage everyone here to to take your time to go through this list and think if you find anything that is missed in this list and you think that is important um we're just out and let's discuss so yeah let's go briefly through the list and get back to discussion I also just want to mention it's the granularity of like a publicly shared list um obviously there's a lot of detail we could drill into uh but that's just kind of not not the purpose here oh yeah thanks Danny um okay so uh yeah I can share the screen actually do this yeah yeah if you want to share your screen I'm not sure that people on YouTube will be able to see but the list has been posted in in the chat and I'll make sure to post it in the in the comments as well right right okay so do you see my screen oh yeah we actually we do see it and on YouTube as well okay it's cool uh okay so uh the first section here is the consensus layer uh so your consensus layer specs are feature complete and the next step is to replace the spec with wanton handle tear also there will be some um insignificant I guess insignificant fixes uh on the other parts of the spec which is networking and API stuff um so the next part is the execution layer what do we have on the execution layer specs right now is the high level design dog that has been um like published a while ago and the Ryan is a spec published after it but it's been one time ago like a couple of months I guess so the next step and what is the priority now is to actually write a specification for the execution layer which will consist I guess from uh from a couple of eaps on the um consensus side on the core protocol side and also uh we need to the specifications for the state in boxing um also there is the change in the Json RBC this should not be significant a couple of endpoints will be deprecated that are related to mining process and probably one or two will be added like the get monsters and finalize block um and the consensus API is also major thing to figure out uh we we have the Json RBC um specification and implementations of it um so but it doesn't seem like it's perfect from the for the production this is what we what has been discussed recently and I think we should reiterate on it and come to conclusion how do we want to implement consensus API what will be the underlying communication protocol and supports yeah there there is a lot of testing items here I want to go into the read of testing yet because we have some other more important things to do like the consensus API and the execution layer Stacks yeah there is this Nets and also um there is in r d section so not all r b questions are answered um as for today uh we have some kind of analysis to do with the transition process um and yeah we need to do the threat analysis uh the attacks and uh uh attacks near the point of merge and the um attack that and analyze new attack vectors that can appear after the merge like this resource exertion attacks um there is the execution layer proof of custody here it's listed here but it's not likely to be a part of the minimal merge uh but uh yeah it's it's here because of its importance and the willingness to introduce it later on because it may affect some decisions made on the spawning these services like the execution layer service and um yeah that's that's right here um so we need to figure out Discovery I'm sure if it's anything I'm major to do on that side so and yeah the state sync um historic State sync and seeing during transition period is also a tricky tricky one so that's all actually for the list thanks yeah thanks for thanks for sharing um it feels like at a very high level the next step is probably for the different client teams to start looking at this and I guess you know going over each of those check boxes and and trying to figure out what actually needs to be done um yeah I yeah and obviously you know we still have like some stuff to finish with London but I think as as that dies down we'll kind of naturally transition there and uh yeah Peter you have your hand up yep so I don't know whether we should be commenting on the stuff now or not sure yeah but he has uh there was the phone to that point which is very new uh in the eagle execution it was uh reusing the difficulty field for the rundown values and uh I would like to request either rethicking that or or I don't know maybe exploring it a bit more because the problem is so the idea behind that point is that since post merge to difficulty will not be relevant it can be repurposed to feed the 32 Byron now and uh and the reason why this specific field was chosen because the evm already has a difficulty of quote so you could just piggyback on that op code and access the randomness Oracle and in my opinion this is not the best idea because I mean it doesn't cost us anything to add a new art code called random so that I don't think we need to piggyback on existing of codes however the other the bigger problem that I cannot see is is that currently the difficulty is a small number I mean the field can be in in ethereum world most of the fields can be 256 bits and the difficulty is insignificantly small compared to that and essentially you can just keep adding it up to get to the total difficulty and every client does it now now if we repurpose the difficulty fields to be a random number then it means that all of a sudden we will use over 32 bits and in the second talk uh we will overflow with the total difficulty the 256 bits now in theory this is possibly you can fix clients so that if this won't be a problem but in practice uh essentially the notion of total difficulty is really deeply ingrained all over the place and it just feels like it could be playing with fire to to change the base invariant that all of a sudden after the fork this this field can become extremely large so that's why my suggestion would be to maybe pick the mix hash field and add a new random so there is another reason that that was selected is that um although it's been uh bemoaned as a terrible practice difficulties used um almost only in the ebm today for Randomness and so that was another consideration on that decision I'm happy to rethink this decision I'm not I'm not married to that um I would worry we need to select something for that value to return um putting one in there or something might actually be dangerous because it could hide Fork Choice bugs because then you end up with like some weird longest rule chain rule that maybe Works some of the time um and so we would need to be careful like if we're worried that total difficulty is going to haunt us we need to make sure that it doesn't accidentally give you the correct value some of the time and and Trigger potential like uh accidental for Choice bugs um so maybe zero is a more appropriate value um but I again I'm not married to replacing it but there is another consideration which is that if you do start putting in zero or one it's going to break some applications that are using the bad Randomness today yeah just a quick reaction to that so I'm I'm fine with going into this direction my my kind of my request is that we should investigate and make sure we we cover all the side effects yeah yeah agreed and maybe we should do a deeper investigation of how clients are and we'll be using in the future before we make like a firm decision here thanks uh I see Mikhail and aksik you both have your hands up I'm not sure who was first exit close first yeah I just wanted to mention for what it's worth in solidity the difficulty field is considered a 256-bit number um so it won't be truncated um whether that's good or bad um that's a question but at least whoever is using it in within solidity or not like in an inline assembly they would get the the complete value for the difficulty got it thanks I I think that it's that the fact that it's not trinky this is good it's as expected um I am I do understand what uh Peter Peter's consideration and I have a question here we can use the mix hash instead of difficulty field but then we will have to change the evm context for the blocks that are produced by the proof of stake chain and switch and jump between these two evm contexts between the execution of proof of work and proof of stake blocks and if this path is less buggy and less controversial then reusing the difficulty field so we can I think we can really go this way yeah so I guess it has implications in both ways yes either way uh I don't think there's anything more to discuss except that there's this quirky thing with this uh this idea and we'll obviously find a good enough solution just people need to be aware of it that it's not not a thing that you would just saw in a half an hour cooking session right yeah and yeah these difficulty thing this um not difficulty but render or random thing is gonna be put into an EAP so it will definitely be discussed with a lot of care cool were there any other comments on the list in general okay yeah and we have a merge call next week so we can probably dive into more detail and you know some of the those items on on that call as well um I can have something a bit yeah unrelated but touching so um for the past uh I know a few months we've been working kind of in the background with super long big priority on uh on the synchronization stuff too for post merge and one of the things we kind of realized is that it is super annoying to to implement the thing for um e65 and below so 66 e66 introduces requested by IDs which makes things cleaner and it's just a pain in the ass to to implemented both for the new request required models and the old non-questified models so as promised about last year October again we'll be dropping uh 65 after London ships so that we can focus on on this merge work it's like a heads up thing yeah thanks for sharing that's good to know does that get does that drop and get released in the London fork or do you mean you're gonna do it sometime soon after the London so uh the whole point was to wait until London is stable so something we know we won't do any Hospital or stuff like that and we will only drop it down and this way we can actually maximize the lifespan of 65 because most people will still just run to London for release and not upgrade got it cool any other comments on the list I I have a kind of a question um what would be the best next step for us to do um in what is in mind is the right the EAP profits or the spec drafts and share them and discuss or should we um do anything else before this that's my intuition because also to do the next wave of prototyping I think we need to more clearly document a lot of the things um even if they're not final and so at least what I think we need to focus on is specifications and the the basis of testing those so we can continue to talk about them and build more prototypes and maybe also like based on access comments in the chat here there's you know for each of these items on the list there's been like a bunch of discussions um maybe I don't know adding like some links I I'm not sure if it's like that document is the right place but like um yeah I how can we get people caught up on like the latest thinking for these uh these different things right um right and maybe that's just actually writing the spec that's based on the latest thinking and kind of explain what a good rationale um yeah but I think that's just one thing to kind of keep in mind like how do we bring people on the journey like of yeah understanding how we got to the decision I did add some PRS and links were relevant but I think we could probably flesh that out and keep adding them as we add things um but yeah the conversation has been on a lot of those like merge calls and Scattered throughout Discord so writing everything down more and better and more often will be a priority cool anything else on the list so the last thing um so my client had a comment about having a higher level discussion around Fork scheduling and others have reached out to me also uh about that in the past couple weeks um even though we got the Vlog for London three days ago um but basically we a few months ago we kind of agreed to after London focus on the merge and potentially have Shanghai you know before the merge if we see that the merge is unlikely to happen by December or you know have Shanghai after the merge if if we see that the merge is likely to happen by December um yeah so that's obvious it's obviously hard to know I think today what the odds are of the merge Happening by December and if we want to maximize those odds we kind of have to start working on the merge and making progress on all these things which is discussed um but I understand doing that obviously puts us in a spot where like we might get to December and not necessarily have time for you know to to implement and test the whole fork with a bunch of different features in it um so that's you know where I'm at I guess with regards to that right now is I feel like we probably need to see a little bit more at least how the work around the merge uh continues and progresses um before we can determine if we want to have like a proper feature fork and consider stuff like uh 35 40 3074 and and whatnot and have like those discussions um yeah so that's kind of how I I feel about things I don't know if others have thoughts or yeah questions they want it answered Well my two senses that we cannot focus on two completely different things so if we start doing the work for merge and then in the meantime well maybe we should maybe we should do Shanghai so the mission High then it's just going to be a bit of a weird situation or nothing at home so my personal two cents would be to get the merge done and focus on features I promote and that does not mean that we would get immersed on by December yeah and I guess the the worst case scenario if that happens is we basically have mere Glacier part two in December right we can always just have a single Fork that just pushes back with difficulty bomb you know by however much longer we think we need I think I was also curious like even a higher level than like specifically things like Shanghai and emerge but how as like core developers do people feel about scheduling Forks like I you know we're I'm pretty lucky that I'm not the person who has to maintain the fork or maintain a client and prepare for a fork but it feels like you know if I was a quarter developer who did have to do that I basically have to just assume oh you know three or four months ago we thought maybe we would ship 1559 in beginning of July and now it's the beginning of August and so that's a lot of uncertainty for core developers and I'm curious if there is any desire to have you know more fixed for schedule um we I think there were a few attempts to do fixed schedules and it always blew up really nicely and we always kind of just Fallen back to the it's ready when it's done or it's released and it's ready state is that something that you feel is going to be sustainable long term or is that something that you think is just how it is right now but it's not really ideal I mean uh so the fact that you say if to merge will be done by December doesn't mean that it will be done so the question is you reach the summer and it's not done what do you do yeah I think the emerge is kind of a special thing because continuing on a proof of work chain we're spending lots of money for security and so it's kind of you know let's do the merge as fast as we can but beyond that once we go back to regularly scheduled feature Forks like is you know just doing things when they're ready like a sustainable pattern for people maybe one different way to frame that question is um you know what's like the target the amount of forks roughly we want to have per year um and I I kind of feel like the difficulty bomb helped in a bit for London in that way that like it it helped like reduce the scope and you know kind of keep us like keep us focused on on on on on shipping like the the smallest set of things that that would that would matter um and you know Peter you have like this this comment that on cordell's earlier this week that if there's Three Forks a year on average you're out um and I think historically we've been you know but kind of like between zero and two um but I think knowing that like knowing what's the amount we aim for can help be a sort of loose constraint there where if if we're saying we want to do one or two forks per year for example then that kind of gives you your Cadence right it's it's like six months um and and and and and then you even though you know maybe you're not ready on exactly the six months deadline you know that like three-ish months in you can you can't accept any more eips and um and and things just need to be kicked out in the next one um so yeah maybe it's like a aiming for a loose number of upgrades per year um yeah you know Peter you have your hands up yeah so um the problem with hard Forks is that implementing a hard Fork is an insignificant amount of time compared to maintaining a client at least in the guesting so we're always very deep in some weird refactors or optimizations or experiments or whatnot and the hard Forks are just so every time we figure that okay now London is coming up then everything needs to be put on hold or or even if not on board kind of like okay Let's do an implement this how can I test and okay it fell apart to change if we treat it a bit so it just puts everything else on hold and uh and you kind of need the time in the in between the hard Force where people can actually focus on getting their clients better and not just upgrading them as Industries for example if I mean turbo gas or or everything they they figured out that they're going to do a different data model which looked very promising and they they need us five years to do it now if if Paragon where they were a Mainland clients that actually were alive or Maynard then it would mean that during those five years they would have had to implement a constantly tweak their own new model to this old hard forks and they would they would be nowhere with their new model so it's if you if you just go hard for a card for heartful I mean it's perfectly fine as long as clients only work on the hard works and nothing else but if you want clients to actually optimize and work on other stuff you can't push people too much and they hear the the problem is that you it's super hard to say I mean you also can't really tell a client okay now you have two months to work on your craft and then we're back to working on artwork because certain projects take more than two months foreign I'm just curious if other core developers have thoughts on like how to deal with the next you know 18 to 36 months on ethereum because there are a lot of like really significant changes that are you know theoretically in the works things like address space extension changing the state try like how can we balance this in the way that clients can continue to do their normal maintenance and improvements but also support these like larger scale changes that's the timing chain comment changing the address space and changing the try both will not happen in the next two years I can guarantee that you say the next two or the next three years yeah I have two I was a bit more optimistic okay yeah two I can see three I would be really surprised uh Thomas yeah I think the obviously London London Fork was very demanding in the amount of work around the transactions so maybe some better Road mapping and clear barriers like this this fending of the EIP some different ideas and people trying to push them just to happen in this coming Fork it was very tiring and distracting like so we could have the conversations and there was lots of conversations on the sides when we were discussing bigger changes the research bigger eips but the moment when they were changing into also maybe let's have them in London that was very tiring and distracting because suddenly it was a bit harder to plan the fork that was just like a one or two months away and I was happy to pick the bigger finds for the next forks and start having this conversation about next forks with people and then it will be less pushy for any big changes that were proposed um and then planning long-term planning for a year one and a half for the client development is also more reasonable but we've seen that over and over again that's we have some changes being dropped like one two three weeks before uh before the forks hitting the test Nets and people thinking that this is okay because it's so normal yeah that's pretty useful um yeah ends guard you have your hand up yeah I just wanted to briefly say that also like it's one of the kind of EFT authors of 3074 which was I think like a good case study here just uh in in the sense that I think if there was kind of clarity that say for example we have two feature Forks uh every year right or like two folks in general and and so basically and they're six months spaced out and I feel like there would be no earth like kind of like it would be very easy to say yeah that yep is better yet there are security concerns that just kind of like take a step back and address them whatever right and then we know okay the next book is realistic to Target this one or we target the next one or whatever I feel like a lot of kind of these problems with with erps really being pushed hard and whatever also comes from this uncertainty where people always have the fear that if they don't make a specific thought it's completely unclear how long they have to wait right and so so kind of like this predictability around folks would also greatly help I think reduce the the amount of kind of pressure people feel to try and get their Erp into this specific next Port because otherwise it might be lost forever or something but I mean of course I'm being a bit kind of hyperbolic but I mean I think the point generally stands and also I feel like relatedly uh things like the the approach with the Euro networks of are physically trying to have more liberal tests today like the test Nets where where you can just experiment with it with additional changes and it just like so basically we have this latent test net where once A4 we basically we have to cut off let's say three months before pork and then we basically just look at the test Nets and take all the uncontroversial earpiece that have been fully agreed upon and that also have pitfully implemented tested already and just pick them and like then we don't have to discuss much and so on so I feel like this kind of kind of General approach makes a lot of sense to me uh Marius yeah I think um that's like the biggest problem I see is testing it's not really implementing the changes um but basically uh the testing team can can start testing or like start real real testing of the implementations after everyone has implemented their changes and of course because like different client teams have different uh uh people working on it and and like like different amounts of funding and and stuff um some teams are faster implementing a change some teams are slower um and so it's it's uh like basically um the testing teams can only start working after everyone else is done and then I think we need at least three months of uh of uh fuzzing like State tests test networks um to to really be like at least reasonable sure that this doesn't break or may not um so I would be totally against like having having a uh uh like like a more stringent schedule I think it might make sense to have two forks per like to to see that we can schedule two forks per year uh but more than that it's it's just it's uh I think it's really heavy on the client teams and um I've I've I myself been pretty burnt out the last couple of weeks um due to uh London and so um I think some some of the other guys feel the same way um that's just my two cents it feels like we need a much longer term Vision on things like I I think like 3074 is a good example of something that shouldn't even be have been allowed to be discussed it should have kind of been shut down in March like I understand why we started discussing it because it seemed like a potentially good change that may have gotten rolled behind the merge and it may still get rolled behind the merger and never ship but because of how soon London was I think we should you know maybe do a better job of saying you know schedule getting the fork scheduled far in advance having an idea of what's going into foreign events not even allowing discussions of changing the schedule um you know six months away from a fork maybe this is just like a naive opinion but I so I I strongly agree with the more long-term planning and it's something I've been trying to think about how we can do and and I think you know there's a lot of like Legacy in the format of like how we run all core devs that that tends to bias towards more shorter term planning um the one challenge with like I feel like we could probably plan one-ish year out not too too poorly but after that it gets very hard because of like kind of this speculative nature of some stuff uh so like for example and Scar mentioned address space extension versus like the vertical try uh migration right and we could say we're gonna do the vertical try stuff and then address space extension and six months from now you know there's been no progress on vertical tries and Azure space extension is kind of coming along um and I know that when like when I was planning EIP 1559 for example I really wanted I thought in March 2020 that we would get it done by the end of 2020 um and that early or mid-2021 which is now we would like have stateless live and I really wanted to have that done before stateless and like obviously the path around State expiry has changed a ton EIP 1559 to like three times as long um so it's yeah it's just like I guess the uncertainty when you do longer term planning gets much higher and if you have a bunch of different initiatives um it's hard to stack them and the other challenge is then it gets much easier to plan kind of the smaller stuff than the bigger stuff so for example 3074 is much easier to plan for than like address space extension even if say you know address takes extension was the thing we wanted to prioritize over 3074. um yeah so I yeah I'm not saying it's impossible but I've said yeah there are like a couple challenges to doing it but I agree we probably need to to move in that direction foreign and this will get even more complicated after the merge because not only do we have to coordinate amongst the execution client teams but we're going to have the whole consensus layer on top as well that's actually an interesting question for um post merge world how much work is needed by the various teams for upgrades we have to upgrade both clients each time or is there possible to upgrade one without the other I think Mikhail would have the best answer there um from the like from the change that are related to the network I don't think it will need to be up it will require an upgrade of both clients each time so for example if we can change client a beacon chain has a hard Fork as an upgrade it will not require like execution clients to get updated too but if we see something like coupling the um can sense to send execution clients in terms of setup in terms of configuration like you're setting up only let me say like um Gaff and it downloads the consensus client that is specified by the user or by default then it will probably in this like if they are kind of bundled then we'll probably acquire the um institution clients to get updated too but that's just my thoughts and this is not like the I I'm not sure it will be the case how naked Federation will work what about the other way around when we want to do a feature change let's say an OP code to the evm will that require a Beacon chain upgrade as well or you can client upgrade consensus client upgrade um if this is just pure evm upgrade uh which yeah yeah yeah so it doesn't require any information uh from the consensus layer like the we can walk roots or so so there are there does exist it sounds like a set of features that would require only execution client updates and a set of features would only require a consensus client updates and then there will probably also be a set of features that require both to update and Tandem and it's only for that third set that we really have to worry about dealing with the complexity of simultaneous updates and coordination and that will probably hit us right after the merge because people will want to withdraw their ether from the beacon chain back into the execution layer um yeah certainly test drive that pretty pretty quickly foreign yeah I don't know if anyone has any more comments on this I feel like I definitely need to digest this conversation and think about it more um but yeah if anyone has has more thoughts Now's the Time to share them I'm sure I have more thoughts but I always miss most of the call so I'm not going to say anything cool uh Thomas yeah I would like to keep insisting on naming the next Fork always like so when we have conversation about London the more time we mention the name Shanghai the more likely it will be that people will start feeling like this is a real thing that happens next and then they can plan a bit more so it's just simply naming it and having it in conversations would possibly already all of us in the previous weeks to move some of the some of the ap's proposals to be in the next forum however strange it feels I think it's simply giving it the name and the rough date is enough fair enough oh sorry go ahead oh uh I guess Alex and Scar you can comment I have a separate question so I found you have a comment for this please go ahead I know yeah I just wanted to Bear the convention for because we were talking about high specifically is this as an example I think it's in some sense also shows though that like what the problem with this is right because I distinctly remember when we talked about what to include in London uh like three months ago or something uh basically uh I I think I was in the position there I I said that it's unclear if we'll have something else in between London and the merge and people were saying no yeah we will have Shanghai it will be in November or December and maybe that will be the merge then there was a little the mergers Shanghai meme or if we don't hit the merge kind of ready by then we'll do a feature folk instead um and now it looks like even if the merch happens say I don't know q1 next year we'll like basically have the merge first and then Shanghai afterwards so kind of like there is a danger in kind of talking about later out forks too soon because this is like a prime example where this information kind of like ended up not being reliable right so it's kind of like being too concrete about things when we end up not doing them there will be a Shanghai in November because the difficulty bomb goes off in December so we have to do something like there will be a patch that will go out in November and we can call it yeah it's a big difference whether that's like Muir Glacier part two or whether that includes say eip3540 and 3074 right oh I would so so I agree with Thomas we should call that Shanghai no matter what's in it like we should stick to Shanghai is a thing that happens after London the next thing after London is the thing that happens in November we don't know what that's gonna include yet but that will be Shanghai but I think from the perspective of EIP Champions you know if you tell them you might be in Shanghai but realistically nobody actually thinks we're gonna do Feature work aside from the merge um then yeah it's it's just quite like it it doesn't really solve their problem right they know something will happen but it doesn't give them a feeling about whether it's likely that that they can be included in that um yeah so the moment we have two options either tell people it may go to Shanghai or tell them it won't go to Shanghai would really like to be able to tell people oh yeah it seems like something that should go and Shanghai or I think this is very reasonable for Cancun and gave them the rough dates of November and May I say got it um Alex you've had your hand up for a while yeah this might be a dumb question but um regarding the merge is um any of those sides so like the the execution side does it know about what Epoch are we in versus does the the consensus side no what block number are we in and I'm asking this whether there's like any um consensus level coordination you know what um like hard work um should be active or whether all the the node operators just have to know what versions of both of the clients they have to run consensus layer is aware of the block number that is on the execution chain and because currently there is a link and there is a link between the student uh yeah actually big and chain clients uses mainnet clients to grab each one data from it and put it on the voting um to onboard new Auditors and process deposits but the EPO whether the execution side is aware of the epoch or a slot it depends on um like an hour and what What will what will be implemented uh it depends on the consensus API obviously we will need a slot or sum of codes like begin block route in the future um and we will probably need Epoch um to signify the Network upgrades after the merge so yeah it's it's not like aware of epoch now so it was now and but it can be it's up to us so would it um does that mean that the the consensus side so the the beacon chain side when it asks the the execution layer to the RPC method it would check what block number it is in and it would just reject building a block if it's just too old or something so all the ensuring that everything is is in sync with the latest hardcore requirements is that then just driven from the beacon chain side yeah yeah that's that's a good question um um are you are you referring to the hard Works after the merge or to the merge hard work itself now it's more a general question after after we are in this merge merged State you know how do we Okay how do we I mean just a question which was asked you know what happens when one side needs to be updated what happens when both sides needs to be updated and um just in general how we'll know the operators know that you know both of the to your clients are properly updated and and just um and to to maintain this independently seems to be prone to two issues right I've been thinking about like for a little bit about how it works after the merch and they've been thinking about a single point of um cardboard coordination whether it's the pure consensus um train upgrade or the execution or both so it will likely be the ebooks of the beacon chain which makes a lot of sense so and the execution client codes will need to be adjusted to be able to enable neologic based on this epochs propagated down from the consensus layer but this is just like the basic thoughts on it does it answer the question yeah that this needs to be discussed I guess yeah definitely um yeah maybe if there's no specific point for this in the the merge roadmap document then but this would be an interesting one to be added yeah that's a good point um cool on that note we're basically at time do people have any final thoughts questions comments in the last minute uh we're gonna hang out in the lounge channel on The Ether indeed Discord if anyone wants to continue chatting about things usually not useful chatter chatter cool well yeah thanks everybody um and yeah see some of you in the Discord and everybody else in two weeks bye [Music] foreign [Music] [Music] foreign [Music] foreign [Music] [Music] [Applause] [Music] foreign [Music] foreign [Music] foreign [Music] [Music] thank you 