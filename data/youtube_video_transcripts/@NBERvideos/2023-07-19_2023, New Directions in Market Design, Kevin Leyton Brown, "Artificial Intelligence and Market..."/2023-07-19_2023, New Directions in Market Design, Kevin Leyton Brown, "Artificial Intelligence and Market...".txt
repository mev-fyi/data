oh my God how do you follow that seriously wow um and furthermore you know what what a great conference it's been so much fun to be here it's so neat to see these you know different verticals of places Market design has had a big impact and people um you know speaking kind of in a coherent way but each of those topics uh and then to just be asked to talk about my entire discipline uh as uh as the topic of my vertical uh it was kind of daunting so I decided to sort of subvert it and try to talk instead about the actual Market design project that I had been part of and try to think about you know how my disciplinary perspective uh impacted me but I didn't recognize that the Apparently boring sounding regulation panel before me would in fact be a bunch of really animated people talking about AI much more boldly than than I'm going to uh so so I I said here that if time permits will be followed by ill-advised rampant speculation but I set the bar for myself pretty pretty high for for ill-advised and rampant so so maybe maybe not as rampant as I thought but um anyway look I I want to begin by saying um AI you know actually like as an AI researcher I it's sort of a frustrating thing because AI is a really ill-defined category and it's got a porous boundary between um you know Ai and the rest of computer science operations research and data driven parts of economics and statistics and furthermore there's this thing that that some people call the AI Paradox which is that once we understand something people no longer want to call it AI you know chess playing programs don't feel like AI to us anymore uh you know John McCarthy invented uh multitasking that used to be considered an AI technique but it's not anymore um I I would say you know talking to Hal in preparation for for this uh talk and he said I'm allowed to quote him on this he said he distinguished between Ai and ml because he understands ml yeah and so so if we've lost ml now we're not even allowed to talk about that as AI that we're really in trouble uh and if the definition of AI has to be that it's something we don't understand that I'm really screwed talking about this for half an hour so I'll let you be the judge but uh but I'm going to take a narrower and more maybe boring perspective about AI I think from the perspective of Market design I want to think about an AI approach you know in the context of Market design as any approach that automatically gets better as we give it more compute and or more data and and maybe that sounds like kind of a low bar but a lot of what we do in Market design doesn't really need that bar right a lot of what we do in Market design is pretty static in terms of the kinds of assumptions it makes in this sense in which those assumptions can be based on data and it's pretty static in this in the way that it leverages compute it has it sort of compute some function and then it's done and so so I want to think about ways that we can and go a bit beyond that um and you know sorry this is not a talk about chat gbt I really wanted to focus on things that that actually you know got completed and had to do with the market uh although I'll speculate at the end lastly the other caveat I want to give is that successful Market design and I think a theme that a lot of people have really been mentioning in different parts of uh of this Workshop has been the successful Market design needs simultaneous consideration of economic computational social constraints and so I'm not going to talk only about the AI elements of a market but I'm going to try to highlight as I go ways that AI made a difference so so overall just trying to think about how AI can contribute to Market design it can be used to help overcome search friction uh problems when matching with an interested counterpart is hard to do that can help people to express preferences particularly when those preferences are complex or non-linear um there's been a lot of focus uh throughout the time that I've been working in the field on designing algorithms to Clear markets in the presence of these kinds of complex preferences and then when Big Data becomes available this opens the door to using machine learning and in a variety of different ways so we can use machine learning to give information to Market participants we can use it as an input to the market rules themselves and we can use it to optimize the performance of the algorithms that that lie underneath the market and we can use it to enable simulations to study how markets uh how well markets worked and I'm going to touch on a bunch of these themes uh in the the case study that I want to talk about which has to do with the the FCC incentive auction um many of my collaborators on this project are actually in the room so uh Carla's here I didn't work directly with Larry but Larry's here um and of course I'm being followed you know as though uh following the regulation panel wasn't hard enough for me you know after my panel then the Nobel Prize winner who led my uh the the thing that I was a small Cog in the wheel of is going to speak after me and uh and you know correct all the mistakes that I make and I want to call out specifically uh my student Neil Newman who uh took the lead uh on and many of the the things that I'm going to talk about here and and and basically I want to say a little bit about you know how um computation fed into the the incentive auction in the first place but then I'm going to focus a lot on how we can study it after the fact using kind of AI techniques at so uh kind of quickly to begin because this is meant to be retrospective uh the incentive auction began as an attempt to repurpose Wireless Spectrum from television broadcast to more profitable uses uh and you know as an economist or an economically minded sort of person you might wonder uh why would we need a government to do this why can't we just allow trade to happen and and indeed our Coast you know even writing in the context of uh of telecommunications options I made this point that uh for an efficient allocation all we need is clear property rights and no fractions but unfortunately there's a really critical friction in this setting that that gave rise uh that you know interacted pretty substantially with some of the computational issues uh which is hold out power so wireless companies need big contiguous blocks of spectrum uh in order to be able to make profitable use of it and and individual channels could threaten to block um entire transactions for big payouts and so at any efficient market would then have to enforce really high payments to each Channel I wouldn't budget balance and the whole thing wouldn't work out and so uh the the people who designed this auction um made a really uh clever um you know definition of the property right underlying the market they really let everything get off the ground and I guess that's been another uh theme of this Workshop that uh you know defining the the property rights the goods you know properly can really make a big difference to the market design and rather than taking these things as given uh so here uh the redefinition was that stations had didn't necessarily own the actual frequency that they were broadcasting on they owned the right to broadcast uh without um facing substantial interference and and this made stations into substitutes for each other fostered competition made it possible to clear a market another question that the FCC faced was how much Spectrum to clear it was I'm not clear how much uh interest there would be for for buying this cleared spectrum and and they didn't want to buy back a whole bunch of spectrum from TV stations and then discovered that they couldn't sell it and so they came up with a bunch of different band plans for you know what they might do if they clean clear different amounts of spectrum um and uh you know given that there's uh in the standard economics case you know you try to make Supply meet demand but but here there's no homogeneous good there's no single price across the entire country um and uh and so this was a tricky question to think about um and another kind of key economic Market design principle is to to try to Define property rights in a way uh that that makes you not have an externality about who else gets allocated so you shouldn't have to care who wins and who doesn't win uh in the incentive auction this meant that uh that you shouldn't be interfered with uh other people who remained on the air if you yourself remained on the air so um when we uh changed the channel That a given station was broadcasting at it was important that it not cause more than minimal interference to any other channel uh unfortunately though this was a computational non-starter to do exactly um the uh the so quantifying the number of customers affected by interference on a given assignment of channels to all of the stations um you know where there's a piece of software the SEC had that they could use to to figure this out it took days of computer time and uh with almost 3 000 stations being packed into 29 channels there were 10 to the 4 300 possible assignments uh Each of which would have needed this much computation to do so so then this just couldn't be done exactly um and so here came another uh really valuable redefinition that got this problem off the ground uh it was and a ruling was made that a station would be considered to suffer minimal interference if no other single station interfered with more than half a percent of its pre-auction audience rather than any aggregate across all of the interference that it received uh and that allowed these pairwise constraints to be pre-computed uh even so even with this simplification thinking in terms of pairwise constraints the problem of determining whether there existed any channel assignment for a set of stations is an NP complete problem it's a graph coloring problem and so the worst case running time and solving that problem uh is going to have to scale exponentially unless P equals NP and so this kind of gives a second response to the sort of cosian argument about why why shouldn't you just allow uh you know the market to take care of all of this because this would um imply that that you know the market itself can solve arbitrary NP complete problems to to reason about this kind of thing so the ultimate clock auction design that the FCC uh put in place had an ascending price option for telecons and where they um they sold Spectrum uh that would be um that would be cleared the reverse auction where they they bought spectrum and when the two auctions terminated they assured that the amount of money that they collected uh was uh greater than or equal to the amount of money that they um that they had to spend in buying uh the Spectrum and if that didn't work then they reduced the clearing Target and started again that's how they solved this problem of deciding uh how much Spectrum to clear uh so I got involved in the feasibility testing part of this uh project um which asked um which as this uh NP complete problem of whether a station could be accommodated on air um if um they they no longer wanted to participate in the auction so so the way that the reverse auction worked is stations were given a price offer you know somebody would come to you and say will you sell your station for 10 million dollars and you'd say yeah that's that's an easy yes I definitely would sell my station to you for 10 million dollars and then in the next round of the auction they'd say well you know sorry about that how about three million dollars and you'd say well you know not quite so easy but sure and then you know they come back to you and say what about you know eight hundred thousand dollars and you'd say no no can do I'm gonna go back to being a broadcaster and at that point uh there had better be some Spectrum to put you in right it's better to be possible that there's actually um you know interference free channel that you could be put on um that that would make the whole auction feasible and so um each price movement in the auction needed to be backed by this kind of proof of feasibility it didn't actually mean that that really was the the channel that we assigned people to uh that was Carlos problem to figure out later uh but this just was a certificate of feasibility and uh this was a tough graph coloring problem and uh there was initial skepticism about whether we could you know in the runtime of the auction solve this exactly at a national scale this was the actual uh constraint graph of the auction um but uh there there was a really beautiful element of the auction design that I want to highlight where if we couldn't solve an instance then um we would just treat the unsolved problems as being infeasible so if we timed out then we would say um well you know this problem um you know maybe is repackable but what it's going to treat it as being not repackable which means we don't lower the price for that station and that that maybe costs us a bit of money but it doesn't break the whole auction and so I really like this kind of Paradigm of a market design that relies on a hard optimization problem which we do our very best to solve but if it doesn't work out then we can just lean on uh a little bit of degradation in in the economic quality of the outcome and have this really nice trade-off there so that's something I'd like to see in more markets kind of going forward so we took the bed between our teeth and we built a feasibility Checker using all kinds of different AI techniques and to try to make it as efficient as possible so uh we encoded the problem as a satisfiability problem we took a wide range of Open Source shot solvers uh we used a technique called algorithm configuration which you can think of as being machine learning in the space of algorithm design where we say I have a sort of hypothesis space of parametrized algorithms and I have a distribution of problems I want to do well on I want to minimize the loss in that space which is finding algorithm parameterization that performs well on this distribution of problems um we also combined this with an algorithm portfolio technique that says I want not just one algorithm but a whole bunch of algorithms with uncorrelated performance I run them in parallel I only care about whichever one of them finishes first um and then we had a bunch of other custom optimizations so so we kind of threw all kinds of different kind of algorithm design AI techniques in this and and we we eventually built something that that was really fast and it much faster than the techniques we had before and the auction happened and it happened once and uh a bunch of spectrum changed hands and a bunch of money changed hands and uh and that's it um so so then we we got interested in in trying to ask you know was the incentive option design a good design not just to you know did it work out well this one one time but how would we think about whether this is a good design and uh here we here's another sensor which I want to use really extensive computation to try to ask an economic question and so I would consider this a AI technique um so we conducted really extensive simulations where we built an auction simulator that uh as far as we know is the highest Fidelity simulator of this auction that takes sometimes days to run an individual simulation and we constructed a bidder model with parameters describing bidder valuations and better Behavior Uh in in the option it's too much detail to go into here we established a probability distribution over all of these parameters and then we draw many samples from this distribution to get you know different possible uh bitter behaviors that we might have in in an auction and then we run paired simulations holding the valuations fixed and varying some facet of the option design and then then we can look at uh what kind of difference it made and we can compare outcomes both in terms of cost uh and into in terms of value loss so how how much reduction and efficiency there was by reducing the number of broadcasters on the air the hope would be you would buy out the low value broadcasters and keep the high value ones on the air uh so uh in in this vein I can now say in terms of our feasibility Checker um not just was it fast when we ran it but did it actually make a difference to the economic outcomes of the auction and and so here here are some cdfs of various different feasibility Checkers that people used it doesn't really matter if you can read it which of course you can't uh but but but what I want to say here is that you know within our cutoff time which is the right side here um the fraction of instances that each solver was able to to solve and the ones at the at the bottom C flex and groby are solving uh sort of uh 20 30 of the instances some of the existing sat solvers are solving sort of 60 80 and then we were solving like 98 of the instances but but did that matter um so for these experiments I'll walk you through the setup a little bit more and then I'll show you some other things and more quickly to show you how we do this and so we simulated uh in this case UHF only National Spectrum auctions and 84 megahertz clearing Target uh we gave the feasibility Checkers 60 seconds and we took a valuation model from the literature that I'll call MCS for the maximum of cash flow and stick value don't don't worry if you don't know what that is and we took a 50 valuation samples from our model and we varied only the feasibility checker on each of those paired samples and we looked at what happened uh and then of course there's some variation just because sometimes you know people with high values will pay you more than people with low values and so we normalized by the the cost and value loss that we got with sad SC our own feasibility Checker and we looked at how much proportionally better or worse we were on each sample for the other feasibility Checkers so I'll have a graph here a scatter plot where every point in the scatter plot represents two simulations one of sad FC uh that was used to normalize and one of the other feasibility Checker that we ran through an entire run of the spectrum auction that involves solving tens of thousands of feasibility checks and you're doing the entire trajectory and then seeing what happened in the end so the the graph on this slide took about half a year of computer time to generate and this is actually the smallest and you can follow along with those numbers throughout the talk this is the smallest amount of CPU time of any of my graphs um and uh and so the scatter Parts on the x-axis I'll show you the normalized value loss so less loss is better that's uh uh and uh normalized cost on the x-axis and how much did the FCC in our simulation have to pay to clear the Spectrum all right here's what we found so the the red diamond for sat FC in the bottom left-hand corner of course is at one because I'm normally dividing it by itself so so all the apps these uh set of C runs are going to be uh in the bottom left-hand Corner that's a sanity check for you um but you know when you do empirical work you don't normally see something this nice right so uh incredibly clean trade-off where we were improving uh both in value loss and in cost as the uh solvers got more and more effective uh so uh really quite and the proportions are quite big so you see like a you know a factor of two in cost you know the uh if we'd use C Flex out of the box we would have paid something like three times as much in cost and lost about three times as much value and the South solvers were better but uh really big effects here um now that was only one value model and I was UHF only so we can do this also uh we fit another value model to the actual bid data that was returned by the FCC doesn't we have only one sample so we had to do a bunch of complicated statistics I won't get into and we also can simulate the VHF packing that the the auctions did and then I can get graphs for for all of these different scenarios I'm not going to walk you through all of them but you see the same pattern occurring where uh you know we we get nothing uh you know we have no simulations where we have fractions less than one which we of course could have had if something else was better um and and we still see a significant gains both in cost and in value loss so this really backed up for us that making these algorithmic improvements can make a a really fundamental economic impact uh but we wanted to ask questions about things besides the feasibility Checker so here's another question we asked I mentioned that uh the um the auction repacked the VHF bands as well as the UHF bands uh so what what they only wanted to clear UHF Spectrum but the way it worked was if you were a UHF station you could be paid a lesser amount to move to VHF which is a lower quality way to broadcast but uh but you would still be a broadcaster and then maybe you would displace some VHF station who wants to go off the air it would have a lower value than you so so this could uh you know in principle lead to gains inefficiency and also in cost but it made the rules really complicated it was no longer um uh obviously strategy proof and uh it was hard to think about strategically and it wasn't clear that there was enough value in the VHF band that this would be worth it um so the we wanted to investigate whether this extra complexity helped and um looking at these simulation plots here you can see uh that it looks like it really did in our simulation it's pretty robustly so here um here I'm normalizing by the with VHF case and every one of the blue dots is a normalized paired sample and without a VHF case looking at what would have happened to exactly the same stations with the same valuations if we and if we look at ran the same auction without giving them the opportunity to pack into VHF and you see it almost always cost more and it usually experienced more value loss the star and these graphs represents the mean of all of the samples so we do see a bunch of variation here but but still looks like this was a good idea for the FCC to have done so way to go Ilia Segal um here's another contentious element of the auction that they got a lot of discussion at the time um it turns out the prices didn't fall uniformly for all the stations uh there was uh scoring that um basically increased the prices first the initial prices for stations that caused a lot of interference constraints uh and uh also um there was a scoring based on population that that sort of was a kind of Meyerson pricing sort of thing that that would offer lower prices to stations uh that looked like they would most likely be willing to accept lower prices uh but the latter was was especially contentious they were actually protesters in front of the FCC carrying these signs and and uh particularly because this was contentious we wondered whether uh whether it was a good idea so here we sampled uh auctions we ran them uh in four different um scoring systems so we looked at um what if we scored based on uh not not at all based only on interference based only on population or based on both the way the xcc did it and here we got a really muddled message our simulations I don't really give much evidence that scoring helped either in terms of cost or in terms of efficiency so um there's you know one case in the MCS model in the VHF plus UHF case that there's a a decent Trend but in the other cases it's really just a cloud of points around the center uh so this really didn't uh this looks like a case where uh you know right in retrospect this might not have been worth the the political cost yeah yeah the guy with the sign well I mean it didn't make anything worse either but uh um the last question I want to talk about was was this question about how many channels to clear uh so the FCC LED Market forces determine how many channels to clear by doing these alternating uh rounds between the the full production and the reverse auction um and this wasn't an element of the design they received as far as I'm aware a lot of comment uh in the lead up to the auction um but we wondered how this compared to just clearing the right number of channels if you had just known where the auction was going to end up and just cleared that number of channels so we ran a four-stage simulation mirroring the real auction uh and we compared it to a single stage simulation that that dress cleared the last round you know again pairing all of our samples uh and to our surprise we found that this this was uh reasonably bad so uh it uh pretty consistently increased costs and reduced value uh compared to not having done this multi-step thing so uh and the problem was uh that you know we would sort of drive a hard bargain in the reverse auction we would sort of force force in some commitments uh you know as we kept grinding the the reverse auction down and then we we sort of made this greedy commitment that we didn't undo later uh and this led us to wonder at what if instead this is something that as far as I know wasn't proposed when the auction was was being considered but but this led us to wonder about it um what if we'd run the forward auction first uh we'd known what the forward auction Revenue Target was and then when we ran the reverse auction we just stopped it as soon as we hit that Revenue Target we Once We Were weren't going to reach that Revenue Target rather than running it all the way to completion as was done so we simulated that as well oh by the way look at the CPU years the last one was 31 years and this one was 50 and so this was a real Beast of an experiment to run which is why we did UHF only um but but here we got um pretty similar outcomes to perfect forecasting so this pretty substantially mitigated those problems from from running the reverse auction longer I think if anyone wanted to use this design again this is a change that I would seriously consider making and so more broadly I just want to argue for this kind of really extensive uh simulation backed by you know really careful statistics as a way of asking economic questions about a system that you can't just gather lots and lots of data about okay so so that that's what I had to say about the the kind of AI that we actually did but I imagine you know you're sitting there thinking yeah okay fine you can call that AI if you really want to Leighton Brown but you know tell me about the sexy kinds of AI that you know I read about in the newspaper uh and uh you know here I think the the fundamental catch in in Market design is is to do with counter factuals so you know machine learning is fundamentally about finding patterns and IID samples and Market design is fundamentally about optimizing over possible worlds and trying to find a possible world uh you know that does well under some objective function and most of these possible worlds are ones we don't have any data about because we haven't gone off and used them for something and so this this you know at least for this kind of uh you know sort of pen and paper sort of Market design where you go off and study something for three years and run an auction it happens once it's pretty hard to see how you know gbt is going to be the the Silver Bullet that solves your problem there that being said here are some ways that I do think that uh that AI I can be impactful in Market design going forward even the sexy kinds and so one is to find you know clever ways of putting ml components into a larger system um something I've got a passion for that others are working on too is building a behavioral Game Theory models from data so trying to understand how and people reason in novel strategic situations by seeing examples of them reasoning in previous uh situations and the behavioral Game Theory literature tends to fit very very low parameter models to data uh and in some cases not even really to think about fitting them to data and but the machine learning person would say this is a machine learning problem I want to find something that General if all I want is is a predictive model I don't necessarily need these parameters to be interpretable um some other ideas are to leverage the natural language powers of systems like uh tragedy BT to do preference elicitation via natural language or to build automated negotiation kinds of systems that that would Leverage The the language abilities of of llms uh and fundamentally I think we're going to start to see large language models used not so much as a replacement for other parts of the system but as an interface into the system so I think we're going to have messy markets with complex preferences things like you know eBay you sort of think of it as a you know giant mess of different sites that you can't quite understand and you could see a language model as a way of you know aggregating all of that textual data and surfacing to you things that you might care about and then you might give it some feedback about uh you know what you care about it might iterate um I should say by the way I I had other challenges of that I didn't mention it I should say what I meant you know one is a large language model uh just it does a constant time computation right it does one forward pass uh to predict the next token and I it can do wonderful things and and that can be a you know a pretty substantial uh computation but it's not going to solve the problem of computational complexity right it's not going to solve an NP complete optimization problem in in one forward pass so this is another sense which we have to be realistic about large language models and and not only that but as you might know you know gpt4 is great at multiplying one and two digit numbers but if you ask it to multiply like four digit numbers it just makes stuff up right because it just yeah and you know so we haven't you know built a system that's you know able to replicate a calculator uh because it's not trying to it's just trying to you know think about you know words that chain together and so I think we're going to see you know in order for these systems to work in economic context I think math is going to be pretty important um and so I think we're gonna have to yeah reasonable people might disagree but you know so I think we're going to have to build these systems in a modular way where they can call out to you know we already see chat GPT being integrated with wolf from alpha but we're going to see you know integration with optimization engines integration with you know other you know economic subroutines it's not like this thing is just going to hallucinate answers to economic questions uh a second thing we can do of course is to learn models that explicitly reason about counterfactuals and uh of course I don't need to tell this crowd that causal inference is a big hot area and Susan you know already said all the stuff about AI so I get to now talk about causal inference I guess but um but uh but we can go beyond that so I think we're going to start seeing people using deep reinforcement learning as an agent model when we build simulation so I think we can ask a simulation question you know a weakness in our simulations is we need to specify how bidders will behave and uh you know in the incentive auction that was okay because it was obviously the uh dominant strategies but in more complicated markets that's a big problem for simulations and reinforcement learning offers a way to to you know do something really interesting there so I think that's a way that AI is going to change economic analysis and I think we're also going to see large language models as a cheap proxy for human subject experiments where you can just give different kinds of treatments to a large language model sort of say this was trained on a lot of you know regular people on the internet writing text it sort of is some kind of a proxy for a regular person it might not be exacts but on the other hand you don't need to get Erb approval get uh GPD so you know if you want to you know you know rerun the Milgram experiment in the comfort of your own home and you know only inconvenience electrons you know this is a way you can try to do it uh finally I I it's something that you know I think the previous panel already touched on to some extent I think we're going to see a lot of Market design not so much enabled by AI but for AI uh I think there are a lot of Market design questions around Ai and I think there's a lot of interest in how we should monetize llms I think we're going to see you know as you know search is shot GPT answering a question for you rather than you going to a web page and this changes the way we advertise so I think we're going to see product placement kinds of strategies or sponsored recommendations um and the technology exists on how to do this but the way we would set up the market is say it's really fascinating to think about and particularly given that llms are so unreliable and they can sometimes do really unpredictable things um I think we're gonna have to think about strategic manipulation of llms so if you have sponsored recommendations think about non-sponsored recommendations right what if I just put up a bunch of text on the internet that tries to influence llms they get trained on that text in order to write recommend you know my products in inappropriate circumstances so I think we're going to see that kind of an arms race there's going to be a question about how we price llm access particularly given that they're they're one of the most expensive consumer things that has ever been served on the internet and also because there's this price quality trade-off right for a lot of things um you know gp2 gbd2 is just fine and it's way way cheaper to serve and so thinking about that trade-off is going to be interesting uh and then we're going to see a custom a fine-tuned large language models embedded in a lot of more complex services and thinking about how to set up those Partnerships I think is is going to be another really fascinating Market design question so I'm going to leave it there um and thanks very much for your attention 