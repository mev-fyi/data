from where i sit i see many empirical researchers who are trying to start using a machine learning method in their work and a lot of people a lot of cases the wheel is being a little bit reinvented and a lot of times the wheel is being a little bit reinvented maybe a little bit awkward and so i thought maybe it's a good time a sign that many of us are interested in in those methods and that maybe we can kind of sit back and and and think about what what can be done already so what i'm going to talk about today is the use of machine learning methods in for people who are mostly doing on the match control trials usually so why i would talk about randomized control trial obviously it's what i do for a living most of the time machine learning and randomized control trials are probably the most important development in empirical methods for for empirical researchers in the last few years rct you know what they are i don't need to talk about machine learning is a set of uh evolving prediction tools uh forest trees lasso ridge uh neural networks etc it keeps moving it keep having more more methods keep developing uh they develop in somewhat uh a further their empirical way and you might wonder what they have to do with each other because for the most part they they are after very different things uh so if you heard uh sandhill for example talk about why we should do machine learning it's going to tell you that we should do machine learning because there are a lot of interesting problems that are prediction problems and machine learning of course is mostly suitable to do protection and i agree with that and then for causal parameters we have so with the machine learning is trying to predict something with potentially a vast number of variables that can be used to to have a good prediction on the other hand fundamental control trial are not looking for prediction they are looking for estimating causal effects and usually they are looking for they are the target is a very low dimensional parameters one treatment two treatments etc so it seems that they could live in their parallel universe without much having to to do with each other and it would be fine and in fact in development there are many interesting application of machine learning methods and some of you in the rooms are working on this on this method for example you can use them to do poverty targeting at the regional level at the individual level you can use it to look at the characteristic of urban landscape how well a neighborhood is doing whether it's a poor rich neighborhood you can so for us for example it can be even for people who do a rcts that can be used as a kind of pre-processing for for creating outcomes and of course the applications are growing and it's all great but what i want to talk about today is not that it's a an area of overlap where you can either use machine learning methods to think about causal parameters and therefore rcts become interesting because they are an alternative to arrive at the same solution at the same problem so we can use their cities to validate the extent to which this effort by machine learning are successful or not so that i'm going to talk a bit about that and then on the other end a city which we think of them as primarily causal or aiming as having as target parameter low dimensional causal causal parameter also face problems which are more akin to the to the big data problems usually not because we have a large number of ends but because we have a relatively large number of sometimes a very large number of variables x's covariates with respect to the number of observations we have and there are three classes of problems that you can become in a sense big data problems one is uh what control variables to choose that's not the most exciting but it's pretty important because that's something that's a question that we ask ourselves most of the time like when you finish the an experiment what should i put in should i put any control variables to in my regression or not and how to choose them the second is the that is something which has probably gathered the most interest among the applied researchers until now is what are the relevant dimensions of heterogeneity in program effects and of course if you have any number of large number of covariates and their interaction and you want to look at heterogeneity that in a sense is becomes a big data problem where you don't have much structure so that could become something like a machine learning problem and the third and i'm not going to talk about that today that's going that's the next project on the pipeline is a we have often situations where we have many outcomes many potential outcomes that we could look at and also sometimes many treatments as well maybe less so and things that we as development researchers do in the field but what people do in a b testing they start having large number of treatments so even though they are interested in the causal effect of the treatment they have large number of treatments and i assume if they are doing that today we also be doing that in some time so now we have large number of treatment and potentially large number of outcomes they are a method that people are already using to deal with that family wise error rate is one of them but this is one can probably do much better than that and but that's not i'm not going to talk about that today so what i'm going to talk about today is the first three questions one is uh um using machine learning for causal effects and what as what can as a what can rct bring to the table in improving those methods and then uh what how to choose control variable and how to uh uh think about it identity in a sense the most important slides was probably the first one and i'm going to have that again in the last one which is we are putting we did put all of the programs all of the codes that are used to run this are in github i think they are usable uh pretty much without much difficulty so in a sense you can fall asleep and think whenever you need any of these things it's there so maybe that's the that's the most one reason why this is useful to talk about that today okay let's let me do this that's the three things that i want to do today uh so first uh uh kind of think about rct as an opportunity to do a long type exercises for uh judging the ability of a machine learning method to give us reliable causal estimates so i'm going to spend a little bit of time on on what is the method that i'm going to subject to to the to the long test uh in fact because this is same machinery that is also used in to think about the the sorted effect the heterogeneity in program effect so that that will be uh we haven't wasted our time uh so the the paper i'm referring to here is a paper by any a large number of authors driven in in large part by victoria zucker who is that's called double machine learning and the objective of the paper is very much to say let's use we want to estimate the causal effect of one variable or two or three a small low dimensional parameter but we have uh there is a lot of heterogeneity between the people who receive this treatment and the people who don't and uh what we hope is and then we have lots and potentially a large number of control variables what we hope is that if we control sufficiently well for all these control variables we can estimate the causal parameter so in a way that's a very very old problem and it's a very very old answer to the problem which is you know you have a you have some you want to know the causal effect of say education on an outcome you know that people who get education are different for any number of reasons it's an age-old method to throw the kitchen thing it's just a larger kitchen thing okay so the idea is how if that kitchen thing is really really really large and has lots of stuff in it how do we uh how do we do how how do we deal with it and that's the idea of the double machine double machine learning can be used with the prior to that victor and hansen and bellony had a paper on double lasso which i'm going to talk a bit more which is very nice very easy to use uh but this is this can be used as the the concept here can be used with any machine learning tool so as those become better then we can we can apply them and the method starts to be used a lot uh so it is kind of relevant to know whether it's any good but we don't really know much about it yet because it has not been it has been validated internally in a way we're saying that if the condition for it to be true are true then we recover the truth that seems to be a minimum requirement but it has not valid been validated much in the real world so that's where i think that people who do rcts will have opportunity to do that any number of times and what i'm going to show you today actually which is based on work that michael and pascaline and i did together is in a sense not a perfect application but that's the only one that i have for now so i think we can do many more i'll tell you why i think it's not perfect okay so just a few minutes on how we uh how we do this so the the inference question is uh it's the causal question is how the predicted value of y change if we increase d by a unit holding the rest constant and uh the only way to think about the way that you can think about it is run your uh your causal regression this way so you regress why on your treatment and then some potentially very complicated function of the z and the beta is the question of interest of course if you wanted to estimate average treatment effect instead of this where there is a fixed treatment effect this can be extended to do that the the trick which is very simple is to go back to our freshvo idea which is rewrite this problem as uh in the partial out form and say well you will regress with you take the residual of regressing y uh on the old z and take the residual of regressing d on all disease and the phrasal verb theorem tells us that the beta is just the solution of running an ols regression of the residualized why on the residualized that's that's that's what that was true then and that is still true today in the sense that it remains true even if uh uh [Music] the residualized for y and for d are estimated with any machine learning method doesn't really matter they are the result of a prediction so our procedure will mimic this idea so what we are doing is the only problem with any machine learning problem is the problem of overfitting so you divide the sample into two parts and you s you use one part to estimate your residualized regression and you use the other part to estimate your oilless regression from the residuals and that works so in practice you do that twice you do the you you cut your sample in two you run the two machine learning problems get your residualized prediction render regression in the other sample and you do it again the other way that's step two and then you take the average of the two and the theorem is that that gives you a consistent estimate of better and that they are standard there are for that for that object that can be estimated uh one note is that you cannot do this by doing you say well why don't we just run a regression where we put d and then through the kitchen sink why do we need to do in two steps and so for example if you wanted to do it with with forest you would you would start with a guess run the random forest on with your y minus the d on your guest and then uh do the other way and st goes until it converts you're going to get very good prediction but you're not going to get the distribution of beta0 minus beta that is not at all centered around zero so it doesn't work to do it that way and that's the the intuition being that you uh you don't know if you are the you could you overfit on one side so what is left on d is the it could be anything uh but the when you do it in when you this is just simulating do it in this with this method so you you residualize one and visualize whether you're as a wire as you realize the d and run other regression on each other you get some something that's centered around the truth so that's nice but of course that's only nice there's no magic there uh it's just the same way that uh maybe for a while we got super excited about matching but it's the matching was only as good as what we were matching on and in the same way there is no magic there the the machine learning is only as good as as uh to the extent that this all of these variables that you have at your disposal do something to to absorb the heterogeneity between the people and maybe uh the covariates are not that good so for uh matching estimate to kind of us to go back on the planet to go back on on some ground uh the la land paper comparing the results of doing matching and many other things actually i think matching was not even in the la land papers it was the dhd weber had where we did the la land exercise for for matching the lalonde paper did it for other methods and then the agent we did it for matching and that has not you know now we need to do this for for for this kind of method for this one and other other that people are going to come up in the in the future so uh we did it uh with pascaline and michael we did it for our again a secondary school study i believe pascaline presented the paper so i'm not going to spend too much on it i'm just going to quickly zero in on this particular on this particular finding so quick reminder on that paper we sampled 2 000 students 10 years ago now and we we have continued to follow them 600 of those randomly selected got a scholarship a large number of those that got the scholarship actually went to school some of those who didn't get the scholarship also went to secondary school so what we are going to to do is to compare uh the an iv estimate where so what we are interested in is the effect of second of going to secondary school on a range of outcomes your test scores fertility outcomes labor market outcome etc we are going to estimate in the control group who is using this double machine learning methods we have like is often the keys in many randomized control trials very rich baseline data sets probably as rich as it gets so we can that's kind of a good good test case from this point of view and then we are going to compare that to estimate the same regression with instrumental variables where the instrument for going to secondary school is receiving the scholarship of course it's not perfect because you're comparing oneness estimate with another and it's an iv estimate and i've estimated have their issues and this one has a number of issues one is that maybe the the iv estimate is not a good measure of causal effect of going to school there could be for two reasons first of all there could be direct impact on getting secondary school on the outcome of interest there could be a financial effect for example people who who the infra marginal who would have gone to school anyway they don't have to pay for it anymore on the other hand people who would not have gone to school are losing money on balance it sort of evens out but that's there could be a income effect for some people going both ways there could be direct effect of going the score of receiving the scholarship uh on self-confidence on the idea that you might be the chosen one you could think that you need to work harder because you were so lucky any number of things that could be give you a direct effect of presuming that course in addition a few kids in the control group go to technical institute instead of going to regular secondary school so there could be also an effect on the quality of going to of the quality of education they receive for the same number of years so keep these caveats in mind i'm not going to do much about them but maybe it sort of undermines the exercise the other caveat that we can do something about uh is that uh the of course the iv estimate is not the average treatment effect for the population it's the treatment effect for the complete local average treatment effect for the compliers so people who are induced by the scholarship to to go to secondary school so is it fair to compare a local average treatment effect for people who are induced who might be people who have worked constraint or people who are particularly smart or perfectly not smart or who knows what to an estimate that is going to be the average treatment effect in the population so this actually can be can be addressed also by kind of doubling down on the machine learning which say well it's okay because i'm going to show you in the second part of the talk that we can do heterogeneous effect so here you can do a heterogeneous effect of receiving the scholarship on going to school is going to give you a good description of who are the kind of people who are induced by the scholarship very rich description as rich as you want it to be so to give you a very rich description of who is induced by the scholarship to go to school so we can reweight the the machine learning estimate such that on at least and this is a large number of them uh the the people look uh similar the people from whom i'm calculating the the population for whom i'm calculating the causal effect with the dll look very similar to the the the compliance okay so that's uh so this we can solve this one we cannot solve uh at least not here so with all that said what do we find so this is for a standardized test scores so not not entirely surprisingly you find that the the ols estimate of the effect of education is much larger than the iv so you tend to have an overestimate of of the ols which is not surprising because the people who are doing the best uh uh in in you know the smartest kids etc their parents will want to push push them a little more to go to school so you have the usual sort of ability bias but this is the test course after after they've left secondary school so you find that the ols overstate the div the machine learning goes about halfway uh it and now those two estimates are not uh not uh significantly different anymore it's still a little bit higher but it's kind of suddenly became closer and the weighted one becomes even closer suggesting that if we have a population that looks the same maybe uh we have uh this is this is progress so it seems that there might be enough uh variables in the data set that we can for as far as test scores are concerned we can um we can absorb some of the relevant heterogeneity mind you if we didn't have the rct we would never have known that but if one does such exercise many many many times with some sort of hoping at some point we will be able to do then we'll learn more about in what conditions this is doing better for fertility is another one where um the so there is there is a large effect of going to secondary school on the number of kids ever had both in uh the iv estimate is quite negative the ols is over is a clear overestimate and note that you when you add the control that sort of makes sense intuitively which is what we are doing here and what we need for test scores it doesn't do much on its own if we ask the machine to pick the control with that double machine learning estimate you get in this case you get you get much closer and in particular the weighted dml starts to be quite close to the iv where we don't make any where we don't seem to well it doesn't seem to help at all is in all of the labor market outcomes so this is for 2016. you see for example uh positive earnings the ols estimate for any positive earning in 2016 is quite negative uh whereas the ivs unit is quite positive so the selection thing here is likely which is at play is likely the fact that if you're rich enough to pay a secondary school you're rich enough to pay tertiary school so those kids are still finishing college as of 2016. that's why they are less likely to be in having positive earnings than in the labor force or maybe it is even related like they're just waiting for something better etc but we saw we have this quite negative effect in the ols and adding control either the intuitive controller the machine learning control does nothing about it at all so this is a place where whatever whatever an observable difference they are between people they are not captured by these hundreds of variables that we are attempting to control for and in 2017 uh we now have many more kids are working this is later they have finished college etc so this is gone but what you're seeing is the opposite which is in the earning the in the long earning for those who are working the selected learning for those who are working there is an overestimate in the ols compared to the iv that again is not moving at all neither due to with the control for the machine learning and it's not a complier effect apparently because controlling for it it doesn't help so for some outcomes this is kind of not it's not this is a glass half we'll half empty it doesn't really matter to me in a way what it does it's uh but for summer outcome it seems that we come closer to the iv outcomes for some like fertility and quite close for some it's very different as i was saying i don't think it's perfect because we are comparing an iv and you could not like the iv in which case you are comparing something which is flowed with something with this fluid so it's not clear what we should prefer so i think what i would really like to see uh is to see applications that where we directly compare the rct treatment effect with any or you prefer the machine learning methods in our case for example if anybody in the control group had gotten scholarships not through us but who you know their friends etc the ideal exercise to have done would have been to come to look at the effect of scholarships on all of the outdoors but unfortunately too few people in our sample got scholarships in irrespective of us but that's what that kind of what uh i think could be done and then we would make progress on seeing i think one one thing that is kind of interesting as far as the research question is concerned is what kind of heterogeneity you can pick up with observables what kind of ether agency you cannot pick up with observables and try to relate that to some model of how the selection works etcetera so i didn't do any of that today but i think this is something that could be could be interesting to do in the in the future so that's that choosing control variables uh it might be a sort of not a little bit boring question for people who do rct after all why would you choose control variable in the first place if you raise the 80 events handbook chapter on rct you should just not put control viable at all you should stratify don't control but the world is such such place that everybody does control for something on the other so in practice i read many many many papers and i don't think i've seen very many rct people that don't feel like they have to include some control so especially when you it happens that by chance and it always happens something some variable of the very many that are included in the baseline someone always goes one direction or the other and you worry that i i i tend to believe that most applied researchers are very well intentioned and so my my sense is that people worry that this may bias the result one way or the other and therefore they want to make sure that the results survive when we put this control they also are concerned for precision which is if there is differences between these variables if if some control variable happen to be different they might affect the outcome so people don't know really what to do like you don't feel that you should not have control and you don't feel that you should control and you feel bad regardless and um that something where i i sense a sense i i personally have a sense of uncertain and settlement and i sort of feel that there is a this sense of settlement is a bit generalized so i conducted a unscientific service of applied researcher practices which is how do you decide what to put in i never heard anybody tell me the dark scenario that i'm putting all the variables that will make the stars just a line for me and now that there is no stars in the air anyway that's there is no reason to do that the data suggests that people don't do that since we don't see people we don't see any bunching and suspicious branching of t statistics in our city papers but what i think people do intuitively what i think people try to do is that they will include that control variable either the variables which are very imbalanced or the variable that they think would really affect the outcome a lot or both and it turns out that doing both is exactly what one should do if you read the belloni channel zukov and hanson lasso paper which exactly this idea of you want to regress a y on a d which could be a treatment with a nuisance parameter that you're not interested in what they're saying is that what you need to do is to include is to run lasso regressions uh to pick what are the relevant regressors that predict d the treatment what are the relevant regressors that predict y and just and then run an ols regression including both set of regressors so my sense is that this is what a lot of people already do and it turns out that that's what one should do so the proposal is that we could just decide to do that remove the angst and this is much better than the other possibility which is to write down the pre-analysis plan that says what you're going to put as control variable because you don't know at the point where you write your pre-analysis plan what is you know how the god of randomization are going to screw you so this is a way of kind of playing with them to say i don't know but i will find out but of course you find this out in a disciplined way so there is no particular reason why you would have any temptation that things would you know to pick what you want i don't think that's what people do but at least it will make clear that it's not what people do so um what we did is to create a little tool so the only difficulty of this is that you need to prepare your data set and then you need to run the to run the regression so running the the the double post lasso is now a game child play if you have a prepared data set because there is a very nice stata command called pds lasso that implements double post lasso the only problem is you need to have a data set that is ready and what ready means is that of course they cannot be text and they cannot be if you have dummy variables they need to be horizontalized and that type of stuff so what we did is that we created a little tool that is also in the github that i was talking at the beginning that that helps doing that will take uh will help clean your data set to make it into a machine readable data set you'll still have to make some decisions about what should be there in the first place for example when you have qualitative options how many do you want once these decisions are being taken then you can just run the run the double post lasso so basically you uh you you gather your potential control variable your plane applies this cleaning process uh uh procedure to the control for which you can use our tool you use uh you can't convert categorical variables into indicators adding the square you can create interaction if you'd like that can potentially create a vast number of variables but it's okay because what lasso is doing basically is to pick the right one and usually a very small number of variables um creating dummy variables dropping the perfectly collinear one and then standardized everything has zero one a mean zero or standard deviation one uh so we've done that for uh a couple of data sets just to see like like make the thing run one hope is that by doing that you also get a lot of a lot more precision because potentially you now control for everything under the sun uh so far i must say it has not been the for the two data sets we've done we it's not it's not been the case it really does nothing for standard error that's the bad news uh the good news it does it does really nothing for estimates either so the one thing that i was hoping to do but it's kind of the cleaning process takes time so i don't know if we'll ever get around to fully do it is to do it for a large number of of papers because a lot of papers now have the data available and then hopefully we would find what we're finding for example in this old candidate set and you'll find a very similar thing in the paper with michael and pascaline on the hiv and education paper that basically uh the the results are are essentially the the same so all that to say is maybe we shouldn't control for anything in the first place or maybe the fact that that's already what a lot of applied research are doing intuitively means that we are very close to where we needed to be at the end so i don't think any of that is going to lead to kind of great breakthrough in scientific practices but it's kind of a useful tool if you were ever asking the question how to choose control variable without without messing with your standard error that's uh that's a method third um thing i want to talk about and maybe spend a little bit more time on is the heterogeneity because this is something where actually i think we could make this is something that could actually have substantive applications of of interest to many uh many of us um so very often and again unscientific survey of papers submitted to various journals success that is almost always in any on the most controlled trial paper or in fact in any identified causal effect paper a moment where you want to show how heterogeneously effects are either because you you want to explore mechanism and that sort of helps to know if some people are more affected than others that's one way to get that mechanism and the other is because you would want to know for example uh whether and for policy making it's particularly important i'm going to talk about that in more detail later is whether the effect would be different in a different population so if the effect if you have a sufficiently large group of people where the first secret is being done it can not only tell you what's the effect in that group of people but it could tell you what would be the effect be and another region where people are poorer our people are taller or less educated or more educated so heterogeneity is very important and again we have often many many potential covariates that we want to look at so there there is i think a clear risk of specification searching and as much as i don't believe that people do it for putting control variable it is pretty obvious to me even by introspection that you know you're going to run a lot of regression and you're going to include what is interesting so that's you know potentially a problem because we don't know if you've done enough tests and you don't and you don't know what was done and not done it becomes harder to interpret the results that are being presented is it just by luck that it happens that it turned out this way or is it is it something real so the solution one which is sort of maybe the fashion in political science or in psychology is to preregister and you know maybe come to economic students to some extent is to pre-register and say what you are going to uh to to look at in advance so let me say if i am the fda and i'm interested in regulating a pharmaceutical farm i want them to pre-register because if the only question you're asking is what the effect of a drug for women versus men i want to know that it has been that experiment as it was designed in order to find these questions i cannot let people yes there is a lot of money at stake and there's only a treatment effect so but for us because the objective of running on the voice control trial in in economics is very la really to just look at the treatment effect and to say this is the effect of my treatment because we are not testing drugs we are trying to learn about the world the idea of prayer registering all of your subgroup analysis seems to me a variance is unsatisfactory because you are throwing all of your data because you didn't know in advance like you just is whatever you were thinking you know maybe over five minutes or maybe over five hours or maybe five days but whatever when you did this moment you only had very very very partial information about what was relevant so if we only pre-registered experiment what would you know what it's kind of very sad in that case we should stop collecting covariates because if we are not going to use them then what's the point so still there is a problem i don't think registration is very good solution for us a potential solution is just to say uh to be very upfront you know this is i'm presenting these few things maybe it doesn't hold up one should try again i think that's fine with me that would be a good way especially since we put the data out so people can try their own things this is kind of the point that gerard and abhijit and eric nobody in their paper which is why let's just present thing and be speculative no problem and another possibility complementary to that is to say okay it's like for a control variable putting machine learning help guiding the process without constraining it here it's very tempting to think well can we use machine learning to guide the process of looking for heterogeneity so we are not tying our hand but we are drawing a path in a way that we are going to follow and you will see a paper this this this afternoon i think or tomorrow that natalia will present i assume since i saw her and i didn't see the other two but that will that use a machine learning prediction of heterogeneity and is asking is the machine doing as well in predicting heterogeneity of the impact of a grant as the as the people i will not tell you the answer but that's a very uh super interesting of course super super interesting use of that of that method another paper that i've seen some time ago not in development actually but by sarah heller and davis looking at predicting who benefits the most from a summer job both of these people use an algorithm that was developed by a vegan athei which is a minor drawback that it doesn't come with standard error so then that's what i was saying about reinventing and fixing the wheel instead of trying to make up some way of doing standard error or asylum or does that by bootstrapping which seems fine but may or may not be work in this particular case so here the problem is that once again once you've said oh it's great we are going to put use machine learning tools to look for heterogeneity you are in kind of a somewhat awkward position potentially because much machine learning tools are for prediction they are not for inference and when you look for heterogeneity implicitly you're trying to make a causal statement which is the or at least a descriptive statement which is the cause the effect is greater for the woman not to say i am predicting a amorphous set of characteristics is that that this set of people will have a larger treatment effect you see what i mean we want to say the treatment is greater for women we don't want to say this set of people will have a greater treatment effect but i can't tell you much about them it's just them and it's the same thing for predictive medicine which is a use where of the of a treatment affected heroinity where people want to be you want to if you want to use so personalized medicine if you want to use heterogeneity in treatment to make a recommendation that's personalized for a person outside of the of the of the set of interests you are trying to you need a proper inference method and the the tools are not really uh designed for that no they're very good at it so how to fix that that's kind of the objective of this paper so one way to do it in that the the seasons both of susan's paper one with verger one with hilo make progress by focusing on one particular method or defining a narrow set of assumptions under which you can make we have some theorem to make inference a statement with the machine learning tool and what we are doing in a way is a completely different approach we are going to say we we're going to try and build methods that can be used with any machine learning tool that doesn't rely on much assumption about their validity at all but we are going to be much less ambitious about what we are trying to to get in particular we are not going to try and recover the entire heterogeneity and treatment effect the entire conditional average treatment effect as a function of all the set of these that might exist we are going to focus on key features that might be of interest so this is going to be much less ambitious but hopefully these are features that are going to be interesting features such that is there any heterogeneity with respect to this rich set of control variable or not so that could be a good first question to ask because if you don't find any heterogeneity with respect to the rich set of control variables and maybe the thing that you found between women and men is is is luck second question is what's the difference who is what's the difference between the most affected group and the least affected group is there a large difference between their effects and the third quest type of questions you might ask is what are the characteristics of people who are in the most affecting group and the least effective group say the most effective quintile or the most effective affected this cell or the least affected desire so we are less ambitious in terms of what we are trying to get at of course one can build more more objects to estimate and estimate them and then in exchange we can use the vast all of the tools available and all of the tools that are going to improve so let me put some notation and make things a bit a bit clearer so suppose that you have uh y1 and y0 your potential outcome for uh states one and zero and you have a vector of covariate potentially very large and an outcome so y1 and y0 are your potential outcomes and the first thing you have is the the the baseline heterogeneity with respect to the to the z so we are going to call that b0 of z and the second one is the conditional treatment effect which is a different conditional between potential completed and untreated condition along the z so in in principle that's our target but we are going to be less ambitious than that and suppose that your treatment effect is randomly assigned uh uh maybe there is a propensity score that depends on the on the subset of the z and the probability score is called is node p of z so the observed outcome is this usual formulation and in particular it can be written as b 0 plus uh d times s0 of z as well as zero of z is the heterogeneity between the between the between the people as a function of z and what we observe of course is is the data uh the y1 or y0 depending on treatment status all the status all the z and the di so we know that 0 of z is going to be typically if you just try to estimate with any machine learning method an interaction y times equal b of z plus d times s of z what you're gonna get is a garbage we are going to get something that is very uh emphasize overfitted and you can't tell much about unless you are in specific circumstances like for example you can do it very well with trees where you're saying half of the sample i'm estimating my trees and in the other half i'm looking at heterogeneity by lifts of the tree this is the susan's methods so what we are doing instead is to say well let's cut the sample in two as well in one sample we're going to use whatever is your preferred machine learning method to predict page line at the heterogeneity as a function of the z and to predict and to predict to try and estimate s of z we know that it's not very well estimated potentially and what we are going to consider that these maps are are frozen that's the auctionary sample it's estimated however well or badly and then we are going back to the to the to the main sample and we are going to we have some results to get the best linear predictor of s 0 the true conditional average treatment effect so the best linear prediction predictor not the full s0 but the best linear prediction using this sz that has been estimated whichever way enough in our auxiliary sample the average by groups the average treatment effect by groups for example by decile of the from the least affected to the most affected and then potentially very interesting for us in particular when we're thinking about mechanism and things like that is the average characteristic of people who are in these different groups and the theorems in the paper is that you can get proper you can get proper inference for these things you can get valid information about all of these features so the uh we have for each of them we have two strategies but i'm just going to very briefly discuss discuss one of them so to get the best linear prediction what you're going to do is you're going to take your soc that has this it's been estimated in the auxiliary sample and then in the main sample run a linear regression of your outcome on the the treatment minus the propensity score and the treatment minus the propensity score interacted by uh with the s of z that has been estimated in the other sample and then the expectation of f of f of z in your main sample and the result is that if you run this regression beta 1 is the average treatment effect and beta2 is the best linear uh projection of the heterogeneity of the heterogeneity that exists so why might you want this regression what's the useful interpretation of beta2 the ray only useful integration of beta2 is that if f of z happens to be a perfect proxy for the true conditional average treatment effect for the true heterogeneity then when you run your regression in your main sample beta2 would be one on the other end if f of z was pure noise whatever it is that you're estimated is just overfitting then when you run your auxilia your regression you're you're just going to get zero so a simple uh uh an interesting one reason why you might want to run this regression is that it's going to give you a very simple test of whether there is any heterogeneity any real heterogeneity in the controller in the treatment effect with respect to the z so before you you're going and start on the fishing expedition of where it might be that's going to tell you whether it's there it doesn't mean of course if you don't if you find beta equals zero it doesn't mean there is no heterogeneity it might be that there is a transient that you you can't characterize by the z you might be able to pick it up with a with a quantile treatment effect regression for example but it means that the z have no power to predict what this heterogeneity happens similarly you can cut the the sample into you can cut your main sample into into bins of a by s of z say 10 bins ranked by s of z and instead of running a linear regression you can learn a regression with 10 dummies where you interact d minus p of z which organizes the regressor with respect to all of the regressor data function of these and all of the dummies this art organization is essential otherwise you you get uh it's it's wrong and finally another one which i think is very nice is this classification analysis which is one same thing you have these groups so you can start looking at the characteristic of people in this group so you can look for example or do i have more women in these groups that are least affected versus more affected do i have more educated people or uneducated in this group that are least affected versus most affected now that you are reassured that there is in fact real heterogeneity that is something that is sensible to do so yes but it's just saying that i have too many uh i have two i have not enough information i don't have enough information to tell so but i'll show you in a few minutes we are we are we did an um an application where we have 300 observations uh and there we cannot do all of these but some of the ideas can still be used i'll go back to in a minute the problem is that so you have two issues one of course the more you also need to cut your sample in two which doesn't help so one of the lessons that if one is interested in doing heterogeneity analysis et cetera in randomized control trials i sort of think it's a good idea to think about doubling the sample size because uh that's but the good news is that it doesn't necessarily need to be doubling the clusters because all of the relevant all of the relevant heterogeneity is at the individual level so uh where you get so sometimes the the the sample size is more constrained by the number of clusters and that's not going to be an issue for any heterogeneity that is at the individual levels because you can you can you can split your sample within clusters what i'm doing which might be even more useful for the rest of us is to try to convince victor to think about whether one can do better than splitting the sample in two whether there is a way of using all of the sample more effectively and i'll show you some idea even here what we are doing is that we are cutting it in two many many times so for inference if we're getting into estimate b of z and the s of z in one part of the sample and then running say the best linear prediction or the getting one half of the in the other half of the sample and then we are doing this again so we are doing this like 100 times for example and the estimate that we are getting at the end is the average over all of these replications and the standard standard error is also given by what happens in all these complications so you get so your standard there accurately reflect the process of sample splitting plus the existing heterogeneity the existing noise that exists in the real world but one can do even a little bit better say by and by by not splitting in half by splitting in less than half and i think we can improve there because a lot of the machine learning methods they are not particularly concerned about data sample size because that's not really they can just run the regression another time and they have another 10 000 people and and life is good so part of the reason why it is sort of useful for us applied people to talk to the economic question is that my experience with my local economic relations anyway is that they are delighted to find problems to work with that are of interest to people like us which is what in a sense started this collaboration with with victor and [Music] so if you if you have something that you wish to be done so for example i wish to have machine learning methods that do not require to split the sample in two this is these are problems that can be submitted i don't know if they can be solved but they can be submitted uh anyway but it is correct that uh i think the main this sample splitting is the main source of the problem and it it can be uh worse than you think uh it can it's not necessarily as bad as you think because of the cluster idea we talked a little bit about the the source of the noise so the source of the noise there are only two two two sources for the noise for these various target parameters one is the regular thing the estimation of uncertainty regarding the parameter theta conditional on the data split and the other is that you split so we develop confidence interval that take both sources into account and the way that we are doing it is by repeated repeated sample splitting um so one example to show you some some numb some from how it looks like we looked at heterogeneity in the morocco microcredit data that some of you might have seen so the the setup was 160 villages in rural area of morocco which were divided into 81 pairs and then one of one treatment one control valid village per pair were randomly assigned within each pair in treated villages you the microfinance organization came in uh and then the data was it started in 2006 and the outcome were done in were measured in 2009. we have now one more wave where we are continuing this continuing this work so we stratified saying you could stratify your sample split by by cluster or but certainly if you have a stratified group you want to stratify by by straighter to keep to keep a member of all the straighter in so how does it work so you you for each iteration so here we do about 100 iteration you split the sample into the main and the auxiliary group you tune and train the machine learning group in one sample and get and get the b and s the baseline heterogeneity and your kind of rough estimate of the heterogeneity as function of disease and then you run in the main sample the best linear prediction which is regressed on the s d then regress on the uh treatment minus probability propensity of being treated and regressing the interaction of treatment minus propensity of treated this is a weighted regression where the weights are one over p one times one minus p similarly you can do the the uh group average treatment effect and similarly you can do your classification analysis by looking which characteristics i've looked at this is not a huge sample so i think it gives us a good idea of sort of the regular regular one of the meal cities we might have these are two of the two you could try that for any of the machine learning methods you like you can then even pick the one that performs the best so this is the two of them elastic net and random forest and you get your average treatment effect and the heterogeneity remember that if we can project zero it means that there is some heterogeneity so here we are finding some real heterogeneity it's not just we know that there is really heterogeneity in in loans in outputs in profit but not in consumption so that's the first answer is that oh there is actually a real heterogeneity that depends on sum of disease so now we can look at how uh sort of look at treatment effect by most affected and least affected people so here we've done five groups this is the average treatment effect for for the sample and these are these are the treatment effect by groups ranked uh from the least affected to the to the most affected and you find that it's only the most affected group uh that has an effect that is significantly different from the from the mean for the from the average treatment effect and it's the two most affected groups that have a treatment effect above zero so this you've done that for for loans you can do it similarly for outputs and for profit for consumption we had no it not much heterogeneity in the linear and you can see that it's also the case that all of these look like they overlap a lot and finally you know that you have your most affected and least affected group you can try and say something about uh so these are the you can try and say something about the characteristic of the people in this group uh what is different about them simply by regressing the characteristics that predict that you are in this least affected group versus the most affected group and here what comes out is that uh it's the the younger people who are more likely to borrow and also have higher profits and perhaps not surprisingly it's the people who already had some sort of non-agricultural employment that are more likely to go and the people who didn't have who didn't have loans although that's not that the difference here is not significant so these are the two things that are coming out as predicting the heterogeneity in this case there is probably nothing to write home about but you know we notice that there is a heterogeneity and i guess it's the younger people who are more into taking this up no they don't have they don't have to come up to be the same just happens it's also the case that we don't have a yeah we don't have an enormous amount to uh to the baseline was done not an enormous amount of characteristics so these these are in this particular case the list of characteristics per se is not enormous but no they don't have to be the same it's whatever and here we put on the we put the three same systematically on the on the graph even when they didn't when they didn't come up anyways so far so that's for microfinance uh so now i want to spend the last uh 20 minutes if you bear with me talking about something that's not even development but it's a it's an application that is perhaps more substantive than so i think what natalia is going to talk about later today is kind of an interesting substantive application of heterogeneity here i just wanted to demonstrate that in capital and there is some heterogeneity in in micro credit where we are taking it that might become interesting is that in the long run what we are seeing is that once we know that there is heterogeneity in in the take-up of the loan then you have a set of people that you can predict is absolutely unlikely to take loans very very unlikely to take loans based on the observables so then you can look at that group since there was no once micro credit was available it was available for everyone so you you don't have anybody who is excluded but if you have a group that's based on observable we can say has never taken loan ever then they can be used to look at external effects on other people market level effects etc which of course in micro credit organizations are interesting so we can know whether this is for them a positive things that there is my confidence or are not for people who don't know so that's where we are taking the that's where it might also become substantively interesting in this case the last thing that i want to talk about is this this application which is from france which is not a developing country which is doing very well but is uh particularly well these days um but is uh something that is a question which i think is uh of uh comes up very often at least in in japan for example it comes up very often is uh you're interested in comparing two interventions and when you're interested in comparing two interventions that have the same objective two interventions that have the same objective of course the best situation is when you have uh run two treatment in the same population at the same time so then you are you're definitely in a very good situation but it is often the case that we want to compare across papers so you want to compare the effect of textbooks to the effect of remedial education and textbooks were done in kenya normal education was done in india so uh there there are differences across the programs and there are also differences across the population and it's of course the the reason it's an active literature to explore to explore how the results change when you have similar intervention in in different contexts or different interventions in the same context and then sometimes you're in this position where you have different in their intervention in different contexts so here we were in one of this situation and basically our objective is to know well we have a slightly different population can we predict what would have been the effect of intervention a on the sample that end up getting intervention b so that we can compare apple to apple in the most fair way possible so that's of course another way in which this heterogeneity analysis might be might come extremely handy so i'm going to walk to the steps to walk you to the steps in this case of the the french program but uh which but this is something which again might be i think very interesting as more and more of our data is not only uranium regression but the data becomes available then people can get it put it together and start estimating a sort of 21st century rocket blender type decomposition of this is how the effect this is the reason why the effect varied in part it's because of the makeup of the population in part it's because of the treatment effect being different so that's sort of one way in which this could be this could be developed that would be very interesting so here we try to do it uh but with a relatively small sample and not as much details we want so to some extent we had to clog it a little bit so i'll do it step by step and explain how we how we did it so we are comparing two programs one uh is launched by a micro credit agency called adi and it was called creation and the other was launched by the social service and it's called groupman creator and both intervention target young people who are out of school and out of work usually in difficult neighborhoods either suburbs of paris or a pretty distressed places throughout france and also in french territories overseas and they target some of these youth who are not in a good labor market position usually have a bad [Music] checkered school history etc but they have a self-employment project so for example they want to build a beauty parlor or they want to build a gym they have this idea that they can have a self-employment project so both of these programs creation and groupon father target these kids and their objective of course is to put them back uh working but there are some different two important difference one is the ade program is selective in the sense that out of this young people they will try and take the most enthusiastic and the one who they act more or less together so they actually have a real project for example they want to start a beauty parlor or not uh something that makes no sense that i don't have i don't know i'm trying to think of something and then i think well actually it could make sense not something that makes no sense so they have some ideas for example a pharmacy i met a girl who had not finished uh high school and who wanted to start a pharmacy so it was not going to happen so in the adi program she would have been kicked out of the program because she doesn't have a a like a live project and in the groupman creator program she would be taken so that's the first difference and the second difference is that and related in a way is that adi really emphasizes the self-employment project and what they're trying to do is to give them the technical skills to succeed so you want to start your beauty parlor this is how you do a business plan this is where you get a loan this is how you get your auto entrepreneur registration complete et cetera et cetera and they use retired small businessman to help them the government creator starts from the project and then uses that as a part of a conversation to try and see you know what makes sense for that kid which could be self-employment or in fact often could be to just get a job so that's kind of the difference between the programs okay so we estimated so the evaluations were done almost concurrently uh what we found is that the the effect of the id program across outcome the id program just has no no effect uh the the the valid test for all of these key measure is zero it really has no effect whereas the groupman creator program uh tend to have very positive effects uh for employment it's not quite significant but for uh long-term unemployment employed for a wage it's actually quite significant i'll come back to that there are more there are more wages and they get they get less welfare payment and so overall the groupman creator program is seem to be effective interestingly the the effect they're asking less likely significantly less likely to be self-employed than in the control group so you can see how they start from the self-employment project and then gently put them towards something that's more more sensible so so when we got this like the first paper i wanted to write was that clearly these group mantra people get it right they start from the project and then they move to what makes sense they don't push people into self-employment people wanted to ride and then we realized that this was not going to be you couldn't quite write that paper because the id and groupman creator populations look different so i haven't put all the the tables of the paper in the presentation but the first thing in which they look different they observably they are different the idea kids are a little bit older they are a little bit more likely to have already a self-employment experience they are less like more likely to come from mainland france and less likely to come from the overseas territory etc so the first thing that i thought we could do is let's match by this observable characteristic so we match by the observable characteristic and it doesn't make much difference the groupman creator program estimated on the observable character these few observable characteristics that we have in common still looks significant and the id program if you estimate it on the gc observable population still looks insignificant okay but then we realized that even after you do that the population still don't look very similar and how do we know that is that because we can look at the control groups of both population and what happened when you do that is that you realize even after you fully match for this few observables that we have in common the control group in addie is just much more likely to have found some job anyways so 68 percent of them become employed after about two years which is when we have our end lines and 44 percent of the group and creator program become employed so the truth that you find in also of course in all all sorts of social programs in the u.s as well as that you know kids get employed anyways and here they get employed they're more likely to get employed in adi program so and this was true even after controlling for for this difference in employment it was true even after controlling this for these few observable characteristics and here where we had where we have a problem is that the id program selects on the basis of an interview that we don't observe who they want to keep so presumably they select people who they think are going to do well in their program which probably correlates very strongly with doing well anyways so they pick those guys and unfortunately we don't have very rich baseline variable for the id program so we not only we don't know on what basis they select but we don't even have a way to reproduce it what we do have is we have very very very very very detailed characteristics of the people who participated in the groupman creator program so for them we have tons of psychological variables how they feel their views on life their optimism et cetera et cetera we have lots of those so we have lots of those for the equipment creator program and of course because a groupon creator program does not select we have some of we can see how uh whether the kids who are some kids are doing better than others in in this program so the question we want to ask them that we are that we can ask we cannot ask the opposite question we can ask the what would have been the program the effect of adi if they took everyone because they didn't take everyone in the first place but what we can look at is what would the effect of the groupman crater program be if they had only picked people who looked like they would be like the adiselexis guys so how do we do that we try to predict in the control group the probability of finding a job anywhere based on the on the baseline characteristics so we estimate the probability to be employed without treatment in the control group we look at whether there is heterogeneous heterogeneity in treatment effect according to this according to this probability of being employed and then we are going to uh try and estimate the so i'm going to to show you that in fact there is a heterogeneity in the effect of the program according to your predicted probability of being employed anyways and then we estimate the uh group mankrator program in a population that is made to look like observably similar to the id population and how do i do that i pick them i sample them to have an employment problem a predicted employment probability similar to what our control group people have in the id program so that's kind of the game plan of this paper uh and the so the way that it uh the way that you you do it so the first thing we need to do is this first step estimating the heterogeneous component which is estimating the probability of being of being re-employed we suffer here because we have 300 observations so first of all we can't do the very complex machine learning method forget it it's like we have of course have more coverage than observation since we have so few observations in the first place um so what we do is that we we do cross folding which is again which is like this many many times partitioning and taking the taking the average of the two to try and use our data set the most effectively while avoiding the why avoiding the overfitting and we estimate uh we estimate a lasso because that's something that is not too too demanding so we estimate that something that can be done on a reasonable sample size so we estimate so we every time we we randomly partition the sample intent split for a given split select our predictors run the logit model do that 300 times and then for each observation so each person we take the median of the 300 prediction produced for this observation that gives you your predicted probability to be employed if you don't if you don't if you don't have the program in the treatment and in the control then we estimate a specification very similar to the one i showed you before with the s of these but since instead of using something completely unstructured i'm using i'm using this predicted employment product probability and following the lessons as before i am orthogonalizing properly to avoid getting bias and i'm using a weighted regression weighted by the by a one over um pz times one one minus pz and as before in the previous paper rejecting the hypothesis means that there is no relevant eternity according to predicted employment probability so you see it's like here in this case i cannot afford to try all of the heterogeneity in the world i'm going to put some structure and but the structure is still kind of flexible but i'm trying to make all uh fit in via this predicted employment probability i'm passing a bit on the day but believe me that i do not i think that we did things properly enough that there is no not a chance of overpredicting the probability of employment in the control which would of course be unfair that if we predict if we pick people who are very likely to get employed and of course they're more likely to be employed so i think we did that properly so the bottom line is in this first and this first rule here is that the probability to be employed the effect of the gc gc program on average on the probability to be employed is point 18 but the the effect becomes smaller and smaller for people who are much less likely to be employed in the first place so this is what hackman calls crime scheming he pointed i think he had in mind more that the screaming would go the other way which is the people that are crimp scheme would be the ones who would be more likely to benefit from the program and of course it's going to depend from program to program in some cases the selection is going to be uh is going to pick people who would benefit the most from the program in some cases like here it picks people who would have gotten the job anywhere they are you know enterprising they have a project there is many other people who would give them a job or are alone so so intuitively it might be that it explains to some extent the difference in the results one way to see it graphically is to look at imaging different groups which select different leader people and here and so they apply different threshold to let their people come in so each orange dot shows the treatment effect if you pick the 20 best for example in this one the 20 percent best kid according to their probability to be employed so you see that you get no treatment effect on employment and if you pick the um 20 percent worse kid least likely to be employed that's where you have the largest treatment effect so that's the equivalent of the group by group that i was talking before so the the difference betw so the effect of the groupman creator happens to be largest for people who have a quite low probability of employment anywhere so at least this program of course we don't know for a deep person because we haven't done it but at least this program was particularly effective for people who in the absence of it would have done extremely poor okay so in itself it's interesting and yeah yeah yeah so we also have weights uh and so for the so the eternity was all done for employment so we also have done labor income wages all the eternity is based on employment so you can see that it's all negative and if you put them all together jointly they want to be negative but the wage itself uh the effect is the the the effect is really noisy it's a chance to be to chance to be employed you so your effect on your probability of employment at end line yeah yeah so no no that is true yeah it is true but that's so that's uh if if it's not necessarily mechanical because even in the id control group it's only 60 percent of the kids who are who are who are employed at the end and the treatment effect for nobody is 40 percent so we don't reach 100 percent but yes in fact it's mechanical although the id people would say no but there is no point it's because i put myself 125. uh so soon your five minute thing is [Laughter] um so the the the idea people would tell you but there is no point working with people who don't have their act together whatsoever so one could imagine uh it going the other way yeah um so last and this is actually my my last uh last graph and last before last slide is that then the last exercise we do is to say well now let's pick a weight to uh create in the gc population a gc population that looks observably like the id population in terms of the predicted probability of employment so the first thing we did is this which is sort of a naive way to do it which is let's take this is the true distribution of probability of employment in gc and these are the weights the first thing we did is we we said let's pick the best let's pick people the best the predict the best people in the gc group best as in most likely to get a job anywhere until we find on average the same predict predicted probability of employment than the id control group and this is going to be a similar population that's the first thing we did and then we realized that it's a little bit naive because then you kind of throw away all this data but it could be that some of them would have been picked and then it turned out that there is a whole literature in some in something uh to create a population that basically sample from a population a sub-population constrained by whatever it is that you're interested to constrain so having discovered this literature we did something that we we picked so there are various ways to do that they have different ideas and one possibility is the paper behind mueller is the idea that you pick uh you pick people you pick weights such that the uh you you on average you you have this constraint that on average the the predicted possibility of employment is going to be what you want so in this case it's 68 percent uh and so people who are more likely to be employed are more likely to be uh picked but everybody has some chance to be picked these are actually our weights and it for it follows a logit basically so it represents the population and satisfy the minimization minimization criteria on noise so we did that to look at to what extent are the difference between the program explained by that so there unfortunately is where we are hit a little bit by our very very poor sample side which is basically the result of all of this effort at the end for this last graph is a little bit of a bunch of noise which is it could explain but it's too noisy to tell uh which is you have the this is the for employed uh the the effect for groupman creator the effect for id is actually a negative point estimate and then the effect for the high molar weight is zero so it's not negative anymore but all of these are very large standard error and for wage employment we have a significant effect of group man creator negative effect of id but insignificant you saw that and here the weights are doing almost almost nothing so here at the end of the day i think we are pushing uh the data a little bit our 394 observation a little bit further than they would like to go but i still sewed the table because that kind of finalized the ghostly end of this particular journey i i do think that the process and the the steps until this very last table we had the results that were sensible and the steps could be useful in in other similar applications so i'll put that back again the most important slide of this talk is here i hope you you know you find something useful your student found something useful uh we are very very happy if this is being uh used and abused and misused and everything thank you very much [Applause] 