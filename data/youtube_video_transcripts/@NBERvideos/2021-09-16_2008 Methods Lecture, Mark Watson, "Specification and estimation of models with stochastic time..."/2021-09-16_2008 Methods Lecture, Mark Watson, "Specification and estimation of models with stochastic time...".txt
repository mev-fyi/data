Mark Watson: I want
to talk about. Get our fun. These are fun. I mean, they're just weird. Not weird they're weird. There's estimating and doing
inference about break dates. You got a regression,
it got a break in it and you want to estimate
when the break was. For this, we have
to suppose that this break thing is a
good way to model it and then we're going to
estimate the time of the break and that's
as of course, estimating anything is easy. But doing inference
about it is harder. You can estimate something
by just flipping a coin, but doing inference
is a lot harder so we'll talk about that. This is of course, Duchamp buys work and buy and [inaudible] and all that stuff. Papers that you've seen or you know the authors
and stuff like that. I want to talk about that. The goal of this again is I'm going to work
through this in the context of a really simple example. The hope is so that you can
see what the key trick is. I think if you see
the key trick, you'll understand when it might work and when
it might not work. Then I'll show you
some empirical stuff. Well, we'll see whether
it worked or not. Then I'll talk a little bit
about Markov Switching Models and two versions of those, some recurrent Markov
Switching Models and non-recurrent Markov
Switching Models. We'll look at those and there's one version of
this that I really like. I want to show you this because maybe
you haven't seen it. Then I'm going to go back
to these models with Martingale time variation because they're really
good tracking models and they make sense
for forecasting. We talked about estimating
the path, if you will, in this filtering stuff or
signal extraction stuff, but we haven't talked about
estimating parameters so I'll talk a little
bit about that. I'll talk about maximum
likelihood estimates of some key parameters there and there are a couple
of problems that show up. One problem is like a unit MA, root thing and
I'll show you that and maybe draw a picture that
maybe Jim drew yesterday, but I'm not sure. You have to tell me if he did, I'll use a different color, so it will appear to be new. I'll talk a little bit about data augmentation as a method. This EM method is
old idea applied here to these models which
makes life very easy. I'll talk a little bit about time viewing parameters
as nuisance parameters. Suppose you've got models where you've got two
sets of parameters, you've got some
time-varying guys and you've got some other guys and you get some parameters
that aren't time-varying. You only care about the guys
that aren't time-varying, but you've got these
time-varying things and you have to worry about the time-varying things if
you don't care about them, except to the extent
that they mess up your inference about the
non-time varying guys. Of course the answer
is going to be here is if it's not too time-varying, it's not too important
but you want to understand what not
too time-varying is and not too important is. That's what this will be. Then I actually not going
to talk about this. I'll talk about this tomorrow. It shouldn't be in this slide. The first set of
things we're going to talk about are our breaks. Think about your favorite model, maybe a linear regression model or something else that
has a break in it. Again, to make the notation
as easy as possible, I'm going to be
dealing with scalars and no x's in the regression because those
needlessly complicate things for the points
that I want to make. Y_t is a constant,
Beta plus Epsilon, and this constant undergoes
a break at date Tau. The object of interest
now is not going to be Beta or Delta. It's going to be when
did the break occur. There has been a
volatility decline in the macroeconomy
when did it occur and we model it like this, so think of these as
variances or something. There has been a productivity
slowdown or increase. When did that occur? We're going to assume it
occurred at some date, so there's a break and so
we're going to date this. It's going to be important later on to talk
about a break date. That's the date at
which it occurred. You've got a sample of size 200. The break occurred at
observation 68 so that's Tau. The other thing is, we're going to talk
about the break date in terms of the fraction of
the sample at which occurred. Observation 68 divided by 200, I guess that's 0.34 so the break occurred 34 percent of the
way through my sample. I will probably
needlessly, excuse me, bounce back and forth between talking about
estimating Tau and Pi, break date and break
fraction of course, if you know the sample
size it's the same thing. We're going to think
about estimating these by Gaussian maximum likelihood. These break dates were
either break fractions and of course that's
just least squares. We're going to estimate
these guys by least squares. Key references here, the paper that I liked when
I started thinking about this is this RE stat
paper by Jushan Bai. He's got some stuff earlier, but this I found are very clear. The algebra is just
tedious as can be. That's not Jushan fault. It's the subject matter. Then there's a
really nice paper. Guys probably you saw
this several years ago. The Journal of
Economic Perspectives did the special
issue in which they had econometric articles in which you can have no more
than three equations. I wrote one of their gym stock and I wrote one of these
and which I think really put in one equation on
vector autoregression. Bruce Hansen wrote a nice
one on structural change so that's really worth reading. Later on when I talk about
implementing this stuff. At least in the context of that, Bruce put together some software which is probably
still on his website in which you could do
this stuff really easily. I found that
software to be used. It's like Gauss stuff and
it's useful to look at because it helps to try and figure when you're
doing algebra, it's nice to have someone who's written a computer program too, because then you can
look at the algebra and look at the computer program and see if you made any
mistakes in your algebra. In this case I found Bruce's program's
really useful for that. I'm going to try and
skip most of the algebra that is in Jushan paper. Yet, I'm going to
try and work out the methods for doing
inference about break dates. There's many complicated
calculations that Jushan does. One of them is this, and I'm not going to do it. I'll tell you why
it has to be true, but here it is. Pi-hat is the estimated
break fraction, Delta is how big the break was. Then what Jushan
shows is Pi-hat, the fraction converges
to Pi_naught, the true break date. This is my estimated
break fraction. That's the true fraction
and the speed at which it converges depends
on a couple of things. It depends on how
much data you have, that's familiar, and it also depends on
how big the break was. Let's just go down. Let's take this and remember
the break fraction is the break date divided
by the sample size. Sample size times brake
fraction is break date. This just says that if I look at my estimated break date
minus the true break date, scale it by how
big the break was, I get a non-degenerate
random variable, a O_P of one random variable. This guy is going to have some
probability distribution. It's not going to
collapse to zero. This says, well, you can't consistently
estimate the break date. Now, if you think about this
for a couple of seconds, you should say, well,
that makes sense. The precision of your
estimated break date is going to depend on
how big Delta is. It turns out this probability
distribution here, in general, is going
to depend critically on what the distribution
of those errors is. In general here
you're stuck, I mean, if you know your
errors happen to have some distribution and you know your break
date is about this size, then you can think about
doing inference here, but we never know any of that. What we try and do is think
about procedures that might work for a wide variety of
probability distributions, and we try and use averages
and things like that, and you central limit arguments to come up with robust idea. Let me just draw a picture to show you what's
going on here. Again, this is obvious, but I think it's worth drawing. Let me draw a picture. I guess I probably
want t down here, t and here's the data. I'm going to plot the data. Here's what the data look like. You can't see this in the back. Sorry. Just listen.
It looks like that. Where did the break occur? Well, it occurred around there. I'm just going to say this
is all just makes it. Is this a pre-break observation or a post-break observation? Well, it might be hard to tell. If I think about what
I'm going back here, it depends on what the
behavior of Epsilon is. If Epsilon is got some
weird distribution with, I don't know, a bunch
of mass out here, there's going be some
big positive ones, well, this guy looks like it's
in the second regime, but it was really
just a draw from that little hump
similarly for this. You're not going to be
able to really pin down, do inference about
this break date, form confidence intervals
about this break date, unless you really know
what's going on here. That's one point to make
here, and that's important. The other point to make here
is just the obvious point. The precision of your
estimated break date, if you will, is going to
depend on how big it is. Well, if I draw these points
and put this up here, the break was bigger, it'd be easier to see
where the break is. If the brake date gets
smaller and smaller, it gets harder and harder to see where the break date is. That's Point 1. Point 2 is, well, people do do inference about break dates, they do construct
confidence intervals using some schemes, so I want to talk to you
about what those schemes are. I guess we know that, in general, those
schemes can't work, because the brake date inference is going to depend on the
distribution of the Epsilon. What some guys do
is this, they say, let's imagine a
situation in which sometimes the brakes
aren't really big. This thing you know
a break occurred. You just look at the
data and you say, G, a break occurred. Often the data
sets that you see, break probably occurred, but I can't be absolutely sure, I can't reject no break
with probability 1, the break isn't big enough
to have power 1 in a test, I might only have power
of 0.8 or 0.9 or 0.5. Let's imagine, so the game that we're
going to play here, the bi-game, is
to imagine breaks that aren't nearly
this dramatic. Think about breaks
that are small. Then you end up
with dots that look like this and I'm not
good at drawing dots, and then dots that
look like that, and now where did
that break occur? Well, what do you do? Well, you average these guys, and then you average these
guys and you say, well, this average is a little
different than that average, so maybe it occurred here, but maybe I should've let
this line go a little further and starting this
line a little later. The key thing now is
the game we're playing here is we're playing
a game with averages. Averages are your
friend in econometrics because average is say you're going to
use central limit. Averages are
approximately normal. You can think about using
central limit theorem and think about doing inference which is robust to particular
probability distributions because you're going to
average out these humps and you're getting
the normal stuff. That's the bi-game. Yeah. MALE_1: Just to be clear about sometimes you talked about
[inaudible] yesterday, there's the fundamental
difference here between when it happen that
suppose to [inaudible]. Mark Watson: Exactly, yeah. Now it's going to happen. I'm not testing did it happen, I want to know when it happened. I want to construct a confidence interval
for the break date, not the size of the break. Yesterday we were interested
in confidence interval, so size of the break, and asking whether zero is in there. If zero is in, if Delta is equal to 0 in
the confidence interval, then the fact that
a break didn't occur is consistent
with the data. I lost my little thing here. Let me just go through
here, blah, blah, blah. I got it. We're going to think
about a situation in which this thing
goes to infinity. This guy is big, so this is like our
normal scaling factor of the difference between
an estimator and the truth. We're going to have
this guy getting big and we're going to try
and think about what the probability
distribution of this guy is generically by
using some averages. This guy is going to be big, but Delta has got to be small or else we run into
this other problem. T times Delta squared is big, but Delta is small. Delta has to be small, but not too small
or it would kill T. This is a big number,
that's a small number. That's what this says. Blubber T Delta squared is
big but Delta is small. For our approximation, we do calculations to
get approximations, I'm going to think
about Delta getting close to zero as T gets large, but T times Delta
square getting big. So that's the approximation
that we're going to use. That's the, if you will,
nesting we're going to use. Here is what we're going to do. We're going to consider again
least squares estimators. What are least
squares estimators? Here is the sum of
squared residuals. If I assume the
break date is Tau, the sum of squared residuals
is y_T minus Beta, that's what's going on
in the first regime. That's the sum of
squared Epsilons between time Period 1
and time period Tau. The sum of squared residuals
between time period Tau plus 1 and T is this and the difference
is that Betas change or the mean has changed from
Beta to Beta plus Delta. So that's the sum of
squared residuals, is a function of Tau. Now what I'm going to do is
I'm going to change Tau. Make Tau bigger and smaller to see how
this things changes and then choose the Tau that makes this guy
as small as possible. That's the least
squares estimator. What we're going
to do is to study the behavior of this estimator, this minimizer of this function. What we're going to do is we're going to look
at this function and see how this function
behaves in the limit. We're going to look at a limiting
version of this function and then think about our Tau, which is the minimizer
of this thing, as behaving like the minimizer
of the limiting function. The minimizer of the limiting
function is going to be nice and nice. That's the game we're playing. Here he is. I added some more steps
to the algebra here because I had this line
and two of these lines and Jim said add more
steps, so I did. Let's just look at this. This is beautiful, by the way. I'm going to do all of this. Beta and Delta aren't the
issue here, it's Tau. I'm going to do all
the calculations assuming we know Beta and Delta. That's another difference
between what I'm doing and what Jushan does. Jushan does it right, but that leads to a
lot more algebra. I'm just going to
assume Beta and Delta, so we're only going
to think about Tau. Now let's think about
this objective function. I'm going to think about
this objective function on one side of the
true value of Tau. Suppose we looked at
this objective function for some arbitrary Tau, which is too big. This is the true break date that I'm looking after
the true break date. How does this objective
function behave? Let's just go through the steps. For those of you in the
back, get your binoculars. I used to carry a
little, anyway. This is w at it is. Here is my sum of
squares function that I had before as
a function of Tau. Now Tau is bigger
than Tau_naught. I'm going to break this up. This guy has T goes
from 1 to Tau, that's T goes from
1 to Tau_naught. But it also has in it, I guess, T goes from Tau_naught up to T and I have y minus Beta squared. I wrote that as y minus Beta
minus Delta plus Delta. This guy and this
guy is that guy. That's my extra
line of algebra to make this completely
transparent to you. This is from T goes
from Tau plus 1 to T. We'll now look at this. What's this? This is T goes from
1 to Tau_naught, that's just Epsilon. That's Epsilon T squared. This is T goes from
Tau plus 1 to T. Tau_naught is less than Tau, so these are just Epsilons. I deviated this from
the right mean. These are all Epsilons. Here I messed up
because I looked at y minus Beta and I should be looking at y minus
Beta minus Delta. This is really
Epsilon T plus Delta. From T goes from Tau
plus 1 up to Tau. That's what this is. Maybe this is another line of algebra I added to make life. Now I'll just do the
arithmetic and I get what? I get the sum of squared
Epsilons from t goes from 1 to T and then
the interesting part, is this part is the Delta
squared part of this and then this cross-product. All I did now is that
this next line, again, because I'm schizophrenic
about using Tau or Pi, I just wrote this in terms
of Pi and Pi times T is Tau. Now what we want to do is take this function and minimize it over Pi or Tau, same thing. This doesn't matter. This is just sum of
squared Epsilons. Let's think about
minimizing this guy. Again, we're thinking
about the case in which Tau is bigger
than Tau_naught. I'll do the case in
which Tau is less than Tau_naught separately and then we'll put
them together. This is only one
side of the problem. Let's look at this part of
the objective function. This doesn't matter for
the objective function. Let's look at this part of
the objective function. I just repeated that part
of the objective function. I got rid of the sum
of squared Epsilons. This bit right here, let
me call this something. I'm going to give that a name. I'm going to call it Nu or
whatever that thing is. It looks like I've divided
it by Sigma squared and that's just
because I want these in standard deviation units. So I just divided
it by Sigma squared because it makes something
nice in a few minutes. Now I'm going to
rewrite this guy. Let me rewrite this guy in
terms of this variable Nu. What's this? This is just Nu. If you will, dividing by Sigma square root
is just scale this. It's not going to
change the minimizer. This is Nu and then I
have two times Delta but, I divided by Sigma squared. So I really have
Delta over Epsilon, and then I have these Epsilons
divided by Sigma Epsilon. These guys are going
to have variance one. That's why I did this. Now we're going to
look at this guy. This is great. This is an objective function. Minimizing this with
respect to Tau is the same as minimizing
with respect to Pi. Pi and Nu are related like this. So this is the same as minimizing this
with respect to Nu. This is the function
that I want to minimize. I want to look at the
argument of this. That's going to help me think about inference
in this case. Now let's look at this. This is looking good. What's this? This is an average and
averages are our friends. Because we're going to
have normality going on. I guess when I apply a
central limit theorem, a central limit
theorem can be applied when I have something
like I add up Epsilons over a whole
bunch of observations, the sample size, and I divide by the square
root of the sample size. We have to look at this and I want to re-interpret
this as the sum as T goes from one
to sample size divided by one over the
square root of sample size. I just have to tell you what
do I mean by sample size. That's what this slide is. Remember Delta is going to zero. This is nice. Delta's going to zero, so that must mean 1
over Delta squared, for example, is
going to infinity. What do we have up here? I'm really averaging this over. How many observations? T goes from 1 to, I guess a whole bunch, 1 over Delta squared times Nu. Then what do I have here? I have Delta, what's Delta? If 1 over Delta squared
is like sample size, Delta is 1 over square
root of sample size. This is just a
familiar calculation. This stuff right here is
going to converge to, well, I got two. There's the two and then
this guy is going to be, if you will, our normal. This is 1 over square
root of sample size. This is sample
size but times Nu. That average bit is
going to behave like this Wiener process
evaluated at Nu. Then what's the
front of this is Nu. This function is going to
behave like this function. Let me draw it.
How do I draw it? It looks like this. Here's zero. I'm going to draw it. Probably not well. Here's g of this thing Nu. I'm going to draw
this for Nu positive. Remember I assumed Tau is
bigger than Tau_naught, Pi is bigger than Pi_naught, Nu's got to be positive. Let me draw this function. I'm going to draw this
function in two bits. Let me draw this part
of the function. Forty five-degree
line, that's Nu. Now what am I going
to add to it. I need to add to what. This Wiener process. What's this to what
I have to add to it? I have to add to it to
something that looks like that. That's this Wiener. I take this, I add it to this. It looks like
something like that. That's one side of the function. You really can't see
the black thing. You can only see that. Now, what would I do? We just did one side of this. Let me do the other side. The other side is going
to look just like this. The other side is going to have a 45-degree line
going like this. Another Wiener process which is going to be
independent of this guy because it's going to have other Epsilons going backwards. There's going to be
another guy here. That's going to be around
some 45-degree line. I could go like that or
something like that. This is what at the end of
the day, what do we do one? At the end of the day, our
estimate of Nu is well, the value of Nu that makes this function as
small as possible. For this particular realization
of the Wiener process, I guess my estimate
of Nu is right here. That's Nu-hat. Then I convert that
into an estimate of Pi and an estimate of Tau. How can we think about this estimated break
date in this situation? We think about this
estimated break date as the minimizer of the sum of
squared residuals function. But appropriately thought about the sum of squared
residuals process can be represented like this, 245-degree lines, that's the deterministic bit, that are minimized at
the true break date, perturb by noise, and the noise turns out to be this Wiener process
on both sides. If we want to think about what's the probability distribution
of Tau-hat or Pi-hat, our Nu-hat, I ask, how does the minimizer of a
function like this behave? That is actually a
well-studied problem. References in by, here's the function written out. Now I'm going to show you the quantiles of this function. Let's ask, what's
the probability that the absolute value? The true value of
Nu here is zero. Because it's Pi-hat
minus Pi or whatever. What's the probability that Nu-hat is less than
some constant c? Let's find the c that
makes this 67 percent the usual quantiles that we
might be interested in if we were constructing
a confidence interval or something. Here they are. I put them in and
the only reason. I put them in is because I wanted to compare
them to this. The normal things for a standard normal plus or minus 1 gives us a 67 percent
confidence interval. Plus or minus 2 gives us a 95 percent
confidence interval. Now plus or minus 4 gives us a 67 percent
confidence interval. Plus or minus 13 gives us a 95 percent
confidence interval. This guy really has tails. As you go from a 67 percent
confidence interval to 95 percent
confidence interval, the width of the confidence
interval increases by a factor of 3 instead of 2. This has some, if you will,
non-standard distribution, but now you understand it. Now we're done. Because now we think about constructing a
confidence interval. What would ingredients or
the confidence interval be? We've got to construct
this Nu-hat thing. What's the ingredients of
the confidence interval? For a break date, I guess I need to
know the sample size. Sigma Epsilon remember
divided by that Delta and my estimated break
date fraction or date. From the table we know right
here is a Quantile 4.4 that is that number 67 percent. I know the probability of
that Nu-hats less than 4.4 is 67 percent confidence interval. If I wanted a 95 percent
confidence interval, instead of using 4.4,
I'd put in 13.8. Here's my Nu-hat, if you will. This is just that. Stick it in. Solve it you get pie in
the middle as usual. Multiply through by T, so it's in terms of
Tau and you get this. This is a 67 percent
confidence interval for Tau based on Tau-hat and the other
estimated parameters. Again, if I wanted to 95
percent confidence interval, it'd be three times as wide. Programs I said Bruce
Hansen's webpage. In their very terse and
that makes them good. Because you have to understand. I'm saying this to
myself because I did this the first time I
decided to program this, I was sure I didn't understand the algebra or the programming, so I sat with Jushan Bai Bruce Hansen, read algebra, looked at his code, and then when they coalesced, I thought that maybe
I understood it. That's useful for me. Let me show you an example in, and I want to be critical
of this example. It's in this stock Watson
macro annual paper. I thought I would
throw this in here. Then I wrote down the
numbers and anyway, let's just go through it. Blah, blah, blah. It's the same example
we looked at last time. When we looked at
last time, we said, what if I want to look at
stochastic volatility in GDP, let's estimate the
stochastic volatility G, wandered, G came down. Another model might be there was a break
in the volatility. Let's ask, if we take this break step function
things seriously, where did the break occur? As we saw before, it looked like it
occurred in early 1980s, but you do buy stuff. This paper does buy stuff. I guess the only wrinkle here is now we're talking about, I had a break date in a mean, I thought about a
break and this. Now I'm talking about
a break and variance. But, if I think about, if
you will, the error term in this equation is something
that I could observe, which I could if I could
estimate this stuff. Then a break date and the variance just says
take this residual error, square it, write the
expected value of the error term squared
is the variance. This is the error term squared. This is expected value
of the variance. This is the deviation
of the variance from its expected value. You can apply the same
mechanical stuff here. Now this is interesting. It looked as if when
we did testing here, it looked as if there was little change in the
mean coefficients. But it looked like there was strong evidence
of a break and variance. Of course, you could
see that in the data. You could see it in the data. It was clear there was
a break in variance. Now, and the estimated break
date, as it turned out, these were data from went
through 2,000 or something. Based on those data, the
estimator break date was like 19832 because
it's quarterly data. Go through the
mechanical procedure that I just outlined here, and construct a
confidence interval. It's here. It's 19,832 plus or minus. It should be symmetric. No, it doesn't have
to be symmetric. Why? This is interesting? I didn't decide and look
at this before I got here. Let's go back here. Come on. What did I have to do here? I had to divide by Sigma
Epsilon, to get this to work. Now, suppose there's a break. Suppose I interpret this as
variance error squared equals variance plus deviation of
error squared times variance. Suppose the variance falls. The variance of the
deviation of the square from the variance depends
on the variance. Because this guy got smaller, the variance of this
guy got smaller. That means that going back here, it turns out for this
particular example, this is cool that
the Sigma Epsilon that you have to divide by because of this variance thing. A decrease in
variance introduces heteroscedasticity here of a
certain of a certain sort. You divide by different Sigma, and that's what makes
this guy not symmetric. These lines go like
that or something. When you form this
confidence interval. I didn't realize that, but, I guess I realized it at some point when we
wrote this paper. It makes a lot of
sense, doesn't. But that was not the point
I was trying to make. The point I was
trying to make is this confidence interval
seems really narrow. What does that mean? I think it means that this can't be based
on much averaging. This must be based on data that at the end of the day looked
like in terms of variances. Oops, I'm sorry, just
the opposite of that. The squared errors were big and then the squared
error has got small. We're pretty sure when
this break occurred. If you're pretty sure
when the break occurred, then when you go
through this by stuff, it means you couldn't have
been doing much averaging. If you're not doing
much averaging, then really this
confidence interval, is probably a poor approximation because you didn't do a bunch
of ads of the averaging, right, that we were
trying to do here. This break for this
particular example probably is too big, for this by stuff, to make
me feel good about it. This is too tight. Do I really think this is a 67 percent
confidence interval? Probably not. Again, that didn't occur to me until I put these in the slides and looked at it this morning. I added this. You guys don't have
this in your things. What do you think about
this confidence interval? This is like an exam question. You put this on an exam and what would students do? What do I think about it? The paper is written by the
guy who wrote the exam. It's got to be good. You don't want to say,
well guys are bozo. Anyway, maybe I'll use this
in exam question next year. I have to TiVo and out of this. That's that. That's his bias though. Anyway, I really liked
this part of the lecture because this is really hard if you read Jushan's papers. This is really easy
if you do this. I feel like I earned
my salary today. Maybe anyway. Now, you can take these models and you could do some stuff. Instead of one break, you could put in 2,
or 3, or 4 or 5. There's interesting work by
Jushan and Pierre Perron I don't want to talk about
it except to say that if you've got a
sample of size 200 and you've got
five breaks in it. Thinking about these things is, maybe that's not
the right way to think about what happened. There's a paper by Jushan, Robin Lumsdaine and
James Stock which says, "Suppose you're looking not just at one series with a break but suppose you're looking
at 2, or 3, or 4 series and they all happen to
break at the same time like income and consumption and there's a change
in the income process. Or there's a
productivity slowdown across a bunch of countries, or increase of bronchi or
inflation comes down on a car, a great moderation of
bunch of countries. Then it's not too
surprising that if you've got multiple
observations on the break date you can do better and these confidence
intervals will get more narrow in proportion if you will to the number of
series that you're looking at. That's what this paper's about. Then you could put multiple
breaks and multiple series together and that's
what this paper about. Now, breaks in historical. The nice thing about the
problem with breaks is that it depends on why you're doing
your empirical analysis. If you're doing your empirical
analysis for forecasting or you're thinking
about the future, having some break in sample that you view as deterministic. You now can be problematic
because then you think about, well after the break
the world is stable. But if there was a break, maybe they'll be another break. But then how do you think
about another break? There's all those problems. I want to now move to models in which we think
about the brakes probabilistically
for those reasons. If you're just
interested in doing some historical analysis like this great moderation stuff it looks like the variance fell. When did it fall? Well then maybe something
like this make sense. Let's give a model with breaks in which
it's probabilistic. The well-known model. James Hamilton's model of
course is designed for this. It's been used by
a bunch of people and a bunch of
different contexts. Y has a mean and a variance? As I've written it here, both the mean and variance
depend on state. The state can be zero or one. The mean can take a one value or another value of
the variance carry one value or another value. In this model as you bounce back and forth
between these states. You could think about
doing estimation and stuff like that using
the filtering things that we talked about earlier. The little subtleties here that are just worth mentioning, is testing for Markov Switching in this model is a
little difficult. You want to say, is our things constant or is
they're switching. Way again, you have
this problem that if the null hypothesis is
true, there's no switching. One way to think about
that is the mean and variance in the two
regimes is the same. Well then you can't
possibly know what the probability is
switching from state to state because it
doesn't affect Y see you've got unidentified
parameters here. But this Andrew's Blowburger, approach where we didn't know the break date could
be applied here, so you could think
about doing that. You can do estimation of this model turns out
to be pretty easy. One easy way is via sorted
data augmentation or EM, and I'll talk about
that in a few minutes. There are lots of
extensions of this and you've seen as many
of them as I have. You can put in more states. You can make these piece
functions of stuff. There's like a million papers that change this in
interesting ways. What I want to note here
though is that one thing that I don't like
about this model or that might not be useful for some things that I think about any way is
the changes here recurrent. You go from recession
to expansion, for that, maybe it's okay. But for this volatility stuff, you went from high to low, or you're going to
go back to high. Are you going to go to lower? Do you really want to say I'm going to go back to where I was. If I change, I'm going
back to where I was. That's not me. You want to move forward. You don't want to
live in the past. This recurrent stuff is
literally living in the past. If you change it, go
back to have a long hair and wearing bell bottoms. That wasn't pretty. What you might want
to write as a model which says If you
change, you change. Maybe you back to the past them, maybe you go forward. I'll do you change. You can think about soup
in this model up like this and here's the model
that I really like that it's a version of that. I just want to
highlight this so that. I think it's inspired by a paper that I read
once by these guys. I actually refereed it
but don't tell him. But I really liked
it by these guys. This was the problem
that they with. What they did is they wrote down a Markov Switching thing. But there are going
to be k states where k might be a number they're going to estimate
because it turns out, and what you're going to do is, if you're in state 2, at date t, then you have two choices. At date t plus 1, you can stay in date 2 or you can move to state 3, you can't move back to state 1. If you're in state 3, you can move to state 4. You're always moving forward. You're always moving
to the next day. Then the question is, well, one thing that's
interesting is state 3 a lot like state 1
that's Hamilton. Our state 3
completely different. What they want to
do is characterize or have the model flexible
enough to estimate. Are the states really
different from one another are not? There are a couple of
ways you might do that. I wrote down a
couple of ways here. They actually do it a
little differently. They use some hierarchical
Bayes methods, but they have this flavor. One thing you might do,
and this is what they did. They said in my
simple model here. Let's just think about the mean. The mean is the only
thing that's changing. They say, the mean
and a given state is equal to an overall mean plus something that
makes us different. The thing that makes
this different is in this case a normal
0 Sigma squared. When you change state,
this is what you do. You draw new Mu s_t. By estimating or changing
the variance of this, you change how much
you're changing and how much the new state is or isn't like previous states. By seeing data in
which it looks like you've changed states a
few times you can get an idea of g when
the states changed, how much did they change? Now of course, to be able
to estimate that precisely, you better of change states quite a few times so you
can get an idea of this. Another thing you might
do is you might say, or they didn't do this, but
this might be sensible. You might say, the mean and state 3 is a
lot like the mean and state 2 plus a
little increment. Anyway, this is if I think about forecasting and adaptively, if you will, tracking something that's
evolving through time. This model, this past
through line, this guy and Timmerman is a nicer
way I think than this. This of course, James model is very useful for many things, but I like this better
for many things. That's all I was going to say. That's  Markov Switching, That's discrete breaks,
but they're stochastic. Next thing is other ways of
incorporating time variation and I want to think about
it in forecasting models. What a real forecasters do. Are not real fun. I'm sorry. What do some real
macro forecasters do? If you ask Larry Meyer, how do you earn your salary? By forecasting, he would say, I have a model, and
that's a model. I have insight and judgment. What he will do is run his model and then look at the
output and change it. That is he'll add factor it or introduce some
intercept shifts. That's subjective judgment. In the current context
of these models, we might think of it
as an intercept or some time-varying coefficients
that are evolving? In here, some difference
in ways to think about a competent Henry. I'm not going to
talk about that. Interestingly,
there's this old work that some of you may know by these famous econometricians
who tried to formalize in a great way
this add factors stuff. What they did is
basically considered or thought about this model. This is the model that
we've been talking about. We could interpret as the Cooley and Prescott
adaptive regression model from the early 1970s. What they did is added an
intercept to the model, which evolved smoothly
as a martingale. This is the thing we've
been dealing with. Then in there thinking
there's some x Betas. We're only talking
about if you will, the add factor part, the adaptive regression
Cooley and Prescott bit. I want to talk about
some issues involved with estimation of this. Again, this is just the
Kalman Filter model so we thought about
estimating of the Betas. To do that, what
you'd need to do in this simple model that l guess there are only two parameters, you need to know the
variance of that guy and the variance of that guy. I guess you'd need
an initial value. See you want to
estimate those maybe by maximum likelihood
or something. In this Gaussian case, probably would want, anyway. We know how to compute
the likelihood function. In studying you maximize it. It turns out you can easily concentrate out one of these
parameters as Beta thing. You can concentrate it out by using a Kalman Smoother thing to give you the maximum
likelihood estimator of that conditional
on these two guys. This is just turns
out to be GLS. This is where you
can do GLS because your x regressor is one and
one is strictly exogenous. I remember, I said I'd
never saw regression with a strictly
exogenous reason. One is okay. This is an example. Now, we want to maximize the likelihood with respect
to those two parameters, Sigma Epsilon and Sigma Theta and let me just talk
about two problems. One problem is a
problem that you run into when there isn't
much time variation. When there isn't much
time variation, that is, when the variance of this guy is small relative to the
variance of this guy. It turns out that the
maximum likelihood estimator is very poorly behaved. I'll explain that. What you might want to do there is use a
different estimator and I'll talk about that
because it's related to some things Jim talked
about yesterday, I think, but I wasn't there. The other thing is, there
are versions of this model that Jim is going to
talk about tomorrow, in which you don't just
have one y and one Beta. You've got a whole bunch of them so he's going to talk
about models with N. One of his models is
going to look like this, but they're going to
be bunches of y's and hence, bunches of
parameters to estimate and the question is
in those models, how do you handle it? Just doing brute-force, give this to a
nonlinear maximizer, might not be the smartest idea and there are some
tricks you can use. We'll talk about data
augmentation, that's the trick. First, let's think
about what happens when this guy is small. You can see the problem is going to come and bite
you in the rear end by considering something
that you probably know from basic time series models from like Box-Jenkins stuff. Here it is. This model, difference y I
talked about this before. Difference y is different
Beta plus different Epsilon so that's Beta plus this and that is an MA1 because
it's got autocovariances that disappear after one so
you've got this model. Now, suppose the variance
of Beta is really small. Well, that means E is
basically Epsilon. Because this guy is small
relative to this guy. If E is basically Epsilon. What's the moving average
coefficient Theta? Well, apparently it's one. If the variance of
ADA is very small, this is an MA1 with MA coefficient which
is really close to one. There are well-known
problems with estimating MA models when you've got
unit moving average roots. This is just a slide that
says that so work through the algebra on your
own if you know it and if you don't, you don't. You know, with MA models, the thick of the MA1 model. I've got an MA1 model with
MA coefficient of a half, Theta is equal to a half. You know, there's another model that's non-fundamental that's observationally
equivalent that has a moving average coefficient
of two, 1 over 1/2. You can always take these
moving average roots and flip them and find a model that is observationally
equivalent, fits the data exactly as well, has the same autocovariances, has the same likelihood so a
half and two are the same. 0.8 and 1 over 0.8 are the same. Now as you get very close
to one, the likelihood, because on one side of one it's the same as on
the other side of one, the likelihood is going to
be symmetric around one. That means that if you'll look at the likelihood function, since it's symmetric
right there, it's got to have a
derivative of zero. If it's got a
derivative of zero, that means that zero is got to be a local max or a local min. Now, imagine a true model in which Theta is
really close to one. Then, except for sampling here. The expected peak of the
likelihood is that Theta but add a little noise and you're going to get
that might hop over to one. This means that in
models like this, there's a finite
probability mass that the maximum likelihood estimator
is going to be exactly one and as the true value of
Theta gets closer to one, that probability gets higher
and higher and higher. This is studied in these papers. There's other papers as well. That says in situations in which this guy is close to zero, you often find
Theta's equal to 1. Theta is equal to 1
is the same thing as saying Sigma ADA is equal to 0. That is, you will estimate
no time variation even when the amount of time
variation is there, but not very big. It's not like you estimate 0.01 and it should be
0.03, estimate zero. That's your run is zero
and you go, damn, zero? It's because of this, it's zero. You keep looking for
more decimal places, but it's not, it's zero. This is a problem with the
maximum likelihood estimator. One can think about using
different estimators here and people have proposed
different estimators. I'm going to talk about one
that Jim and I proposed, and there are other ones here. I'm going to talk about this
just because it fits in with some stuff that Jim
talked about yesterday. I have to draw this picture. I think Jim did something
like this yesterday when he talked about
weak instruments. Did he talk about media
and unbiased estimators? Inverting test statistics. They are confidence sets. That's what we're
going to do here. I'm going to do it again, but in this other context. This is what we're going to do. I'm not going to go
through algebra here, I'm just going to
show you the idea. Big for those of
you in the back. Let me call the parameter
I want to estimate Gamma, and that's going to be Sigma
Eta over Sigma Epsilon. This is same notation
I used yesterday, actually in the same setting. If Gamma is small, write the maximum
likelihood estimators, that's Gamma is zero, Sigma Eta is zero. If Gamma is small, we got this pile of problems. We'll often estimate by
maximum likelihood Gamma to be exactly equal to 0. Here's the strategy, we're going to use
a strategy much like Jim talked about yesterday. For example, probably
in the context of Anderson-Rubin statistics
or other statistics. In this case, we're going to
construct some statistic. Here's a statistic
that's interesting. This Gamma is small. Yesterday we talked
about ways of testing whether Gamma is equal to 0, and we had this test to QLR test and this test and child test and now bloom test and
all of these tests. Let's just take one of them. Generically, let's
take this test. I always use that thing. That's some test like the
QLR test, the contest. This is our test, testing whether that guy is equal to 0. What did we do yesterday? Yesterday we showed
that under the null, that is, if Gamma is equal to 0, this guy converges to something. We figured it out and
it's got some critical, we figured out its quantiles, those are critical
values for the test. This guy converges to something, let's call it squiggle, and that guy has some
probability distribution f. Well, now what we're going
to do is we're going to modify this just slightly. Let's think about
Gamma as being small and let's do this. Let's work out this
probability distribution for any small value of Gamma. We already get it for
Gamma is equal to 0, but let's do it for Gamma being equal to another small number, not zero, just
another small number. Well, then this guy is going
to converge to something. It's going to depend
on Gamma though. Maybe I should put
a Gamma down here and this probability
distribution is going to depend on Gamma. Now, we're just about done. Did Jim do this? Do you guys nod your heads? Feel good about this? Good. These guys are
nodding their heads. Guys in the back you
nodding your heads? He's nodding his head. Cameron's got it. Is it too great
econometricians in the back who've been taking notes? This one I know what
we're going to do. Let me do this. I'm going to use
different notation if you've forgotten what I use. Here's this probability
distribution. Let's do this. Maybe I will use
the same notation. I get to use the same notation because I'm going to screw up. Here we go. I guess in the notes, this F thing I call A, so let's call it A. That's the CDF. The CDF of course is a function, depends on where
you evaluate it. Let's think about the
median of the CDF. Let's find the value of x that makes the
probability that squiggle is less than x to
be equal to 0.5. That's going to depend on Gamma. If I have different Gammas, I have different
probability distributions so the median of the
probability distribution is going to change
as I change Gamma. Let me put Gamma down here. Let me draw this. Well, here is Gamma
is equal to 0. Here's the median if
Gamma is equal to 0. I'll do it right there. That's the median of the
probability distribution of the test statistic under the null that we worked
out yesterday. Now let's work out
the distribution when Gamma is equal
to 0.1, small number. Statistic is going
to have a median. The probability distribution
we have a median, let's work it out there. Applauding, boom. Gamma is equal to 0.2. I'll put it like that. I can't draw on this, so it's monotonic
as you noticed. I'm going to do that for
all these different values. Then you get some function
that looks like that. That's this median function. This is this median function. Here's the estimator. This is very clever. This is what we're going to do. We're going to go
out to our data and we're going to compute
this test statistic. We don't care if the
null is true or not, we're just going to compute
it because it's fun so we're going to compute
the test statistic. Let me come over here. Here's the test, that's the
value of the test statistic. This is what my estimator
Gamma is going to be. I'm going to go like this. We're going to read over to the median function
and then go down. Did Jim do this? This is value-added then. This is interesting. Here's a property
of this estimator. With probability 0.5, the true value of Gamma
is bigger than this. With probability 0.5, the true value of Gamma
is less than this. This is a median
unbiased estimator. The probability
0.5, it's too big. The probability 0.5,
it's too small. Why is the true value of
Gamma is bigger than its 0.5? Well, if the true value
of Gamma was up here, then with probability 0.5, I would have found a
value bigger than this. If it was less than
this with probably 0.5, we would have found a
value less than this. This turns out to be a median unbiased estimator
of this statistic. This guy turns out not to
have a pile of problem, turns out to perform much
better in this situation. Jim probably did this, but let me just finish this
off because this method, of course, can also be used to construct
confidence intervals. Using whatever it is, the method of confidence bends. Did Jim talk about that? Which looks like this. Instead of the median function, let me do this. Let me get rid of that. Let me plot the 95th quantile and let me plot
the 5th quantile. Then how do I construct a 90 percent
confidence interval? Well, I go and
compute my statistic, whatever it is here, and I read over like
this, like that. This value in here is my 90
percent confidence interval. Because five percent of the
time this guy is too small, five percent of the time
this guy is too big. This is a general way of constructing confidence
intervals given one statistic, if you can compute its quantiles as a function of the
parameter of interest. You see this applied right in this weak instrument stuff that Jim talked about yesterday. A standard application of
this is you want to construct a confidence interval for the largest autoregressive root and you know you can't use
normal approximations. Jim has this old JME paper
in which he does this, the method of confidence
bends to do that. It's a standard way
that people do that. I like this because this is
like statistics one-on-one. Here's some data, some stuff. I'm not going to talk
about this because we're running out of time. I need to spread these. I had too much time this morning and now I'm running out of time so clearly I need to
smoosh this stuff around. I want to go to this
other issue now. This other issue is estimating this model where you might have a large computational
problems in large model. I want to consider a version of this model where you've got
many unknown parameters, not just Sigma Eta
and Sigma Epsilon, but you've got like
a zillion parameters and you got to do non-linear
maximization of the likelihood. An example is something Jim's going to talk
about tomorrow, a standard little factor model. Y is now a vector with
n variables in it. This is an example of
something I'll talk about. These y-variables are
pushed around by, let's say one latent factor f. These capital Lambdas, these are the factor loadings or the coefficients relating
the factors to the wise. These Epsilons make
these Epsilons IID or something that are
uncorrelated with one another. Let this factor
evolve as an AR1. Now if you want to
estimate this model, you got to estimate all of
these standard deviations. All of the standard
deviations of Epsilon, standard deviation of ADA, you've got to estimate Phi, and you've got to estimate
all of the Lambdas. If n is big, like
200 or 300 or 400, this is a lot of parameters, and the question
is, what do you do? Well, an easy way to do it is, here's an example of
data augmentation. This EM thing, which
is to ask yourself, it's always important to put
a close parenthesis here, wouldn't this be easy if I
could observe data on f? The data augmentation
method works well if the answer to the question, suppose I had data on blur, is then life would be easy. If that's the question
and that's the answer, then what I'm going to
tell you is going to help. Here, it's obvious if
you had data on f, this would be really
trivial to estimate. You'd regress each of
the y's and the f's. You regress f on lagged f and you'd call it a
day, you go home. That's what we're going to do. That's what this data
augmentation EM thing. Here's a textbook
treatment of this. There's this old
paper by Paul Ruud that I like a lot
when I read it. It's like journal
of econometrics or something because it's terse. It's just like this page. Then a couple of theorems, just boom, and you get it. Here's the idea. You've got some observed data. You've got some unobserved data. This is going to be our y, this is going to be f. This is the likelihood
for the observables that you care about. You imagine or you construct, you think about what
would the likelihood be if I could observe
what I can't observe. That is, if I could
observe the x's. That's this. What you want to
maximize is this. But here's the trick. Iteratively, this is
what you're going to do. You're going to compute for
some parameter value guess. Guess some parameter
value, Theta_naught. Compute the expected value
of the likelihood from the complete data
conditional on the data that you have using
this parameter value. That likelihood, of course, is going to depend on Theta because it's a function of Theta and now what you're going
to do is maximize this guy. When you maximize that guy, that gives you another
value of Theta, you're going to stick it
in here and do this again, and you're going to
iterate this thing. Then it turns out
there are a couple of three nice results. Result one is that
if you do this, somewhat surprisingly, the likelihood always goes
up or never goes down. Number 2, if this
thing converges, it converges to a local maximum. Hopefully that would
be a global maximum. That's how you can
hope of an optimizer. Exponential families, think of them as
normals in our case, this turns out to
be even easier. Here's the game that you
play in exponential families This is what you do. I got to do this. Suppose you can do this, write down the usual
function which says, how do I go from the data, now the observed data
and the unobserved data, to compute the maximum
likelihood estimator. That's this function h, which is a function of some
sufficient statistics. How would I construct the MLE if I could observe these guys? These are some
sufficient statistics. Now compute the expected value of the sufficient statistics. Given the data you observe, apply the same function to them. That's an iteration. Then you take this guy, you stick it back in here
and you do it again. Let's just jump to this example so you can see it because
it's so beautiful. It looks messy, but this is just regression. This is x prime x
inverse x prime y, so if I could observe f, what's the formula for the maximum likelihood
estimator of Lambda? Well, it's just you
regress y and f. What's the formula for
the variance of Epsilon? Well, it's just
the usual y prime, y minus y prime x, x prime x inverse
x prime y thing. That's what that is. What's the maximum
likelihood estimator of Phi? Well, it's just that. What's the maximum likelihood estimator variance is just that. These guys, all of
these sample moments or the sufficient statistics. This tells you how you map
the sufficient statistics into your estimators,
just regression 101. Now what do you have to do? All you have to do is find the expected value of all
the sufficient statistics, and that of course,
turns out to be easy, the Kalman Filter Smoother
tells you how to do that. You have to compute guys like, what's the expected
value of Y times f? Well, that's Y times f. What's the expected
value of f squared? Well, that's its mean
squared plus its variance. The estimation procedures
really simple here, gets some parameter values, run the Kalman Smoother,
compute these moments. Take these moments,
stick them into a regression package boom, you get new estimates. Go back up here, do it again, and what's really
nice about this? For those of you that have tried to write computer programs involving anything complicated. When you write a
computer program evolving anything complicated, you always make a mistake. Then you fix it and you
make another mistake. Finally, you can't find
any more mistakes, and you think you're finished. But you really worry because
you found lots of mistakes, and the only reason
you're not finding another mistake is that you
can't find another mistake. What's very nice
about this is you've got this little
theorem which says, if you do this, the likelihood will always
go up, not go down. If you run this algorithm
along the way you compute the value of the likelihood
and you see it fall. It means you made a mistake and you need to fix it
and you need to find it. It's like a wonderful
debugging device. I'm not going to
talk about that. Let me talk about
if I can have you for just two minutes. Because I have to show
you one more thing because this is important. Here's the problem. You got to model, and it's got two sets
of parameters in it. All models have two
sets of the parameters, the parameters that
you care about. Then all of the other stuff, we have to estimate to figure out the stuff
that you care about. The stuff that you don't care about that you have
to estimate anyway. Those are called
nuisance parameters. Then there are the parameters of interest to things
you care about. Let's suppose that
you're in a world in which you've got some
parameters of interest, column Theta_1, and then you've got some
nuisance parameters Theta_2. Maybe, an example
from this is Hong Li and this a paper by
Mueller and Li is, I forget what Hong
was interested in. Maybe it was something like a new Keynesian Phillips
curve or something, and it had some
parameters in it, and she was estimating these
using some VAR techniques. There were some other
parameters for the VAR, and her worry was, Gee, she'd seen all this literature on
VARs are always unstable. They have some time-varying
coefficients in them. She was worried that maybe
using these VAR methods with potentially time-varying
coefficients would mess up per inference about the
parameters of interest, these coefficients in the New
Keynesian Phillips curve. That was the example. What she did and what will
wreck and work Mueller and she did in two papers was
steady this general problem. Let me tell you this
general problem in the context of this example. In the context of this example, suppose that Beta is the
parameter of interest, and Alpha's just a nuisance. You don't care about
the constant term, you care about the
slope coefficient just to make life easy. Suppose the intercept
term, the coefficient, the nuisance parameter, has time variation
in, but Beta doesn't. Does the fact that this
guy is moving around like this mess up
inference about data. That's the question. Now, it turns out you
can do some calculations and you can see when
it's a problem, when it's not a problem. Here's the idea. In this little regression thing, how do you estimate Beta? Well, you deviate x's for means. You put the means and
the constant term. You look at deviations
for means and the x's. That's all I've done here,
deviated from means. Now, right out, so now these are all
deviations from mean. This new constant term, still has time variation in it because that bit's time-varying. Well, you don't write
out the OLS estimator. The OLS estimator for Beta, which doesn't account for
time variation, is just this. This is the usual formula for root key
Beta-hat minus Beta. Beta-hat minus Beta is x prime
x inverse x prime Epsilon. That's what that is. Then it's got this little
contamination factor. This is the part that comes
from this time variation. What Mueller lead you is ask, when this part is small, when this part dominates, and it's pretty clear
to see what's going on. It turns out if the amount of time variation isn't too big, if this Gamma isn't too big. Then it turns out that this part is small
relative to this part. Asymptotically, this part
goes away in a sense, you can ignore it. It turns out that this term
is small in situations in which the power of
this test isn't one. If you were sure you
could see time variation, then this is a problem. If you can't be sure you
can see time variation, then it's not a problem. Now, if you could
see time variation, it was really clear. What would you do? You'd fix it, if it's there
but you can't see it. Well, you didn't worry about it. But these guys say, don't worry. You can't see it, if you're not sure it's
there, no big deal. No big deal in that
that term is small, that's the usual term, and you can do inference
in the usual way. This is really cool
in important stuff. That's what the
rest of this does. 