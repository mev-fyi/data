Nikhil Agarwal: This
analysis using data from matching markets. As [inaudible] Parag
elaborated, there's a large literature studying
various mechanisms. These literature study
is primarily from a theoretical perspective
in which we compare various mechanisms
based on efficiency, fairness, incentives,
and stability. This literature has resulted in many debates on the best
forms of market organization. For example, Parag mentioned
some trade-offs between the immediate
acceptance algorithm versus the deferred
acceptance algorithm. There are some
trade-offs that have been identified here
by these two papers that I've cited, and there's some work to be done in
quantifying those trade-offs. Similarly, the debates
about centralized versus decentralized
assignment systems and the effects of
centralized systems on other things that
you might care about. For example, salaries in the
case of the medical match. One of the things
that we've learned through this investigation is that theory does not always result in unambiguous answers. In these cases,
there's a role for empirical work that is based on estimates of preferences
of agents in these markets to
make some progress. The reason why
preferences I think, are important in this
investigation is that the theory really
takes these preferences as primitives and the
outcomes are ultimately mediated through the
assignment system via these preferences to
result in a final assignment. In fact, practical designs, as [inaudible]
Parag pointed out, often use stated reports of agents preferences to
determine final allocations. Additionally, from a
research perspective, these formal mechanisms
result in and generate rich administrative
data often on stated preferences of
agents as well as outcomes. They are fruitful ground
for empirical work. What do I see as empirical
work doing in this field? Outright you can engage in
variety of positive analysis. For example, you might be
interested in quantifying the determinants of
parents preferences for various schools or colleges, what they value more or less. This can be important from
achievement perspective. In the labor
economics literature, we're often interested in
quantifying the value for various job humanities because they result in compensating
differentials. You might also be interested in analyzing the
effects of variety of policy interventions such
as taxes, college tuitions, subsidies, quotas on
the final allocations, on the distribution
of outcomes as well as equality of access. In this type of investigation, preference estimates can facilitate general
equilibrium analysis, particularly when
you do not have random assignment
of various types of market institutions or taxes across various markets
of similar type. Now since you are
going to ultimately estimate preferences, to engage in this analysis, you can also engage in
normative analysis, understanding the welfare
consequences as well as from a distribution perspective of these various policies
or mechanisms. In this frame, the investigation is highly complimentary to the theory
in the evaluation of trade-offs providing a
quantitative counterpart to the economic forces that have been
identified by theory. It can in some cases also
result in analysis where theory has proved intractable or has resulted in
ambiguous answers. You're probably
most familiar with the revealed preference approach in the investigation
of consumer choice. There we are
comfortable assuming that an agent who goes and
buys a particular box of cereal has bought
the one that gives him the highest indirect
utility as compared to all the other boxes of cereal
that were available at the prices that they were posted at in the grocery store. However, as Al pointed
out, in a matching market, you cannot choose your
most preferred option, you must also be chosen from the other side of the market. You just cannot decide
to enroll at MIT. Your partner needs to agree
to marry you, hopefully. In Peer-to-Peer
platforms, you also require mutual consent on both sides that are
engaging in a transaction. This becomes a little bit complicated in matching markets. What I would like to suggest is that the rules of
the market places, the various rules that you've
been exposed to today, are going to determine
how you interpret the data in order to engage in this revealed
preference approach. You need to be cognizant
of the fact that a student might not
apply to Harvard, MIT, or Stanford because the student might think that it's very
unlikely that they're going to get in instead of a revealed preference for not liking one
of these schools. By thinking about these
incentives and the rules and how they translate into the decisions that
agents should be making, you might be able to
make progress and analyze the preferences of
agents in these settings. In fact, organized
marketplaces, I would say, present a unique opportunity
for doing this because of the administrative
data that is often available on outcomes
or submitted rankings. Additionally, the well
understood rules of the market places can assist in the various modeling choices
that you could be making. Now, I'm going to talk about mostly such organized
marketplaces today, and this is going to
be at the cost of a significant body of related work in labor markets
and marriage markets that use these similar
equilibrium concepts and transferable utility models to study decentralized markets. There are some papers that
are cited here that if you're interested in
analyzing those cases, you should take a look at. Let me briefly outline the preference model, and
give you a quick primer on the standard
discrete choice case because I'll be coming back to the principles that we use to analyze the standard
discrete choice model, when I discuss the
analysis of rank ordered data this is typically
the school choice case. Then I'll go on to a more limited data environment
in which you do not observe the
stated rankings of various pooling options
but look at only the case where you have data on the final matches of
who got what job. Let me tell you a
little bit about the preference model that
we're typically interested in. This literature has usually
modeled the indirect utility of agent i over
possible options j. This could be a student i who has a preference
for various jobs, or for various colleges, or a worker that has preferences for various jobs in terms of certain observable
characteristics here given by X_ij that might vary based on only
the student or only the job or only
the college are both. A preference for these various
characteristics Beta i, these are going to be
random coefficients typically with some
parametric form, and unobserved quality of the job or the college, and Idiosyncratic
preferences Epsilon_ij. This is a particular
parametric form, but for various purposes
you might want to change this, and it could be
made more flexible. In this model, it
is without loss to normalize the utility
of a reference good, usually what we call the
outside option to zero. Since utilities are only defined up to a positive I
find transformations, the scale of the utility
also requires normalization, so you need to do this
for some characteristic. We'll typically be making parametric assumptions
in order to assist estimation
although there are some nonparametric
identification results here. Let's think about the standard discrete
choice model, which I hope you
are familiar with, but we'll help you frame
the way in which I think about analyzing these problems. Here I've got a picture
of a two good case. On the horizontal axis, I've got the utility
of Option 1, V_1, and on the vertical axis, I've got the utility
of Option 2, V_2. Let's say you see an agent
who picked Option 1. This indicates since this agent caught his most
preferred choice, since we understand our
discrete choice case, that V_1 is larger than V_2
and that V_1 is positive. V_1 is positive because otherwise this
agent wouldn't have chosen Option 1, and would
have not picked anything. So what we do here is to look at the
options the agent picked, and then figure out which
region and utility space, does this choice correspond to? In this case with
two options and the possibility of
not taking anything, there are three regions
in utility space. This gives you as a function of the parameters or likelihood, or choice probability, and other forms of
empirical information that maps to model to the data. Sure enough, there are
many different estimators, that you could take
this basic idea and this basic insight to, you could do maximum
likelihood by looking at the likelihood that
your utility vector, given the parameters, maps
into any of those regions, there are Bayesian
approaches that are very popular in the
marketing literature. The maximum score estimated
dating back to the 1980s. More recently, there
have been developments with moment inequality, methods, and popular methods which
allow for certain types of endogeneity as well to be used in the method of
moments-based approach. You're probably familiar with
the logit function form, in the case where Epsilon_ij has the extreme value
Type 1 distribution, where you say that
the probability that agent i most preferred
option is j, is given by the ratio
of the exponent of X_ij Beta for the option
that the agent picked, over the sum of the exponents
of all available options. This revealed preference picture resulted in this particular
likelihood function. Now, what we're going to do next is to look at the
rank-ordered data case, where an agent has listed
several different choices in order of some references
that they might have, and think about how you might engage in a very
similar exercise. I will start with the
non-strategic choice case before moving onto the
strategic choice case, and tell you a little bit about the limitations
of this approach, and the key empirical findings that we have found so far. Let's talk about the
non-strategic choice case. This case is the case
where, for example, I'll mention you might
have a mechanism in which it's the
dominant strategy to report your
preferences truthfully. What I mean by that is that
it is in your interest to rank your most preferred
option at the first choice, at the first slot, your
second most preferred option at the second slot, and so on. Now, let's say that
agents understand this, and they indeed behave
in this fashion. Going back to our
previous picture, where you have say two options, V_1 and V_2, Option 1 and Option 2, and potential rank order list
that you could write down, you now have a split
of this same region, based on the rank order
list that you observed. An agent that ranked
1 and then ranked 2, indicates that this
agent's utility lies in the region that
says Rank 1 bigger than 2. It indicates that 1
is better than 2, and that 2 is better
than nothing. So V_2 is positive. This was a finer partition of the utility space that was
revealed by this agent. You can use this to go ahead,
and estimate your model. Here, I've got the logit model, but now I've got a random
coefficient Beta_i on this agent's
utility for X_ij. In the logit case
where Epsilon_ij is, this called a mixed
[inaudible] case sorry, Epsilon_ij has an extreme
value Type 1 distribution, the probability that an
agent ranks j followed by j prime is given by this
exploited logit form, where you first have the
expression that indicates the probability that option j is the most preferred choice. Then you have the
probability that option j prime is the most preferred,
ignoring option j. In the denominator
in the second term, I've got k naught equal to j. Because of the IIA property
in the logit case, it boils down to this
simple expression. If your functional
forms are different, the likelihood will look a little bit
different because it would just be the likelihood of being in one of those regions. But nonetheless, those
other function forms are typically also estimable. Now, when you have
multiple choices, it gives you a lot
of information about these random
coefficients Beta_i. These are idiosyncratic
preferences of agents for various options. An agent, for example, who might like schools
that are very big, or schools that has good
student to teacher ratio, many of these schools
at the top of the list, at the cost of other
schools that might have more extra curricular
activities or other features that might be attractive to other students. By looking at the
correlation between the characteristics of
the top rank choices, you can learn about these
random coefficients, and this ordered
data turns out to be very important and
very useful in characterizing the
heterogeneity in preferences for various
school characteristics. There are several
papers now that have documented patterns in which there is significant
preference heterogeneity along socioeconomic
characteristics, as well as unobserved
characteristics of agents. Here are some estimates from my paper with Attila and Prague on preferences for ninth-grade
schooling programs in New York City. I've got various
different specifications and select coefficients here. The first two columns do not have any random coefficients. The second column has some
observable characteristics. The next three columns take robustness to later
rank choices seriously. Suppose, for instance, you are 10th rank
choice is not as well informed as your first
few rank choices. What you see is that you see
very intuitive patterns. But students that performed well in eighth grade in math, seem to have a differentially
strong preference for schools that have high
good map achievement outcome. These are correlates and
descriptive of preferences rather than to be interpreted as causal
estimates which indicate what would happen if you
increase the achievement of a school. On the right-hand bottom, I've got a block of estimates that show you
the random coefficients. You'll see that these are
very precisely estimated. This is because
in New York City, students rank up to 12
choices, and therefore, you can learn a lot about the idiosyncratic preferences, and the extent of extent to which there's unobserved
heterogeneity in preferences. That was fairly straightforward because students who are
telling you the truth. What would happen if
you had a mechanism in which students had
some incentives to misreport their preferences. As Prague, I've mentioned, in the immediate
acceptance mechanism, which was used in Boston, and is still widely used for
better or for worse. Students are prioritized at higher rank choices. Some students might
rank as school first, not because they like it, but because they would
like to gain priority at this otherwise competitive
school or might want to cash in on their
priority at the school. From a student's perspective, there's a trade-off
between gaining priority at the
true second choice or ranking the true
first choice first and taking a risk that
they do not get assigned. If you put yourself in
the student's shoes, a student should be trading off getting into their most
preferred choice with some small probability
if you really like it, if you really like
your first choice, even though there's a small
probability of getting in, you might as well rank it
because it's worthwhile. If you're not so crazy about your first choice
and you're roughly indifferent between your
first and second choice, ranking your second choice first might guarantee your
position there, and that might be
a better decision. I'll discuss the
baseline case in which agents make
optimal choices. Not all agents might be
fully sophisticated, but there are various
approaches in this literature that
allow for cases where there are biased
beliefs or a mixture of naive agents and
sophisticated agents. Other approaches that also play somewhat less
restrictive assumption on the knowledge of the
students on what they know. But it's instructive
to think about the baseline case of fully rational behavior,
or think about how you might engage in this
revealed preference type of exercise when agents are
making strategic decisions. Here I've got a picture
in which I have three illustrative
rank order list of three illustrated reports R, R prime, and R double prime. The horizontal axis is the probability of getting
assigned to School 1. The vertical axis is the probability of
getting assigned to School 2 as a function of
each of these reports, our R, R prime, and
R double prime. Now, the vector V shows you the direction of the
indirect utility of a given student. The student has indifference
curves that are linear, given by those dashed
lines increasing to the North East. This agent would likely
choose either R or R double prime because
that is the one that is going to maximize the
expected utility. If you look at all agents
that ranked school R, these agents reveal that the indirect utility must
be in this shaded region, must be pointing in
the shaded region. Why is that? Well, if these agents indirect
utility vector was pointing to the
left, to your left, then this agent would prefer increasing the probability
of getting into School 2 and would not like a high probability
getting into School 1 and we'll probably pick R prime, which is the lottery
to the northwest. Indirect utility vector was
pointing down, that is, this agent does not
prefer School 2, then this agent would be
picking R double-prime. The lines here are the pivotal indifference
curves that make R optimal. The shaded region tells you all parts of the utility space, all directions of
the utility space that make this choice optimal. Now going on to R prime
and R double prime, you can engage in a similar
comparison, and you can figure out what their
utilities must look like. Now, moving on to the utility
space or familiar picture, you see again V_1 and V_2 and I've labeled three regions, C_R, C_R prime, and C_R double prime. These three regions
correspond to the regions where report R, report R prime, and report R double-prime
are optimal. R might be ranked School 1 first and rank School 2 second. R prime might be only ranked School 1 and R double prime
might be only rank School 2. Again, you can take this
picture and this partition. What you can do is write down your likelihood function or
your choice probability. Then you can go back to
your favorite method. Method of moments similar
to maximum likelihood or any of the other ones that I had in one of my slides earlier. You can use your techniques
to proceed with analysis. You can also have
some mixture models in which there's some
naive agents that report truthfully and some sophisticated agents
that behave like this. That'll just give you a
mixture of two likelihoods. This approach has been used by several
papers at this point. I would like to show you
that it's important to think about this formal
strategic behavior. Since we are in Cambridge and since I have a paper on it, the year I've given you various schools in the Cambridge Elementary school system, the public school system placed in order of how many
school ranked first, crime and parks is the
most populous school. You can see the top-left number shows you that for a
paid lunch student, even if the student
ranked first, is only a 22 percent chance that the student is going to get
assigned to Graham Parks. A few rows down, you've got Fletcher Maynard, which is not such a good school. If you're a Peabody
and you rank it first, it's a sure shot that
you're going to get in. If you look at the
outcomes and you look at the region where
it is located, you will probably know that
it's not a good school. Sure. In the next
two sets of columns, I've got estimates
of how desirable these schools are in terms
of willingness to travel. In terms of how many miles a typical student is
willing to travel in order to avoid getting into
Fletcher Maynard. It suggests that Fletcher
Maynard agents are going to travel 1.3
miles in order to avoid getting into
Fletcher Maynard as opposed to the average in the Cambridge Public
School System. Likewise, Graham Parks seems to be desirable since
agents are willing to travel 1.3 miles in order
to go to Graham Parks. Now, if you instead
assumed that agents were sophisticated or hide
rational expectations about the probabilities
of getting in, you will estimate
that Graham Parks is what 1.9 instead of 1.3 and Fletcher Maynard
is actually negative 2.3 instead of negative 1.3. Why did I get this result? Well, the students
at Graham Parks first not only indicating
that they like Graham Parks, but they're indicating that
they're willing to take a small probability of one-in-five chance of
getting into Graham Parks. Therefore, they
indicate that they have a strong preference
for Graham Parks. Likewise, students are not
ranking Fletcher Maynard, even though it's a sure shot. Therefore, their willingness to rank other schools instead indicates that Fletcher may not, must be particularly
undesirable. Let me move on to some empirical findings and limitations of this approach since it is not a silver bullet. Some of the findings that we
found is, as I mentioned, a significant preference
heterogeneity for various coding options based on socioeconomic
characteristics. We found that
coordinating, admissions, and assignments is of
primary importance as it limits inefficiencies since many students would otherwise be placed
at unranked choices. I'll mention the congestion that New York City
public schools faced where about one
in three students were students not in placed to one of the options that they
ranked before the reform. This results in
significant inefficiencies that a centralized
system can solve. The immediate acceptance
algorithm seems to have some benefits in
terms of screening students that have
a strong preference for particular school, and placing them
at those schools. However, this particular finding depends on
students really understanding how the
mechanism works and having rational
expectations about how competitive
various schools are. Without very
sophisticated students, these benefits are somewhat
limited and at any rate, these benefits come
at the cost of assignments that are
somewhat are not stable. There are some
limitations here as well, these models take strong
stances on parent information. If you want to interpret
these estimates in terms of welfare, you need to make the assumption that students
understand and parents understand what the
characteristics of the various schools are, and what's good for them,
and they're making choices in their best interests. Peer effects are
largely ignored since we only look at the value of assignment of your
own assignment rather than other students that might be going to school
with you, and in that sense, Josh might be saying that if many students from
that neighborhood go, I'm going to pick that up
and those students are not so great and I
value my classmates, value the composition
in schools, classmates in my school, perhaps I'm picking
up that I won't have good classmates instead of the teachers aren't very good. There's limited evidence
on the monetary value for better schools and part
of that is data constraints since public schools in the United States do
not charge tuition typically and it
would be good to have estimates of the monetary
value for better schools. There's also limited evidence on the downstream
effects of assignment on various achievement outcomes. That was the relatively
nicer case where you had information on some
form of stated preferences, whether they're
truthful or strategic. But in many cases, you might have only information on employer-employee
matched data. So you might know
what types of workers are sorted into what types of jobs and what they are paid. But you might not know exactly what the rank
order on the list was, or the various job options. Perhaps nobody collected
it or perhaps there's a centralized system that does allocate
workers to schools, but that centralized system wasn't willing to
share the data. In these datasets, you often
do have information on various characteristics
of the workers as well as the firm's,
perhaps salary, other measures of amenities, the prestigious of the job, the qualifications
of the worker, the education of the
worker and you typically find that there are a
significant sorting in proxy for quality. For example, medical
students that are trained at
prestigious schools tend to train when they go to residency at prestigious programs
and large hospitals. Now, you don't want to take this and straight
away say that as a wrestling that
is matched at MGH prefers MGH to Brigham Women's because it might have just been that Brigham Women's wasn't willing to hire this resident. You might want to work a
little bit harder in order to think about what the
preferences of agents might be. You cannot directly use the data interpreted in terms of
revealed preferences approaches that I
developed earlier. Second, you need a two-sided
model for preferences because as I alluded to, programs have preferences
for residents as well. Now, the literature
has typically used the pairwise stable
equilibrium assumption which tries to model a frictionless market in
order to make some progress. As you've already learned, this equilibrium
restricts the matches to follow to satisfy
two properties. First, no firm
should be assigned more workers than
it has capacity and each worker
has only one job. Second, no worker is placed at a proposal form that prefers that worker to a
currently assigned worker. This is the no
blocking condition that was introduced earlier. Now, substantiating
this assumption requires some knowledge of
marketing institutions. There are various
types of failures that might occur in matching
markets congestion unraveling being some of
the ones that were faced by the medical residency market before it was centralized. I'll talk about some
of the estimates that are obtained in the market for family
medicine residents using this equilibrium
assumption, but this assumption
could be used in other contexts when
you're confident that market failures that will result in blocking pairs
are not significant. The key problem here is how do you learn about preferences of both sides of the market by just
observing final matches? To build intuition,
I will talk about a stylized canonical model in which the utility of agent I, worker I for firm J
is the same across all workers and it's given by some characteristics
of the job, the wages or the salaries
offered by the job, and some unobserved
characteristics that might make the job
desirable captioned by z-j. Workers are going to
be differentiated by the human capital of the worker, where X_i Alpha is the observable component,
and there might be certain things
like test scores or letters of
recommendation that you do not have great data on and so there are other determinants
captioned by Epsilon I. In this model, whether there's transferable
utility or not, you expect and as long as in the transform
digitally case, you have a supermodel
matching function, you expect to see positive
assorted matching on UNH. The top worker gets his
pick and picks the top job. The median worker is left
with the median job. Using this, I'm going to
try to explain to you what you can learn
from sorting parents, which is what types of workers match with what types
of firms, and why it might be useful to
take advantage of the fact that typically many workers are matched
to the same firm. Now there's an
additional issue which is the radiant
endogeneity issue, I'm going to skip that for today but with a control
functional approach, with the availability of
instruments, of course, you can deal with this rage
endogeneity issue as well. There are many related
papers that delve into the econometric
issues, and examine the transferable
utility case and the input transferable
utility case, in the marriage market case, and in the case of mergers and other applications as well that use similar
equilibrium concepts. Let's say that you
went to your data and you found that
residents that are trained at high
NIH-funded medical schools tend to match with
large hospitals. So 30 percent of the matches
are between high and large, 20 percent of the matches
are between high and small, then we have low and large in 20 percent and low
and small 20 percent. You can see some positives
matching and so you might be willing to
say that residents at high NIH-funded medical
schools tend to be better, they are not always
better because there's this Epsilon
term that could be test scores that rearrange students across medical schools, but this suggests that large
hospitals are preferable. However, you won't
be able to look at this table and
really tell me about the intensity of preferences for either large hospitals or high NIH-funded medical schools. Here's why. It could
be that you're in a world where high NIH-funded
medical students are always preferable to low
NIH-funded medical students and the test score
doesn't really matter. They go ahead and
whenever they pick, they pick two large
hospitals with 60 percent of the time because there's
something else about the hospitals that
make them desirable. You get this contingency
table, the smashing table. Alternatively, it
could be the case that only 60 percent
of the residents at NIH medical students'
schools are better than resonance at low
NIH-funded medical schools. Whenever they go ahead and pick, they would pick the large
hospitals and you'd end up with the
exact same thing. You can't tell whether
this is driven, this imperfect positive
assorted matching is driven because you don't observe
everything about resonance, or everything about programs. Here's where it
could be useful to have data from
many-to-one matching. You can ask the question
whether residents that are matched at the same program, irrespective of the
quality of the program, tend to be similar in the
observable characteristics. Suppose you had everything, you had all
observables that were relevant to a
restaurant's quality, you'd be able to construct
a very good aggregate of this resident's
Human Capital Index. What you would find
is that the variation within the program
in this index, this observable index of human capital would
be extremely small. Why? Because within
a small program, you'd expect that the
residents are very similar. Otherwise, some residents
could find a better job, or the program could find better residents in a
pairwise stable match. What you can look at is whether
within program variation and use that to figure out the program's
preferences for the residents. If x strongly predicts
human capital, then the within program
variation is small, and if x does not strongly
predict human capital, then the within program
variation is large. Once you do this
using the information available in
many-to-one matching that is not available
in one-to-one matching, what you have is you have the preferences of one
side of the market, which allows you to roughly construct a choice set
on the other side of the market and engage in a revealed preference
style analysis. To show you how this
manifests in the data, I've got the fraction
of the variation in a resident characteristic
that is within program. This is in the family
medicine market. You can see that almost all of the variation in the
gender of the resident, 96.4 percent of it, is within a program which is consistent
with the gender of the resident not being informative about the
Human Capital Index. You also see that
the degree type, whether it's a foreign
degree or not, a significant portion of the variation is
across programs. There are some programs
with many foreign residents and other programs with
fewer foreign residents. That is consistent with the case that foreign degree is
informative of human capital. The most convenient
method, in this case, turns out to be a simulated
minimum distance estimator where you take moments that are based on the overall
sorting patterns, the extent to which you
have assortative matching and is within program
variation moments, and find a parameter Theta that determines the distribution
of Human Capital Index and the resident's preferences that best matches these moments. What can you do with this? Well, you can quantify value
for various job amenities. In this case, you might want to interpret this as the value of training provided by
various programs. These estimates suggest
that, for example, a resident is willing to
take a salary cut of about $5,000 in order to train at a program associated
to the hospital, the one standard deviation
higher case mix index. Residents are willing to
take salary cuts to train at the same state in
which they were born or the same state at which they went
to medical school. These could manifest
themselves in wages in terms of compensating
differentials, for example. One of the issues
that came up with the medical match was
that it was criticized as a direct and implicit
mechanism for suppressing the wages
of medical residents. There was a lawsuit in
2002 that claimed that the fact that medical
residents make about $40,000 less than
nurse practitioners or physician assistants
is indicative of the fact that the match is
causing this wage depression. The reasoning in this
lawsuit was based on perfect competition benchmark
in which they said that your substitute sources of labor that do the same thing should
be paid the same wages. Now, there has been
some debate about this, whether the match is responsible for this weight suppression. But we do not see, for example, in the gastroenterology
fellowship market that the presence of a match is related to
the distribution of wages. This market transition from having a match to
not having a match. There have been theoretical
results as well on this question and they've mostly resulted in somewhat
ambiguous answers. Let me suggest to you that medical labor markets should be better characterized
as markets in which there's imperfect competition
amongst programs, and that could result
in some degree of wage suppression. What are the sources that
you might have in mind? Well, there's
accreditation requirements here which limit the number of residents each residency
program can hire, their fixed cost of
operating, and there are certain economies of scale here, and programs that are
heterogeneous in quality. MGH, people are willing to take a salary cut in order
to train at MGH and it cannot hire all
the residents that are available and would
want to have a job there. Just simply these sources of imperfect competition
could result in weight suppression. In fact, a very
conservative bound on the effects of
these sources of imperfect competition
result in a $23,000 depression in wages relative to marginal product of labor network training cost. There are other
sources, for example, simply the licensing might be valuable and there's
restricted entry into that, that could result in
additional markdowns. If programs are productive
to heterogeneous levels, that also results in
certain degrees of rent. But these particular sources of imperfect competition result in a lower bound of $23,000. MALE_1: Let's say
it was $17,000, it's upper bound on the shared price
[inaudible] training? Nikhil Agarwal:
Well, so if you're thinking about that as
a cost of training, then this is a markdown relative to the marginal
product of labor relative to the
cost of training, so this does not speak to the value of the physicians
assistants, and in part, the salary of physician
is to some extent, Medicare and Medicaid reimbursed programs for those
costs of training. It's a little bit hard to speak exactly to that
particular bound. What this says is that these sources are
responsible for 23,000 and additional things
like just the value of the certification, the value of having
a certification for practicing medicine might
respond to some of this. There might be other things like if programs are
differentially able to produce with resident
labor those rents, those productivity
rents will also accrue to the program. But I can tell you a little
bit more about this offline. But this is supposed to be interpreted as marginal product of labor net or training cost. If you think of net of
marginal training costs, so if you think of
physician assistant salary as a marginal training cost, it doesn't account for that, it's net of that it's different
from the recurring in human capital model. Now, these sources of
imperfect competition are not directly related
to the match and wouldn't
necessarily go away if the match was not present. Therefore, it's not as easy to conclude
that the match is responsible for the
majority of this or any of this depression. In addition, it's
important to point out that centralized designs a lawful order contracts and salary flexibility, but
I have run out of time, but I encourage you
to look at the slides where towards the
end he talks about how you might incorporate a
labor market model within a centralized
clearinghouse that allows for salaries to be set
alongside the matching process. Now, this approach has been used in other contexts as well. This idea that you can restrict the final equilibrium
outcomes in terms of either some notion of stability to recover something
about the match surplus has been used in many
different contexts. The venture capital
context has been studied by Sorenson. There is a significant
literature that studies marital surplus using
pairwise stability. The efficiency of various
market mechanisms have been also studied using these
similar approaches, the value of mergers, and determinants of public
school teacher matching. But in all of these approaches, including the one
that I laid out, there are some
restrictive assumptions on preferences that
need to be made. For example, I said that residents are differentiated
by Human Capital Index. In other cases, you
might have to bend agents into discrete
groups based on the observable characteristics and map preferences accordingly. Or you might only be able to recover one single match
surplus as Sorenson does. There're some
restrictive assumptions required, and based
on your application, you might want to pick
one of these approaches. In all of this work, we have a richer
matching function. But of course, this is
at a cost of having a frictionless market for either the labor market
or the marriage market. There's one recent paper
that does incorporate some degree of friction
in this marketplace. It'd be interesting to
see if we can combine a rich matching function with some market
frictions in future work. In conclusion, I would like to say that it'd be
interesting to think about what the effects of
various market systems are on various outcomes that are intrinsically interesting
for other reasons. These are important markets
that are organized in these ways; education markets, health markets, and other high-stakes
environment, marriage. It'd be interesting
to think about how various systems result in other outcomes
like achievement or mortality or other
things of that form. A comparison with
decentralized market is somewhat limited so far,
and that'd be interesting. But let me suggest that there
are two features that I think are common that are
present in these markets, that are also present
in many other markets that are characterized
by rival goods that are non-excludable. In many of these cases, you have heterogeneity
in the types of objects being allocated, types of partnerships being formed under capacity
constraints. There are few options
of each type. In addition, there are also often limitations on
the price mechanism, particularly when publicly
funded goods are allocated. There are variety of
other settings in which this occurs. Of course, allocation
mechanisms, public housing,
organ allocation, allocation of medical
care waitlist, where you can see these
two features at play. In those cases, you
might want to think about how the market works, the rules of the market,
the available data, whether you have
stated preferences or only final outcomes in order to think about what the best way is to model the nature of the agent's decision problem to make progress on understanding what agent's
preferences look like. 