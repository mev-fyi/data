Edward Miguel: It's
great to be here. Thanks again for the invitation. I'm going to speak on
a number of topics related to open science,
research transparency. Let me start with a
little bit of an overview of what I'll be
going through today. I'm going to be covering
material that you can find at a number of what
I'm calling key references. Some of these are published papers, some of my
co-authors are here, I think Rachel's here, that I published through the years. The last on this list, the last key
reference is actually a new book that is
coming out Tuesday, July 23rd, on Transparent and Reproducible Social
Science Research with co-authors of
mine, Jeremy Freese, who's a Professor of
Sociology at Stanford, and Garrett Christensen, formerly at Berkeley, but now an economist at the Census Bureau. The press has been
very generous. They've actually given me advanced copies they're
in the box here. I just wanted to offer
anybody the opportunity. The first 10 people
who email me now, can walk away with
a copy of the book. I haven't signed them yet. If you want it signed,
I'll write whatever you want on it or just
have it plain. I have an 11th
copy here for Jim. I'm going to give this
to Jim to thank him. Bust out your smartphones, whatever. It's all good. A lot of the material
that I'm going to be talking about today is covered in the
book and so you're getting like a first glimpse
of some of this material. There are a lot of
different topics in the book, different chapters. We talked about ethical
research, publication bias, specification searching, registration and
pre-analysis plans, meta-analysis, multiple testing adjustments,
data-sharing, and a number of topics around replication,
reproducible workflow. All of those are
covered in the book, because of the limited
time slot today, I'm not going to be able
to cover all these topics. I'm mainly going to focus on the topics that are
highlighted in blue. I'm going to talk a lot
about publication bias, study registration and pre-analysis plans,
and then replication. These are some core
issues in the field. But if you're interested in more I'm happy to chat more about it, they're covered in the book. We also put together a MOOC, a Massive Open Online
Course for future learn, so you can take part
in a run of those plus the Berkeley Initiative for Transparency in the
Social Sciences, BITSS, which Jim referred to, has a very large library
of lectures, audio, video, lecture materials, some presentations I have given and many more by other scholars. You can dive deeper into
these topics if you want to. Let me start with an overview of what research transparency
and open science are and how they're being thought
about in the field today and then I'll get
into problems and then some approaches to dealing
with those problems in economics and other
social sciences. A common definition that's being used by open
science organizations these days is that
research transparency is advanced when the claims we make as researchers
are verifiable. The process that
we go through when we conduct the research
is verifiable, it's clear, it's understandable. The methods we use are
verifiable and the products and the findings generated by the researcher are
also things that people can check for themselves. I think the motivation
really at the heart of it, for this kind of work is to improve the credibility
of our research, by allowing others to
check what we're doing, our data, our approach, the hope is that we can
reduce the amount of investigator bias or other
types of biases that may creep into our research. In many ways, I see the
open science movement and the research transparency
movement and economics as the natural next step in the credibility
revolution that I think has affected all of our
fields in all of us who do applied micro in economics from Leamer
to Card and Krueger, Angrist, Pischke and
so many others over the last couple of decades. There's also beyond the
benefits for credibility, there is a normative
perspective, that I think is
important to talk about. I'm going to spend
a few minutes now, going through the conception of the scientific ethos that was popularized by Merton in 1942. Robert Merton was a very
famous Harvard sociologist, one of the most famous
sociologists of the 20th century. Also the father of the
other Robert Merton, who won the Nobel Prize in
economics, as it turns out. Merton really draws on a range of different literature to make the case that there is a very well-defined
scientific ethos and ethos of how to do
research properly and actually open science research
transparencies at the heart of that ethos. Let me briefly
talk about some of the values that he
emphasizes in his writing. The first value is universalism. This is the idea that
we should accept or reject scientific claims, based on the science
and not based on the attributes of the individual who makes the claims. We should be able to
understand the science well enough that we can evaluate it. This is where
verifiability comes in, based on if it's a
mathematical proof, we should be able to
understand that proof, regardless of who
makes the proof it should be a valid proof. Same thing for
research findings. An extension of this view about the fact that one social
identity doesn't matter, is the idea that
science will progress fastest when scientific careers are open to people from all different kinds
of backgrounds, because everybody can
contribute to science. There's an example we
talk about in the book, a real failure of universalism. This is David Blackwell.
People may know of some of his contributions. Very famous applied
mathematician who made early contributions to game theory and
statistics that we use all the time actually in economics and other
social sciences. David Blackwell was the
first African-American inducted at the National
Academy of Sciences, the first black tenured
faculty at Berkeley. But in the 1940s he wasn't allowed to attend
lectures at Princeton. In the early 1950s, he
was hired at Berkeley and then his hiring was rescinded
because he was black, etc. This is someone who contributed
so much to science, who had been born 5, 10 years earlier or hadn't had at least a little bit of
good fortune along the way, never would've been able
to contribute to science. We would never even
know his name probably. There's a lot of scientific
potential out there. Universalism is the idea that
we can all contribute to. It shouldn't matter
if you're rich, poor, black, white, man, woman. What should matter
is the science. The second core value
is communality. Communality is the notion that what we produce as researchers, belongs to the
research community. When we produce findings, we publish them and then
others can build on them. Open data is a very
natural fulfillment of this notion of communality whereas hiding what we're doing, hiding our process of research, hiding our code or
our data is really goes against this value. The third core value
is disinterestedness. That when we do work, we shouldn't worry about what others are going to
think about our findings. We should do the
best work possible and shouldn't try
to find something, because we think it will be
to our personal benefit. Or maybe if I find this particular result,
I'll make a lot of money. We should try to
find the truth as researchers and
basically deal with the consequences of that rather than being really
self-interested in what we do. We shouldn't tip the scales
because we think they're personal ramifications of what we're finding as researchers. Then the fourth bid here's what Merton calls
organized skepticism. This is the idea
that we're able to make scientific claims that are valid and really
understand their validity because other people
scrutinize them. Obviously, the journal
refereeing process is part of that scrutiny, seminars and conferences
provide some scrutiny. But Merton really sees this as the special sauce of science. In no other realm of human
activity does he say, is there so much scrutiny to claims before we accept them. We don't just accept
things blindly. They have to be subjected
to scrutiny for us to believe them. Again, you can see open
science values and research transparency is really central to all these values. One question we could ask
is, how closely do we, as scholars live up
to these values? There's been some surveys
trying to do this. The Anderson et al is one
of the larger surveys. This was a survey done
about a decade ago. They tried to get a
representative sample of researchers who have
been funded by NIH. There's lab scientists,
biologists, but there's also public health
specialists and there's some health economists actually
in the sample as well. They had about 50
percent response rate. Again, maybe broadly representative but not a
perfect response rate, still I think we can learn
something from this data. They asked a whole range
of questions about people's attachment to
these Mertonian values. They didn't call them
Mertonian values, but they said, what do you
think about sharing data? What do you think about how self-interested you or
other scholars are? I think the data's pretty
interesting to look at today. They asked three
types of questions. The first thing is
they asked about people's subscription
to these ideals, meaning do they basically
agree with these ideals? The shading here, the
light gray means, again, there were a
series of questions and they create an index. But the light gray
means, overwhelmingly, scholars agreed with the ideals. The mid-gray means, they had mixed answers and
then the dark gray means, they basically opposed
the ideals in the most of the questions
that they were asked. Here, when asked whether
they believe in the ideals, 90 percent of scholars said, "Yeah, I really believe
in these ideals." I believe we should
be disinterested, I believe we should share data, I believe in all these values. I think it's striking. If we think about our
graduate training for most of us at least, I was never taught
Mertonian ideals. No one ever sat
me down and said, this is the scientific ethos. I think people absorb it. They join a research
community where these attitudes are prevalent and they absorb those views. Now in some other
fields in medicine, there's actually
more discussion of research methods
along these lines, but less on social science. The second set of answers here regard
people's own behavior. Do you actually in your behavior generally, live up to
these ideals or not? Now here the number of
falls a little bit, it falls to 70 percent, but still, most
scholars say, "Yeah, I do live up to these ideals." Then there's that middle
gray area where people, "Well, I do, I don't,
sometimes I do." Then there's a subset of people 10 percent
who are like, "No, I pretty much don't live
up to these ideals." But again, the vast majority of scholars seem to in
their own behavior, want to live up to these ideals. The really striking one
is the bottom panel. When they asked people about other scholars
in their field, there are other scholars
living up to these ideals. Now, only about 10 percent of respondents think most scholars in their field are living
up to these ideals. There's a little bit
of a gray area there. But there really is
this disconnect between the ideals and how
researchers feel their field, what behavior in their
field looks like. The researchers, Anderson et al call this
normative dissonance. We want to live up
to these norms, but it feels like people aren't living up to these
norms. What do we do? Do we buck the trend and
do what we think is right or do we go along
with what's going on? I think there's two questions that immediately
come out of this. One is, how can we bring our
practices back in line with our ideals since most
people really do seem to be attached to these ideals
and they're so attractive? The Mertonian ideals really
resonate with a lot of us. A lot of us go into a
research career for altruistic and idealistic
reasons and these ideals speak to that perspective. But even before we get there, the bottom question
is important. How severe are these
problems really? Like maybe people on surveys are negative, maybe
it's not too bad. Maybe the problems people are talking about are second-order. That's really what I
want to turn to next, is just go through some of the evidence that
exists on problems that exist in economics and other
social science research and try to quantify the extent of these problems or the bias that
they could lead to. Let me turn to that.
There are three types of problems that I'll
focus on mainly, and I'll go through
them in order. The first problem is fraud. I think fraud is
probably quite rare. It's impossible
to know for sure. But even if it is quite
rare, it's really corrosive. Instances of fraud and science get tremendous
amount of attention, tremendous press,
and probably damage the standing of science with
the public, with funders. Economics hasn't had major fraud scandals
in recent years, but other major
social sciences have. Social psychology
has been rocked by multiple fraud scandals
in the last decade. Some of you may have
heard about them. There are several of
the leading figures in social psychology who were discovered within
the last decade to just be fabricating
their data. In many papers, the most famous case
is, Diederik Stapel, who was an incredibly famous
Dutch social psychologist who fabricated data. Uri Simonsohn
discovered cases of several other North American
social psychologists, and I'll talk about
how he discovered fraud in their case. Then in political
science a few years ago, David Broockman, who
was a grad student at Berkeley at the time
together with Joshua Kalla, and Peter Aranow discovered
fraud in a paper that had just been published
in Science in 2015. A get-out-the-vote
experiment where again, the data was fabricated and they were able to
discover fabrication. In all these cases, actually, open data was really important and the ability to access
data was important. For Broockman,
Kalla, and Aranow, they got what was purportedly the replication data and code from the
science website and found odd statistical anomalies. In the case of Simonsohn, who actually discovered
fraud for several scholars, he looked at very simple
summary statistics and published papers and
found weird patterns. This is like a very good
forensic use of statistics. Most of the social
psychology experiments actually are very small samples. If you look at a
treatment arm with 20 and another treatment
arm with 20 people and other treatment
arms with 20 people. The difference in means, say for baseline characteristic or something across
those groups, those differences in means
should be pretty large on average because you have a
lot of sampling variation. But he found that there
was minute differences in all of these papers between the characteristics of the
different treatment arms. When he's simulated data, so this data, like
his simulation, about how likely it is that
they're basically be like almost no difference between the means across
these treatment arms, he found that I was like
15 in 100,000 chance. He did this for
paper after paper, leading to dozens
of retractions. This is new stuff and
social psychology, it's fair to say it was
really rocked by this crisis, and I'll come back
to that later on. It led to a very rapid changes
and a lot of practices. But sadly, there
are many instances of fraud and science. Gregor Mendel who we study almost certainly, I don't
want to overstate it, but a lot of
historians of science believed he probably
fabricated his data. Ronald Fisher, the
famous statistician, wrote a paper in 1936 that
was very similar to what Uri Simonsohn did in Mendel's
experiments with the pea. Remember this in the
monastery and somewhere smooth and some were wrinkled. The proportion of the
recessive wrinkled pea was supposed to be about
a quarter on average, but he only had like 10 pea
plants or 20 pea plants. The proportion was almost
always exactly a quarter or it was like 76
percent, 24 percent. Fisher writes a
paper and he's like, this is basically impossible,
this can't happen. It turned out a lot of Mendel's theory was actually right. But he made up his data to prove the theory
as it turned out. Well, it was a true positive,
but it was fraudulent. A weird case. But Fisher emphasizes
fictitious data can seldom survive
careful scrutiny. Open data could be
very important there. Let me talk for a little bit
now about publication bias. This is a really important issue to consider and there's really been a bunch of new studies documenting publication bias. The most famous discussion
of this is by Bob Rosenthal, his discussion of the
file drawer problem. He's a psychologist. His idea is if null results
are hard to publish, journals don't
want to find them. There could be a lot of failed null result experiments sitting in the file drawer of labs around the
country that we never see leading to publication bias. It may not just be
null results may be controversial results
aren't published, results that don't
conform to theory, etc. This is some old evidence. This is actually from the 50s, and if you're interested in
the intellectual history around open science, I would definitely recommend
the Sterling '59 piece. Sterling was another
psychologist. He just documented
how many published studies in the leading
psychology journals had no results or
significant results. He finds they always have
significant results. That's basically
what got published. Only about three percent
of studies were nulls, and he writes about how
this is a big problem. Publication bias seems
to be a big problem, not just in psychology then this is the Turner et al study. Here is data from
medical trials. Most of these are medical
trials in the 80s and 90s, this data. They're able to actually look at really cool
data in the study. They actually got the universe
of clinical trials that the FDA had given approval to, even stuff that
never got published. They have the internal reporting on what the results were, and they can look
at the types of results and see what ended
up getting published. It turns out in about
half the results were positive results, meaning significant
results, and about half were null or mixed results. Among the significant results, they're all published or almost all published and
that's a finding that you'll see comes
out in other cases. But among the null results about 2/3 were never published, even 10 years after the trial. When you look at this,
given that the null results are a pretty sizable
share of the literature, a huge share of the
literature disappears. If we only relied on
the published evidence, we'd get a very misleading look at what the results really are. Here, significant results
are something like three or four times more
likely to be published. The most irrelevant for
us as social scientists is a recent paper that came out in science
about five years ago, Franco et al, this figure
is from Franco et al. It's similar to the
previous figure. Franco et al is based on
data from what were called time-sharing experiments
in the social sciences and some of your tests. Some of you may be
familiar with it. This was a large online
platform that allowed economists, political
scientists, psychologists, and sociologists to enter basically survey
experiments into this large online sample
and then see what the effects were of these different questions
that were asked. There are some
economists in here, and folks from the other fields. This was NSF funded
and actually, every study that ended up
getting questions deployed in the online survey went through
peer review and they're considered pretty
high-quality studies, and they're studies
from the early 2010s. This is like pretty close
to where we are now. About a decade ago across the social science is pretty high-quality studies. Again, there's sort of
because these authors worked with NSF
and they were able to contact the original authors, they know what the results were even if they
weren't published. That's what they rely on here. Strong results,
significant results are almost always written up. The dark shading at the
bottom means unwritten. They're almost
always written up. They're mostly published and
the ones that are written up and unpublished were
in process probably. Whereas null results, 2/3 of them are never
even written up. They don't even enter into
a working paper series. You would never even
know about them. Statistically
significant results are 3-4 times more likely
to be published. Really distorting literatures if there's lots of
results disappearing. In field, and I could
show a bunch more and a lot of scientific
fields people have documented
similar patterns. But even in social
science recently, this is still an issue. Why are these results
disappearing? I think the consensus
it's a combination of journal decisions but also author decisions and we're
going to equilibrium. If I know journals
and referees like significant results and
they frown on no results, I don't know, I don't write them up,
I don't send them out, I don't want to waste time, I have other things to work on. Researchers' effort
gets directed towards more exciting results or more statistically
significant results. We're in this equilibrium where null results are never seen, of course, there's lots
of social costs to this. Maybe I'm carrying out
an experiment right now that someone else
did a few years ago, but I don't know because
it was never published. I've never heard of it. There's
all this wasted effort, wasted time. Of course, this could lead to inappropriate public
policy decisions. If people are relying on the
published evidence to make health care choices
or environmental choices or other things. The file drawer problem
is part of the problem, closely related to what I
was just talking about is author manipulation of results or what's called p-hacking. Meaning within
research projects, authors very often
have some flexibility, some degrees of freedom to choose regression
specification, to choose covariate adjustment, to choose the measure they
want as their outcome, to choose a subset of their sample there
are going to focus on, and across all these choices, get their p-value below 0.05. There's ample evidence of
this activity or really what we can really measure
is the combination of the author manipulation
and the publication bias. How this usually shows up is in published literature, you see big spikes and p-values, if you look at the
distribution of p-values, big spikes just below 0.05, and this pertains in
literature after literature. Here are two kind of
well-known examples by Gerber and Malhotra. One for the leading
sociology journals, one for the leading
political science journals. You can see on the
x-axis the Z statistic. The coefficient estimate
divided by the standard error. Of course, the 0.05
level is about 1.96. What you see in
this literature are huge spikes in the
number of studies just below 0.05 in terms of p-value in both of these fields. Again, this isn't a
natural distribution. There's no distribution that in the absence of publication
bias would have these very strange
bunches and spikes. There's some
manipulation going on. Just in case you thought
economics was immune to this. This is an example
of studies from leading economics journals
from about a decade ago. The Brodeur et al was
published in AEJ Applied. Again, showing big spikes
right at 1.96 and seeming like a canyon of p-values, not many 1.5s, 1.6s, 1.4s, but lots that
just get over the line. Again, probably due to some combination
of author activity and journal publication bias. It's not just our main
coefficient estimates, that's what it was just shown
that show manipulation. A more recent paper
by Brodeur et al, look at F statistics in IV first stages
were for a while, even though we
moved away from it. For a while, there was
a feeling that you needed your F stat to be greater than 10 to avoid weak
instrument problems, if you were overidentified, and so you not surprisingly
see a spike in the first stage test
statistic right at 10. Again, there's a
bunch of new papers showing over and over
again unnatural patterns. Even in recent papers. Actually, these are even more recent than the other
Brodeur et al paper. These are from a few years ago. Let me give you just a
couple of other examples, but I won't show
you figures from political science Lenz and Sahn got posted data, so data and code from papers
recently published in AJPS, American Journal of
Political Science, one of the leading journals in political science
and found among main coefficient estimates
40 percent were sensitive to basic
covariate adjustment. Again, it felt like
they're not saying all these authors
manipulated their data, but they're saying that
there was ample scope to do so or so it appeared. Then finally Snyder
and Zhuo have looked at 60 economics
journals and find that a lot of what
they call sniff tests, things like balance tables or overidentification
tests disappear. If you have an RCT and you look at balance across your
treatment and control group, five percent of the time, there should be a significant difference between those arms, even in a properly
randomized RCT. But actually, those of us who do RCTs
present to the world, there's way less than five percent significant
coefficient shown suggesting we're hiding
failed sniff tests. How can we correct for
this now that there's some evidence of this
publication bias? There's actually a very long
tradition of this almost as long as the tradition of
discussing publication bias. Rosenthal had what he called
his fail-safe N. Again, Rosenthal was the
person who coined the term a file drawer problem. He said, given a body of
evidence with let's say 20 studies that show
significant effects, how many nulls studies
would there have to be so that we'd change our mind about this
literature as a whole? That actually, this intervention, whatever
we're studying, probably doesn't have
any meaningful effect on participants. He came up with rules of thumb about how many
studies might be reasonable but of course, I think there's
been a lot of pushback against this view because
it's very hard to know how many studies
were never published. It's very hard to
know how many labs ran similar experiments. Maybe in certain
research fields, maybe certain branches
of science where there's only three labs in the world with a certain
piece of equipment. Maybe there you can
have a sense of how many missing studies there are, but not so in lots
of other fields. Again, I've cited a few
other examples if you're interested of folks
that again are trying to undo publication bias. The key in a lot
of these studies is the very particular quantity, which is the probability of publication as a function
of test statistics. The simplest case would be
significant non-significant. If we actually knew what the publication
probabilities were as a function of the z-statistic, we could in theory
undo publication bias. We'd have a certain distribution
of test statistics. We'd know that let's say z-statistics that are close
to zero or underrepresented, we'd weigh those up. Whereas those that have
larger z-statistics aren't lost to the
research community. Maybe we don't need to
weigh those up by as much. That quantity turns
out to be important. That's one way to go and I'll
come back to that later. The simplest test
though that people have used focus on something
even more basic, which is called the funnel plot. Maybe people are
familiar with this. The funnel plot just relates coefficient estimates to the
precision of the estimate. The reason why this
matters is imagine that we have precise estimates
and imprecise estimates. If there isn't publication bias, what we'd expect to see is both the precisely
estimated effects and the imprecisely estimated
effects would be centered at the same effect size. But the imprecisely estimated
effects would fan out. They'd just be really noisy. They'd be imprecisely estimated. We'd get this funnel plot. But if there is a
correlation between the effect size and the
precision of the estimate, its prima facie evidence for some publication
bias, in particular, it could be the case
that imprecise estimates that generate a favored result, whether it's a statistically
significant result or result that conforms to theory is more likely
to get published. Let me make this concrete. This is an example from a well-known literature
which I'm going to return to later in the labor
economics field. The literature on minimum
wage effects on employment. Pretty contentious field and some of the early
meta-analysis work in economics by Card and Krueger were very much focused
on exactly this issue. This is a more recent
take on the literature. Here, the estimates that are higher up because you
see on the y-axis there's one over the
standard error that are higher up are more
precisely estimated. You can see the precisely
estimated effects of minimum wage increases
unemployment are actually all
very close to zero. But what you see with the imprecisely
estimated effects towards the bottom of the
figure is true as expected. They start to fan out, but in
this really unnatural way. Only the studies that show negative employment effects appear to have gotten published. Whereas if there wasn't
publication bias, we'd expect symmetry here. This is consistent with a lot of publication
bias in this field. Other authors have
argued that there's publication bias and other
fields in economics, in the value of statistical
life literature, in many other literatures. If we don't see
many null results, we've already figured out a lot of the null
results disappear, how much confidence should we have in the published results? That's where we're laughed
and a lot of literature where most things published are statistically significant. This was the focus of
work by John Ioannidis, who's a very famous
medical researcher. Actually, this paper
is 2005 paper, is one of the best-cited papers I think the last 15 years, so definitely worth
a read if you're interested in this issue. He doesn't do anything
super-complicated, just applies Bayes rule the way others had
before and says, well, given what we know about
basic statistical parameters, how many results that are statistically
significant should we think of as false
positives or real effects? Again, if all they
were observing are statistically
significant results, this becomes very important. How much confidence should
we have in those results? He calls this likelihood
of a real effect, the PPV, the positive
predictive value of research. One minus the PPV is the
share of false positives. The expression here, again, you could derive
using Bayes rule, 1 minus betas statistical power. Alpha is our usual statistical
significance level. Ideally, we might want
to be powered at 0.8. Alpha might be 0.05. R is an important parameter. This is in a particular
research field i. What's the ratio of true relationships to null relationships
that are being tested? When a study is launched, how likely are we to really
reject the null hypothesis? If we're in a very
early-stage field, we don't really understand
much it's speculative. We might actually expect
R to be pretty low. We're trying lots of stuff. Most of these are probably
going to be nulls. Maybe in a more mature
research field, R is 0.3 or 0.5, or maybe even one if
there's even odds that sign we're studying
does have a positive impact. Then the final term in
this expression is u, which Ioannidis
calls author bias. This is the likelihood that a result that isn't
statistically significant is
turned statistically significant by authors due
to specification searching. Due to covariate adjustment. Due to focusing on particular
sub-samples or measures. Maybe even due to fraud. But all the ways in which results that shouldn't
be statistically significant are published as
statistically significant. Let me just give you
two examples here. In the simplest case, let's
think of a really good case. We're powered at 0.8. That's what we're told in
development economics, we should power RCTs too. Alpha is 0.05. Let's imagine this is a
pretty mature research field, so R is one. There's pretty even
odds that what we study we'll reject
the null hypothesis. Let's let u be zero, so there is an author
manipulation or bias or fraud. In that case, the PPV
is greater than 0.9. The vast majority of
relationships that are statistically significant are
meaningful relationships. That'd be a very good case and one where we can really believe that the science is credible. But there are other
cases that may be in some situations also realistic. Imagine that actually, our
studies are powered at 0.5 to detect an economically
meaningful effect. That's actually not that bad. There's a lot of studies
powered at less than 0.5. In practice, we maintain
five percent significance. Let's assume that R
is a little lower. Maybe one out of
three relationships is a real relationship. Let's test it, so R is 0.5, and let's allow author bias. It's hard to get a handle on, but let's say author
bias is 30 percent. Thirty percent of
the time there's a null result that
authors through, again, selective presentation
of results or data report as
statistically significant. In that case, PPV is about half. About half of all
significant findings are false positives. This is just a cautionary
point that through some simple examples
we might think that our published
literatures contain a lot of false positives. What should we do? One of the responses to a lot of these concerns has been a
push for more replicability. I'll talk about that
throughout the talk. It turns out that
lack of replicability is another problem that generates further concern
in the literature. I'm going to get to some good
news at some point, guys. I'm seeing a lot of down faces. But first I'm just going
to go through this. Computational
reproducibility is basically the ability to verify
published results, to download the
replication data and code, and basically generate
the results that are there. Two recent attempts
by Galiani et al, and Chang and Li show
that if you do this over the last few years for published economics
empirical papers, you can only readily reproduce results in about a third
to a half of papers. A lot of this really has
to do with the quality of the replication
data and code. Which is very often
of poor quality, it's poorly documented, there's
missing variables, etc. Even when you can get the data, very often you fail to
replicate the results. We have a long way to go. The new AEA data
and code policy, which you guys read about
probably in an email last week is an attempt to deal with
exactly this problem, to make sure at a minimum, the papers that are published
in the AEA journals achieve computational
reproducibility that we can verify those results. The data editor and a team that the data editor will hit
up are just going to check code before stuff gets published and try to make
sure that there's some basic documentation
in replication files. There is really an
active effort to improve practice
and economics here. I think the AEA is really, setting the tone
for other journals. In 2005, the AEA adopted an improved data-sharing
policy for the AER that ended up getting copied by lots of other journals in
the economics field. My guess would be
in a few years, a lot of other journals may
move in this direction, pre-publication
verification of results. But beyond simple verification, there's a broader issue
of replicability. This has really been
the focus of a lot of work in lab experiments. There's some famous examples
of replication efforts in psychology and in experimental economics
and I've cited them here. The idea is that scholars
attempt to replicate well-published famous
lab experimental papers using the same protocol, trying to have as
similar a population as possible to really see if
these studies replicate. If you try to do the exact
same thing in the lab, how likely is it that
you get the same result? The results here, I think most people would say have been mixed but quite discouraging. There's different definitions
of replicability and I'll talk about them in
a slide or two. But the headline results
from psychologists, fewer than half of
these well-cited, well-published papers
replicate and in experiments economics
was more like 2/3. Let me show you
the summary here, this is from Colin
Camerer's 2016 paper. He's the one who led the replication effort in
experimental economics. But he also brings
in the data from these social psychology
experiments. I want you to focus
on the two groups of bar charts on the left. One is, did the
replication study have the same sign and significance level as
the original study? All these original studies were significant at less than P, at P less than 0.05. In psychology,
relatively few did. Again, these were supposed to be quite well-powered
replication studies. In economics is more
like 60 percent. But even comparing the
relative effect size as across the original studies and the
replication studies, again, in more than half the cases you could reject
equal effect sizes, and in the economics literature about a 1/3 of the cases, you could reject
equal effect sizes. Effect sizes didn't
differ randomly, they differed in a
very systematic way, which is both in the
psychology experiments and an experiments economics work. Effect sizes and
replications tend to be much smaller than the original
published effect sizes. It's really asymmetric
and in many cases, and so what you can see
in this figure here, these are the papers
that were published in leading econ journals relatively recently, very
high-quality studies. The one here is the normalized effect size
from the original study. You can see with one
or two exceptions, that all the point estimates are less than one, and a lot
of them are just half. Some of them are zero. Again, trying to replicate
the same result doesn't necessarily lead to
the same finding. We went through a
lot of problems, lack of replicability, lack of computational
reproducibility, lots of evidence for
publication bias. Even in other social
science fields, pretty recent instances of high-profile fraud just
now in the last decade. This isn't 1950s anymore. What can we do, and what does
research transparency do? This is where we
start getting to I think some of the positives and where there has been
progress in the field. There's a number of areas where the meta-science or open science community has made progress. I think there's been a shift in the last few years
among people doing this research away from
just documenting problems, towards trying to figure out how we can improve
our literatures. It's a pretty exciting
transition in the field. Let me start with the first one. One of the things
that open data, basically, sharing of data in published studies
over the last decade has become the
norm in economics. But when I was in
graduate school in the '90s was incredibly rare. This change has actually
allowed us to better diagnose and maybe even
correct for publication bias. In a couple of different ways, I already talked about the
Franco et al study that looked at those time-sharing experiments in the
social sciences. They were able to figure
out what publication bias look like because they
got access to data. But probably in my view, one of the most important
papers in this area in the last few years is the
Andrews and Casey 2019 paper. Maybe some people are
familiar with it. It's forthcoming in the AER, It's really neat paper. What they do is
they actually get the replication data
from Camerer et al. What I just showed you, as well as the psychology
experiments, there's open science
collaboration. Having those two estimates
in hand allows them to quantify publication
bias in a new way. Which is really interesting. What they're very interested in doing and really
what I see is their key contribution
here is to figure out the likelihood that null results get published and do
that based on the data. Basically, what they
do is they have a draw of estimates from
an original experiment, they call that Z and from
the replication experiment, they call that Z^r. Again, the replication
experiments were meant to be the same design, similar population,
the same protocol. In theory, you should
be drawing from the same distribution here. They're going to look at
patterns in the size of Z and Z^r that could
really only be explained through
publication bias. Again, they'll get very
interested in this quantity, what they call Beta
sub p. What are the odds that a null
result gets published? Very important thing for us to do if we want to correct
for publication bias. What do they do? I won't
get into all of it, but they make a
very simple point which is an elegant point. Which really relies on the
symmetry that you should expect between Z and Z^r these are drawn from the
same distribution, then they should be
interchangeable. If you look at the
joint density of Z and Z^r they should
be symmetric. For every estimate where
Z is larger than Z^r, there should be one where Z^r is larger than Z on average. In particular, in cases
where the Z estimate, the original estimate is
statistically significant, but Z^r is not statistically significant there should
be the mirror image. There should be some cases
where Z^r is significant, but z is insignificant. What they basically
do is quantify the degree of asymmetry. That gives them a
non-parametric and pretty elegant measure
of this P function. They're in particular are
interested in p of a, for odds that a significant
finding are published, and p of b, the odds that a nonsignificant
finding are published. Now, they make a convenient
normalization, which is, they assume p of a is 1
and they're going to get the relative publication
probability for a null result, relative to a
significant result. That's really going to
be their goal focusing on asymmetry in
this joint density. Actually, if you look
at their results, and let me just go through
a couple of these panels. I'll start with panel a,
which is something that we looked at before on the left, this is just the distribution
of test statistics. W here is the absolute
value of the Z stat. Again, this is the experimental
economics literature. Papers published in
top econ journals, experimental economics
papers published in top econ journals in the, between 2011 and 2014. You see a huge spike at two. This is our first
flashing signal that maybe there is some
publication bias here. The middle panel, those
with they focus on so to the right of the gray
vertical line is a, to the left is b. That's nonsignificant. Then the x-axis is the
original estimate. Again W, the absolute
value of the Z stat. The y-axis is WR, the replication test statistic. What you can see is there's a ton of
test statistics just over the line at two in
the original studies. But when you replicate them, almost all of them
are not significant. By symmetry, we shouldn't
expect to see that. We should actually
expect to see a lot of mass in the top left here too, not just in the bottom right. This suggests there's tons
of missing studies that basically, there's a lot of null results that
were never published. Actually, their best estimate, which I found
shocking when I first saw it is that Beta
sub p is only 0.03. Their best estimate
is null results. Significant results
are 30 times more likely to be published
than null results in experimental economics. Shockingly, large number. Larger than what Franco et al, Turner et al those earlier
estimates showed you, which was like three
to four times. If this is true, it's just
absolutely shocking number. What they're able to
do once they have this function, P of z, the probability
of publication as a function of the test
statistic is they can actually correct
estimates and come up with what they see
as adjusted estimates for all these experiments. That's the black
line there which falls way to the left
of the blue line. Other than for the
original studies that were very highly statistically significant
to begin with. The studies that had a Z-statistic greater
than three to begin with, tend to replicate pretty
well is if you look closely that previous figure and there's really no adjustment
because they replicate. It's an interesting
point that being marginally significant
correlates with lack of replicability in the experimental
economics literature. One last point from
Andrews and Casey. They develop a second method
to back out Beta sub P, the probability of null
results gets published. Here again, they look at the
minimum wage literature, which has been the
subject of so much energy in labor economics. Now, the right-hand panel
is like a funnel plot, but instead of higher
values being more precise, which is standard
they presented where higher values are less precise, and instead of positive values being a positive
employment effect, they decided to make that a
negative employment effect. That's what they
did here. But if you flip this and flip it again, what you'll see is
it looks a lot like the plot we just
discussed before, where there's a lot of very
precise estimates close to zero. Then you see there's
all of these estimates. That gray line to the right is the line
of Z equals 1.96. There's just a whole collection of studies right on that line, you see the associated spike
in the plot on the left. There guess is the minimum
wage literature, again, using a different method
than previous authors, is just riddled with
publication bias. It's a little less
stable, their estimates, but they think
significant results are 3-100 times more
likely to be published, 100 seems like a lot to me. Three is very close to what some of these other recent
audits have found. The bottom line is, now that we have open data, we can use new
methods to diagnose publication bias may be
correct for it and it's given us a handle and new
pieces of evidence that in economics journals
in the last decade and in top journals, there appears to be if we
believe these estimates, substantial publication bias,
this is still a problem. Good news. Let's start
getting into some good news. This was bad news. What does transparency do? What are new methods do? There's some new evidence that certain types of methods or studies that use certain
types of methods tend to have less publication bias. There's some work, this is the Brodeur
et al 2019 again, that present plots
like I showed you before with the distribution of p-values where there
was a spike at 0.05 and shows them for
different types of studies. They have 25 leading
economics journals between, I think 2014 and '17. Again, very recent data. One of the takeaways is field experimental studies
tend to have less of a spike at 0.05 and many more null results
and other literatures. Here again, there's a lot of
data and I think you're now familiar with these
distributions of p-values. Maybe for simplicity,
just focus on IV and RCT in the corners. You can see in published IV
studies in econ journals. Again, there's a huge
spike right at p of 0.05 and there's relatively
few null results published. If we then compare that
to the distribution of test statistics
from RCT papers, there's still a little
bit of a spike at 0.05, but much less and there's
lots of null results. There are significant
differences in the distributions
across these methods. Now look, there's a lot of differences between
these studies, different samples,
different areas. If I had to guess as
to what's going on, I would guess that a lot of the field experiments
which dominate the RCT been here and I think there's a lot of development people in the room. In development to work
on a limited number of large experiments, we put a lot of time, maybe
years into one experiment. There's a lot of money,
there's partners that want to see results. I just think that translates into a lot more results
getting written up, even when they're null results relative to methods that may not have as much of a time cost. But I really do think the time
cost is a big part of it. I just showed you in the experimental
economics literature, they do RCTs too. In the lab, there seems to
be tons of publication bias. It doesn't necessarily seem
to be the method per se, because even lab experiments and field experiments seem to be on opposite extremes in
terms of the extent of publication bias in the
recent economics literature. But there is some hope here that certain types of studies
have less publication bias. Number 3, study registration may increase the reporting
of null results. The evidence here is a little
weaker, it's suggested, but let me walk you through
what we've learned so far. I think people know that
the AEA Registry has been around for about six years. Now they're similar tools in psychology and
political science. Not in sociology yet. That's one major social
science field that hasn't had the same open science movement or awareness of these
issues I think so far, even though there's some
individuals like my coauthor, Jeremy Freese has been pushing things forward in sociology. There were a couple
of early studies that use pre-analysis plans, I think Finkelstein et al and the Oregon
Health experiments, a famous case, and
then Kate, Rachel, and I wrote a paper in development. I
just want to give a little background of how the value I see in
pre-registration, how our own experience
shaped that in our paper. There's no way you're
going to be L3, the coefficients here.
But in our paper, we were working in a setting where we were studying a
development intervention, community-driven
development that supporters of the intervention
thought would change local institutions, would change local
social capital, change local politics, were a whole range of
local outcomes that people thought could be affected by community-driven development, which is an attempt
to really democratize village government in some
ways and be more inclusive. When we looked at the
possible outcomes we can measure and
that we did end up measuring, there
were many of them. There were a couple
of 100 outcomes we could focus on and
we were very concerned upfront about what
this meant for what we would present and what
donors would be interested in. That's when Kate and Rachel had a great idea to write a
pre-analysis plan and try to pin down the analysis a little
bit more in advance. When we pin down the
analysis and look at all the data in the fairest
way we could think of, we get the result in Panel A here, which is a null result. Overall across these
institutional measures, we don't see much change. There was no significant effect, we get pretty precise estimates. But what we were able to do in subsequent analysis
and a published paper is basically cherry-pick
a subset of outcomes. Again, when you
have 200 measures, some are going to be
significant by chance. Cherry-pick positive outcomes, cherry-pick negative outcomes, and write up a
coherent story for why community-driven
development could have had positive effects
or negative effects. I think seeing how
easy it was to do that exposed really converted me to thinking that these
issues of research, transparency and pre-registration
are very important. I wonder how many
published papers that I've read when there wasn't
a pre-analysis plan, when I can't check the survey to see what questions
were there, when I can't check the data. How many papers that I've
read are really just Panel B? Really just
cherry-picked results, and I think that's
something that really motivated my own
interest in this issue. I think it's still
early to assess the aggregate effects of
registration in economics. There is some work
underway trying to do so by Georgia Ofusu
and Dan Posner. They're looking at
registration in both economics and
political science and trying to get a handle on what
is evolving in our field, but the one thing
that we know for sure is that registration's
really taken off. In the first six years
of the AEA Registry, now the total number of
studies registered is 2600, that's as of May when I just
checked a couple of days, but I think it's 2900. The pace of registration
is going up over time. If you search the database, you'll find 34 percent of registered studies have
posted a pre-analysis plan. Some of the studies that are on the registry are
actually older studies. A number of research
organizations have been encouraging their members
to post on the registry, including NBER,
who's been certainly encouraging affiliates
to post on the registry, including the Poverty
Action Lab, etc. There's a bunch of older
studies that are posted on the registry from the era before there were
pre-analysis plans. Among more recent studies, the share that have preannounced those plans is
almost certainly higher. When I searched on the
database over the last year, it seem like among newly
registered studies in the last year was
about 40 percent had pre-analysis plans. The rate of adoption of pre-analysis plans
is also increasing. We don't yet know what pre-registration has
done in economics, but we can learn something
from the experience of other fields, in
particular medicine. A lot of what's gone on in economics over the past
couple of decades. The adoption of RCTs was definitely inspired
by medical trials. The adoption of the registry was inspired by
clinicaltrials.gov. They've had a registry
for about 20 years and there are a number of benefits that have
been documented in medical research and clinical trial research from
having a registry. One of the first things
is now that there are pre-analysis plans and registration documentation
in medicine, it's possible to see how
published papers deviate from original plans and a number of studies have done those
audits in medicine. That's something
that can immediately provide some accountability, but the existence of
the registry also appears to be leading to the reporting of
more null results, and this is very
important again, no results seem to disappear
in field after field. Anything that leads
to the reporting of more null result is almost
certainly valuable. This is from Kaplan and Irvin, and so it's a paper
published a few years ago. What they have is a
vertical line in 2000 when clinicaltrials.gov
was founded. Now, this isn't the
perfect research design because other things
change over time, but this is one
literature on, I think, nutritional supplements
that were funded through the same NIH funding arm. There's some commonality here and what you see is
in the early days, lots of significant results. Those are the pluses, not
too many null results. It looks like there's a bunch of pretty weekly powered studies because there's a bunch
of null results with huge relative risk reductions
on the left-hand side. After clinicaltrials.gov
was founded and Medical Journal said to be published in our journals, you need to register
on clinicaltrials.gov, that was their policy, all of a sudden, we see lots
of null results. A couple of significant results, but a real change in the proportion of null
results published. This is pre-post, but I think if you talk to
clinical trialists, they would recognize
the value of that clinicaltrials.gov
has played. In the earlier era,
lots of trials, including trials funded by self-interested
pharmaceutical companies just disappeared all the time and it's almost
impossible to do that now, it's much harder to do that now. I think the hope is
with registration and economics and other
social science fields, we will have more null
results published and have a more representative
view of the literature. What's another set of policies
that appear to be again, there's some evidence
could be influential? I think we all recognize that if we're going to change practices
in our field, we're going to need
cultural change, norms will need to change. There's probably no set
of organizations or institutions more
important for changing research practices
than journals, because they publish, we need to publish
for our careers. What journals do can really
influence the whole field. There was a really neat
experiment that was carried out a few years ago in eight
health economics journals. The editors of those
eight journals issued an editorial
statement. Emphasizing the importance of
publishing null results and sending reminders to referees when they were asked
to review papers, to say, judge this paper based on its question and its quality, and not just on whether it's statistically significant
in terms of results. Really basic, really
simple, but again, since it cuts against
the grain of publishing, a signal that maybe we're entering into
a new equilibrium. Blanco-Perez and Brodeur have
a new paper which is now forthcoming in
economic journal that studies this natural experiment. They compare these eight
journals to a couple of other applied micro
non-health journals, and they look at the share of published null results over
the last four or five years. Again, not the
perfect experiment, but a diff-in-diff that
we can understand, and that gives us some signal
as to what's going on. What you can see
here is they have a pre-period and then the period of implementation
and then a post-period. All of this again is just
over the last five years. The light gray line are
the control journals. I think it was Journal of Public Economics
and Labor Economics, those were the two journals. Then the dark gray line are the eight health
economics journals. In the pre-period,
they're both publishing actually about 50
percent null results, 50 percent significant results. But then there's this big
drop in publication of significant results or a
rise in the publication of null results after the
editorial statements. It's pretty interesting
to think about saying that's pretty low-cost, pretty easy to do. But if the editors of
leading journals and economics got together and put out a statement like that, I think it could help promote cultural change in the field and start shifting expectations. Let me get to my
last two sections. I'm about, I think an hour in, and I want to talk
about innovations. Again, some new
developments in the field. I'm going to focus mainly on pre-registration where
there's just been really fertile lots of activity and lots
of developments, and then at the end I'm
going to close with some new data and
some takeaways. We've already discussed
pre-registration and pre-analysis plans a little bit. I think we've talked
about the benefits of pre-registration. If I register a study and
it never gets published, at least it's registered,
someone can find me or find the study or
find that there was someone who worked
on a question. It opens up the possibility of filling in gaps
in the literature. I think the paper
trail is one of the most important uses of pre-analysis plans
and pre-registration. The existence of a registry
and plans almost certainly constrain the extent
of bias reporting. There's going to
be a referee who looks at the plan and says, "Wait a minute, you said your
three main tests were 1, 2 and 3 and they're
nowhere to be found or you did it really
differently, explain why." So there's some accountability. There could be very
good reasons why scholars chose not
to publish those three specifications, but they'll have to explain why they decided not to do so. Another benefit is
Point 3 here in generating correctly
sized statistical tests if authors have had
access to data and they've run scores or
hundreds of tests, and then they end up
publishing one result. I really don't know how
to judge that p-value. I don't know what
it means because they've run so many
hundreds of tests. If I pre-specify what
I'm going to do, then you know what
that p-value means. Even if you run multiple tests, you could use a multiple testing adjustment to account for the fact that you did test
several different things. In my own mind, I think Point 4 is
actually pretty important. I think having now written a number of pre-analysis plans, being forced ex-ante to really
carefully think through every step in a research paper
and maybe not every step, but a lot of the steps
in a research project, can strengthen the design, can highlight gaps
in what you're doing and ultimately lead
to better quality research. That may be hard to
quantify in the long run, but I do think it's
something that I've observed. These are
some of the benefits. There could also be cost
to pre-analysis plans, and I think Ben Oakland has
written an important piece, and others have
also commented on the cost side of
pre-analysis plans. Writing a pre-analysis
plan takes time, and maybe takes
time that you would spend on something else. There may be a trade-off there
in terms of time spent on a pre-analysis plan versus
other research activities. One of the things that
I've found now that I've done quite a number of pre-analysis plans over
the last decade is pre-analysis plans
certainly take time. But they take a lot
of time upfront, and they do speed up the research process
on the backend. Once the data comes in, there's a whole set
of analyses that are basically ready to go
that are core analyses. That doesn't mean it's
the end of the paper, there's still a lot of
work to be done and going beyond the plan or
understanding robustness, etc. But the time cost is
really a movement in time rather than
necessarily fully additive. I do think overall, it does take more time to
write a pre-analysis plan even when you factor in
the time savings later on. But the time costs aren't
as painful as they appear to be upfront because you end up
saving time later. Another question has to do with the length of
pre-analysis plans. How detailed should they be? Should you try to specify
everything from A-Z or not? I think my sense, again, having written plans with
different coauthors, having refereed papers and
seeing pre-analysis plans, is there really is a lot of variation in the types of
plans people are writing. Some are really detailed. I tend to write pretty
detailed pre-analysis plans, some are less detailed, but my own personal view is even a relatively sparse
pre-analysis plan that lays out primary outcomes, core analysis and core questions that researchers are interested
and have a lot of value. There's a lot of value
just in that plan. If researchers want
to go beyond that, I think it could also be useful. But you get a lot
of value even from a relatively tight
pre-analysis plan. That's my own take
on the literature. The third point that's
often raised about pre-registration and
pre-analysis plans is, will the rise of
this new approach mean we lose flexibility? We get our data,
we find something interesting that we didn't pre-specify or that
we forgot about, and then we never
publish that because you can only publish
pre-specified stuff. I would say that is certainly not the case in my experience, every paper that I've written and published that uses
a pre-analysis plan, has had analysis that goes
beyond the pre-analysis plan. I think every paper
I've refereed, every paper of students of mine who had pre-analysis plans, have gone beyond the plan, and as long as folks are clear about what was pre-specified
and what wasn't, I think this is a good thing. We can learn more from our data than we
thought of ex-ante. We learn things
in the field with their patterns and the data
that are very interesting, that should be explored
as long as we're clear about what was and
wasn't pre-specified. I think norms and
development economics at least are evolving in a positive direction because flexibility has been
baked into the process, and I haven't seen
much rigidity, but we'll see how things
continue to evolve. That's just an
overview on where I see the pre-registration
discussion right now. I want to talk about
four innovations. I'm going to talk about
1, 2, and 3 quickly, and I'm going to spend
more time on Point 4, and then I'll move
into the conclusion. The first, what I
call innovation, is forecasting study results. One of the critiques that
or justification for not publishing null findings that you sometimes hear as well. I write a paper I presented, I get a null result
and people like, "Well, that's boring. We knew that," like it was crazy to think that that was going to have any effect anyway. That labor market intervention, anybody could have
told you that, that was going to
be a null result. But did we really know? It turns out people have been trying to investigate this. DellaVigna and Pope
had been collecting expert forecasts in
experimental economics, trying to figure out what
people really can and can't predict in the lab. They have a couple of
really interesting results. They do find that when you take the forecasts of academic
experts together, you take the average
of forecasts. There is signal there. They actually can predict results in the lab pretty well. But there's tremendous
variability across individual scholars, and one of the really
neat findings is more senior scholars who
are better published, more senior in terms
of academic rank, are no better at
predicting results than graduate students
or more junior scholars. It isn't like there's
folks who really know the literature who
just get it spot on. There's really a
lot we don't know, and I think the forecasting
work has made that clear. Actually, collecting the
research communities' priors on a certain intervention
could be very useful so when we get results, we can really figure
out what is new, what have we learned versus
what did we already know? Right now, Eva Vivalt, Stefano, and the team at BITS, the Berkeley initiative
for transparency and the social sciences have funding to build out
a forecasting platform, and the idea is
maybe in the next. I think several people and with some work that Rachel and I have done, we've done forecasts, others, con camera, and
others, the idea is, in the future, before we do an RCT and development
in a few years, it may be standard
to both register our pre-analysis plan
and just as a matter, of course, gather priors
from experts in the field so we can understand what people's forecasts
are about results, and then we'll know
how much we're updating when we get
a set of results. That's the vision
for the next step on forecasting that's
already a few steps in. Another innovation is just not
pre-specifying tests, but pre-specifying
the research process. There's been a couple of reasons that have
motivated this work. One is I think all of us who've written
pre-analysis plans have realized we forget
things, we miss things, we make mistakes, or we
just don't have the data in hand to make certain
decisions ex-ante, so we leave things half done. There have been a couple of approaches to try to improve on a pure pre-analysis plan to let us learn from data ex-post, but learn in a disciplined
way that's pre-specified. There's actually a
couple of approaches. The first is a
hybrid approach that uses split samples where there's both a exploratory
sample where you data mine results that are then tested on a confirmatory sample. I'm citing Anderson
and Magruder here, but Marcel [inaudible]
who's also here has a very closely related
approach as well. There have been a few
approaches: Marcel's, Anderson, and Magruder to pre-specify
split sample approaches. It's pretty interesting that
we can learn from data. They actually examine the data that Rachel
and I have from Sierra Leone on
community-driven development using this approach and found several dimensions of
heterogeneity that are significant that we didn't specify in our pre-analysis plan that are pretty reasonable
and pretty interesting. There really is a lot
that one can learn here. A related approach to learning from data ex-post and
not tying our hands too much ex-ante is to pre-specify machine
learning approaches. For instance, when
we run a regression, we may want to do the right
covariate adjustments, to control for the right things, have more precise estimates, but ex-ante we don't know
what to control for. Machine learning
tools are well-suited to deal with that
problem, so recently, Ludwig, Mullainathan, and Spiess have proposed pre-specifying machine
learning tools. This is something that actually several of us have done in pre-analysis plans
the last few years I see Johannes is here, so in our project
with Michael Walker, Dennis Egger, and Paul Niehaus, we pre-specified covariate
adjustment using machine learning in a pre-analysis plan a
couple of years ago. There are ways to enrich
pre-analysis plan, so they're not as
rigid and we use the latest statistical tools. We just pre-specify them. I think this is an
exciting direction. A very big open question to the literature is what we're going to do about
observational work? So far a lot of what
I've been talking about has focused on
either lab experiments or field experiments
where it seems pretty straightforward
to apply a lot of these ideas, let's
say pre-registration. What about observational work? Fiona Burlig, I think
I saw him in the back. Fiona wrote a paper recently first documenting that
the large share of economics papers remain
observational even a couple of decades into this
whole RCT revolution. Also making the case for how the approaches we
use for experiments can quite straightforward
way be applied to prospective
observational studies. Observational studies where
you don't have the data yet. There's a survey around that
hasn't been released yet. There's an election that
hasn't happened yet. You literally can't data mine, but you can pre-specify
what analyses you want to do when that
election happens. In fact, in some work going
through existing registries, the work by [inaudible]
and Posner, they actually found
that four percent of pre-analysis plans that they found on the AA registry and the EGAP registry were registered prospective
observational studies. There's been at least a
couple dozen of these now and I think it really
is a potential growth area. We can apply these tools
in a straightforward way. There is a difference though, between perspective and non-perspective
observational studies. Non-perspective
studies are studies where the data is already
out there and exists. People have been looking at it maybe you've been looking at it, and the question should those
analyses be preregistered? I think it's quite
challenging to do so because it's
really hard to verify what data access authors had
before they wrote the plan. We might enter a world where people data mine and then write a plan ex-post and it would contaminate our thinking about what pre-registration
really is. Now in health some
leading scholars, and this is the
Dal Re et al paper and one of the co-authors,
you need this, is giant in the field
of medical research, have been arguing for registering non-prospective
observational studies. But I think in the social
sciences there's really less support for that and it
feels like a big challenge. Anybody who can think
of ways to bring these ideas into
non-perspective work, it could be very valuable. Let me talk about what I want to focus on for the
next 10 minutes or so which is pre-results review. There has been a
growing trend in other social science fields
around pre-results review, basically reviewing papers
without knowing the results. This style of review is sometimes called
registered reports. This is something that's
really taken off in psychology and
cognitive science. It's actually been
one of the really nice benefits of the BITSS, the Berkeley Initiative for Transparency and Social
Sciences Community, is we bring together
scholars across five or six different
disciplines a couple of times a year and we learn everything that's going on in
other social sciences. This is saying that's taken off. I think it's pretty attractive conceptually to say, "Hey, we can judge research quality
based on design, data, the importance of the question, and publish it even if
it gives null results, even if it generates results
that go against theory." Sometimes results are seen as contradictory or they
contradict theory. They're controversial
as a result, but those actually may be
the most important results to publish, maybe
the theory is wrong. You could see a lot of value in publishing these results. Again, you could
ask yourself are we really prepared to
evaluate these papers? Well, we actually evaluate things all the time but
don't have results. Any of us that serve
on NSF panels, NIH panels that decide which graduate students applying for funding
are going to get travel awards or
basically deciding based on the question and the proposed data collection
like what should be funded? What's valuable? We're
already doing something similar all the time. The way review works
and again this is based on some templates developed by the Center for Open Science
that we've been modifying, and I'll talk about
the endeavor at the Journal of Development
Economics in a second is as follows, instead of the main review where referee reviews of paper here there's two
stages of review. There's a first
stage where authors submit something that
is called a proposal. It's really similar to a pre-analysis plan
but with more detail, conceptual detail, detail
about the literature, an extended pre-analysis plan. That is reviewed by referees and there could
be some back and forth, something like a revise
and resubmit with the author of that point
to clarify certain things. If the referees and the
editor decide that, that is a very valuable study
that should be published, it's of the caliber that should be published in a journal, it gets what's called an
in-principle acceptance. An in-principle acceptance
is a little bit like a conditional except,
something like that. Then the authors go back, collect their inline
data, write up results, and then submit the full paper for what's called
stage 2 review. This is the full paper. The idea here is the journal has committed to
publishing this paper as long as the results
are presented credibly, as long as interpretation
is reasonable, and as long as there
weren't major data problems along the way like I was
going to write this paper but there was a civil war in South Sudan so I can
never collect the data. That paper isn't going
to get published because there just
wasn't the inline data. But if the inline data looks
to be of good quality, interpretation is reasonable, the journals committing
to publish it. That's the idea. It turns out, and this is just a figure
from some recent work by Hardwicke and Ioannidis, they basically track, look
at the left-hand panel, the spread across journals
of registered reports. You can see in 2013 no
journals were doing this, and then especially
psychology journals, and neuroscience journals, and a few medical
journals have just started adopting
it very quickly. As of early last year, there were about 100 journals. By today, there's over 200 journals that are
actually accepting registered reports including most leading psychology journals and neuroscience journal. This is an article
format that has really taken off really just in a few years I think because
it is so attractive. Is there a precedent
for this in economics? Actually, there is. There's a very
interesting precedent. The earliest example to my knowledge of a
pre-analysis plan and pre-results review
in economics is a paper published by
David Neumark in 2001. According to my colleague
at Berkeley, David Levine, who was editor of the Journal of Industrial Relations
at the time. The late Alan Krueger
had an idea in 1996, in the middle of these heated minimum wage debates
between Card, and Krueger, and Neumark,
and other people, Alan Krueger had
the idea of let's write down specifications
we agree with, pre-specify them, and then run analysis after the next
minimum wage increase. He had this idea in 1996. In '96 and '97 there were
minimum wage increases. That was back in the
era where we still had minimum wage increases
in this country. Data was released. Now it turns out that
Card and Krueger ended up not participating in this activity, but
David Neumark did. He shared his analysis plan with David Levine who
committed to publishing the paper as long as it was interpreted reasonably,
all these conditions. The timeline was the
idea came out in '96. Neumark participated by sharing his analysis plan in '97, there were minimum
wage increases, data was released
and then eventually the paper was published. There actually is this precedent for something similar in a very charged literature
where the idea was if we can agree ex-ante
we can make progress. David Levine said the
original idea for this, he thinks actually comes
from Danny Connorman. Danny Connorman in a number of experiments with
psychologists in the '80s and '90s developed what he called
adversarial collaboration. He would reach out to folks
who disagreed with him in literature and said we
disagree about this effect, let's work together and
design an experiment that we both agree is the right experiment and
write it together. He did this numerous times. Danny Connorman's idea was the inspiration for the Krueger, Levine, Neumark case. This provided inspiration for the Journal of
Development Economics led by Andrew Foster, Dean Karlan who are
here with support from BITTS to try out previous results
review and economics. The journal
development economics seem like a natural place to do this because
experiments are widespread, pre-analysis plans
are widespread. This is all very well
charted territory in development economics
and staff from BITTS putting in a lot of
work to develop answers to frequently asked questions and how to guides for referees, like, what is this new thing
I'm being asked to do? Also, BITTS staff ended up interviewing a number of
the authors and referees just along the way to
make sure there were no problems along the way. After the first
year, there's been quite a positive response. There have been about
46 proposals submitted. I'm not sure if it's gone
up in the last week or two. Five of those have already received
in-principle acceptance. They're conditionally accepted. They're out in the
field getting there. They're inline data and then
they're going to submit their stage 2 submissions
to be reviewed. Nothing is yet fully accepted, but there's a number on the way. I think there's another
dozen or so that occur more that are
currently under review. In the interviews that staff have had with
authors and referees, no real red flags
have been raised. A number of referees have
found the process reasonable. There are some questions, but it wasn't totally
foreign to them. A number of authors
have found it useful. One of the things that's a
theme that has come out is a number of more junior
authors have found it useful. Authors who are going
on the job market going up for tenure, trying to get promoted, trying to get the job, etc, find it really useful
that in a few months, even before they've say finished collecting
their inline, they can get an
in-principle acceptance from a really good
field journal, like the Journal of
Development Economics. There are some values
for the community. Recently, Andy and Dean,
and other editors at the JDE have made pre-results review now a permanent
submission track. This will be a normal
way to submit papers to the JDE and
experimental economics, so leading field journal and experimental economics is
launching their own pilot. The idea is spreading. In my view, bird's eye view, I think there are some
other journals that will be natural fit to, at least, try out
this article format. I think a journal
like AR Insights, which is already pioneering
new article format, shorter article formats
that are more similar to scientific journal articles, could be a natural place within the economics
profession to try out a new format
like this, for example. Let me just wrap up in the next 10 minutes or so and then open it up for questions. I want to present a little
bit of new evidence that we have on changes in
social science. I've been working with a number of co-authors at Berkeley, at Princeton, at MIT, and then Garrett who's
now at the Census Bureau, collecting data on open science
attitudes and practices across the four larger
social science fields, which are economics,
political science, psychology, and sociology. We attempted to get a
representative sample of scholars who had published in top 10 impact factor
journals in these fields and also a representative sample
of graduate students at leading North
American departments. In all these fields,
we contacted about 6,000 scholars last year. We got a response rate of
close to 50 percent so, of course, we'd have
loved it to be higher, but we're getting, at least, some basic representativeness
across these fields. One of the things we're finding is over the last few years has really been a rapid
change in attitudes and a rapid rise in
adoption of new practices. Let me just show you a
summary figure here. This is data restricted
to scholars who have had their Ph.D. for, at least, 10 years. We're already research active as of maybe around 2005 or '6. This is their own
self-report on when they first used a certain practice or engaged in a certain
practice that we could see as a
research transparency. Posting data has been increasing
dramatically over time. At this point, the majority of scholars across fields
have done that. Posting study instruments or other study materials has
been increasing over time. I think for lots of us
who do field experiments, it's normal now to post our surveys online,
for instance. Pre-registration was very
low until a few years ago, there weren't even registries, and now is starting
to increase a lot over the last few years. The shading is our attempt
to deal with non-response. We did some auditing. We actually went online
and tried to search for whether those who didn't respond to the survey engage
in these practices by searching data verse,
registries, etc. The shading is a range of responses taking into account our
non-respondents behaviors. But you can see similar
patterns across the board. How do different fields fair? I think everybody likes
to compare themselves, maybe to different disciplines. The top left here is economics, political science,
psychology, and sociology. You can see in economics, there's been a big
increase in data-sharing. Almost all empirical economists
now are posting data. It's almost universal. Again, in the last few years, there's been a rise
in pre-registration. That's the orange
line at the bottom. Political science and economics look very similar in terms of the adoption of data-sharing, sharing instruments,
and pre-registration. Psychology and sociology are
really interesting cases. Psychology according
to these measures was really a laggard
until a few years ago, very little data-sharing, very little posting
of instruments. They really didn't have a
traditional data-sharing. But because of the crisis
that I talked about, there is multiple
instances of fraud, the feeling that the majority of published findings
were false positives. There's been huge
change and you can see now over the last few years, very rapid adoption of a lot
of practices in psychology. Sociology is the outlier. There has been
gradual increases in data-sharing and a
few other practices, but there hasn't really
been an identifiable, as I mentioned before,
open science movement in sociology in the same way. Part of the difference
between these fields is really driven by
research method. This will be my last result
and I'll start wrapping up. We broke down scholars across disciplines into those who
mainly do experimental work, quantitative,
non-experimental work, and folks who do more
qualitative or theoretical work. Some theoretical economists
or people that do macro theory will be over there. A lot of sociologists
are in that right panel. There's a lot of
empirical economists and political scientists
in the middle panel. Then you have tons of
psychologists doing experimental work and some
economists and psychology. But the thing you can
see is definitely among experimental
social scientists, there's been very
rapid adoption of these practices in
the last few years. We're going to run
another survey this year, extending things out. A year or two and
if trends continue, we might see maybe the majority of experimental
social scientists, for instance, have
pre-registered. But that's the thing we'll
collect in the field. Just one last bit of data
before starting to close. We also asked people
about their attitudes and their views about others'
attitudes in this survey. This is a little bit like that first survey I showed you. The Anderson et al where people thought they were
doing the right thing, but no one else around them
was doing the right thing. We found some
interesting patterns. One of the things that was pretty striking for me, I think, we have 70-something development economists in our sample. Over 80 percent of them support
pre-specifying analysis. I think 70 percent said
they strongly supported pre-specifying analyses
and other like 10 percent moderately supported
and then there were a range of views from there. I found that pretty interesting. That idea has become
quite widespread, at least, among those who
responded to our survey. But what we find is when we ask people about others' attitudes, we find this lag or
this difference. A lot of people claim to
support open science, but think lots of their
colleagues may not as much. We're going to figure out
over time how much of that is exaggeration by respondents or whatnot as we track actual
behaviors over time. Let me move into wrapping up. I just have a couple
of takeaways here. The open science movement,
the move towards research, transparency and
reproducibility is, in my view, really
logical for us. We've adopted improved methods. We've adopted a
bunch of practices like data-sharing that have begun to improve economics research and
social science research. I think the research
transparency agenda and the practices I've been
talking about today can improve the credibility
of research and they also resonate with our values as researchers
and scientists. I think they really tap into
a deeper sense of how we should do our work that a
lot of us can believe in. As I just showed
you in the data, the change is already
well underway. Another takeaway I
want to mention is, there's all this recent
evidence about how important a problem
publication bias still is. In economics and other
social science field even now in the last decade, it is an ancient history. There still is a lot
of publication bias. We still have a ways to go. Our literatures may not
be as reliable or as credible as we need them to be, but the good thing is now
meaningful change is possible. There are a lot of new
tools on the table. There are a lot of pilots like the pre-results
review pilot being undertaken and things like editorial statements
or promoting replication that can move
us in the right direction. I do think in the coming years, we are going to more and more
evidence about hopefully, what works in this space to generate more credible
bodies of evidence. The other thing that we're
going to need to sustain all these changes really
is cultural change. We're going to have to change
our mindset about what is worthwhile results
or worthwhile project. I think all of us who train students or talk to
our colleagues, etc, should really encourage others to work on really
important problems, really hard problems,
make progress on those issues and not worry as much about the results being exciting at
the end of the day. Because if you're
collecting good data and working on important problem, even null results are
really important. I think that's the
cultural change that we're still far away from, but we need to move
towards. Thanks. 