Joseph P. Ferrie: It's
in every respect, teamwork with a large cast of extremely indulgent
co-authors who have allowed me over the years to
just keep on thinking of different ways to merge
different stuff at every turn impeding my progress on the things that I
had promised them but eventually, coming up
with something that I think is hopefully useful to
the larger community. Second thing to note
is that the 1200 on the front slide is
not actually a typo, will actually start
in roughly 1215. With an example of just how wide the range of sources
are that we can now subject to the linking and matching algorithms
that I'll be talking a little bit about and that Martha will be talking about in even greater detail. For those of you
who don't remember your canon law history. In 1215, the Lateran
council among many other things
decided that you could get married to your
fourth cousin but not to your third cousin and anything further out than
fourth they didn't care about. But they also put into place a very elaborate procedure
that allowed people to seek dispensations from
this aspect of canon law. Those dispensations
began a long trail of paperwork that starts with an interview with the
local parish priest, goes up to the archdiocese and then from there all
the way onto the Vatican. Where a decision is made, paperwork flows in the
opposite direction and eventually after
a period of time, sometimes a matter
of several years, you'd find out whether you
could marry your second, or third, or even first cousin. The idea here was that
the exception was to be granted in
extraordinary circumstances. Among the circumstances
that they thought extraordinary
enough to allow these violations of canon
law with situations in which it was necessary to avoid further subdivision
of the family land. What's nice about this
is that it gives us an important piece
of information about population pressure on the land. But for the purposes
of this lecture, what's nice about
it is that it's got a long paper trail and that the church entertains
a lot of these requests, hundreds of thousands of them, over the course of the eight centuries
from 1200 down to 2000. The Catholic Church,
like my parents, never throws anything out. That means that this stuff
is in church archives all over Catholic Europe even today and it's very
easy to access. Here's an example. From the Diocese of Gerona, which is just north of Barcelona on the Spanish
Mediterranean coast. What you can see here
is one page from the extensive dossier that's generated for each
one of these cases. The first thing that shows
up is a family lineage that describes in detail the
exact names and birth dates and circumstances of the people whose relatedness
is the impediment to this marriage taking place. For every one of the
300,000 or so documents of this sort that
we've uncovered you get this detailed lineage. This is interesting in and
of itself because it tells us where population pressure was seemingly impinging
on marital decisions. These things are all
over the place and span a long span of time. But the value of this resource, like many of the other
resources that we'll talk about over the course of the
next 90 minutes or so, increases dramatically
when it can be combined with other sources. In particular in this case, parish birth, marriage
and death records. Now, with the combination
of the dispensation records and these local parish
records from which we can infer population
growth we can see how much population
pressure was necessary to result in these
dispensation requests. In a rough way, create
a proxy once we have these other records for population pressure across
the rest of Europe. For a long span of time
during which we have neither census records nor good information on arable land. The basic idea that
I've described here, combining population
records with administrative sources
therefore is not new. It's something that we could do going back well into history and with these extraordinarily
rich sources. What's really new
today and what I think prompts the interest in what Martha and I will
be talking about, is the fact that the scale of those sources that we can
now link and the criteria by which they become
matchable have now evolved to the point
where this is actually feasible to do on a
very large scale. In the case of the dispensation
records we know the name of the bride and groom
and their parents and their marriage
year and their parish, this gets us into the birth records and their death records with
just a little more effort, but this is again,
not completely novel. This is the thing
that people have been doing with US sources like tax records and censuses since
the 1950s and the 1960s. James Malin, Merle Curti, Allen Bogue, Stephan Thernstrom, more recently beginning
in the 1880s, Rick Steckel and Jeremy Atack. What changes between this
first group in the 60s and more recently the 80s
and beyond is a shift from retail to
wholesale linkage. Retail being the work that Stephan Thernstrom did
in poverty and progress. Where he took the entire
population of Newburyport, Massachusetts in 1850 and looked for the same people
in 1860, in 1870, in 1880 and
painstakingly by I went through the entire
manuscript census schedules for Newburyport to
do those linkages. Now with the availability of data that's already transcribed, that's already indexed
and that contains all of the relevant
identifying information, these linkages can
be done wholesale, which as we'll see is both
a blessing and a curse. There's a bit of a challenge in doing this successfully
in ways that will ultimately provide
convincing results from the data that's
produced in this manner. There's something
of what I'll call a renaissance in linking, especially with historical
records that we've been able to see over the
last several years. Some unique historical
circumstances combined to make this linkage
much easier. Just in the very recent past have these circumstances
become so important. One obviously
computer processing power and storage costs. I could store the entire 1790 through 1940 US censuses
on a single thumb drive, on every single individual
in the country, on a single thumb drive, with all their
identifying information, everything they
reported in the census. There's also now something that graduate students take
advantage of routinely which I didn't have access
to when I was doing this as a graduate student which is cheap offshore
transcription services and for records that even
some that are transcribed that are written by hand rather
than type but better for type records, better
OCR technology. Finally, there's
been a real change in the degree of
intergovernmental cooperation. For example, the
Census Bureau and Social Security now routinely
exchange data in ways that make it possible to attach information from one
source to another in ways that dramatically enhance the utility of either source. For example, Social
Security records contain detailed date and place of birth and attaching those
to census records, which generally only
record year of birth and state of birth makes
possible a whole lot of exercises looking
at circumstances at a specific time and a very specific place
in the life cycle. I like to think that
this renaissance and linking is comparable to what John Abed, we'll
talk about in a few weeks. Which is what began with the LEHD project
at the center for academic studies in the
Census Bureau in the 1990s. Which was a linkage project of justice scale involving
cooperation across a variety of government agencies
and it produced an absolutely invaluable
resource for studying things like flows into and
out of employment and the life and death cycle
of individual firms. The challenge that we face
that I'll try and say a little bit more
about is combining and making use of all these
sources that have now become available and that in itself
is no trivial concern. Fundamental to the concern that people will have
to have in looking at the research that
emerges from this is the reliability of the
links that are made. A point to which I'll
return in a few minutes. Let me just give you one
example of the information that becomes available through this inter-agency cooperation. This is an SS-5 card, which is what everyone fills out when they enter the
Social Security system. It's the information
that they use to identify you when
you come to start collecting your Social
Security payments at age 65 or 67, or whenever. It has information
that pretty much only you should easily know. For example, this person, one Eldred Gregory
Peck in San Diego, California, born April,
I think that's the 15th actually of
1916 in San Diego. His parents are
Gregory Pearl Peck and Bernice Mae Ayres. We know that he is living in San Diego around
the time that he applies and he's
doing so in 1937, so he's already 22
years old when he applies to enter the
Social Security system. This obviously is the
movie actor Gregory Peck, but basically everybody has one of these cards
somewhere in the system. When Social Security allows the Census Bureau to
take this information and attach it to census records it produces something
quite extraordinary. I'll give you some examples of historical linkage and then talk a little bit more about
modern linkage and then get onto the problems
and challenges. Among the things that
we've been able to do in just the last couple of years that were
virtually impossible to do as recently as 10 years ago, linking multiple generations across US population censuses. The entirety of the 1790
through 1940 censuses except for the lost 1890
are now publicly available. Beginning in 1850, the census begins
to ask questions in a way that makes it
possible to identify every individual
within a household. It asks their full
name, their age, their detailed place of
birth at the state level. That makes it possible to
follow those individuals over time by linking them to
subsequent censuses. In order to adequately do this and to understand how
good a job we've done, you need to clearly
account for some of the obvious problems
under enumeration, the census never gets everybody. Mortality. If you
observe someone in 1850, it's possible that
they will not still be alive as of 1880. Then there are myriad mistakes and variations over
time in things like the spelling of the name and in the reporting of
age and birthplace. All of these are impediments to successfully locating
the same individual at different points in time. Nonetheless, in my own work, we've gotten linkage rates
about as high as 35 percent. That's a point to which I'll
return in a little bit. Here's an example of linkage
from 1850 to 1880 to 1910. This is Thomas
Goodrich who's located in Kalamazoo County, Michigan. He's here as the head
of the household. He's got a couple of kids, Charles, Maria, and Charles. Actually, one of
these is a typo. The other one is not
supposed to be Charles. In any case, in the
next generation in Montgomery County,
Michigan, you can observe the child who's listed as
the first Charles there. This is an individual
who's got exactly the right age given what is age was reported to be in 1850, he's got exactly the
right place of birth for himself and for both parents. There is only one
Charles H Goodrich, even allowing for
variations in spelling, variations in the
middle initial, who would fit into
this family based on the entirety of the
1880s sense to population. We can then take Charles, his son Thomas, and locate
Thomas in the 1910 census. He's now living in Seattle and he's got a son named Francis. Francis, I ran out of space
to show more of the slides, but subsequently ends up in
Detroit all the way back in Michigan after the family
had moved out to Seattle. The ability to link
four generations like this and to do this for
hundreds of thousands of families is a complete novelty and something we've only
been able to do within the last five years
with the release of the full count files from
ancestry from 1850-1940. Just to show you that I
actually had to at one point look at extremely illegible
census manuscripts. This is what the original pages would have looked
like if I had had to transcribe them
for this exercise, that's Thomas Goodrich,
that's Charles down there at the head of
the household in 1880. This is Thomas, as the head
of the household in 1910. What can we do with this? Once we've got
households in which we link fathers to sons, grandfathers to fathers to sons, great grandfathers all the way down to their great-grandsons. What we can do with that
information, at least at first, is to look at an
enduring question in American social history, which is how much mobility has there been across generations? For this exercise, what we've done is simply
rank occupations by their educational
content and tried to get a sense by birth cohort what mobility looks
like over time. This is showing the correlation
across generations, so increasing correlation
implies decreasing mobility. We start over here with
the 1830s birth cohorts. Mobility looks
relatively stable. Mobility seems to decrease
because the correlation increases for cohorts
born after 1860. It then levels off for
the cohorts born from 1890 down through 1910. Then from bits and
pieces of information that we have in a whole
set of other surveys. It looks as though
the correlation is somewhat higher on average
in the 20th century. The second part of it
than it was previously. This stuff over here at the
right-hand side is not new. All these datasets were
available before we did the linkage of individuals
from household to household. But all of this stuff
from a 1830s down through the 1910 birth
cohort is completely new. We could also take
a look at some of those very early
life circumstances because we know people's
detail place of birth. What we've done here is to
look at individuals who are measured in terms of
their intelligence at the time they enter the US
Army in the Second World War, they take something called the army general
classification test, which is basically an IQ screen. Individuals who
scored highest on this test were most
often shunted off into the Army Air Corps
because they required and consistently
succeeded at getting the highest scores on the AGCT. Making the case to
the joint chiefs of staff that this was an enterprise that
required people with an unusual degree
of intelligence. On the left-hand axis, what you have is the
probability that an individual was assigned
to the Army Air Corps, basically a proxy for
whether they scored in the top quarter on the AGCT. Then at the lower axis, what you see is the pH of the water supply
in the place where these individuals
lived as children. We get this by linking them from the World War II
enlistment records back to where they
resided in 1930. There's a well-known
relationship in water chemistry between pH and something called
plumbosolvency, which is how much
Lead leaches into the water for a given
amount of Lead piping. It's a conveniently
non-monotonic relationship. It basically looks as the
mirror image of this. Plumbosolvency is minimized at a pH of between seven
and seven-and-a half. If you look at some
cognitively sensitive outcome, as in this case, whether you scored
high enough on the AGCT to get into
the Army Air Corps, you would anticipate
if Lead is a problem, that it should be a greater
problem at places that are either side of the minimum
point of plumbosolvency. That's exactly what you see. The scores are
dramatically lower at points either side of roughly
seven in this calculation. This is for cities
that had lead pipe. If we look at cities that
don't have lead pipe, there's no relationship
between pH and the probability of scoring high on the army general
classification test. This seems pretty convincing at least to me that what
we're observing, what we're looking at pH is a good proxy combined
with the presence of lead piping for water-borne
lead exposure early in life. Again, the thing that
we could not have done, not just easily, but not at
all. Just a few years ago. One more, and this is some
work with white lottery. This looks at the effect
after 18 years of winning a plot in the 1830s
to Georgia Land Lottery, the state of Georgia auctioned
off part of its land, but most of it is distributed
through a series of land lotteries
between 1805-1832, we took the winners of
the 1832 land lottery, which is basically the
Northwest corner of the state, which includes
modern-day Atlanta. We followed them over the
next 18 years to see how they're doing as of 1850. What you can see is that for individuals who want
a plot of land, but were at the lower
tail of the distribution. The effect of winning
is virtually nil. It's not until you get to
above the median that it appears winning has
any substantial effect on your observed 1850 wealth. It's extremely large that effect once you get to
the very upper tails. This suggests that in this case, if the treatment is receipt
of a plot of land which in this case is worth roughly median wealth in the state
of Georgia at this time. If that's the treatment, then this is a really heterogeneous
treatment effect here, that the effect is
much larger for people who already
were doing quite well. That has some implications if you want to think about just making people wealthier by transferring money directly
into their bank accounts. More examples, a couple, or even non-labor nourish. The things that you
can now do with these linked historical records. You can identify
financial actors, for example, owners, directors,
shareholders of firms, and observe what they
look like over time, how their
characteristics differ, how people involved with
different enterprises differ. You can look at something that, I think, Jeremy mentioned, he was going to be
working on years ago, which was the persistence of
manufacturing firms using the manufacturing schedules
of the census of population. One of the things that we're actually possibly going
to be working on in the very near future is radioactive fallout
exposure in childhood and adult outcomes including IQ. This, too, is something
that's only possible once you know exactly where
people are growing up, down to the level of the city
or census defined place. Early life, lead exposure and Late Onset Alzheimer's
disease is an issue that's been of increasing interest in the community of psychiatric
epidemiologists, but there's been absolutely
no data to do this at scale and get a sense of just how strong these
relationships might be. Because there's
obviously no dataset currently available
that follow someone backwards from a Late Onset
Alzheimer's diagnosis to their early life exposure
to waterborne lead. The kind of data that we have now allow us to do just that. A project that involves
Claudia Olivettii, is looking at the
pre-school experience of individuals between
1943 and 1946, when the federal
government was funding large numbers of
preschools in order to encourage women to enter
factory employment when conscription was
at its peak in 1943. We can link those individuals, or at least the people who
were in the towns that were exposed to this
preschool experience, to their adult outcomes from
2000 to 2015 and even to intermediate outcomes
as early as 1973. Rick has been working over
the last several years on slavery and the
intergenerational transmission of hypertension. This, too, would have
been impossible to do as recently as
five or 10 years ago. Finally, there's also
interesting work being done on the impact of environmental disasters on
outcomes like migration. Rick Hornbeck has
done some work on the Dust Bowl and
he's got other work in the pipeline that
looks at similar issues. The Census Bureau has gotten
into this act itself. That is, it has
become increasingly interested in allowing
individual researchers to come in and take
advantage of sources that they have prepared
for linkage in ways that would allow us
to do things with modern census data
that until now, we've only been able
to do with historical, publically available
census data. Data that's already released because of the 72 year rule that prevents the manuscripts
from being released until 72 years after
the census is taken. The Census Bureau has undertaken something
called the American Longitudinal Infrastructure
for Research on Aging project that
makes it possible to link individuals across
currently closed census records to do so within a secure
sense as research, dataset, etc., environment and to do many of the same kinds of things that I've
described us now being able to do with
historical sources. The idea is basically to use
the information that census has from Social Security and
other places like the IRS, that makes it possible to assign a unique nine digit identifier called a PIK to each individual. That number persists with that individual across
multiple data sources. PIKs internally are
used to facilitate unduplication and enter a laugh or record linkage of this sort. Once it's done, once an individual is picked
in one of these sources, they can be very easily linked across a variety of sources. There are some
examples listed here. For the Census Bureau, anything that's been run
through their PVS system has a unique identifier
consistent across sources. Clearly, we've already
got 2000 and 2010, says Sandra and I are
working on completing the picking of the
1940 census that makes it relatively
straightforward to link individuals who were
children in 1940 to their outcomes in the
2000 and 2010 censuses. We're working on 1950-1990, asked me again in
about six months. The current population
surveys from 1973, 1979, and 1981
through 2015 are all picked and therefore available for linkages of these sorts. The Survey of Income and
Program Participation, the redesign of the
SIPP is also picked. The original NLS cohorts, Social Security
earnings records. Far too many things for
me to actually list here. There is a treasure trove
of information that's there and there's an
increasing willingness on the part of the Census
Bureau to allow people to come in and exploit
those resources. Now, how about the matching? How about how
comfortable we should be with the reliability
of these matches? Orwell always said, that tired metaphors, actually any metaphor
that's been used before is probably a bad one. But sometimes, I think the tired metaphor was just the right one. Here, we'll take one from Bob Fogel's essay on circumstantial evidence
and the two roads book with Jeffrey Alton, he describes the chains
of inference that we as not just
economic historians, but anybody who works with
quantitative evidence have to keep in mind. The links that comprise each of those chains of inference. The weakest link in
most of what contains a linkage component is clearly the quality of
the links themselves. This is especially
problematic with very diverse sources because
those sources have been created for different purposes
and they don't have in mind how easy it would be to link those people to other
sources of information. They have their own
institutional imperatives that lie behind the collection of the information in
the first place. Weak links are a problem. But not all weak links
are created equal. Some of them have a
different impact on the confidence of
our inference that we draw from work with
linked data than do others. For example, when linkage generates right-hand
side variables, thinking about a
simple OLS regression, a weak link is going
to generate noise, which is often okay. It's just going to reduce our ability to precisely
identify a particular effect. Martha has some important
counter examples dealing with intergenerational mobility,
which we'll talk about. When the linkage itself or
its absence is the outcome, that's when we have to be
particularly cautious. Because if you're trying
to infer something like migration or mortality, purely on the basis
of an individual not being successfully linked, you really have to be careful at taking account of
all the things that could possibly go wrong in
the process of that linkage. That's the only way to
make a convincing case in that situation. What
have we learned? One is that there's a clear trade-off between
scale and precision in linkage is that
wholesale versus retail contrast that
I described earlier. That trade-off is inevitable. What we need to do,
and again, Martha, we'll talk more about this is to assess the linkage rate, how many individuals are
successfully linked, the reliability of the links, in particular, how many
false positives we get, and also how much bias there is. How different are the
characteristics of the individuals that
were successfully able to link versus those
that we can't, so she'll say a little
more about that in a bit. Keep in mind that
there is no one point on this trade-off between
scale and precision. That's ideal for every
single circumstance. You cannot just
unilaterally say that a particular degree
of bias is bad, or that a particular degree of false positives is crippling
to a particular analysis. You need to keep in mind
what the context is and what the magnitude of the effect is that you're trying to assess. Keep in mind also that even modern linked data are imperfect in ways that could
be fatal in some settings. As part of doing
the census work, I had to get a duns number
from Dun and Bradstreet, one of the Dun and Bradstreet website and I've put
in all my information, my social security number, and all the information
that they wanted. You do when you
forget a password, it asks for answers to a set of questions that you should only have the information to answer. What was the model
of your first car? What town did you meet
your current spouse in? What was the name
of your first pet? I answered all three questions and I got all three
of them wrong. But then I went back
and I asked them to give me the
questions again and I ran through the procedure again. I realized that for each one of the questions I answered wrong, there was an answer that
pertained to my father. His first model of
a car was an Edsel. Mine was a Galaxie 500. He met his spouse in the town where I was
born, Edgewater, New Jersey. I met mine in Chicago. His first pet was a black dog named Scottie,
a Scottish terrier. Mine was a cat named Blackie. But nonetheless, all of the
answers pertained to him, and it was clear
what had happened. Dun and Bradstreet thinks
that I'm my father, which in itself is fine because he's got a higher
credit rating than I do. It means that if I want
to refinance again, I'll get a better deal
than I would otherwise. But it means I'm in for a
world of hurt when my father passes because Dun and Bradstreet will
think that I'm dead. To change that would be a
very complicated process. The people at Dun and
Bradstreet just said, "It's not really
worth it right now. We'll deal with this when
your father is gone. But for now, just leave it." I got my Dun's number. But Dun and Bradstreet is
supposed to know these things. It's supposed to use the
same very best sources and techniques that are available to anyone doing
modern record linkage, and they still get me
and my father wrong. We don't have the
same middle initial. Our birth dates are
off by 35 years, and I haven't lived
in Edgewater, New Jersey since the age of 18. They're way off in
ways that will be crippling for me if I have to deal with this
in a couple of years. The bottom line here is, even modern data are imperfect in ways that
can be a problem. The best approach to all of
these challenges I think, is recognizing the
imperfection of linkage and its possible impact on the outcomes that
we're trying to measure. To be as upfront and transparent about this possibility
as possible, and to do everything to try
and assess how large it is and whether it's likely
to overturn our findings. Whether we have actually
generated data that produces results that are incorrect solely because of the way
the linkage has proceeded. I'll talk about something
called synthetic truth in two more slides and Martha will talk in more detail
about this as well. That was lessons learned. Let me talk about some lessons
that we haven't learned. Things that I don't
want you to take away from this presentation. One is, again, apologies to Orwell for the tired metaphor. But don't toss out the baby
with the bathwater here. That is, don't dismiss
work involving record linkage solely because there are problems
associated with it. Get a handle on what
those problems are, and try to understand
whether they really do bias the results in
a fundamental way. What I've got here is a
heavily redacted excerpt from a journal referee report. Not on my work. But rejection of the
article was recommended. "B has a paper forthcoming in HM that compares false positive rates using different linkage methodologies. Human eyeballs on
the data does best. The false positive
rates for some of the mechanical linkage
methods are best described as 'scary'," which
itself is in "scary". You can correct me on
this during the Q&A, but I'm pretty sure
that no one has ever rejected a paper because
of scary standard errors. It's not standard errors
that we care about. It's the ratio of the coefficient Beta to the standard error that
we care about in itself, and even that needs to
be viewed in light of whether the
coefficient that we're interested in is of substantive
significance rather than simply statistical
significance in the way that Deirdre
has been urging us to look for going
on 32 years now. How can we address
these shortcomings? How can we convince
readers, and editors, and referees that we've done a good enough job in the
linkage that they should have faith in the conclusions that we reach based
on the link data? I'll talk about something
called synthetic truth very briefly and
then we'll finish up. I'm not talking about
alternative facts here. Instead, I'm thinking
about a way of generating truth data
for verifying links that's generated synthetically. It doesn't rely on
individuals actually eyeballing the data or getting genealogists
to do the work. The big problem in assessing linkage technologies is
finding true matches against which to compare these algorithmically
generated automatic matches. The option that I've hit upon, and it's not even close to
being an original idea. People in the record
linkage literature have been doing
this for decades, is to pretty much roll
your own truth deck. Again, that's a horribly
mixed metaphor. This is something that any
paper with linkage could easily incorporate
in an appendix. I don't want to just say could, I want to say should, that this is something
that I'd actually urge prescriptively. Editors and referees
here, I'm talking to you. It basically consists of taking an existing data set
and screwing it up as the information
would be screwed up if it was collected again from the same individuals
X years later, which is fundamentally
what we're doing when we're trying
to link them over time. As an example, I've taken the 1850 US
Census of Population and screwed it up to mimic
the same individuals, as if they were appearing
in the 1880 US Census. How do we do it? Well, we think about what
are the things that, based on our experience
in doing record linkage could possibly go wrong under enumeration, as
we mentioned earlier? Drop some of the
observations from the quasi 1850 data set, but leave them in
1880, or vice versa. Mortality, some
people don't survive, drop them from 1880
leave them in 1850. Spelling variants for names, first, middle and last. Myriad ways that
this can go wrong. Year, place of birth
mistakes, again, a whole bunch of ways
that can go wrong, but that can go wrong
in predictable ways. What are plausible
magnitudes for these? Well, we take advantage
of the fact that David Hacker has
estimates for under enumeration and for mortality based on the census in
just this time period. The city of St.
Louis was actually enumerated twice in 1880. I think in part because
St. Louis thought that their population had grown much more rapidly than
it actually had. They thought they
were going to be the real capital of the
Midwest rather than Chicago, and they implored
the Census Bureau to come back and
recount them again. They did so just a
couple of months after the original 1880 enumeration. Because this was done at a
time when we actually have street addresses for every
individual in the census, it's quite straightforward to match individuals and households without actually matching on particular sets of
characteristics. You can see exactly how
far off things are. How many people
misreport their name, or how many people give
a year of birth that's X years off from what they reported the very
first time around. What I've done here
is just giving you a couple of quick
examples of how that process of generating
synthetic data and trying to match it between
the original source and the synthetic one plays out. I'll give you just two things
to take away from this. If we just look at
individuals who are aged 5 to15 in the original
census in 1850, and allow no tolerance
in their reported age, that is, we assume
that everybody reports their age
exactly correctly, and we allow no variation in
the spelling of their name. We assume that everybody reports their name with
exact correctness. What we wind up getting is 94 percent of the
population linked. Not 100 percent,
just 94 percent. The reason it's not 100
percent is because of pure disambiguation error, being unable to simply
distinguish between two individuals who share the full set of
characteristics exactly. Exact spelling of name, exact place of birth, exact year of birth, and those individuals you
can't distinguish among. What to do with
them is a question that I'm going to
set to the side. But in this exercise, you don't get the entire
population because of them. If we move on and instead allow a three-year
age discrepancy, we allow for some variation in the spelling of
the names, first, middle and last,
and then we mess things up in the ways
that I described. We screw up their birth
year consistent with what we know the screw
ups were in St. Louis. We do the same with their
name, their birth place, their parental birth places, and then mortality,
and under enumeration. If we do that, we get
down to about 36, 37 percent linkage predicted. Thirty-six or thirty-seven
percent is the fraction that we actually do link with
this synthetic data. That's pretty much
close to the usual. That is, the number that
people usually are able to derive when they
do this linkage. This suggests that
this is one way of assessing how reliable
record linkage is. The other thing to
note here is that, in terms of the fraction
that are correct, that is the fraction
where we've actually matched to the
correct individual. It's 98.2 percent, which means that the rate of false positives in this
data is only 1.8 percent, which is quite low. But I still haven't
gone as far as I could. I haven't built in correlations across the types of error, and I haven't built
in particular bias, but that's exactly the
thing you can do and do in a way that should
ultimately satisfy readers, and editors, and referees with this
kind of data that's designed to be appropriate for a particular matching context. Let me finish up. In terms of conclusions, clearly there are linking exercises that are
going to produce data that's more
valuable than the sum of its parts and any of the
individual pieces of data. Process is throughout
with challenges, but it can nonetheless generate invaluable insights that you'd not get from any
other kind of data. We need more standards to
assess linkage quality. Again, Martha will
talk more about that. We, by whom I mean, all these kinds of people, can help in this process of standardization by requiring
basic analyses of linkage, reliability, and bias
as part of any paper that submitted that has a
substantial linkage component. We need to do this now
because the range of possibilities is only
going to expand. The time to address
these concerns is not even now, it's yesterday, but this is definitely
the time to be worrying about exactly these
issues. Thank you. 