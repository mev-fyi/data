i'm pleased to have the opportunity to speak at this nbr conference on big data and high performance computing so the way i'm going to come at this issue is more from as as tony just said uh from a policy perspective or what challenges we have or as economists we always like to think in terms of what are the constraints it's the constraint optimization that is the key and what sort of constraints are unique to performing economic analysis with the help of big data that that is going to be the crux of my remarks and and then we will see in q a some additional issues that might crop up an introductory story the term big data is new uh even when i was in graduate school in the mid-80s nobody used the word big data on the term big data but but the underlying phenomenon is anything but new and it is certainly not unique to financial economics consider for example this is the year of census so i think this is very pertinent example although i'm going to take you back more than a century consider for example the u.s census which is taken every 10 years as required by the u.s constitution it is a seemingly simple task to count people and report demographic information such as marital status and family size yet by 1870 the quickly expanding u.s population hampered the ability of the census office to tabulate results effectively in fact the 1988's in 1980 no i'm sorry 1880 census which was hand counted took nearly 10 years to complete in other words the 1880 census involved big data okay herman hollerith saw the opportunity and left the census office before the 1880 census to develop a machine that could count and tabulate the results that's a good example of an entrepreneur his machine was tested in 1887 and it was quickly leased by the census office for the 1890 census it also speaks to when there is a good technology how quickly it gets adopted not just in the u.s you wait you see his success in 1890 led to contracts with foreign governments and private companies hollywood machines were used in 1891 so within just one year for censuses of canada norway and austria railroad companies used them to calculate fair information and so on so forth in other words hollerith machines efficiently solve many important big data problems of the day okay today 150 years i don't think i did the math quite right it's more like 140 years later where do we stand we stand on the mountains of data that are inconceivably larger by some estimate the world generates more data every two days than all of humanity generated from the dawn to the time of the year 2003 okay how much data is generated by or for the sec one easy answer is that scc's electronic data gathering analysis and retrieval system and this is what everybody calls it as edgar edgar receives and processes about 2 million filings a year but those filings are themselves complex documents many watch many of which contain scores of pages numerous attachments and many thousands of pieces of information okay so what is big data i think it is kind of old age anyone older than me is old and any data bigger than my computer system can process is big okay so what does big data mean to the sec the sec processes and maintains several big data sets one for example is the option pricing reporting authority data or opera data one day's worth of opera data is roughly two terabytes so we are getting that every day and and that is just one of the data sets so big data are often characterized by so called three v's which are volume velocity and variety okay and i might be here on shakier grounds in terms of because i'm slightly venturing on the technical fields that you are familiar with but be kind to me don't point out any mistakes okay volume is the quantity of data velocity is the speed at which data are created and stored variety is the heterogeneity of the data in term of data type and data format to this list of three some would add a fourth v which is veracity is the quality and accuracy of data and scc we worry a whole lot about that and there is and and a lot of the both policy challenges as well as data challenges are on uh they center around the veracity both intended and unintended veracity okay so what are some of the policy challenges like the census office centuries ago the sec faces a big data problem today and that's what leads me to comment on what i want to highlight in this talk what are the policy challenges that stem from big data at the sec okay it is helpful to begin with what's the mission of the sec because it is against that backdrop that we can understand the challenges at least from through the lens of the sec so the mission of the sec is to protect investors maintain fair and orderly and efficient market and facilitate capital formation okay so i see several big data policy challenges in light of the sec's three-fold mission first and that is security security of data let me begin with security which is a primary concern of the sec in fact you know when i decided to join the scc countless number of my colleagues from near and apart they said my god you know can we lay our hands on some of the data you know because we know a lot of data and i i made all kinds of promises without realizing that once i landed there i realized that my biggest enemy is this concern about security which which which basically means that scc in fact is careful about letting even people within the sec to look at that data so it is it's almost like well we gathered the data and it's somewhere there nobody knows what it is but but it is there kind of so so that's security is an overriding concern and the volume velocity and variety of big data make security particularly challenging for several reasons big data are harder to store and maintain okay for example it is harder to ensure that only the right people at only the right time have access to only the right amount of data right type of or amount of data second big data are bigger targets for bad actors okay for example portfolio holdings data for all investment advisors are more valuable than portfolio holdings data for one investment advisor and weekly portfolio holdings data are more valuable than annual portfolio holdings data so these challenges get harder as certain data sets start to include more personally identifiable information pii or identifiers that link investors and institutions within and across data sets so the sec must be mindful of the data it collects and its sensitive nature and the acc must be principal responsible user of that data naturally data collection is not an end unto itself that the sec must not be in the business of ill-defined and indefinite data warehousing for these reasons the sec continues to look into whether it can reduce the data it collects or reduce its sensitivity one example of this is the cc's approach to form input one of the things the other thing that i realized once i went to the scc was that the number of acronyms i think they have a fetish for acronyms i think if i was to get paid on the basis of dollar per acronym i would be a rich man okay so the form n port which is now which is a new form of reporting both public and non-public fund portfolio holdings to the sec the commission recently modified the submission deadlines for this information in order to reduce the volume of sensitive information held by the sec so the the moment portfolio holdings are stale information they are naturally less sensitive and that that's the simple way in which but of course you know sometimes for reasons of enforcement or to catch bad actors so to speak you do want to collect information on a more timely basis either at least for internal purposes so so that's the that's the trade-off that one faces of course you can reduce the security by collecting information at a much later stage but it is neither sensitive nor particularly useful for some other purposes so this simple change reduce the sec's cyber risk profile without affecting the timing or quantity of quantitative quantity of information that is made available to the public so to the extent that public is interested in gathering this information not on the most timely basis but just having access to that information so that that can be used either for research or for whatever investment purposes then having making it collecting that data at a later date and disseminating to public at a later stage reduces the sensitivity of or enhances the security aspect of the data without sacrificing at least without sacrificing entirely the usefulness of that data let me turn to second policy challenge and that is technology for example the potential trading gains from having computer systems and other technologies that are even just a little faster and smarter than the competition are enormous okay thus there is a technology arms race between trading firms that are striving to get the best technology and the best personnel so when when you were talking about how fast these things are i was worrying about it not looking at the benefit of it but i'm thinking that some people will lay their hands on that faster technology processing technology they will lay their hands on it sooner than others and that will give them a technological competitive advantage which runs off all with some of the missions of the sec which is to have a level playing field for different players the media regularly reports that institutions that are increasing their use of ai artificial intelligence machine learning and related tools however there may be fixed costs to the deployment of these technologies that exclude small fragmented or less resourceful investors second there are cultural differences between organizations that affect not just the choice of which technology to deploy but also the timing of deployment for example hedge funds might be able to adapt new technologies such as cloud computing more quickly than pension funds are able to do so third some technologies are inherently challenging for the sec to monitor to mention just one example consider artificial artificially intelligent algorithmic trading ai algo trading which trade through time in non-predictable ways suppose an ai i'll go eventually start spoofing without the knowledge of the algo creator okay now what is spoofing all of you are familiar spoofing is a prohibited activity that involves creating and canceling a large number of trades in an attempt to convey false information about market demand so how should the sec respond to that to spoofing and speaking of fast moving technology how does the sec develop or attract a workforce that not only sees and understands the current state of the art but that can also envision and prepare for the future the sec has prioritized and supported the development of a workforce with big data skills and experience over the last 10 years dira so i'm chief economist and director of division of economic and risk analysis our head headcount has grown from a little over 30 people to nearly 150 people today okay next i would like to talk about communication another big data policy challenge is communication because the sec has diverse stakeholders the sec focuses on main street investors meaning individual retail investors who typically invest through their 401 k style but for stakeholders our stakeholders also include pension funds municipal bond issuers brokerage firms hedge funds and the congress the issues surrounding big data are complex and increasingly require specialized training to understand so it is challenging to communicate the essential parts of these two of these markets to each group of stakeholders indeed one size does not fit all while i'm talking about communication i would like to mention an important detail about hermann hollerith back to the census example that story a key insight into the census data problem was the realization that the variety of the data could be dramatically reduced by requiring the data to be transcribed onto what we would now call punch cards okay not anymore only in the 70s i also started with punch cards with all of the data in one standardized form it was relatively easy to build a machine that could tabulate the information the principle still holds to today for example the sec has required filers to tag some data using methods such as xml fix fpml xbrl and more recently inline xbrl now don't ask me all these acronyms but there they are there but dramatically reducing the variety of the data tagging trend tagging transactions tagging transitions and electronic document from being human readable into one that is also machine readable so tagging is the key element here a perennial challenge of the sec is to find cost of effective ways to reduce the variety of financial data without loss of substantive information so and that's a big challenge an additional feature of data tagging is network effects it is well known that data tagged in 10ks can be linked to data from other forms and other firms perhaps it is less appreciated that data in tag documents could be linked across regulatory boundaries and even national boundaries provided the regulator community required similar data tagging for the sec a key benefit of cross regulator consistency in tag data is the ability to understand better the nature of the risk in the financial markets the markets today do not stop at national boundaries so looking only at international data provides only a partial picture of the system's risk and that's why this is overall part of communicating across regulatory and national boundaries and to the extent that data are tagged and that way we can access data from across regulatory bodies and across jurisdictions that would help us have a clearer picture of what the challenges or risks that are facing so what are some of the research opportunities the second key question for today is about research opportunities in the era of big data i do see many opportunities research opportunities certainly for dira's financial economist but more broadly for academics for industry and for anyone anyone who values financial data broadly i see opportunities based on large databases that are available now or that might be available in the near future i also see even more opportunities based on changes that are being made to existing data sources in addition to a myriad of academic questions big data will continue to help the sec and other market regulators identify and shut down bad actors i mentioned earlier the option database in addition to that opera database that i have already mentioned i would like to highlight an additional data phase afterwards i will highlight two other areas that will open those for new research opportunities the first one is called cat don't don't get excited about this being something related to the feline community but on july 11 2012 the commission voted to adopt rule 613 under regulation nms nms is national market system this was a significant mile maker milestone mile marker along the path to create and implement the consolidated audit trail that is cat cat when completed by the self-regulatory organizations the cat will provide a single comprehensive database enabling regulators to track more efficiently and thoroughly all trading activity in equities and options throughout the u.s market so this is very ambitious in fact this is one of those we we have a weekly and monthly meeting on this and when i when i went there and we were having initial first couple of meetings someone said well don't get too excited about this you know this has been going on for eight years you know so so i i don't know how long it will take but we do i i'm optimistic and we do see some light at the end of the tunnel and that light is not just an incoming oncoming train but it is you know so so this database we hope will transform the market surveillance and enforcement functions of regulators for example regulators will be able to track the activity of a single individual trading in multiple markets across multiple broker dealers the cat will not be available to academics or industry for research marketing or other purposes but my my hope is my sincere hope is that that's not a permanent statement but that is sort of something that will be revised over time obviously many of the those who supply information which means hedge funds or others who are trading in in these securities they like to protect their identity because they think that how they trade is and they have every right to believe that is their proprietary information that is their intellectual property and even if in hindsight if those trading strategies become public knowledge then we would be sacrificing their uh intellectual capital and that's the reason why they are hesitant they are strongly against the idea of making this data public but like everything else i think hopefully some of those concerns might recede over time and we would be able to make it public this is strictly not a commission statement this is my just personal expectation time will tell whether that is true okay the second element that i would like to discuss here is standardized structural languages the three v's of big data are volume velocity and variety it is hard to imagine that future finance data sets will have less volume or less velocity than they do today so perhaps the best way to make future data sets more manageable is to mimic hermann hollerath census solution by attacking variety okay since the mid-1990s most sec documents have been submitted to edgar i i talked about edgar a minute ago although the submissions are electronic and can be easily read by a human or any computer they are not machine readable because they are essentially unstructured electronic paper okay content not only chained across filers and across time but so too did the formats plain text html pdf and others the financial data okay that is machine readable that you can aggregate across form but the footnote data for example right now is not machine readable nor is mdna section that talks about management's assessment of the company's prospects that's not machine readable because it's not structured in a fashion that can be compared across corporations it can be read in the sense as a text it can be read but it is not machine readable subsequent initiatives by the sec have made it easier for people machines and regulators to read and understand the disclosures on edgar an important milestone was reached on may 7 2003 so i'm really talking about ancient history here when the scc adopted its initial requirement to file forms these are forms three four and five using extensible markup language xml i believe that the structuring of these forms in xml lowered access costs and analytical costs making this information more valuable to the market since 2003 many more forms have now submitted and these were the other acronyms i mentioned xml fix fpml xbrl and most recently inline xbrl structuring disclosures so that they are machine readable facilitates easier access and faster analysis that can improve investor decision making and reduce the ability of filers to hide fraud both are important to to the sec and also i hope to the community at large structured information can also assist in automating regulatory filings and business information processing in particular by tagging the numeric and narrative-based disclosure elements of financial and risk return summaries in xbrl those disclosure items are standardized and can immediately be processed by software for analysis this standardization allows for aggregation comparison and large case statistical analysis that are less costly and more timely for data users than if the information were reported in an unstructured format structured data will likely drive future research in cooperation finance and macro economics okay finally standardized identity the lei okay another common big data problem is accurately and timely connecting disparate big data sets for analysis this problem is exacerbated by the broad range of identifiers used by federal agencies the irs has the employer identification number ein or pin sometimes it is called also the federal reserve has the research statistics supervision discount identifier rss d finra has the central registration depository crd and the sec has central index key cik a recent report identified 36 federal agencies using up to 50 distinct incompatible entity identification systems in my opinion these difference these differences raise costs and burdens for the federal agencies as well as their regulated entities and this is talking only about the u.s make it international and the problem is multi-fold and that is a real issue and and the genesis of interest in lei apparently stemmed when in 2008 when it was realized that lehman is in trouble and the fed asked gee you know who are the customers or who has these derivatives or other positions in in lehman or who are the counterparties and that information wasn't available because there was no way of having a common identifier or linking the folks who were taking the other side the position in those to a particular entity such as suppose it was citibank it wasn't clear that from that who had traded in that derivative it could easily be linked to the ultimate uh economic entity so anyway so that that apparently led to some greater interest in thinking about the standardized identity the global legal entity identifier lei is a 20 character alpha numeric code that provides a single unique international identifier enabling accurate identification of legal entities okay this this this is a not-for-profit organization based in frankfurt and and they have been trying to get uh everybody in the world to adopt this lei and and of course you know there is a little bit of pushback uh because will this impose some unreasonable cost on on entities or registrants in periodically expanding real resources in getting this lei and also renewing that identification on the other hand the benefits are but that might be more in a public good kind of sense and that's why there has been some tension but i think its popularity is growing lei offers a single international connector for disparate big data sets while also reducing the current regulatory burden associated with each agency's unique identification system the lei includes level one data that serves as corporate business cards it answers the question who is who the lei also includes level two data that show the relationships among different entities it answers the question who owns whom the lei serves as a rosetta stone to identify clearly and uniquely firms and entities participating in the global financial markets recently the commission released rules that mandate the use of lei when are associated with security-based swap transactions the lei is now a component of mandatory swaps transactions reporting in the u.s europe and canada europe has mandated future lei usage widely including in payment and settlement activities as well as structured finance i believe that the full benefits of lei have yet to be realized as some companies may have hundreds or thousands of subsidiaries or affiliates operating around the world more benefits lie ahead as the lei becomes more widely and comprehensively used the alia allows more transparency regarding hierarchies and relationship mapping this will support better analysis of risk as they aggregate and potentially become systemic so in conclusion i am looking forward to today's robust discussion of policy challenges and research opportunities in the era of big data you obviously are helping expand the future of finance in important ways that will surely have positive externalities on markets investors and businesses thank you for giving me an opportunity to share these thoughts thank you 