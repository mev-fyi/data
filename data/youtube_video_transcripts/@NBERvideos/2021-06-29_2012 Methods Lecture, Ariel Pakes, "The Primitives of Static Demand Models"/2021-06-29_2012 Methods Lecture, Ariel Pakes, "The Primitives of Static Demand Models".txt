Ariel Pakes: This first lecture is very much an introduction. It's an overview of where
we're coming from and why. I see people in the audience, so I think know all this. There's different
levels in the audience, and we'll try and do. The second thing
I want to say is, even I have slightly
different ways of doing this, I tend when I give
lectures also in class to hand out
a full lecture. I can't tell if I do
that just because of my own insecurities or
because I think it's good. But the advantage of it is my students come back
afterwards and say, well, I didn't get
in the lecture I can read through the notes,
the whole thing. The disadvantage of it is, there's a lot of
stuff on the board. Because it's exactly
what you have in your hands what's going
to be on the board. That's too much on
the board at once, so you're going to
have to listen to me. But you can write down
easily one or two things, and then come back to it. The reason I do it this way, is my students tell me
that afterwards they find it very useful
afterwards to go back to the notes and be
able to read through them. But that makes the lectures
a little problematic. We're going to be interested
in the analysis of demand with differentiated
products and we'll look at some of the implications
of that for markups and profits and for price indexes
and hedonic regressions, which are all directly
related to that. What I want to start with is why are we so
interested in that. The demand system essentially embodies much of
the information you need to analyze incentive
facing firms in a given market. The two types of
incentives we usually deal with immediately in
IO and sometimes more, are pricing decision, and the incentive to invest in alternative types of investments either capital, R&D,
or advertising. If you are going to look at the response of either prices or investment decisions
to a change in the environment or in a policy, the incentives facing you would be coming from the
demand system largely. Now, there are actually two
or three other components of the decision-making process which you would actually do. If you're going to recompute a static equilibrium
after a merger, say, you would need to demand system, you would need a bunch of cost system for the various people, and [inaudible]
equilibrium assumption. Essentially that's
what would go on. The demand system
is the thing we typically have most data on. Cost data is generally
proprietary. That's not true in
regulated industries, but in most industries is
generally proprietary, worst with in IO. We use it in Nash and
prices or something else. I'll show you why we use it. Because it works out being
a good approximation, and that approximation
is good enough for certain things and it isn't good enough for certain other things. But in the end, you're going
to end up relying a lot on the demand system because the pricing assumption
will just be Nash in prices and the cost system often because you don't
have data on cost, you will derive it from the
Nash in pricing assumption. You will just figure out what the cost must have
been to set prices equal to the prices that we saw prior to the merger or
prior to the tax change, or prior to the
environmental change. The demand system ends up being probably the most
important piece of tool that you're going to use when you do
any of these analysis. That's the main reason. The other reason is
welfare analysis. The way we're going to work these modern demands
systems is we're going to work from
individual utilities. We start with an individual
utility function, we have a distribution of those utility functions
across agents, and then we're
going to explicitly aggregate up to
get market demand. Is that clear? If you are
going to analyze welfare, you could actually use
this system to analyze the distribution of outcomes to different people's utility, the distribution of
consumer surplus this policy or this
environmental change or decrease in consumer surplus that the policy and
environmental change did. That ends up being important
for all sorts of issues. For example, if you want to find out the benefits to
consumers of new goods, you could do it that way. That's the kind of
thing that underlies, we'll go back to that question, that particular question, it underlies much of the
policy towards R&D activity. If you're going to analyze the outcomes of
regulatory policies, you'd want to know the
whole distribution of impacts across consumers. For two reasons, really, often, because the regulatory
policy itself is directed at changing
consumer benefit or different kinds of consumers benefits and also
for reasons that the regulator typically either an elected official or appointed
by an elected official. He'll be sensitive to
the distribution of consumer outcomes from
the policy that he does. Then there's going to
be the construction of price indexes like the CPI, and it's used for a multitude
of things in the economy. It's used for indexation
programs, tax brackets, government salaries and the
like. We'll get to this. But if you actually
figured out that the CPI overestimated the cost of living index by one percent in
something like 10 years, you saw the budget
deficit problem. It's not a small number. I'm going to now take a step backwards and say where
we're coming from, and then I'm going to go
to where we were going to. Typically, there has been two breakdowns of demand
systems in the literature. One set distinguishes between representative
agent models. There's a single agent with
a single utility function, we derive demands for that
representative agent. The other side is
heterogeneous agents. That's one breakdown. The other breakdown is going
to be that we're going to do demand directly
on product space, so there's going to be
a demand function for each product as a function of
the prices of all products. That's product space
demand systems and what I'll call characteristics
based demand functions, which is the utility from a product just depends
on its characteristics. If I knew all its
characteristics, I know the distribution of utility that comes
from, is that clear? The demand function, we're
going to change this space. The demand function
will be on tuples of characteristics or in
vectors of characteristics. That's where we're
going to end up. But I want to talk
a little bit about the products and why
we ended up there. You'll see the
product space stuff is still good for some things. It just doesn't work out
to be good enough for many of the IO applications.
They just can't handle it. These are the
demands systems that are been in the literature
for a long, long time. The first demand systems
ever in economics. Typically, they were applied in representative
agent models, not in heterogeneous
agent models, and that's historical. When we started using demand
systems in economics, you couldn't actually
aggregate up over a bunch of people and get the
aggregate demand system and do it on a computer. The computers just
weren't good enough. We had a representative
agent and the assumption is you need
it to go to aggregate. There are some assumptions
that allow you to aggregate explicitly into an analytic form that I could take for
a demand function, so it'd be a distribution of utilities and they
would sum over them, and it actually be an analytic
form for the outcome. But they ended up being
extremely sensitive to the distributional
assumptions and very hard to use when you're using the actual distributional
assumptions in the population at large. For example, the one thing
that probably the most important
differentiation between consumers for price effects, which we're always interested in, is income distributions. You'd want to mimic the
income distribution when you aggregate it up, and it end up being
very hard to do that. I'm going to start with
this little digression. This is no longer a
problem right now. Even in either product space, we're going to use it in
characteristics space, but either product space
or characteristics space. It's just not a problem anymore. It's because of the advent of computers and
simulation estimators. This is a small digression. I'll do it quickly,
but it will tell you why we should always be doing
heterogeneous agent models. I'm going to let z denote the consumer characteristic that differentiates the different consumer's utility functions. That might be family size, income, a lot of things. At least the demographic
components of z are usually available to us from
something like the CPS. So this is not a data problem. We may not be able to match individuals to the
products they bought, but we know the distribution of individual characteristics
from the CPS. We're going to have a model
for individual utility given its characteristics, the price of the
product of interests, and the price of other products, which is p minus j, and
some parameters which will determine the
interactions between characteristics and preferences. Then the aggregate demand is
just the integral or the sum over everybody's utility will generate for them a quantity, this quantity for individual
characteristics z, and then the way you'd get aggregate
demand, you just sum out. You want to put
down an integral, you can think of some,
it's fine, I don't care. It's easy enough now to calculate this with
modern computers, so it's just not an issue. What you do is you
take ns random draws. You just take random draws from the CPS or from the
distribution in the CPS. Each random draw is a q. It's a random draw
and z, it's a q. For that Theta I can tell
you what that zi would do. That's what the utility
function tells you. If this is my zi and
these are the prices, and we have the parameter vector that before the function will tell you what he will buy or how much of what he will buy. Then you then just add up. For every random draw, you add up over the quantity is bought, and you divide by ns,
so that gives you an average of these people. That's your estimate of the
demand at that Theta vector. If E is the expectation of that, the expectation of
any one of these, it's a random draw from
that distribution. The expectation of any one of these is exactly
the right thing. Is that clear? The variance
goes down like one over ns. I can get an unbiased estimate
with as much precision as I like by increasing ns
sufficiently. Is that clear? There just isn't a problem with doing heterogeneous
agent models anymore. It's a little bit of computer,
but with modern computers, this is like milliseconds. It's just there's nothing to do. I'm going to argue that even
if you're going to work in product space, again, I probably should have
two sets of things, a bunch of points for the
lecture, and then a handout. But I'm too lazy to do that, so you just get a handout. Even if we stay in product space and we're not going to
stay here very long, I would argue that with modern
computers there's really very little excuse not to do heterogeneous agent
models anymore. There are two very good
reasons to do them. One is you can combine data on different markets and/or time
periods in a sensible way. If you had market level data, which is the typical thing, say you're going across
counties in Connecticut. The one thing we know is that the impact of price
depends on income. People with low income or
low wealth of other kinds, care about price more. You want to combine data
from different markets. I don't know, we're in
Connecticut, we're doing counties and there's granites
where all the guys from Wall Street live, and there's, I forget
the name of the city, they just went bankrupt. Starts with a B, Bridgeport. The distribution of
income in Bridgeport and in Greenwich are very different. You should expect to get very different price
coefficients in those two. If you just put all the data together and then assume
a representative agent, it's not clear
what you're doing. Whose price coefficient
are we estimating? Why are we estimating it? Why should they be the same in the two cities? Is that clear? Whereas it's very easy
to do this once you have your model and let the
income take care of it. The other reason to do it in heterogeneous
ancient framework is because often we really do want the distribution
of responses, particularly from government
policies by the way. This table is designed
to convince you of this, maybe the Bridgeport example,
Greenwich example did, but this is a table of counties
across the United States. You can see that this is the average fraction of
people in 0-20,000 range. This was done in 2004, is probably about 2001 data. If you go down here and
here's the variance across counties in
those proportions. You can see particularly
in the high-income groups, where most of the
products are directed, the coefficient of
variation is about one. That's a lot of variation in proportions in different income classes
across different counties. It would make it very difficult to aggregate without something that allowed for
the heterogeneity of the different counties. We're going to stick with
heterogeneous agent models, but even if we stick with
heterogeneous agent models, there's still a question
whether we want to do product space or
characteristic space. I'll tell you now, I'm going to start
with what happens in product space and
I'm going to tell you why we don't use it
that much in IO anymore. Although in cases where these
problems don't show up, you might well use it.
There's no reason not to. As long as you're
careful when you use it. Then I will go to
characteristic space. Whatever we do, whether we start with product space or
characteristic space, we're going to start with an individual consumer's
utility function. Then derive the quantities that the individual does and
then aggregate over people. I'm going to start
with the individual consumer demand function
or utility function. When you use product
space from the point of view of markets, differentiate
product markets, you end up with too many
parameter problems. Let me explain what happens. If you're doing demand
between capital J goods. You're going to have a quantity on one side for
each of the goods, even if it's only log
linear or linear, you've got the price of each
good on the right-hand side. That's J coefficients on the order of j coefficients
for every good. If you have J goods
as j squared. Parameters you have to estimate
even if it's only linear. Is that clear? It's
not unusual to have over 50 products
in a market that you're doing differentiated
product analysis with. Indeed, the order of examples that I've used is
going to come up with one of the sets had 100
products, the other had 200. Even if it's just 50, 50 squared is 2,500, there is no dataset that's
going to allow you to estimate 2,500 parameters with
any precision at all. That's what we call the too
many parameters problem. I don't know what the
official title is, but it's the problem that moved us into characteristics
space, really. The other problem with
doing it in product space, is if you're trying
to analyze a market, one of the questions you
want to ask typically is, what would be the
demand for a new good? Is that clear? You
can't do that in product space until the
new good's already out. Is that clear? Because there's just no
information and the data on it. If you have
characteristics space, we'll come back to this. There's things you can
do because you can ask, I now have a distribution of preferences over different
characteristics. If I put in a new set of characteristics,
what would happen? Because there's information
on preferences over tuples of characteristics and we will have some information on the
characteristics that you put in. Those are the two real reasons that we moved the characteristic space
and we'll come back to it. The first reason, by the way, under a lot of historical
work on-demand system. Everybody knew this
was a problem, we weren't the first people
to say it was a problem. There were various
ways around it. There is a Hicks composite commodity
theorem which is said, I'm going to assume that all
prices move in proportion. Then you can collapse
the demand system. But that doesn't allow you to analyze substitution
between products. It just allows you to look
planning expansion path. Similarly, there's things
like CES preferences. CES preferences means that any two goods are equally
substitutable for each other. You can't really analyze
competition between goods in a market because they're all equally competitive
to each other. Also it has very funny
welfare implications, so Aviv will get to
that, worse than the logit implications which are bad enough with your logit. Those, we ruled
out automatically. The one that actually
got some bite in the literature is
Gorman's polar forms. This is a multi-level budgeting. The idea of multi-level
budgeting is pretty simple. You first allocate expenditures
to groups of goods, and then you analyze within each group separately.
Is that clear? I know how much
money I'm going to allocate to cars or
to something else, and then I'm going to
have to do the demand for individual cars within
that budget allocation. To do that, you need
two properties. One is you have to have what's
called weak separability. What happens in there is that the purchase or the
prices for goods in one group have to have exactly the same impact on
the goods in the other group. Because these are
sub-utility functions for vectors of goods. The cross effects
can only gum through the effects on these Vs. There's a contribution
of BD here, but the cross effect between V1 and VK between the
goods in one and K. These are grouping things
have to be exactly the same. Then the second thing is
you have to be out so that allows you to substitute down and do each one at it separately given their
budget. Is that clear? The second thing you need is
you need to be able to form a price index for
the budget group. What Terrence [inaudible] did, was he gave necessary and
sufficient conditions for these things to be true. The only things that work, so these are necessary
and sufficient. It's an if and only if
statement, is weak separability. The separability between
these bundles of goods and constant budget shares within a group or strong separability, which is additive utility. One group is added to the utility then other group
is added to another group. Then the budget shows don't need to go
through the origin, but they still have to
be a straight line. This is what happens in the
almost ideal demand system. This is all in the
way of background. But if you see almost
ideal demand systems, that's essentially what
they're analyzing. Every good has to have the
same income expansion path, which we don't think is true. The groups have to satisfy this weak separability
condition. You can think about
doing this and if the sets aren't very disjoint, if you don't have too many sets, you'll find out
that it's going be very difficult to do it because there are
expensive small cars, and there are
inexpensive large cars, and expensive large cars. You'll have to start
splitting them out. If you wanted to do VERs, you wanted to do something
that had to do with imports, you'd have to also split out the goods that came from Japan, from the goods that came
to the United States. They'd all be separate groups. You have to assume that
this characteristic, that there's no cross-price elasticity between the groups. There are hard
conditions to meet and actually they're going to
reappear a little bit. The other problem with
them, by the way, is it's very hard when you have a continuous characteristic to start splitting it
up into groups. The continuous characteristic of most interest to us is price. Is that clear? Price
is distributed continuously and you cannot
separate it out into group you'll see, because in demand
systems there's going to be an error term and
it's going to be correlated with price typically. If you started separating it out between
high-priced goods, low-priced goods, you'd get
the error terms selected. You'd have a selection problem
on top of everything else. This, by the way, it goes with all grouping procedures
nested logit, as well as the almost
ideal demand system. Those problems are
going to reappear. Characteristics space.
That's background, that's why we are where we are. I'm going to give you
a short introduction to characteristics space now. We're going to treat products as just bundles of
characteristics. If I know the characteristics
of the product, I know the utility
for the product. Now, these characteristics, some of them will be observed, but we're going to allow for unobserved
characteristics also. I'll show you why in a second. Essentially, what we've done
is we've changed this space that we're operating on from product space to
characteristics space. Why is this an advantage? It's going to get
rid of some, but not all of the problems. What we usually say is a
new space typically has much smaller dimension
than j. I can summarize the taste for a
product by k characteristics, where k is much smaller than j. If for example,
we're doing autos and we thought that there's a preference
for each character, just say it's linear and there's a preference for each character, so preference for price, there's a preference
for car size, there's preference for SUV, etc. Say, we had five of
those characteristics, that's probably not enough. But say, we had
five and we assume the distribution of those
preferences was normal. You'd have five means and then you'd have the
variance covariance matrix, would be 6 times 5 divided
by 2, works out 15. That gives you 20 parameters to estimate and you can
get all 50 times 50 cross-price elasticities from those 20 parameters.
Is that clear? That's what gets rid of the
too many parameter problem. That's the too many
parameter problem, and I'll come back to these problems
associated with that. We're not going to get
rid of all the problems, I'll come back to the
problems in a second. Now, the second problem with characteristics
basically for IO was this inability to get demand for new
products, if you recall. But now, we have an
estimated demand system. It's on characteristics space. Now, I put in another good with a different couple
of characteristics. Is that clear? I
just ask what the person's will buy against all these goods,
including the new good. Again, there going
to be problems with that, but in principle, at least, it's doable.
We have a chance. All the literature starts
with a gun and we call them Lancaster and then Dan McFadden started the
econometrics of this. There is also this extensive
related literature in IO, which was really a literature
about product placement. If you read Tirol about where are we going to
place new products, it'll be models like
this in characteristics. Hoteling, there are
people on a beach, and one characteristic is where the guy sells ice cream and the other characteristic
is where you are. That's really just
a characteristic. We'll return to
that in a second. There's been a lot of
literature on this. The two problems
we had before are going to reappear here in
a slightly different form, and it's because
the appearance is slightly different
form that sometimes, we can get around them. Again, if there are a
lot of characteristics, you're back into the too
many parameter problem. Is that clear? The whole problem would be
if I had 50 characteristics, I'm back to where I was before. Typically, by the way, producer goods have really
a very small amount of characteristics, you
have to keep track of. Think of computer chips, you need to know the speed,
CPU, maybe reliability, maybe size of chip, and if you do R-squares of
quantity and price on that, you'll get R-squared is
about 0.95 and 0.98. Where it's not quite true
is in consumer goods, because if you really
thought about a car and the differences
between different cars is 5,000 characters, you could put in the
leather in the seats, the color of the car,
and everything else. We're not going to
be able to put in every characteristic
that matters. We're going to put
in what we think are the major characteristics,
but then, we're going to allow for an unobservable characteristics
which picks up all the little things that we didn't pick up in the
observable characteristics. Is that clear? This
unobservable characteristic is going to end up being
a little bit of a problem because there are
all these little things. It's not so true anymore, but a mercury atleast looks
like just like a Lincoln, if you did all the
standard car size, seats, everything, they're
just like a Lincoln. What the Lincoln had
was a lot of little do that that made people want
it and Lincoln price too. The price of the Lincoln
was more than the price of the mercury because of
all the little things that go into this unobservable. Once we allow for that, we're going to have a
price endogeneity problem. There's going to be priced
on the right-hand side, and then there's going to be this unobservable
characteristics, which is correlated with
price. Is that clear? Just like you have a
standard demand analysis, the problem here
is it's going to be in discrete choice analysis, is it's going to
be buried inside as we will show you
in next lecture. It's going to be buried inside a non-linear function because
it's discrete choice, so it'll be the probability of some event which has got to be non-linear because it's got
to be between zero and one. Is that clear?
Standard things that you've been taught
don't teach you how to do non-linear
instrumental variables. We need a solution
to that problem, and that's one of the
things BOP will bring. That's the two-minute
characteristic problem. On the new good issue, you
have to be quite careful. You'll do a reasonable job on goods whose
characteristics of the new good are
similar to the goods that are already sold. They're somehow spanned by the characters of the goods
that are already sold. My favorite example of this, if you try and do it for goods that are out of the
characteristics space, you're going to do terribly because it's all
just a projection. Typically, very new goods, the reason they're
very new goods is because they found
another dimension. My favorite example of this is the desktop and the laptop. If you would have figured out before anybody put in a laptop, you would have tried to
figure out what will the demand be for the
laptop at different prices. The characteristics to the
desktop that typically went in were memory, speed, and resolution
of the screen maybe. All of those were
better on the desktops and on the laptops when
they introduced the laptop. The laptop had a higher
price so you would predict zero demand for the laptop. What's different
about the laptop? The size and the weight. Even if you had size and
weight of the desktops, it's just so much out of the space of the size
and weight of it. There's all that projection that would actually
want haywire. You have to be a
little bit careful when you're doing your goods. People who make real
money on real new goods, it's because they think it's
something that really is out of this space of things
that we know how to do. Now, I'm going to become
a little more technical. I'll just start you off and then [inaudible]
will come in. I'm going to start
with a model of a consumer choosing at most one of the finite set of goods. By the way, welcome
back to this. I'm going to define a
product as a bundle of characteristics which include this unobservable
characteristics. I'm going to assume
preferences are defined directly on those
characteristics. Then I'm going to assume that each consumer
chooses the bundle. At least, until
the last lecture, I'm going to assume
each consumer chooses the bundle that
maximizes its utility. It's only going to be one good, and it's going to
choose the bundle that maximizes the utility. Consumers are going to
have different preferences over these characteristics, so different consumers are
going to do different things. Those preferences are
going to be related to the characteristics
of the consumer, some of which might be observed and some which might
be unobserved. The way I'm going to
get aggregate demand is I'm going to just have this utility function for each consumer as a
function of Theta, this parameter vector which
tells me the interactions, how much a person of this income group dislikes price versus some other income
group dislikes price, or family size, likes car size or whatever you think the
irrelevant interactions are. Those Thetas are
going to determine how important those things are. I'm going to say
fix Theta and that every consumer choose
his maximum if that were the true Theta. Is that clear? That's going to get me a
demand for each consumer. If I'm going just
to aggregate data, I aggregate over the
types of consumers to get aggregate demand.
Is that clear? That gives me the
model's prediction for demand conditional on Theta. I know what demand was. I find the value of
Theta that makes the model's prediction as
close as possible to the data. That works, I'll be consistent
and asymptotically normal. That's the model, they're
all economic models, no matter what
anybody tells you. There's a model, conditional
parameter vector. Then there's data, and you find the parameter vector that makes the predictions of the model as close
as possible to data. That's what's going on when
you do general [inaudible] Now, I'm just going to make that a little bit more formal. The utility of the
individual i for good j is going to depend on xj, which are the characteristics
of the product, and pj, which is the
price of the product. Price is just another
characteristic. We generally treat
it differently because when we're doing
equilibrium calculations, price will respond
and will assume, at least, statically
that the xs are fixed. The products have
given characteristics, but the managers of
the products can change price in response
to environmental change. Is that clear? Whereas it takes time to
change the characteristics. We're not doing, but if we
were doing dynamic models, we'd also endogenize
the characteristics, that's the difference between the dynamic model and
the static model. xj, the characteristics
pj are the price, and nu i are the
individual attributes. They were Zi before, I should have kept
that i probably, but nu i is in the literature, so I'm going to use nu i. There's going to be a j inside products and one
outside product. Is that clear? The j inside products are the products
that are actually competing in the market. They are the cars that are
competing in the market. The outside product is what you do when you don't buy
any car in the market. Is that clear? It's
a fictional thing. Often, it's model
as the utility of income after I pay the price of the car or
the price of the good. What's the utility
of that income? It's important for
several reasons, keeping the outside good, it's the only thing
fictional in the model. If we didn't have
an outside good, we couldn't model
aggregate demand, which is because everybody
goes repurchasing everything. Again, I need people moving
on the extensive margin. I couldn't model
aggregate demand. Also, if I didn't have
the outside good, I'm not analyzing a random
sample of consumers, I'm analyzing a sample of consumers that did
purchase a car. They're going to
be different from the random sample of
consumer in the CPS or otherwise, and I'd have to take account of that in the
estimation algorithm. Questions are fine. If I
didn't have an outside good, it'd be very hard to analyze the benefit of new
goods because much of the benefits of new
goods are drawing people in who weren't
purchasing a good before, and now, there's
a good that they liked enough to purchase. For all those reasons, we keep the outside good and
analysis almost always. The product characteristics, of course, are constant
across consumers. Now we want to know what's
the demand for product J? Remember everybody has
this utility function back here, conditional on Theta. The demand for product J
are just fix that Theta, how many people have
uij greater than all the other uik's for that Theta? You fix there,
that means you fix everybody's utility conditional
on their characteristics. They're going to be
some people where uij is greater than all
the rest of the uik. Those are the people who
are going to buy uij. Formally what you do
is you find the set of these news and then you just find the number of people
who are in the set. You just sum up all the
people who are in that set. That gives you the
prediction of the model for the share of good J given x. When there's x
without a subscripts, it's the whole vector
of x's one for each product and the vector
of p's that gives you the prediction from the
model and then what Aviv is going to tell you
how to do is fit that prediction to
the actual data. You have the data on the shares, and you just fit the
prediction to the data, you find the Theta
that fits the best. Then there are different
ways of doing that. If you want to total
aggregate demand, you'd have to
multiply the shares by the size of the market, which is capital
M. Some details. This utility function
is only cardinal. That is, I can multiply it's invariant to what's
called affine transformations. For each individual now, I could multiply all of the utilities that individual
by a positive constant, and I could add a constant
negative or positive to each of the utilities and
it wouldn't change his choices because all
that manages the ordering. Is that clear? In
econometric terminologies that means I have to
normalizations to make. The way they're usually made is I set the utility the
outside good to zero. That's like saying, I'm subtracting the
utility outside good from each of the inside
goods. Is that clear? That has implications,
by the way, and usually what happens is
there's an error term in the utility function
and the coefficient of the error term becomes one That's the positive
normalization. That's the positive constant. You've got to be a little
bit careful by the way because now you're estimating
the utility for good j. You're estimating for a car. You're not really
estimating for car now, you're estimating the
difference between the car and the outside good. If the outside good
is the means of transportation to work
and you're in New York, it's very different
than when you're in Framingham. Is that clear? You've got to understand
that what this uij is now, it's really uij minus
ui0 and that has an implication for how you
model what's going on. Here's some simple examples and then we're going to
finish this one on time. The other ones, we might not. There are two models that
rows in IO all the time. One was this simple
horizontal model, people on a beach. We're going to
generalize from these, but let me give you where
we're starting from. There are people on a beach and ice cream
vendors on the beach. Delta j is the location
of the ice cream vendor, nu i is your location, and then you don't like price. This is a model with two
product characteristics, the location of the
ice cream, and price. In this particular model, characteristics are
called horizontal because different people, because they're in
different locations, like different products
differentially. There's not common
ordering of products. Depending whether I'm
close to the product or not, I like it
more or less. Oops. These are typically called
horizontal characteristics where there isn't a common
ordering of products. We don't all agree on
the order of products. The alternative is the
vertical characteristic model. The characteristics is Delta
j and we all agree on it. We all agree that one good
is better than another good. The difference between us is that we have different
coefficients on price. We have different incomes
or different wealth, and we care about
price differentially. You have a free normalization. This nu i really is
just a ratio of how much you care about quality to the ratio of how much
you care about price. Because I could
multiply everything, I just normalize this to be one. That's the other
one that appears in IO and I'm going to tell
you the generalization. The other one you've probably
seen is the logit model. The logit model just
says uij is Delta j, which might be a quality
minus price if you like, and then these Epsilon ij's. The intuition is there
is a mean utility and then there's
differences across individuals of the utility
for this particular good. The reason you've heard about the logit model is because it has two nice features. Those Epsilons, by the way, they have to be for the general model without
the logit assumptions. Those Epsilons have to
be independent across individuals for a given product. I look at the Epsilons across individuals for given product,
they are independent. I look at the Epsilons across products for a given individual, they also have to
be independent. You're going to see that those
are the bad assumptions. But if it also has this functional form which
is extreme value type 2, it has these wonderful
magical properties which are the probability of
the maximum being a good has a closed form and we will show you
what that is shortly. I can tell you what the
probability is of choosing good j without ever doing an integral over the
Epsilons. Is that clear? I know the functional
form for it and moreover, I can tell you the
expected welfare of the agent, given the Deltas. The Delta is a carrier sum. These are the two
properties of logits. They're the only distribution
that I've ever seen, and I think there's a proof that they're
the only one that exist that has these
two properties. Remember, for each individual, everybody with me, the
probabilities of what he's going to choose is
just the probability that Delta j is the max. If I wanted to form a likelihood or a moment based on that, I need that probability. This is telling you that,
that probability has this nice analytic
form. Is that clear? I don't have to
do any integrals. I can get it exactly right. Is that clear? Before
you had computers, this was an invaluable asset. Because you couldn't
do it any other way. Similarly for the
expected utility. The thing that you should
realize about this, and we're going to come back
to it in different ways is this model implies that no
matter what good you chose, you chose good j, your preferences
over other goods are the same as anybody else's. Because it's an IID
Epsilon take out one good. Is that clear? The Epsilons of that good have no
relationship to the other goods. Epsilon. Is that clear? This is called the
independence of irrelevant alternatives
assumption or IIA. I'm sure you've heard
about it. It has very many bad properties. If you think of what's
going to go on, you increase the price of
a big car. Is that clear? You think that people
that will leave the big car will go
to other big cars. Is that clear? The
reason they bought the car in the first place
is they wanted a big car. Is that clear? That can't happen in the logit. As I say to my students, if it does happen, you have a
computer programming error. Mathematically it can't happen. Those are the things
we want to get rid of, those kind of things.
Oops. Wrong page. Let me show you just how bad these are before and then show you
the generalizations and then Aviv can
tell you how to handle the generalizations. I'm going to think of
two. I could have done the horizontal model,
it's very similar. I'm going to do the
vertical model. Assume we were doing
the vertical model and we're going to try and
take that to data. We all agree and the
ordering of goods, the price of J, we want to
estimate the Deltas and maybe the impact of the
distribution of the news. What's going to
happen in this model? What's going to happen
in this model is if I ordered the goods
by their quality, I have to order them
by their price. Because we all agree on
quality and nobody's going to buy a lower-quality
good at a higher price. The order of quality
is exactly equal to the order of price.
Is that clear? That's going to have
to be in the model. Now consider what happens. There's the J is good. Let's see what
happens when we do something like increase the
price of the J is good. We've ordered the people, so their J goods are
increasing in quality. People with a lower
new high with the least aversion to price
by the highest quality good. That's the only
difference between people as these new highs. They'll buy the highest
quality goods and so on. What's going to happen
is I'm going to have the people who buy good J. There's going to be a bunch of people who are almost
indifferent between buying good J and the one that's slightly higher-quality
J plus 1. There's going to be some
people who buy good J who are almost in different
[inaudible] but the lower quality good J minus 1. Now I'm going to increase
the price of good J. The only people who can move are the people who bought good J because they were better off before another price
of good J is going to. The only people who move
with the good to buy, that people only will
be the people on the margins that were
indifferent before. Now the price of good J went up, the guys that were just on
the margin of buying good J plus 1 will probably
move to J plus 1. The guys who are
just on the margin, J minus 1 will
move to J minus 1. But the only people who move
are the people who have J and they only move to
J plus 1 or J minus 1. Again, this is just
an implication the model won't allow
anything else to happen. If you consider like the
auto market which I've used is going to go into
more detail one. You just line of
cars up by price. Say we got the Ford Explorer, which is an SUV. It was a neighbor
in price space. The Mini Cooper S, which
is approximately right. Then the next neighbor is the
GM Sport utility vehicle. If I increase the price
of the Ford Explorer, the model is going to say that people can go to
the Mini Cooper S, but nobody will
substitute to the GM SUV. It's impossible.
It can't happen. Similarly, if you think about the elasticity
with respect to price, and remember, elasticity and we're going to
come back to this. They determined markups.
Because you have price. If you're using
simple Nash formula, this price equals marginal
cost plus a markup. The markup is one over
the elasticity of demand. The elasticity in this model
are going to be determined by the density of
people on the margin. Because if I increase
the price and there's a lot of people
just on the margin, they'll be a lot of people
who will move over. Is that clear? If you had a symmetric
distribution. For example, in the
coefficient of price, which we think is usually round. The elasticity on one side
will be exactly equal to the opposite side's elasticity because the densities are going to be the same in a
symmetric distribution. Is that clear? The people who have very
high aversion to price are going to have the same elasticity as
the people who have very low aversion to
price by construction. If you take that to the
economics of the problem, it means the people who buy the markup on
the high price cars are going to be exactly
the same as the markup of the people that were
buying the low-price car. We know that's crazy.
It's crazy in the data, but it's also crazy for
good reason because high-priced cars have high sunk costs and
they have to have high mark-ups just to justify the sunk costs, if nothing else. Those things have to happen
in the vertical model. That's what's wrong with
those kind of models. Now, the logit models. I'm going to tell you what goes wrong with the logit model. I'm going to give you
the two generalizations then I'm going to
hand it over to Aviv. The closed form for the
probability is just exponent Delta minus p over 1 plus
the sum of the exponent. You can go and prove that,
it's a lot of pages, interchanging variables
and formulas. Every time I do it, it
takes me a day to do it, but it's true.
That's the answer. These are the analytic forms. This implies that if
you just work it out, the derivative of good J
with respect to the price of good Q with respect to the price of good K is
going to always be SJ SK. Is that clear? Two cars with the same share have the same cross-price elasticity
with every other car. The model will not
allow anything else to happen. Is that clear? The example we
used in the paper, I think is the paper was written when the Hugo was in for what? The Hugo was a Yugoslavian car, which was only in the market for one or two years.
It was a lousy car. It had a very small price and
it had a very small share, but it was a lousy car,
but had a very small price versus enough to induce
certain people to buy it. It had almost identical shares
to the high-end Mercedes. Because a high-end
Mercedes costs $90,000, very few people, but
it was supposed to be a very good car. Is that clear? This says that when you
increase the price of Hugo, people go to different places. You increase the price
of the Mercedes, people go to different places
and they're going to go in exactly the same proportions. Is that clear? Which is crazy. It also implies that if
we had a single product, Nash in price equilibrium,
they have to have exactly the same markups. Everybody knows
that the markup on the Mercedes is greater than
the whole price of the Hugo. Just to make any sense, the own price derivative is S1 minus S. That's the
reason that things with the same price have to have the same markup because the own price derivative
is the elasticity. Again, no data will
ever change this. If you get a computer program which tells you use the logit, it tells you it's
different from this, you've got a bug in the computer
program. It can happen. Here's the generalizations and this is what we're going to work with at least until the
last two lectures tomorrow, where we're going to
go into newer things. The first generalization I'm going to call
generalization 1 is what Anderson [inaudible]
and their book on the theory of this stuff
called an Ideal Type Model. Steve Berry and I call the
Pure Characteristic Model. All it does is it generalizes on both the horizontal
and the vertical model that allows for multiple
characteristics, some of which can be horizontal and some of which
can be vertical. It looks like this. It just says, look the agent
with characteristics nu i, which is a vector,
preference for goods, is he has a separate
preference for each good. It might actually
depend on many news. It might depend on his income. For each characteristic. It
might depend on his income, as demographics and
everything else. We have a bunch of
things which are termed my preference for
each characteristic. Then we have many
characteristics. The preferences can be horizontal or vertical
on each characteristic. You're supposed to
estimate the Thetas. For obvious reasons, it's a
pure characteristic model. The characteristics of
individuals interacted with characteristics of products.
That's what's going on. You can show by the way, that the ideal type model that theorists use,
which is this one. There's an ideal type for me, which is nu IK and then there's
characteristics of the products which are XJK. The ideal type model is exactly the UIJ model up above after you do
appropriate normalizations. This is just the theory version of everybody has an ideal type. We have a metric of how far I am away from the ideal type. That's what the pure
characteristic model is. The difference. Let me now just say that's the pure characteristics. BLP is identical
except for one thing. It adds an Epsilon IJ. These are going to be
Typed II extreme value, the same thing that
was using the logit. Now, however, there's a difference because
what's going on is now I increase
the price of good J. Very particular
people leave good J. Those are the people who
liked the characteristics, the people who bought
good J, is that clear? Or the people who liked the
characteristics of good J. Because of the nu IJ, they're going to move
to other products with similar characteristics. Because each
individual has a nu I, so he likes different
characteristics differentially. He bought this car. Some people will leave because the price of that car go up. But their preferences or
characteristic are constant, so they're going to move
to similar kinds of cars. That breaks the IIA property, the independence of irrelevant
alternatives is property. Because if I'm in
this kind of car, I like cars that are of
these characteristics. If I bought this kind of car, that's because I like cars
of these characteristics. When I go to look
at the other cars are going to be cars like that. Let me now talk a little bit
about the difference between the pure characteristic model
and BLP just for a second. The only difference between
these is the Epsilon IJ. You might well think this, you can show that the pure characteristic model
is a limiting case of BLP. It's not a special case. It's actually
difference matters. The way you can say
that is just put a Sigma on this and
normalize something else. You always have to
normalize one thing, here the normalization
is on the Epsilon IJ, it has a coefficient of 1. Put a Sigma and Epsilon
IJ and normalize something else and take
that Sigma to zero. Then the limit, you'll get
pure characteristic model. Why am I telling you this? Why do I bring up the pure
characteristic model at all? Because BLP has problems
for some markets and the problems are related to the presence of the Epsilon IJs. The Epsilon IJs, so when you think of
unobservable things that are affecting demand, they're either individual
characteristics that are unobservable or product characteristics that are unobservable and they're
not just from anywhere. Is that clear? If their individual
characteristics are unobservable, they're going to be
correlated cost products. If their product characteristics
that are unobservable, they're going to be correlated
across individuals. You think that the Epsilon IJ, if [inaudible] they
should be there. The real reason there
is computational, which has nothing to
do with economics. Is that clear? You might
want to get rid of them. If you got rid of
them, you'd be back at the pure characteristic
model. Is that clear? The reason I say
it's a limiting case is because it works out
that it's very hard. If you really did have
a Sigma up here and you divided by Sigma and let
that Sigma go to zero, which is the way
you do the proof, you get coefficients
that are very large. Because everything else
is going to blow up, you're going to divide by
something going to zero. Is that clear? Computers
can't handle that. If you really believe
the real world was pure characteristic world, and you asked maybe
BLP, you'd think, well, if it was pure characteristic,
maybe it would replicate the pure characteristic
model by just getting a Sigma which is small enough. Is that clear? But the
computers won't do that. It doesn't do that very well because you'd
need a Sigma that's very unique to evaluate numbers which are too large
for a computer to handle. The question is, when
is this problematic? There are two cases where
it can be problematic. What happens with
the logit errors is they can go off to
infinity and negative minus, there have what's
called full support. If you put in another good, even if it's very close to the
goods that are already in, somebody is going
to like that good infinitely more by the model because some guys are going to draw an independent Epsilon
that's way out there. That's going to limit
cross-price elasticities. You cannot put goods
close enough to get their cross-price
elasticity too high. It's impossible. There's
a lower bound to it. Now again, we've done a bunch
of simulations on this. It only ends up being real problematic when
you have a lot of products and some that are very close to each other.
Is that clear? Usually, BLP does pretty well, even if the real model
is pure characteristics, which is the Monte-Carlo
that we did. The Monte-Carlo that's
been done on this is you assume the real data is a pure characteristic model, you estimate BLP, and you see how bad you do. The only time you
really do badly or when there's really lots of goods very close
to each other. You can ask whether that
will ever happen or not. But if it does happen, you
can have problems with BLP. Similarly for welfare, you can have problems. Again, you might want to go to the pure characteristic model, not approximately
straight going there, that model ends up
being hard to estimate, just hard to compute. People are starting
to use it now. Aviv is going to tell you how
to compute BLP in a second. It's much harder to compute the pure characteristic model. Though with new
advanced techniques, it's being computed
by quite a number of people that have been using the pure characteristic model. For most part, the reason
you see BLP is because it's easy to use rather than the pure characteristic
model. Is that clear? For most markets we've analyzed, it doesn't make much difference. I'm going to stop here and now Aviv is going to tell
you how to do BLP. We're taking a break
first. Yeah, Dennis. Dennis: [inaudible] Ariel Pakes: Stripped
of the original. The linearity doesn't matter, the covariance is going
to be a small number. Let me just tell you
what Dennis, just ask. Interpret it. Let it be Delta J and then I have a full variance-covariance
matrix of errors. Is that clear? BOP is a
special case of that. Not quite because that can
have observable things, but if I only had
unobservable characteristics, if I didn't have micro-data which matched
individuals to what, I would have exactly
what Dennis said. I have a
variance-covariance matrix. The variance-covariance
matrix has j times j plus 1 divided
by two parameters, which is back to the
j squared problem. Is that clear? Then
the question is how do you structure it and what this is doing
is structuring it. It's saying people have
different utilities for different characteristics
and that's what's generating the
covariance across the products and much of
the cross-price losses. For example, if you don't have the right characteristics
in there, then you're not going to get the right substitution patterns for exactly that reason. But the structure that's
being imposed here is from a statistical
point of view, is the structure of
the covariance matrix derived from preferences
on these characteristics. That's what's going on from the statistical point of view. Dennis: [inaudible] Ariel Pakes: But that's
what we're doing. Dennis: I know. Ariel Pakes: That's
exactly what we're doing. I'm not saying it's the
only way of doing it, but that's exactly
what we're doing. We're letting it feed through the characteristics
of the product. That's going to structure the covariance matrix and
that's what's going to reduce the dimension
from j to j plus 1 to k, to k plus 1. That's exactly what we're
doing. Nothing more than that. Dennis: [inaudible] Ariel Pakes: You're going
to see some of them. Have you ever gone over
something like nested load, just a special case of this? You'll see there are problems with their problems that
people run into with them. Dennis: [inaudible] Ariel Pakes: If it's cars and you're worried about
traveling to work, you really want to know if
there's a subway system, what's the other ways of getting
to work from this place. In principle, you'd like
to also know, by the way, is what the second
car holdings are just the other person which puts you into a whole
another set of problems. But because the second
car holdings were correlated with the preferences for the first car holdings. But you definitely
want to know things like that and that might
differ between cities. The UI0 would be different
between different markets. Dennis: [inaudible] Ariel Pakes: What
you would try and do is you would try and get the characteristics that
determine the outside good, that's what we did in BLP, and put it back in the model. The utility of good J depends on the
characteristic of J and the characteristics of
the outside alternative. You would estimate
them jointly and the characteristics of
the outside alternatives would vary across markets. That's what you'd try and do
and that's the logic of it. 