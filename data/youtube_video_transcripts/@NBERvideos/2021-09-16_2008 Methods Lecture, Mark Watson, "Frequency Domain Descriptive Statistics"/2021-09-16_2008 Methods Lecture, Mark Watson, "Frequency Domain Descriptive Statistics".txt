Mark Watson: This is what
I'm going to do today, but I'm going to come back to that. Let me tell you a little
bit about the course before we get going
in today's lecture. When Jim and I were
thinking about this, we thought about some
themes, if you will, that time-series
econometricians have been thinking about over the last decade or
15 years or so. One thing is there's
been a lot of work, and a lot of interest in
low-frequency variability. Obvious things, unit roots, co-integration, fractional
stuff, all of that. But it shows up there, but it also shows up in
models of instability when you've got time
variation parameters, and some structure. It shows up if
you're interested in thinking about
stochastic volatility, macroeconomists thinking about great moderation or things like that worry about that
low-frequency variability. Now, in variances
instead of means, but it turns out to
be the same problem. We want to do inference
so we need to use hack standard errors right and the same issues
show up there. Of course the issue of long-run identification and structural vector
autoregression. All of these are
low-frequency things. We want to talk about those. The other thing is, econometrics, if you will, is all about identification which makes
econometrics I guess, different than statistics
maybe, I don't know. But anyway, there's
been really lots of work in understanding inference in situations where you
have weak instruments, and Jim is one of
the guys in that, and so he's going
to spend some time this afternoon
talking about that. Then of course, it's a
standard, if you will, topic in time series forecasting and we'll talk a
little bit about that. There's been some new
techniques if you will, thinking about evaluating
or assessing forecasts. Those are the main topics. Some tools that some of
you are familiar with, all of you are familiar
with some of these. Some of you are familiar
with all of them. But the key tools
we've listed here, standard stuff, VAR, central limit theorems, filter spectra. Those are all standard tools, tools that may be less familiar
empirical process stuff, functional central
limit theorems, and simulation methods. If you MCMC or bootstrap things or maybe
a little less familiar. We'll blend in these
tools in the context of the particular
substantive problems that we're going to talk about. This isn't going to be a survey so if we don't talk about
your work, don't get mad. You can get mad, just
don't get vocal. Here's what we're going to do. You've probably
seen the outline. Today, this is, if you will, what's new in time
series econometrics, we're going to start
with what's old. The first lecture
probably is like 1960s. That's because some of the
stuff from the '60s is pretty good, like Woodstock. That's what we're
going to do and Granger [inaudible] was good. That's what we're going to
do. That's his classic book. That's what we're
going to do initially, I'm going to talk about some
frequency domain stuff. Again, this is old stuff, but we're going to be using these concepts later on
and lots of other stuff. It's important that
we review them. That's what's going to be
today's first lecture. Then the second lecture is, I'll talk about the
functional central limit theorem and start talking about low-frequency
variability in relationships and that's going
to be structural breaks. I'll talk about testing there. The testing problem turns out to rely on this FCLT functional
central limit theorem stuff. Then Jim is going to come
in this afternoon and do weak and many instruments. Then tomorrow, again, I'll
be here in the morning, and I'm going to do
filtering, linear filtering, and non-linear
filtering if you will, the new tool there will be some Markov Chain
Monte Carlo stuff too, as one way to do some
non-linear filtering. You'll learn a little
bit about that. The application, there'll
be a few applications, but probably the
application that I'll spend the most time on is a stochastic volatility thing
and looking at changes in the variance of inflation
and output or something. We'll think about that turns out to be a non-linear
filtering problem, or you can cast it as a
non-linear filtering problem so we'll think about how
you might solve that. Then I'll finish up this stochastic time
variation stuff in my second lecture, and I'll talk about
time-varying parameter models. I'll introduce those today. Then Jim will come
in the afternoon, and he'll do a structural
vector autoregressions in 90 minutes, and then he'll do all
of DSGE econometrics in 90 minutes. That's fine. He's
got the hard stuff. Then I'll do on Wednesday for those of
you that are still here, I'll do hack and talk a little bit more about
low-frequency modeling, and I'll do forecast assessment. Then Jim will come and
talk about models where you're doing a time series model so you got a long time series, but you also have a zillion variables that
you're looking at. Large N and large T issues, he'll do some dynamic
factor models, that's one example of that, but he'll do some other stuff as well so that'll
be the afternoon. As I said, the way we planned on doing
this was you'd have these slides in front of you and hopefully maybe with the
exception of this lecture, that will be the way
things are realized. We're revising these slides in real time as when
we look at them and prepare these
lectures and we will correct typos and stuff, over the next few days. Slides will be posted
on the NBR website, but probably not until next Monday or something
when we've taken out, at least in my case, the really stupid things
that are in these. If you see really
stupid things, tell me, because once you post
it, then you're dead. There's lots of
references because even though this isn't a survey, and we want to tell you guys where you might want
to look some stuff up. In my case is the list of references will be
available Wednesday, Jim has his references
organized by lecture, I have mine all
thrown in one file, and I'm still working
on my lectures. Mine will be available Wednesday Jim's will be
available every day. For those of you that don't
get enough econometrics from 8-5, and want to
read in the evening, you can go to Jim's thing. Level of the course. I wanted to mention this. This is a real diverse group, and they were like experts here. Some of them are sitting
here, and I've told them not to shake their heads when
I say something stupid. But there's another one. God. Anyway, and I've said
I'm not talking to you guys. I'm talking to someone else. Anyway, so the level is
not going to be Anna. It's going to be someone else. Questions. This is like a
big core so in terms of, I was supposed to say here clarifying
questions please ask. I mean, like
something's wrong in the slides which you
have in front of you, you're supposed to pretend
you have the slides in front of you and gross typos. Point those out so we don't get confused as we go through this more broader questions about how does this
relate to my research? Probably it's best
to postpone those until beers or
coffee at the break. I think there's only coffee
at the break, actually, not beer, but we'll
need some beer later. That's it. Let let me now go back to what I'm
going to do today. These are a little out of order. Introduction to the course. Of course, I'm going to go through just some basic
jargon real quickly, just to remark, just so
that we're all together. What basic jargon is, and then I'm going
to move into talking about some frequency
domain stuff. Again from the 1960s. The applications of this
are what makes this useful. Now, are a couple of things
people do filter data. They use HP filters
or bandpass filters or these filters, or those seasonal
adjustment filters. It's important, you learned
this in graduate school, but maybe you've forgotten this. It's important to realize, what those things, those transformations
are doing to the data. I'm going to remind
you of that and we'll go through that carefully. It's a reminder, but again, I'm going through it carefully. Maybe the application will be, we'll talk a little
bit more about bandpass filters
than other filters just because you have to
talk about something. As an example, that's
something fun to talk about. But what I will talk
about there will apply to other linear
filters as well. Then we'll talk
about the issue of, what if you want to use one of these things in real-time, you want to estimate
an output gap in real-time, and output gap is you filter the
data to find a trend, and you subtract it
from the series, and that's an output gap. Suppose you want to
do that in real-time. You want to know what
the output gap is in the second quarter of 2008? Well, we'll see
if you'll look at these optimal filters,
they are two-sided. They require the future
and the past to figure out what the value of
the trend is today. That makes sense, but that's useful if you're
doing historical analysis, but if you're doing real-time
analysis, it isn't useful. The question is,
how do you modify these two-sided
things so that they work as well as possible in a one-sided world where you only have data up
to the present? You don't have future data,
and it turns out that it's trivially easy to do
that. I'll show you that. It's also trivially
easy to figure out how much uncertainty
there is in that, the fact that you only used one side of the data
instead of two sides. How big, how much uncertainty is there around the
current output gaps? As the application of this
1960s spectral stuff, we'll think about if you will, output gap measurement in real time, and variability
of output gaps. Most of this stuff
will be univariate and indeed when I teach anyway, most of the stuff I'm
going to be talking about is a simple examples and because simple examples get
the main idea across or get many of the main ideas
across and in this case, the simple example is
going to be looking at one series and you
can get many of the main ideas across, except you can't get
correlations across series, across when you're
looking at one series. I'll talk a little bit
about the extension of this to more than one series multivariate
spectra and of course, that's just instead of
a variance looking at a covariance matrix, and that's got a bunch
of variances in it, but it's got some covariances. That's all that is. Then I'll say a few words about
spectral estimation, but not much because
it turns out that this will be exactly
the problem that we want to study when
we think about estimating hack
covariance matrices, hack standard errors, turns out, as we will see, and as many of you know, it's exactly the same problem. We're going to spend an
entire lecture on that. Here I'll just wave my hands and say, we're going
to do it later. Blablabla, blabla, blabla. You've got these
slides in front of you so we can go through this. These are just words
I guess and letters, subscripts, some little
colons and stuff. Here's some stuff you'd know. Y_t is going to denote a
sequence of random variables. It's going to be a sequence
of random variables generated by some
probability law or described by some
probability law. The word, when you've got a sequence of random variables, the word that you use to describe the probability
laws of stochastic process. Realization is a sample
of size one in this case, that's one draw from
this stochastic process. You get a whole bunch of Ys. I'll probably be
schizophrenics about capital Y and little
y in the lectures. But here it's important to draw the distinction between the random variable
and a realization. Stationary stochastic
processes are stochastic processes
where that are stationary and
that's what that is. You know this stuff but again, I just want to have
it all on the record, if you will. Autocovariances
are autocovariances. The covariance between Y_t and Y_tk periods in the future. Autocorrelations are those
covariant stationary if the autocovariances
and stuff first, and second moments don't depend on time so you can get rid
of the time subscripts. White noise is white noise, which I always pause because
sometimes you can hear it. Can you guys hear it? You can, almost. Anyway, it comes from them, comes from up there. It's that sound. That's a covariance
stationary process which has mean 0 and
no autocovariances. A martingale is like
a random walk and that your best guess of
Y at day t plus one is Y at day t. Conditional on some information set
that includes Y. That grows a
martingale difference processes just the difference
of one of these Ys. Your best guess of a martingale difference
is of course a zero. A lag operator, L times y. L is
this operator that operates on sequences, shifts all elements in the
sequence back by one period. L2 denotes L operated
on the sequence twice, so you shift it
back two periods. If b is a constant, you can interchange b and L. Linear filter, we'll
talk about these. When I use the word linear filter I just mean
a moving average. Here is this moving
average filter. I'm going to denote
it by c of L. What it is is if
x is c of L of y, it's a moving average
of the elements of y in the past, and in the future
where the cs are weights. These cs are just constants, like 2, 14, numbers like that. Y can be a filtered version of a white noise in a couple
of different ways. Difference equation way,
like an auto regression, are just as a simple
moving average as in a moving average process. I'll typically use Phi or AR polynomial and Theta
for MA polynomials, and I always put
minus signs in here. Most of the world puts
minus signs in here, except I think RATS. Does RATS put minus signs
in? Anyway, I don't know. I hate that program. Don't tell them that. Then you can put these things
together. That's this. This is familiar. This is the Wold
decomposition theorem. The Wold decomposition theorem says if you have a covariance stationary
process basically, you can represent it as
a long moving average, where these Epsilon shocks are the one-step ahead on forecast
errors of Y constructed using a linear forecasting rule where the forecast is
linear in lagged values of Y. Epsilons that have this
property they're one-step ahead of forecast
errors from some data that you observe. Those guys are called
fundamental. That's jargon. Jim will come back
and talk about fundamental stuff when he talks about structural vector
auto regressions, because it arises there
these structural errors, things that you can
figure out by looking at current and past values of
the data that you have. That's one famous decomposition, of course, that we all know. Another famous decomposition is a frequency domain
decomposition, sometimes called the
Crania representation instead of the Wold
representation. Sometimes it's written as the spectral
representation theorem. You can see it in
lots of places. Brockwell and Davis is one. We're going to spend some
time talking about this. It says, going back here, you take Y, and you break it up into a whole bunch of little
pieces, these Epsilons. Each of these Epsilons are uncorrelated
with one another and they're homoscedastic, they all have variants, Sigma Epsilon squared, and they have time
associated with them. Epsilon t minus 2 is the forecast error that you
made at time t minus 2. They depend on time. This is another decomposition. You're going to take X, you're going to break it
into a whole bunch of uncorrelated pieces. These pieces now are
going to be called zs. Each of these pieces
is now going to correspond to a
particular frequency. You're going to have a
low-frequency component, and a high-frequency component, and a business cycle
frequency component. All of these
different components, I'm going to go through
this in great detail, are going to be
strictly periodic, they're going to be
uncorrelated with one another, and importantly, they're going to have different variances. One component corresponding to the business cycle might
have big variance. If you generated a realization
from that process, it's got a big business cycle z, so you're going to see
a big business cycle. Another process is going to have a big low-frequency component. You generate a realization from that, and it's
going to look very trendy because that component
is going to be important. We're going to spend, really, the rest of this first lecture this morning talking about this. Here's some data. This is housing starts or
building permits, and it's pretty cool. Your eye, of course, goes directly to here, but that was not the
reason that I chose this. I chose this because it's got all this
interesting action in it. We're going to think about
describing this action, doing some other
things to try and isolate certain bits of this. If you look at this,
well, what do you see? Well, really doesn't have
much of a trend in it. That's interesting because this isn't difference to
de-trend it or anything, it's just thousands of units. Who would've known? It's got, well, some business
cycle stuff in it. It's got that. It almost looks like
a business cycle. Remember, there was
this that didn't happen to housing
that was interesting, but we're making up for it now. It's got this business
cycle variability to it. It also, of course, not surprisingly, has
important jiggles and jaggles. This is mostly data. These jiggles and jaggles
are, of course, seasonal. You don't build a lot
of houses in January, you build more in July. All of this stuff is the seasonal
variation in the series. What are we going to
try and do or what's this spectral stuff all about? Again, it's as you know, as you learned, but
it was reviewed. It's thinking about
the series as being composed of there's the
business cycle bit, there's the seasonal bit, there's the low-frequency bit. Let's do some calculations
to see if we can figure out, what's the relative importance of those different
contributions? Then the filtering
exercise is going to be, can we do something
to this series to highlight a
particular component? Can we do something to just
extract the seasonal bit? Why might we want to do that? Because we might
want to throw it away and that's called
seasonal adjustment. We might want to just look at the business cycle component. That would be some
business cycle band-pass filtered thing. That's what we're going
to talk about doing. Then when we talk
about real-time stuff, we'll ask about, chief, you were doing that
stuff in real time, what would you be
thinking about down here? Holy moly is what
you'd be thinking. This must be just what I said. I now teach with slides. When you have slides
up there I keep looking and thinking
I have to read these, but I don't have to read these. I can just speak and
then you can read these. Undoubtedly, I've
said this stuff. Fine. One thing that's
going to be interesting that I mentioned, this
low-frequency variability. That's what hack is all about. That's what robust
standard errors are all about and regressions. We're going to
think a lot about, how do we estimate the spectrum, which is the variance component of the very low-frequency bits? Because that's going to
tell us the variability of sample averages
as it turns out. I'll show you that calculation. Again, most of you have seen it. Now what I want to do is we had this Crania representation, the spectral representation, Y dependent on the zs
with this integral thing. I want to show you
where that comes from. It takes about two slides. Then there's some slides of calculations that
are not interesting, but I put them in here
to fill the pages. This is what we're going to do. I'm going to show
you four equations, four processes, four models for Y. I'm going to start with something really simple,
and then move to something just
slightly less simple. Then basically, we're done. Here's the simplest Y process. It's just Y_t is the cosine of Omega t. I think it
looks like this. I remember this
from high school. I think it looks
something like that. It keeps doing that forever. A couple of things about
it, it repeats itself. It repeats itself exactly every 2Pi over
Omega time periods. If Omega is really big, high-frequency, this thing
repeating itself quite often. If Omega is very small, it takes a long time
to repeat itself. Omega's frequency, low
frequencies are low frequencies. The thing about this thing
I've written it here, I got it wrong. It starts at one, and has an amplitude of one. This is a pretty boring process. You might want to spice
it up a little bit. How could you spice
it up a little bit? Well, you could change
Omega, we'll do that. Or you can make it
start right here. You could shift the origin and maybe you could squish it, it's attenuation,
or make it bigger. That's amplifying.
How can you do that? Well, a simple way to do
that or one way to do that is just to add a sine term, make it a weighted average
of a cosine term, and a sine term with
weights a and b. Then the initial value is now a and the amplitude is now square root of a
squared plus b squared, so a and b are getting the initial condition, and changing the amplitude. That's fine, there's
nothing random here. Let's make this a
stochastic process. One way to make this
stochastic process is just to think
about these weights, a and b as random variables. That's exactly what this
spectral stuff is all about. Now same thing, except a
and b are random variables. They have mean zero, they are uncorrelated
with one another, so a and b are uncorrelated
with one another, and they have the same variance,
Sigma squared. That's just the process
I just made up. From that, what can you do? Well, you could figure
out what's the mean of Y. Well, it's the mean of a times this plus the mean
of b times that, the mean of a and b are both
0 so this Y has a mean 0. What's its variance? Well, variance of
this plus variance of this plus 2 times
covariance, covariance is 0, so I get Sigma squared times cosine squared plus Sigma squared times sine squared. Then in high school, you learned cosine squared and sine
squared adds to 1, so you get the variance
of Sigma squared. You could work out the
covariance is two. It turns out it
looks Sigma squared times the cosine of Omega k. If I set k is equal to 0, just as a check, I get
the cosine of zero and that's one. That means I didn't do
any obvious mistake in my calculation. This is about all you need to know for
spectral analysis, is this process. What
are we going to do now? Well, we're just going to take a bunch of these guys
and add them together. Now let's go here. This is the same thing, except we're going to
take these process that I just had. That was for one Omega,
like a seasonal. Now let's take another business
cycle guy and add to it. Another high-frequency guy, add to it, another
low-frequency guy. That maybe gives
us six components. Let's do the same thing. Four here I've written, I
guess has n components, but think of n as being equal to 6 business cycles,
seasonal, low-frequency. Then we'd need three more. Let these a's and b's be
just like they were before. They are uncorrelated with one another, across all frequencies. But now let's make this a
little more interesting. Let's make the
seasonal component more important than the
business cycle component. What does that mean? Well, I want the a's and b's for the seasonal thing
bigger than the a's and b's for the
business cycle thing. But these a's and b's
are random variables, so what does it
mean to be bigger? Well, it means probabilistically they're going to be bigger. We'll just give the
seasonal a's and b's bigger variance. Then if we generate a
realization of that, chances are those a's
and b's will be bigger, the series would be dominated
by its seasonal component. This is just a
decomposition, if you will, where you've got
heteroscedasticity, where the heteroscedasticity
is frequency related. You can work out.
What's the mean of this? The mean is zero. What's the variance?
Well, this is a sum of a bunch of
uncorrelated guys. The variance before in our simple process
was Sigma squared. Now it's a sum of
these Sigma squares, corresponding to each of
these different frequencies. The auto co-variances
are also going to be a sum of the auto
co-variances that we saw before. This is a nice decomposition
of variance, and a decomposition of
auto-covariance. Now, I only put six
components in here. What if we put in 60 or 600, or 6,000 or as they
say, six kazillion? If you put in six kazillion, all frequencies, you'd
put in six kazillion. You want to make n really big, and when you make n
really big like this, you make n really big, you look at all frequencies, in this case between
zero and Pi. Case I'm thinking about
discrete-time data. I'm not thinking about data
you that see continuously. Now I'm thinking about data you did see discreetly one month, the next month, the next month. Then it turns out you only
need to go up to frequency Pi for reasons that have to do with if you guys
know these doors, wagon wheels and
movies and stuff. I'm not going to talk
about that. This is just the same thing
I had before. There's my a, there's my b, here's my sine and cosine term. I want these a's and b's to be uncorrelated
with one another. I want to mix to have
a different variance, the variance depends on
frequency. That's it. Now that variance function, how variance depends
on frequency, is this spectral
density function. Instead, I wrote before Sigma squared j
where that was frequency, I could have written that
as Sigma squared Omega. We're going to see in a second, I'm going to start
calling that S of Omega and I don't know why, because this looks more natural. Wrong way, no. I'm going
to skip these slides. Here's an admission. I hate sines and cosines. I just hate them. I can't do any calculations involving
them except this, and you can see I
got that wrong. I want to write it in
a different notation because I hate these things. When you see sines
and cosines together, what do you think about
if you hate them, you think, what if I wrote that as a complex exponential, e to the i something? E to the i something has
cosine and sine in it. Es are okay, differentiating an e to
the something is easy. Differentiate a cosine,
what do you get? You get either the sine
or the minus the sine. It's confusing, it is. Just to save myself
making obvious mistakes, I'm going to convert this
cosine and sine thing into something with a
complex exponential because then I'm
dealing with ease. That's all this is. Instead of writing a
cosine plus b sine, I can write this as e
to the i Omega times g plus e to the minus i
Omega times g conjugate, where g's are these things. These slides are going
to be given to you, so you don't have to
write this down, I know. I know this doesn't look easier, but it's easier for me, so I'm going to drag
you through this. You can do the same thing
with all these other things. When I wrote down the
spectral representation to begin with the theorem, it said why was
this integral of, it didn't have sines
and cosines in it, it had e to the i Omega t, this is just sines and cosines, times these increments,
these increments z are just these a's and b's. The variance of the a's and b's, we're going to denote by Sigma
squared Omega or S Omega, so that's going to be the
variance of this D-Z process. That's what this is. Then
you can work some stuff out. The stuff you can
work out is easy. What's the expected value of Y? Where we saw in the case where we just
had six components, it was zero, what was
the variance of Y? The variance of Y was the sum of the variances over
the different frequencies. We're going to want
the same thing here. The sum is now going
to be an integral. Gamma 0 is the zeroth
auto-covariance. That's the variance. That's the integral of
e to the i Omega k, k is zero, so that's one. This variance function added up. That's just the sum of
the Sigma j squared. Then we have the
auto-covariance, they had co-sine stuff. That's what this is,
the general formula, and this just does it. That must be what
this is, summarizing. Blah, blah, blah you
guys can read this. Read this to yourself. Can you? Except in the back where
they should give binoculars. Can you see it? How many
fingers am I holding up? Four, cool. This a variance function, it's the variance of the Omegath component
corresponds to period 2 Pi of Omega. It's a variant, so it's
got to be positive. If you ever computed one of
these, and it was negative, you live in a weird world, or you made a mistake. The way I did it
here it's symmetric. It's typically plotted
between zero and Pi. The symmetry may be
important for something we do later or may not be. But let me just say it. The last slide which I went
through quickly showed how I could get
autocovariances from spectra. Another convenient thing
is you take this in your muck around with
it a little bit. It turns out you can solve for this as a
function of this. Just do that. That's
what this is. This is going to be
useful because this says the spectrum can be written as a function of
these autocovariances, and sometimes we know
autocovariances. We know autocovariance
is like for white noise. Then we can figure out
its spectrum quite easy. For a white noise process. All the autocovariances
are zero, the spectrum is really boring. It doesn't have
any Omegas in it. It's flat. That's why
it's called white noise. It's got a flat spectrum and that's why it
makes that noise. That you can't know there's
rumbling. That's not right. Anyway, we're all
everyone's here. We're up to about now
we're like in 1962. Not quite. Here's building
this building permit thing. I estimated the spectrum
for building permits. I'll tell you how I did
that in a few minutes. It was really quite simple. What I did was I estimated
an AR model, and then figured out what the spectrum of the implied AR model was. That's, we'll call that an autoregressive spectral
estimator later on or later on where
we'll call it an AR hack. Var hack is what it's called. Anyway, here's the spectrum
of a building permit. This has got some cool stuff. Here, it's got some peaks. This is, I plotted
it in log scale. These peaks are really big.
Where's the first peak? The first peak is around like, let's call it 0.1. Omega is equal to 0.1. Period is 2 Pi over frequency. 2 PI over 0.1 is like 60,65. These are months. That's like 5,6
years or something. This is maybe that's not 0.1, maybe it's 0.07 or something, that's the business cycle stuff. Then this peak is around 0.5. That's 2 Pi over 0.5. Let's call it 12. This is seasonal thing. As it turns out,
these other things are seasonal things too. These are frequencies
corresponding to periods of six months, four months, three
months, two months. Those are the harmonics. This plot just shows what, well we can see in the figure. Here's this hack thing. I'm going to come back to it. I'm going to do a
whole lecture on it but Jim mentioned
that I should say something here
because it's going to show up in stuff
before Wednesday. It's the spectrum
mid-frequency zero. Long-run variance. Long run, infinite period. Frequency zero. Here's our formula for the
spectrum at any frequency, sticking Omega is equal to 0. E to the 0 is 1. I get 1 over 2 Pi sum of
all of the autocovariances. Why is that interesting? Well, why is that going to be interesting desk for inference, well, here's my first typo. Suppose I have a bunch
of random variables, w, and I compute the sample. Oh, no. This isn't
bad. This isn't wrong. This is what I wanted to write. This isn't a typo, it's just bad notation. Here's my model for
those of you in the back I will write large. Here's a simple model. Y is Mu plus w. W is a mean
zeros stochastic process. I want to estimate the mean of y. Compute the sample mean of Y. That's what must
be what this is. Of course write the sample mean of Y minus the true mean, is going to be the
average of the w's. Scaled appropriately by the square root of
t. That's 1 over the square root of t
times the sum of the w's. This is what we would typically apply a central
limit theorem to. We would construct a
standard error and construct a confidence interval for the mean in the usual way. Well now let's think about what the variance
of this thing is. That's going to be our
variance that we use in our asymptotic
normal approximation to the sampling
distribution of this. We'll do this again later. Work out what the
variance of this is. It's the variance of a sum. The variance of a
sum is the sum of the variances plus all
of the covariances. All of the covariances here,
just the autocovariances. This guy turns out to
be blah blah blah. Which is blah blah
blah rewritten. This is the sum of all
of the autocovariances between minus t plus 1 and t minus 1 plus
some little junk. This little junk is small. This guy is basically this. Now suppose the sample
size t gets large. As t gets large, this
is just the sum. As j goes from minus
infinity to infinity. That's just this
multiplied by 2 Pi. This guy is going to be
basically 2 Pi times f of 0. This thing is sometimes
called the long-run variance. Sometimes there's the two Pi there and sometimes there isn't. But so the jargon of long-run variance is people
are interested in this. Then they give it this
spectral interpretation. Now let's go through things a little bit more deliberately. Now I want to study
properties of filters. Here's the thing. These are called filters. They're really just
moving averages. X is going to be a
moving average of y. It's an average of
future ys and past ys. It's l to the minus thing. L is the lag operator so l inverse is the
forward operator. We're moving something
forward in time. L moves it back, sometimes old guys
used to use b, and f, b for back,
f for forward. When I learned this,
Clive Granger said b and f because he said he always
got confused with l, could be lag or lead so
he used b and f. I mean, that's a fine thing. Nothing wrong with that. But now just L to the minus r means moved forward r periods. This is just the
moving average of y. Now, how are we going
to interpret this? We're going to interpret this, this is called a filter. It's called a filter
because it does something like what iTunes does. iTunes, you're
listening to music, writing, you click somewhere, like over here, you click down here
someplace in iTunes, and it brings up that little thing that's got those fake knobs that
you can move up and down. You can turn up
the bass turn down different frequencies. You can click on what
music you're listening to. When you click on the
different music you're listening to, what does it do? It amplifies certain
frequencies. Like it amplifies the base and attenuates
other frequencies. It turns up the volume on certain instruments and turns down the volume on
other instruments. As it turns out, that's what this c of L
thing is going to do. It's going to turn
up the volume on certain frequencies, and turn down the volume on
other frequencies. What we want to do
is figure out how does it do that or why can
we think of it in that way? Then importantly, we're
going to ask, Gee, suppose we wanted a little
iTunes thing that turned up the volume on business cycles and turn down the
volumes on seasonals. How could we cook our
filter weights to do that? That's what we're doing. These are just linear
moving averages , are familiar things. We're going to interpret
them as filters because they're like
filtering sound. What I want to do
is ask, what's y? Remember, y is a sum of a
bunch of sines and cosines. That's what the spectral
representation was. Y is this building
permit series. It's got seasonals in it and business cycles stuff
in it and stuff. Now, what we're
going to do, well, we're going to take y and we're going to do
something to it. We're going to pass it
through this filter, take this moving average of it, we're going to get x. X might be the first difference of y or the seasonal difference
of y or something. Two examples of filters and I want to ask what does c of l do to these cyclical
properties of y. Let's just look, let's go
back to our original example. Let's just think about one of the components of
y, the Omegath. Cosine of Omega t. Y is really a bunch of
these for different Omegas, but let's do them one at a time. Let's just do the first one. Again, I hate cosines. I have to write
this as a complex exponential just because of my learning disability
and learning difference. I didn't mean to be
funny there actually. I'm going to write this. I'm going to put a two in
front of it so I can write it as e^I Omega t plus e to the minus I Omega t. The sine terms disappear
and I got two cosines. Just going to make life easy. This is a strictly
periodic process with period 2 Pi over Omega. Now I want to take y, I'm going to stick it in here. I'm going to construct
its moving average and I'm going to look at
x. X is also going to be some like weighted average of cosine of Omega t. It's going to be
strictly periodic. Because y is strictly periodic, but well obviously it's
going to be shifted in time. Because I got this moving
average stuff going on. It's going to be
shifted backward or forward or something, and these c weights, they're not all ones and stuff, so it's going to be
squished or amplified. That's what we want
to figure out. How much is it shifted
in time and how much is it attenuated or amplified? You have these slides
in front of you now? Your page numbers may be different because
I changed this. Do you have a page that
looks something like this? Now, here's the thing. Now, this is just algebra, just for a couple of lines of algebra. They're really neat. X is a moving average of y. What's y? Remember Y_t is e^I Omega t, so y t minus j is e^I
Omega times t minus j. Take Y_t minus j is
e^I Omega t minus j, e to the minus I t minus j. Now, just rearrange this. Now, a couple of things, when you rearrange this,
this is what you see. You've got this as
a summation over j, I've got e^I Omega t. That first little bit
doesn't depend on j, so this little e^I omega
t bit right there, I could pull that out in front. Similarly over here, I get e to the minus I Omega t. I can pull that bit out in front,
and that's what this does. That's what this does. This pulls the e^I
Omega t out in front. This pulls the e to the minus
I Omega t out in front. Now, this is what's
really nice about this. What was y? Y is e^I Omega t plus e to the minus I Omega
t. These little bits in front, that's y. Then there are these moving
average weights, the c's. What are the moving
average weights? Well, they are these sums. We've taken this c of l times y. We've broken it up in a
way that's convenient. Here's y, and here
are these weights. Now, I've used some
funny notation here, some notation that just so
we're together on this. Green is never good. Here's an example of a c of l. That's an example. L times C_1 plus L
squared times C_2. Then that sum, this first sum right here,
what would that be? That would be C_1 times
e to the, what is it? Minus i Omega 1 plus C_2e
to the minus i Omega 2. That's what that
first sum would be. Now, notice that this is
just the same as this. If I replaced L with e
to the minus i Omega, because then I get
e to the minus i Omega to the 1th power, e to the minus i Omega
to the 2th power. This number, whatever
this number is, I could write it just like this. Only instead of
putting an L there, I could just stick in that. This is just the number represented by that
sum, and it just so happens I have a convenient little
notational device for it, which is given by
this moving average. That's what this is. Now, I'm going to do this. Now
I'm going to do something. Now it's again an
arithmetic here. Because I introduced this
complex exponential, this number, it's got an i in it so it's a complex number. This is the way I'm going to
write this complex number. Complex numbers like here. This is a complex number. How do you write
complex numbers? A bunch of ways to write them. Here's one way to write them. Real part plus complex part. Here's another way
to write them. Another way to write them is to, you remember this
from something, you put the real part down here and you put
the imaginary part here then this is just
some point in this plane. This is the A part of this and the B part
of this is like a point. Not a line, it's like a point. Where that's like A
and that's like B. That's another way to write it. Now I just have
this point in here. Another way to
represent this is what? Well, I could write it as, well, like, I don't know, Pythagoras
or something. I could write it like it's
how long this green line is. I'm going to call the length
of this guy g times the, and then I have to tell you
what that angle is, Theta. I can also write this guy
in polar form like this, g times e^i Theta, where, what's Theta? Theta is the angle whose
tangent is b over a. This is all going
to be important. I know it seems like complete, but it's going to be useful. Theta is the inverse tangent
of b over a, and what's g? G is the length of this thing so that's the square root of a
squared plus b squared. Cool. Great. Now, this is great because see
that number right there, I'm going to write like this. This number, I'm going
to write like this. This number is the same
number, but with a minus i so I'm going to write it
like this as well, and put a little minus
i in front of it. That's what this is. Now, as it turns out, now rearrange. See this, the
complex exponentials really now they are friends. Now they are friends because I know how to multiply
them and stuff. Going from here to here is easy, it's just this. That's easy now. Now I'll undo the
complex exponential. This is just, I'll put it
right back in cosine form. Now remember what was y? Y was 2 times cosine Omega T. What
have we done to y? Well, we've done
two things to it. We shifted it back in time
by how many time periods? Theta over Omega. What was Theta? Theta is the angle whose
tangent is b over a. We multiply that
times some number, g. What was g? G is the absolute
value of this thing. It's how big C is. We've taken this
moving average filter and we've said it
does two things. It attenuates this
or amplifies it, that's this g thing, and it shifts it in time. What this does is give you a convenient formula
to figure out what g, the attenuation factor, is and what the
shift in time is. This g is sometimes called
the gain of this filter, and this Theta is sometimes called the
phase or phase shift. Now, we did this, of course, for one Omega business cycle. This thing, change the business cycle by g and shifted it back
in time by Theta. If we did this for another
cyclical component like the seasonal, we go through the same
calculation, but now, the g and Theta would be different because
this number is different. I should really write g of
Omega and Theta of Omega. I have a gain function
and a phase function. What I want to do now is look at these for a
couple of filters. No stuff to say. That
was hard, not hard, you had to do algebra,
that's always hard. Now let's look at one of these. This is example 1. Let's suppose you just arrive to this thing late and really hung over and someone said,
here's my filter. What does it do to y? You'd say it shifts it back in time by two periods
and that's it. As an algebra check
we better make sure that this filter shifts it
back in time by two periods. Sure enough, the
gain turns out to be one and it shifts it back
in time by two periods. Great. Here's something a
little bit more complicated. This is from Sergeant's
old textbook. This is the greatest
part of this textbook. This is why this
textbook is famous. It's because of his
small kid of e^i Omegas. Those of you that
have read this, that's the really nice bit. In there, he's got this gain and phase calculation for what he calls the
Kuznets filter, which is a filter that
Simon Kuznets applied to some annual data and the filter he applied
to some annual data, he was interested in doing
two things to his data. Number 1, I have these
in reverse order but he was interested in
eliminating some trends. What he did is look at sintered decadal
average differences. He looked at, I'll
recall this b of L, L minus 5 minus L^5. That's Y_t plus 5
minus Y_t minus 5. That's a 10-year difference. What the 10-year difference
is to get rid of trends. Then when he did that,
the series was choppy. He said, oh, let me construct some
simple moving averages, five-year moving averages of
these decadal differences. All reasonable things, get rid of trends, smooth out some stuff. What's the filter he
applied to the data? Well, c of L times a of L, just apply both of
these things so we can compute the gain and
phase of these filters. I'm not going to
show you the phase. The gain is easy to compute. If I take something
and multiply it times 2 and then I take that thing
and multiply it times 6, at the end of the
day I've multiplied it 2 times 6 or 12. The gain of C of L is the gain
of b times the gain of a. Gains multiply. I can compute the gain
of this, and the gain of this, and you can just do that. That's just the simple
little calculation. You could do that in
Excel or something. Just compute these
numbers in the usual way. I did that here and here's
what the gain looks like. Again, this must be in
sergeants textbook. It must be there.
It's really nice. What did it do? It eliminated low frequencies. It got rid of the trend. That's what you wanted it to do. These numbers go from 0-2. The average height
there is about 0.2, so it attenuated
high frequencies. That's what you wanted it to do. But what did it do? Well, it amplified
something with frequency of around 0.3. So 0.3 corresponds
to about 20 years. What did he do? He took iTunes, and turned up the volume
on 20-year cycles. If you take iTunes and
you turn up the volume really loud on the drums, what do you hear when
you play a song? The drums. What did Kuznet see when he looked
at data filtered like this? He saw Kuznets cycles,
20-year cycles. Now that wasn't
because they were 20-year cycles in the data, it's because he turned up
the volume on the drums. Just as an example,
here's white noise. Process it, pass it
through this filter, take these data, process it, what do you get? You get that. Well, you just did what you did. One interpretation of this, it's like you messed up. You did something that seems sensible and then you got this, and then you said the drums are loud because the drums are loud. That arguably is a mistake. Here's some other guys
that did some stuff. I got to look at my time. Yeah, we're fine. This
is Julius Shiskin. He was at the BLS or
something in the early '60s. The story is, in those days, seasonal adjustment was
like a job description. You could have a job as
a seasonal adjustor. You went to work every day and you seasonally
adjusted things, and you did it by things
were plotted, and you drew lines, and you seasonally
adjusted things by hand. He thought, maybe I could train a machine to do what
these guys are doing so I could throw these guys out of work and that would
be a good thing. He said, let me write a computer program that
mimics what these guys do. He wrote a series of
computer programs, experimental programs,
X1, X2, bla bla bla. I'm going to show you what X11, we still keep this X numbers. We now have X12 or
X13 or something there basically X11
with some bells and whistles I'll talk
about in a few minutes because it solves the one-sided
output gap problem. But here's the steps. You're not supposed to
be able to read this. But it's like Kuznet
is going crazy. It's firstly do this
thing, you do that. Here it's got eight steps and this is an approximation,
they're really more. Now, how did he
come up with this? He just came up with this, he just tried it on bunch of stuff. We would compare
his results to what a seasonal adjuster
was really doing, one of these experts, seeing if he was getting the same numbers. By trial and error, he came up with this scheme. This is all linear filters, so what's the gain of this? I mean, holy moly. This is so cool, because what did he do? Well, he invented something
that basically path, here's a gain of one. Left the volume unchanged
on those frequencies, turn the volume way
down at the seasonals. By careful work here, by chance, if you will, he figured out a filter that did really
just what he want it to do. That's great. Now you might say, we know if you do this
filtering you can get it wrong, or you can get it right. Maybe we can be smart, and figure out how can we get
it right and not wrong. Here's the HP filter. You probably don't have
this in your slides, but you guys know it better than I do probably
it eliminates trends. It does what it does,
that's pretty good. I got go ahead here, I'm
going to come back to this, we're rolling here. I'm going to come back to this. I'm going to come
back to this stuff. Now what we're going
to do is let's suppose we want to construct a filter which keeps
certain frequencies. Let's try and keep
the trend just for fun to solve that problem.
Let's try and keep the trend. Let's keep all frequencies between zero and
Omega lower bar, and let's turn down the
volume on everything else. Really, let's listen to the drums and eliminate all
of the other instruments. This is an example
of what would be called a band pass filter, you want to pass a certain
band of frequencies, the drums, low-frequency stuff. What do we want to
do? Well, we want to cook a filter that does it. We want to see if L,
it gives us this. How do we do that? Well, I think I say, gee, you want the
phase to be zero. If you want the
phase to be zero, that says you don't
shift it in time. That must be you treat the future the same way
you treat the past, so on average, you haven't
shifted it in time. This is going to be
a symmetric filter, symmetric around zero, so you have to
introduced a time shift. Then what do you want to do? Well, I want the gain of this filter to be one
over certain frequencies from zero to Omega lower bar to be one there
and zero every place else. I want some gain function
that looks like that. How do you do that? Well, it turns out to be quite
straightforward. I'm not going to go through
the details of this slide, but now you write out what the Fourier transform of
this filter which is this. These are sum that
we saw before, it's just that
number over there. Then you do a
little calculation, and you convince yourself that this is an identity
where that's the gain. Then you're sticking
one right there, and then you do the integration and boom out pop
your seaweights. It's just straightforward. Now, here they are. Remember this guy is symmetric, so it's got future
values and lag values. Now a couple of things
about these seaweights. Here's what they are, C_j, the weight that you
put on the lag of the series j periods ago are the lead of the series j
periods in the future, is given by this, it's proportional to one over j. It's declining, but
it's declining slowly. When we're talking
about one-sided things, we're going to worry about that future data might be
pretty important. That's what this says. This says I got
the low-frequency. Suppose I wanted to
get everything except the low-frequency,
what would I do? Well, I'd take the original
series, subtract this. That would give me
a high-pass filter, this is a low-pass filter. What if I wanted just
the business cycles? Well, I could low-pass here and low-pass here
and then subtract. So by subtracting rectangles, I can get any rectangle I want. That's what this is. This is a thing from this
Baxter and King thing. Apparently they wrote
it in year 199. Maryanne doesn't look that old, Bob is starting to
look a little old. In general, you'll
want an infinite number of leads and lags. What they showed is that if
you wanted to approximate, the best L2 approximation
to the gain function, where you're going
to truncate this over a certain
number of periods, then it turns out you
can just truncate. But I don't think that's an interesting calculation for reasons I'll talk about
in just a second. Suppose I want to pass periods
less than eight years. I don't know if
that's the right, period's less than eight years. This is a gap filter. Periods higher than
eight years are trends. Periods less than eight years, I might think of as
deviations from trends. I want to compute an
output gap or something. These are monthly data. I'm going to apply this to the index of industrial
production in a minute. One thing you can see is what you want to put a lot of weight on the contemporaneous value. These weights are dying out, but they're not
dying out real fast. Even like way out here, this is like 600,
that's like 50 years. There's still something
non-trivial there. If I was doing
something real time, if I just truncated here, I'll like committed a big sins, not something you'd be proud of. Here's something I did
just so you see this. This is a log of real GDP. Here's the series. It only goes up to 2004
because this is an old plot. Here, I just did some
band-pass filtering. The question about what I get at the end, I'll tell
you in a second. Here's the low-frequency bit. Here's the business
cycle frequency bit. Here's the high-frequency bit. This is what you
would expect to see. One thing that's
interesting here is, what is the high-frequency
bit look like? High frequencies. What
is the trend look like? Trend. What does
the business cycle look like? Business cycle. It wouldn't be interesting
to look at this and say, this appears to repeat itself every six or
seven or eight years. Because it's cooked like that. I turned up the volume
on those frequencies. That has to happen. It's interesting
of course that you see this volatility decline here very clearly.
Of course why? Because it's got a lot of
high-frequency action, and so you're going to see a
lot of repetitions of that. Here's where I want it to go. I have to take a breath.
This is important. Let's suppose it's 2008 and I want to construct
the output gap in 2008. If I could see into the
future perfectly well, in my mind I would know what GDP data is going to be
like for the next 50 years. I could do it. I just put
in GDP data in the future. That would be, if you will, the infeasible estimate
of the output gap today. Well, what should I
do if I don't know the future value of GDP
to stick into this? Let me now construct my best guess of the
output gap today, conditional on information
that I have today. Well, this is just beautiful,
simple linear model. It doesn't get any
easier than this. If I want to construct
my best guess of this, get a minimum mean square
error estimate of x, I just do it one
element at a time. This has current
and lagged values of y in it. I know those. My best guess of current
lagged values of y, are the actual current
lagged values of y. My best guess of
future values of y, I don't know what those are, so I forecast them.
What do I do? I apply the optimal
two-sided filter, to a time series
where I padded out the future post sample periods, and maybe pre sample periods with forecasts and back casts. That's the best thing to do. Now John talked about this in the context of
seasonal adjustment. This is what X-12-ARIMA does. X-12-ARIMA is just X-11, but applied to the series
with forecast and back cast appended where they use
ARIMA models to do that. Here we might use ARIMA models, we might use vector
auto-regression. We might use DSD models to stick in forecasting
and back casting. There's a paper by
Larry Cristiano, Ontario Fitzgerald
that approaches this, I see an alternative approach. I had originally written here in a more complicated and
less general approach. But anyway, that
was a little snide, so I didn't want
to include that. So don't tell him I said that. I actually saw Larry last week and I told them
I was going to say that because I didn't
like that paper. Anyway, maybe you noticed. Why? Because John did this in 1978 in a great way. That's it. I got one minute left. You can compute how uncertainty about my output gap today. What are the errors
in my output gap? What's the fact that I replaced actual future values
with forecasts? From the stochastic process that I used to do
the forecasting. I know how big those
forecast errors are likely to be on average. I know what their variance and covariance properties are, so I can compute the variance of my output gap in real-time. Here's just a calculation
that does that. This is IIP Index of
Industrial Production. Here's the actual
series with a trend. Here's the output gap
computed in this way, just the way I said. In the middle here I had
future data and past data. At the ends, I didn't. Out here, I don't
have future values, but I have a bunch
of past values. Here, I have future values, but not a bunch of past values. That's my best guess of this
optimal two-sided thing. Here's the standard
error of this, which I could compute in
a straightforward way. I know properties
of forecast errors. I know properties of
back cast errors, smoosh them around in the right way and compute
the standard errors. You can see, you get the results here that
you would expect. That if I'm computing this
output gap in real time, it's got a standard deviation of about two and a half
percentage points at an annual rate.
That's pretty big. I don't know very much about
what's going on there. For reasons that we all
understand. I'm out of time. Here are some other
stuff you can just look at on your own. Here are these
standard errors for real time, one-sided gaps. For industrial production, I guess the number was two. For the unemployment rate, it's about a half of
a percentage point, for real GDP, it's about one. These are constructed
using forecast that I constructed from
simple auto regressions. You might ask,
could I do better, since I'm sticking in forecasts, if I can forecast better, I can get more accurate
estimates of gaps. You could say, well
what if I use the VAR and stuck in a bunch
of good stuff? You can do a little
better, but not very much. I'm out of time and that we need a break. Let's take a break. Then when we come back, there's a couple of
bits here I need to highlight before we go
on to the next lecture. Good. Let's get some coffee. There are a couple
of things I want to finish in this lecture
before we move on. The first one is quite
important conceptually. The second one is a formula
that we'll use a bunch. Let's do the first one first, which is this conceptual issue. Here it is. This is
going to sound dumb, or obvious or something. Maybe it is. Here's
what I have in mind. Here's your run-of-the-mill
regression written in the notation of any good
undergraduate textbook. You want to regress y on x
and x and u are uncorrelated. That's what you need for like least squares to be good in all of the usual ways. You get a consistent estimator. But if you don't have this, you're dead, or what you've done is you've
done something silly. Question arises G, what if I think about estimating
this relationship but instead of using ys and xs, I use filtered ys and xs. I HP filter things, or I first difference things, or I seasonally adjust things, or I do any of a number
of other things. How's that going to
change the regression? Is it a good thing? Is it a bad thing? What are the pluses and minuses? The pluses are a little subtle. The minuses are really obvious, so let's just talk
through the minuses. Y filtered is filtered y, x filtered is
filtered x and notice I use the same filter
on both of them. What have I done? I've
taken this equation, filtered y, filtered x, and filtered u, and I'm going to
have the same Beta. That first things first I
haven't if you will change the definition of
Beta here or have I? If I'm going to run
this regression, what do I need? Well, I need the
regressor and the error term to be uncorrelated
with one another. I guess this looks like that except it's
got superscripts, so you feel good. But you can't just do that, x filtered has lagged
current future x's in it, u filtered has lagged
current future u's in it. For this to be true in general, it's going to be the
case that x and u are uncorrelated that
all leads and lags. When is that going to happen? Never ever. Can I think of an example? No. Never. It's very hard to
think about a model where; I don't know the buzzy, strict exogenated or whatever the buzzwords all
you want to use, that x and u are going to be uncorrelated
to all leads and lags. In general, this is just
econometrics 101 reasoning. You always tell students, first thing you do when you write down a regression you can ask is x and u uncorrelated? If x and u are uncorrelated, then go to step 2, if
it's not start over. Here, you started
in the right place, but then this messes it up. This is very dangerous. Is it going to work? Well, it depends on the filter, it depends on the model. You can imagine writing down situations in which this might hold but in general it's not, so you have to think
about it very careful. Of course in general that's
not going to hold either. You have to think about that. Just to make sure we're
on the same page, this is exactly the
same reasoning or the same argument that
people make if you've got this run-of-the-mill
regression model and you think your error term is serially
correlated, it's an AR1. In the olden days we
learned if you add an AR1 error you should correct
for serial correlation, and then someone said but wait a minute that's just
applying a filter to both sides and that changes the orthogonality conditions
that you're assuming. What did we learn? Well, we learned that
correcting for AR1 errors or any other time series errors
might really mess you up. People don't do that anymore. They use hack standard
errors in general. Why? Because they're
really worried. This is just the same thing. I guess the thing is
if you're going to use filter data
think really hard, and if you're going to do GLS, think really hard. Say anyway. That's the conceptual point. I just wanted to say that. This is obvious but
needed to be said. Obviously this is supposed
to be a y over here, and that's an x. This is called page 38 here. I don't know what page
it is for you guys. It's around this x11 thing, probably the page after the lx. FEMALE_1: Thirty-three. Mark Watson: Thirty-three. Here's just some notation. This is what we know. If I take y, this
should be y and I filter it by c of
l when I get x. What have I done? I have taken y the Omega-ith component of y and multiplied it times
g by this gain thing. If g is less than one, I squished it. I attenuate it. If g is bigger than
one I amplify it, and maybe I shifted
it in time to buy this phase but let's
forget about that. What I want to do is ask if
I know the spectrum of y; so think about the spectrum
as the variance of y. If I know the variance
of y and I multiply y times 2 what's the
variance of x? Well, it's 4 times
the variance of y. It's the same thing here. If I know the variance of y and x is equal to
something times y, the variance of x is
going to be that constant squared times the variance of y. That's what this
little formula says. The variance of x is the constant squared times
the variance of y. The constant squared
turns out to be something you can
write like this. It's this filter times itself, so this is just the
constant squared. Why is that useful? Well, it's useful
for lots of things, but it's particularly useful because it allows us to compute the spectra of ARMA models
in a very simple formula. This is going to
be useful because when we do hack standard errors, their estimates of the spectrum and sometimes we're
going to do that by estimating an AR model and then figure out what the
implied spectrum is. That's why this
formula is useful. We're going to do this.
Now what do I have? I've got a series Epsilon
which is white noise and y is a filtered
version of that, so I can figure out
the spectrum of y as what g squared times
the spectrum of Epsilon. What's the spectrum of Epsilon? Well, here's the general
formula for the spectrum, but Epsilon is this
white noise guy so all that junk goes away. The spectrum of Epsilon
is just this constant. That says if I want to compute the spectrum of
these ARMA things, I compute well here's g
squared times a constant. G squared can be written
as well as c of l, c of e the i Omega, c of e minus i Omega. That's g squared. That's the formula. Now let's look at the
formula a little bit more deeply because
we're going to have to plug in numbers later, so let's look at it
a bit more deeply. It's this Theta; I erase this thing over here, it's this Theta
evaluated e the i Omega, Theta evaluated e minus
i Omega. What is this? This is just Theta of L
where wherever there was an L I put an e the
i Omega or an e to minus i Omega divided by Phi of L but now
evaluate e the i Omega, so I put that down there. In particular if I'm interested in the
spectrum mid frequency 0; is what we're going to need,
spectrum mi frequency 0, I put in Omega is equal to 0. All of these e things are e
to the 0s those are all 1s. This is 1 minus Phi 1 dot dot, dot minus Phi q times itself divided by 1 minus
Phi 1 dot dot dot minus Phi; that should be a
P, times itself. That's a simple formula
for figuring out the long-run variance
of this process. That's a formula
we're going to need. Then what else did I do? I didn't do this and
I'm not going to do it. Again, I did everything
for scalars. You can make things vectors. Just work through this
on your own, I don't. Do what you want. Again, now we have a spectral. Instead of a variance, you
have a covariance matrix. Instead of a spectral density, you have a spectral
density matrix, and in the off-diagonals you have these things
like covariances, they're covariances
between the z increments. You can just read this. Blah, blah, blah, blah. Spectral estimation, I'm
going to come back to this later, so I'm going
to skip this. That's the end of Lecture 1. 