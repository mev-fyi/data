Daron Acemoglu: It's a
great pleasure to be here and to follow Matt. Unfortunately this
lecture won't have any cute graphs like Maths. I'll have a few graphs later, but not in this lecture. The other thing that's complimentary is that
Matt actually gave a great introduction to coming from the
viewpoint of networks, and how they're
going to be useful for thinking about a range of economic and social
interactions. I'm going to start actually
from the other way round, and we're going to start
from a very simple but pervasive problem
in economics, it's that would think
about externality. It's already a huge area within economics to think about various different types
of externalities, productive externality,
social externalities, informational externalities,
peer effects. What I'm going to try
to argue and develop it from that angle is
that networks are actually both a
powerful language for thinking about these
externalities, and actually implicitly part of any investigation of
such externalities. Being more explicit
about that both makes it more useful, and gives us
perhaps some additional ideas. In the process, several
important topics on which many people in this room
actually have worked are going to come back
to the surface here. The plan is, I'm going to talk a little bit about using
networks for language, for externalities,
so then I'm going to immediately jump to
games over networks. I'll motivate why. In my opinion, actually
for almost all of the externality type empirical and theoretical
investigations, we cannot avoid the issue
of games over networks. I'm going to spend
a bit of time going to an equilibrium of
games over networks, and then I'll talk about
identification problems, and then ways of dealing with those in the context of
empirical approaches, and I'll then talk about a specific application that illustrates some
of these issues, and then I'll end
with a couple of important observations
about important, not because I'm saying it so, but important from the viewpoint of the limitations of what I've covered on thinking
about endogenous networks. Let me take a particular topic that I know a little bit about, which is Human Capital
Externalities, and there are many papers and important works
in economics. Going back to Marshall's
Principles of Economics, Lucas, Jane Jacobs, Rauch and so on, and all of these specifies certain types of externalities. But actually when you
look at the details, the externalities are different about who the originator
of the externalities are, and who actually receive the receivers of
these externalities are. For example, Lucas writes down an endogenous
growth model where anybody who has human
capital creates human externality on
everybody in an equal manner. In Marshall, human capital
externalities are from workers to others within
the same industry. Jane Jacobs, although she has different parts and
her book, for example, talks about how managers across different industries
in the same city create ideas externalities
on each other. You can see that these
externalities are going to be different in terms of who
creates externalities on who. How do we actually
think about this? Well, a natural way of thinking about this is
in terms of a network. We can think of the economy
as consisting of n units, and workers are firms
or individuals, and then we're going to have the adjacency matrix
that Matt introduced. I'm going to call that G
and boldface G in fact. It's going to represent these
networks of interactions. Differently than what
Matt talked about. He hinted at this, but I'm going to be much
more explicit on this, is that rather than friendship type networks which are both undirected,
which means symmetric. If I'm your friend,
you're my friend, and also mostly because
of measurement issues, we treat it as zero ones, I'm going to generally work with networks that are both
directed and weighted. Which means that I can create an externality on you and you may not create an
externality on me, and the strength of that
externality is going to be one of the
parameters that's going to be specified. In particular, we're
going to be looking at situations like this where a node J is going to create an externality
of strength G, I, J, and that's going to be our graph or the network representation
in terms of a matrix. As Matt already made it clear, it's entirely
equivalent to think of a network in terms of
the matrix or the graph. I'll go back and
forth between them. This is the matrix, and then I'll also use the i'th column of G
by G_i, like this. Then I'm going to just
adopt the convention, although I don't need to, that you don't create an
externality on yourself so the diagonal elements of this
matrix are equal to zero. In other settings, there
might be self-loops, but that's not really important. Let's take the very
simple linear form. We have an outcome variable
Y for individual I is Y_i, it depends on his
characteristic X_i. In this context, I'm going to
think of X_i, for example, a schooling and why I might be your log
earnings for example. At this level, this
would be just like what is done generally
in labor economics. I look at the effect of your
schooling on your earnings, controlling for other things. But I'm going to
allow for this effect of others on other
human capital, on individual I. I'm
going to do that, I'm going to take the
column of that matrix. The column is
essentially measuring which the effects into individual i. I'm
taking the I'th column, and then turn that
into a row matrix, and then multiply that with the matrix of all of the
X's and the economy. Essentially, this is a flexible
way of writing what is the impact of the
human capitals of everybody else on
other individuals. Then there might be other controls and
so on and so forth. For instance, if we
took Lucas's model in which everybody creates
externalities on everybody else, all the non-diagonal elements of G would be equal
to one or would be equal to some number scale is not so super important here. But if you take, for example, the specification
that Marshall has, it's going to be
a link only from me to other people who are
workers in the same industry, in the same area, and so on. Now, check Manski wrote
somewhere over there. Call these contextual effects. These are effects from
predetermined characteristics. Now you could also think of a different type of externality. For example, you can think
that what actually I'm affected by isn't the
predetermined characteristics, but perhaps I am affected by the actual outcomes
of other individuals. I can use exactly the
same language here, except that here I multiply, I take the inner product
of the column vector G_i with the outcome vector Y. Now, one view is that this Equation 2 is much
more challenging and perhaps less natural than the equivalent one that
I just showed you, one was this one with a
predetermined characteristics. This one here has the Y. Of course you could
combine the two or you could solve out here, do some straightforward
matrix algebra and express everything in
terms of X's, of course. But conceptually, one might think that
this is less compelling or more difficult for some
reasons and my colleague Josh Angerson who with whom I've had several
discussions on these issues, mostly disagreeing but
sometimes agreeing. For example, develops
this point in his recent labor economics
article on exactly this. But in fact, what I'm going
to try to show you is that even if you start with
this equation here, and then think of this as an
equilibrium thing which we normally like to do
in economics because these x's are not randomly assigned,
they're not exogenous. In fact, you're
going to always be mixing endogenous and the
contextual effects and then you have to think about the identification, and the economic challenges
in this context. So in fact, as an
illustration of this, let me go back to something that I did almost 15
years ago here with Josh when we were trying to estimate human capital
externalities following on the footsteps of Rauch, and others looking at the
local labor market. My view at the time was
exactly like what Josh argues in the 2014
that the Equation 1, which is where the effects are from the predetermined
characteristics is easier, and more natural than perhaps we can
deal with that. Then we made a bunch
of assumptions in the context of
a simple model. Actually, it turns out that those assumptions
were buying us a lot. In particular, let me
illustrate that as a beginning. What we assume there
was that you maximize your log income as a
function of your schooling. What that assumption
essentially gave you was that I could make my decisions independent of the schooling
decisions of other people. But that was a
very special case. As soon as you go away
from a situation in which schooling affects
your income exponentially, the usual log wage depends on years of schooling, and you maximize log income minus
separable costs of schooling, then what you're going to have
is a general situation in which when I look at
my best response, I cannot ignore the schooling
choices of other people. In other words, my
schooling choice in best response is
going to look like exercise some function of the exact externalities that
are being created on me. In some sense, even if you start with an
equation like this, where the effects are from predetermined characteristics
onto outcomes from schooling to income rather than my income directly
affecting Matts income, you will end up with the endogenous effects, and
you have to deal both with the contextual effects and the endogenous effects when you want to try to deal with this. This is a roadmap, so I'm going to be much
more explicit on this. Now to be more explicit on this, let me formally introduced the outcome equation
that I'm going to use in the game over network. I'm going to again look at
exactly the same setup. An economy consisting
of n agents, and these n agents are going
to interact according to some adjacency matrix g
and I'm going to call the particular way in which they influence each other
an outcome equation. To make life simple, so here, because things were affecting through
logs and exponents, etc, let me take the simplest, ever possible set up you can
do, which is a quadratic. I'm going to take a
quadratic interaction, which is that my
x_i affects y_i, and then there is this
interaction effect with some coefficient Gamma i, and then there
is this going to be this quadratic term here that the extent of the interact
extent of the externality, again, determined by
the inner product of the ith column of the
adjacency matrix and the vector predetermined
characteristics schooling is going to have a
coefficient here in front. In particular, let's be a little bit more specific here in
terms of this equation, there is an error term here, Epsilon i, there's
another term here, C_i. Why is there an error term here, C_i that multiplies x_i? Well, as we will see, if you
don't have that error term, this would be a
really weird model so there's some reverse
engineering here, but it's not a crazy thing. Then I'm also allowing a lot of these coefficients to be heterogeneous
across individuals. But most importantly, there are own effects, so this is x. Alpha i
is these heterogeneous own effects. There are the spillovers, which is this Gamma i is the impact of the schooling
of others on my education, or is the impact of the
characteristics or features or predetermined features of
others on my characteristics. But then they're also
interaction effects that these things are actually impacting me so I'm going to call these
strategic effects. In particular, whether this is a game of
strategic complements or strategic substitutes
is going to be determined by the
strategic effects. For example, whether Phi
is positive or negative, as we'll see in one second. I'm going to take
this Equation 3, and I'm going to think of this as the outcome
equation, as I said, as it's written in bold
because given the x's, this determines
what the outcomes are [inaudible], of course, subject to the Epsilon i
and C_i random error terms. Now, in theory, one could just take this equation and try to estimate this by
some non-linear methods, for example, non-linear
least squares. Now, what we're going
to see in fact, as we go along is that in
addition to a variety of other econometric
problems that have been identified and
emphasize in the literature, there are going to be just deeper impossibility
result unless all of the x's, I've happened to be
randomly assigned. Therefore, there is of course, always one benchmark case which none of the things that
I'm saying are relevant, which is that all of the
x's are randomly assigned. If all of the x's are
randomly assigned, there is of course, no game because people
are not choosing x's. Therefore, the statement here that I'm going
to make is that as long as all of the x's are
not randomly assigned, so then there is some
degree of choice even if there are some exogenous
shifters of these choices, there are going to be some important
identification problems that I think networks clarify what they are, and
they're also going to clarify perhaps how we might go about them in order to
get to some of these issues. Now, to make this more
operational, of course, I also need to introduce
a payoff function, and let me do that in the
simplest possible way. Assume a quadratic payoff
function and in particular, since x is what I do in order to generate output like a
schooling that has a cost. Let me take that to
be a quadratic cost with some cost coefficient Theta, and then my payoff
is linear in my outcomes. As I said, this is the
simplest possible game. It's a quadratic
payoff function. It's quadratic because it's got x_i square as a cost and
it's also quadratic because my xi and your xi interact in a bi-linear form, and
other quadratic term. It turns out that this is in fact pretty much identical or a special case or a slightly
more general case of a number of papers that have been written in the literature. Very interesting papers by Culvar Mego Patachine and
Zenu on crime networks. Very well-known paper by Culvar Mego Zenu and Ballastor
in Econometrica on who is the key player rework by [inaudible] on public goods and a variety of other papers. Best responses are pretty
easy to derive here, and let me just give
them what they are. Since this is quadratic, you just maximize
that quadratic payoff function or in other words, you stick that outcome
equation here into here, and then you maximize that and then this gives
you the following best response function. First of all, there's a max here because x your years
of schooling or whatever investment you
are doing that is creating the externalities can not be negative so you might be
at a corner solution. But the more interesting
thing is what's in here, so there is this Alpha i, everything is divided by Theta because Theta is
my marginal cost. Then there is this Phi Tau
over Theta times g prime x. This is a strategic effect and then there you have
the error term here. Now you see why in
the outcome equation, it was quite expedient
to introduce this error term here. If you didn't introduce
an error term here, just the additive error term would have washed
out when you do the optimization, and the
best response equation wouldn't have any error
term and of course, no empiricists who is worth
its salt or her salt would be running regressions
without error term so it's pretty useful to have
this random effects here. You can also see that Phi here governs whether
this is a game of strategic complements and
strategic substitutes. In particular, when
Phi is positive, what it means is that when that externalities summarized by the inner product of
the ith column of the adjacency matrix
and the vector of investments is high, that I want to do more
and when Phi is negative, when that externality on me
is high, I want to do less. That's exactly
strategic complements where strategic substitutes. In particular, this is a game of strategic complements
if and only if Phi is positive, and it's a game of strategic substitutes if and
only if Phi is negative. Now, let's first of all ignore the corner solution. I'll come back to
the corner solution. In particular, suppose
that all of these X's are positive so that
I can forget the max. Then I can write
all of these things as stacked together as a set of equations in matrix or
vector form so it says that the vector of X's is equal
to the vector of the Alphas, which were these own
effects that we were trying to estimate these externalities, and again written
in a matrix form. Here instead of GI have the
entire adjacency matrix G. I'm picking different columns of that matrix depending on which individual I'm looking at. Then the error terms here and then the error
terms, and the Alpha I put tildes on them so that I forget
the Thetas there. Now, of course, this
looks very familiar. It has x on the left-hand side, x on the right-hand side. You can rearrange
things and take inverses provided that
this inverse exists. In particular, you end up with the solution given
in equation 5. X is equal to, it depends on my
characteristics as summarized by Alpha
and my error term. Alpha is the heterogeneous
characteristics and the Cs are the error terms which I know
before making my decisions. But all of these
things are multiplied with this inverse, I minus Phi over Theta times
Gs or G transposes inverse. Throughout here, I'm assuming
that inverse exists. In fact, this is a unique
interior Nash equilibrium. If that inverse
exists, it's unique. That simplifies things that I'm going to come back to it. Note that throughout, even though interior
equilibria are unique, there could be
multiple equilibria because there could
be corner solutions. If you want to think about
it in the easiest way, take the strategic
substitutes case. In the strategic
substitutes case, if those strategic
effects are large enough, I am Matt's neighbor, Imagine I create externalities on him, and he creates
externalities on me, there could be
multiple equilibria where in one of them
he invests a lot. Because he invests
a lot, I go to the corner and I invest zero. Because I invest zero that
supports him investing a lot. But there could be
another equilibrium in which I invest a lot, Matt invests zero, and
so on and so forth. Those kinds of
multiplicity could exist. For now I'm ignoring them. Now, let's come back to
the centrality measures. Matt introduced the class of
eigenvector centralities. One specific example of that
is the Bonacich centrality. Essentially that's given
by this expression. The Bonacich centrality is that you take the
adjacency matrix, you pick a scalar number A, for now think of it
as a positive number, and then you take this inverse in identity
matrix minus A times G, and that multiplied by the vector of ones gives you the Bonacich
centrality measure. Essentially this is
doing exactly what Matt did in his eigenvector
centrality. What it's saying is that I'm
defining my centrality by whether the people
I'm connected through the adjacency matrix
are themselves central. It's the recursive notion
or fixed point type notion because if I am central
then the people are connected to me are central
and so on and so forth. Now, what is this coefficient A? Well, to understand
what that is, is you take the expansion
of this inverse, so that gives you something I'm going
to use again also in the last lecture when we come to economic interactions over
a network, for example. You can always write this
as this infinite sum of the powers of the matrix. This clarifies that what
you're doing is you are taking different
combinations of how I go from Matt to Karle and from Karle to Jeff and
so on and so forth, but you're penalizing if these walks that go from me
to you are long ones. An A is how much
you penalize it. In particular, you're putting a penalty of penalizing
these long walks. If you didn't have the A, then long walks versus short
walks would be equivalent. Now, of course, A here
naturally is negative, but you can think of
generalized Bonacich centrality where it could be negative also. Now with this in mind, when we look at this, this has a very nice structure. It says that my
investment is going to be my characteristics times
my Bonacich centrality. Why is it that the
Bonacich centrality, or this particular version of the eigenvector
centrality matters? Because what I want to do
is I want to downgrade or upgrade my investment
depending on whether other people near
me are investing or not. Well, to know that, what I do is I think through
about their investment, but their investment just
like my investments. Instead of, again, recursive
notion so that's where the Bonacich centrality
type ideas come through. Now if you want to characterize
the full equilibrium, not that it's very important
for what I'm going to do but just for completeness, arrange the agents
such that first A agents are the active agents which have positive investment, and then partition the
G matrix like this and then you can write
these Nash equilibrium such that for the active agents, this first-order condition that I wrote for the
interior solution is satisfied, and for these guys this holds as complimentary
slackness form, and then this could
be unique or not. Now, whether it
is unique or not, you can characterize that just the simplest way of
thinking through that is actually to note that
these quadratic games, and many other of these games
are also potential games. Now, potential games essentially means that there
exists a function, a potential function, where when you look at the derivatives of that
potential function, they give the first-order
conditions and the potential functions are useful both for
characterization. They always have an equilibrium. Equilibrium is a solution
to an optimization problem. They're very useful
for computation, but they're also useful for
issues like uniqueness. Why? Because since we're thinking of a
potential function, then if it's concave
everywhere, then it cannot. Most have a unique
equilibrium so that's a sufficient condition. Here the potential
function is this thing. You can easily check that this thing is going to have a concave if this quadratic form here is positive definite, and that quadratic
form being positive definite is again related
to the eigenvalues of the G matrix
and in particular, a sufficient condition is that the middle term there itself has to be positive definite. Therefore, the eigenvalues of the G matrix should
not be very large. In particular, the
absolute value of the minimum eigenvalue has to be less than Theta over phi. The reason why I'm
going through this is because these are very simple conditions to
check if once you have an applied problem, and then more so when you come to
the estimation point, you can actually
verify these things. One issue is that
if you're going to estimate a model like this, if it has endemic
multiplicity of equilibria, you'll have to worry about which equilibrium
you're picking, etc. One thing you can
do is actually you can estimate and
then check whether, given your estimates, it satisfies the
uniqueness conditions. That's part of the reason why I'm actually going through that. Now, but the most important
thing that I want to emphasize here is that, and this is what I said already, but now it becomes much clearer. Even though we started with the outcome equation that
had only contextual effects, just like Josh and I
in our 2000 paper, we may have hoped to get away without getting into the issue of endogenous effects. It turns out that you
generally cannot do that. Going back again to
this equation here, I refer to Equation 4 here, so let me do the Equation 4. The special case in
which I can ignore the endogenous effects is only the special case in
which Phi equals to 0. When Phi equals to 0
this term drops out. But as long as Phi
is not equal to 0, you end up with
endogenous effects in which your X influences me. Now, it's different than
what I started with because when I talked of
endogenous effects at first, I was talking of your Y, like your labor income
affects my labor income. You may have thought that that's a really strange thing: how does your labor income
affect my labor income? Well, we are economists, so we can always come up with a model in which it does so. But here, this is saying that the endogenous effects
are going to show up, not in the outcome equation, but in the best
response equation, and in an inevitable way provided that Phi
is not equal to 0. This might still bother you. Josh is not here to
express his viewpoints, so I will channel Josh here. He says, "Well,
this doesn't make sense because how is it that what you do at the moment affects
what I do at the moment? What is the usual
causality channel here?" I don't think that's
a bad question. It's, I'm choosing my schooling at the same time as you're choosing
your schooling, so this is a Nash equilibrium. In a Nash equilibrium, formally, we're both making our choices exactly at the same instance. How is it that what Matt
does affects what I do? This is what game theory says, but game theory says it
in a very refined way. It doesn't say the actual choice that Matt does affects me; it says that my expectation of what Matt does affects me. There's a slight difference, and this does matter for
some empirical issues. But I think it's unavoidable as long as you're going to
be game theoretic about it. Again, what I'm
wanting to argue, even if It's going to make me sound like a broken record, I think if you're really interested in these
issues you cannot avoid the game theoretic
interactions over networks. The second interpretation
is that perhaps we should think of this as a dynamics of a stationary distribution. If you don't like X affects X : Justin's current decision affects my current decision.
You may not like that. There's something perhaps
a little fishy about that. But what you can do
is you can write exactly the same equation,
but now dynamically. Except that I made a typo, but I actually meant to put T minus 1 for
the externalities. The effects from the externalities
come from T minus 1, from your last period's choice. Then you do the best
response dynamics of this so the environment doesn't change, but
choices change. Then the best response
to the equation is going to look like this, in exactly the same as before, except that I have T on the left-hand side and T minus
1 on the right-hand side. Then the Nash equilibrium
that I just characterize corresponds to the
stationary distribution of this dynamical process. In some sense, if you don't like the X currently affects X, if you give it a somewhat of
a dynamic interpretation. Now, if you have the
dynamic interpretation and longitudinal data, you might want to exploit that. But even if you don't have
the longitudinal data, the game theoretic
interactions would still translate into a
stationary distribution in which the endogenous effects are interpretable and work out
in one way or another. Do the simplifying
assumptions here matter? Of course, they do; the quadratic and all of this
perfect information. But at some level, they don't really matter for the big picture points
that I'm trying to make. If you want to generalize this, you can put instead
of a linear game, you can put a non-linear game. Some non-linear
games are going to look like this one already. Even more non-linear
games are going to be quite well-behaved
under some assumptions. For example, there's a very
nice paper by Nizar Allouch, which looks at these. You have best response to the equations that are
still interpretable. You can have
uniqueness conditions, which are again, related in
terms of the same objects. You can also think
that for all of these random error
terms being known for everybody is not so reliable
source or reasonable. You can do a maths paper
with Galeotti, Goyal, and Yariv in Review
of Economic Studies, and putting complete
information. Again, a lot of the insights at the level that I'm
specifying generalize, but of course, when you
come to estimation, you have to do different things that I'll talk a
little bit about that when I come to the
specific application. What are the
identification challenges? Ever since the work
by Deaton and Manski, it's very well understood
that there are reflection-related problems
and other problems in the identification of
these sorts of externalities, both of the contextual and the endogenous
effect variety. Throughout here, when I'm
talking of identification, I'm not referring to the lack of identification of the relevant
regression parameters. I'm talking of the
identification that you get some
regression parameter, but you don't know what that regression parameter
actually means. It doesn't correspond
to underlying structural or causal
numbers estimates. Let me be a little bit
more specific about this, although I don't know that how much more
detail I want to give. For instance, the
simplest regression that you can run here, not here meaning not
in this context of this model but in this context
of these general issues, is going to be something like:
I regress Y_i on my X_i, so there's a B_own effect, and then there's B
spillover effect, which is this X-bar_I. This problem is
already hard enough. It's hard enough because as
people understood long ago, and in a specific version of this Josh, and I tried
to develop in our paper, is that even if there
are no externalities, if you run this regression, this B spillover number will not be estimated
to be equal to 0. For example, it's going to be different if the OLS estimates of own effects differ from the IV estimates
using group dummies, or if there are differential
measurement error, all sorts of things. All of these problems become harder when you have
the endogenous effects, for example, group means of outcome variables on
the right-hand side. These problems are
all well understood. That doesn't mean that we
have great solutions to them, but at least, there are some approaches
for dealing with them. Instead, what I
want to emphasize are two other problems. One of those is also
somewhat well understood, but I want to emphasize it because it's
going to come back. The best way of thinking
about this is through, Bruce is here also, Bruce Sacerdote does very nice QJ paper in
which he looks at random assignment and random
assignment of students, and then he finds that their
outcomes are correlated. So if I'm randomly assigned to another student who has low GPA, I tend to have low GPA. But this result, though very interesting
and though that it does identify the effect of the correlation
structure in the data, what does it actually say, whether the effects are
contextual, or endogenous? It's very difficult to say
because in particular, if I run a regression like that, these error terms are
going to be correlated. Essentially, you
can think of what Bruces' regression doing is that it's running a regression
like this, and it has randomized assignment
of individuals. You can think of this Y-bar as being as randomly assigned. But that doesn't solve the
identification problem because the error terms are
also potentially correlated. For example, if I am assigned
to Matt as my roommate, but we both happen to be in a room that's above
the train station, the train is going to go over all night and neither of
us are going to study, neither of us are going
to be able to sleep. We're both going
to have low GPAs. But it's not really
an externality, it's an example of the
correlated effects. I'm emphasizing this because the key issue that we
have to deal with when we come to these games over networks are these
correlated effects. These correlated effects are
going to be endemic because just the nature of being friendships is that you're
going to spend time together, you're going to be
in the same place, or it's going to be
geographically in the same place. Also the endogenous networks, I'm going to be
friends with Matt, and this is already here
emphasized in the context, homophily, only if we have some characteristics
together, which might be then create a correlation in
the error terms, or might be the source of our differences from
other units in the study. The other thing that
I want to emphasize is I think slightly
less well understood, but I'm sure other
people have run into it, is that even ignoring
these things, the fact that all of this
is underpinned by a game also creates some special
identification challenges. To do that, let me take the best response equation. Again, let me forget about corner solutions since
none of what I'm saying is particularly modified
by corner solutions other than becoming
more complicated. This was the best
response equation, x_i depends on
other people's x's. Take this and this was the outcome equation, and put this x into the
outcome equation. Then cancel terms and then what you end up
with is this equation 8. Now, this is the
outcome equation when people actually behave according to the Nash equilibrium
of the game. Now what you'll see
is that some of the important
parameters that you cared about have actually
disappeared so in particular, you care about the own effect that has disappeared and you can about this strategic substitutability that
has disappeared. Instead, you have a
quadratic relationship between y and x_i squared, whether it's quadratic
or not, I don't think that's super important. But the coefficient
is something entirely different from what
we were looking at, which is now replaced by the marginal cost of
the effort level. In other words, the
beginning I said, you could take this
outcome equation, perhaps you could try to
estimate the parameters of this equation through some
non-linear least squares. What this says is that
if this is in fact an outcome of game-theoretic
interactions, you can't do that even if you
have exogenous sources of variation in x_i as long as there is some
residual choice, that won't be possible
because people are going to make their
choices in such a way that it exactly nullify
some of these effects and both Alpha and Phi
have totally disappeared. Now at this point,
you might say, well, I knew this network stuff
was too complicated, unnecessarily complex
then we should just go home and forget
about it because we have no way of dealing with it. But actually, you can
get identification here. What it requires is that it
requires you to estimate this equation 8 jointly with the best response equations. Because if you did
that from equation 8, identify Theta and
the spillover effects on local average
treatment of Gamma, and then once you
know Theta from the intercept of the
best response equation, you identify the
own effect and from the slope of the best response
equation times Theta, you identify the strategic
complementarity. There is enough
information in the data to identify the
parameters that you want. But it requires that you
estimate these two jointly, or you can estimate them
as single equations, but you do the inference on them jointly rather than just focus
on the outcome equation. Is this feasible? Well, let's get a little bit
more deep into it. Now the next challenge
that we have to face here is how are we going to make it operational
to exploit this network structure together with the endogeneity of the x's? I'm going to talk about
two approaches here. Actually, let me come
back to this slide. I don't need to slide now. Let me first about talking
about network structure here, and then I'll come
back to that slide. It's a really nice, very creative paper by
Bramoulle, Djebbari, and Fortin in the Journal
of Econometrics, and it is essentially uses
the following idea. The idea is that you can use the assuming that the network is known, and I'll come
back to that in a second. For the next five slides, I'm going to assume
that the network is known so assuming that
the network is known, you can actually use the
structure of the network, in particular parts of the network that has
sparseness in it in order to get identification, and the idea
goes something like this. Imagine that you take
a triangle Karle, Matt, and myself. If I am friends with Karle
and I'm friends with Matt, but the two of these guys
are not friends together, then anything that is a characteristic of
Karle that affects his education choices through the strategic complementarity or strategic substitutability
will also affect game theoretically
my education choice. But it won't affect
Matt's choice because they're not friends. Therefore you can use that, one affects Matt's choice, conditional on my choice, because the only
channel of influence between them is
through me and Matt. Using that idea, you can use Karle's characteristics
as an instrument for my education to look at the impact of my education
on Matt's education, so that's essentially
the idea which is written here in a slightly
more mathematical form. Now this also works if Karle and I have
correlated error terms, but it crucially
requires that there should be no correlated
error terms between Matt and Karle and the idea there is that while these guys are not friends and perhaps they're not friends. They should not be, they didn't
choose to be friends and there is no natural for
homophily to kick in, perhaps they're not
friends and as a result, there is no reason
for the attempt to have naturally
correlated error terms. Now I think that reasoning is not fully
compelling, however. Well, it's not fully compelling because if the three of us have this
triangle of friendship, it's likely that we live in the same locality or we
are at least hanging out in some usual places so any geographic
correlation structure is going to affect things. It's also difficult
to come up with error structures or you could come up of course it's feasible, but it's difficult to think that plausibly I'm
strongly correlated with Karle because I'm friends. I'm strongly
correlated with Matt because we are friends, but these two guys are not
correlated so that may not be too super
plausible either. In other words, this idea of exploiting network structure, I think there's some
interesting ideas here, but these correlated
effects really make it that what you come up with is often going to have the
difficulties of interpretation that I tried to highlight using Bruce's roommate example. Now, another idea is to
come up with instruments, of course, if you can
do that, that's great. But in this context,
I think instruments have to have some additional
special features. In particular, the idea here is come up with an
outside instrument, something that changes, that's not related to the network and affects
Karle's choice, then that could be an
instrument for Karle's behavior when I look at his impact on
me and so on and so forth. Now the idea is that if
these network instruments are really outside variables and they are really orthogonal, they're randomly
assigned to Karle, there are good reasons depending on the setting that they might also be orthogonal
to the network. But if they are orthogonal
to the network, then you can take products of that with
the network matrix, and there'll be orthogonal to
me and so on and so forth. In fact, if you have
an outside instrument, that is not only just
orthogonal to yourself, but it's also orthogonal
to the network, then you can take the product of that with the network matrix and it's going to be an
instrument for lots of other things in the network
so that's the idea. Now, is it important that in all of
this the network is known? Now in the Bramoulle
et al thing, it's actually not trivial that
the network is not known. Because if the
network is not known, it could be a problem
because there could actually be a
link between Matt and Karle and I think
there's no link. Then I use Karle's
as an instrument and that's going to
call an invalidity. But in this idea, in this network
instruments idea, you don't actually need
the network to be known. It could be unknown up to some parameters, and then you can estimate those parameters. You do need to have
some structure because otherwise you're
back to estimating n times n minus 1 parameters or n squared parameters
depending on whatever it is so that's not great. Let me try to give
you an application of this from a paper that I wrote. What I want to emphasize here is not the details of the paper, but how this thing can be operational so this
is a paper with Camilo Garcia-Jimeno at Penn and Jim Robinson at Harvard. Essentially what
we're interested in is the effect of state capacity, like for example, the
local state capacity. Does municipality
have employees? Does it offer certain
types of services on public good and prosperity
outcome in a municipality? But recognizing that it's
going to create spillovers, health or law enforcement or judicial things are going to create spillovers and so here, you can think of this
as a network game. The G matrix is going to be the municipality network with some unknown cost parameters. Then we're going
to apply this to Colombia using municipality-level
data from Colombia, which is a good setting
for several reasons. One of them is that these issues are generally viewed to
be important in Colombia, lack of state capacity, lack of local services,
and so on and so forth. Secondly, the specific way in which the local
state developed, going back to colonial times, gives a potential candidate
for these instruments. What's the model? This is the context
that I just mentioned. It's a very large country
split by the Andes. Lots of places in which there's very little state capacity and that's been
true historically and so on and so forth. But given the time is short, let me not dwell on it. The model is identical to
the one that I've talked about with just
some small changes, except that the g_ij is now is equal to 0 for
municipalities that are not neighbors, and for municipalities that are
neighbors is a function of the distance between them, and the elevation so how
rugged is the terrain? How difficult it is to
reach one to the other. These parameters here, Delta 1 and Delta 2 could be unknown. Then what about instruments? Well, there are no
perfect instruments, but in this case, the way that the
colonial state expanded, gives you some interesting ways of doing things and
let me actually focus, given that the time is
short on these Royal roads. These royal roads are
essentially one of the few investments
in infrastructure that the colonial
authorities did. They built on the
existing Inca roads, but they were very
very idiosyncratic, and essentially they
soon disappear. The royal roads
were important in determining where the
colonial state went, where they decide it was, where their garrison,
and so on and so forth, but they have nothing to do with the current network structure, current road network, current
transportation structure. At least it's a candidate,
and then it turns out that it's rather well-behaved
on certain things, it's not really related to current roads and
so on and so forth. What I'm trying to
sell here is not that this instrument is valid but
just the general concept. Imagine that this
instrument is valid. The other thing that
you really want is that even if you have a prior that this instrument is valid
as a source of variation, it might still create a problem. This problem is what's
on the slides, but I skipped it because
time was short, which is that the concern that you have is about these
correlated error terms. It's just so happens that these instruments are themselves
spatially correlated. They're correlated
over the network, then correlated error terms are going to project over them. One other desirable feature of these instruments
that you want to use in a network
setting is that, it's not really mimicking the
network structure itself. Here is the correlation matrix for distance to royal road, colonial officials, and colonial state agencies and then looking at it
neighbor's average. For the royal roads it's
a geographic variable, of course, it's
going to be a little bit publicly correlated, but if you look
at the other one, colonial officials and the
colonial state agencies, it's very uncorrelated
with the neighbors. It's at least prima facie it has these properties that
it's creating variation, it's not related to
the current network and it's plausibly exogenous. How do we then go
about estimating this? One thing you could do is
you could take these things. In the paper, we have many
more outcome variables, but I'm doing it for the
first stage number of municipality employees and these are their best
response equations. The first thing you could
do is you could estimate the best response
equations one at a time. Not really do systems
identification which is use the outcome equation and the best response equations
at the same time. Or you could do the outcome equation, and
the best response equation at the same time using GMM or some other
nonlinear method. Let me focus on this
column and this column. You can see, and it's
a very robust result, when you do this best
response equation, you always get evidence
for strategic complements, that in places where your neighbors invest
more in state capacity, you seem to be investing
more in state capacity also. That's robust when you do
estimate things non-linearly. Then this is the
prosperity equation, the second outcome equation. Now I'm looking at
the fraction of the population that's
not in poverty. This is the own effect, and this is the
externality effect. You have both the own effect and the externality
effect precisely estimated and you'll see that the GMM gives very
similar results, and the own effect is larger than the externality effect. But of course, the
externality effect is on several neighbors, so you want to scale that up. But instead of scaling
it up right now, I'm going to do what
I'm going to do is, I'm going to do a
thorough experiment. Until now, this wasn't the selling point of motivating
the games over networks. My argument is that you really have to take
the games over network seriously if you're going to think about
these externalities, there really isn't any
way of avoiding it. But once you take
them seriously, then you can do more interesting
counterfactual exercises. Just to illustrate
that let me just do one counterfactual exercise,
and then I will conclude. As I said, in Colombia, out of those 1,000
municipalities, more than 500 or almost 500 of them are very very
low state capacity. Let me take a very
radical policy and take all of these very
low state capacity, very few number of services and employees relative to population and take them to the median. Then this is the first panel. You take all of these municipalities and you
bring them to the median. Of course, the median
doesn't change. But the fraction of the
population that's not in poverty goes up from
57 to 60 percent, so that's a pretty large effect. Then you can do a
decomposition and you can say how much of this is due to own effect and how
much is due to spillovers. The spillovers are not
implausibly large, but they are large,
they're not trivial. They account for about 40
percent of the effect. But this exercise is actually
partial equilibrium. It doesn't take into
account that once you do this change through
strategic complementarity, other places are going
to respond also. One advantage of
the game-theoretic model then you can say, well actually let me take to
due the full equilibrium, which is that once I do
this policy experiment, let other municipalities that weren't directly
affected respond to it because their neighbors now are affected and because of the
strategic complementarities, we could have done
the same thing if it was strategic substitutes, but in this case is
strategic complementarities, they're going to adjust also. Now you do that, now you get a whopping
increase from 57 to 68. Moreover, about 75 percent of this increase comes
from the network effects, which these municipalities
are responding. Now, should you take
these numbers seriously? Well, they've turned
out to be very robust, but they're based on
a lot of assumptions. What I want to emphasize is not that you want to
believe these numbers, but I think once you take the interactions
over these networks seriously in the context of externalities
or peer effects, you can do a lot of interesting stuff and there are still things that can be done. Moreover, none of this
depends on linearity. In the paper, we actually do a lot of things nonlinearly, then you can do the
nonlinear estimation. As I said, linearity matters
for the specific equations, but the idea generalizes. I want to summarize. I want to conclude by just
saying one final thing, which is that, in all of this, I took all of the
various challenges, but I've stayed away
from one of them which is going to come
now in the next lecture, which is the
endogenous networks. All of the issues I
have emphasized are present even when the networks themselves are not endogenous, but they get multiplied
when they are endogenous. But they also become even
more interesting and some additional
interactions come in. Let me talk about just a
paper by Karle, Sacerdote, and West because I think
it illustrates some of the issues with the endogenous
networks very nicely. Essentially what these guys did, is that they estimated
peer effects across cadets in the
US Air Force Academy, and then they found that
there are these pure effects. I think it's a fairly convincing paper they have in jolly, and they also
repeat that in part of this paper that I'm
mentioning in Econometrica. But also more over
these are non-linear. High-ability cadets
create more externalities on low-ability cadets, but low-ability cadets don't create much of a
negative externality. On the basis of these
nonlinear externalities, they said, well, there's an optimal way of
organizing these quadrants, so that we can try to
maximize the exposure of the low-ability cadets to the high externality creating
high-ability cadets. Somehow, I don't know
how they did this. They went and convinced the US Air Force
Academy to allow them to do the randomization based on the solution to the optimization problem that they solved. As a result, they generated squadrons that were essentially, at least the treatment group, that were designed to
maximize this externality. They went then, and they tested the result waiting to find or expecting to find a large positive effect and they find exactly
the opposite. Now the treated squadrons had negative outcomes for
low-ability kids. What happened? Well, I think what
happened, I think is a good lesson in taking the endogeneity of
the networks and then again, the networks that were
perspective even more seriously. What happened is
that in creating these squadrons that maximize exposure of low-ability guys, they created a lot of bimodal squadrons in which
there are lots of low-ability people
and then lots of high-ability people
without middle people, because the middle
people are wasted according to this
objective function. You want to replace the middle people with
additional low-ability people, so that they can get the exposure of the
high-ability people. But then what they found in their follow-up surveys and investigations is that
without the middle, these bimodal squadrons had an entirely different form of social network interactions. When the middle
people were there, the high-ability and
low-ability interacted, but when the squadrons
became bimodal, now the interaction structure
changed because people now sorted to the low-ability
or the high-ability group, so this really cut the externalities from the
high ability to low ability. This highlights a very simple
but very beautiful example of something more general that, if you actually want to
take these games now seriously together with the endogenous
network formation, whenever you changed policy, you might actually change
the network formation with quite important implications for what the conclusions will be. I'll just stop there, and take some questions even though
I'm a little bit over time. 