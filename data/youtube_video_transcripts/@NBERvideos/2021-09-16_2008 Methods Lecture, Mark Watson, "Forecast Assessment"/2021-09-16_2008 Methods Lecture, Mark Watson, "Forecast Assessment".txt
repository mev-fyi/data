so um so this is a about a forecasting assessment um so um um this is what it's going to do um i i want to spend a couple minutes about talking about why you would want to use forecast assessment tools and um you know you know the short answer is to assess forecasts right okay so we can get over that quickly right but you know then the question is well well what if i don't forecast why you know do i care about any of this and and the answer you know maybe yes so i'm going to talk about this as a as a model diagnostic or how it might be useful as a model diagnostic and and then you know i have to um it's useful to remind you about sort of basics from forecasting so we'll talk about some forecasting basics um and then i'll talk a little bit about um you know if you're uh if you need to forecast with estimated models um and you want to you need to estimate the parameters like uh you know of an autoregressive model you're going to use for forecasting you know how might you do that and there's a couple of natural ways you would think about doing that as it turns out um and and it's useful to compare those ways and then i'll move into the main part of the talk which is forecast assessment and forecast assessment i'm going to break up into really two main components component one is uh assessing forecasts made by forecasters right so you know the survey of professional forecaster guys do some forecasting and you want to see whether a particular forecast is optimal or something or you give some you have some financial data and it should be a forecast of something right how would you think about examining that and then i'm going to talk about um how you might use these fork these forecast assessment tools to help learn about models right and the you know canonical example of that right one thinks about a leading example is like you know macy rogoff you know uh random walks forecast well that tells us all exchange rate models are terrible right and it's not so much that you really care about forecasting exchange rates it's that you care about exchange rate models right and the fact that they forecast that random walks forecast better is troubling in some sense right and so i want to talk about you know how these tools might be used to think about if you will that macey rogoff question and um and how that is turns out to be different than this right and then there's going to be a part c that i'll put in which is kind of a combination of these um okay so why forecast or why sort of thinking about this well you know obviously i just this is sort of straightforward but i i guess it's worth saying so um you know if you want to evalu if you were uh why would you forecast well if you're a forecaster then you should forecast if you're uh you know uh again thinking about evaluating some financial data from financial markets that you think should in some sense be a forecast then these tools are useful and and you know increasingly as i said you know like this exchange rate things right so so forecasts are are used to evaluate models you know why would you you know why would you think about using forecasting to evaluate models right and this is you know sometimes uh troubling right if if you want to evaluate a model you know if you write down your null hypothesis as the model is right your alternative is something specific that you can write down right then you've got a null and you've got an alternative right in like naaman pearson told us how to do that right what's the best way to think about you know evaluating a model in that way um you know that's kind of as much as we sort of teach that's kind of not the world we live in right this or you know we're kind of interacting with the data in you know informal ways right we're trying different things we're sort of data mining or learning from the data um and and so that sort of you know sort of strict name and pearson thing probably isn't always the best way to think about inference right and and so one worries that in sample analysis has this overfitting right this sort of data mining you know as it turns out um you can show this in a in a variety of ways i'll i'll show you a calculation later that sort of you know it's not too surprising that if you've data mined in sample and you get a good fit in sample right that hurts you out of sample right so this doing out of sample stuff is really kind of a nice way to penalize if you will you know um a data mining so this sample overfitting leads to out of sample i called it underfitting and so saving or doing some forecasting um is kind of useful obviously when you're thinking about forecasting one thing you might be thinking about is comparing models or you may be asking whether your model is stable through time you know we know there are ways you can test for stability but again those are sort of particular models of instability so you might want to be richer or looser or something right so that um this um might still be useful and finally you know there are some models you know that um you know jim i guess talked some examples talked about some examples yesterday in that you know even doing the eval doing the in-sample evaluation is just difficult i mean they're just complicated models and and and uh and yet you might be able to forecast from them right so this is a you know perhaps computationally convenient way to think about them evaluating those kinds of models okay so for all of these reasons you know this is useful and there's been you know really a lot of literature developed in the last decade or so which um focuses on you know originally some of the old literature focused on this and i'll go over that but sort of more relevant has been you know some literature which is has thought about using cousins of these standard techniques for evaluating models so so that's what i want to sort of stress here okay so let's just get some basics out of the way so that you sort of understand some of the features if you will of optimal forecasts that we'll be looking at or checking for or something when we're evaluating forecasts so the notation that i'll try and use throughout is y is the variable that we're going to forecast i'm going to think about forecasting it at age periods in the future because when i learned this i learned this from clive granger and he always said h periods hints because that's what the way talks right so h was the natural index to use for hints right um okay and um although he probably had n plus h actually because he said well you're standing now in and going hence h right so um okay but anyway so we're going to be t plus h and x is going to be a vector perhaps a long vector of stuff we're going to use to forecast why in the future you know x typically of course would include you know current and lagged values of y you know there's some situations in which that might not be observable in which it wouldn't be there but quite generally think of x as including at least lag values of y i'll tend to denote the forecast made at time t of y at time t plus h i'll use like this same notation we used in filtering right so f t plus h slash t is f for forecast t plus h what i'm looking at right and slash t means based on some information through time t the forecast ar is gonna is y minus the forecast and i'm just gonna call that t plus h and i should be you know maybe there should be a t there someplace too but uh i'll just use this and hopefully we won't be confused um you know there's some uh you know usual kind of thing if we think about optimal forecasts we need to write down you know a loss function and and uh think about um you know expected loss right so i'll write down the loss function generally in a very special way so i'll say your loss from a forecast just depends on the forecast error okay so um l is going to depend on e and then risk of course is just the risk associated with a forecasting rule f is just the expected value of e where e is the error that arises from using that forecasting method right so standard kind of notation standard results from you know quadratic risk if if if loss is quadratic you know squared error loss so risk is then going to be mean square error right you want to minimize the expected value of e squared um then you know as it turns out the best function of the data to use right is the regression function so your best forecast the forecast that solves the minimum mean square problem is to use this regression function right and then the challenge and jim is going to talk about this challenge this afternoon right the challenge is uh is to figure out what this regression function is right and um if you've got a small number of x's right you know there are lots of ways you could think about it you know it's just a regression problem with a small number of x's so you could do linear things you could do non-linear things that would be sort of reasonably straightforward jim's going to talk about you know estimating this at which you know x is a high dimensional object and then um the the challenges that that brings about okay so if you've got a minimum mean square error forecast what are some properties of the forecast here so so right what's the game we're going to play you know someone gives us some forecast we're going to say g suppose this is a minimum mean square error forecast maybe that'll be our null or something we can look at the actual data look at the forecasts subtract them get the forecast error right and then we can say does this forecast error behave like a forecast error would if it came from a minimum mean square forecast okay so that's going to be forecast assessment right so um we want to know some properties of what the forecast error from one of those guys should be one of those minimum mean square forecast should be right and what do we know well of course since this is a regression function right it better be the case that you can't predict the forecast error using data that you had or else you weren't using okay um so um you know the error should be uh uncorrelated with uh the things that you you use to forecast it should have expected value conditional that's on particular it should be uncorrelated with them um if uh you know um obviously if the if this information at date t includes current and lag values of y then implicitly it includes current and lagged values of the errors that you would have made if you were forecasting y and day t right so the forecast errors have to be uncorrelated with lagged forecast errors you could have computed at day t right so you get this m a result that these forecast errors should be m a processes of order h minus one right so another standard result you know and finally of course right you can write y is equal to f plus e this is an identity right but if you've got um an optimal forecast these two guys are uncorrelated with one another right so the variance of y is the sum of the two variances right and then obviously the variance of the realizations has got to be bigger than the variance of the forecast right so if you see a forecast that's looking like that right and you're seeing a y series that looks like that well you know that that that couldn't have been an optimal forecast of that right going the other way you can't know right your y's might look like that and the optimal forecast may be very flat that's okay right that just says it's difficult to forecast okay so those are some of the key properties of minimum mean square forecast now the the um the um when you're doing forecast assessment of course you know you can sort of check for these right but you might say well heck right what if the forecaster's loss wasn't quadratic right then do any how do these generalize or does anything generalize right so how do i check for to see whether forecasts are satisfy some properties if if the forecaster's risk right wasn't mean square error and there are some results on that so let me just show you a few things that might be useful ah drat never mind i'll go here and i'll come back to where i was okay so um so there are probably um a couple of papers that are worth looking at there's this classic paper by clive granger published in some obscure clive granger kind of journal um but you can find it on the web from your hotel room at the sonesta did it just last night um and um uh two nights ago actually um but um right what did uh clive show and this is sort of generalized in this um et paper by uh christopherson and diebold um that if your data are clive did this for a data that were uh gaussian and and these guys sort of generalize this in a reasonably straightforward way to data they're conditionally gaussian so suppose that you know y t plus h given x t is gaussian okay so that's a lot but suppose it is right um time varying variances right so tails can be moving around the unconditional tails can be moving around so this is you know not not this is there's something here right um then it turns out that um the um optimal forecast for really sort of any loss function right turns out to be the minimum mean square error forecast this is the regression function that's why i use mu plus some function of the conditional variance if the conditional variance is constant that's what clive was talking about that's what granger was talking about in a 69 paper right then it turns out that you know your best forecast is the regression function plus a constant the constant depends on the variance right but it's just a constant so if you thought about assessing forecasts right as long as you put constant terms in and you didn't really worry about them right they would soak up right this bit right so that that was the power of uh you know of that result now if you think you've got you know conditional heteroskedasticity going on well this constant is time varying right so that lose that result loses some of its bite right in terms of forecast assessment here's a little proof but it's not interesting um and then um there's a a a nice paper by these guys and and again this paper was focused on forecast assessment it was really focused on this you're gonna run some you know mincer zarnowitz regressions or something right checking whether forecasts are optimal right and they fail the sort of standard mincer zellner or zozarnowitz um kind of constraints aren't satisfied and you want to ask yourself is the forecast not rational or is the loss function not quadratic right so what they did was say let's consider a class of loss functions that as a special case are quadratic but also can be asymmetric and also can be you know like linear or non-linear in a particular way and what they studied is suppose i'm constructing forecasts which are linear functions of x and my loss looks like that right how can i characterize some features of the forecast errors and then they have some results and what they can then do is see if those were see if those characteristics seem to be satisfied in things like mincer zarnowitz regressions right so you can sort of suit that i'm not going to go into detail about that but you know there has been some progress on this you know but it's still pretty special right it's still pretty special you know this is kind of flexible but you know if you look at it you know anyway it's kind of flexible okay let's go back here because there's one thing i left out which is this um forecast combining this is useful because it practically it turns out to be a good idea if you're really 4k if you really care about the future say you're thinking about forecasting because the future matters to you um and it's also turns out to be useful because it suggests some things one might check for if you're assessing forecasts right so here's here's the result so you got um two forecasts right so you got you know forecaster company a is doing some forecasting forecasting company b is doing some forecasting they both give you these forecasts right you're too lazy to do the forecast yourself right so because these guys get paid right and they do some stuff and they just gave you the numbers right and and so you want to ask right um how should i use the information and forecast from company a and forecast from company b write about future inflation right and so what you want to do is you want to kind of put them together right and well you know how do you put them together well you know it's really kind of easy right you know optimal forecasts are given by regressions right so you want to say you know what's the regression now that your information is not all of the x's in the world it's just f1 and f2 right so the way you should combine them is you know you should compute your combined forecast call it f watson f mark right f mark is the expected value of y given f1 and f2 right and that would be the best minimum mean square forecast given the information i have if i'm you know if i mark i'm really lazy and i would do a linear regression right because i know how to do those i know the functional form so if i was doing a linear regression i would just regress future y's on historical values of f1 and f2 right and get the regression coefficients a constant term beta 1 and beta 2 and this would be my combined forecast okay and this um you know procedure is kind of spelled out in a paper a couple of paper paper by bates and granger and grainger and roman often um and and obviously you could extend this to uh to more than two forecasts this turns out to be uh this turns out to be something that's generated like an enormous literature like in like five million sites to bates in granger because it's like really useful i mean like people real you know you look at the survey professional forecasters and you know you got a whole bunch of you you got to forecast gdp in the future you go to the philadelphia fed website you know and there's a bunch of forecasts and then you just take advantage of all of these guys who've done this forecasting you don't want to buy into any particular one of them right but you know you got all this information you kind of combine it in a useful way there's some challenges right because um when you do that you know survey professional forecasters are lots of these guys right and so you know instead of just having two of these maybe you have 10 right and then of course to think about doing this in practice i have to run regressions of y on historical values from those 10 guys right and i'm not going to be my feasible combined forecast isn't going to use the true betas it's going to use my estimated betas it's going to use beta hats right and chances are you know these forecasts are highly values are highly correlated with one another right so that getting good estimates of these individual betas might be difficult right usual kind of problem in multiple regression so you know a lot of this literature has been trying to sort of figure out if you're in that situation you know you know how do you do something other than ordinarily squares when you've got um you know um a lot of correlated series that you're interested in using for combining and there's a result that is um quite striking in this literature it's it's surveyed in in alec timmerman's handbook of economic forecasting paper a couple of years ago um which is uh you know like incredibly troubling to some people um which is you know let's suppose i have 10 forecasts here right and i know i have some theory which says i want to use a regression function and i want to estimate it somehow like by ols or by shrinkage or by this or my favorite bayesian method or my favorite this method or time viewing parameter regard you know lots of ways you could think about estimating that um it turns out in practice right when people do this the best thing to do seems to be just to average these guys you just got 10 guys you just average them right you know maybe one guy's a real idiot so you throw them out right that is you use a trimmed mean instead of a regular mean or maybe use a median right but that turns out in practice you know often often always it seems to dominate this kind of thing if you've got a large reasonably large number of forecasts so you know so so this is something that's called the forecast combining puzzle it's a puzzle because whenever you think you have an explanation to put the solution to do better than it you fail miserably right and having written a few papers failing miserably trying to solve this um i know jim and i know okay so so that's so that's what this is okay other loss function so we did this we did this okay so now let's go to um so now let's go to uh estimation so you know here's an issue um if you're estimating so so so now now the game is this right now the game is you want to do forecasting right you want to forecast why two periods in the future right that's that's your goal in life right if only i knew the value of gdp two periods from now right that's what you want to do right and um um how do you do it right so that's the question so um you know let's make life easy right you're interested in forecasting two steps ahead and you know that at least you know to a pretty good approximation these are gdp growth right gdp growth is arguably kind of well described by an ar1 okay why not right pretty much ar1 right coefficient 0.3 or something right so i want to forecast that two periods ahead right so what are you going to do well you could say g you know gdp growth falls in ar1 know the ar coefficient my forecast of gdp two periods ahead is going to be well here's the notation it's going to be a linear function of gdp growth today right where beta is phi squared you know 0.3 times 0.3 okay okay so the natural way to think about well okay so now the problem of course is you're not exactly sure what phi is maybe it's point three maybe it's point three five maybe point four i don't know right um you need to estimate phi right so there are two ways you can think about doing it right the natural way to think about doing it or right is to say well i go out and estimate the ar1 model boom i find out what phi is i call phi hat right i construct my forecast by plugging into this formula right what phi should be right so i put in you know i estimate to be 0.32 so i put in 0.32 times 0.32 so another way i iterate the forecasts right i know the forecast two periods hint should be five times the forecast one period hence i forecast one period ahead and then i iterate on that and forecast that one period ahead right and that's what this is using this one period ahead model i iterate on the one period ahead model to get the h period ahead model right another thing you might do right is just to say well heck right i'm going to restrict my forecast to be a function of yt right the best forecast is the regression of yt plus 2 on yt that's the minimum mean square error forecast so let me run a regression of yt plus 2 on yt and directly estimate what that beta should be right so that would be this direct forecast right and you can see you think about this for you know three seconds right you can see the pluses and minuses right of these different methods right if in fact the data are really generated by an ar1 model then the best thing to do is to impose that structure estimate an ar1 model and then go with it so the iterated thing is the best thing to do if you've got the right model but if you're worried about misspecification right the ar1 model might not be right right the ar1 model is a good approximation to forecast y one period ahead that's how it's designed right but it might not be the best way to approximate the forecast why two periods ahead let me just go to the objective function that i care about there and do this right so this is called you know direct and iterated forecasts there is you know bias variance trade trade-offs in the usual kind of way um you know this is more robust right this is going to have a lower variance right but it might be biased because of you know because it's looking at the wrong objective function okay so there there is as it turns out you know a pretty big a literature on comparing these things remarkably big actually um i think i have it we have written years from from uh cox david cox to frank shorefied of have written papers on this and a whole bunch of folks in between um there's a nice little survey paper and sort of you know these papers sort of nicely go through and characterize you know how big the misspecification should be for doing you know this instead of that and you know and and sort of get your intuition um some of this has some sort of information kind of criteria of the sort that um jim's going to talk about later today i was i have his lectures all smooshed in my mind so i know it's later today but aicb i see kinds of things to help you decide on which to do and under what class of miss specifications these things work right but um uh when jim and i were we're doing some forecasting work we always we thought it was completely natural to do this the direct thing why because you know these you know always kind of miss specification you know we're not i'm staying away from that right i mean this these models can't be very good right so let's just do this do the direct thing and that's got to be the way to go right and then for some reason we were well we were probably like talking to max right and he said you know i don't know you know i think this iterated stuff is best and we said ah you're just full of baloney right it's it's this and he said no it's this right and then so you ask yourself well how do you tell right well you kind of you know it kind of depends right on the data right depends on the stochastic process right so we have to ask for the kinds of stochastic processes that we look at that we study right which of these is the best thing to do is the misspecification so bad that this is right that's what jim and i thought right or is the misspecification small enough that the variance gains you get from this outweigh the misspecification right so what we decided to do was sample some stochastic processes if you will from the kinds of things we look at so we looked at you know 170 monthly macro series not particularly interesting what they are they're just typical kinds of things right to get an idea of um you know which of these were the right um things to do and then we we estimated some auto regressive models and some vars and stuff like that you can imagine extending this to sort of var land and then thought about forecasting you know three through 24 months ahead okay and then then you just look at this empirically right and you ask you know for the kinds of data we look at right what should you do right and and we were actually quite surprised our jim and i were quite surprised max probably wasn't surprised because he forced us to think about this and you know let me just look well let's do this here's the numbers that are reported here okay and these are sample mean square errors for this direct thing divided by sample mean squares for this iterated thing so if miss specification is important the direct mean squares should be smaller than the iterated mean squares and these numbers should be less than one if the misspecification isn't important then you should get some reductions in variance from the iterated these numbers should be bigger than one so bigger than one says iterated is right is better less than one says direct is better right and so what do you see well you see um well let's go right let's go three months ahead and let's look at auto regressive models where you choose the lag length by bic right bic is going to tend to choose small lags small number of lags right that's when misspecification is going to be most important right because you've left some stuff out right you've got omitted variable bias which is important and sure enough in that case you know you find that this direct thing is kind of better right but if you do something sort of more reasonable like using aic right are using reasonably long legs then it turns out that this iterated thing is actually a better thing to do right and it turns out interesting just comparing not relative mean square errors across methods but across lags that aic turned out generally to dominate b i see in terms of forecast and that'll come as no surprise to those of you that have done these kinds of calculations so that our conclusion from this was that for the kinds of data that we look at these are macro kinds of series us kind of macro series going to be different in different economies and different finance or in different fields but that for these ar var kinds of models for forecasting doing something like aic lag length correct selections and then doing iterated forecasts you know at least to these horizons seem to kind of work better and that is that's a surprising result that's a surprising result so it's worth highlighting yeah yeah no yeah probably not maybe yeah no no that's right that's that's that's right so now i mean i'm gonna sort of so these are seasonally adjusted there's a couple of things going on here these are seasonally adjusted series that doesn't bother me the fact that i don't want to forecast seasonality doesn't bother because i'm not interested in that there i might be for other reasons right so that's fine there's another important question right which is but this seasonal adjustment thing right treats current data different than historical data because this seasonal adjustment filter we saw in week one was this two-sided thing and what it's doing at the end is padding things with forecasts right it's doing all that stuff these calculations were done using the historical data right which are really have sort of somewhat different properties than things seasonally adjusted in real time right because of this endpoint problem so you know to really do this right we should have collected real-time data on all of these series and done real-time done i'm going to call this we call this pseudo this is called pseudo out of sample forecasting because we really didn't go back in time and do it we could have really mimicked what would happen if we'd gone back in time by using real time data i'll say a little bit more about that as this goes on but that potentially could be potentially could be important oh should you use real-time data okay i guess that's next okay here um i just talked about this so um what am i gonna say okay so what do we know historical data day that you can download from you know the bls or the bea you know that has gdp in 1984 quarter one is not the gdp data that were released in 1984 quarter one or quarter two or quarter three or quarter four right there are some you know important revisions that go on right so the question then is if you're doing forecasting and you're building a model and estimating parameters should you be using because if you're doing forecasting you're going to be using the data that are out when you're doing your forecasting real time should you be building your model on the kinds of data that are like the data you're going to be using for actual forecasting should you be using real-time data or instead right should you be using historical data that have been clean purge changed right the the revised data and you know i think there's no there's no single answer to that right and so what i wanted to do here was just list a couple of issues right that are worth thinking about right when in a situation like this right so one thing okay so here's the model you're going to use for forecasting you've got y and x so so when you think about real time data right you've got to think about do you want to use real-time why do you want to use real-time x do you want to use finished you know i mean you got to make a decision about y and x and the dec in the decisions are the the questions are somewhat different right so for why obviously you know i guess the question is what do you want to forecast right do you want to forecast the final revised value that is in 1983 forecasting 1984 do i want to forecast what's going to be announced in 1984 about 1984 or do i want to forecast i'll call it something closer to the truth what in 2008 we look back and see what was really going on in 1984 right depending on why you're forecasting both either of those might be the right answer something in between right so people kind of have to in this forecast evaluation stuff and all you have to make this decision right um sort of a clear maybe cleaner thing is to think about um you know should i be you know what x should i be putting in here right and you know that kind of depends so i'm going to you know what do i want to do i care about the expected value of y given x right or in this linear model i care about the projection of future y on x okay and i care about the projection of future y on the kind of on the x that i'm going to use right not on finally revised x right which may have a different stochastic process than the x i'm going to use right so it's kind of natural to say well what i'm interested in is the projection of y on the kind of x that i'm going to use the i'll call it initial value right not the revision right so then so that's fine that's what i want that's the object of interest it's beta from the projection of y on the initial value of x right and then there's just the question what's the most efficient way to estimate that projection right and that depends on the properties of the revision errors okay so here's just two examples just to drive this make this clear hopefully make this clear in your mind okay so what do we know well you know we know errors and variables bias right so um you know if you're regressing you know y on z but instead of z you put in z star right where z star and z measure with air you get some attenuation bias right so um uh see so you wouldn't you wouldn't want to think about the regression of y on z is the same as the regression of y of z on z star those are different things the projections are different okay so that in that case you've got you know you've got something like you know z is z star plus noise or z star z plus noise i always get these confused okay so that's one that's one thing right so you get attenuation bias if you've got errors and variables going on right so there's another thing to think about though right which is um well it's two stage least squares right so i'm interested in regressing y on x right here here's one way i can do it here's a bad way to do it i'm interested in the regression of y and x and x is uncorrelated with the error term right but i'll use two stage least squares anyway right so instead of regressing y on x right i'll take x regress it on something get x hat right and regress y on x hat right that's two stage least squares now we know in that situation two stage least squares is consistent right the regression of y on x hat is okay but it's not as good it's the same regression coefficient as regressing y on x so the projection hasn't changed there but i want to use x because x has more variable variability than the fitted value from the first stage of a two-stage least-squares regression right so in one case replacing you know z with z star is bad because the projection changed in another case replacing x with x hat is not a bad thing because the is not a terrible thing because the projection is the same but it's not efficient because i could have used the original x so the same kinds of things arise here right you know this is x i care about the projection on the initial i got some error do i interpret this error as errors in variable kind of error in which case i don't want to use x right or do i interpret this as two stage least squares kind of error in which i do want to use x because that gives me more variability and that all relies on what's the correlation structure of this revision here so you can think about this as errors and variables two stage least squares are in the revisions literature right you can think about this as news or noise right so i'm going to get this probably backwards so i'm going to say it and then it's right with probability .51 and it's the opposite probability 4.49 okay so if revision errors are noise you want to run the regression on the initial data if revision errors are news you'll want to run the regression on the final data some people are not in their head so that the probability's gone up from 0.51 to 0.53 okay okay but anyway this is what you need to think about okay you're all okay okay okay so now so that's what i'm going to say about estimation so now let me move on to forecast assessment okay so um so i'm going to do forecast assessment in two pieces okay forecast assessment one is you're evaluating forecasts from forecasters okay forecast assessment two is going to be you're using these kinds of tools to evaluate models okay now or i don't know a mix of people in here for many people in here this isn't the interesting thing because you really don't care what forecasters say the evaluating models might be more interesting but that you kind of have to see what the game is here to figure out how you would use these same tools or what the changes would be if you were using them to evaluate models okay so we're going to start here okay so sort of standard this is all old stuff right so there's nothing there's nothing new here so what are we going to do we're going to think about i'm going to think about minimum mean square error forecasts minimum mean square error forecasts and their errors have some properties let's look at the errors that come out of realized for let's look at realized errors that come out of forecast and let's see if they have those properties or not so that's all this stuff is okay so here's um you know a famous regression uh mincer zarnowitz regressions which come from this paper book uh you know i've never read this paper right and in fact i don't think anyone's ever read this paper because you've read this okay because i was trying to you know i said well i have to do mincers arnold's regressions i said i should put in a site for mention mincers arnold's regressions right so i started googling you know someone someplace you know and i come up all these with all these papers and all these papers called these mincers arnold's regressions there's no sight or anything they're just it's like saying ols you don't say gauss 1842 right you just say ols right so anyway this is this is just like ols i guess or something okay so anyway but there is a paper and it will be in the list of references which will be posted with the revised slides next week okay beginning of next week okay so um so here's the um you know here's the basics so now now the game is you have some data on why you've got the realizations on why you've got the history of the forecasts right and you want to see whether the forecasts are optimal right so what are the properties of optimal forecast for the optimal minimum mean square forecasts are you know f t plus h given t should be the expected value of y t plus h conditional on information available at time t right so you know if i regress this on this right i should get a one right there i should get a zero right there and i should get a zero on any other w's i stick in that could have been used for forecasting okay so boom you know so that's what you do um if you're so i hear inference issues just sort of list them here um if uh h is more than um 1 so this is more than one period ahead you know the error terms are going to follow a moving average process right so you have to worry about serial correlation in in your x times u process your w process right so you better use hack standard errors right those errors will be h-dependent right so you can use some uh you could use some uh parametric estimator um in many cases you might be interested in forecasting something that's persistent like inflation right or interest rates or something right in which case uh you know if this guy if y is persistent it better be the case that f is persistent or it's a pretty lousy forecast right so then you worry about persistent regressor kinds of problems right when you run this you know you worry about unit rudy persistent spurious regression kinds of inference problems and so you want to get rid of the persistence and of course the easiest way is just subtract yt from both sides of the equation right and then you've got data that's a lot less persistent right so you think about doing the entrance here this is a somewhat different thing right because you put a minus guy there and a minus that there so this is kind of moving that kind of closer to one or something but that's the way life is gets rid of that problem if h is large so you're interested in long run forecasts then these u's are going to be very persistent right and that's going to affect inference you know hack regular hack probably isn't going to work well um i'm not exactly sure what to do here there's an old paper by uh jim and matt richardson about this but i'm not sure if i read this today that i it was a great it's a great paper but we've learned more about this kind of stuff and maybe uh it would be interesting to revisit this maybe not maybe it's they finished it but maybe it would be worth re-reading and i probably should have done that okay next thing uh combining forecasts right so you got two forecasts f1 and f2 company a and company b are both forecasting inflation next quarter right and you know someone says company a is really the way to go and company b those guys are just a bunch of hacks right and you might want to see is company a write and company relative to company b is hacks relative to company a right so that says of course is that if you know if the forecast one is the optimal thing when you combine it with forecast two forecast two shouldn't be useful right because company one forecast one um you know included all the right information so you know beta naught should be zero beta one should be one beta two should be zero right so that's just the combining regression inference issues are just the same this is just a mr zarno it's regression with a particular value of w right so they're forecast some people um you know if i tested this um you know this is what um one two three parameters here so it's a three degree of freedom test i might want to have a more powerful test that is look in a particular direction right so a more powerful test for certain kinds of alternatives so sometimes you see people turn this into a one degree of freedom test they pose a constraint that beta naught is equal to zero say i'm not going to worry about that right and impose the constraint of beta 1 and beta 2 sum to 1 right which might be a reasonable thing to do you and then you want to know whether beta 2 is equal to 1 after you've imposed that constraint rewrite the regression like this and you regress the forecast error from forecast one on the difference between the forecasts and forecast two and forecast one and do a t-test on that coefficient okay uh we'll spend a bit more time um on these these are because these are going to show up in the in the next little section these are um lost function tests so um okay so you got forecast forecast one forecast two and you wanna ask you're really here not interested in are are these things optimal you kind of wanna know is forecast one better than forecast two let's forecast one just the same as forecast two not the same not give you the same number but give you errors of the same magnitude right so i guess one way to say that is that if i use method one is the risk for using method one the same as the risk for using method two r is the risk from one smaller in which case i should be doing that right so is the expected loss from using forecast one equal to greater than or smaller than the expected loss from using forecast to okay so this is what i want to know right and how would you check this out of sample well you know let's do this for quadratic loss right so then i i want to ask is the mean square error for model 1 smaller equal to or greater than the mean square error for model two well i can compute those mean squares out of sample right and just see whether they're the same or not right so i guess the easiest way to do this is to look at squared error for model one minus square error for model two at date t now you can see my dreadful notation oh this should be a two right here or else these are guys are pretty trivial to see if they're zero or not so so this should be a two right and what's going on here oh good a backup is scheduled no okay um and so you know asking whether you know the mean of this guy is the same as the mean of this guy it's the same as asking whether the mean of the difference is zero so i could form d bar the average difference in the squares and test whether that's equal to zero or not okay we're going to come back to this because this turns out to be a an interesting test that's done later uh inference issues sort of standard kind of inference issues sometimes this is called a uh does a lot of people have done this right a classic reference is diebold and mariano jbs 1995. there's a history of tests for this cristiano other people have did tests like this um and there's a long string of uh people who propose this sort of basically thinking about this in a gmm using gmm to sort of think about this and in ken west handbook of economic forecasting paper he surveys that literature okay so this is different this is kind of cool okay so this is kind of different so this is um this is um okay so now here's the here's the game you got a benchmark model like something's a random walk right everyone's been using a random walk model right um but then you think then uh people kept saying you know i can do better than that or i can you know i can right and so like a whole bunch of forecasts are produced right and and you want to ask in this collection of forecasts you've got k competing models right you want to ask is any of them better than my benchmark right is any of them better than my benchmark right well you know what you don't want to do of course is to say well let's ask if two is better than the benchmark if three is better than the benchmark if four is better than benchmark because eventually by you know sampling error one of them will appear to be better right even if it isn't right so you want to use sort of you want to think about the fact that you're looking at max's or something or extremes right when you do this analysis carefully right so this is what hal white does in his in his reality check paper right so reality check is is reality is the benchmark is right you want to check that reality so you've got a large number of competing models and you want to ask your null hypothesis is again you form the the difference in loss between any given model in your benchmark model this so that's the average loss for model k right and your test statistic is let's look at the biggest deviation over all of these different models and again you're not going to do this one at a time you're not going to fall in the trap of saying the maximum of all of these random variables has the same distribution as the marginal distribution of any one of them right the marginal distribution of any one of them we work out right here but now you're looking at the max right so what hal does in this paper is well work out write an asymptotic approximation for the distribution of this max right talks about how you can carry out the test how you can find critical values for the test that is you can evaluate some quantize of the null distribution using either sort of standard asymptotic things this will be the maximum of a bunch of correlated normals as it turns out and and then you could just figure that out if you know the covariance matrix of these you know and he also talks about some bootstrap methods that you might use turns out this might not be the most powerful test so here's the here's the testing problem hal says here's a reasonable statistic to look at right to evaluate that and then works out its distribution it turns out that might not be the most powerful thing to do if that's kind of what you're interested in right the max might not be right you might want to throw out some models that are obviously bad because then you're not maxing over so many right so if you got 30 dumb models and 20 good models you don't want to say gee i'm like maxing over 50 you want to take the 30 dumb ones and throw them away and then just max over 20 then that changes of course your critical values because now you're only maxing you know in 20 dimensions right and that's what peter hansen does in this 2005 paper right he sort of refines this okay so other kind of fun stuff well you know sometimes forecasts are in point forecasts right so sometimes they aren't sometimes they're those right so you know this is from the most recent inflation report from the bank of england right and you know this is inflation up to the end of their sample right and then moving forward they don't want to give they don't want to tell you just what their minimum mean square error forecast of inflation is they want to tell you what the predictive density is right so this is their predictive density going forward right so in 2011. it's highly likely they think that inflation's going to be here right but it could be down here up here right so you've got not a point forecast you've got a density so they're going to tell you what's the probability you know uh in 2011 what's the probability of inflation is going to be in this range here in this range here in that range right so one thing you might ask is you know how do you evaluate we know how to evaluate 0.4 catherine how do you evaluate density forecasts right how do you see is this is this is this really the right density so that's kind of a different problem right so so there's been some work here and and let me just tell you the key trick our a key trick so a key trick uh for this discussion paper by diebel okay and the key trick is uh the key trick is this okay so i'm gonna get this wrong i'm gonna say this i'll say this with authority okay so it will sound like i'm right but i'm just making this up okay but i heard this one so okay so uh so how do random number generators work so suppose i want to i want to generate a normal what how does a random number generator do that here's my answer okay this may be completely wrong but i'm going to say it okay so this is what the random number generator does the random number generator generates a uniform on zero one right and then uses [Music] you know right here is the uh here's the normal cdf so this should go all the way down to like minus infinity to plus infinity right keep keep going right and this is zero one right so what the normal random number generator does is say it it generates a uniform boom right and then how does it get the normal it goes over here and reads down the z value right and gives you the z value i think that's what it does why not right okay so all it has to do is it has to know how to generate uniforms really well and then have the cdf coded in them if it wants to generate chi squares or keys or something it just has to have different cdfs built into it right and then it can from its good uniform random number generator give you you know draws from any distribution as long as it knows the cdf okay well but you know gee bold gunther kdu right is they say just the opposite if someone says you know someone says you know i i whoops it's got to be monotonic if someone says you know that's my predictive distribution function right let me evaluate that right what might i do well let me look at the data that actually obtained the data that actually obtained should be draws from that right so let me go here at this date and you know i get a yt boom boom boom come over here that better be uniform okay now the next date what happens well they give me another predictive density something again it should be monotonic so forget about that right right it's a different density right look at right y t plus one you know that should be uniform right so by plugging in realizations of the data into the density that these guys said i better be getting uniforms out right so they do this inverse transformation right evaluate the cdf at the realizations right and they should be getting a sequence of uniform random variables out right and um if this is one step ahead these uniforms should be independent if they're eight steps ahead the uniform should have some h dependence in them right so then you've just got a bunch of you random variables and you just check to see whether the uniform right again if it was one step ahead and you just cared about the marginals you know you could use a commodore smirnoff test or something like that right so that's kind of the trick here and you can do something like that for interval forecasts as well because the distribution is different day by day right right that's the truth right okay so now i want to change gears i want to change the question okay i want to change the question and i want to think about using the same kinds of tools right but to evaluate models okay so this is what i have in mind um the key reference here is this west paper in econometrica in 1996 and he surveys a bunch of things in his 2006 handbook of economic forecasting paper so what are we going to do well what we're going to do now is is we're going to include in our um evaluation calculations we're going to include sampling uncertainty that arises not just because future wise are different than what we might have predicted but what we predicted came from an estimated model not the true the extra simple variability that comes from estimated parameters okay sometimes that sampling variable matters and sometimes it doesn't it matters conceptually so this is forecast better than estimated model 2. we're asking the model one that we don't know for better than model two that we don't know okay so that's that's important okay so let me i'm gonna just jump ahead for a second again i just want to make this clear um i want to know whether model one evaluated at some true parameter values forecast better than model 2 evaluated at some true parameter values so if this is a random walk and this is some exchange rate model right if i'm using this to evaluate the exchange rate model i might want to know does my exchange rate model forecast as well or better than a random walk now i don't really know the exchange rate model completely i don't know the parameter values right so what i have to do right is when i do this forecast evaluation i have to stick in some estimated parameter values right but it's just important to realize that it might be that forecasting model 2 is right that is that exchange rate model is right but the fact that i estimated the parameters made it appear or made it forecast worse the estimated model forecasts worse than the random walk model right so you see this in you see this idea this distinction clearly made in um you know in in well all the engle and westwork clark and west and and barbara rossi's work okay so here's some just some notation just to sort of get this uh get this right uh so what so right what the game here is to use you know pseudo out of sample forecasting pseudo out of sample so sometimes this is called p-o-o-s pseudo out of sample p-o-o-s poos poos okay that's not good like arch arch is good poos i'm using poos just doesn't make it does it right huh okay it could be work it could have been worse but anyway we're not going there okay um okay so the the game is here you've got some data right and you've got some data you've got capital t observations right and you're going to take your your sample of t observations and break it into two pieces right um you're going to use the last p observations to do your prediction right so the last p observations we're going to call the prediction periods okay and the poo strategy um carefully outlined in west is as follows use the data use observations 1 through r and estimate the parameters of your model call those theta hat right that's going to depend on how many observations you use i'll put a little r down here right and then forecast h periods in the future so forecast y at time period r plus eight using data through r and this estimated parameter so you're mimicking what you could have done in real time again forgetting about real-time data issues okay and then you move ahead one period iterate ahead one period and then you have two choices or i'm going to say you have two choices choice one is you could just augment your sample by one period now you've got r plus one data points right to do your uh estimation so you estimate data using observations one through r plus one and do forecasting so this is some this is called recursive poos right or you could do you could add one observation and drop one observation which you might do for two reasons for convenience we'll see later on for some calculations or because you might think that there's some instability here or something so you want to use rolling samples okay so we got recursive poos and rolling poos they only differ in you know how many observations you're using to estimate data okay and then we're gonna do this loss function stuff right just as we did before just as we did before but now we're gonna account for the fact that we've got estimated parameters okay now here this is like the nasty part of this research right the nasty part of this research is it turns out that it kind of really matters whether the two models that you're comparing are nested or non-nested okay so what's a nested model what are some nested models random walk model one ar model model two random walk is a special case of an ar model those are nested non-nested model random walk i don't know moving average right you can't get a random walk from a moving average right so those are non-nested models okay so writing this in terms of regressions right a nested model might be written like this right model one is a special case of model two i just said gamma equal to zero non-nested models is these just have different regressors in them okay so we're going to think about loss function tests in estimated models where we care about loss not for the estimated model but we care about loss for the true model i keep repeating that because it's important and we're going to do it first here which is easy and then here which is not easy okay okay yeah forecast averaging okay great question okay so the question is well now let's suppose i have model one model two and i wanna average and i have another one which is an average of those you know i guess if the two models if the two models are nested clearly the average is going to be nested because one of them is a special case because they're special cases of one another if they're non-nested and i consider an average as well then i've got one nested model and two non-nested models so what i'm going to talk about is not the non-necessary i'm going to think about i think is not going to handle that okay it's going to handle combining tests but it's not going to handle if i actually add another forecast which is a combined one okay so non-nested model so this was the subject of of ken west 1996 paper okay so um he talks about nested models but he says this doesn't work industry models okay so so here's the i just have to go okay so here's the here's the so this looks tedious but right here's model one here's model two they're non-nested right forecast for model one is going to be x beta right the um i'm gonna think about one step ahead stuff just to make life easy right um for true forecast error is epsilon t plus one estimated forecast of course is going to be x t times beta hat estimated with data through time t estimated forecast air is going to be epsilon hat that's going to be the true forecast air and this junk that comes from the fact that i use beta hat instead of beta similarly for model 2. okay now what i want to know is i want to know if the risk associated with epsilon is different than the risk associated with e right but i don't observe epsilon and e i observe epsilon hat in e hat and those have epsilons and e in them that's the good part of them right plus this junk right so i need to ask if you do one of the tests that we just outline and you use hats instead of true e's and epsilons are you messed up right that is is this junk important and the bottom line that i'll just show on the next page is that everything is okay everything is okay so life is good okay and let me just show you okay so d is the difference in loss for one period so it's going to be epsilon squared minus e squared right and i want to look at the expected value of that but all i have is d hat which is e hat squared minus epsilon hat squared minus e hat okay now go back here just for a minute okay so think about squaring this guy right that's going to be part of this what's it going to be it's going to be this guy squared plus this guy squared plus 2 times the cross product similarly for that guy up there okay so boom boom okay so blah blah squared minus blah blah squared right is blah blah right thing i want right plus all the other junk right what's the other junk well it's you know the second component squared cross product second component squared cross product okay now if you think about this for a second right and pretend you're ken west right see okay you can do that right okay take out your pencil sharpen it right think analyze all of these points well what do we know right this is what we used in the standard loss function test right so that's like normally distributed or something right now look at these other guys right well look at these look at this right here right this is like e times z right if that model whatever it was model 2 kind of made any sense at all z and e have to be uncorrelated right so you know i got z times e and then i'm so these guys are uncorrelated and gamma hat's got to be pretty close to the true gamma because i estimated using a large number of observations so this bit's going to be pretty small right similarly up here similarly for that term this bit has gamma hat minus gamma squared so that's going to be kind of really small right so what can show is in cases in which you've estimated these regression models by like these squares so that these guys are uncorrelated with one another that this guy is equal to this guy plus some terms that are tiny okay if these guys as it turns out if you were using iv methods to estimate this so that these guys are correlated well then you have to worry about the stuff and ken has some corrections in the paper that does that okay but key thing this guy is basically the same as that guy so you can proceed you can do loss function tests just like you did when you were comparing forecasts forgetting about the fact that you estimated parameters so to compare models all i have to do is pretend these guys just gave me four casts and i'm going to proceed yeah well yeah yeah but if yeah i'm gonna hold off on that okay so now okay so now i want to show you the difference here so now i want to show the difference so this was this was what did i say non-nested models okay now i want to talk about nested models now here's the problem with nested models what does it mean for model one to be as good as model two and they're nested model one is going to be performed as well in model two if they're nested if model one is the same as model two right because if these really belong in here model two is gonna be better so if model one and model two perform as well and they're nested well then they're the same model well you know that's a drag because if the same model that guy's the same as that guy oh well that's a problem because if we go over here to our identity right then the difference between these guys which is this plus all of this stuff before dividing by the appropriate thing i said this part dominates the right hand side of the expression now this part is zero and all you're left is all of this stuff oh man that's a drag because it says that if you do one of these loss function tests here you've got to think not about this which was easy to analyze you've got to think about what the distribution the whole difference between these models comes from the fact that you've estimated parameters if you could know the true parameters you just look and you'd say are they giving me the same forecast they should be identical you know date by date right so now what you've got to do is you've got to work out the distribution of these guys and and that's quite tedious okay but luckily there are focused researchers who can do these tedious kind of calculations so mike mccracken and todd clark and and others that i think of clark and mccracken in a series of papers really sort of carefully studied expressions that look like this and you can imagine how ugly this is right because these guys are like estimated recursively and i mean it's not fun um but um but you can work that out and what um they showed is that well they worked out the limits of limits of these things and the expressions can be kind of messy um key thing is that the expressions are sort of functions of normals there's averaging going on right and that says you know how can we approximate critical values well that's pretty easy right you could just run a simulation with gaussian errors right so you can figure out critical values for all these things using parametric bootstraps okay um i want to do something now because uh i want to do um two more papers in this this is kind of survey-ish like but i want to do two more papers because they're kind of cool what they do is they one of them the first one is this paper by clark and west in 2006 that those of you that have done exchange rate stuff have seen because they engel and west use this and right so this is done in in lots of recent papers so i want to show you what's going on here and what they do in this paper is they've got nested models okay and they don't want to do clark with clark mccracken complicated stuff right they want they have a they have a trick right and their trick comes from this their trick comes from well considering um considering uh model one which is really special model one is going to be a random walk which of course is relevant for this exchange rate macy rogoff stuff okay so let y be uh if you will think of it as a change in the exchange rate okay so model one says changing the exchange rate is a martingale difference sequence right model two says no no no i can predict it using some model okay so um if you will under the null that these guys are equally good that's the same as saying beta is equal to zero okay so let's go through the same kind of thing here right if you're evaluating these using estimated models right well forecast for model one is really easy it's zero right and you're going to get the true forecast error right forecast from model 2 where you want to use x beta but you're going to use x beta hat right so your forecast error here is going to be well under the null let me suppose the null is true for these calculations you're going to get this true forecast error right and then you're going to get the sampling error that came from the fact that you estimated data but beta is really zero right so what do you have here if you look at the mean square prediction error the mean square prediction error here is going to be the average epsilon squared the average prediction error here is going to be well it's going to be this guy squared right so i'm going to get this guy i'm going to get this guy squared and i'm going to get 2 times the cross product right an expected value this cross product again epsilons and x's uncorrelated right so what am i going to get i'm going to get this guy plus this guy okay now the important point of course of this that todd and clark talk about is that if the random walk is the right model it shouldn't forecast the same as model 2 it should forecast better right because model 2 is contaminated by the sampling error model 2 has kind of the fundamental problem in it but it's got this overfitting problem in it that came from the in sample stuff so what they point out is gee you know if i want to evaluate the random walk i don't want to ask whether these are equal right i want to subtract this over fitting part from this guy and see whether they're equal see that okay and what's cool about this right is that it's pretty easy to estimate this guy because you know x you know beta hat and get rid of that you can compute a sample average right so what they do and i'm gonna i'm out of time so i'm just gonna skip through this um quickly what they do is they consider the out of sample mean square error for the random walk minus the out of sample mean square error for the model 2 right but adjusted for the amount of overfitting you would expect it to have right if the random walk model was right so they put these guys on equal footing right they get rid of that sampling error okay and then you can work out the distribution of this and this is highlighting the notes it's actually there's some interesting stuff here about do you expect this guy to have a normal limit the answer is no so you got to do some stuff but i'm going to skip that given time and here's some here's some numbers so this is from their 2006 journal of econometrics paper and can you guys see this uh oh i love this you can make things bigger i can make it even bigger okay so these are like um forecasting dollar other country exchange rates um you know let's look at um japan okay so the out of sample period is 1990 through 2003. okay and then here's the out of sample mean square error for the random walk 11.32 here's the out of sample mean square error for a model in which they regress the change in the exchange rate on a constant term and an interest differential your monthly data right sure enough the model with the interest rate differential doesn't forecast as well mean square 11.55 versus 11.32 but if you say g if the random walk model was true right this model should have some overfitting in it how much overfitting should it have well you can compute it 0.75 you can compute the amount of overfitting right so this you can compute the amount of sampling error that comes in this from estimating the beta if the null is true so you subtract that from that and sure enough it looks like that model as a model not as a forecasting tool with estimated parameters actually would have done better by about 53 basis points okay and you can do this for these other things right so this is kind of their their insight um about uh which is very nice about this stuff okay now in these notes that will be available they're not available now there's another paper that's worth talking about that i don't have time to talk about um but rafaela was sitting here through most of this so i wanted to uh and it's a nice paper anyway um but there's this nice paper by geocomenian white and it's about a different topic right and it's about this topic it's about does forecasting model one with estimated parameters perform as well as forecasting model two with estimated parameters so it's not about whether the models are right it's about these guys as forecasting black boxes where you're estimating parameters is right so this paper is about a fundamentally different thing it's an interesting thing but it's different than this okay and and um i was confused about that when i first read this so i just wanted to highlight that it's just a different question and it answers the question in a nice way okay so um we gotta go to lunch so i'm gonna hear some more slides but you know this is what i've done um it's my last time to talk to you guys so let me just say thanks for uh sitting through all this it really has been quite a an honor to uh to do this uh so thanks so i'll see you this afternoon 