Sydney C. Ludvigson: Today,
I'm going to talk to you about consumption-based asset pricing and GMM type methodologies. We might just start with
a question which is, why should we even care about consumption-based
asset pricing models? I mean, after all, as students of financial economics know, the best fitting
empirical models are those explain asset returns
without their asset returns. The reason we care about
these models though, is that the true
systematic risk factors are macro economic in nature. They are derived from the inner temporal
marginal rate of substitution over some
numeraire consumption good and asset
prices are derived endogenously from these. So if you want to explain why asset returns behave
the way they do, we need to study these
types of models. Now, some
consumption-based models work better than others and we'll start with a very
basic one that doesn't fit the data very well at all
but it's a classic model. But I will emphasize here that all models are misspecified. Any model that we can dream
up will be misspecified. Macro variables, which are the inputs
to macro models are often mismeasured more than
our financial variables. Therefore, I will
argue in this lecture for a movement away
from specification test of perfect fit that is given sampling error of the
model is literally true, toward estimation and testing methodologies that
recognize explicitly that all models are misspecified and toward methods that
permit a comparison of the magnitude of
misspecification across multiple
competing macro models. Now, this is a methods lecture, so we're going to focus
mostly on just the methods, but I think these
things are important in choosing which
methods to use. Here's what I want
to talk about today, we are going to start with a classic example of a
consumption-based model, GMM estimation of
representative agent model with constant relative
risk aversion utility. Then talk about reduced
form generalizations of this model that incorporate conditioning information in the stochastic
discount factor and I'm going to call these scale
consumption-based models. Then structural form estimation
of generalizations of constant relative risk
aversion utility in the form of recursive
utility functions, for example, that studied
by Epstein's in and while. Here, there are two types of estimation methodologies
that I will discuss, although there are others in
the literature and I don't have time to talk about them, nor do I know as
much about them. But one is a semi, well, I wrote semi non parametric
minimum distance estimator. Abusive terminology was
also called as just semi parametric minimum distance estimator, and this would be
appropriate way to estimate these models when the law of motion for the data is left
unrestricted by theory. Then I'll finish up
by talking about some simulation methods which would be
appropriate way to estimate these forms
of utility functions when the theory explicitly restricts the law of
motion for the data. I'm going to talk about just two examples in the literature on the do this in application. Then finish up with
some final thoughts about consumption-based
asset pricing. I guess we were supposed to go for an hour and a
half and then have a break and then we'll talk some more and
then at the end of that, I'll try to leave 10-15
minutes for questions, if you don't mind just
writing down as we go along what your questions
are, that would be great. This is a review
passed in 1982, GMM. Remember that this is
a method of estimating theories that imply a set of moment restrictions and
that's what we see here. You have an economic
model that has some r population
moment restrictions and notice that these
are unconditional means. But you have some function h and it is over data, h by one
vector variables that are known at time t. Theta is an a by one vector of
coefficients, and of course, the idea is we want to
choose Theta to make the sample moment as close as possible to the
population moment, so that's the idea behind GMM. The sample moments are just the sample analogues
of the population moment, rather take the sample mean of this h function is also
an r by one vector. Y, let's stack all the observations in the
sample into a vector y_T, so now, we have a t times h by one vector of observations. The GMM estimator
is just Theta hat, which minimizes this scalar. There's this quadratic form. Choose Theta to make this as small as
possible and this is just a sequence of r by r positive definite
weighting matrices. Could be a function of the data. If r is equal to a, the number of mole
restrictions are the same as the number
of parameters to be estimated then you
could, in principle, estimate Theta just by setting
each of these gs to 0. You'd have the same number
of equations as unknowns. Of course, these g's are
not necessarily linear, so there may not be
a solution to that, and we're going to talk about
a very pertinent example in which that is
exactly the case, there is no solution
in just a minute. But if there is a solution, you would be able to find it by setting each
of those to zero. GMM refers to the
use of equation 1 to weighting these
sample moments, when we are estimating a, but number of parameters of a is less than the number of
normal restrictions. It gives us a way to weight, we have now more equations than unknowns and we
need to figure out how to weight those somehow. Hansen, of course, established the asymptotic
properties of this estimator, consistent asymptotically
normal under some assumptions, which I'll skip over here and also established an
optimal weighting matrix, So W equal to S inverse. What does optimal mean here? It means within the
class of GMM estimators, if you use this
weighting matrix, you'll have the smallest
variance of all GMM estimators. It doesn't mean that this GMM
is asymptotically optimal, and it's not necessarily
the least variants within the class of
asymptotically normal estimators, but within the class of GMM estimators it would
be the smallest variance. This S here is the spectral
density matrix if frequency 0, there's h functions and you will have to come up
with an estimate of that, and so obviously,
j can't go from negative infinity to
infinity in your estimate, so people use things like [inaudible] estimator of S. You weight different js
here differently. Usually, the lags
farther out get greater weight and then you
cut them off at some point, so you can easily
find examples of these estimates of
S. But what I want to talk about today is that in many asset pricing applications is actually not appropriate to use S inverse or an estimate of S inverse as a weighting matrix, and so we'll get into that. But keeping that in
mind, if we were to do this classic Hansen
GMM estimation, we would see that S
depends on Theta, of course, which depends on S. The standard procedure
is just to employ an iterative procedure and you obtain some initial
estimate of Theta. Let's call that Theta
1t by minimizing Q, remember, Q is just the
quadratic objective function subject to some arbitrary
weighting matrix. Usually, people start with the
identity weighting matrix. That gives you this
initial estimate, then you can plug that
in to the formula here, formula for the estimate to obtain initial estimate of S. Re-minimize this Q given your initial estimate of S and obtain a new estimate of Theta. Now, you could continue
iterating until the successive iterations give you arbitrarily
small differences in the parameter values or you could stop after
one iteration. The estimators based on
a single iteration have the same asymptotic
distribution as the estimators based on arbitrary
number of iterations. But of course, in finite
samples, properties of these estimators will differ and may depend on the data
generating process. Iterating offers the advantage that at least your estimate is invariant to the choice of initial weighting matrix and invariant to the
scale of the data. Here's a classic asset
pricing example that Hansen and Singleton estimated
in 1982 published paper. We have a representative
agent who's maximizing this
utility function, so C is consumption and there's
utility over and then we have what's often
called power utility and asset pricing, or it's an isoelastic
utility function where Gamma is a coefficient of
relative risk aversion. Gamma's 1, this
just log utility. If there are n assets available
to be invested in, and there are n
first-order conditions, which are the first-order
conditions for optimal consumption choice
and these look like this. Now, this script r is just
denoting the net returns, a number like five percent. Then we add one to make
it a gross return. They are i of these equations. Here announce
a moment restrictions, but there's a t here, remember that GMM applies to unconditional moments so we have a conditional expectation here. We'll deal with that
but let's first rewrite these moment
conditions this way, so I just divide both sides by c and then subtract the 1, so now, I have this equation. Why is this important
transformation? This is important because GMM applies to strictly
stationary data, and in this moment condition, we have the level of
consumption floating around, but we need all of these variables all of our observations would
be plausibly stationary. It looks like the
growth rate is at least much more
plausibly stationary. There's a few people who
would argue with that, but it's certainly more
plausibly stationary than the level of consumption
which we know has important trend components. We're going to rewrite the moment restriction
in this way. Now, here we have
two parameters to be estimated, Beta and Gamma, and Beta is our
parameter vector, so it's a two by
one, Beta and Gamma. If we use x star to denote the information set that
investors have in this model, then this is now our moment restrictions
and just rewriting this explicitly conditional
on this information set. Now, let's take x to be an
observable subset of x star, this might be something that the econometrician can observe. Then this conditional model here implies the
unconditional model here. Now, I'm taking essentially
Kronecker product with each of these i's with X and taking the expectation
of that product because we have transformed this into an unconditional
expectation. This just follows
because what's in the expectation
operator here has an expectation of zero
according to the theory, so according to the theory, this whole thing should be uncorrelated with
anything in x-star, including some subset
of stuff in x star and so therefore,
this product has to be zero in expectation. Now, we have again N
moment restrictions, we've turned this into a set of unconditional
movement restrictions. X can be some, will
call those instruments, it's m by one. Now R, which is a set of moment restrictions
that we might be taking to estimate and test
on the data is n times m, so we have n asset
returns and we have m elements of x, so we
have now n times n, n times n moment restrictions. This would be our H here, and this model can
be estimated and tested as long as r is
greater than equal to 2, as long as we have at least
two of these guys here. Notice that we could just
have x, could just be 1, as long as we have more than at least two asset returns here. Let's just take the
sample mean of this to get G. Now, of course, again, we just minimize
this quadratic form and choose Theta
to minimize that, that's our estimate
of Beta and Gamma. Then what hasn't showed is
that you can form a test of over-identifying
restrictions by its trivial to compute once
you've done the GMM, you just simply take Q, which is the
quadratic form here, plug-in your estimates, so you take the minimized value of the objective function,
multiply it times t, and then has a chi-squared
distribution with degrees of freedom r
minus a asymptotically. What does this mean to be a test of over identifying
restrictions? This is a test of
whether the model is actually true because it says, all these are our sample
moments is close to 0, as we would expect them to
be given sampling error. We want them to be exactly
0 if the model is correct, so that's what
this is test does, it tests whether
the model is true. By the way, for that
test to be true, you need this weighting matrix to be the S inverse there, it can't just be any
arbitrary thing, then this test holds. You can form other
versions of this test when the weighting matrix
isn't S inverse but doesn't have quite
as simple form. Now, Hansen and singleton took this to the
data and estimated this model and they used lags of consumption growth and
returns for their x, they didn't just have ones, they had some
variables in there. For the asset returns, they had a broad stock
market index and then some portfolio in industry returns. So returns of stocks over various industries. These non durables and services expenditures for consumption, this is fairly standard, and consumption-based
literature, at least testing representative
agent models. That was their data and they
had some estimates here, the Beta looks subjective
time discount factor, it looks fairly okay, close to somewhere around 0.99 across their various estimates. Risk aversion was
estimated to be low, 0.33 to 1 essentially. There's no equity
premium puzzle here, I'll say why that
isn't just a moment. But the model was
strongly rejected according to these kinds of over identifying restrictions. Then later on, in
Campbell Von McKinley, if you look at that book,
they report results. They don't actually have a table but they say in the book, and I know this to
be true from playing around with this stuff myself, that these over-identify
restrictions were rejected in their paper, but they're much more
strongly rejected if the set of test
assets includes both equity returns
and some type of commercial paper rate or
short-term interest rate, like a treasury bill rate. Why is that? That's because this model with these
preferences cannot capture the predictable
variation and excess returns over
commercial paper over a short-term treasury rate. If you think about
what's happening here, E(M) times r minus
1 has to be 0, so if R is moving around in a predictable way and M is not moving just covarying
just the right way, then E of M times R is not going to be equal to one and that's exactly
what happens here. This has led researchers to turn to other model of
preferences actually because this is a
very strong rejection of the power utility model, that constant relative
risk aversion model. This predictability
and excess returns in this model, you
might think, well, that might be explained by heteroscedasticity and
consumption growth or might be explained by
time-varying covariances between consumption growth and returns because that
would generate in the models some predictability
and excess returns. In fact, GMM is a distribution
free estimation procedure so it would allow for that
kind of heteroscedasticity. If that could then explain
this kind of predictability, it shouldn't have lead to strong rejections in the model, even stronger rejections
that occur when these excess returns
are included. It is for this reason that
many researchers said, well, we need to abandon this
entire set of preferences, there must be something
wrong with that. Now, notice that Hansen and Singleton did use some
conditioning information there, X was not just a vector
of ones as I said, this is what I'll
call scaled returns. If we go way back here, look at these moment conditions. This is a gross return. If this were just
a vector of 1s, you start out with some
equity return right here. Now, you're multiplying it by X. Now, you're really looking at a different
set of asset returns. You no longer just looking
at the equity return, you'll get the equity
return times X. This turns out to be the reason why they don't have an
equity premium puzzle, because we're no
longer trying to explain the excess returns
on the stock market. We're taking that
multiplying by X. Which moments you're
actually using to test will bring out different
puzzles with this model. But that was not the
puzzle that came out here with these
so-called scale returns. I'm going to talk about scale
returns more little later. But another limitation with
the classic consumption cap M is the large unconditional
Euler equation here. Now let's go back to
thinking about x as being 1, for example. Well let's call these
pricing errors, even when these parameters
are freely chosen to fit the data. What do
I mean by that? Suppose we just take M, I'm calling M a bunch of things. M is the intertemporal
marginal rate of substitution overconsumption, M is the stochastic
discount factor, and sometimes, I'll call it
the pricing [inaudible] , but it's always this thing. Now, if we define Euler equation errors or
pricing errors, for the case of raw returns,
that would be this. There's no t here, so
these are unconditional. Excess returns, the
error would be this. If the model is true,
that should be zero. Then let's use GMM to choose
parameters Beta and Gamma. There's two parameters here. This is again just the G or the sample means
of these errors. This is just as we've
been talking about. W can be anything
for this result. You can use the optimal
weighting matrix, you could use the identity. This result doesn't
depend on that. But now, let's just plot,
take for this model, use post-war data from '54-2002. Let's plot what these errors
are as a function of Gamma. These are for the
excess returns. Here, there's only one. We have two sets of
asset returns here. One is just a stock
market return. We have one parameter. Beta is set down here in order to minimize the Euler
equation for the T-bill. But so now, we're just
choosing Gamma to make this excess return error
as small as possible. To give a sense of how large this error is relative to
the assets being priced, we're going to
plot the root mean squared Euler equation
error relative to the root-mean-squared return, which is just the sum of the square root of the average square returns
of the assets being priced. These lines are almost
on top of each other. There's one line for the
stock market return, cross value weighted index, and then another is a
multiple set of returns. We have the stock market
return in these six portfolios sorted on the basis two and so on the basis of market
capitalisation, high and low and then three, on the basis of
book-to-market ratios, a portfolio of stock returns. But what this says is that the root mean square error can never be driven anywhere near to zero no matter what Gamma is. In fact, the smallest
it can be as around Gamma is like 112 here. It's about 60 percent of the average square returns on the assets being priced. For the single stock
market return, this is just saying that
the Euler equation error is 60 percent of the average
annual stock market return, so it's really big. This model is not anywhere near as being able
to capture that. This is very large
magnitude of error. Unlike the equity
premium puzzle, these large Euler equation here cannot be resolved with
high-risk aversion. Now, we took some leading consumption-based
asset pricing models, including habit based
models have done very well in matching other aspects of asset pricing data like
Campbell Cochrane model and the variant of that studied by [inaudible]
Santa and Bernese. Long run risk models that have
also become very popular. We'll talk about estimating
one of those in a minute and limited participation model
studied by [inaudible]. Then we ask whether those
models could explain the failure of the standard
model in this dimension, and none of them could. In other words, if those models really did generate the data, then what you would find is that if you
took that data and did GMM estimation of the standard constant relative risk aversion
consumption-based model, you would always be able
to find values of Beta and Gamma such that the Euler
equation for those, that constant relative
risk aversion model, we're exactly satisfied whereas this is not
true in the data. In this world, these
models are implying that observationally equivalent
model for a different set of parameters in which must be
the standard classic model. That should work perfectly well. Now we found this to
be a striking anomaly actually because much
of the early evidence, like the Hansen-Singleton
evidence that the classic models, Euler equation errors or Euler
equations were violated, provided a lot of the impetus for developing these newer
models in the first place. What this means is
that the data on consumption and asset returns are not jointly log-normal. Almost all of these models, they're approximately jointly
log-normal if not exactly. MALE_1: Is it okay
to ask a question? Sydney C. Ludvigson: Well,
Jim, you may have missed. He requested the questions
would be left in the last 15 minutes because
we're video recording. But if you don't mind. Now, we've talked about using non-optimal weighting
matrices in GMM. What I want to talk about now is reasons why NASA
pricing applications, we often don't want to use the optimal GMM weighting
matrix. Why is that? Here's a bunch of
different reasons. They may vary from
application to application. One reason is that assessing specification here
and comparing models. Suppose we have two
estimated models of the SDF. One maybe the classic
consumption-based model. OID restrictions
are not rejected. Suppose they weren't rejected. Then say the cap M, which is just a simple linear
model and where this is the returned aggregate wealth
are all invested well. Now, here you might find that the OID restrictions are
rejected if you do this, Hansen optimal weighting matrix and tests the over-identify
and restrictions. Are we right to conclude the model 1 is
better than model 2? Well, no, because this J test depends on the model specific S. It's as if you're comparing
apples to oranges. What can happen here
is that model 1 could look better than model 2, simply because this two
classic discount factor and the pricing errors
make G more volatile. They're more volatile they
make G more volatile, and that's just blowing up
the variance of everything. It's making it harder
to detect deviations from zero in these sample means. But there's not because their
pricing errors are lower. This one can actually have
much lower pricing errors and not be rejected if it's better measured, the [inaudible]
is better measured. If you want to compare models, you can't do that this way. The classic way to do that is based on work by
Hansen-Jagannathan, if you want to compare
models, the idea is, let's say you have a candidate
staccato discount factor as a function of parameters, and you're going to use
this distance metric. Where this is again
just a square root of a GMM criterion function where you're using a
specific weighting matrix. This is G. G is the second
moment matrix for returns. Again, this is just
the same thing we've been estimating sample mean here of N times R minus 1. Notice that this
distance metric does not reward stochastic discount
factor volatility. G is the same for all models. Now, you're putting them
on an equal playing field. It's suitable for
model comparison plus this distance metric is a measure of model
misspecification. Two ways of thinking
about that, as they show, it gives the distance between the candidates to get
the discount factor and the nearest point to
it in the space of all stochastic discount factors that price assets correctly. It gives the maximum
pricing error per unit norm of any portfolio formed
on these N assets. Now we have a metric. It's suitable for model
comparison and it's a measure of this instead of
misspecification of the model. That's really one of
the great appeals of this distance metrics, recognizes that all models
are misspecified and gives us a measure of how
much each model is misspecified and provides
a method for comparing models by assessing which
model is least misspecified. Now, an important
problem though is how to compare these HJ
distances statistically. 8:45. What time for break, 10:00? What does it say? Sorry.
What? 10:15 I stop. What time is it now, 9:15. An important problem
then is, well, how would we think about comparing HJ distances
statistically? We can now compute HJ distances and see the model A has a
different one for model B. But how different is
model A is from model B. We have results
for knowing how to test whether model A's distance is itself different from zero and model B's
different from zero. That doesn't tell us how different they are
from one another. What a possibility
that Chen and I worked on recently is to use a method based on
White's reality check. This method allows
one to compare statistically HJ distances
among K. There's some number, finite number of
competing models using White's reality
check method. Here's this method. What you do is you
take your benchmark. So now you've estimated HJ
distances for a bunch of models and you can rank them
by distance or squared it. I'll explain in a
moment why we use the squared distances here. Now, you write these
by squared distances. Let's just take your
favorite model. Let's say usually
it'd be the one with the smallest squared distance. The question is, is that the smallest statistically
from the others? Now, the null then would
be the squared distance on model 1 is smaller than the squared distance
on a second model, or the D2 here is
the competing model, the next smallest
squared distance. Then they'll form
a test statistic, the script TW, which is root t times just the
difference in these distances. The intuition here is
if the null is true, the test statistic should not be unusually large given
sampling error. Now, if you had
distribution for a town w, you would simply get
a one-sided tests. You'd simply reject the null if the historical value of
this test statistic, which you can easily
compute given that you computed squared distances or distances for each
of these models. That's greater than
the 95th percentile, you would reject the null, but it's less, you wouldn't. The appeal of this method
is that it applies generally to any stationary
law of motion for the data. There have been a few methods. One that I know of
that was developed for VAR specifications
with Gaussian errors. But this doesn't require
that, just stationary law of motion for the data into multiple competing models
that are possibly nonlinear. Doesn't have to be a linear
stochastic discount factor. It's actually quite general. What about the distribution? Well, the distribution of Tau in practices is computed via a block bootstrap. That might be time-consuming if you have
a highly non-linear model, but it's in principle
straightforward. The reason is that it is because Tau W has a complicated
limiting distribution. It's actually just easier to compute it via
block bootstrap. Of course, the bootstrap
is unjustified unless we know theoretically that we have a multivariate joint continuous limiting
distribution under the null. Well, fortunately, we
know that at least we have proofs of that for
applications that apply, for applications that are relevant to most
asset pricing models. For parametric
models, we know that from results in Hansen
[inaudible] and Latimer. For semi-parametric models, which we'll talk about later, which include an
unknown function. We have results of [inaudible] . This is a very general
methodology that can be used to compare distance
metrics across models. Why else do we use
non-optimal waiting? Are there other
potential reasons? Here are some reasons that
we might just want to use the identity matrix. You have an econometric problems near singular S
inverse or G inverse. This is just an asset. Returns
are highly correlated. If you have large
N and a modest T, we can have many asset returns. If we were to look at the universe of stocks, for example, that would be a
very large number, and then our time
series dimension isn't necessarily that large. If T is less than N and the covariance matrix for the an asset returns a singular. We can invert that at all. Well, if T isn't
sufficiently larger than N, then this matrix can
be near singular before the conditioning you
run into all sorts problems. The whole issue of the
JPES about 1996 or 1997, study the finite sample
problems with GMM. Most of them revolve around poorly conditioned values of this S and G and finite samples. Get rid of that and just use I, in some ways,
that's more stable. The other reason that
people may want to use the identity weighting matrix and asset pricing implications is that the original test assets that you use have often been carefully chosen to represent particular economic
characteristics such as size and value. Now, if you come along and use a weighting matrix like S inverse or G inverse,
you're undoing that. One easy way to see that is just to
note that if you use one of these
weighting matrices, that's the same as using the
identity weighting matrix. Then doing GMM on a reweighted portfolio of
the original test assets. Now, you're no longer
testing anything on the original test assets,
you've moved onto some other. That's just easy to
see by noting that a positive semi-definite
matrix is a factorization. It looks like this, so I
can write this like this. This is the same
objective function, but this is now the
identity waiting. If you write out the
elements of this, you see that what
you're really doing is GMM with a different
set of portfolios, not the original set of assets. The other thing is that these reweighted
portfolios may not provide a very large spread and average returns
somebody could, it depends, you're choosing this weighting in some
statistically optimal way, but now you don't know what you're going
to get in the end. That means there's less
for the model to test. These are at cross-sectional
asset pricing models and we are ultimately testing
whether the covariance between the stochastic
discount factor and the gross return is enough to explain the spread and average returns
on these assets. Now we don't have
a large spread, there's not as much
for the model to test, could also be this
reweighting implies that these portfolios
look crazy. They might imply implausible
long and short positions on the original test assets. But here's a reason
not to use I. Many things in applied work, there's judgment that comes into play about what you actually
are trying to achieve. Let's say use the
second moment matrix of returns as a
weighting matrix. Then the GMM objective
function is invariant to the choice of test assets, or at least the
initial choice, to extent that the set of
assets out there can be spanned by the
set that you have. Is this rich enough?
Then now, this will make your estimate invariant to the choice of test
assets that you have. That's nice feature
and that's easy to see by just form a
portfolio A times R from the initial asset
returns R. R is N by one and A is N by
N. Now, we've got different portfolio of linear
combinations of these Rs. Now, notice that these
weights sum to one, so A times a vector of ones, just back to a vector of ones. But now, I can
simply rewrite this, this is what we
would estimate on the original set of
test assets if we use the second matrix inverse as a way of weighting
the gene and injective function as suggested by Hansen and Jagannathan. We can rewrite that and
that's the same as this. Objective function
is invariant to any portfolio transformation of the original test assets. With W equal I, that
will not be true. Your GML objective function
will depend on that. Again, you may want that if
you're very interested in testing whether the
model can explain those aspects of that particular
set of asset returns, but you may not want that, depends on the application. Let's talk a little bit about some generalizations
of the standard model. As I said before, people have moved away
from these preferences because they don't seem to
match the data very well. But it doesn't seem like just patching it up with
some heteroscedasticity and consumption growth and with these preferences is
going to fix things. People have started looking
at other preferences. Now, many
consumption-based models can be approximated and reduced form as a linear
function of consumption growth. This is now lowercase
letters are long variables. A simple example is back
to our old classic model, consumption cap M, and
then these weights here, a and b are fixed. These are just fixed parameters depending on Beta and Gamma. But you can look at
more complicated models and with habit formation, for example, with Campbell and Cochrane, or Menzly et al., at least they had a
habit formation model that depends on some
parameters here. Now, x is a habit and your
utility depends on c minus x, not just on c. They call this a surplus
consumption ratio. What we see happening here is the log surplus consumption
ratio ends up here. These are just
non-linear functions that are part of their model, these size and Gammas. Actually, any habit based model could fall into this category. You can approximate and it will look
something like this. First, I'm going to
constantly need these to estimate something but they
don't use this reduced form, but they also have a habit, simple habit structure and it will be
approximated and it'll look something like this. Now, in this model, the a and the b move around with the surplus
consumption ratio. In this model, surplus consumption ratio moves
around risk aversion, so risk premia are
moving with S. Now, we have a situation where M shouldn't depend on a
and b in a way that proxies for time varying
risk premium should be good proxies for a time
variation in a and b. With that in mind, we can think of various
empirical specifications. Martin let down, I looked
at one where we said, well, let's just let a and b be
linear functions of some z, and z is a cay thing, which is a cointegration
residual between consumption, asset wealth, and labor income. It's related to the log consumption-aggregate
wealth ratio. But it also is a
strong predictor of excess stock market returns. This is what we would consider a proxy for risk premium
moving around over time. So you're thinking of putting forecasting variables for excess returns in there. This is very reduced
form because this is not necessarily a structural
estimation by any stretch. But yet it differs from the standard model
and that we have this state dependent
weights here. The pricing kernel
is now a state dependent function of
consumption growth. Other examples, almost any model can be approximated this way. Take a model has two elements and two arguments
in the utility function. You have some
housing consumption, some numeraire
non-housing consumption. This is just a CES
aggregator over those two and then you have a constant relative risk
aversion over the aggregate. Now, the log of m could be
approximated in this way. Now you have time during
weights potentially here. This S, it's the relative share of numeric consumption
and total consumption. These are relative
prices here, 1, 2, 3. For example, [inaudible]
and Newburgh, have a paper. They have some incomplete markets model. Because of incomplete markets, there's imperfect
risk-sharing and one of the ways that you
can insure against idiosyncratic shocks. In this model, is
you could borrow. But how much you can
borrow depends on which collaterally you
have, housing collateral. The value of housing determines
how much you can borrow. In the aggregate, amount of housing collateral
varies with the price of the house and that's
indulgence in their model. But you can imagine that the housing collateral
ratio, ratio to what? Amount of housing collateral
is relative to something, say the amount of
housing wealth. Forget now it's a ratio,
but think with amount of housing collateral
is that goes up, there's more risk-sharing
because people can insure against risks now more. They can borrow more and
insure against risks. Risk premium should go down. In their specification, the amount of housing
collateral is what's in z. They take this to
the data as well. Their z is a function of
the housing collateral. Here, risk premium move around, not because risk aversion
is moving around, but because the quantity
of risk that can be shared or insured against changes over time with the
housing collateral. At this point, it's important
to point out that there's two conditioning
that's often confused. Think about the Euler equation. Euler equation often
looks like this. It's a conditional mean here, our conditional one is some z. There's an unconditional
version which is just, take unconditionally
expectations of both sides and now we have this. It's a condition down to unconditional expectations
by the law of iterated expectations that second bullet point
holes from the first. Now, there's two forms
of conditionality. One is what I referred to
earlier as scaled returns, and that's a way of capturing this conditioning information. You take the return, this is now gross return, I plus that script R. You
take the return here, you chronic with a
set of instruments, 1 and could be constant,
and then some z. Then the other is you scale
factors and that's what I've just been talking about in
the previous two slides. Some set of factors, starting out with some
fundamental factor like consumption growth. Now suggests that M,
the pricing kernel, stochastic discount factor is a linear function of that with potentially
time during weights. That's what we've
just been talking. Scaling factors looks like this which is different
from scaling returns. Now, the scale consumption-based
models that people have looked at are conditional
in the sense that M itself is a state dependent function of
consumption growth, which is not true of
the classic model. That means factors are scale. The scale consumption-based
models have been tested, however, on unconditional
moments, that is, testing the model's
unconditional implications, not the conditional
implication so that we're not scaling returns here. There's no conditioning
going on here. What this means is the scale consumption CAPM models turn a single factor model with
one fundamental factor, which is consumption grow into a single factor model with
state-dependent weights into a multi-factor model
with constant weights. I think I'm using now f for multiple factors, which
is a little confusing. But M, we'll write
like this for example. Now we've turned a
single factor model with state-dependent weight into a multiple-factor model. We have factor 1,
factor 2, factor 3, which is a product of z in next period's
consumption growth. Now we have a
multiple-factor model, let's just call these
multiple factors f now and a vector. We no longer have a single
consumption factor. Now we're going to have
multiple constant Betas for each factor rather than a single time-varying Beta for
consumption growth. This is not a
conditional consumption CAPM in the sense that there's a single
time-baring consumption, no there's just more
than one constant Beta. This just shows you how to derive the Beta of
representation. Just ignore time into
season and let F be f' stack with 1 and then M is a linear function of f. We start with this set
of moment restrictions, this is in Euler equation. R times f' times b is just
plugging in this, to here. Keep in mind these are
unconditional moments, there's no t here. We're testing the model on the unconditional
implications. If you write that out like this, you just use the
definition of covariance. You write that out like this, then you can show that
the expected return on each asset is a function of this and then rewriting things. Where is b-bar? That
might be a typo. I'm not sure what b-bar
is, I think that's a typo. It's not coming to me now, but this line seems to
be the same as that, if b bar is just b. In any case, at the
end of the day, you end up with something
that looks like this. You can rewrite it so
things look like this. Now, what is this piece here? That's the Beta. It's a multiple regression, coefficients of
returns on factors. This piece here is like Lambda or sometimes it's
called the price of risk. Notice that we now have
multiple constant Betas. We have multiple
sources of price risks. But you can estimate this in a cross-sectional model using the Fama-Macbeth methodology and Michael Brandt is going
to talk about that. I won't say much about it,
but here's just a preview. What you do, you will
estimate these Betas in the first-pass time
series regression. To Beta i's, these are just multiple regression
coefficients. Returns on factors
for each return. Then second, you have T
cross-sectional regressions, one for each period
in your sample. You're going to regress
excess returns. This would be some
reference return, maybe a T-bill or
something in practice. On these Betas, you're
going to store each time you get a different
Lambda and a different Alpha. Then just take the average of those as your
estimate of Lambda, you would do the same for
your estimate of Alpha. This is the pricing error. It should be zero
for model's true. You could test where this
pricing error is zero, but then this would be the variance of
your estimated Lambda. Then you will want to report
t-statistics that are corrected for the
fact that you've estimated the Beta
in the first stage. Michael Brandt will tell
you more about that. But here, the point is
that the scale models that conditioning has
done in the STF. Now, we've turned a model that's a state independent function of consumption growth
into wanting to stay dependent function of
consumption growth, but not in the Euler equation. This could give rise to a restricted conditional
consumption data models. So notice that now that
we have multiple factors, 1, 2, 3, you've got
three Betas here. We could rewrite this, this way. We could call this a
time-varying consumption Beta. We could look at the
properties of that, but notice that this
is not what you would necessarily get from estimating a time very
consumption beta by modeling time variation in this particular
conditional mean. It's unlikely to be
the same and notice also that you have to control here for an additional
risk factor, or lagged risk factor. This is going to move around with some proxy
for risk premium. Now, why am I making such a big deal
out of this distinction? Well, it's important because with conditioning in the
stochastic discount factor, theory often provides
guidance about what kinds of variables
you'd want to put in there. These weights move around with some proxies for risk
premium usually. If you have a handful of good predictor
variables for returns, you could think
about using those, but that's really what you would typically a few
variables that capture risk premium should enter if your estimator when
this reduced form lines. But if you have conditioning
in the Euler equation, you now have to model the entire joint distribution of M and R. This could require many
variables beyond that, just a few which
capture risk premium. We're trying to model the conditional
expectation of M and R. In principle, that means
you have to take M and R, take the expectation of that times every
variable that's in the investor information set that you're conditioning on and every nonlinear
transformation of that. That's in principle what the conditional
mean of that is. That can be an awful
lot of instruments and a finite sample,
and in practice, the results can be
quite sensitive to your chosen
conditioning variables. These variables may fail to span the information set of
market participants. Suppose you could go
out and you try to test ET of m times r equals 1 for some model M and you put in some conditioning variables X in therefore to capture that. Now you might not find
the model work so well, well, it's not clear what you've really found maybe it's
conditional on those variables. Maybe you haven't
included enough conditioning variables. There's lots of
information out there. Investors might be
conditioning on, maybe you don't have enough
non-linear functions of these variables in there
and you're just not approximating the conditional
expectation well enough. We don't know. One
partial solution and attempt to deal with this by summarizing the information on large number of time series with a few estimated dynamic factors. I've done this in a
couple of papers with [inaudible] . I didn't really have time to talk
about that today, but you could take hundreds or potentially
even thousands of economic time series and estimate small
number of factors, and you can test statistically for how many of those should be and summarizing information and all those hundreds
of time series. Bottom line here, we have some conditional moments of M times R. Those are
potentially very difficult to model
because we don't necessarily know whether we have a rich enough information
set that we've relied on. This is a reason to focus
on unconditional moments. Reason to focus on these
moments, and testing models. Second is that these
models are all misspecified,
they're not correct. The interesting question to me is whether the state
dependence of Amman on consumption growth implies
less misspecification than the standard fixed
weight consumption cap-M. The interesting question is not whether the scale model or any other model is literally true because I don't
think they are. As before, we could
compare these models on the basis of HJ
distances and use it as White reality check method to compare them statistically. We can really get at
the question, okay, given that these are
all misspecified, which is least misspecified
at the setup models. I don't want to talk about generalizations of the constant relative risk aversion model. The generalizations that we're no longer going to
be thinking about reduced form generalizations. Approximate linear state-dependent functions
consumption growth, but instead estimate the
actual structural form of a particular generalization with recursive preferences
and that one study by Epstein's in a while. This is recursive utility is of interest because one
is that it affords a far more greater degree of flexibility as regards
attitudes toward risk and intertemporal
substitution than does the standard constant
relative risk aversion model. This flexibility is of
interest often because it's unclear why an
agent's willingness to substitute consumption across random states of
nature should be so closely tied to
her willingness to substitute consumption
deterministically over time, as it is in our old friend, the constant relative
risk aversion, classic consumption CAPM model. Those two are very
closely related. Risk aversion, the coefficient of risk aversion
is the inverse of the elasticity of intertemporal substitution
in that model. Secondly, because these
preferences deliver an added risk factor for
explaining asset returns above and beyond the risk factor in the standard consumption-based
asset pricing model with constant relative risk aversion utility, so we'll see that. That's good because that
model doesn't seem to have the right risk or
enough risk factors or something to explain the
spread and risk premium, we observe an asset return data. But there's really been
just a small amount of econometric work, at least estimating structural models and
recursive preferences. There's a few papers I'm
going to talk about, but these are only starts and there's this gap
in the literature. Actually, there's
an entire gap of structural estimation
literature in my view, but part of the reason is that these are
complicated models to estimate structurally. I wanted to discuss here two examples of
estimating Epstein's and [inaudible] models. For general stationary
consumption growth and cashflow dynamics, we're not going to restrict
those, that law of motion. We're going to talk about
some work I've done with Xiaohong Chen and
Jack Favilukis. Then when restricting
cashflow dynamics, that is when the
cashflow dynamics are explicitly
part of the model, such in the bonds OR your own style, long-run risk models. When you're talking about
some paper by bonds all go on top and which
estimates this model. In the first case, the
data-generating processes left unrestricted. As is the joint distribution of consumption and
asset returns. This is a distribution
free estimation procedure. A lot like GMM, so it ties
back into our theme of GMM. In the second case, both the data-generating
process and the distribution of shocks or explicitly modeled. They are part of the theory. You exploit that in estimation. Here's some basics
from this model. We have V. What is V? V
is a utility function. This is the recursive
utility function. We're going to just focus
on this specific case that Epsteins in
a while studied. This is just one version
of recursive utility. Where you have a CES aggregator, over consumption, V is the continuation value of the future consumption plan. R is this nonlinear
function of that, the script F is
information sat upon which investors are
conditioning expectations. Theta is a relative
risk aversion measure, and 1 over Rho is a measure of the elasticity of
intertemporal substitution. Beta, again, is a subjective
time discount factor. This is the way the
utility function works. Now we're going to be estimating an Euler equation
that uses this. Again, we need to, all of our
estimation methodologies, almost all of them
require stationary data. We need to transform everything. We're using plausibly
stationary variables. We're going to divide by
C. Now, I'm going to have a formula for V over
C. Instead of just V, just divide everything by C. That's where you end up with. Hansen lead in this
in a recent paper. They worked with this form
of the utility function. A special case, of course, is when Rho is Theta
and then you collapse right back to the constant
relative risk aversion separable utility model with V equal to Beta times C. This is just the
power utility model. Switching notations. Now, Theta is what Gamma was before. Marginally substitution, here is the pricing kernel with
the added risk factor. What do I mean by
that? Here we have, this looks a lot like and in fact is exactly what
our old friend, the classic consumption
cap end had. This will get rid of this and you end up with
just this and this. This would be your
stochastic discount factor. Now we have this whole
multiplicative term here. You can see how
this really expands the scope of the asset
pricing implications, potentially in the
right direction. Because now, I mean, the question of whether a model explains asset
returns is really the question of whether
the M co-varies with gross returns
in the right way. Because E of M times
R minus 1 must be 0. Discount of returns are always
the same for every asset. They have to be an
expectation equal to 1. That's what the model implies. The model is going to do well, and then each asset has its own unique
covariance with them. That's why we get
different risk premium. M has to move around
in the right way. You can imagine that if
this doesn't move around, maybe this stuff, which is dependent on
the continuation value of the future consumption plan does move around
in the right way. There's some scope for
help here with this model. Obviously, a difficulty
that we face in thinking about estimating models that have this structure, is that we see that
this, first of all, there's this
observable in general, we don't know what
this V over C is. We don't know what B is. This observable function
of future consumption. It embeds the script R, which has an expectation of a non-linear function
of expectation. That's also tricky to
deal with in estimation. I think about GMM, that
would be a problem. Can just apply GMM. We'll
see that in a minute. One approach to this problem
in practical applications, that was used by Epstein-Zin, in a Empirical paper
is to say, well, look, we could derive an
alternative representation of M in which we relate the continuation value of the future consumption plan to the return to all
invested wealth or grade to aggregate wealth. If we do that, we could rewrite this stochastic discount
factor this way. Now we have this piece, which looks like
the standard piece. Raise it to some power, but
we have now 1 over R_w, where R_w is the
aggregate wealth return. In this application,
what did they do? They said, well, this
is straight forward. If we had data on our
W, we can just take it. This model and estimated by GMM. Just the way we've
been talking about. Instead of having this piece, PRM, we have the whole thing. I just need data on our
W. What did they do? Were they said, well, let's take the aggregate stock
market return and use that as a
proxy for our W, plug that in, proceed with GMM. That's a really reasonable
place to start and might try a few different
observable returns. The problem with that of course, is that our W represents a
claim to future consumption. That's itself an observable. There are lots of parts
of aggregate wealth that, for example, human capital, that we don't observe
the return to. Many estimates suggests
that human capital is the largest component by a large fraction
of human wealth. Actually, start market
wealth is very small. We might be a little concerned
that were not adequately proxying this unobservable
aggregate wealth return. In essence, we haven't really solved the problem of having the observable B by doing this. Because our W is no
more observable than B. There are some special
cases, however, that will make this
problem go away. This unobservability of
V or our W. For example, make this model more tractable. One is the EIS is one, suppose, and log consumption growth follows a linear
time series process. This is log linear. Then the log of V over C does have an
analytical solution. Actually, this is what Hansen, Heaton, and Lee exploited. There's another paper, [inaudible] aggravated data. This is a paper in 2005 JPE. Then Malloy Moskowitz
and Vissing Jorgensen use disaggregated data and stock market holder
consumption data based on [inaudible] work measuring the consumption
of stockholders. Now, once you have
analytical solution for V over C as a function
of future consumption, you can just proceed to
estimate in that case. But you need the EIS to be one. You need this
assumption to be true. If returns and consumption
are assumed to be jointly log-normal
and homoscedastic. Then you can show
that risk premium are approximate log
linear function of the covariates
between returns in news about current and
future consumption growth. Now, if this were the assumptions
that you were making, all you would need is a
model of expectations, multiple steps in
the future in order to compute these news terms. That's easy to do from a VAR. That's what people like
Campbell have worked on. Estimating models where they've made
these approximations. You obtain a cross-sectional
asset pricing model. You have a bunch of
excess returns and their functions of
these covariances, returns in these news terms about future consumption growth. But of course, these are all special cases. The EIS equal 1, implies the consumption of
wealth ratio is constant. It seems to contradict
statistical evidence but it's not. You might ask about what's
statistical evidence since aggregate wealth
isn't observable. We don't know for
sure that it's not. But for example, the cointegrated residual
during consumption as to wealth and labor
income should move, be highly correlated, not perfectly correlated
depending on some assumptions you make about statistical properties of labor income with the
consumption of wealth ratio. That then would move around expected returns
and then the data, at least the observable
component of that is collinearity residual does
forecast expected returns. This is a highly
suggested, consumption of wealth ratio is not
constant over time. But also, joint log normality of consumption and asset returns is strongly rejected
in the data. We talked about one
aspect of that earlier, which is there's no, you take a single excess return. You have now, for
any parameter Gamma, you can never drive the
Euler equation to zero. That would say that the
joint distribution, that would be true. Back up. If consumption and asset returns were jointly log-normal, you would always be able to find a Gamma that made that
excess return zero. Because essentially
you end up with a log linear specification, two linear equations
and two unknowns, Beta and Gamma, then
you can solve that. This says that the
consumption and asset return data are
not jointly log-normal, and indeed, statistical tests strongly reject
joint log normality. Consumption growth
itself looks pretty log-normal in univariate way, but not the joint distribution of consumption and
asset returns. Okay. Well, these considerations to
us point for the need for estimation
methodologies that are feasible under less
restrictive assumptions. We'd like to be able to
estimate this model and say something about its
parameters and how well it fits the data without making these kinds
of approximations. Although these are
very good first steps. Now, we don't necessarily want to make these
approximations. We'd like to be able
to evaluate the model against asset
pricing data and we don't want to make
these approximations. Because without doing so, we don't really know how well these models fit the data once we've relaxed
these assumptions. We don't know for sure
that these assumptions are innocuous for reasons
that we just discussed. Keep in mind, here we are with this is pricing kernel that
we're dealing with here, and remember there's a Script
R is function of here. It's a non-linear function with a conditional expectation of the continuation value of the future consumption plan and we're going to write
it like this actually. We're functions of
stationary variables. That's the goal. What I'm first
going to talk about is some work that I've done recently with Xiaohong
Chen and Jack Favilukis, in which we use a semi-parametric approach
to estimate this model. We do not need to proxy for the unobservable
aggregate wealth return. We're not looking out there
to find a proxy for this. We don't need to
log-linearize the model as in some of these
previous approximations, and we're not going to place parametric restrictions
on either the law of motion for the data or the joint distribution of
consumption and asset returns, or the value of keep
reference parameters such as the elasticity of
intertemporal substitution. The goal here is to be
as general as possible. Then what can we do? Well,
we could obtain estimates of the structural
parameters of the model. Beta, relative risk aversion, Theta, the inverse of the EIS. Once we have those estimates, we can evaluate how well this model fits the data relative to competing
specifications. Then we could investigate
the implications for the aggregate wealth return and for the return human wealth, if we're willing to make
some assumptions about the relationship
between aggregate wealth and human wealth, and stock market wealth. So
for example, in this work, we will assume that the aggregate wealth return is a portfolio weighted average of the return to human
capital and the return to stock market wealth,
that's an assumption. Okay, so this is
semi-parametric approach is an example of a [inaudible]
minimum distance procedure. We'll talk about what that is. Okay, so let's go back to
our first-order conditions. Here's the first-order
condition for optimal consumption choice in the Epstein's in while model. Okay, and again, you've got the script R are
embedded in here. Now, notice that in
this general state, you couldn't just take this
to the data and do GMM. In other words, you're
not going to chronicle this with X, where X is some known
conditioning variables and go off and do GMM. Why? Well, because this Script
R embeds an expectation. For GMM, you have to
have the expectation all on the outside. We have a problem in doing GMM. There are other problems
doing GMM here. The other problem is we
don't have data on v, so we can't just go
to town like that. First of all, what you can do is recall that
the definition of the utility function scaled
by consumptions is this. Now, let's take that definition, substitute n for this Script R, invert that substitute in
here, and write it like that. Now, we have n
moment restrictions and these are Euler equations. If you have n test
asset returns, you've got n owner
restrictions and then this is a cross-sectional
asset pricing model. It says that we're going to
explain a cross-section of average returns by how much they co-vary with
this entire thing, which is the stochastic
discount factor. The entire content of all of these models is in the
stochastic discount factor. That's where the economics is. These moment restrictions
will form the basis of an empirical investigation. Now, I'm going to call this
a semi-parametric model. Econometric model
because we have both the finite
dimensional parameters and let's lump these
into a vector Delta. We have three finite
dimensional parameter, Beta, Theta, and Rho. Beta is subjective
time discount factor, Theta is coefficient
relative risk aversion, and Rho is the elasticity of
intertemporal substitution. We have a vector of finite dimensional
parameters and then we have an infinite
dimensional unknown function, which is V over C. That's a sense in which
is semi-parametric. Both of these appearing
in the model. The reason we have to deal with this unknown function is
because we do not want to go back to proxying for the
aggregate wealth return because that is also unknown. We can be in the same
position of having to deal with the aggregate wealth return into being an unknown
function stuff. In this particular
application, let's say, let's assume that f is a function mapping R2 into r so the V over C
is this function. It's a function of lag v over c and current
consumption growth. Now, of course, here, again, like with GMM, we're going to assume strictly
stationary variables. Consumption growth is
strictly stationary and ergodic and this
function is such that this V over C process is asymptotically
stationary and ergodic. Then this particular arguments would be justified, for example, if the change in
log consumption or consumption growth is a
potentially nonlinear function of a hidden first-order
Markov process X. Now, if you wanted to have a second-order Markov process, the proof would probably
imply that I haven't done it, then you need a second
log of V over C here. But what you can show us under fairly general assumptions,
if this is true, then the information in this
hidden Markov process would be summarized by
these two arguments. If consumption growth has a first-order Markov process;
it could be non-linear, then this says that the
continuation value of the future consumption plan divided by consumption growth is some other non-linear function of V over C and of current
consumption growth. The question is what is F? We're going to use past
data to tell us what F is. That's the non-parametric
part of this. We're going to solve for F using the observations we have
in our historical sample. Just a note here that with a non-linear
Markov process for X, F can display non-monotonicities in both of its arguments. We're not necessarily
looking for monotonic relationship
between V over C and V over C and
between V over C and consumption growth. Note that this
Markov assumption is only a motivation for
the arguments here. The econometric
methodology itself leaves a law of motion
for the data unspecified, but we do have to say what the arguments are in
this random function. I would argue that this is not a crazy assumption for
aggregate data at least, at a first-order Markov process is probably a reasonable. The higher-order
Markov processes are probably not necessary to describe these linear
time-series process for consumption growth well. But you want to make more
general assumptions you can, but this is just a motivation for these particular arguments. But again, this is a methodology that's a distribution-free
methodology. If we're going to now have F_t denote the agents
information set at time t and let's put z_t plus 1; let's put all the
observations of t plus 1 into a vector Z, and then let's now define
Gamma_i to be this thing. What is this again? This is
whole mess is just what's in the expectation operator because that's what we're
going to now just pull this whole thing Gamma on. I know this depends on i because it depends on
the I_Fs that return. That's Gamma_i; there it is and then let's call
the Deltas_naught, Beta_naught, Theta_naught, Rho_naught, and F_naught
true parameter values that uniquely solve the conditional
moment restrictions. These are the Euler equations. With Delta_naught and F_naught, then we know that these Euler equations
are exactly satisfied, so just use the naught to
denote true parameter values. Let's let W be some observable
subset of F. Again, this might be something the
econometrician observes. Again, that's just
as before the GMM. Very important distinction here is we're going to be working
with conditional moments, we're going to have
to approximate this conditional mean. We're not going to
able to turn this into an unconditional
moment restriction and I'll explain
why in a moment. That would be what you could
do if you want to do GMM, but here we can't do that. We're going to stick with
using conditional moments and let's let W be some subset of F. We have
these moment restrictions. What's the intuition
behind this is a C, minimum
distance procedures. There's a lot of pieces
to this going on, but first let's just take
the minimum distance apart because this is slightly different from GMM although
I will show you that this whole thing
can be implemented as an instance of GMM. Theory implies
that that's right, that these conditional
moments are 0. They must be zero for
every observation. M_t; let's call this
conditional moment, this conditional mean
zero. Let's call this M_t. Since M_t has to be zero, M_t has to have a zero variance, and have a zero mean,
and all the moments obviously have to be zero. We in principle find parameters. Here those parameters
are the Alpha and the Delta by minimizing the variance
or the quadratic means. We can think about minimizing
this and find parameters. Of course, we want to find
parameters to make this zero. We don't want to find parameters
to make this as small as it can be and then we'll find parameters that
make this negative, so we need to square it
somehow. Something like that. We could find parameters by
minimizing the variance. Of course, we don't
observe m here. That's one issue. We're going to have
to estimate m. We're going to have to estimate
this conditional mean, but suppose we have an
estimate plug in M-hat. Since this is a
conditional mean, it has to hold for
each observation t. It has hold for
every observation. Well, how do we deal with
all these observations? But usually, we have
more observations and parameters to be estimated. We need a way to weight each of these observations and using the sample mean is
one way to do that. What you could think
about doing is minimizing this
means sample mean, minimizing the sample
quadratic norm here. I was thinking about
choosing parameters to minimize a sample
quadratic norm. This is a minimum
distance procedure and it's useful for estimating distribution or for
distribution free estimation involving conditional moments, this conditional means here. Let's contrast that with GMM. GMM is used for
unconditional moments, so at the end of the day, we have got some f of x, Alpha. Alpha is for some
parameters and we're taking an unconditional
expectation here. We saw that in the Hansen
and Singleton application we started with a
conditional expectation, return that into an unconditional
moment restriction. That was important. You can incorporate
conditioning information in that methodology by
including instruments, but at the end of the
day your estimation is done on an
unconditional moment. With GMM you have this
unconditional moment, you take the sample
counterpart which is the sample mean; call that G, then you choose parameters by minimizing this quadratic form. With GMM, it's as if you
average and then square. Here with minimum
distance estimation, we square and then we
average. Slightly different. This has a few moving parts, so try to just bear
with me a little bit. We have these true parameters
we'd like to estimate. We have a set of finite dimensional
parameters with this unknown function.
Again, what does it say? This is V over C. Again, this is this minimum
distance problem. This is just this quadratic; what we were talking about
here, this quadratic norm. But now I've just stacked
everything into vectors. There's whole vector of Gammas. We have one Gamma
for each return, so instead of just putting an I here I stack these
into an entire vector, 1, 2, n. Those entities. Again, this Gamma is just what's in the expectation operator. Let's say for any candidate
Delta, Beta, Theta, Rho included in some compact se. Forget about this
V-star, that's a typo. We don't need V-star, F-star. F-star is a function of Z_t and Delta which will also just get rid of that C
have a generic argument. Let's define F-star this way, it is the organ of
this quadratic norm. F is itself the version of a compact set
for functions here. This would be the argument,
let's just define it that way for any Delta. It's clear that the true Delta parameter or that
this true Delta, then F_naught is the same as F-star where these coincide. Because again, the true
parameter value should set this thing to zero actually. The first step this is going
to be a two-step procedure. The first step for
any candidate Delta; let's estimate this F-star; an estimate of F-star, and that's obtained using a
C minimum distance procedure that itself consists
of two parts. The first part is
we're going to replace the conditional expectation with a consistent
non-parametric estimator. What do I mean by the
conditional expectation? Again, talking about the conditional expectation in the Euler equation we'll replace that thing with a consistent
non-parametric estimator which I'll give you an
example of in a minute. Second then we will approximate the unknown function F by a sequence of finite
dimensional unknown parameters and these are called seeds. They're K_t dimensional
parameters. The approximation
error will decrease as the dimension increases
with the sample size t. We've taken this unknown
F, we're just plugging in a known function but
it has to become increasingly complex as
the sample size grows. Then in the second step we will obtain estimates
of the true Delta; the finite dimensional
parameters, by solving a sample
minimum distance problem such as GMM. I'm going to explain why we use these two steps, but let's go back
here for one second. First, let's talk
about the second part of the first step, which is that we now are
going to approximate the unknown function F by a sequence of finite
dimensional parameters. Here's b over c. It's
a function here. We've already said it's a
function of these things, but it's also a
function of Delta We're going to approximate
this with a bivariate C, we have two variables, this is a bivariate C. We're going to say that
F is approximately, let's write this as F_K_T
of Delta, this thing. What does this mean? We have some sieve coefficients that depend on Delta. Here they are. Then we have some basis
functions that have known functional forms independent of Delta,
here they are. These can be things
like B splines. In fact, that's what we
use in our application. In principle, it
could be any number of things and some things
work better than others. Could be polynomials. For this particular
thing, the polynomials apparently don't work so well. They're not shaped preserving. You could get into an
entire literature on that. But here, we use
cubic B splines. You can see that
this function can become arbitrarily general,
the larger we have K_t. The more parameters we
allow this thing to have. You can see what
this is. This is V_t over C_t is a function
of lag V_t over C_t. To get this recursive
things started, we need an initial
value for V over C. What we're going to do
is just estimate that. Note that V naught or V_0 over c_0, we
can estimate that. We're going to take
that as an unknown scalar parameter
to be estimated. That's just another parameter to be estimated in this model. We're going to be
estimating a bunch of parameters involved
in just to C the one. That's going to be one of them. The other one's going
to be all these A's. Bunch of A's. If I give you a V_0
over V_0 to start, you're going to
estimate, these are the parameters we are
going to be estimating. These two. Now these are
known basis functions. But given this, this, and this no basis function, if we have data on consumption, we can use the data
on consumption. We can use F_K_T to generate a sequence of observable
V over C. Now is that we have data on V over C. As you start out
with some initial V, V_naught, V_0 over 0, you put that in here and put in your initial
consumption observation. You pick some initial
values for a, you have a known basis
function over that. You now have next
period's V over C. You put that back in
continue like that. Because now we've generated an actual time series
on the V over Cs. Just recall that m is our
conditional moment restriction itself and we defined it this
way. That has to equal 0. Now the first step, C minimum is just an estimate
of f hat is therefore going to be based on the sample analog to the population minimum
distance problem. Here, I've substituted
the sample mean. Now we're just taking the
arg min picking. Go back. This F_K_T, this is now a parametric
function in practice. We've turned this
unknown function into just another
parametric function. It's a complicated one and if you have
enough observations, it should be highly dimensional
in order to capture potentially highly
non-linear aspects of this function
that we don't know, we're trying to figure
out what it looks like. But now we have a
parametric function. We're going to choose
the parameters, those Alphas and
that initial value in order to minimize
this quadratic norm. Now, I've put hats
here because we need an estimate of the M and I haven't told you how
we're doing that yet, but soon we have that estimate. This is going to be a
nonparametric estimator of M. Not to confuse, there is two non-parametric
parts to this. One is the unknown f function. Then the other is we don't know that the conditional mean, we have to come up with a nonparametric
estimator of that. We have that, then
we can compute this. We're going to do this
arg min minimization for a three-dimensional
grid of values, Beta, Theta, and Rho. Whole bunch of different
Beta, Thetas, and Rhos combinations. We'll do that. This is estimate of f star. Remember f star is just, we choose the f to minimize
this for any value of Delta. Here's an example of a
nonparametric estimator of M. We needed an m hat. We're going from M to M hat. Let's call these
J basis functions of known conditioning variables. These are functions mapping
and dimension of W into r, let's call those instruments. Suppose I have a
single instrument. Let's call it x. These are
functions of that instrument. In practice here, you
can use almost anything. But just for example, polynomials you might use x, x squared, x cubed,
x^4, and so on. If you have x and y, you
could use x times y, x and y alone, x
squared and y squared, x times y square root
of cube and so on. You can use polynomials. These are basis functions have known conditioning
variables. Let's just stack all those
functions into a vector here. Then we can define this
big, huge P matrix, which is take all
those functions and stack them for
every time period,. This is now T by J. Now we're just stacking known basis functions of some conditioning variables. We're calling those known
basis functions instruments. Then this is a sieve least squares estimator of
M. What does this mean? This procedure is equivalent to regressing each
of the Gammas. Gamma is what's inside the conditional
expectation operator. It's all that stuff. It's the stochastic
discount factor times the gross return
minus 1. All that stuff. You take that Gamma, regress it on the
instruments, and take the fitted values as an estimate of the conditional mean. That's what this
is equivalent to. Be a sieve of least squares estimator and the
conditional mean. Now, just like before, as you let J_T go
to the infinity, you're getting an arbitrarily good estimate of the
conditional mean. That's what a theory
would suggest. That's what this just says. Take these Gammas,
regress them on basis functions of known
conditioning variables, take the fitted value now as your estimate of
the conditional mean. Now, we have m hat essentially. Now, an attractive feature of this estimator of the f star, we haven't gotten into
the second stage yet. But attractive feature of
this estimator is that it can be implemented as
an instance of GMM. Y_t is all the
observations that we have. Stack them all here. These are the conditioning
variables that were forming basis functions over. These are all like consumption
and returns and so on. G_t, these are our sample moments that
we would do GMM on. Notice that this is exactly like what you would do in GMM. You have some instruments
and you're concurring that with each of the arguments
of each of the expectation. Was that we had with
Hansen-Singleton, the only difference is now
you have basis functions of conditioning variables
as instruments. Don't just throw
the W in by itself. That's all the same.
The difference is in this weighting matrix here, you have to use a specific
weighting matrix. Now, what is this? This is just the identity
chronically by p prime p. This is matrix of instruments, basis functions of known
conditioning variables. Now, this weighting
gives greater weight to the moments that
are more highly correlated with the instruments. Why is that important? It's important because you
understood intuitively by noting that variation in
the conditional mean is what identifies the
unknown f function. You have no variation of
the conditional mean, you can identify the function. This is also why we have to work with conditional means here. We can't just go to GMM route because we're not
going to be able to identify the function. The f function is in here. If we think about it, if you could invert
the moment condition to give back to the f function, need not be able to
identify the f function. Of course, the moment
it has to move around so you can have
an unconditional mean. That is why this weighting gives greater weight
to moments that are more correlated with
the instruments. Want to get a lot
of variation of the conditional mean in
order to identify this f. Now, if the model is literally
true and the true parameters, then we should have these moment conditions
are exactly satisfied. These are now step back. Now we have our f star, we've got an estimate of f star, which we're
calling f hat. But given this, if
the model is true, we can always take any
subset of information, put it here and take an
expectation that should be 0. Now, we're coming into talking
about the second step, estimation of this Delta naught. We could then form sample moments of this thing. Here's some other facts here. I'm sorry if it's confusing, there's two types
of conditioning. The conditioning we're
talking about here is Ps. That's to estimate m hat. We need m hat in order to
estimate m, the first step. Now we're going on to talk
about estimating Delta. Now we don't need
the Ps anymore. This is an unconditional
moment restriction. This also has to be true
if the model is true. Now we can work with
unconditional moment restrictions because suppose we
know what F is, we don't have this problem of estimating an unknown function. Well, sample moments would
just be given by this. This is again just like a GMM sample moment with instruments. Except, for now, we're
going to plug in our estimate f hat for
a given Delta here. You plug that in
there. I put that in red to emphasize that. Now, regardless of
whether the model is correctly specified or not, in this procedure, we're
going to estimate Delta by minimizing the GMM function. What do I mean by correctly
specified or not? Suppose that you can't make
the Euler equations zero, because the model is
misspecified. That's okay. Then we're going
to be estimating so-called pseudo true parameters by making the
earlier equation as small as possible and choosing the Delta to be as small as possible even if it's non-zero. We want to allow for that because it gets
back to this theme of all of these models, in fact, our misspecified, we shouldn't expect them
to hold exactly correctly. The question is, which
are least misspecified? That's the interesting question. Regardless of whether
these Euler equations are identically equal
to zero or not, we're going to still
estimate Delta by minimizing a GMM
objective function here. This is our g, we plug in our f hat. We've estimated f hat
in the first stage, so we now proceed with GMM. F hat is no longer unknown, so we could just do GMM. Examples for the
weighting matrix could be anything like identity or you get the inverse of the second-moment matrix or returns as Hansen-Jagannathan
recommend. Again, we're not going to
get any efficiency here, but these would be reasonable
examples of matrices. I'll talk about why they choose some of
these in a second, well, especially this one. Notice here though that f hat is not held fixed in the staff, it varies with Delta. Every time we change Delta, we go to a new f hat. That's because we already solved f hat for a whole
grid of Deltas. We can just change
it. We already know. We have many, many f
hats depending on Delta. This estimator f hat
is obtained using the minimum distance
over a grid of values for Delta. That's
what I just said. Let's choose Delta
and then, of course, the corresponding f hat to
minimize this GMM criterion. Now, we have an estimate
of Delta and f hat. Of course, once we
have Delta and f hat, we have an estimate of
this discount factor. Now, you might ask
yourself, well, why should we be estimating
this in two steps? After all, we could estimate all the parameters in one step by minimizing
the SMD criterion. Let's go way back here and
look at this thing here. Instead of just
do arg min F_K_T, where meaning pick
all those A's here. We can also pick the
Beta, Theta, Rho here. You got a one-step. Well, so we would argue this for asset pricing and this
can be less desirable, at least for the
applications that people are often interested in. Why? Because, for example, we might want estimates of the coefficient
relative risk aversion and the elasticity of intertemporal
substitution to reflect values that required to match unconditional
risk premium, at the equity risk
premium for example. This wouldn't be
possible if we use the SMD criterion to choose Beta, Theta, Rho because that criterion emphasizes
conditional moments. Think back to the beginning of the lecture
where we said that Hansen-Singleton didn't have
an equity premium puzzle. They didn't need to estimate
a hard Gamma because in fact, they were no longer
looking at the equity premium. They're looking
at equity returns times some conditioning
variables, and so those became totally
different set of returns. Suppose you want to take
this model and say, well, how well does it explain a
particular set of returns? In finance, people
are very concerned, for example, with
the value premium. You take portfolio returns, sort them on the basis of
book-to-market ratios. It turns out that
portfolios with high book-to-market ratios have really high average returns, and those with low
book-to-market ratios have really low average returns. The CAPM and many models
can't explain that spread. You might like to
know, well, does this model explain that spread, if it does, how high does risk aversion have
to be, and so on. But you would not
be able to answer that question if you
would estimate Beta, Theta, Rho using
the sieve minimum distance criterion
because you no longer be looking at those
portfolio returns. It's the same thing
that happened to Hansen-Singleton
when they were no longer looking at
the equity premium. Say estimate a low Gamma. I suppose this just
gets to one of the places where
judgment and purpose about what you're
doing really comes to play because in finite samples, especially if results
depend on the moments you choose to match to estimate the parameters
or something. The other thing is
that the sieve minimum distance procedure, and
this is what I just said. This effectively change
as a set of test assets. Now you're back using
linear combinations of the original portfolio
returns when you pick parameters to minimize that sieve movement
distance criterion. We may be interested in explaining those
original returns, so we're undoing all that. Again, as I said before, these linear
combinations may imply implausible long
and short positions and the original test assets. They may not give you
a large spread and average mean returns
across assets. Again, there's not much
for the model to estimate. You have to estimate the
unknown function with the sieve amendment
distance criterion because you can't identify it otherwise. That has to be done. But here for these other parameters,
you don't have to. This procedure allows for
misspecification since the Euler equation doesn't
need to hold with equality. I'm just going to estimate, in that case, pseudo
true parameters, whatever parameters make the Euler equation as
small as possible, their error as
small as possible. As before, just
want to emphasize, you could then compare models, especially this Epstein's in modeling might have a
couple of other models is one of the things we did by the relative magnitude
of misspecification, rather than asking whether
each model individually fits perfectly given
sampling error. To do that, you would
just use W equal to inverse in the
second stage, GMM, you compute the HJ distances for a bunch of models for this one and a bunch of miles and you can
compare the model. That's another reason to do this two-step procedure
because you have a way of assessing
model misspecification, comparing it to misspecification
of other models. Again, you can just test
whether these HJ distances of competing models are
statistically different. This is different from testing
whether the HJ distance itself is different from zero. We would say don't even
bother testing that. It shouldn't be exactly zero, because I don't really
believe this model is complete description
of the data, but maybe comparing the
Epstein-Zin model with the standard constant relative risk aversion case
where you restrict the coefficient of
relative risk aversion to be the inverse of the EIS, maybe it does better than that and that's useful to know. Last thing I want to talk about in terms of estimating
these models. Again, the goal here is let's try to be as
general as possible. If we can, let's not use estimation techniques that rely on assumptions like the EIS is approximately one or everything is jointly log normal because we might not
believe those things, those restrictions are true. This is another
methodology that if theory provides more guidance or restricts the Law of Motion for some of the underlying variables
then you can use simulation methods to get around the fact that
this continuation value is observed. This is an example of
that. This is published. Bonds on [inaudible]
are estimating as a simulated method of
moments estimator of this long-run risk model
of bonds all in your room. This is restricting
the specific law of motion for cash flows, so-called long-run
risk law of motion. There's also going
to be some such caustic volatility in here too. This is the Bansal, Gallant, and Tauchen conversion of
the long-run risk model. Now, this is a slightly
different from the original Bansal
& Yaron model. I'll explain where it differs. But here, we have lowercase letters are logged variables so we have
consumption growth equals some constant mean
times the component x t lagged plus a shock that has a time-varying
standard deviation. Now, this x t or xct
is an AR1 process. In the Bansal-Yaron model, nothing is estimated, they pick some parameters to match
certain things with the data. Then they solve the model
and see how well it does just by matching things. By matching moments and
no statistical comparison in model moments and
actual moments which is presenting modes
this calibration. They calibrate this
sphere to be very high, so that you have this
persistent component. Now, this shock here might
have a small variance, you can control that here. I wonder if this should
have a T on it actually. I'm drawing a bit of a blank, but this might have to have a t, I might have forgotten
to put the t, we'll have to check then. But in any case, this thing could have a
small variance on average, and then they indeed they
calibrated it like that. They wanted this Sigma
Epsilon x is really small. There was this
idea that you have this small but very persistent component
in consumption growth. Why does it have to be small? Because if you actually look at consumption growth, I mean, it doesn't seem that
it's that persistent. There are some serial
correlation in it, but it doesn't seem like
it's that persistent. You don't want this to be a huge part of the variability, and consumption, so you're
going to start off small, but if it's very persistent, it can have important
asset pricing effects in models with these recursive preferences for the timing of the resolution of uncertainty really matters. You have cashflows that have small variance over
short horizons, but the variance grows with
the horizon over which this measured the
annualized variance. This could make a
huge difference in a model with Epstein's
and preferences, even though it doesn't make
any difference in a model, that's a special case of
that where Rho is equal to Gamma inverse, Theta inverse. That's how they calibrated it, and this is what we
call long run risks. Actually, in this model, dividends and consumption
are two different things, because they don't want to make the equity return via claim
to future consumption. So dividends are a
levered up version of future consumption. This part of the
consumption risks though, gets into the dividends. So they would just specify a dividend process
it looks like this. This is very similar, this is a "leverage parameter" where they expect
to be greater than one. Whereas dividends are a lot more volatile and consumption. Whereas here we're just going to estimate all these parameters, we're not going to
calibrate anything. Now, what is this S? In this particular
version of the model, S is a cointegrated residual for log dividends
and log assumption, and notice that the
cointegrating coefficient here has been imposed
to be one negative one. Because S is a
stationary variable, but it's D and C
are non-stationary. The linear combination of log D minus log C is stationary, so S is a stationary variable. This is a way to write
a cointegrated system, in which on a restriction has been imposed that the lagged. This is like a vector error
correction representation where the restriction has
been imposed that the lagged cointegrating
residual does not affect one of the two
variables in the system. Of course, it has to
affect at least one of the variables in the system or otherwise variables
aren't cointegrated, but here it's not affecting consumption growth,
just dividend growth. That's another
restriction that they are imposing, they're
just imposing this. This was not actually part of Bansal Yaron's original model. They actually didn't
have any cointegration between consumption,
and dividends. Over here they impose that. I was just talking to Wayne. First one, he's doing
some estimation of these models under some
simplifying restrictions but also imposing or not
imposing cointegration, and finding that imposing cointegration allows the
models to fit and be negative. In practice, I guess I
didn't find it's ever negative given their
perimeter estimates. But in general, that
could be a concern, and there might be various ways to try to deal with that. In their sample,
they find it never goes negative
[inaudible] estimates. Then the other thing that's imposed here is that
all of these shocks 1, 2, 3, 4 are normal
IID variables. Here, we're putting a lot of structure on the Law of
Motion for the data, and that's different from
the previous methodology where we left those
unrestricted, but here the model is implying
this kind of structure. Well, in particular the
long-run risk piece that's very important
to the structure. This is a simulated method
of moments procedure, and the important background for this work is
go out and talk, and this is classic paper
about which moments to match. Gallant C and Tauchen
a good survey papers as well in the survey paper, because these are
very useful to me in understanding this approach. Here's an outline
of the steps here. Combine this cashflow
specification, this whole thing with the Epstein's in recursive utility, so that's always still there. Exactly, easy W stuff
that we talked about, it's the same for this model. Beta, Theta, Rho are still the structural parameters
of that utility function. You're just now superimposing on that this law of
motion for the data. You solve that model
independent of the data. You solve the model,
you have some way of solving the various
ways you can solve it. If you're going to do
long simulations of the data as we will be doing
here in this procedure, you could solve it using
the long simulations. This model is close
to log linear, so you can actually take log linear approximations
of everything and do an approximate
analytical solution, that works very well too. But in any case, you'd have
a way to solve the model. Now that you solve
the model over a grid of values of
these deep parameters, let's call this deep
parameters Rho d. This is the Beta,
Theta, Rho from the Epstein's in utility function, and all of these other
parameters from here. All of these guys, the Mu's and the Phi's an the Epsilon. Notice that in the model, of course, the V, the continuation value of future consumption plan is
completely observed, you can solve the model,
you can see what that is. It's the fact that in the data we don't know what that is, but within the model,
we know what that is. There's no question of
not being observed, so we solve it for some
grid of parameters, and there's quite a
few parameters here. Then for each value of Rho on this
multi-dimensional grid, let's combine the solutions
of the model with a long simulation of the
model of length n. Now, given modern
computing technology, you can compute very
long simulations, which is good because
to get consistency, you really need long big
N, simulate for what? A long simulation just once for each Rho d that you choose. This means take Monte
Carlo draws from the normal distribution
for these shocks, and compute the simulated
implicit values of the endogenous variables, and then you're going to write, so that's what you're
going to do here. You're going to
form what's called an observation equation for the simulated and the
historical data. Here in this Bansal,
Gallant, and Tauchen, they used an observation
equation that consisted of there's some judgment
as to what to put in here. They chose variables
as they felt were useful for summarizing the main asset pricing implications
of the model. These would be things like the dividend consumption ratio, which is presumed
to be stationary. Consumption grows itself, the price dividend ratio, and RD is the log return on the claim to dividends
or an equity return. But I mean, in
principle, you could choose anything here, but it's going to be a set of observations for the
simulated an historical data. Then you're going to choose which row most closely matches moments between
the distribution of the simulated data
for these variables, and historical data
for these variables, and by match, I'll make that
precise in just a minute. That's the basic idea. Here's how it works, you have y hat. Remember that we
have simulations of length n from the model. Let that be a realized
simulation from the model. Of course as well
of we condition on whatever Rho d you chose. You have a bunch of these
grid that these y-hat. Then let's let y Tilda be the historical data on
those various variables. Now, the Tildas mean
historical data and hats means simulated data. You have a T here,
this is the length of our sample on historical data. Then we're going to define
an auxiliary model of the historical data for that y. For example, could be a VAR,
that'd be a simple one. In fact, I think that's
what they ended up using, although they started
with some other non-linear variance,
not sure what. But I understand from
George at this they experimented with
other variants. But suppose it's a
VAR for example, it'll have some density, whatever it is, it's
going to have a density. The data will have a density and it'll depend on
some parameters here. This is a VAR and this
density, these parameters, Alpha are going to be
the coefficients on lagged endogenous variables and the elements of the
error covariance matrix. These will be VAR perimeters. I think that is a
VAR for simplicity. In any case, it should be a good law of motion
for the data. They say you should be doing specification test at this
point based on something, I base information
criterion to choose this. If a VAR works, or if you're restricting to VAR linear models is the specification test on the
number of lags and so on. You can think of
generalizations of VAR that a non-linear, you should include
those as well. But this is what we call
an auxiliary model. It should be a good law
of motion for the data, let's call this an f model. They call it an f
model for sure. Now there's a score function for the f model because again, this has a density. The score function is
just the derivative of the log of this density. Let's call this just to
find s of f this way. Now, what is this
density in fact? Well, in practice, they
use a normal density. In principle, you don't
restrict yourself to normal and you can also try
departures from normality. One way to do that is to use Hermite polynomial expansions
around the normal. This involves more
parameters of course. But you could ask whether
the methodology work better using non-normal
distributions here. In the end, they ended up
just using the normal. Let's think about that
as the normal density. This is the score
function for the f model. Here we're just talking
about the historical data. The quasi-MLE estimator
of the auxiliary model and historical data is just
what we always do for MLE. Just pick Alpha Tilda as the argmax of the
log-likelihood function. What does it says, the
log-likelihood function again, this is just historical data. This the VAR, and
this is, VAR lagged, VAR coefficients, and the elements of the
error covariance matrix. Picking them to maximize
the log-likelihood function for the historical data. The first-order condition here for this MLE optimization, you said the derivative of
the log-likelihood to zero, which is the same as saying that the average of the
scores should be zero which is the MLE
first-order condition. This cap t means the in-sample,
the sample variance. We have a sample mean here. You couldn't have l lags, that's why I'm going
from l plus 1 to t. Now, here's the idea. Since this is true, so this since in the
regular historical data, this first-order
condition should hold, which means the average of
the scores should be zero. Then a grid estimator for Rho d, which is the
structural parameters of the underlying model, would be one that sets
this thing to zero. Now notice what's happened here. Instead of t, we have n. We're now taking averages
over one long simulation instead of ever what we're
doing is we're using this score function from
the density of the data. We're using the
parameter estimates of the density for the data, but we're plugging in the
simulated data from the model. Of course, that depends on Rho. Because for each Rho, you have different simulated
what set of y's. You could see how
you could choose Rho to make this
approximately zero. That's the recommendation,
that's the instruction. Choose Rho to set this to
be approximately zero. Now, this thing has
dimensionality equal to Alpha. If Alpha were exactly equal to the
dimensionality of Rho, again, it was same number
of equations as unknowns. You could just solve each one, assuming there is a solution. Because again, we
have non-linear equations here, these scores. But if these are a set of moment
restrictions, there's a GMM. Again, this is just a set of unconditional means that
we want to set to zero. There's Alpha of them
dimension Alpha. If Alpha, the number of moment restrictions is greater than the dimensionality of Rho, you can use GMM. Let m hat be this thing. It's dimension Alpha by 1. The GMM estimator would be this. Typically, you have
a rich f model or auxiliary model and you're
going to have therefore, more moment conditions and
parameters to be estimated. These would be your sample
model restrictions, and the GMM estimator
is just this. Pick Rho d to
minimize this thing. There's this quadratic form. Ultimately, this is a GMM
estimator of Rho d. Now, what is this weighting
matrix here? Here, I just repeat
that from above. This thing is the inverse of
the variance of the scores. It's an information matrix. But it's dated determined from the f model. This
is what this is. It's all data determined
from the f model, historical data estimates,
parameter estimates. Is what that is. Now, the simulated data, of course, will follow its own density, let's call that p, p of the joint density
of all these lags. These are the observation
equation that depends on Rho. Note that there's no
closed form for that. We don't know what that is. Because these are functions of the endogenous variables
have a non-linear model. It turns out we don't need to
know that, thank goodness. The intuition for
this estimator is that recall that we
have this m hat, which is the sample
moment restriction that we're trying to
set to zero and GMM, we're choosing these parameters. That converges almost surely
to m as n goes to infinity. This is why it's important to have large simulation length as n goes to infinity
that converges to this. Now, what is this, this just says we're using the Monte Carlo simulations to compute expectations
of the scores under the p distribution. That's what this is, the
expectation of s under p. That's what I'm
repeating here, expectation of s
and under p using Monte Carlo to compute
the expectation of anything we want under
p is a general result. Now, if f really
weren't equal to p, if the data-generating
density were the same as that for historical data were the same as that
for simulated data, then what this would be is the mean of the scores
in the likelihood. That of course should be zero, given the first-order
condition for MLE. That is exactly what we're
trying to set to zero. That's why this makes sense. If the data actually do
follow the structural model, p then this thing
should be zero, and this also can form the basis of a specification test. This gives you the logic behind these particular moments. This is a GMM estimation. The moments are the
score functions. Here they are. The moments are the score functions which depend on Rho score
functions of simulated data where the score is coming from the historical data and is dependent on the parameters of the historical data estimates. They'd also talk about how you can do a specification
test based on this. Again, now, keep in mind that
the specification test is like the test of Hansen's
OID restrictions. It's asking, is this
model literally true? Are these things exactly zero? These moments? That's
what the test is. Whereas I've been arguing that maybe the more interesting thing is not whether these
things are on their own zero or whether they're less far from zero than
some other model that we might be interested in. Nevertheless, many people
want to compute these tests. Summary here you're
going to solve the model many times
for many values of Rho. Each time you do that,
you're going to store a long simulations of the model for these observation equations and variables. Store those. Then you do a
one-time estimation of the auxiliary
model, just once. Then you're going
to choose Rho_d to minimize that GMM criteria, all of which can now
be computed from your single estimation
on historical data. Using all of the various
simulated variants of your model based on
different values of this. It's a messy problem
because you have a lot of variables
here in Rho_d. There's a lot of
things to store, but technology marches on and we can do this more and more
as we go through time. Advantages of using these
score functions as moments. The question is what moments
would you be matching here? Recommendation here
is match the scores. Why is that a recommendation or why is that advantageous? One is simply computational. You just have to do this
one-time estimation of the structural model. Now, this is very useful if f is a non-linear model here
we've been thinking about a VaR and in fact
in their application, that's what they ended up using. In that case, there's not much computational
advantage because the VaR can be estimated
many times very quickly. But if the non-linear model and you're searching
over parameters, this is a huge
computational advantage. You think of one of the contemporaneous papers in this literature that
was dealing with simulated method of moments
is a paper by Tony Smith. In that particular application, you had to estimate the structural model every time you change the
parameters of Rho_d. It was a different procedure
where you're actually matching on the VaR coefficients of the structural model
and the historical data. Let's see if I can
remember how this works. Here, what are we doing? We're taking and fixing the parameters of
the auxiliary model at their estimated
historical value. Then we just keep plugging in different values of Y
hat as Rho changes. What that does is to say, no, every time you
change Rho hat, it won't be the score function, but it will be we will recompute the density of the historical data based on different values of Rho hat. That's probably not very clear. In any case, in Tony
Smith's method, you have to keep re-estimating the auxiliary model every time. Every time you change
the parameters Rho. That could be computationally
very burdensome. This is a huge advantage if you have a non-linear
auxiliary model. If the F-model is a good description of the
data as we want it to be. Then at least under the null, the MLE, you didn't
have MLE efficiency. That's another reason to use
these particular moments. Now if the
dimensionality of Alpha, if the number of
moment conditions is greater than the parameters
that you're estimating, then these simulated method of moments are consistent
and asymptotically normal. Of course, assuming is the
big assumption that we almost always make that
the auxiliary model here, in this case, it's the auxiliary
model is rich enough to identify the non-linear
structural model. Now sufficient conditions for identification are unknown. This is just accounting rule. This is a order condition
where there's, of course, a rank condition implied
in the background and sufficient conditions for those to be satisfied, I
don't think are known. Well, I asked George Tapping wasn't clear on what they were. It's always a problem
in nonlinear models, not necessarily specific
to this particular case. Essentially, we have to assume
identification in order to get these asymptotic results. Consistency and
asymptotic normality. Now, of course, if you're
doing this in practice, the sign of not having an auxiliary model that's not rich
enough to identify the data would be very
large standard errors and parameters, you'd
have some hint. You have trouble identifying
these parameters. You can expect very
large standard errors. The big issue here is, are these the economically
interesting moments? A lot of arguing goes around on among the authors of these papers and other
people in asset pricing. Sure, we're choosing these
moments, these scores, and then we're weighing
them in a particular way for statistical efficiency. But as we do that, again,
as we've talked about, we're undoing the
carefully chosen set of characteristics
that we originally chose these tests
assets to match. This regards both the choice of moments and the
weighting function. Again, as we've spoken about. Using this way, or maybe you're not asking them on and whether it can explain
the value premium, for example, and a
competition might say, "Well, maybe that's the
wrong question to be asking because you
are just hiding this specification
in other ways." We need to give more weight to the statistically most
informative set of moments. This may not coincide with the economically most
interesting ones. That's a big unresolved issue. There's nothing that we can ever resolve that actually other than the discussion goes on. Here are my concluding
thoughts about this. There's been a small
and maybe the smallest, slightly growing
body of work that's linked financial markets
to macro-economic risks. These will be given
by the primitives and the intertemporal
marginal rate of substitution over some
numeric consumption. Of course, you could
have other elements of the utility function, not just consumption, but we
saw housing as an example. But there's always a
numerator consumption good, and that's going to be in there along with potentially
other arguments like habits or
housing or something. No model that relates returns to other asset
returns can actually explain asset prices in terms of primitive
economic shocks. These models are only descriptive even though
they fit very well. That motivates us to study these consumption-based models. So far, many of these
consumption-based models have been evaluated using
calibration exercises. You talked a little
bit about the bonds, all your own example of that. Well, of course, the
crucial next step, and that's a great
place to start, but a crucial next step in
evaluating these models is structural econometric
estimation of the model. Try to cover a couple
of ways that we've tried to get started with that in the most general
circumstances, but a lot more work can
be done in this area. However, these models
are imperfect. They're never going to
fit the data infallibly. I have argued here for
a need to move away from testing if models are true, literally because
they're not towards a comparison of models
that are based on the magnitude and
misspecification. I've tried to suggest
one way in which you can actually do
that statistically. An example is this scale
consumption model. Rather than ask whether the
scale models are literally true, we could ask, for example, whether allowing for
state dependency on consumption growth in the stochastic discount
factor reduces misspecification over the analogous non-state
dependent model, which would be the
classic consumption CAPM. Macro data, are especially
troublesome in some cases because unlike
financial data measure with potentially a lot of error. We can't really
expect these models to perform as well as financial factor models of the stochastic discount factor. One way to think about this, the true systematic risk factors are macroeconomic in nature. If you buy that asset prices are derived endogenously
from those, then the financial factors that work really well
in explaining or rather in describing
returns statistically. These could represent the
projection of the true M, which is a function
of macro variables or consumption variables, say, on the portfolios. These would be what we would
call mimicking portfolios. But if that's the
case and they will always perform at
least as well or better than if they're better measure the
financial factors, then these mismeasured macro
factors from the true M. Then it doesn't seem
sensible to run horse races between
financial factor models in macro models. The goal here is not to find better factors than the
financial factor model, but actually to try to explain the financial factors from
deeper economic models. 