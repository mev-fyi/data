and um welcome back from from the short 10 minute break um it's a pleasure to to be here and to continue with uh Russia's presentation where she she left off so together we've been doing quite a bit of work on regression discontinuity signs for the last almost a decade now believe it or not and some avoid presenting today is is a summary of some of the main results that we we came up with along with some other things that we learned from other Scholars along the way and they're all summarized in this set of slides so Rocio gave a gentle introduction to to different Rd designs and Frameworks and then presented quickly the idea of Rd plots and I think that is Never Enough emphasis in saying that those devices are extremely useful for this visualization that they can be very damaging if you try to learn something from them alone uh putting myself for a minute and an econometrician hat I should say that it's always very important to do principle estimation and inference their deep Bloods are great uh to visualize are the designs they can actually be misleading in many cases and so I'll touch base on that quite a bit uh for most of my presentation today I'll try to show you how sometimes curvature of the underlying regression functions or variability of the data itself uh could could lead to misleading conclusions empirical conclusions in otherwise very clean or or good looking Rd designs great and then she also uh covered the idea of local randomized experiments or local randomization in the context of RDA designs and I want a touch base on that any longer I'll just leave a couple of ideas from that when I get to to my part and so my part is about the other approach that I think is also quite popular and these two approaches in many ways are combined and originally if we were to do a history lesson which we want um you will recognize that these two approaches are coming together and splitting at some point from a common approach and the common approach was all the way back because it could it could be found all the way back to the beginning when when people were thinking about regression discontinuity science and conceptually they were thinking of them as local randomized experiments or Global even randomized experiments in some sense um and then however they would use arguably very parametric uh approaches to estimate those effects and eventually do inference for on on those parameters um and so these two branches of of thinking about regression discontinuity science I think both come as a response uh to those early approaches in the literature uh the local randomization approach takes very seriously this idea of local as if local as if an experiment locally and then tries to deploy ideas from analysis of experiments and then on the other hand the local polynomial methods are come as a response to to the global approaches that you would find early on in the literature and I'll show you some examples of that as we move forward so what is the key idea in in the local polynomial approach World well it is constructed from an identifying assumption as as it should be I think so you first think about what the parameter of interest is and then how you can actually think of identifying it uh from from the data and in order to do that you need to build some sort of assumption which is most likely untestable and so it has to be uh believed to to hold in the underlying data generating process from which we want to try to draw uh inferences and so uh what is the idea in this context well it's a continuity one so Rd designs uh from a continuity perspective are interested usually in parameters that are defined as a single point on on the space of the score running value or the score variable or the running variable and in many cases the most popular of those parameters is the average effect and so uh the more proper way to define it will be an average treatment effect at the curve and that means essentially that the um the parameter of interest is nothing more than the difference in average responses under treatment control and the treatment status or control status for units that happen to exhibit and score value that is equal to the cutoff point and so in itself this may not even be called a causal parameter by some and we can have a long and hard philosophical discussion of whether or not these can be called a causal parameter that may depend on your views on what council it is about how to define the causal parameters but putting that aside for today the key part of this slide is to argue that something that is unobservable that is based on the underlying potential outcomes of which only one of them we can see for anyone you need depending on whether they they have a scores below the car or for above the kind of uh that difference can be represented as something based on Observer about data and so the key going from the left to the right the second equality is just saying that we can represent the parameter of Interest as a function of functionals of observable characteristics of the underlying data generating process and I mean observable or estimable at least identifiable characteristics of that underline data generating process and so we're going to be able to estimate these quantities and what is a core idea well the core idea is that if we're interested in the vertical jump then we could in principle try to estimate the value of the conditional expectation for the control group by estimating that regression function to the left of the cutoff and then somehow take a limit on Project it down or extrapolate it if you if you wish or or interpolate it depending on on on the values that you have in your data but ultimately generating an estimate of the conditional expectation from the left to the uh kind of point and then you will you will do the same from the right you will try to estimate uh the relation function from the treated for the treated units or the units who have a scores of the kind of and then once you have a good handle on that condition expectation then you will try to project it down or predict the value of that condition expectation I said what is the core identifying assumption that is operating the background is is one of continuity and continuity here means that in the absence of the treatment assignment rule in the access of the abrupt discontinuity in treatment assignment due to the design you would have seen this conditional expectation functions to continue in a smooth or a smooth way or continuous way right so what we're saying here is that in the absence of a treatment assignment due to the design there should be no uh changes or not abrupt changes these continuous changes in those conditional expectation functions and if that was to be true then we could attribute any discontinuity uh coming from the left and coming from the right of those conditions protection functions to the treatment aside for the two or three minute side um and so that's the core identifying assumption that will be operating in this continuity frame so it's very different conceptually from what you would see in local randomized experiments where the identifying assumptions and not only restrict the relationship of potential outcomes with the score but also say something about the assignment mechanism all together okay very good awesome so uh having that in place now we can think about how to actually estimate and conduct inference on on that treatment effect from a continuity or local polynomial perspective and so in the literature I think uh It Is by now quite clear that a global approximations are not a good idea I think Rocio show you this picture which I am I have to show it again because it's just too cool not to show it really uh it basically says that Global feeds um which are actually the default in the Rd plots are certainly not the preferred method to try to infer uh the the jump and the Cardiff right and so the idea is I think quite simple what really happens is that when you try to feed a regression function using a polynomial approximation of a sufficially high order uh typically what happens is that the feed is fairly good in the center of the support it but it turns to exceed and this can be mathematically shown it tends to exceed with a quite a bit of variation on the Tails or in particular the boundaries on the support of the compact support that you're trying to estimate in general that would not be a big issue but it just happened to be the case that in the regression discontinuity science it is the behavior of the function of the cutoff which is from the perspective of the estimation and the boundary is exactly the point that we care the most about and so from that perspective there is a clear trade-off and again in this case from going from Global fitting to local feeding and so in a sense what is science modern ways of analyzing and estimating and conducting influence on our daily designs they will unavoidably search for localization so they will try one way or another to zoom in to name to a neighborhood around the cutoff discard observations away from the cutoff and then try to do inference uh using only those observations which essentially uh I believe it mimics more closely uh the identifying assumptions that we impose in regression discontinuated science we cannot really say much about the distance the vertical distance or any kind of difference uh in general for the population in an artic design but we can say something maybe for those units that are close to the kind of instinct um and so with that in mind uh in local polynomial settings usually what you do is you search for localization and now you have a bunch of options in the way that you can do this and so today what I'm going to try to to talk about is basically focusing on um uh most common approach in the literature because I believe it is closely related to traditional regression based analysis uh in applied microeconomics with a couple of twists that we need to take into account in order to accommodate the specificity of regression discontinuity designs and so the way that we're going to do this is basically um we're going to propose at least to to first and foremost think about localization as a core argument in the analysis so the way we're gonna do this is by first uh choosing a polynomial order um which I'm gonna denote by B uh and that will control the the degree of the polynomial uh that we're going to use to approximate those conditional expectations in the example uh here that b would have been four and this feed could have been Global and then this could have been a very very sensitive feed near the cutoff but perhaps a good approximation of the underlying condition expectations in the middle between 60 and 80. and so in the context of of local polynomial analysis instead what we're going to do is we're going to choose a lower order polynomial feed um and perhaps some waiting scheme that allow us to down weight observations as they are further away from from from the kind of point in the space of the score but regardless of the way it scheme perhaps there's no waiting scheme there is just a traditional at least Square regression for for a polynomial affordable one or two that would be linear or quadratic say then the key aspect of the relation is continuity design is that we're going to force ourselves to localize and so localization in our designs will be controlled by a choice of bad news so in the local randomization a setting um instead of age we use The annotation of little W just to mimic the edges of the window where the local randomization assumption was believed to to give a good approximation to the underlying data generating process in the context of local polynomials we're going to use the letter H that determines again the distance from the garage that defines the region where we are going to conduct estimationally and so the way that the procedure typically works is you choose um the desired order of the polynomial you choose some weighting scheme sometimes it's called a kernel and given these two characteristics then you go ahead and choose a bandwidth that uh purposely discards a lot of observations and zooms into a nearby region of the cutoff point and so there are different ways of choosing this bandwidth I'm going to discuss two of them and their corresponding optimality property so as everything in econometrics and possibly in life there is no one Global Optima so everything is relative to what your beliefs are or in this case in econometrics or statistics we typically Define it in terms of a risk function or the loss function so at the end of the day we have to set the rules of the game you need to Define what we care about and once we care about something we might be able to obtain uh an optimal choice in general there is no optimality that is a uniformly true for everything it's all relative to what you think to be important uh for your problem and so in the context of language selection there are two criterias that I believe are particularly useful to think about how to select a bandwidth one refers to a point destination criteria I.E we're trying to find a bandwidth that give us the best possible guess of what the treatment effect is in a very special sense and the sense here will be means whenever so for those of you who may not remember what that is that is just something about how good a point estimator is has meteorized by the square respective Square difference between the point estimator and the actual Target parameter in our case the vertical jump report or the cutoff point and so that means whatever essentially trades or traded trades of uh bias and varians in a particular way and so a second way of choosing the value is that I'm going to talk about briefly is relating to inference properties so one could say hold on a minute I'm not particularly interested in developing optimal Point estimator for their day treatment effect although arguably that is a major goal in practice but also I may want to ask a question how should I how should I choose the bandwidth if I wanted to have the best possible gaussian or distributional approximation in order to conduct inference and one way to answer that question is by looking at the errors that you make but not in terms of mean Squad error for Point estimation but in terms of discrepancies between the sampling distribution of your statistic typically a t-test of sorts and the its limiting distribution and of course as anything in life very few things are gaussian but many things are called to be close to the ocean and so um we can meet rise or characterize the discrepancy between the sampling variability or the sampling distribution more precisely of the statistic and its Target limiting distribution the normal distribution and so those errors can be characterized and not surprising they will also be a function of the bandwidth and there will be a function of the bandwidth in a different way and so you will trade off those errors in a different way and eventually you will get a different optimal way of choosing the bandwidth and there are many other ways to do okay great so I'll touch you based on that briefly and show you practice how things change when you decide how to choose your man and what are the drawbacks of shift in this as well once you do that you're ready to go because you know the polynomial order you know the waiting scheme that you decided to use and now you can go straight ahead and choose the bandwidth and once you have that neighborhood selected you are ready to construct your point estimator so again the point estimator will estimate the regression from the left the regression from the right and then it will construct that jump and that will be the point estimator that we're going to call the Rd treatment effect and then once we do that now we have to decide how we're going to conduct inference um and hopefully we would like to use a procedure to form confidence intervals or perhaps conduct hypothesis tests that are procedures that are valid and from some objective and demonstrable way and so I'll talk about a couple of ways to do that but they're all going to be related to an idea that Rocio and I and some of my clothes and amazing collaborators um have put for what over the years which is the idea of robust bias correction and so I'll talk a little bit about how this idea comes about and why we think is a useful way of trying to understand uh or characterize better issues um related to invalid inference that otherwise will come out and if you were to apply the naive or the simplistic approaches that sometimes I've seen in practice and so I'll talk about that as I get there okay so let me try to ground this idea so now I'll go a little bit quicker so I can get to the to the meat of the analysis so as I said before we're trying to approximate regulation functions um from the control side and the treatment side locally so forgot to run all these think of the most prototypical case I would argue it should be the default in most applications every time I get to review a lot of Rd papers better words and many of them are applied and whenever I see that the linear feed has been excluded I typically worry a little bit I'm not saying that you should always use this you should only use this but certainly a linear approximation is arguably a very safe first step in the analysis and then to have you do this when you just be equal to one and then you select a window uh by choosing a bandwidth age such that all the units to the left of the cutoff but up to that value H is um they are in in the control group and then all the units to the right are in the uh treatment group okay and so we're going to split that and we're going to analyze them separately from the left and from the right and in each case we're going to just run at least squares regression so we're gonna run a list of aspiration and weighted three squares relations from the left and I waited with square regression from the right and so here the weights I was just gonna try to capture this idea that observations closer to the god of uh provide arguably more identifying assumptions whatever that means but certainly we would like to upweigh them and then observations farther away from the cutoff but still in the region of analysis that there might by the bandwidth They will receive relatively less weight and so sometimes we like that idea and so in order to do so we can choose different waiting schemes that um one could use and so there are three of them that are quite popular in practice and the three um are plotted here and they are arguably the most interesting ones so the first one which is a default um from the point estimation perspective because of its optimality properties which is not super important today is is known as the Triangular kernel so this kernel is a waiting scheme that down weights observations as they played as they are placed away from the cutoff in terms of their score they are downgraded linear right so you can also down weight them quadratically and that is typically called the epinephic of Kernel or we may choose not to wait or down weight observations at all and that is typically called the uniform so the uniform kernel is going to resemble and a traditional least squares regression whenever you use the uniform kernel you are not waiting or down waiting or avoiding observations or get the equal weight all we are really doing is localizing dropping the observations outside the neighborhood determined by the bandwidth and literally running a list Quest regulation and so you can of course run a truly squares regulations one from the left one from the right or you could also package all together in a single least Square Federation and the single squares regression will simply Define a dummy variable for whether or not units are about the cutoff and then you will interact the dummy variable with Disco or in this case keep in mind it depends you should also wait by the waiting scheme if there is such a waiting scheme unless you choose a uniform kernel and then of course this representation is only valid for b equal to one if you were to think of quadratic fitting locally then you will still localize this way but then you will have to add the quadratic Square term and interactions as well okay very good so once you chose those those um those ingredients we're ready to explain how the procedure works I think it's fairly straightforward so I'm going to go fairly quick imagine this is a true data generating process notice that there is a condition expectation to the left there is a condition expectation to the right in fact there are continuous at the cutoff in this data generating process that we created and importantly they are not parallel right so that's something that is kind of important too are these designs are going to give us something about what happens there at the cutoff point from a continuity perspective or perhaps something around here if we are willing and able to apply a local randomization uh analysis but certainly it will be hard to learn something about down there right so the power of the idea the identification power of regression continuity signs it's not really local to the kind of fun so anything that that you aim to say or do beyond that point uh webinar team or the literature teams as extrapolation which which has a natural analog uh to external validity if you wish in the context of randomized experiments and so are these signs are going to have high internal validity they're going to give us a lot of information about what happens here because of but very or no information about what happens outside that region in the absence of additional assumptions and there are quite a few different approaches out there which I don't think today I have the time to to touch base on but there will be references at the end and some review articles that we are writing at the moment that will touch base on those ideas as well okay great so getting back to this uh this is the regression discontinuity setting and so this is the true underlying model but we don't get to see that what we get to see is data the data in this case is plotted in red for those above the cutoff in blue for both below those below and of course here I'm putting together the true underlying condition expectation functions as well as the uh data in reality you don't even see that all you really see is the data and this is a very clean sqlite example where the data is clearly separated from above and below according to the groups in reality this data will never be as nice as that so just to give you an example imagine that I wanted to plot for you the look of Miller data which is such a fantastic paper and data set for us to try crazy things on so far we haven't been able to break it although we keep trying um and here if they are the plot that you will get if you were to get a bunch of beans which effectively says plot each data as an individual dot right so if we put a thousand beans on each side essentially each pin will contain one or two observations maybe even less and and so this is exactly basically a scattered plot and now you can see that there is no plane separation as you would have hoped to see in my stylized example nonetheless the question remains is there a treatment effect a formally speaking that we can estimate and later attach uncertainty okay great so here's the data set in digitalized world so how does a local polynomial method local methods work well as I told you before you choose a bandwidth the bandwidth localizes immediately removes all that data that's gone for us and now we're gonna do the analysis only using the data inside the bandwidth region and then the way we do it is quite simple we run in local perhaps a weighted least Square regression of some power p in this case P equal to one is linear and then we predict the value at the at it and then we do the same from the left and then we predict the value at the cutoff and so here let me pause for a minute and again are the designs are fundamentally identified at the limit and therefore there is an extrapolation component that is unavoidable in other words and most and typically never you will see observations exactly at the Canon so you would only be able to estimate the integration functions with observations that are slightly away from the cutoff maybe the first observation landed right there then right here then here then here in the particular application of the weather Miller by construction there is one and only one observation at the current which is a 300th municipality that determined the cutoff itself other than that all other observations are away from that point and so it's unavoidable that we're gonna estimate using data that is around here and here and then we're gonna predict essentially to the boundary of the support and so we can dig digress quite a bit about whether this is extrapolation or not and what exactly identification means in this case if this is something that we do quite commonly in Improvement in statistics or econometrics when you think about estimating functions and Boundary points is a is a variable the conditioning variable is continuous then the probability of seeing any one observation of the boundary will be zero and in practice you will never have those observations so there's always a tiny little bit of extrapolation that is going on underlying any regulation is continuity inside and with that comes bias so bias tends to be a big problem because usually you try to predict what the conditional expectation is at that point but of course your linear approximation only does so much of a good job because it's an approximation to the true underlying conditional expectations so in these examples you can see that the predicted value differs from the actual population condition expectation value and this vertical distance here is what we would call the bias or the final sample bias I mean of course bias is defined in terms of expectations so at the end this is just one realization uh but the idea is the same so essentially there will be a vertical distance that captures this difference and we're going to refer to this as mispecification error it's natural right so we go near the cutoff we Define a region and then we postulate that a linear regression is a good enough approximation for these um underlying unknown conditional expectation that needs not to be linear at all and so the question is how much of a mistake are we making right and so intuitively you can imagine that if you were to choose a large bandwidth and the condition expectation exhibited enough curvature then the linear approximation could not do a good job right so in this example for instance the condition expectation looks like this and like these and then because the bandwidth is also called large whatever that means um the bottom line is that you see uh that the linear feet would not do such a good job if we were to shrink the bandwidth then we would be approximating what is a by assumption a continuous function by identifying assumption and therefore in a local neighborhood more local than H2 we would be able to do a better job and so that's exactly the idea right so if we were to shrink from H2 to H1 then all of the sudden the same linear model could do arguably a better job hopefully a better job under some assumptions and so this change in in bias uh from from a large bandwidth to a small band with uh it's gonna play an important role in the analysis I mean ultimately our methods are gonna be all models are fundamentally specified and so the questions how we think about the specification and what we do about it and so in the context of bandwidth election this is very clear if the only goal was to reduce the specification we would just shrink the bank with as much as we can of course in practice there is a naive response to the problem because the smaller the bank with the less observations we have to do the estimation and so the price that we pay for reducing specification errors naturally is larger variability and eventually inability to estimate the function at all and so we need to trade all these two and that's where the trade-off comes in to select the batteries so we can select bandwidth base on a criteria for example for Point estimation that would be the idea of means whatever and it means collateral settings we're going to be trading off via some variants and finding the optimal switch spot where that bandwidth lines that give us the best trade-off between bias square and variance and the idea here is very simple let's just show you a picture about it if you choose a very small bandwidth bias is very small but variance is too large we are better off by slightly increasing the bandwidth and improving on the trade-off and then as I keep increasing the bandwidth I'll reach my Optima and as soon as I cross that point again the trader flips and continues up rise of the means whatever is a beautiful comebacks objective function that you can solve for and under some conditions and assumptions you can even give a fairly good characterizations of how this Choice looks like so in the case of a local polynomial regression the choice would look like a fraction of the sample size in particular power of the sample size times a constant and the constant needs to be estimated the estimation requires some additional steps which are not super important for the purpose of today but then eventually you can construct an estimator of that language and that will be the default in the analysis so typically you choose a bandwidth by fitting this constants and then selecting the bandwidth in a data-driven way and that's very important right because data-driven procedures take a little bit of the discretionary from the researcher away so they reduce peacocking they increase transparency and they allow for something that is fully replicable so you don't need to start asking questions like why do you choose a bandwidth of 5 or 10 or 15 we can start thinking about okay what the data is telling you about what the bandwidth should be and of course there is not a unique bandwidth so here I put a second bank without there this bandwidth is different it has a different rate of convergence I.E a different fraction of sample size or power of the sample size is being used and more importantly a different trade-off between bias and variance is being done so in discovered a shareholder approach which effectively targets the best possible distributional approximation of the D statistic distribution sampling distribution to the gaussian distribution in the ins in the sense of Correction these two are minimized there are distances minimized uh by this particular choice and here you can see that instead of trading off biosphere and variants where you trade off is values and values and so again there is no right answer you have you just need to choose what you think is best for the problem at hand typically the default will be to construct a point estimator that is optimal in a means quarter or sense and that would be your starting point so many people practice and I would argue is a good idea they decide on a bandwidth for Point estimation and then they would like to keep that bandwidth fixed part of the reason is because then they're using the same observations both for a point estimation and uncertainty quantification the only difference how to use them and so that will be the procedure at the heart of the procedure I will discuss in a in a few minutes okay so there are a couple of ways of doing these I told you this all these before in practice you're going to put hats on these things so as you know econometricians love to put Hudson stuff and that's kind of our job so we just put hacks on things whenever we don't know them that means you go to the data you get your best case of those and then you plug them in and you go with that okay awesome so once you have these ingredients we're ready to go so how we're gonna do inference here well I hope it's pretty straightforward once you decided on how to estimate um this point and this point and notice this is just a least Square X problem so we all know at least squares very well so we could very easily rely on traditional liquids inference to try to learn about the uncertainty on our point of teenagers right and so the typical approach here would be to form a t-test right a difference in means but for the regression is intercepts and then that will put the two intercepts which will be the three manifested point estimator for the treatment effect the vertical distance estimated and then you will put down here the standard error of the difference and so to the extent that this behaves like and it does a true sample problem then the variance of the difference is the sum of the variances and so you can just plug in your preferred uh like a wide robust and cluster robust or whatever version you like to estimate the variance of The Intercept from the left the variance of The Intercept from the right you plug them in and you get yourself a great detest statistic and then comes the hope the hope is that whenever you do that that the statistic behaves like we know traditionally squares models or discards problems cliche that is the details enjoys a distributional approximation which is a normal zero one and so if we get that we are in business right and that would be what you typically appeal to every time that you use red in stata or LM in R what you do is basically you postulate a linear regression model you assume it's correctly specified then you appeal to either final sample or large sample distribution and features and then you just form your preferred confidence interval which is point estimator plus minus say two times uh other networks and that will be the 95 typical confidence interval what's the problem here well the problem is one of the specification errors right so as I told you before the key distinctive feature between regression discontinuity designs and traditionally squares problems could be that in regression discontinuity designs we know that we are mis-specified furthermore we take advantage of the specification in order to construct an optimal bandwidth choice so if you remember correctly I said oh the bandwidth choice is a ratio of variance to bias but of course if bias was Zero then there will be no choice other than infinity therefore the bandwidth will be the entire region that makes sense because if the model is truly linear why would you localize at all in practice we don't like that in practice we do localize we don't believe the relation function is linear and therefore we are facing a natural trade okay we're going to choose the bandwidth the buyers cannot be zero but if the bias is not zero then in general the distribution approximation is not correct because it's not centered at zero in general it depends on the choice of bandwidth that you decide to use and in particular if you were to use the means whatever Choice then you will discover that the limiting distribution exhibits A first order bias and that postulates a challenge now we need to account for that bias if we want to go ahead and conduct inference we cannot just rely on 1.96 quantiles and go ahead and use these Quest methods we cannot simultaneously choose a bandwidth for means quite error optimality and then pretend that there is no bias we need to choose one either we choose the bandwidth in some other way or we account for the buyers after that works and so in The Proposal that we typically use and recommend what we do is essentially to provide a bias correction that comes about for controlling for that extra specification error that you introduce when you use the mean square error bandwidth and then because this specification error has to somehow so you need to actually adjust the location of the confidence intervals by relocating them at their centering being relocated to a point where this bias is no longer differs or very present um then that induces additional variability and that additional variability can be accounted for and so the procedure that we recommend and having our software is implemented typically what it does it adjusts the location of the confidence interval and then accordingly enlarges the intervals by adding an account or trying to account for the extra variability that the bias correction estimate introduced in the first place and so at the end of the day the procedure will shift confidence intervals in location and scale right so these confidence intervals will typically be displaced in their location and they will be enlarged by this additional variability introduced in the scale and that doesn't mean that the confidence intervals will lead systematically to less statistical significant results for example because it all depends on the trade-off of the displacement to the enlargement of confidence so let me show you a couple of examples of how this works so this picture tries to capture the idea of different possible bandwidth choices and in particular that means whatever optimal will lead to least squares uh confidence intervals that by construction which have a first of the bias is a mathematical fact I don't think it's up for discussion in general of course it can happen to be that that particular bias constant is zero at that point in which case this would have not been well defined to begin with other than infinity of course um so once you have that there is a bias that leads to over rejection of the null hypothesis so in practice we will see that uh choosing the optimum means whatever bandwidth for Point estimation in Rd and then constructing confidence intervals using list squares ideas will lead in general to mean specification in the location of the confidence intervals and overrejection of the null hypothe so that means that we're going to be overreacting the null hypothesis of an old treatment effect and a back of envelope calculation tells you that coverage the adult confidence intervals will have will be roughly 82 percent for a 95 nominal level confidence interval that means you're going to be rejected roughly 10 percent of the time incorrectly the null hypothesis so how do we fix that well as I told you we can re-center the confidence intervals but then we need to enlarge them to appropriately okay and so that's kind of the idea and this problem might or might not be what more important as you move yourself in the scale of bandwidth if you choose a large bandwidth right large relative to the mean whenever one then the problem will be exacerbated more buyers will kick in and further the distance uh in the centering of the confidence and from the true nominal the two population Center if you go to the left of the mean square error something that is called understood then you can mitigate these problems use the bandwidth relative everyone and eventually use it enough you might be able to regain nominal levels uh theoretical and large sample nominal levels that have been postulated of course that implies a loss of power so typically as you do that you can see the confidence length the length of the confidence intervals become to larger and larger and that is the mechanical again because the bandwidth is shrinking less observations are available the variance is larger and the confidence intervals explode okay great so let me now show you this in action for five minutes because I think it's cool to see how all these ideas come about and so what I'm gonna do is I'm going to try to go back to the original paper of Louetta Miller and try to replicate the results and in particular one thing that I love to do always when I see these papers is to say well in this paper they chose a bandwidth of 9 18 and 36 of course they are not to blame they did that but four we didn't have all the theory and ideas that we have today but nonetheless it will be cool to ask the question what is nine relative to the mean Squad error bandwidth that you would like you would obtain nowadays and so we can do that and so let me just show you an example of what that would look like um if I can find it of course maybe down here oh there we go beautiful so the typical uh example would be like this so this will be a plain vanilla estimate for the Lewis Miller data so on the left hand side you have 2400 observations on the left you have about 300 there's a few missing data values points there and then the conventional here is nothing more than um this pinky uh intervals and procedures right so the least squares approach to this problem and then they are uh you see now that you have a point estimate and because this is chosen to be mean Squad error or optimal that means that this point estimate is a mean square error Point estimate of the Arctic manifest associated with it you can construct the confidence a standard error the standard error is the straightforward least squares variance standard error in this case it's an icrestic robust estimator and then we have two options we can do inference based on the least squares approach that is the one that assumes no bias which again it would be inconsistent with the idea of choosing a mid-water or bandwidth in the first place but nonetheless you can do that I guess and then alternatively you can actually control for the buyers bias correct and then enlarge confidence intervals accordingly and so that will be the confidence interval that comes out up here and in this example for instance the p-value actually decreases it goes from 0.046 to 0.42 that essentially means that the shifting of the conveyance interval away from zero is more than proportional or some codes depending on the distribution relative to the enlargement of the SK and so in this case the p-value I'm sure it goes down so it's not even true of course and by doing that this robust approach you automatically use significance it really depends on the application at hand and now more importantly what is now so as I said before they use this nine here so and they use 527 observations right right here so what is nine for them um and let's check well here the optimal bandwidth is actually almost seven so they are already a little bit overs move they have a bandwidth that is larger than the optimal bandwidth and then that means in the picture here that if this is seven then there are about nine they're about there right so in this in this example they are a little bit away and further out from what it would have been the optimal error bandwidth in the region where the buyers of this specific Asian error is even larger than you would expect to see and so um so that's okay that's what it is and so we can actually replicate their results so we could actually use nine as they did and so here's an example where we can just artificially I mean mechanically choose a bandwidth of nine and then see what the result would look like and now you can see that with a bandwidth of nine um the uh coefficient the point estimator actually is decreased by about 10 to 15 percent um and that may be arguably potentially due to this specification error with two large bandwidth to approximate correctly the condition expectation the avocado and so uh in particular if you want to replicate the results you will see that that result actually is a minus 1.895 in order to do that they actually use a uniform kernel because back in the days sequence regressions with no waiting so instead of using a triangular kernel what they need is they did a uniform camera so here is the replication for them with the uniform kernel and now you get exactly the minus 1.89 52 which is a result reported in the QJ and then there is some issue with the standard errors which in the time I'm gonna touch base on today but if you explore the code you can play with it and get a sense of how that Okay cool so that's a short replication of that in the interest of time and now in the last minute or two since I started two minutes late I I earned my two minutes I'm gonna mention quickly ideas of falsification and validation I know I know I'm not going to be able to cover everything here and I knew that coming in so I just gave you one slide of this and and that's like basically it's just a reminder that regression discontinuity designs to me at least and one of the reasons I really like them but working on them was that they to me uh they were particularly good at falsifying very fine validating interpreting they have a bunch of interesting features that are unique and not necessarily portable to other observational studies methods and in particular here I mentioned a few so there is a very interesting way of thinking about whether there is manipulation of the units near the cutoff right so underlying the idea of continuity or the idea of local randomization there is this notion that units should not be able to systematically place themselves in one treatment group or a control group by their choice in other words if they could systematically place their scores about the cutoff or below the cutoff then obviously the comparability will no longer be there and intuitively you would expect them to be quite different between treatment and control groups and so this idea was originally both I am more practitioners and and then later it was former life and investigated in ways and so there is a very popular test which is called the density continuity test was proposed by Justin McCready and that one in particular seeks to try to check whether there is a disproportionate jump in the mass of units on one side of the cutoff on the other and so there are different ways of doing that I'll show you a very quickly one visually because I think it's kind of cool to see they just basically estimate the density of the core variable from the right and from the left and they hope to see that there is no discrepancies not surprises in terms of the number of units on either side of that kind of you can also look at the same idea from a binomial assignment test perspective and so this code immediately tells you P values for household pricing it is to see different number of units on each side of the cutoff and then you can see very quickly the P values are away away from any significant levels that means that the number of observations expected to be seen on each side of the regions around the kind of are roughly what you would expect to see if you were to randomly assign them and that kind of Assad appeals to the idea that they are not systematically placing on one side or the other of course this is neither necessary nor sufficient they could be placed in themselves one side or the other but they will do it in such a way that they're compensating accounts and so this is just a very nice metabolistic way of thinking about the problem and then here's a list of many other ways of falsifying and investigating the validity of the design I won't be able to touch on all of them let me just mention that pre-intervention covariance is a great idea this was proposed by several people including Lewis and Miller they have a great second table that I could not do here where they check whether they are the the treatment the treatment has any effect on outcomes or pre-intervention covariates that are deemed to not be affected by the treatment and so that's a great check that indeed the channel the treatment effect is coming from to the outcome that we care about and not other there are many other tests that you can do good so I'll stop here for the interest of time so let me just thank not only on my behalf but also on Rocio since we are sharing the slides um to begin with and let me just mention that in this website here um where to go there um you can actually find a all kind of references that for for all the stuff that we are that we discussed today I 