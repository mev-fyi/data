Aviv Nevo: There is
code on my webpage, but it's a little
bit outdated and there's some minor
mistakes with it, that's really not a big
deal, but there's a couple of bigger mistakes on it. But it still seems to work
it generates numbers, and one of the things is the computation of
the gradient there. It's missing a term, but it still seems to somehow work in the sense that it
converges to what seems like reasonable numbers but there's a couple of
other issues with it. I actually have
updated code that a student wrote and I keep going back and forth as
to whether update it. But that's one place you
can look at the code again. It's a code, not the best code. I know that there's
better codes out there. I know that Fox, Sue, and Dubay, they actually have either on each of their websites or one
of their websites, but they actually have a
code for the MPAC algorithm, and I believe that part of because they have for
their comparison, they might also have the BOP nested fixed
point algorithm. Actually, if we really care, I'm sure someone can actually Google them during the talk
and I can update you later, but I'm pretty sure
that there's actually for sure code of impact, and they might also have
the next at fixed point, and my guess is that
their code, it's updated, they've written in the
last couple of years, so it's probably more efficient and better at
the code that I have. I believe there's a
couple of other codes floating out there so if you look, I'm sure you can find. Most of them are
either MATLAB codes or something of that sort, people are asking me
when the state are going to write a code like
that, I don't know. I don't know if they'll
ever do it and if they do how good it will be. There are things.
They're usually for the basic aggregate data stuff. If you want to add any
of the micro-moments, add some of the pricing itself, you're going to have to do
some programming yourself. It's probably a good idea to do it anyway, not to just take, I know it's a little
bit of entry cons, but it's probably worth doing. But anyway, that's
just in terms of code. In terms of the
topics so I think Aaron and I decided that each of our last lectures will be on much more closer to
the frontier and stuff that we're
currently working on or stuff that's more active, and I was really
debating on combining some combination or some subset of four different topics, and then I decide I'm just going to talk about one of them, and even that I
think I'm not sure I can fit it in an
hour-and-a-half. I'll just mention
the other three briefly, just in terms of, I think these are directions that are active and
if you're looking at explore ideas or
things to look at. The three that I'll
mention that I'm not talking about are
our instruments, relaxing the discrete
choice assumption , and flexible models. Let me just talk about
each of these quickly. The issue of instruments, I think there's two areas
that are being looked at. One is the whole issue
of weak instruments, and that relates a little bit of the computational problem. I mentioned that briefly. Personally, I think, but again, I'm biased because it's my paper that I'm going to
refer to but personally actually, I think
exploring the whole issue of invalid instrument
or inference with invalid instrument is
an important issue because as you saw when we got to talk
about instruments, you guys pushed me a little bit, rightfully, and there's
only so much you can say. You say, well that's
the best we can do. Yes, there's issues
with them and stuff. An alternative approach
that I actually took in a paper that's just
forthcoming now in the review of economic
and statistics reached that with
Adam Rosen is to say, well what happens if instead of having a moment in quality, we have a moment inequality? So we say there's
an instrument Z, but it's actually correlated
with the error term. Can we still say anything
and it ends up that you can set identify so building on the ideas that
Ariel just talked about, you can actually
bound the parameters, and in some cases, the bounds can be reasonable. Maybe because we haven't
employed a lot of instruments, we haven't got the point where
we have a point estimate, but we actually
get the intervals are what I think might be useful for some of the
stuff that we looked. But we've only
looked in the case, and even though we
set up the model, the problem in general, we only looked at the linear case, which means we only looked
at the logit models and we literally
actually use some of the serial numbers that I showed you earlier
to say let's go back and explore what
if these prices in other cities are
indeed correlated. That's one area that I
think we can actually do a lot of work
with whether it's finding ways to find
better instruments or working with these methods or
something, an alternative. The second issue is the issue of looking at choices
are not discrete either because we have multiple
choices or because we have issues of multiple plus a discrete choice and
then a usage decision. For example Ariel in the
previous lecture just briefly alluded the
issue of let say if you look at the air conditioning, so there's decision or
appliances in general, which one do you take and
then how much you use it. So there's this old paper by Dubin and McFadden and
'84 in Econometrica. But in some sense that's
almost begging to be "Updated" or reused
in this new generation, and then there's others
choice situations where we actually see people choosing
more than one option. We know it's like, well, you choose one, but
you could think, well, what would you do
if you actually see people choosing three options? Or what do we see
if we see people choosing a number of options that satisfied
their budget constraint? I have $10 or I have a
cart that I want to fill. If I'm going to a supermarket, I'm going to buy up
until that full, and it could be maybe full after two items or could
be full after 10. These are actually, I think, problems that, there
has been some work on, but you could
probably have more. Then finally, the issue
of flexible demand, I think I just briefly
alluded to the results of hail and very inhale on identification of
non-parametric models, which we probably
maybe could have actually spent
more time actually going through the results. But I think a very
interesting follow-up is not just identification, but then trying to
connect this to estimation because as we saw, it's very important
to try to get this flexible functional
form and try to get the right demographics or
the right unobservables. But then also say, well, what's the distribution that
we want to put on that? How important is that? How can we really
try to really get more flexible
functional forms to get at this and I think
it's a ripe area, and I think different people have been approaching it from different way a lot of it using more household-level data. I know Stephan [inaudible] here, he's done some work on
this with other data set, but it's to prime for some
of those ideas to actually come in to this literature. That's a third area that I'm not going
to be talking about, but one that we probably could have spent at
least one lecture. Instead what I'm going
to be talking about are issues of dynamic demand. Let me just start
for motivation. Up to now, we've focused
on static demands, although it was very clear that dynamics are
in the background, both on the demand
and supply side, but I'm just going to focus
on the demand side now. If you talk about cars, obviously, cars are durables, so before lunch they
weren't after lunch, they became durable suddenly. But we talked about cereal. Well, cereal is storable, so if you buy it in one week, it doesn't mean you have
to consume it right then. Right now, some of this stuff
we saw was over a quarter, so you say, if you
buy one-quarter, maybe you're going to consume it over and maybe some of
these issues don't exist, but we'll see that's
going to bring that in. More generally, dynamics can arise in demand for
different reasons. I talked about
storable, durable, but you can also think
of habit formation. It could be that
what I purchased last time is going to
impact what I do today, there's some switching costs, if you're thinking
of, I don't know what cellphone providers or
which telephone I use, or maybe loyalty to an airline. You can give all these examples, so it's either the habit
formation or switching costs. There might also be
some learning on the other end where their
new product is introduced, it takes a while until they actually learn the
characteristics. In the model, we always
assumed you know what they are almost
instantaneously and you learn as they go along. So a new product is introduced, maybe it takes a while for the consumers to actually
figure that out, and that might be
important if you're doing the welfare from a new product. It's not even clear
which way it could go. Some products, you
might say they're not buying it because they
haven't learned about it. Some product could be
exactly the opposite. You may be for cheap products, there's a new yogurt available on the shelf
so let me try it, so there's this
big jump initially and then people try
it and they don't like it and it goes down. You have to be a little
bit careful about these things when you're going to estimate demand
and use that for, let's say, evaluating
the welfare gains. I'm going to focus today on
storable and durable goods, and there's a lot of
different ways that I could address this. What I'm going to try and really focus is on three things. One is to say, what happens to static estimates
if dynamics are present? What goes wrong? I'm going to try to
talk both in terms of our estimates are
biased in which direction, but also think in
terms of modeling, what parts of our
model are missing? I'll try to point that
out in different parts. Then I'll talk about
various models of estimating dynamics
or the challenges of estimating dynamics and the way that the literature has tried to deal with it. I'll also talk a
little bit about estimation with consumer
and aggregate data. Here I'm not going
to go to level of detail that we've
gone in the past, so I'll just quickly talk
about some of the methods. But I won't really
go to the level of detail that we've gone
in other lectures. I just don't have time.
That's this last point. Before going to the
specifics let's just mention the challenges
in estimating demand. We've already seen this, even when we talked
about static demand, one of the key issues with
this too many products, too many parameters problem
that we had to deal with, and in static models the
way we solved it is we basically took the product and projected them on a
characteristic space. We said, we start with j, where j is a large number, maybe 100, and
projected them on, let's say k characteristics
or k could be 5-10 in most cases
sometimes not even that. Actually usually
the challenge is to find enough meaningful
characteristic, not the other way round. Now, for dynamics, if we want to do that, at least without putting
any more structure, that's still not enough
because now what we need to do is we need to keep
track, so if you say, I will be much clearer about this in a second,
but if you're thinking, I'm now going to
make a decision, and my decision depends not just on today's character, but also the future. I now have to form expectations about all of these characteristics
of all the products, what they're going
to be in the future. That's a lot of things to keep track of and the most
general thing is even if suppose I just have price and one characteristic but I
have a lot of products, I now have to keep
track of what's going to happen to
these in the future, so I need a way to somehow
reduce the state-space. Now that's even harder
because that's going to be interacted with consumer
attributes because different consumer might care about different
things differently. When I have to form this expectation about
different things, it's not like I can just
say let's just take some a single index. I have to figure out how
that's going to interact with the different
consumer attributes. It's really not practical and I need to somehow
reduce the dimensions. I'm going to call it two
different solutions. I'm going to talk about three different
papers, essentially. Two of them in storable goods
in one and durable goods. One of the storable
goods, one durable will take a very similar
approach, different, but I'll try to highlight
how it's different, but a similar approach to it. Then there's another of this. You can see up there a simple
demand model for storable, which actually is going to
take a different approach to trying to simplify
the problem. I don't think any of these is the latest word on the topic, or maybe the latest word
but not the last word. I think there's a lot that we can do and
you can see there's going to be a lot of
strong assumptions here. But we need to do something
to make some progress. Let me start with
storable goods. Let me just set up the problem. This is a typical pattern for
when I say storable goods, think of goods that you
buy in a supermarket. That's what I have in mind. This is actually a price
graph in one particular store over a year for the price of
a two liter bottle of Coke. But you can actually produce, and there is in the literature a whole bunch of
different products. You'll see the
details differ a bit, but it's all the same idea. You have a regular price with occasional price reduction going back to that same regular price. Occasionally, you see
the whole plateau shift. But this is a pretty
common pattern that you see across many
different store format, different countries and
different products. That's the pattern.
Seeing this pattern, I'm going to ask two
different questions. One which I'm just going to ask now and not really address. But I think, to me, it's two different
questions that come up. One is, why do we
see this pattern? The second, and I'll answer on the next slide
and that's going to be the focus is how do consumers
respond when they see it? Even if we take this
pattern as given, say, we don't understand
where it's coming from, the question is,
what is it doing? If you see this pattern,
you can say, well, one thing is say maybe it just so happens that if you
go to our pricing equation that we had before the
static [inaudible] It just so happens
that the price and demand condition change it, this was the statically
optimal price. Theoretically, that's possible. It's hard to imagine that the
conditionals would be such that it will give you
exactly this distribution, but it is theoretically
possible. That's Option 1. Although I highly
doubt that's the case. More of serious attempt at addressing this go back
to a paper by Hal Varian. I guess Hal Varian is the
motivation for both of our lectures this afternoon. In a paper well, basically
I can go into details, but there's two
types of consumers. Consumers that know about
prices and those that don't, or customers that are loyal
and those that are not. Stores are basically
competing between them. The equilibrium involves
having a mixed strategy. The mixed strategy
basically sales are basically the
mixed strategy. They're randomizing
between the regular price and the sales price. There's some appeal
to this theory. In particular, it
actually gives you some randomness that
we see in the sales. Maybe it didn't look
as random there, but in others here it does look a little bit more random. But it has some problems. In particular, if it really
is a mixed strategy, there should be no
state dependent. Whether you had a
sale last period, it shouldn't impact the
probability of a sale today. You're basically tossing a coin, the same coin every period. When you look at data, there
is some state dependent. In most cases, if you
had a sale previously, you're less likely to
have it today and that's not as consistent
with this theory. Also, this theory, at least, the strict version of it
you should see a continuum of prices and not a
two-point support, but there's ways to modify the original paper to actually get the
two points support. Another story that people have proposed as
multi-category pricing. The idea is that
you actually have a sale to bring people
into the store. They buy other products. That's probably
part of the story, but it's hard to
imagine, it's all of it. Then the final theory, which is my favorite and the one
that we're going to build on to try to look at when we go to demand is intertemporal
price discrimination. The idea is that people differ in their
price sensitivity, but also in their ability
or willingness to store. There you can think
of there's people who store and people
who don't store. The idea is that you
want to separate these two and what you're doing with the sales is to basically temporary
price discriminate. There's an older theory
literature, starting with Sobel. Actual look. This was
endurable goods but had the same idea of in temple
price discrimination. Several more recent papers. Probably, the latest
one is eagle Endo. I have actually a
working paper that uses the next model I'll talk about encloses this and actually literally shows here
are the profit, the welfare gains that
you can actually get. Well, the profits and
the welfare implications of this intertemporal
price discrimination. That's in terms of the
sale just to supply side, just to get this out of the way for the rest of what I'm doing. I'm just going to
say sales are there. Let's just see what implications
that has for demand. For today's talk, I'm actually not going to close the model. I'm just going to
focus on on-demand. I guess here what
I wanted to say is what do consumers do
when they see the price? I think the answer
is, they store. The idea is that product
goes on sale, at least, some consumers, the
product goes on sale, you say, ''Hey, why
wait till next week, let's buy it today?'' There's a lot of different
evidence here that talk about both in the
economics literature and in the marketing literature. But instead of serving this in a dry way,
let me show you, I think the one table
that at least for me, it really drove things home. This is a table looking again at two-liter
bottles of Coke. What we have here
in each cell is the average units
sold in a week. Average number of
bottles sold in a week. I believe this is actually
average over several stores. First, what we have in the different rows is we look at the period that our sales and
those that are not sales. How do I define those? I just go back, look
at that picture. I think it was very clear what was sales and what wasn't sale. If you look at the
margin there at the end, you can see once I have a sale, I sell significantly more than
when I don't have a sale. Even though those cells were
fairly large, but still, you look at the quantity at
how much it jumped up at, it's almost tripled
during periods of sale. That in itself it's interesting to see
the economics work. People respond to price,
but not surprising. What's interesting here is if I condition on what happened
in the previous period. I look here as to
whether there was a sale in the previous
period or not. You can see already here. If you look at the margin,
you see that there is a fact, but this is actually
conditioned also on today's factor and you can see holding today's price
or today's state constant. You can see that I sell less if there was
a sale yesterday. Similarly, this was no sale and this isn't
a case of the sale. There's actually a significant
effect of how much you sell on a sale or how much quantity you
sell during a sale depends on whether
there was a sale in the previous period or not. Now, you might say,
oh, you forgot, there's omitted variables,
so this is Coke. But you say, well
what happens if actually Pepsi is omitted in. It's exactly the opposite. What happens is these periods, it's really just about
Pepsi having a sale. That's what you're telling
him that that's not it. I can actually show
you this table conditional on whether
Pepsi has a sale or not, and it actually
even looks nicer. But that's not
what's driving it. Later, we'll actually see if
you do this in regression, in control for a
bunch of other things That's not driving it either. By the way, if you're
wondering whether these differences
are significant, I have the standard
errors in there for you. You see they're
actually fairly small. The differences really
are insignificant. To me, that says it seems
like consumers are storing. Before we go and try to model
this, we might ask, well, suppose they do then what are the implications for everything that we've
done up to now? When consumers store,
we have to realize that there's two things. First is that the
purchases that they buy. What I see people purchase
is different from what they consume so
there's a separation. Purchases both can go to
consumption and to storage. In a static model,
they're the same. There's no difference
between them. But here, it could be
that I'm buying today because I want to
consume tomorrow. That's going to be a key issue. Our object of interest, what we really want is ultimately if you want
the preferences or you can think of it as
the consumption function, if you want. I basically want to know, but if you think of it,
I don't want to use the term demand because
demand is confusing. What I mean is purchases
or consumption. But what I want to know is
given people's preferences. If I take people's
preferences and then give them a set of prices, how much would they consume? How much would they buy if there were only
buying for today? If we have that, we can
then do different things. Mostly, what we care about, we want to know some
long-run effects. What we want to know
is not necessarily the response to a one-week sale. If we're in marketing,
we care about that, but in economics, I think we care more about
long-run effects. We say what happens if
now Coke merges with another producer of soft drink and what will be their
incentives to raise prices? What would be the
long-run effect? If you see that pattern, you might want to say they're going to have more
or less sales, but that's a separate issue. If we just look at a
static supply model, it's a different
type of incentive. There's two separate issues with a static demand estimation, and it's important
to separate them. One is just a pure
econometric bias. You might say, well, there's
some omitted variable that's maybe correlated
with my price incentivity. But I think more
important than that, thinking about this as
an econometric bias and assumption is not the
right way really. What we want to think about
that there's a difference between short-run and
long-run response. Even if there's no bias, even if we have no
econometric bias, that we're getting the
right coefficients, we want to separate between how consumers are going
to respond if it was just a temporary
reduction to price versus a long-run
reduction to price. We change the price
for a long time. The static model can't really
give us that separation. In most [inaudible] I say we really care about the
long-run response. The problem I think with static estimates is it
combines these two. In some sense what it gives
us is not even clear. Not really given us that
short run or long run. But if we ask suppose we do the static estimates
and use them to compute some elasticities and pretend
as if they're long-run, how would they compare to long-run responses that we would get from a dynamic model? I'll actually confirm this later when we do the estimates, but you could ask here. It ends up that if
you do that exercise, you go and estimate
a static model and then use the same data with these sales and
estimate the dynamic model, what you should expect to get, and that's what you actually
get when you do it, is you get that the
static estimates overestimates the
own price effect. You're going to get the
people respond more to price, are more price sensitive from the static estimate
than what you would get from a
long-run dynamic. The intuition for that I
think is pretty simple. What we saw there, we saw that there was a huge
response to sale. Sales really spike up. Quantity sold spikes
up doing sales. Now, the static model attributes that people
are very price-sensitive. You give them $0.20 less and they're going
to buy a lot more. The dynamic model realizes
that a lot of that is really just people buying today for tomorrow's
consumption. If you basically reduce
that price permanently, kept at that level permanently, people of that effect
wouldn't be there. It could actually
be the situation where maybe people are not price sensitive at all in the sense that if you
reduce by 20 percent, they're not going to
increase their consumption. You can imagine some goods. I don't know, think
of laundry detergent. People say I have
to do the laundry. It's not like you're
going to do less laundry if you actually
reduce the price, but you'll still get this
big response to sales. That's the own-price effect. What's interestingly is that the cross-price effects actually tend to be underestimated. Static demand estimates, and actually what we found
in a lot of the papers, that's actually a bigger effect than the own-price effect. It's actually when
we first found it, we were a little bit puzzled by it and we almost
thought it was a mistake, but the more you think
of it, it makes sense. A lot of the terms I use
here might make more sense later once you see it in
the model, but basically, what happens when you can store, think of demand for Coke where
the alternative is Pepsi. That's the competing product. Now, if you can store Pepsi, the relevant price is
not Pepsi's price today. It could have been
Pepsi's price yesterday or two weeks ago,
whatever it is. There's an effective price, the price at which you could
have bought the product. What that means, this
effective price, think of it as some minimum or some function of the
whole history of prices, that's the relevant price, and the price today
is not relevant. If you just stick today's
Pepsi's price in there, you might think they're
not responding. You look at two periods,
one where there is a Pepsi sale, one in which there isn't
it, and say, "Hey, it's not impacting Coke's
quantity sold at all." But the reason you think is
not impacting it's because, well, some people have already
bought it in the past. In some sense, think
of it almost like a measurement error problem. You don't have the
relevant price and you're attenuated towards yours. Not quite right, but that's
intuitively what's going on. That's what we expect would happen to the demand estimates, and now let's go and say, "You've convinced us it's
important, what do we do now?" Let me actually propose a
model of consumer stockpiling. I'll go through some of
these details of the model. The key thing that I want to get from here is to
actually show you what are the challenges and
how the literature has gone around this. As I said, some of these ideas, I'll show you how they work
in durable goods market, but people have used it for
other examples as well. We're going to start here from the utility
of the consumer. This is the utility
the consumer gets from consuming at time
t. That utility is a function of Ct. Ct here just emphasize
everything with an arrow on top is a vector because one of the big
challenges is going to be how to reduce
the dimension. I'll show you that later. I want to emphasize when
thing's a vector and when they, in some sense, stop being a vector after
different assumptions. The utility from consumption
there is usually the consumption from
all the j products. This is a j dimensional vector and these Vts or new ts which are just
shock to consumption. It's going to change your marginal utility
from consumption, therefore, changing how much
you consume each period. Plus, here, Mt is a numerator, multiply by some margin
utility of income. This is all the other goods. The consumer's choice
is the following. They're going to have to
choose how much to consume, which brand to buy. I'm going to still let
the purchase just be of a single option, but an option I'm
going to say it's both the brand and the size, and I want to make
that clear because that's going to
play a role later. There's a question of not
just do I buy Coke or Pepsi, but in some sense, what
sides of them do I buy? That's going to be the
decision. How do I do that? Well, I'm going to have my discounted expected
flow of utility. That utility is going
to be the utility from consumption minus some
cost of storing inventory. Again, at this point, the inventory here is going to be a vector because I have an inventory of each of the
brands that I'm solving. Plus I'm going to have
these terms here, which are basically
what we've been seeing for the last
day and a half. This is going to be,
what we call up to now, the utility from purchase,
plus the Epsilon. Take away the first
part and that's basically what we've
had up to now. I'll talk a little bit
more about what this is doing and what we're
trying to get here. We're going to
maximize that subject to the following constraint. First, inventory
can't be negative, as consumption can't
be negative either. Choice is going to be discrete. One of the x is one of the
size here is a zero size, so not purchased,
so I'm going to buy exactly one option. One of the options
is not purchased, so just exactly as
we've had up to now. This is the transition
of inventory. Basically, each
component by component, what I do is I take
the inventory I had today plus whatever it
is that I purchased, if I purchased, and that's
whatever the size is, minus whatever
consumption I had. Fairly straightforward. In terms of the
stochastic structure, there's basically three components that I
need to tell you about. One is this Epsilon. It's going to be just
like we've always assumed it's going
to be an i.i.d extreme value type 1, just the logit error term. Second is these shocks
to consumption. You only just going to
assume that they're i.i.d over time and across consumers. Both of these, by the way, we can relax if you want the
Epsilons or these V's to be correlated in principle but it becomes a
computational nightmare. From a practical point of view, relaxing these, we can maybe do it a little bit but
not much more than that. The final component, I'm going
to assume that prices and advertising are going to follow a first-order Markov process. I think there are two
assumptions here. One is the fact that it's
a first-order process. Allowing for higher orders
is actually not that big of a deal and I
can show you later. What's actually
not on this slide because it's assumption
is not so important now but I'm going to assume
that this process is going to be exogenous. Exogenous in the sense that firms set it and it's not responding to stuff
in the model. That you could say, well,
we've been talking the last day and a half of an endogeneity and
how to deal with it. I'll show you later how we deal with price and endogeneity. But one of the big
differences, by the way, between this literature,
at least this first part, I'll get back to it later. The static stuff is really the discussion or the emphasis put it
into endogeneity. Here endogeneity is dealt
in a very different way but I should emphasize the data structure is also a
little bit different. I'll get back to this when I talk later when we see
exactly how to deal with it. Let me just quickly write
the value function. I found anything really
that interesting here. But basically the
value function, going back to what
we had before, it's just the flow utility plus the discounted expected
value function. Hopefully, you've seen at least some
dynamics before new. I know this is
maximizing again over the vector of
consumption j and x. A very useful thing
to write here is not the full value function which
has in it the state level also the stochastic term
in which the eigen sees, we don't, instead we
actually write what's called the integrated
value function. We basically integrate out over that login error
terms and the V's. Basically then this is
what this looks like, where we have this EV function here and the EV function there. Essentially what we've done with this log of the exponent, we've integrated out
the logit error terms. Remember that inclusive
value that we have that's basically
used implicitly in here. Then we're also integrating
out this integral here and the part of it is that we're integrating
over the V's. Then the final thing
that we're taking expectation over here
is just the prices. Consumer has to take
the expectation over the future prices, that they don't know it. It's going to be very
useful to work in this space because it ends up that for the
purpose of estimation, that's really the
only thing that we need and we're
going to be looking basically to solve
this equation. We're going to look for a
fixed point, basically, a value of this EV that goes both on the left-hand
side and in here, that equate if you want. The key problem here, instead of doing this, so this is all if
any of you seen, John Ross's' work
and Anarioss' work and in the mid '80s
and on patents. This is all almost
standard from there. The thing that's
complicated or difficult here is implicit in here
and that is the state. If you look at
these are the state that we have to solve for, at least at this level, this is going to be a vector of j inventories and prices here
and potentially anything. I just assumed prices are the only thing that's evolving. But if advertising or characteristics or anything
like that is evolving, that's going to be
in here as well and prices again at
the whole vector. It's only today just because
it's a first-order Markov. Actually, if it was a
higher-order Markov, I'd have to have
lags here as well. But even with this
first-order Markov, you can see I already have 2 times j state variables
and j could be very large. I have to figure out a way to reduce this dimensionality. We're going to do
this in two parts. We're going to reduce what
I call the holdings part, that's going to
be the inventory. We're going to make assumptions that are going to reduce this. Then we're going to make
assumptions that are going to reduce prices. Again, I'm going to deal
with these separately. Just to look a
little bit forward, later on, I'm going to solve, put up the exact same
problem for a durable goods. There the problems a little bit different but I'll show you in some sense it's exactly
the same problem of holding of the quality
of products that you hold and then your prices
or characteristic, your expectations
about the future. It's going to be exactly the same structure and there's
actually going to be parallel in both his literatures of how we reduce the dimension. To reduce holding, what we're going to make is the
following assumption. We're going to make
this assumption that the utility you get is just abusing notation a bit here because I'm
going to use CT and VT, just take the arrows off. What was once a vector
is now a scalar. Really all that it
is, the scalar is just a sum over the
components of the vector. What that's saying
is the utility, I don't care which
brand I consume. All I care about the
total consumption. Similarly for inventory,
my inventory cost, how many ounces is
that I'm holding? It's not which brand. The simplest way to think about this is, I'm at the store, I buy whatever brand I want and I come home and I just
pour it into a barrel. In that barrel is a mixture of whatever all the
different soft drinks that I ever bought, at that point it
becomes colorless. I can't actually
tell them apart. I'm going to store them
together and I'm going to consume them together. Remember if I go back to this here and that's
an important point, we have this component here. That's the usual thing that we had in the discrete
choice model. There is going to be
differentiation here. These are if you
want, all the x is really what we're going to
do for the application. These are going to
be advertising, that's going to be pricing. That's going to be kind of
all the differentiation but it's all going to be
the time of purchase. At the time of
purchase, I really care if I buy Tide or Cheer, if I'm looking at the
laundry detergent. But when I get home,
suddenly I don't care. It all goes together. Seemingly you can think, well, how you could've put
these two things together. The truth is, it's
actually a little bit hard because on one
hand you could say, well, this is a very
natural assumption. You could say, well,
do I really think that thinking the case
of laundry detergent, how much I use laundry detergent is going to depend on the brand? Probably not. There's whatever needs I have and that's what
I'm going to use. On the other hand, you could say in longitude is
actually well known, it's very highly differentiated. People really care
about their line. If you actually look at
individual level data, most people over a
period of two years buy at most three different
brands of laundry, most just by a single one. The question is, how do you
reconcile these two things? That's really the key issue. In the modeling you
have to confront that. What we're doing here is
we're going to say there's somehow this
advertising marketing. They get you to buy something at purchase but
when you get home, it doesn't really impact
how you do laundry. We could relax this somewhat
by thinking of segments. Rather than actually saying, I'm going to collapse
this to a single scalar, you could say, I could actually collapse this to a lower-dimensional vectors. If the thing I'm
looking at cereals, you could say, you know what? I have two or three
types of cereals, one for the adults
and one for the kids. Then within the
adult, if they don't really care if it's Cheerios, Corn Flakes or what it is
in terms of consumption, and the same for the kids. But I need to reduce that
dimension significantly otherwise this isn't
going to work. What we've done is gone
from now having to find this expected
value function, where we have these two vectors. At least now we have a
scalar and a vector. Actually, we can even
simplify the problem further, not just in terms of dimension. You can actually
show that already under the assumptions we made, how much you consume is only a function of
how much you bought, how much your inventory
is but not which brand. It's actually a
very short proof. What we can do is say, now this EV is still the
same state-space but before I was maximizing
over c, j, and x. Now, I'm only maximizing
over c and x. That's going to be
important later on when we want to simplify
the estimation. It's still the same dimension of the problem but it's actually a slightly
simpler problem because I'm maximizing over a
slightly different object and what's going
to fit in here is something that I still
haven't defined, is going to be this
inclusive value and I'll get back to
that in a second. Yes. MALE_1: You don't have a
constraint on the [inaudible]. Aviv Nevo: That's going to go
through the cost function. The cost function
could, for example, say that my cost is infinite
above a certain level. In the next model, we'll actually have a
similar constraint, actually not exactly that
but that's one way to enter. Now when we end, we've
done two things. There we reduce the dimension of the state-space but we've
also reduced the dimension of the problem and you'll see
that's going to potentially come and help us later
on for the estimation. Now, I'm going to try and reduce the state-space and prices. The key concept here, which I've actually already used
on the previous slide, and we've already seen
is this inclusive value. We've already seen
it but let me just repeat in case you
just walked in. The inclusive value if
these epsilons attribute i.i.d is defined
the following way. It's inclusive value from
a subset of options, and it's the expected utility from that subset of options. A key to note here is
that this is i specific. This inclusive value. It incorporates
both the product of the choice based but also the preferences of
the individual. Now, this is going
to be a natural way to reduce the state-space. Remember part of the promise of why I have to keep track of all these prices
and characteristics and how they're evolving, well, it ends up that if you want to know
what's going to be my utility in the future, if I'm thinking, well, do I buy today or
do I buy tomorrow? All I need to know is, what's my expected utility of the
storable goods, the products. But if you think of later,
we'll do it for durables, you might say, how
much your products are going to improve later on? How better are they going to be? It's a natural way to
reduce it because this is going to be my expected
utility tomorrow. It's i specific,
that's the key here. That's going to be
an important role, allows to reduce the things and it's going to
play two roles. First, it's going to
allow us to basically say that the expected utility depends only on the statistic. But in order to really
simplify the problem, we're going to have
to add an assumption which I'm just going to
add on the next slide, which is something about
the transition probability. I need both of these
assumptions that will be a reduce that I mentioned
on the problem. Yes. FEMALE_1: [inaudible]. Aviv Nevo: Absolutely
right. If there are segments, you're
absolutely right. You'll see in a
second, I haven't actually told you how
I'm going to do this, but that's exactly
what I would need. At least in terms
of the dimension. I'm just looking here. I thought I actually
had a discussion here, did I skip it? At this point, yes,
it can, sorry, at this point actually I
haven't made any assumptions. UI is still indexed by I, and it can depend on the
individual characteristics, not product attributes. Now, if I think that there is one potential product that's an attribute that's important, I could maybe try
to pull it out. I'd have to think how
that would work here. I've told you, if you're
going to do this in segments. Segment you can think
of as an attribute. The question, could I
allow for a continuous, single one-dimensional
continuous I might. But it's not obvious to me
right now that I could. Segment if it's something
discrete, yes, I could. If the curves discreet
like a segment or so I separate let's say, if it's detergent,
I could say, okay, there's detergent that I end
up what I mean asked you, but segment detergents
that are useful for colors or for whites then I
could separate that. But care a more continuous
characteristic, I might still be able to do, but if I were cooker on my feet, maybe I could figure out
how to do it right now, but I'm not a 100 percent
sure that we could. But for now that's that. The other thing by
the way, I actually thought I had a slide
later that discussed this, but maybe I'll end up here. The other thing to
think about, just let me say here about
the differentiation. One way to think about the differentiation and
one way in which you could actually have the
characteristics or the specifics to say, well, the utility here, you can think of it as
having two components. There is a linear component
and a non-linear component. Now, if I'm willing to assume that there's no discounting
or the discounting is minor, I could think of the linear
component being brand specific and then
it's actually a way to think about what these
xi's are capturing. The idea is that if there's a brand specific
utility that I get from consumption and there's
no discounting. The key here that
there's no discounting and that it's linear, then basically at
any point in time, if I buy 128 ounces of laundry, I can tell you "Here's how much utility I'm going to get." The reason I need that there's no discounting
is because doesn't matter when I consume it. B, it has to be linear, so it doesn't matter
how I consume it. It just a matter of 128 ounces give me
a certain number of utility and that's in some sense already captured in this xj. Indeed, if that's the case, what you'd want here, and I'll show you later when you
go to the estimates, what you'd want is not xjxt, but what you want is
x times xjt or xij. You basically wanted
to say if I buy a product that's
double in quantity, it gives me double the
utility under this model. I'll actually, I'll
show you later, we can actually impose that later when we go
to the estimation. But again, to do this, I have to have no discounting and I already have this already. I basically say
that's not exact. It's an it's an approximation. I couldn't wave my hands
a little bit and say, that's what I'm trying
to capture here, that there is a non-linear
component of utility, that's not brand specific, but the part of the brand
specific is is linear. I can generalize in that way. I think the nice
thing about that, it actually then
tries to explain us we increase in
these products, what this xi? Where
is it coming from? Or how is it related to? Let me go back here. I was basically at the
point of saying, okay, we're going to make
this assumption about the inclusive value and it's going to help
us in two ways. Now we can say the
expected utility depends only on that and something was already in the
previous slide. If I add this assumption A2, which basically says
that transition. If I say "Okay, all I
need to do is to compute my utility tomorrow is I need to know what the
inclusive value is, what the expected utility is." Now, all I need to assume now that distribution conditional on the whole vector of prices is the same as if
I only condition on today's inclusive value. In other words, if I
take two vectors of prices that are different, but yield the same
inclusive value, the transition to next period
is going to be the same. Now, that's a real assumption. It's going to be some
byte luckily though. By the way, just strong
assumption for two reasons first is the first-order
Markov assumption. You could say maybe at
the higher-order markup. That I can actually relax
by just adding more lags. Even in the original thing
at j should be pt and pt minus 1 and there should
be higher orders. But the other
components fact just, all I need is this
lower-dimensional vector. Now it ends up and
I'll show you later, if you're willing to
buy my assumption A3 that we'll get to, I can actually do
this in some sense offline and I can
actually test it. I can actually play
around and see what do I need to predict these
different transitions and throw different
things that I can actually relax this a little bit by throwing largest
inclusive value I'm going to have a
few other things. By the way, there's
inclusive value here is a vector because there's
an occlusive value, one for each size. There isn't a single
one, but there's one for each size basically of that. The bottom line is, so I went basically from
the fact that I had 2j dimensional vectors, to now I have a scalar
plus a vector here, that's the number of sizes. The number of sizes are
going to be a much smaller. They're going to be
an example we look, it's going to be about
three or four dimensional. It's still a fairly
large state-space, of basically having
four variables in it, but it's much larger and
much more manageable. Let me talk a little bit about data identification and
then when I talk about estimation and introduced
another assumption that's going to even potentially simplify
things further. The data that we're
going to work with is consumer level data. It's a little bit actually
similar to the homes can aid, although it's actually a
different type of data. It's again, it's consumers
that are followed over time. The advantage of this
data is you also have the store level data
with it you can actually match up with prices. But again, these are
scanner data that people tell you
everything that they buy. I'm not going to say a
lot about identification. I will say that there's
no formal proof. We think we're
formally identified, but we actually haven't sat down and work through the details. Informally, the
way you can think about this is what
the parameters are identified from and here
it's really key to have the individual
level parameters is basically from
sequence of purchases. For example, suppose I see
two households that buy, and I see them over
a certain period of time and they buy the same total quantity. But suppose that the
average duration between purchases is different. Suppose, let just, for sake of argument,
assume that they buy very regular intervals. One of them buys every two weeks an buys every six weeks. A key here, is to hold the
total quantity constant. What I can infer from that is that the first
household that buys every two weeks has a
higher storage cost. That's basically what's going to identify the storage costs, is that you want if the average holding
the quantity constant, looking at the average
duration between purchases. Similarly, what's going
identify the utility function? That's going to be again,
a similar exercise looking not at the
average duration, but the variation in duration. Again, it can make
a similar argument. Now, all of this again, I know it very hand-wavy, but that's basically
the idea behind. Let me just say a word
about pricing arginine because we've talked
so much about it. Basically, what we're going to assume here is
really this xijxt, this unobserved
component of the brand, it's going to be xj or jx. The idea is that we're
going to pick it up with a fixed effect. We're going to have its
individual level data. We're going to throw that
in there and then resume or whatever additional
variation just because of the sampling that
we have, the individual. Or if I want to impose this assumption that
I was talking before, I'll actually say
it's x times cj. But the idea is that
this is going to control for all the
price endogeneity. It's not that there isn't
branded endogeneity, just that I'm
controlling for it by these brand size
specific fixed effects. The other thing that's
actually going to be very important to see any
variation and this overtime and that's
actually going to be very important to include
feature and display. Feature and display for those of you, this is going to
be scattered data. Those of you don't
know most Canada have some measure of whether the product was either
featured and you know, those little bulletins
that you get home. Or was it display
differently in the store. Those things are
actually very important. They really change how much is purchased and they
are correlated with prices sales and to
happen more when these things are featured and displayed and we want
to control for that. I think it'd be very important and I'll talk about later. In principle, you could say, well, could we nest in here, some BOP inversion or think of Orioles lecture
just before lunch, where we talk how to estimate static demand models
with consumer data, in principle, you could, but it's going to be very hard. Think of it as a price index, that's really all that this is. The inclusive value, it's
the expected utility, but it's a price index
that's evolving over time. Think of that just
like a single product, what I want to know, is
what's the probability of the sale tomorrow. I'm going to decide
if I buy today, what's the probably
of a sale tomorrow? That's what this is. MALE_2: Do that suggest
that the characteristics of the product are
changing over time? Aviv Nevo: They could,
but not in principle. MALE_2: Seems the way you've
set it out [inaudible] Aviv Nevo: No the way
that I've set it up, the thing that do
change over time here, these variables are E_jt, which in principle could
be characteristics and for the type of products I'm going to talk about
here, they don't. MALE_2: [inaudible]. Aviv Nevo: No. The main
change could be through here. But these hear what they're trying to capture
is you're going to try and capture the
advertising and display variable. Those are the only things
that are going to change. The other things,
these Psi jt's, I'm going to say that the
characteristics of the brand, they're going to be
picked up by brand fixed effect and in some sense, that's really all that there is. Now, you'll see later on when you go
to other products like camcorders or TVs or stuff like that there you'd want the characteristics to change. But we're going to use
the same structure there, but that's going to be
in a different setting. MALE_3: [inaudible]. Aviv Nevo: I am going to have individual parameters
that I'm going to. This is all going to be parametric distribution
of indigeneity. Even though at this point
everything is indexed by I. But of course you don't want to, I should say, of course, but I am going to assume
some parametric distribution of these and I want to
estimate its parameters. I am going to
basically learn about. I'm going to look at
your time series, but I'm inclusively going to learn about you just from that. I'm actually going
to be doing some pooling across individuals. MALE_3: [inaudible] Aviv Nevo: Oh, these are
product, sorry, I mean, it wasn't clear that these are product size fixed effects. It's basically either variant, the J and X level. These are not in an
individual level. These are not individual
fixed effect. These are going to
be product level. Well, for sure need a very long panel and I'm
not sure that long ago. I don't know. I find
this hard enough to do, but yeah, I'll wait
here while you do it. I might be waiting for awhile. How are we going to
estimate the model? I'm just going to sketch it out, but hopefully you've
seen enough of these to at least get
the flavor of it. We're going to follow what
what John Ross as labeled as the nested algorithm and
it's very similar to the ideas that we've done all that kind of
a different approach. There's going to be a search over parameters and within it, there's going to be a
procedure that's nice. Just like in the BOP, we search over parameters
within an inversion inside. The only thing is here, it's not going to
be an inversion. We're just going to solve a
dynamic programming problem. The idea is, I'm going to guess a value of the parameters. For that I'm going to solve a dynamics problem.
What does it mean? I'm going to look basically for a fixed point in that Bellman equation
that I had up there. I'm going to say the EV
that on the left has to equal to the EV that
inside the integral. I'm going to basically look
for a way to solve it. There's various computational
methods of how to do that, that I'm not going to be
talking about it all. Then I'm going to
basically search for a parameter that maximizes the likelihood of
the data, that does. That's the original
sort of arresting. The truth is we probably
should have done this, this is building up comment
that Ario made earlier. We probably could have done
this within a method of moments instead of a
likelihood framework, but it just set it up
originally as I like. That's the original rust idea, we actually have to deal
with two potential issues. One is the fact that one of our policy variables and one of our state variables
are actually unobserved. Because remember, we
choose consumption, we don't actually see
consumption in the data. Inventory was a state variable, I don't actually see inventory. The way we're going to
deal with that is we're basically going to
say we're going to start with initial values, initial guess for inventory, and then solve the model
and then given the model, you could actually say, well, what's the optimal consumption? Use that to update and then basically create a
sequence if you want. It's as part of the procedure we're
basically saying, okay, what's the optimal
prediction of the model? We're going to use that to
update the state level. This is all conditional
on a guess. Now one way you say, well, how do I know that
initial guess? Well, we tried a bunch of
things to deal with this. Initially what we said, it will just start
a guess and let the model run for
a few periods and not used as initial period for estimation and then can only use for a following periods. We do that at the end and
we tried different guesses. It ends up actually, the initial guess doesn't really matter that much once you let
it run for a few periods. Because basically what happens is very quickly in some sense, the model takes you because
consumption is endogenous, takes you to where
it wants to be. It basically if you start with an inventory that's too high, it basically you have these very high consumption.
You can think of it. It just basically
you're throwing out in laundry detergent down the drain to bring it
to reasonable levels. If you're starting for too
low you just need to get a few purchases until you build up an inventory
that's reasonable. It really didn't matter as long as it was in
reasonable levels, what initial
inventory we started, the results really weren't
sensitive to them. There's alternative things
you could do recently, there's this EM algorithm, Acuna and Miller proposed
could be tried to used here, although I don't know that
anyone's actually done that. The final simplification
we're going to do, this still is actually quite a complicated model to estimate, especially if you
want to have rich and a lot of parameters in it. What we're going to
do is we're going to try and split the likelihood. The likely what we're going
to try to predict this, which brand you choose and then which size you're going to choose. Which
brand or tangent. Think of a size here, I'm
going to think of which box? We got to think of
how many boxes? Because it could be
that basically that the dimension which
long you store. What we're going to think
of is which size because in the data we're going to look at that's going to be the key. We're going to split,
and I'll show you in a second, what does justification. We're going to split it into
three steps estimation. The first is going to be a
static conditional logit. It's going to be a logit at the individual level data with all the demographics and
everything in there. It's the type of logit that Ario showed us when you're
looking at the microbial P. If at the micro-level with all the
demographics in there, and it's going to be
conditional on a given size. Basically, what I'm going to look at is I'm going to look, what size you bought
and in that size, I'm going to say now you're only considering options
of that size. I'll show you in a second
how I justify that. But I'm just telling you
now what we do, right? I'm going to estimate
that conditional logit. I'm going to look at what you bought and then I'm
just going to look at which brand new you bought
conditional on the side, you only consider
options of that size. That's going to be
four-step and it's going to end up that
I'm going to be able to estimate almost all the
parameters that way, that kind of advertising and display and all these fixed effects and everything that way. Then I'm going to use that to estimate this inclusive value. That's what I need to compute; I need all these
parameters to compute that and then I can estimate the
transition. This is static. It's very easy to do it,
bit hidden state of. This, again, I can
actually once I compute these inclusive values, again, I can
actually do outside. This is where I can
explore to say, you don't like this particular
restriction that I have, I can throw higher-order lags in here to see if
they help predict, I can actually try to add
more variables in it, and it ends up actually they had very little predictive power. But it's actually very simple to add those as an exploration. Then finally, I'm going to do this dynamic problem
that I showed you. But this is where
what I'm going to do, the dynamic problem
is only going to be at the choice of the size; so it's whether you
purchased or not and then what size you purchase. It's not which brand, but just the quantity decision. That's what we're going
to try to do here. Now, I think this
actually captures a very intuitive idea in which you think
that the dynamics, the really important dimension
is how much you buy, and the brand dynamics, at least in this problem, the brand that's not
really a dynamic decision. I will now formally
show you what we need to assume beyond
what we've already assumed for this to work
and is the following; we basically have to put restrictions on the
unobserved heterogeneity. Basically, we have to assume
that the distribution of these Alpha i's and Beta i's, these are all these
parameters of utility; they're
random coefficients. The utility we're going
to have to assume that their distribution is the same whether I condition on
the size purchased or not. In other ways, what I
want is to say that which size I purchase
does not give me any information about
the distribution of these random
coefficients because if it does I have a
selection problem. The simplest way to
deal with this is to say there are no
random coefficients; it's basically all
observed demographics. There are none using all those notations or
all these z's and none of these news or using minor demographics that are observed and non of
the unobserved stuff. But remember, I
can actually have a very rich model
and in principle. Some of what we do,
and this is actually goes back to your question. In principle what we're going to do we're actually going to have household brand fixed effects, and we're going to allude
to some idea that we have a long enough panel that we can actually
estimate that. We can do that because we've
made this assumption. Yes. I'll get back to the
effect of price here. Implicit in here, it's
not just my price it's my expectation
about future price so that's really the
effect of prices. Let me ask you to
put that aside. I can talk to you about later, but the key here issue
is the fact that the dynamics are going
to enter in this step, so the effective price or the dynamic price where
it's going to matter, it's going to matter
for how much I choose. But it ends up that if you
buy my assumptions is that basically the conditional
on the sides. Now you've decided,
I'm going to buy 50 ounces of detergent
or I'm going to buy, whatever, 24 rolls
of toilet paper. Which brand you choose, that's going be a
static decision if you buy my assumptions. The dynamics are
only going to enter and the effective price
is only going to enter in this choice in this
part of the problem. The way it's going to
enter here is because I have an expectation
about future price. I haven't really defined
ineffective price here yet and I'm not going
to in this model, it's going to show up
in the next model. Here it's implicit. But here the key is the fact
that I can really separate, so this decision it's not dynamic which I think
is an intuitive idea, although you might
not necessarily like the assumptions
that I have to do to in order to get to it. Anyway, once I have this additional assumption which obviously restricts the
unobserved heterogeneity, I can actually estimate this first step;
have this split. Really, there's a big
trade-off here between speed. This is not just speed
in the sense of, I can save few minutes
on my computation, it's also because I can
have a much richer model because if I wanted to solve
the full model there's only so many
variables that I can actually maximize over just because it's REL said yesterday, it's exponential on the cost. While here I can
have a model with actually hundreds of parameters, including all these
fixed effects and various other variables, so I can make it a
much richer model on the observed part, but I have to give up
on the unobserved part. That's the trade-off
that we're facing. Now, the truth is, again, we can estimate it without
this assumption, but we'd be somewhat limited. Let me just quickly
show you a few results. I'm not going to show
you the full thing. Yes. MALE_4: [inaudible]. Aviv Nevo: I am
estimating it separately. MALE_4: [inaudible]. Aviv Nevo: That's
actually the one slide I didn't put in here. It's a bit hard to say
what it looks like. I can't tell you,
I forget exactly. If I look each of these inclusive values I'm
going to have four of these; three or four for
each of the sizes, and so I regress each one
of them on the other. Remember, the inclusive
values are going to be individual specific. The process I'm going to
do some pooling across; I'm going to divide
the household based on their characteristics
to six different bins, depending on whether they
live in an urban area, suburban area, and their sizes. I'm going to have a different
process for each of these; so it's about 50, 60
households per each of these. Just regressing that,
just a linear regression of the inclusive value, tomorrow is a function of
the inclusive values today; those three or four. I get an R-squared of
about 60, 70 percent. Throwing in additional lags adds very little
explanatory power. MALE_5: [inaudible]. Aviv Nevo: It enters
through the Omega. Basically what it
means is it's utility so you can think of it, if it was just a single product, I would literally get
that sales factor. But it's basically
smooths it out because it's aggregating over
a bunch of products. Basically, if you think, you don't see it as much
in the time series, but you base time
series you see is that there's periods
with nothing's on sale and then there's periods when something else on sale. If it actually ends
up, let's say, it's not your products on sale, a product you don't care about, your Omega isn't going to go up. But if it's a product that, let's say Bob does care about, it will go up. It's basically a smooth
version of that, that's the way that
I think of it. It's not really a price,
it's a utility index because I never
convert it to dollars. But that's really what
it's trying to see. It's trying to tell you,
what's your expected utility tomorrow from this whole
bundle of goods. Yes. MALE_6: The inclusive
value here is [inaudible]. But if you move this
[inaudible] do you get the same stem that this
is going to go [inaudible] Aviv Nevo: I agree with you that there is going to be an issue. I think a big issue
of using all of this, forget for a second
the estimation, is the fact that if we want to use this
for merger cycle, oh, here's the bias
and the man estimates. But in some sense, if I'm
going to plug this into a first-order condition coming out of a static pricing and say, well, that doesn't make sense. In some sense, I need to
solve the dynamic supply. That's exactly our
motivation in this, what I call the
simple demand model. I'm going to get to that next. But you're absolutely
right in the sense that the next slide is
going to be about biases. But I'm going to tell you what that does if you were
to do merger analysis, but it's merger analysis in a static framework and you say, well, why should
I do static here? I'll get to that
in just a second, but it's a very good question. Let me just show you here. This is the first step of
the estimation. Which brand? I choose conditional on size. Remember because this is static, I mean, I can throw the
kitchen sink into this really. The restriction now is the data, it's not the computation. I'll just build up gradually and try to show. Here we just ran. This is essentially just price and you get a certain
price coefficient. Now what we're going to add
is brand dummy variables. Once again, this
whole issue that once you go to
individual-level data, there's still some endogeneity and you want to control for brand and you see it goes
in the way that we expect. The price coefficient
double in absolute value. The thing that we
do next though, is we actually are
going to put feature and display in here still with brand dummies and now the price coefficient
actually goes down. Actually interesting, it ends up going
down exactly where the maximum likelihood was without actually these
additional controls. The reason this happens it's a little bit
counter intuitive, but it's something
that we forgot that actually endogeneity can
push you in different ways. Really what happens here is
whenever there's a sale, they also promote it a lot. The question is
is it coming from the price reduction or is it coming from the
advertising effect? To do that, you need to
control the basic, say, I want to look at the
cases where there was only a price reduction
without advertising. Once you do that, you actually
get a lower response. When I didn't control for it, everything was loaded on price. But it ends up that once
you control for it and this is where it's important
because if we wanted to actually do
some dynamic model, so other people have proposed a similar problem for this or similar models for this where they didn't do the split, but they couldn't actually add, for example, feature
and display. That's going to be almost more important than
everything else that we do. It's intuitive. It makes a lot of sense that if advertising is correlated, you want to separate the
advertising effect from the price effect. That's
going to be important. The rest of these columns, they explore various
things like adding demographics and you see
these next columns do. These are brand dummy variables
just at the brand level. Now I'm going to
start interacting them with individual
characteristics. Going back actually to your
question of heterogeneities. First I just interacted
with demographics. I forget what demographics
we put in here. You see that doesn't do much in terms of the coefficients
that we care about. But of course, there will
be endogeneity here. Next, we're going
to try and actually interact it with size
to get it this effect of things and you can
see the effect here. Then we do a whole
bunch to basically, we get to the point
where we're looking at brand household dummy variable. We look for each household
because they're choosing a very small number of brands
over this time period. It's actually not that many
fixed effects per household. We actually have a weekly
data for two years. We have a fairly long panel, so we can actually do that. That's basically
what we do here. From here we compute
the inclusive value and then we estimate
this process, which I probably
should have shown you what it looks like, but it basically looks
like a smooth version of the prices. You don't get as much of the sale or the
jumps but there is that similar pattern of
things evolving over time. Let me just show you one
set of numbers here. Actually, I cut and paste this table and realized
it comes out funny. What this table shows you, this is the ratio of the short-run elastic
or natural run. The elasticity is computed
from the static model. If I basically just did the
same thing that I did in the previous table but now included all the choices and got a price
coefficient from that, divided by the
long-run elasticity that you get from
the dynamic model. What we have here is we have, it's 428 ounces of laundry detergent over
different brands. Here what's cut off in the
different columns again, are different sizes
and different brands. You can see in each column there's the one number
that's over one. That's the own price
elasticity for that size of that brand. You can see 123, 142, 1.2. Here it's actually 0.9, 144, and 129 for
the private label. Roughly if you average that, it means that it's about
30 percent higher. The estimates from
the static demand are about 30 percent higher than those from the long-run estimates from the dynamic. That's the own price. MALE_7: [inaudible] Aviv Nevo: Yes. Exactly.
I basically say let's change price and
keep it [inaudible]. The way we did it in the model is we said whatever
the price process was, whatever the sales were, we kept that same trend and
we just shifted it all up, let's say by 10 cents. We just kept that whole trend and there's different
ways you can do that. Maybe they make different things but we thought that was the
most reasonable way to do it. FEMALE_2: [inaudible]. Aviv Nevo: Let me get it,
maybe in a second to that. The other thing that we have, these are all the
cross-price elasticity. You can see they're
actually way below one. I mean, on average there
are about, I don't know. I mean, just roughly looking
about 20 percent or so. You actually see the
larger effect is the one on the cross-price
and not on the own price. Now if you actually
wanted to use this like let's say for
mergers and going back to your question, just plug it into
a static demand. You can say, well,
why should I plug in a static pricing equation? That's because that's
what we know how to do. That's the best answer I have. Not a good one, but
that's the best I have. They both would work in the same way because
if you think of diversion ratio is usually
what we care about. I mean, they're both
going diversion ratio is the cross over the own. Basically what it
will tell you is that static models would think that the competition is much
less than it really is. In principle and really the
only cross that's actually higher but I didn't emphasize
is the outside good. It makes sense because people aren't buying
that frequently. They typically buy every
five or six weeks. There's fairly high
substitution in the static model to it
but the dynamic model, one could even say
in laundry detergent there is no substitution
to the outside good. It's not like I'm going to
stop buying or buy less. You might buy a little
bit less laundry, but not that much. That's really where
it's all coming from. The static saying there's
a lot more substitution to the outside good than
the dynamic model. Dynamic model say
products are much closer in characteristics
space or in competition, therefore, the effect
of a merger in this static framework if you
just look at that price, will actually be
much higher. Okay? That's this more complex model. Let me just quickly go through the simple model and I think it's
late in the day, so I might not have as much time to go through
the full detail, but just give you a
little bit of the flavor. Part of it is exactly answering the type of questions
you want it. Let me just set up the
motivation and then talk about the specific you want. I think what we've
learned is neglecting demand dynamics it's important and could use, I'm
putting results. But the estimation even after all these simplification stuff was actually quite complex. This is something that took us a while to do. We need to do. At the end of the
day, I still had five-dimensional
state-space and I still thought it was really pushing that the levels
of what we could do. Required consumer level panel, which a lot of times
it's just not there. We talked about it whenever you have it, It's great to use it, but sometimes you
just don't have the consumer-level data. Maybe most importantly it's very difficult to derive
the supply model. To say, okay, we
have this demand, well now we want to close it. But I can go through this, but I'm not just even
formulating what the right state variables are in that model is
actually quite hard. There's a couple of ways
we can go about it. We can say, okay,
maybe we shouldn't go to assuming everyone
knows everything. But the usual way
we think about it, the full information case, you'd want to actually
have some distribution of inventories that
you know about. It gets very complicated
very quickly. We wanted to know, is there a simpler model
that will allow us to do it. Especially you say,
it's one thing if what you want us to write a paper
about demand estimation, right, then you
probably want to do something like that
previous paper. But suppose what you want is
you want to look at supply, or suppose it's
actually just an input into some other problem that you're really interested in. Then maybe you don't necessarily have the
time or the effort. We were looking at,
say, are there are simpler ways to look at it. The simplest things
that we explore, we're saying suppose
you actually aggregate, let's say over time. With this weekly
data, I understand, but I used to claim this
for my job market paper, the serial paper, I used to
say, it's quarterly data. You don't have to worry about storability and I believed it. But if you think about it that it's hard to actually
show that formally. When you're working with
non-linear functions that clear that this
averaging really helps. We thought, let's
try to explore that. That's what we tried to
do in this paper that's joint with eagle handle and
we explored various things. we explored
aggregation over time. We explored various
throwing in various lags of prices and quantities and we ended up with
what we have now, which is some form of
adding lags in the model, but it's actually
more model-driven. Let me try to take you forward. Now, the model I'm going
to propose to you, it's going to have
even in some ways, even stronger assumptions
than what we had before. But the key to remember
is not like before was model-free or
assumptions-free. There were a lot of
assumptions in here. The question is, how much stronger are the
ones that are in here? The key is going to be the rest, we're actually
going to be able to estimate it with aggregate
data and hopefully, I'll convince you
that it's actually reasonably well identified
under our assumptions. The key to this is going to
be the storage technology. You'll see what we're going
to say instead of you have physical units
that you can store, it goes a little
bit to the ADSL. Is there a cap on how
much you can store? What we're going to put
here is we're going to put a cap on the number of
periods that you can store. Which in some sense
the funny assumption, but it's going to bias a lot. At least if you're going to
make a strong assumption, at least make sure you
get something for it. Hopefully, we're
going to get enough here that you're willing
to live with them. As a result, what
happens is going to make the supply side tractable, I'm not going to have any
results from supply model, but actually, that's
what this paper does. We've actually now
solved the supply side. We can show you what
that looks like. Let me just outline the model. There's a lot going on here, even though it's a simple model. There's nothing complicated but just a different way of thinking that it always takes a
while to understand. We're going to make
basically three assumptions on heterogeneity, on storage and
about expectations, which effectively
we had also before. On heterogeneity, we're going to make the following
assumption. We're going to assume
there's going to two consumer types and the simplest version of
the model assumes that there's a proportion of them, Omega that just does not store. There's a proportion
1 minus Omega that stores if they find
it profitable, but at least consider
storing and there's a proportionate Omega
that just never stores. Think that their
costs, they live in studio apartments in New
York, just can't store. That's the way to think of it. Now, ideally, what you'd
like is you'd like to have this endogenous
depending on the prices. But for now we're just
going to take this as an exogenous variable.
That's assumption 1. Assumption 2, which I
think is the critical one, is the fact that inventory
is going to be free. There's not going to be a
cost of storing inventory, but it can last for
only T periods. It's free until it expires. If you think of
perishable products, that really fits
this assumption. But we're going to make
this assumption for Coke that has a shelf life of, I don't know, two, three years. Not as long as Twinkies, but longer than what
we're going to have here. But we're going to make
this assumption that it's only going to store and
T here is going to be, for a lot of our work
it's going to be one. One period. You can buy today and you have to
consume it tomorrow. We can extend it for
longer t's, but you know, at some point it actually
becomes non-tractable. It's a very strong
assumption if there's one that you should
object to, it's this one. But I'll show you what we're going to get from it and
we're getting it a lot. It's going to really almost borderline trivialize
the problem. Then the third is we're going to make an assumption
about expectation. The simplest one I'm going
to work with is about, it's a perfect foresight
of future prices. I can also work with
rational expectations model. It gets a little bit
more complicated, but it's still workable. A lot of people actually
object to this more than the second one, but
actually it ends up, this one is the one
we can relax and that's why I'm actually
emphasizing the second one because I really do
think that's the one that I can relax
almost anything. If you don't let me have that maybe I can fudge
it a little bit, that's where things are
really going to fall apart. Let me just go
through this quickly. Each type of consumers, either those that store or
those that don't store, have a separate
utility function. It's again, it's a quasi-linear. Notice here that
it's indexed by T so later when you go to the
empirical estimation, that's where the error
term is going to be. I'm going to parameterize
something that content plus an error term
to the demand. Absence storage. If
we couldn't store, there was no reason to store, these are going to be what I'm going to call the
consumption function. If you want the static demand , be a little bit
thought of it carefully. If there is no storage, this is what the demand functions
would look like. If Q is going to be
the consumption. Now I'm going to denote
our purchases by x. Remember I have to
separate between the two. What are the purchasing
patterns look like? Here you'll see this is the
power of this assumption. Lets me do it under
T equals one, which means it's going to be
weekly so I can buy today, and whatever I buy today, I can either consume today
or consume tomorrow, but after that it perishes. I have to consume it. It ends up that, and I'm
going to do this, let's say for, let's start with a single product and I'll
see how we add others. This is where the effective
price is going to come in. With a single product, I basically have four states. Then I'm going to denote SS, NN, NS, and SN. There's a typo there. Where
the first letter refers to whether there was a
sale or not yesterday. N is no sale, S is sale and the second one is to
whether they want today. This is no sale
yesterday, no sale today. What am I going to purchase? The non-stores, they're
always just going to purchase the static demand. That's easy. But how
about the stores? Well, the stores they basically in each period
they're thinking, do I buy for today and
do I buy for tomorrow? Each of these states, that's
going to be different. Let's start with
these two states where there was no
sale yesterday. In both those periods,
I'm going to buy for today so that's this component. In this case, there's
no sale today, so I'm only going
to buy for today. No need to buy for tomorrow because there's no
sale. Why should I buy? Similarly here in this
state there's a sale today, so I'm going to
buy for tomorrow. Now, we have in this model
there's perfect foresights. You might ask, well, if I
know there's a sale tomorrow, why don't I buy
wait till tomorrow. We're just going to add if you want a tiny
little bit of noise or a tiebreaker because I'm indifferent and
I'm going to say, if there's a sale today, even if I expect that
there's still tomorrow, I still buy today just on the off chance that maybe I
can make it to the store. In both of these periods, or both of these states I'm
going to buy for today, in this day I'm also going
to buy for tomorrow. How about the states where
there was a sale yesterday? Well, if there was a sale yesterday I already
bought for today, so I don't need
to buy for today. That's these two zeros. If it's not a sale today, I'm not going to buy
it all because I'm not going to buy for tomorrow. If there is a sale today,
I will buy for tomorrow. That's the consumption
think of code. How does the price of
Pepsi enter into here? Well, here what I have is the effective price
and this goes back. The effective price
of Pepsi is basically the minimum of Pepsi's price yesterday and prices price
today or price at T minus 1 and T. Note that the effective price
is a little bit different in both
of these cases. When I buy let's say for today, the effective price of Pepsi is either today's price or yesterday's price because
it could have been that Pepsi was on sale and I bought yesterday for Pepsi
or it could mean today. But basically the
effect of price, the fact that I'm
taking the minimum here takes care of that. When I'm buying for tomorrow, the effective price is due today or tomorrow's price of Pepsi. What's nice here is that A I get this very simple structure
of what I buy today. But it also gives me this idea of how do I take care of
all the other products. It's all through this
effective price. I'm going through
this quickly, but I think you'd all agree
this is significantly simpler than the
Bellman equations that I had up there before. Let me just quickly
say something about the assumptions and then I'll try to conclude because
I have 50 more minutes. I think we're
getting a bit late. It's a bit heavy for
a quarter to five. A_1, this was this
assumption of splitting. Again, you can see it as an assumption on the storage
costs and principle, we could actually make this
a fraction of the price P, and then it's more endogenous. We could do that. The issue
is we want a simple model. It's very easy to
make this model more complicated and you have to ask, is it worth that complication? The storage technology, that's
something we really need. But as you saw it really allows us to simplify the
state-space because there's no leftovers to carry and it really detaches through
this effective price. The other product.
It's easier to allow for larger T and it's
also easier to allow. This goes back to basically A_1. Here I have stores
and on stores, I could also have people
that store for zero period. Those are non-stores.
Those the store for one, for two, for three periods. I can actually allow for heterogeneity along
those I mentioned, especially if it's
fixed proportions. Then in terms of the A_3, perfect force is much
easier to work with. But it ends up, I'm not
going to go through it, but rational expectations
is doable as well. Okay. I'm going to skip this, but if you can ask,
how are we going to estimate this from
aggregate data? It ends up that if
you can go through the accounting of the states and that's what I
go through here. It'll actually allows you
to recover these actually nonparametrically and I can go through this if anyone's
interested to come up later. But really, once you start
thinking of these stage, you can really see there is one stage where only the store's purchase
and I basically, the difference between
those two states recovers that and
then I unraveled. It's actually, we don't
have a formal proof, but actually here I think the formal proof is not
that hard to write. We basically, we talked through it in words,
in the paper. I really do think the
model here is actually nonparametrically identified, which I'm actually not sure. There's a nonparametrically
identified we store level data. Actually in some sense that's over identified if there is some overlap in what's
considered a sales price. Certainly I don't
want to go into, but there's actually
different prices can be sales or not
depending on what your expectation is
about the future and in that range actually the
models even over identified. Okay, and the same
idea extends for more products than
with p equals 1. We estimate this basically, it's nonlinear estimation for things that I'm not
going to go into, but this is actually
all done in STATA. We're going to estimate
this using OLS, but you could also
use IVS on it, right? It's basically the whole
work here is how to formulate the demand model. Here, by the way, when
I say we estimate, we're actually going to have
a very simple demand model in product space, right? Just going to be a log
linear demand model of just, Coke, Pepsi and
the generic brand. But we could in print, we're
actually working on this, extending this to
a BOP framework. But we're not going to do this. Just quickly demand for Cola's, we estimate this
using scanner data. This is the regression
that we look at. I'm not going to dwell
on these numbers. The key thing and partly what
we want to show here is, and this is to feed this into supply side is we want to
actually show that that is true genuinely
between the stores and unsorted goes with
the motives for sale, and it ends up that the
stores are significantly more price sensitive and that's the key for wanting to
price discriminate, right? It ends up that the stores are more price sensitive so
you actually want the skim them out and the demand
model lines up with that. Indeed, once we put it
through the profit, it actually is profitable to do. I went through this quickly, but just to outline, let me just spend
literally five minutes or even less on durable goods. It ends up that durable goods and that they're very
similar to storable goods. Although the problems that arise in these markets
are very different. But I don t think
it's because of the nature of the product. It's because of the nature of
the pricing that you face. In storable goods, what you see is you see this temporary
price reduction. In a durable goods what you see, and this is actually something I'll talk about at
the end of the day. Yesterday in many
of these things, what you see is declining
prices over time. The real problem is, do I buy today or
wait till tomorrow when the quality adjusted
prices are going to be higher. Because I'm going to
get either lower prices or higher quality or both. That's the real
dynamic issue here. But in terms of actually
the modeling and ends up, there's almost a one
to one modeling. In principle, we could
have actually written this in one framework and I'll try to show you
that in a little bit. The implications for
demand estimation. It really depends
whether you have repeat purchases or not. If there's no repeat purchase, so if I buy a durable good
that I'm off the market, then the problem is
that the static model, you need to account
for the change in distribution of consumers. The idea is that you're getting off some
of the consumers, probably the least
price sensitive ones. If the price is declining, you're getting them
off the market. That's what would go
wrong if you just did a static demand
estimate is the fact that your distribution of its rigidity, you're
holding constant. But unless there's refreshment and it has to be refreshment
of exactly the right type. You're actually going
to be skimming off. There is a selection
problem, as you move along. The other thing that
you're missing in the stack modesty is the
option value of waiting. You're missing that
because you might not be buying today
because you know what, I'll wait for tomorrow on the static model has no
way of capturing them. With repeated purchase
so if what you're saying is consumers buy and then they're back on the market. It's still the distribution
stays constant. But what you're not accounting
for this expectations. But maybe more importantly
is the fact that the outside good is changing. What can happen is
think of a new car. You could say, well,
I'm on the market every year for new car, but my outside option if I just bought a new
car is pretty high. You have to give
me a really good, either a really great option or really good price to
get me to drop that. That missed in the static
models that we have. What are the examples? I'm going to skip
this, but you can actually add this as
a simple example. We say there's just a
uniform willingness to pay and prices just decline in a
uniform way along this and static demand
is just conflict. You're basically selling in a constant number of units
all the time and you think, well, you don't
care about price, but in reality you do. Let me just skip that. Basically, if you want, this is the the problem
that you have here. Let me just see, it's basically the whole issue
is the fact that, you're outside option
is changing, right? Because that's what you
purchased last time and that's the main thing
that the changing. It ends up that in terms of
reducing the state space, you can do the same type of
things that we did before. The key difference here is that here when you do this
inclusive value, you actually have
to define what I'm calling the dynamic
inclusive value. What I mean by that is the dynamic right before
it was just a price index, just basically the utility. Now, in order for repeat
purchase for this to work, you need to also include
the expected value. Now it's not just the price
index is actually has an endogenous component of the consumer's decision problem. Even though you could
say, it's the same type of assumption about
the inclusive value. I think it's actually too
much stronger assumption because before we
were just making an assumption about the
evolution of supply side here, built into this inclusive value is some dynamic behavior
of the consumer. But if you're
willing to buy that, the restrictions are
actually similar? Yes. MALE_8: [inaudible]. Aviv Nevo: Yeah.
I'm happy to buy that because I've worked on the storable side,
but I actually, just for fairness, I think we have a similar problem
to storable because they might want to game
it as well and put products on sale and
coordinate and stuff. Yeah, but I agree. But the main thing that I think differs here
is the fact that this inclusive value also included the optimal
behavior of the consumer. Then you think,
what does it mean to now assume it involves a certainty according to some exogenous process
like we did before. Yeah. No, I agree. But anyway, so that's in this literature, but otherwise it's actually the similar of the restrictions of the holdings that we had
before the inventories. The similar here is
the fact that you're actually even when there
is repeat purchases, you're only actually
holding a single product. When I repurchase, I basically
throw away the old one. Because otherwise I'd have
to have a whole vector. You think, of cars, I'd want to have a whole vector of what are the cars that I hold as opposed to I just
hold a single car. There's a similar
assumption here. That's the sense in which the problem is actually
a very similar. I think what's
different is really the price process that you face and that changes
the dynamics. I think in the storable goods, if there were
constantly evolving, you can almost
think of it as like a almost like a
durable goods problem. Actually here you
can estimate this with individual level data or consumer level or a
market level data. Let me skip the details. Let me just say so the paper
that does this is a paper by Gautam Gambhir
and Mark Reisman. They study cam quarters and they actually estimate
this using market level data. What they do, let me
just say a short word, they basically taken, just
like in the previous table, we took the dynamic
programming problem and embedded it inside
the likelihood. Think of doing the BOP model. The BOP, there's two nests. There's a search for
parameter and an inversion. Now think of there's a third
nest inside the inversion, which is actually solving a
dynamic programming problem. Think of a nested within a
dynamic programming problem, nested within an inversion, nested within a search
over the parameters. Way beyond my
computational ability but if anyone can do it, Gautam can and he did,
that's what they did here. Let me just show you
quickly numbers. These are numbers from
four different models. Basic dynamic model. That's basically a
model that they liked. This is without
repurchases and that's the dynamic model
with micro moment because we're in the BOP world, you can do everything
that we do static with and without micro moments. Then they have the static
models so think of this like a BOP model applied
to camcorders. You can look at various things. Let's just look at price. You can see the
effect of price is nonsignificant in
the static world, becomes much more significant or become significant
in the base model, actually even slightly more significant when they
add the micro moment. Once again, the same
lesson that we saw before, the micro moments
going to really help you with getting precision
because it's actually, they don't change your
point estimate as much, but the standard
error goes down by about a quarter of what it is. That's a similar pattern that you grow
throughout all of this. Okay, so I'm going to stop here. Any last minute questions? 