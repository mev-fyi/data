yeah so i'm um the director of the national center for super computing applications um the um exceed is one of the projects of nsf that uh is homed at the national center for super community applications so i wanted to um first set the stage i'm really interested in participating in this workshop to learn from you i'm not an economist so i'm going to cast what i talk about from what i know and then i hope to learn from the rest of you i do have a lot of experience in developing algorithms tools and applications and that's again one of the reasons i want to hear what it is um your community needs what would help accelerate you as much as possible i have been doing this for a long time longer than some of you have been around that picture is from the 1985 intel annual report it's me and a graduate student awkwardly leaning over 128 processor parallel computer so this has been going on for a long time i think most importantly currently i'm running this center whose mission is to essentially bring advanced computing to all areas of scholarship and i'm particularly excited about the opportunities to bring advanced computing and i'm saying advanced computing instead of high performance computing because i think what's really most important is taking advantage of the digital revolution to advanced scholarship in some cases this means high performance computing in other cases it means bringing to bear the tools that we have to unusual problems and just as an example of that we recently helped create a new primary source in medieval studies turns out that pilgrims would scratch graffiti on stone columns in churches and cathedrals and this was known but that data was acquired in a fairly ad hoc fashion people would sort of sketch what they saw of course you could do laser imaging image enhancement and so forth and get very precise no observational bias kind of details from that sort of information it's not really high performance computing but it is the use of advanced computing tools and so as we think about what we're doing here please don't put your mind in the straight jacket of oh what i do is not high performance computing think of it as the use of computing to solve your problems i am going to assume um particularly having looked over some of the talks that there's a wide range of hpc experience here so that's why i'm going to be talking about the trends if you have specific questions about tools or how one would go from where you are to greater scale or greater use of hpc i'm going to be here all day please take advantage of that and then finally there's a lot of support for making use of hpc resources last year you had some really great talks i went through and looked at them from some of the staff at exceed about the resources that are provided by the national science foundation for the use of these systems i really recommend that you take advantage of those and look at those so what do i mean by high performance computing so everybody has a different definition that people of course tend to focus on the definitions of um the kind of computing that they use the definition that i'm most comfortable with is that it's computing where performance is important uh and it could be important because you have to get an answer in the next 30 milliseconds it could be important because there's no way you'll be able to get the answer in less than 10 years unless you pay attention to performance you've got a whole spectrum like that and there are many different cuts in terms of the way we look at this one common one is to talk about capability and capacity machines so capability systems are are massively parallel usually massively parallel computers bring a lot of computing power to bear essentially on a single problem single application that's running so that they have to compass the capability to run those problems capacity machines provide a lot of aggregate capability but it tends to be used across a greater swath of concurrently executing applications and so that swath of currently executing applications um might for example be part of the same study but it might be doing uh uncertainty quantification or statistics on the solutions and so forth another way to look at hpc is by use sometimes we talk about tightly coupled machines these are machines that are organized in a way that they can communicate frequently and with great efficiency so that they can remain connected and coordinated as they're working on the same problem and i'll give a few examples of those a little later another mode that's often called high throughput this again is an approach where you may have lots of separate or nearly separate tasks that have to be computed you might have millions of those or billions of them and you need to run each of those but each of those individual tasks doesn't have to interact at all or very much with the others and so the the real challenge there is i've got a huge lump of work to do i have to get that through the whole system um and of course everybody's talking about big data and machine learning these days um in some sense those are subsets of these but in other senses um uh it is different um in particular talking about the availability of different kinds of data one of the things that has transformed big data is this sort of availability of larger sets of data the ability to move more of the data around plus the combination of enough computing um to be able to implement algorithms that have been around for a while but had been computationally out of reach and it's maybe one of the other things that's been very exciting about hpc is the great growth in performance so that ideas that were really interesting but infeasible over a decade or so can become quite practical and another way of sometimes looking at these systems is sort of by configuration are all the computing elements the same are they a mixture does that mixture contain things that are often called accelerators which are processors that are optimized for certain kinds of operations and of course there's the cloud and i'll say a little bit more about the cloud towards the end maybe one of the most important thing here is that there's no single metric for high so how we define high depends on what you're looking at i took this diagram out of the national academy's report that i co-chaired on the future of cyber infrastructure for nsf through next year since i drew it i figured i could take it and it's a it's a very approximate figure but it's intended to point out that in addition to the compute axis so the number of say floating point operations or integrator operations that you need it to perform for some sort of application um there's also um there's an axis for the i o and file performance so the ability to move data in and out of the system and then there's another axis for essentially how tightly coupled it is how efficiently can you bring all of the processors to bear in a big system on a single problem so there are a lot of examples of the use of hpt this is um even what's here is a tiny subset my own research area has focused on the solution of partial differential equations one thing to note is um what might not be a a a terribly um dense mesh say ten thousand by ten thousand by ten thousand grid um is already um a terabyte with one byte per grid point so you need ten terabytes uh effectively just to hold a single field value um and that's not a terribly refined mesh now of course there are lots of techniques one can use higher order methods one can use mesh refinement and so forth but many simulations are struggling with just the spatial resolution that they need there are applications today that require over a petabyte um to represent the data on which they're working and so um hpc makes those systems available so it's not that there are problems that require a petted byte and they're out of luck they're problems that require a petabyte and they're running so that's one of the things that this revolution high performance computing has given us in body simulations range from molecular dynamics uh for biomolecules uh one example that we did on our big supercomputer uh in an uh early test phase was we ran what's called the hiv capsid so it's the shell that the hiv virus uses to protect itself the the research goal there is to understand um the the geometry of that because the way a lot of drugs work is a combination of the geometry in fact often a large part is the geometry and then some of the chemistry and so if you understand the structure you can start thinking about how you might attack it a lot of stuff is used for the analysis of large data sets and so this example is the reference elevation model of antarctica this was done by paul morin and his colleagues uh the university of minnesota using our blue water supercomputer the accuracy of this map is one to eight meters that's better than the maps that we have of the western united states um it's done at low lower cost because it's all based on taking satellite data and essentially doing stereo imaging and and by looking at the differences figuring out what the elevation is so not only is it cheaper but you can do it as often as you can afford the computing and so you you can add a time axis to the maps that we get other areas of science are doing this too we're a big part of something called the large synoptic survey telescope project which is a primarily nsf um funded project with the department of energy primarily responsible for the camera which is by the way the most uh the highest resolution camera ever made um that will do the same sort of thing for astronomy it will image all of the southern sky that it can see every three days looking for changes and this is something that astronomers haven't really been able to do in the past because it takes so long to gather the data so long to analyze it so so long to look for it in fact our requirement as part of that project is the detect of change and send out alerts within a minute of the image being taken at the telescope which is in chile and so high performance computing has really made it possible to transform the way we do a lot of this science but hpc is also used as i mentioned for large collections of separate computations so if you're trying to do something like uncertainty quantification understand the variations in what you're simulating um you may need to run many models hundreds thousands millions depending on on the science each of those may not be very big but in aggregate they require a lot of computing power as i view it hbc uh applies to that as well um i put this light up slightly somewhat to scare you and somewhat not too um because one of the messages part of the reason i want to talk about trends is as we'll see there's been really about three decades of stability in the computer architectures and that stability is coming to an end so that's um that's both scary but it's also an opportunity and it's an opportunity it's of course it's opportunity for people like me a computer scientist to find other ways to use that but it's also unleashed a lot of innovation which i think which provides opportunities to the scientists so um just uh to go over these very quickly just so you can um uh understand what are the sort of salient features of these on the far left um this is a machine called the sunway tycoon light um it's one of the fast it's that's the processor one of the fastest machines on the planet um it's a very heterogeneous thing so it has two kinds of processors um one is intended to work effectively on large arrays of data another processor is intended to work on everything else which is hoped to be a small part of the computation it has it doesn't have the kind of support for uh working with memory that uh most of the systems that you're familiar with um have um without getting into the details most of the systems uh that are used today put a lot of energy and effort um cost um and impact on the performance to um shield the programmer from certain details um it's an interesting question for a bunch of economists whether that's the right decision to make in the case of this system from china for a number of reasons um particularly schedule they couldn't do that so the processor is simpler it was very fast it took much less time to produce and deploy than is typical for processors done in the us this next machine is actually an abstract machine was put together by the department of energy to start thinking about what the next generation of big machines would look like and again the feature here details are less important the feature here is that it's made up of different kinds of processors and we'll see why why people have been driven to that um i did want to mention a very different approach is often uh statements made that um it's very difficult to design processors it takes seven to ten years uh to deliver one and whatnot so this is a adaptive epiphany five it's a thousand um processor uh processor but very high power efficiency did not take that long there's an earlier version of this with i think 32 cores which you can buy on amazon and was funded with a kickstarter so how did they do that that processor has nothing except just what it needs to do the set of problems that it's interested in and so focusing again on having the processing elements do just what is necessary and moving away from fully general purpose systems that try to shield the programmer from a lot of the details is one of the trends we see and it's going to be driven by in many cases the economics again both the costs in terms of schedule and dollar cost for putting them together this last one this is a diagram of the node um on from one of the two recently delivered machines for the department of energy currently ranked uh this one's this diagram is from the machine rank number two on a list that i'll talk a little bit more about it uses a power nine processor from ibm plus four nvidia graphical processing units one of the reasons i think that's interesting is we have a small system just 16 nodes intended for deep learning research and it is essentially the same diagram ours also has a one other little wrinkle for experimentation the point here is that um hbc is not really that different the pieces that go into hpc are in many cases similar to the pieces that go into your laptop um so it was nice having the power down is actually a wonderful processor a little more expensive you get but you get some things for that but a lot of um of the needs in hpc is getting performance out of the same um kinds of environments that are running on your laptop okay and then you know as we look for example at what might emerge from china what is emerging from japan is their next generation of extreme scale systems they they're all heterogeneous for some definition of that um so they all are a mix of kinds of processing elements that have been designed to optimize for different operations so i wanted to say a little bit about some of the trends so it one of the things that's been maybe most remarkable about computing over the last 30 or 40 years has that computing power by one measure has been growing exponentially and i mean exponentially mathematically so we sometimes see in the popular press exponentially meaning fast um but i i speak as a mathematician um and there's a consequence of that is that in many cases if your application didn't run fast enough the best solution was to wait because exponentials are wonderful and this is sometimes described as a consequence of moore's law has everybody heard of moore's law okay so now i want to make sure that you actually heard of moore's law because it tends to be um ascribed to performance doubles for example uh one definition that people will say oh moore's law means the computing power doubles every 18 to 24 months wrong that's that was never moore's law moore's law was only about the um essentially the feature size the density of components on a chip more or less at fixed cost and that that um that the uh uh the feature length would decrease um exponentially um for he thought ten maybe ten years or so um what's much more important is it turns out as you make features smaller um it means that you can make them operate faster you just sort of think that this that uh if you've got a you've got a switch you've got to move it you know or you have to move the electrons across it or if the physical switch you have to move the switch you make the switch smaller you can move it faster so there's a thing called the nard scaling and and the coupling of moore's law which was making the features smaller and denard scaling which said that you could make things then faster did lead to a lot of performance but we'll see that um that uh that advantage has um has ended of course the reality is much more complex um uh we tend to focus on the floating point um and integer operations um they're easy to measure uh as a numerical analyst we always make our students um do combinational complexity estimates in terms of number floating point operations uh the aside is that's doing our students a tremendous disservice um because it turns out that for most of the uh numerical applications uh what um controls the performance which actually is much more relevant in terms of understanding the performance algorithms is the comp is the complexity of the data motion and so um and it's not always the same um and in fact one of the the key benchmarks has a very big difference between those two and has led to uh some misleading conclusions okay maybe the the so the things like memory access i'll say a little more about that and uh and access to data is often um more important so maybe the important thing to note is that moore's law it's not a law but it has become an imperative so it has helped drive the industry to find engineering solutions to hard problems in order to stay on this law uh we'll see in a moment whether they've done that um another interesting way to look at um computing in the sciences this is a thing called the branson pyramid this was from an early report originally in 1993 this picture is from an update it was added i think the original figure came from 2006 and it was in a report to uh i think the nsf in 2011 and so um the original one had like one teraflop system on the top that was a leadership class machine so leadership class machine being the fastest machine that you could possibly have down to center scale campus scale and you know certain desktop scales and between the original and the update that went from one teraflop to hundreds of teraflops ten teraflops um one teraflop and tens of gigaflops if you update that today it's hundreds of pedoflops for leadership class tens for center machines um ones for uh one pedoflop or so for campus level systems um you can get desktop machines now that are about 10 um teraflops and those tens of gigaflops that's what your laptop can do so and what i've left off of this are these commercial data centers uh it's very hard to find out exactly what their capabilities are they tend not to talk about them but you can look at things like the number of servers they claim to have or the amount of power that they consume so today's leadership class machines consume somewhere between 10 and 20 megawatts google i think google's talking about overall it's data center is consuming about a gigawatt so i've been talking a little bit about high how do we what is high how do we measure performance so in high performance computing the best known is something called the high performance linpack or hbl there's a website top500.org been doing this list for 26 years this benchmark solves a system of dense linear equations then one where essentially all the matrix elements are non-zero when it started 26 years ago that was representative of a lot of of scientific computing almost nobody does this kind of of computing um across an entire machine anymore um yet we still run this benchmark um there are some other benchmarks um there are also maybe much more uh relevant benchmarks that are drawn from application sets um but the one of the things that's really interesting um about this hpl benchmark is it has been collected for 26 years and there's a lot of data and so i pulled these two um basically the same chart um off of the uh the website um showing um so that the green line is the sum of the performance of all 500 machines on the list the jagged thing in the middle is the performance of the machine at the top of the list and the smoother line at the bottom is the performance of the machine on the bottom of the list the number 500 machine and then over here they they did a curve fit and drew lines through it um so this would probably be you know a great example for your class of how not to do curve fitting um because we look at it like this so let's focus first on the right so this is just the bottom of that list so this the the blue curve that you can barely see there is the same as the blue curve here um that's the performance of the machines on the bottom of the list and what you see is that before 2009 was essentially a straight line and after 2009 it was essentially a straight line but with a different slope what happened there was the end of dinard scaling this is when processors stopped getting faster you may remember 2006 2007 processors were two and a half to three gigahertz what are processors today they're two and a half to three gigahertz maybe three and a half all right um the deployed machines there's a little bit of lag there um and um a nice thing is i i look at this data every six months when it comes out and so i just add more data i haven't done i haven't redone the fit in years um oh let's see there's the most recent data so again the the slope is is a good fit to the red line um since 2009 the other other one i want to point out um because it's a lot of people you know moore's laws everybody says moore's law is either dead or it's not it's doing fine um i found this wonderful graph in an article in scientific american in 2004 and this was essentially right before bernard scaling started to hit and although it's hard because of the colors here the projected clock speed for um like 2019 is about 70 gigahertz not three um and uh there's similar things for the um the gate length um so what what has happened is moore's law really has ended if you think of it in that very rigid description what has happened is that moore's law has been permuted into a statement that we will continue to make progress which is why moore's law will never end because we will continue to make progress i did want to say a little bit about then these are some examples of machines on the top of the list i'm going to go through these quickly just so you can see some of the progress so on the left is the high performance linpack which is the dense matrix no longer representative of much of real science on the right is high performance conjugate gradient this is a sparse matrix problem this is a much more recent benchmark so there isn't as much history and uh however it is much closer to what a lot of um science applications particularly ones that use uh simulations from pdes use um an interesting thing is is to match up the number one you know the machines on the left with the machines on the right and one of the things you can see is that while machines that are are powerful super computers are powerful supercomputers um doing well on the high performance linpack does not necessarily mean doing well on the conjunct gradient application and one other thing to note is the top machine on this list didn't even make the part of the list that i've showed you for the high performance linpack more recent systems given to something like this where currently yes these two top machines are the same as the two top machines but then um again we see that um the performance on the linpack benchmark the top 500 benchmark is not as indicative of the performance that applications are likely to see there are some other um oh yeah and i should say that that uh in the system these leadership class machines sort of the the most powerful machines are really the top four there's a significant break like a factor of three to get to the next machine now again um again it's a number of examples of different benchmarks there's one called the graph 500 which looks at oh there's a breath for search and a single source shortest path algorithms have been run on a variety of systems it's not as exhaustive so it's hard to draw the same sort of statistics but again what what's good on this is not the same as what's good on the high performance linpack another thing to note um you can look at the top 500 data and they've got ways to look at at a number of the parameters over time and one thing that's interesting um is this one this is the use of accelerators so the use of specialized processors targeted to a sort of subset of the application and the left is is based on how many systems the right is based on the total performance of the systems the most important thing to note here is that while it is true that a lot of these systems are still sort of the same sort of uh processor that you have in your laptop except design you know what's called server uh uh server version of the processor there's continued growth in the use of these more specialized processors and the other thing i should say so this um this phone as a computer is in fact in every way more powerful by all of the numbers however i want to measure it it's a better computer the supercomputer that was on one of those earlier versions of the top 500 list and was a super computer at one of the nation's um weapons labs so just this phone um 30 years ago um would have been um more powerful than the most powerful machine on the planet and so that's um uh oh and then and part of why that is the processor in here is very heterogeneous there are lots of specialized pieces of this to handle different parts of different problems it's one of the reasons that it works so well so one of the messages here um yeah one of the messages why don't you have to be careful with benchmarks you should be look if you're looking at performance it needs to you need to be looking at what makes sense for you another is that as we're going forward it's going to be increasingly important to think about how algorithms and applications map onto the sorts of processors so in looking at this we see a growth in the use of these specialized processors often for gpus the chinese systems are based on digital signal processors which is um similar to but you know different than a gpu the gpus grew out of the out of the gaming market um it turns out anybody knows about computer graphics knows that a core operation there is a matrix multiply with a 4x4 matrix and then doing a zillion of those digital signal processing doesn't have the same [Music] building block but has a lot of the same elements and one of the other things is um hpc systems are really not that different so i want to encourage you don't be scared of them a lot of the features of the systems are similar to what you have on your laptop um we do focus i think too much on the floating point operations um i o and memory i can give a whole talk on on how one can look at um thinking about the memory that you move in these numerical algorithms um to see that that provides a much more accurate uh representation of performance in fact um in 1999 we won a um there's a prize in hyperbolic computing called the gordon bell prize for reaching very high performance on some application and we won a prize there for a combinational fluid dynamics application had an unstructured mesh the key thing there is that people didn't think you could make that run fast what we did was we showed that it was running as fast as it could because if you just thought about i have to move data from where it is to where i can compute on it do my operations and put the result back and you looked at how fast the memory system was and how fast the operations were you know it turned out we weren't getting five percent of performance we were getting 95 percent of performance now we're only getting five percent of the floating point performance but we were getting essentially the full performance of the system um and that again is one of the things that's important is we look at high performance computing is focusing on performance one has to look at the right kind of performance um there is um there is a lot of software both applications libraries and tools that have been optimized for this so a lot of people in computational science working with for example pdes that make use of software libraries that manage distributing the data across the systems doing the communication doing the linear and non-linear system solves and so they can focus on the science so it's um uh it's important to also look for existing tools that can help you help you build up the applications that you need um so this denard scaling ended more than a decade ago and so as a consequence what we see is that the individual processors haven't gotten much faster we've been able to continue to claim more and more performance where's that performance come from it has come from parallelism uh that is harder means the algorithms have to have enough concurrency codes have got to be written to take advantage of that um moore's law really is ending despite what certain large companies want to tell their marketing people but because it's been redefined as progress it will never really end it will just continue to slow down again as it's been mentioned the ability to to aggregate and share and work with data is at least as important so again i sort of encourage you not to think hpc is that floating point stuff right hbc is that performance stuff um and the specialization is driving computer architecture so the good news about that is we're finally getting innovation um as a computer scientist was always pretty depressing over the years in fact i know people who would have had a group of coffee mugs for failed computer companies and what people would see oh i can make i can make a faster computer and i will do something really clever and by the time they had delivered it the exponential had overtaken them well with the end of denard scaling that stopped one of the reasons that gpus had become so important is that they were a different solution they were an innovation in early innovation in that space they have simpler processors but more of them often they run at a lower speed than the general purpose processors which gives you certain other benefits your phone completely depends on specialization and so going forward this has become more and more important okay so um first question i usually ask people who say i want to use you know hpc is really i sort of hope so but um in many cases it's really not that it may not be necessary so one of the most important things to do here is to really understand what your need is so if you were um an experimentalist in another field you would go through a fair amount of work to design your experiment one of the problems with computing is it's too easy to write code and run it um and particularly if somebody else is paying for the computer time you know why not right that is that has developed a number of bad habits um across the community so no one has been immune about this it's important to be thinking about how fast should this code run how fast could it be running and more and more that's going to be important as you start looking for resources on which to run your computations and one of the important things in looking at this is just looking at the computations looking at the floating point that you're doing or maybe at the integer operations you're doing is often not a very good predictor of overall performance you have to look at some other things you have to look at again at how memory is moved in and out of the system you may have to look at the data access if you suddenly running on a system where you're charged for data access you really need to be looking at that and you look at the available concurrency so some algorithms just don't allow you to take advantage of parallelism and so having 100 core chip on which you can use one core but you'll probably still get charged for 100 um isn't necessarily going to help i do want to say there are people who work very hard at producing very very accurate models of performance what i'm arguing here is that what you really need is decision support so you only need an approximate model and it needs to only be good enough to answer the question of um can i afford to run this or can i convince somebody that it's running well enough to be worth putting it on their system um of those models are getting a little more complicated they want to um you know for the for uh people who are doing mpi programming and distributed uh parallel programming um there's a a very simple model that has been effective for at least 30 years in this paper i we show that because of the architecture of modern computer nodes one needs one more term to recapture the accuracy in the model so for the advanced distributed computing parallelism users i suggest you have a look at that it's also argued that it's more important to use the right algorithm then have the fast machine there's this interesting chart this originally appeared in a dewey book on the science case for extreme scale something science scales report which contrasts the performance increase calling it moore's law but mostly due to denard scaling from an early machine um up to uh what was in the present day with the algorithms for solving a dense linear sparse linear system arising from a 3d pde starting with a banded gaussian elimination so just use the part of the matrix that's non-zero all the way up to something called full multi-grid and what you saw when you did this was essentially exponential increase in both that you were basically you could either take an ancient machine with a modern algorithm or a monitor machine with the agent algorithm to get the same performance so a lot of people like to look at this i do like to point out the asterisk by the time we get the full multi-grid we're doing of one work per data value returned there isn't a lot of room to carry that further and so there is a need to use the more in more modern machines as the algorithms get to be this good another thing another trade-off and you know there's i want to say up front there's not necessarily a right answer in here another trade-off is between performance and productivity so this was a study um published at sigmon in uh 2014 looking at some graph frameworks um and they had one that they called native um which was that they wrote the problem using low-level stuff um and then this is the the time so down is good um thing to note is that giraffe is one of the most popular of the frameworks this was a hundred times slower than the native but i mean it was bad because if 100 times slower meant it took one second instead of 10 milliseconds who cares um but if it meant that it took 100 days instead of one day then maybe you do care um what i found interesting was eventually about halfway through the paper you discovered that native meant they did it in mpi which is the message passing interface that i have been working on for a long time okay so i did want to have a little diversion um on the hpc side because again i think the um the focus and concern is often in the wrong place and so we often talk about the scale you know how do i run a program on a system with a million or a billion cores we have systems with over 10 million cores today um you know and how would i program that um that frankly in many ways that's the easy question um the real questions um are getting a better understanding for how the performance runs on a single node a single you know even understanding how a code would perform on your laptop is in many ways more important and there's been a focus sometimes on something called performance portability which is a dream but it's a dream that i think actually gets in the way of finding a good in pragmatic solutions a lot of the the challenge in using hpc systems is really making use of these increasingly complex processors you know whether it's in a machine with 20 000 nodes or the machine that's sitting on your desk and then i io is at least a bad for this in the past there's been a tight connection between an execution model execution model is the way you think the machine works and i can come back to my my point that that we we tell our students the wrong thing when we're explaining complexity of numerical algorithms because that's based on execution model that all of the cost is in doing the floating point computation that hasn't been true since a little after i was a graduate student which was shortly after the invention of fire and so an execution model that reflects where the true costs are is important in understanding what is a good algorithm for the problem you know understanding how to create a good algorithm understanding how to go after tuning a system um this divergence um has left us with programming models and systems based on this overly simplistic execution model which also don't serve as well and so there's there is this big mismatch between the user's expectation and system abilities um often we said we try to fix this by making the compiler do it a lot of computer scientists will say nobody should be tuning up their code because the compiler should do it um i'll get back to that in a moment so i do want to say in many ways the easy part is the distributed memory parallelism um there is a heart there are two hard parts there one is the algorithms so you have to be able to have an algorithm that has enough concurrency um and that has enough work that doesn't require constant interaction with all of the data everywhere once you have that algorithm expressing it um is in fact pretty easy and so that's why it's either not too hard or it's impossible one of the big challenges is handling the representation of that distributed data one criticism that's often leveled against mpi is that it doesn't help you do this i agree with that criticism power is also a strength because we find that as people put together applications their needs their distributed data structures are often very specific to their application which means what you actually need is something that helps you put together something that works with your distributed data structure mpi doesn't help you but it doesn't get in the way and so many people have been able to build tools based on that question came up last night at dinner is how old npi is um mpi is older than some of you um it's about 26 27 years old now um there was a nice point about a decade ago when i said mpi can now vote mpi now has to buy its own health insurance and it has grown a lot from implementing a fairly basic model of um based on what's called communicating sequential processes to one that embraces a number of parallel computing models so that's why i like i prefer to call it a parallel convenient system that allows you to coordinate applications in different ways and it is still actively being developed there was a draft for the next version was released at our annual super computing meeting in dallas last year we'll say that applications are still mostly mpi everywhere that means on every core they're running a separate mpi process for our machine blue orders we did a workload study uh which you can look at one of the reasons for that comes back to my point that it's not just a floating point one of the things that mpi everywhere gives you is a very effective way to manage memory locality and it turns out memory locality is often very critical for performance and so the bad news is it makes you manage it the good news is it doesn't get in your way and in fact in some sense the good news is it makes you manage it so it makes you manage it instead of pretend that it's not there there are a number of challenges i'd be happy to talk about those uh uh you know during the rest of the day um the hard part really is inner node performance this is just one example a lot of performance comes from parallelism now so a lot of those multiple cores but a lot of that is instruction level parallelism so vectorization within a core a lot of applications actually don't vectorize very well and a couple years ago we looked at a a bunch of code that we had a smart graduate student a smart graduate student was able to vectorize all of that code and we okay so the compilers are supposed to be as good as people how good are they well none of them could vectorize all the code the graduate student so there's 21 examples that none of them could do at all and no compiler dominated all the other compilers it was there was some loop for which each compiler was able to vectorize that no other compiler was able to um i o is often also overlooked and pretty much by everybody part of it is everybody thinks i o is awful and so the expectation drives the fact that it's awful uh i went to two examples um from our group um we had one example where it was a fluid dynamics calculation the reading a description file to do some computation thing runs for a day turned out it was taking hour and a half to read the description file well that was only like 10 percent of the run so that's okay then we were trying to do some debugging where when we read the description file take one step and exit right that was you know figure that would take a few seconds i was taking an hour and a half um it it turned out no one no one had just done a performance model how fast should we be able to read that that data wasn't that big we actually reduced the amount of code by taking advantage of some features in mpi and the time went from 4500 seconds to one second at one second i'm still not happy because if i look at the performance that's too slow but it's now within a factor of baby four um we then were able to take that code to another application in quantum chromodynamics so that's the standard model of physics and that one was merely 48 times faster again because those things run forever so nobody was paying attention to it but that was a lot of computed value and if we look at how bad it is we did a study where we looked at some of these systems um and i am almost done so um the big thing here is so this is the number of applications that achieved a certain performance so very few um applications achieved 0.1 of the available performance and that's a factor of a thousand over there so okay so um some of the challenges um popular focus on the computation that's not necessarily where the issues are um a lot of us do to memory so is the data there how easy is it to access um on these systems sort of execution model data moves and lumps they're different size lumps depending on where the thing is in memory algorithms have to think about those lumps uh does make it a little harder but that's true there are however a hierarchy of tools so i don't want to scare you about this but make sure that you adopt tools at the right levels so i'm going to close by just quickly mention there's a lot of resources that are available national science foundation provides resources both through exceed and a thing called the p-rack which is for the um the high-end system the department energy has a program called insight which is quite open and provides access to their big supercomputers a lot of the other agencies have their own things for example the dod modernization program has stuff a lot of institutions provide resources so we do some of our partners do nsf is increasingly looking to the institutions to provide resources at that scale um there are um cloud resources um cycles can't be stored so if you're really really flexible um they're um they'll call everybody calls it something different but there's a way to essentially get stiff stuff at a steep discount because they haven't been able to sell it they're good ways to request a time these slides will be available so you don't need to go through this very much should take advantage of exploratory or startup allocations um focus on the science um and make sure you can show that you're using the system wisely so it's another reason to use these startup applications to show that um we're using this widely because all these systems are oversubscribed the exceed ones are oversubscribed by a five-fold um the dua ones i think are even worse um so i did want to close by saying just a teeny bit about clouds um so cloud computing means a lot of things but a lot of it is like load sharing among users um flexible allocation demand um uh solving a lot of good data frameworks and software sharing um nothing's free um an interesting thing about clouds is that if you use a machine about 30 of the time or less the cloud is is going to save you money if you can keep your machine busy all of the time the cloud is going to cost you money and this has been shown over and over again in many disciplines we have a member advisory board who retired from a financial firm and a third was essentially their their metric because if they were going to keep the machine more than a third busy they would buy a system it was less than a third they'd do it on the cloud um there are a number of studies um there's a very very detailed one that you did a while back we did an update um in our national academy report nasa did another um one recently um the rule here is for from so many other things is you don't have to believe anybody but you do have to do the numbers and you have to do them carefully i don't want you to go away thinking that i'm opposed to clouds they actually have a very important role um to give you um access to different systems um there's a lot of innovation going on in that space so taking advantage of that uh it really can complement center resources and i do want i did want to show this example for a personal reason so about two years ago amazon a team at clemson working with amazon ran on 1.4 million cores concurrently that's more than i've ever run on um the personal reason is that's my son who also discovered a bug in the amazon software um so the paper was delayed while the lawyers argued about it and it's really fun to be at one of these presentations in washington where somebody put up a slide that looks kind of familiar there so in summary high performance computing really is any place where compute where performance is important um the technology is mostly familiar there are of course some things that are a little different but it is mostly familiar don't be scared of it um it uh to take advantage of it you need parallelism somewhere um parallelism might be separate things to do parallelism may be very tightly coupled or something in the middle the technology is going through a disruptive period so we're going to see lots of changes and i mean there are lots of opportunities for doing new algorithms and working with people who are trying uh new things it also means that it's going to be disruptive and that there are many um sources for hpc help you know starting with me today i'm happy to talk with anybody and then i did have a few questions that i'm curious about in terms of working with this community and seeing what hpc can do to help accelerate your signs thank you 