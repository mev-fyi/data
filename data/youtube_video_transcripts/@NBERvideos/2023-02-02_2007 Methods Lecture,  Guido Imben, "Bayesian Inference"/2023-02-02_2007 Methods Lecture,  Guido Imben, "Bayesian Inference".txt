um basically um now talk a little bit about the basics of of basing inference then I'll talk a little bit about uh a theorem called uh bronstein from Mrs theorem and essentially what uh the theorem says it's uh that in most cases in practice it doesn't make any difference whether you do work from a Bayesian perspective or from a from a frequentest perspective in some sense that there is a incredibly important result and so it allows people to to typically be somewhat vague about uh what they're they're actually doing and partly I'll use that to argue that that pretty much all empirical work is really implicitly uh Bayesian and that in cases where and that's sort of I'll I'll try and make that point by sort of discussing a couple of cases briefly about Burns Infamous doesn't apply and where doing things from a frequentist perspective makes things very uh very complicated um then I'll I'll talk about uh some of the sort of the main advances in uh Invasion work in the last uh 15 years which is really computational and the these methods are generally referred to as Mark of Team Monte Carlo methods which allow people to use Bayesian methods to use formal basic methods in a much wider class of of problems than was possible uh before and to now make these these these basic methods actually much more applicable than frequentish methods in much easier applicable and then frequencies methods in a lot of cases and then the rest of the the time I'm going to discuss five examples I want to look at some of Rossi's work where he looks at demand models for the an absurd heterogeneity and preferences um I'm going to look at some some of Chamberlain's work looking at panel data where he allows where instead of just allowing for individual specific intercepts he allows for both in the individual specific intercepts as well as individual specific variances and it turns out there's a lot of evidence for heterogeneity and inferences they can potentially make a big difference for for substantive questions I'll look sort of very briefly at some work I did with Chamberlain looking at instrumental variables methods with many instruments I'll talk more about that setting tomorrow but here I'll look briefly sort of at a Bayesian perspective on that problem um the fourth example is a paper by Gary can some co-authors looking at binary responses endogenous regressors kind of showing how how what type of models would work well there from a Bayesian perspective last Model is a last example as a model with a discrete Choice model with multiple unobserved choice characteristics but again from a frequency perspective that would be very difficult to handle the likelihood would be very complicated and multimodal but from a Bayesian perspective that is that is conceptually very straightforward to uh to yeah analyze so um so by way of introduction there sort of this surprising sort of compa especially compared to the statistics literature about where Bayesian work is um probably almost about 50 percent of uh of the papers in in journals these days there's actually very little formal Bayesian work in in empirical work in economics that's sort of surprising for a couple of reasons yeah one is that this methods actually often very attractive from a computational perspective and from a modeling perspective especially in settings with with many parameters uh sort of random coefficients settings where large sample approximations don't tend not to work particularly well so part of the reason there I would um at least my my interpretation of that is in a lot of these cases but a lot of standard cases without too many parameters basic message is going to be very similar to uh to frequencies methods and and this is uh the burns Infamous theorem and I'll be a bit more specific about that uh in a bit but so in many cases doesn't actually matter and so in the few cases where it does doesn't matter people have tended to stay away from uh from that sort of also surprising because in cases where Burns Infamous doesn't apply this methods can be extremely uh awkward and sort of it's sort of very hard to imagine people doing empirical work really sticking to to the formal frequentest perspective and then just sort of to give a sort of very stylized example but suppose you took the frequentis perspective very serious so you wrote down a model first you came up with a procedure for constructing a confidence interval you weren't going to do it's many different versions of this model you just committed to a particular model so that the p-values would really have the properties they were supposed to have then you got the data and you followed your procedure and the sum some of these procedures that have been proposed there with some probability end up with empty confidence intervals now how would you possibly report that in the in the end it's very hard to imagine reporting a confidence interval where you're not willing to say that at least approximately given that confidence interval you have 95 confidence that the parameter is in that set if but it's clear that if you have an empty confidence interval you're completely sure by that point that the true value of the parameter is not in that set and so the the the idea that you would that that would be an acceptable procedure is it seems it's it's a I remember a long time ago yeah going to I mean do some lectures by Lima who made the response very eloquently uh sort of called the frequent this perspective there the Preposterous it sort of it's based on these presumptions about what you would actually do given the data that that clearly don't apply to what what people do and and moreover there are procedures that people that make no sense to actually follow so the the main case I want to make here in this this lecture that in most cases and a particular in the sort of in cross-section cases of the type that we've been discussing in uh the latest yesterday through these three days it doesn't actually matter that you can in the end interpret confidence intervals exposed condition on the data is giving you 95 probability and then they make they make perfect sense in cases where these this equivalence between Bayesian results and and classical frequencies results doesn't hold it is I find it very hard to imagine sticking with the the frequentest results and so when I say that there's been very little the formal basing work in uh empirical Beijing papers in empirical work in economics at the same time there have been very few papers certainly in in using cross-section methods where there's actually where there's a serious conflict uh between uh between the two almost all the cases that we normally look at we get asymptotic normality Burns information supplies and you can interpret confidence intervals from uh from a Bayesian perspective nevertheless in there in some cases it actually it can be very useful and can can be a major help to be more formal about the Bayesian part and and take into account some of the issues that come up and then exploit some of the the Bayesian methods uh and so I'll try yeah make make a case for that so um and then thinking about when I Was preparing this so in fact I talked to a bunch of people um a bunch of econometricians were sympathetic to Beijing methods about why they were not used more widely and so the the main reason the main I think perceived reasons uh were that first of all sort of the implications of the burns the information theorem that in the end it doesn't matter that sort of whether you use frequently Supply or Beijing methods you get more or less the same result and you can interpret things uh either way and certainly in in my own empirical work I've done some very limited Bayesian explicitly based in work but all the other and all all the other the other very limited empirical work I've done there's been that was not explicitly based in it's all fallen into this category where it doesn't really matter but I'm pretty sure taking a Bayesian perspective wouldn't change the substantive results and so the first one I think is a perfectly uh good reason second one is that it's uh that formerly Bayesian methods require you to specify prior distribution and that's that's clearly a very um I think that's that's generally perceived as a considerable uh barrier to using these methods it sort of makes these things uh somehow not objective you would uh you put some information in that that is that you cannot directly motivate from from theoretic on theoretical grounds and it may actually make a difference so that I want to I would nah I think I think that is that is much less convincing reason one reason is that in large samples and this is again to respect to the information's theorem in large samples it's not going to matter unless the prior distribution is is particularly dogmatic it rules out particular values of the parameters completely rather than being somewhat flexible second argument that by this if I don't find this particularly convincing is that um infrequent this methods to be similarly rely on large sample approximations we always rely on large sample normal approximations and we pretty much ignore the the complicated sort of the issues uh coming from that a lot of cases it's clear that large sample normality is not a particularly good approximation but it's pretty much no alternative we can't do exact uh finite sample things sometimes we can get slightly more accurate approximations to the to finite sample distributions but those are typically very complicated and often not very satisfactory so I think I think the difficulty in in choosing a prior distribution probably exaggerated in practice even though it's sort of in the the statistical literature this has led to a huge literature trying to come up with somehow a way of uh of specifying the universal prior and objective prior somehow a prior that doesn't that is not subjective and that the that would be easy to transfer from one problem to the next and sort of clear that in general solution to that question doesn't exist and so in the end you cannot really avoid spending at least some time thinking about a prior distribution unless you're in very simple cases where there's very very few parameters no but things really don't matter I'm certain um argument that that is often made is that Beijing Masters formerly required you to have a fully paramedic uh model so that they they rely on specifying a likelihood function rather than just the more limited set of more limited part of the model so nowadays so for example demise method of moments and all those different variations require you only to specify moment conditions that don't require you to make a full set of distributional assumptions and I clearly make things more more complicated for Bayesian methods at the same time in cases where you're interested in in parameters that rely only on moment conditions it is often not not so hard to specify a relatively flexible model where the results for the the parameters of of Interest are not all that sensitive to the specifications of the the remaining part of the model I said I'll try to illustrate it a little bit in discussing some of the examples where by the end in all these examples people specified fully parametric models but I would argue that that many aspects are probably not the some of the key results are probably fairly robust against changes in the specification there um and the last issue is computational difficulties and there I think there was a very valid concern sort of up to the early 90s but this explosion in the literature on on computational methods Basin analysis in this Markov chain Monte Carlo methods have pretty much taken care of that and now in most cases these matters are actually much easier to apply than than frequent test computational methods to the extent that sort of for in discrete Choice analysis where for a while simulation methods um sort of direct frequency simulation methods were fairly popular now nowadays based in computational methods are being used there even sort of at the expense of having to to put some more Bayesian structure on that but they tend to be much more effective than than frequent disk methods go to the basics and so what I want to do is just first give it the general specification then talk about the normal example and so that a lot of them may be familiar from introductory statistics courses many many years ago but I just want to kind of establish some common common language as a part of the Maybe it's sort of more cynical view on the lack of Bayesian work in theoretical economics then in some sense there's actually very little to be done there they're pretty much the only thing you need to do is specify the parametric model specif choose a prior distribution and then the rest is all just computational issues and there's actually there's no there's not a lot of results to be uh to be proven uh and there's not a lot of difficult theorems to state and so in fact it's all fairly straightforward to set up as opposed to much of the frequent this work whether there's there's certainly considerably more interesting results uh more challenging proofs to be done um so what is the general setup you specify a model for a random variable uh given some parameters we sort of think of that as as a function of the parameter rather than the the datum we call it the the likelihood function we choose a prior distribution for the the parameters so some probability function or probability Mass function for continuous parameter then the posterior distribution just follows from the from those two the having a model and a prior distribution allows you to get the joint distribution of the parameter and the random variable you can integrate outer parameters you get the marginal distribution facts take the ratio and it gives you the posterior distribution so in practice it can be very difficult to do that calculation analytically but and so one way of simplifying that a little bit is instead of looking at the posterior distribution itself the right that without the integrating constant in the denominator which is a function only of of X not of theta because that's been integrated out and so written that way the the posterior distribution is proportional to the product of the the likelihood function and the prior distribution and so that that's really the the key there that's that's really the starting point uh from Bayesian analysis we're going to specify a model that gives us the likelihood function we're going to choose a prior distribution and we're going to work very far about a fair amount about whether that sort of a sensible prior distribution and then but given that we can infer the posterior distribution as I said this is kind of the key uh it's not really a result uh well it's it's a result the sort of the key result years ago when I went to uh basic the basins used to have yearly conferences uh in a beach resort in uh in Spain and so as part of this conference instead of giving umbrellas the way the MBI does they would give out beach towels imprinted with base theorem so this this last result on the on the slide here and it's uh which was not a pretty sad on the beach really so so now let me just go through um the the normal case and so I'm going to do three versions of this uh and hopefully this this is all there very familiar but I just kind of want to be clear about uh what we yeah what I'm doing here so suppose the product if you have a scale around the pair of wax which is a normal sorry yeah pick the prior distribution from you to be also be normal uh with mean zero and and variance 100. than what you would do in uh and now this in a simple example you can do this All analytically you can figure out what the conditional distribution is of mu given X and doing these calculations what you get is a normal distribution with mean equal to x times 100 over 101 and variance 100 over 101. that's all very uh straightforward um now let's make this slightly uh more General as opposed to the model is uh normal distribution would mean mu and still known variance but General variance Sigma squared and as let's look at what happens the if we have a distribution from mu that is normal with some mean mu naught and variance Tau squared and I'm going to see it see what happens for for different values of uh of Tau and and mu naught this case you can do the same calculations and what you get for the posterior distribution is that it's still normal mean weighted average of the prior mean mu naught and a weighted average and the observation Acts with weights proportional to the inverse of the variances and with variance equal to the inverse of the some of the precisions and so um let's kind of think about that result for uh for a second supposed to actually really sure a priori before seeing the the data what the values of of new we could capture that by Jason Tau squared equal to a very small value that would mean that the weight given to the observation would be small relative to the weight given to uh mu naught and the posterior distribution would just be very close to the prior distribution The Other Extreme if you're very unsure before looking at the data what the the value of mu is we could pick a very large value of of Tau and in fact we can let Tau go to infinity and the first the posterior would still be very well behaved and it namely would be normal with mean X and variance Sigma squared that's sort of a very interesting case and now we have a prior distribution that isn't really a valid probability density function namely it's flat on the whole real line nevertheless it still works very well here and we still get a well-defined uh posterior distribution and it sort of would seem to capture very well the idea that before looking at the data we're very unsure about the value of mu and that somehow and so somehow this this prior distribution seems to capture it sort of seems to be very uninformative uh and so this literature I was referring to earlier tried to do this in lots of cases tried to come up with prior distributions that somehow capture the idea of uh of ignorance prior to seeing the data that doesn't really work in general and then and in particular it's not always a good idea to use a prior distribution that is flat that is the way we the way that worked very well here but in more in in other cases using a flat prior distribution can actually be very informative about particular functions of of the parameters and I'll come back to that when we uh when we look at some of the the examples I see some cases by using a flat prior distribution there would be a very bad idea and essentially it's sort of the same reason why trying to estimate all the fixed effects uh in a panel data model can be very it can be very bad idea um now last part of the the normal example uh is the sort of same cases before but now we have an independent uh draws from this uh this distribution using the same prior distribution we now get a posterior that is still uh normal with means still a weighted average of mu naught and X bar and the variance given there at the the bottom of the page now I'd say I want to now continue that example a little bit but now now move into the the importance information theorem so what happens here if um if n is large so looking at let me just go back here for a second so I want to look at the limiting distribution well at approximations to this posterior distribution if uh if n is large in fact what's going to happen is that the variants it's going to uh to zero and the mean is going to be centered it's going to convert to the average of the access and so that's not necessarily very interesting uh limit to look at so just as in the stat and figure this analysis it's useful to scale that up by the square root of n by the square root of the sample size and then if you look at the limiting distribution of the square root of n times the difference between the sample average and the parameter conditional on the on the observations so with with mu still taken as the the random variable you get a normal distribution with mean zero and variant Sigma squared and so the key thing is that that limiting distribution here is irrespective of the choice of the prior distribution in this case it was just irrespective of the choice of munol and and Tau squared as long as Tau squared is not equal to zero but more generally we could have had any prior distribution as long as it had positive support on the whole real line and as long as it didn't it wasn't too large and we would get the same result that the limit for this scale difference between this the sample average and mu would converts to a normal distribution same time sort of the the most simple uh Central limit theorem and effect in this case the exact result would be that conditional on the parameter and so doing this by fixing mu and looking at the difference between X bar and mu scaled by the square root of n we also get a normal distribution between zero and variant Sigma squared and so what does it uh what is the implication of that uh if we look at a standard confidence interval so in this case there would be X bar minus uh so let's look at 95 confidence interval X bar minus 1.96 times the standard deviation divided by the square root of N and so here obviously the way it's written doesn't make any sense there should be a plus here and a plus there but sort of the standard confidence interval would have 95 percent coverage but in addition conditional on the data then 95 confidence interval would have 0.95 probability of containing the the true parameter so given the data given and this given the upper and lower Bound for the confidence interval the probability that the parameter you can interpret the confidence interval as saying that the probability that the parameter is in that interval is 0.95 Now sort of now so the traditional introductory statistics books spend a lot of time arguing about the interpretation of this confidence interface it's the confidence intervals that are random the parameter is fixed and if you keep moving these things 95 of the time you you include the the True Value here this is saying we can also think of it from a bit just for a fixed interval that the probability given we had a particular data set the confidence interval is uh 0.8 to 1.5 the probability that the parameter is between 0.8 and 1.5 is 0.95 and that's that's not just being sloppy that it would be motivated by a formal Basin analysis where you have pretty much any prior distribution there you wish and so the um I say I'm not going to go through much detail here but the Bernstein formation theorem says that this is true in general yeah now so in general that sort of with some limitations uh but pretty much in all in sort of most cases where we have asymptotic normality for for maximum likelihood estimators where we have rude and consistency no it's sort of so in cross-section settings where we have independent uh observations we have rude and consistent estimator namely the maximum likelihood estimator we can interpret the the statements sort of slightly more involved because to be so you have to rescale the parameter by the square root of n but in the end the interpretation is that you can interpret the standard confidence interval as as a Bayesian probability interval with probability 0.95 the confidence interval the true parameters in the confidence interval with probability 0.95 given the data and this is in the end sort of I think this is in the end the way confidence intervals are typically interpreted it's sort of it's obviously always hard to see it say exactly what what people mean in that case but it's no it's very hard I think to argue that really what we're interested in is what would happen had we got a different data set rather than being interested in the coverage probability given the the data we actually had but this the burns information theorem says it doesn't really matter for for all intents and purposes in in a lot of the cases we're concerned with it's fine to interpret confidence intervals that way now when this is not apply and so this is actually there's very few cases uh where it really doesn't apply one of them is um and so these are not cases uh I typically worry about um that sort of partly what we're not doing much time series um these three days but so one case where where it really doesn't apply is in unit route settings if you have a simple Auto regressive model then it's still true that if you look at the likelihood function it's perfectly well behaved if you have a normal prior distribution for the auto regressive parameter you get a normal posterior distribution and so from a Bayesian perspective there is no unit root problem in that sense you would get a normal posterior distribution you would get a normal you would take the posterior mean at subtract twice the posterior standard deviation and that will give you the Bayesian probability interval however if the true value of the autoregressive parameter is effect equal to one it's not the case the conditional on the parameter the maximum likelihood estimate or the least squares estimator is normally distributed and you don't get a you can't construct a confidence interval by taking the least squares estimate plus or minus two standard errors that doesn't have the right coverage if in fact the parameter is equal to one or if the parameters is close to uh to one and so in that case you're you're forced to choose between these these different uh paradigms and I think okay Nam I think they're sort of taking the frequencies perspective makes it makes things very complicated because you end up with with confidence intervals that can look very awkward I think in this case they're not necessary not necessarily uh simple interval but it can actually be disconnected sets I think it can be empty in in some cases but it's it's a lot of work um to uh to get standard to get frequentest confidence intervals even though it's extremely simple to get uh Bayesian probability intervals I said it's very nice paper by Simpson ulick here in economic n91 it's where they discussed this problem at grade length and they sort of they look at the behavior of the likelihood function and and why these problems arise so in that case you do have to make a take a stand on where what uh what you're trying to uh to get a sort of personally I find it very hard to see the the case for for looking at frequencies confidence intervals and submit them I think that there's certainly a number of people in the profession would agree with that there's certainly a number of people in the profession uh we disagree with that but I think the large majority sort of somewhere in the middle where they look at problems where it doesn't really matter and where it doesn't uh it's it's not a question you really forced uh to uh to answer um now let me talk a little bit about the the Practical issues uh of uh of doing Bayesian inference and so traditionally sort of up to the the mid-80s early 90s so that there was this this General uh set up where you specify any prior distribution in a model and then you can figure out what the posterior distribution is in practice you can actually really do that and so in practice sort of if you had this normal suppose you have a normal model for the for the data then you pretty much had to pick a normal prior distribution because otherwise you couldn't do the calculations uh and so there's sort of always this bit of a disconnect between now what what was being preached about the generality of this and what you could actually do in practice is well you can choose any prior you want as long as it's a normal prior because otherwise you can't you can't really implement this but it um this um this numerical literature sort of developed a very powerful set of methods for for obtaining draws from the posterior distribution given the data so the idea was would be to just generate uh to sample from this posterior distribution and then approximate the posterior mean by just taking the average of of these draws so there's a couple of key ideas that that make that work in extremely complicated uh models and I just want to briefly mention these three ideas and sort of give some sense why this this um this can be really powerful and so the the general idea is uh to construct a sequence a constructed chain of values such that for lots that after doing this for a while Theta KB can be viewed as a draw from the posterior distribution of uh of theta given the data and so um the way you get there is to set up an algorithm that given a current value of the parameter Theta K and given the data gives you a distribution to draw from you draw from the distribution you get a new value Theta K and if the original Theta k you get a new Theta K plus 1. if the original Theta K came from the posterior distribution then the next one will also come from the posterior distribution this sort of seems a little circular because now you have you need a you need to draw from the posterior distribution to um to initi to start this uh but to the the now what turns out to be this isn't there in most cases irrespective of what the starting value is if you just do this for a while the you can interpret Theta K as a draw from the posterior distribution of theta given given the data and so the way you implement this is you just pick pretty much an arbitrary State and not you then run this chain and it's in the terminology of this literature you run this market chain on the Carlo for a while you get a million values Theta 1 Theta 2 up to say to a million they're not independent but they're all coming from the marginal distribution and so because as long as they're not completely dependent as long as the dependence is somewhat limited you can estimate the posterior mean by just averaging the the last K minus K naught values so you let this uh burn in for a while you let it to take out the effect of uh of the initial starting value you let this run for a while um and then it's throwing away the first K naught minus one dros and then the last K capital K minus K naught average to get the posterior mean you calculate the second moment from that to get get the variants and so the the that is all sort of that's sort of the general idea and then what's uh what's happened in this literature that there have been a couple of very specific uh insights that can make this work uh very well in uh in practice in models where other methods weren't working particularly well because obviously at this point it's not clear why it would be so easy to come up with sort of the key thing here is obviously to come up with this distribution F of theta given Theta K and and the data so there there's a there's a couple of of of insights so one uh one Insight is uh the the Gip sampler idea to partition the factor of parameters uh Theta into two or more parts and instead of sampling uh static K directly from the conditional from some conditional distribution of theta indexed by say okay in the data it may be easier to sample one given the other and the other given one so for example if you're trying to sample from The Joint from a joint normal distribution from a bivariate normal distribution maybe it's difficult to sample from a bivariate normal distribution but it would be very easy to sample from a univariate normal distribution you do one first then the other and then keep iterating between these two now that's a very simple example on there it's actually not very hard to sample from a joint normal uh but the I'll give some examples and and this these empirical examples later make it extremely useful to partition this parameter space into Parts where it's very easy to sample from one given the other and the other given uh given the first and so the the second um one and that's sort of you can also feel that this is a part of the they gave sampler this idea called Data augmentation and so here let me just illustrate that with a with an example suppose we're interested in in the center regression model turbine model uh and there's some latent variable that follows a linear model uh y Stars X beta plus Epsilon what we observe is is the maximum of zero and Y star I suppose Epsilon has a standard normal distribution we could free up the variance as well but that's that's not really uh in an issue here so if you actually observed what I started this would be very easy we could we would just have a linear model uh we have a linear model if you have a normal prior distribution for for beta that would actually be an analytic solution for the the posterior distribution but we don't observe our Star All The Observers the maximum of uh of zero and Y star and now there's no analytic solution for the posterior distribution and if beta is sort of reasonably high dimension now trying to get the integrate out the likelihood function to get the posterior distribution would be extremely uh difficult messy so what is the what is the way to deal uh with that the the Insiders that is it's very useful to view both the latent variables the Y stars and the betas as potentially unknown random variables obviously if You observe some of the Y stars but the ones that we don't observe are going to be viewed here as as random variables and so then the sort of data augmentation combined with The Gap sample consists of two parts first given a current value of the parameters beta we draw the the missing we impute the missing y Stars not by that conditional mean we draw from the the conditional distribution of Y star given the data and given beta so if you actually observe y star that's that's the general distribution but for the ones where we where what we observe is y is equal to zero y star comes from a truncated normal distribution that's very easy to draw from and that's very fast we do that for all the missing observations given this initial parameter beta k then in the Second Step given the all the the partly observed and partly simulated by star we draw from the condition no posterior distribution given the data and given the the why start that again is very that is just a normal uh posterior distribution so it's very simple to uh to draw from and now we're going to iterate that because obviously for imputing these y Stars we use the the wrong we just use the single value of beta I said that isn't necessarily the right value beta we need to um sample from different values of beta and we do that by sequentially going back so by going back and forth between drawing this between simulating these missing y stars and drawing from the posterior distribution of of beta given uh given the Y Stars you do that a million times you take the last 900 000 values of beta those are viewed as uh as random draws from the posterior distribution of of beta and in this case now let me see in this case getting it and there's no way of getting an analytic solution for the posterior distribution getting doing this by numerical integration actually directly getting the form for the posterior distribution is pretty much impossible if you do this for bait for bait of a dimension greater than uh than five or so but getting draws from the posterior distribution is extremely easy it sort of you set it up in a couple of uh I mean it's very easy to program and then it um um it's a matter of letting the computer run for for a little bit and you get all the draws you want and so the this this idea is just extremely widely applicable um the sort of there's a lot of work so some of Ross's work on in the discrete Choice case does this for uh for multinomial probate uh models and I'll come back to that later but almost all these latent variable models the feature is that the analysis would be so much easier if we observe the latent variable and these Bayesian computational methods exploit that by imputing these these latent variables and then doing iterating between that amputation step and the analysis given the the complete data um the last sort of these these insights in in some sense this is the most uh General one and sort of allows you to deal with all the cases that don't you can't really take care of with directly with data augmentation um again if you're interested in in drawing from a particular posterior distribution uh and the idea is that it may be easy to evaluate the likelihood function but it may be difficult to draw from that and now the the idea is to draw from some relatively arbitrary distribution and accept or reject the the draws with some probability that depends on the ratio between the the posterior and the at the the candidate value and uh this uh this distribution withdrawing from Q of theta so you calculate this probability row and we keep to accept a new draw with some with probability row and reject it with probability of one minus rho ideally you would draw directly from the posterior distribution but the whole point is that that's typically very difficult but using any widely dispersed distribution will work here and so what the way you use this in practice is to use a normal distribution or some in with a relatively High variance and use that to generate the draws and then accept or reject them with the probability coming from from this ratio okay so now let me uh spend the rest of the the time on the on some of these examples as it is a whole series of paper by uh Rossi McCulloch and Alan B and and various uh co-authors this is a this is a fairly early one there as a in fact sort of the model is more limited than what they they have used in subsequent work but I like the application here um so they're interested in uh sort of optimal design of coupon policies the idea sort of supermarkets could give blanket coupons to everybody but they may be they may do better they may be profitable to make the the value of the coupon differ by by household or by individual and so you may yeah give individuals or households that are more price sensitive coupons of different values than than households were not or if you're doing so the application here is to uh to can to demand for cans of of tuna so if there's a particular household that only buys whatever taken off the sea you might have to give them a big coupon before they're willing to buy nah I don't actually know any of the other brands um I'd have to look that up but sunkists yeah so so but it sort of it's clear that there's an in principle you might want to tailor the value of the coupon to uh to the individual you're giving the coupon too and you see a very simple way would be if you actually had individual information about these households sort of their their income or their demographics but you could imagine that um with this scanner data you may actually know for each household what they bought before and so you may try to tailor the the value of the coupon to their whole purchase history and so in order to do that you clearly so you for some households you're going to have a lot of data you're going to be able to estimate very precisely how sensitive they are to uh to price changes for other households you know pretty much nothing and there's no there's no there's nothing you can say about uh the optimal value of of the coupon for for those households so you're not going to be you shouldn't try well there's two things one you need to allow for a lot of heterogeneity in individual preferences to make this work because if everybody is equally price sensitive then a blanket coupon is going to be the right thing to do no matter what the data what the results of of a model with constant uh elasticities but given that uh for some households you're not going to be able to estimate things very precisely you don't want to just estimate these models all separately and so what uh Rossi McCullough and lmb do is use a hierarchical random coefficients model where they allow the each household to have individual no preference individual household specific preferences for the for the different characteristics of these goods and so the um I said I guess it's pretty much a standard setup in demand analysis in IO these days uh you have a utility for household i product J and purchase time T depends on the characteristics of the product at that particular time it's actually the X should be indexed by day uh this would be a j um with individ with household specific coefficients beta and then there's a Epsilon IJ T that added on that is independent across households uh products and purchase times um so the the Choice specific characteristic here would be some in this case it would be price as well as some some Market marketing variables uh but at this the the this particular brand is featured yeah or not and so here in this analysis all the the choice characteristics I assume to be exogenous I think that that sort of always controversial in the in the i o literature but I'm I'm not going to nah sort of address that here really um and so so partly to take account of the fact that for some households they have very few purchases they're they have very limited information you couldn't possibly estimate all these beta parameters separately so would they the way they model this is by having the modeling the household specific taste parameters beta is coming from a normal distribution center at uh mean that depends on on permanent individual characteristics plus some some normal error term with some variance covariance Matrix for the for these unobserved components now so in principle this is just so far this is really just a random effect setup and so you could try to estimate the the common parameters the gamma here that the sort of factor loadings for these individual characteristics The covariance Matrix for the unobserved components you can just try to estimate them using uh using maximum likelihood but partly what Rossi McCullough and Allen B want to use this for is have this feed into this optimal coupon policy so they really need for that in principle they need the the household specific betas and so setting this using a Bayesian perspective here will in addition to giving you posterior distribution estimates for the the common parameters will give you um estimates for the posterior distributions for the bay test and even though the model itself here may be uh maybe normal the marginal distributions of these betas are necessary well in general are not going to be normal they come from mixtures of of normals and sort of in a complicated uh complicated way and so now you know we wouldn't have a distribution theory for this uh for these for the sampling distributions for these uh beta hats and so it's much conceptually much cleaner to just look at the posterior distribution there and use that to figure out what the the optimal coupon policy is so just to uh and this is obviously necessarily very helpful for for implementing this for for this particular case or for any other case but just to give you a flavor of what this uh what implementing this would look like now you would sort of use a combination of this uh these Gibson data augmentation steps we want to draw the from The Joint from The Mark the posterior distribution of the betas the Gammas and the sigmas and the Sigma J squares which interferences of the idiosyncratic utilities for the different choices um the way to do that is separate things in uh separate the first augmented the parameter factor with the latent utilities and then separated into a couple of different subsets of the parameters let's refer one step is to draw the household parameters given imputed utilities and given values for the common parameters that's going to be very straightforward forward because we now you would have a standard normal linear model uh for the utilities given with the beta as these parameters and with a normal prior distribution at that point and you can do that for each household you can draw the the sigma J Square sequentially because that's just a normal linear model with known mean and unknown variance you can draw from the posterior distribution of the common parameters gamma and sigma given the betas because that's going to be uh again a normal linear model and the last step is just drawing the an observed utilities given the all the parameters and given the data and doing this sort of one household one choice at a time conditioning all the utilities for all the other choices this is going to be involved repeatedly drawing from a truncated normal distribution so the key thing is um that all that this in principle this huge uh posterior distribution but sort of if we have an observations we have n of these parameters beta I as well as all the the as well as the common parameters can be broken up into a number of steps that are all extremely simple and uh and straightforward to do putting them all together gives you a very gives you conceptually very straightforward way of getting draws from the the posterior distribution of the um the household parameters given and sort of given the data and so just to to illustrate this I've ever some figures from this Rossi McCulloch and Allen B paper so here for for 10 of the households uh sort of labeled there somewhere at the um the bottom of the the figures at the top for each of these households they look at the posterior distribution for the price coefficient taking into account different information sets so the idea is that if the supermarket could only use demographics for in for choosing so if well if the supermarket had no information about the households all they could do is uh is choose the same coupon value for each individual now for each household but if they had more information they might be able to tailor these coupon values more specifically to the households and the the different figures here show how much information the supermarket would be able to infer about the the price coefficients for these particular households given different information sets so the bottom right you see that if all they knew was demographic and if all they could use was demographic information and instead of moving to the top left you can see that if in fact they had full information from sort of all the choice information that is in the data set they would be able to infer much more precisely for each household how price sensitive they were and obviously that would be very valuable um for um for tailoring this uh this coupon values to the to the households okay um let me look at a second example um so this this is uh based on a paper by uh by Gary Chamberlain and Kay hirano they were interested in in deriving predictive distributions for earnings uh using panel data so they had a linear panel data model where local earnings is linearly related to some covariates and then there's Auto regressive component in in this model of vit here there's an individual specific permanent component Alpha I and then there's a the idiosyncratic component uit that is independent over uh over time and Acro as well as across individuals so the vit follows the first order the regressive process there is a common parameter um the with the initial condition coming from a normal distribution the initial value coming from a normal distribution and the increment coming from a normal distribution and the key the the key difference between this model and and sort of many a lot of the models that have been used in this setting uh is that they allow for an individual specific variance so this HR here so the idiosyncrat the last term in the log earnings model um as a standard normal component multiplied by one over H is a very one over H is the standard deviation and so trying to estimate this trying to estimate sort of both the individual specific uh component the additive individual specific component Alpha I and H I directly clearly wouldn't work with sort of the standard frequencies argument would be that for fixed t with the number of observations going to Infinity you wouldn't be able to estimate those consistently in there from a basic perspective uh the sort of the pro in some sense the problem is that using a flat prior Distribution on a very high dimensional parameter space uh doesn't really uh work very well and so analogously to this uh hierarchical model that uh Rossi um we call like an lmv use Chamberlain and hirano use a random effects or hierarchical model but his Alphas are viewed as coming from a normal distribution and the ages are viewed as coming from uh from a gamma distribution and so now instead of directly trying to estimate this promises thus they go to use prior to specify prior distributions on the Sigma Alpha squared as well as the promises of the gamma distribution M and Tau but the the hope and the presumption that by now having a model with relatively few parameters the prior distribution is going to be much less important uh and and so that's very likely to work well here just having a couple of parameters with the large amount of data is going to give you results that are fairly uh insensitive to this to the choices of this prior distributions and so using uh data from the PSID but they they the main thing I'm going to show from from this paper and this is just a table taken from that paper they find that there's a lot of evidence that this heterogeneity and this uh these variances and this reverse a lot of this the the standard models assume that the variance is the same for all uh individuals they find that there's actually considerable variation in the and um in the this considerable dispersion in the distribution of uh of AIDS and so here in terms of the the conditional standard deviation of that uh that term you see that the 0.1 quantile for this standard deviation of uh of log earnings is 0.05 and the 0.9 quantiles is 0.45 uh if you average in overall overall in the videos you know more specifically Nam they want to actually look at predictive distribution at the individual level instead of taking two particular individuals one who had a relatively large amount of variation in their their log earnings sort of after taking out this uh the other components among we had a relatively low amount of variation with the sample standard deviation of of the residuals 0.07 versus 0.47 what they uh what this model shows you is that even five years later there's much more dispersion in the earnings distribution for for the the second individual for the individual with with a lot of uh variation early on then then for the the individual with very little variation in uh in their their earnings early on it's sort of clear that this this for for various uh policies of individual Behavior this may may very well be uh be very important okay um let me do a couple more examples uh I said this is uh from an early version actually of a paper and um that I wrote with with Gary Chamberlain where we're looking at uh the the many instruments uh problem from a from a Bayesian perspective and so the um the way we set it up there is we looked at the reduced form for uh for years of education so this is um so this is applying this to the the the angris Kruger problem Vietnam look at a case where we had a potentially large number of of instruments so it will have a linear reduced form for years of education as a function of of all the instruments in a linear reduced form for for log earnings with no the parameter beta multiplying the the reduced form of the of education so beta is the return to education here the parameter of Interest assuming uh joint normality for the reduced from disturbances you get a like loot function so now we have a fully parametric model we get a likelihood function that we can try to use either by doing maximum likely decimation or by using some Bayesian procedure and so the key here let's see the focus of that paper was that if you're interested in beta we may be concerned about the sensitivity of of those inferences to the choice of the prior distribution with particular in particular with respect to the the use of so many instruments and so one way to think about why that that could be a problem is um to to think not just in terms of the prior Distribution on on Pi one but to think about the prior Distribution on the sum of the squared pies in some sense the sum of the squared price measures the total amount of information that is in the instruments uh if we if we're concerned about weak instruments we're concerned about the possibility that all the Pi's are equal to zero or close to zero and so in fact concerned with the the possibility that the sum of the pi the squares of the pies is close to zero and now if we use a flat prior on a large number of uh of this price if we use a flat Prime on Pi one in the setting where we have a large number of instruments that essentially the implication of that is that we put all the probability all the prior probability Mass on the sum of the pi squares being fairly large if we say that the the probability that any of these pies well sorry if we if we choose a prior that is very dispersed that puts a lot of Mass on sort of that that is very flat in the large part of the parameter space the implied prior Distribution on the sum of the pi squares is necessarily uh necessarily very little Mass on the on the area around zero which is actually the area where we're most concerned with and so now if you're concerned with the parameters being in a particular area of the parameter space you clearly want to avoid choosing it using a prior distribution that rules out that that area and so what uh what we don't show is that if you actually do that if you choose a flat prior distribution for pi one you get pretty much two stages least squares back so in some sense the flip side of that is that you can interpret to stagely squares estimator as pretty much the same as using a Bayesian approach where you use a very unattractive prior on on the first States namely a flat pry on the on the first stage and so what uh would be that did is uh use an alternative prior Distribution on on the price but we assume that each of the first of the reduced form coefficients came from a normal distribution with some common mean and and common variance and so that now we're we're reduced the problem to choosing a prior for uh mu pi and sigma squared Pi that's a very small low dimensional parameter space that's very easy to think about what uh what what a prior distributions that are relatively uninformative and then what you actually find in the data is that most of the posterior mass is concentrated on on very small values of uh Sigma Pi squared which makes perfect sense you know that uh the two values for these pies all have to be pretty small they don't necessarily all have to be equal to zero but you in fact have pretty strong substantive prior reason to believe that this that all these reduced four coefficients should be fairly small and ignoring that by by using methods that sort of treat these spies as being completely unrelated and possibly being extremely large is is in this case a very wasteful uh way of of using the the data and and one that is uh that turns out to be extremely misleading yeah okay um let me do one more nah in some sense that's a much sort of very substantive uh example here um this is a paper by Gary Gallery son Karan and town we're interested in estimating the effect of hospital quality on um mortality and so obviously you don't want to just compare mortality uh sort of raw mortality rates by Hospital uh because this is considerably concerned that considerable reason for concern that more sick patients would go to better hospitals or at least we try to go to better hospitals and as a result good hospitals may have relatively High mortality rates just because they get sicker patients so the way resource authors set us up is they set up a linear model for for latent mortality that depends on the the hospital you go to as well as on on individual characteristics uh then in addition they model Hospital Choice as a discrete Choice problem but now we're just as in uh in much of the the current i o literature there's a large number of uh of choices I said they they modeled this through latent utility setup where the utility associated with going to hospital uh day is is linear in the characteristics of uh of that hospital sort of possibly interactive with some individual characteristics and individual and potential patients then choose the hospital that has the highest utility now for them sort of given their their characteristics and so the way they model the endogeneity here is that they they allow for correlation between the unobserved preferences for the hospital the Ada ideas as well as the um in the unobserved individual mortality Epsilon I through this linear relation where the Epsilon is linearly related to all the the Ada ijs with hospital-specific coefficients Delta J so um then sort of this is obviously again a case where you have a fairly large number of parameters trying to figure out what the likelihood function look like looks like evaluating the likelihood function would be extremely difficult in principle to in principle this would be possible using simulation methods in practice this would be a feasible but setting this up from a Bayesian perspective and using these Markov chain Monte Carlo methods it's actually going to make it fairly straightforward to get draws for for all the the parameters of interest and for doing any any policy experiment that you might want to do in in this case and so but they um again the the log likelihood function in this case would be hiding non-linear frequent this methods in the end would require that it sort of be well approximated by a quadratic function which is very uh which is clearly not going to be the case uh here it's going to be uh multimodal um and so that's that's part of their motivation for using basic methods uh here as well as computational that it's just going to be much easier and so and when in their empirical analysis they actually find that this works very well they find strong evidence that the selection into the hospitals is um is non-random and sort of a new variety of other things okay um let me do the last example this is on some recent work I did with uh with Susan um so we were interested in in discrete Choice models sort of in these now somewhat related to the the Rossi McCullough lmb settings but sort of more closely related in a sense to the the Barry Levinson Pecos set up but in contrast to that to that study using um using individual level data and we were interested in um allowing for for possibly multiple unobserved choice characteristics sort of we've done some theoretical incident we did some theoretical work showing that in principle in order to to be consistent with observed Choice data you may need more than than one unobserved choice characteristic once you allow for that it's going to be extremely hard to uh to do maximum likelihood estimation you again end up with a likelihood function that is going to be multimodal and so any approximation that is fundamentally based on on a unimodal likelihood function that is approximately quadratic is going to be uh very inaccurate uh so what we did instead is set it up uh from a Bayesian perspective sort of using the the pretty much the same latent utility setup that that is these earlier papers used allowing for allowing the utility to depend on Choice specific characteristics with individual specific parameters then allowing for an observed Choice characteristics within with individual specific parameters as well as some idiosyncratic error term and then modeling the individual specific parameters is coming from a normal distribution center at a linear combination of individual characteristics with some um some parents and then so even though in principle this this would be very hard it would again be very hard to set up to calculate the likelihood function uh setting up a Markov chain Monte Carlo algorithm to get draws from the posterior distribution is conceptually very straightforward you break it up in lots of different parts Where You observe conditional various and observed latent variables and with all the steps being being at least conceptually straightforward and so now I mean there'd been earlier work where people had had used you know multiple unobserved choice characteristics uh in particular some work by Elrod and Keane and gutler and shekhar instead of talking to uh we presented it at some point in with Marquina in the audience and he was he was very much in agreement with who sort of his point was that had he known about those basic Methods at that time which was the time really they were being developed he would have used those for that paper as well because it seemed a much more sensible method to him because they were with what they did the computational burner was was very heavy okay um any questions here yes I'm worried that at least in the beginning especially if you have a diffuse prior or impressed prior and you're doing these frogs and if we have a product used it might be competition with methods for either waiting for paragraphs or some sort of thing like that that would actually make the initial part of the numerical procedure more um yeah I mean this is actually the the first comment that let me first make a comment on the first uh thing you said I think it's sort of more the perceived computational difficulties I think nowadays these methods are actually just much simpler addition a lot of the these basic computational methods have actually been implemented in in packages um there's a um there's several of those packages around I don't actually have that much experience uh with any of of them but um there's a program set up software package called bugs that there's a lot of this um there's actually got an email from Peter Rossi the other day saying a lot of the the models he's implemented uh he's made software available for but that's a that's an r and some I suggested he writing in state and he said no but so if you want to use R which is this is actually freely available um but then maybe yeah but then then you can use a lot of these things very uh very directly actually on the same pane if someone asked earlier about some of the software for the the things we talked about yesterday and so for for some of the evaluation stuff there is some status software available um the the some references on my on my Harvard website so you can look at some of them up up there and but then sort of getting back here so the the I mean there's a lot of theoretical work on how well these Markov chain Monte Carlo methods work that doesn't seem to quite capture quite how well they work so it turns out that in practice they just work phenomenally well much better than you you then you would have reason to expect and so is it in most in most of these models they just pretty much I mean you you clearly want to start somewhere where you think the parameters are but unless models are very extremely non-linear and and irregular these things tend to converts very fast and and work very well and so um I mean I'm not quite sure um to the in the um in the notes there's a bunch of references to the literature it's one of one of the most readable introductions um to uh to that literature and actually from a more econometric point of view is a 2004 book by uh by Tony Lancaster um where he also kind of gives some links to some of the software but it just turns out in practice these things work extremely well so there's a lot of things you might expect could go wrong but it's it's very rare that these things actually uh do go wrong in in practice yes no I think sort of fundamentally these methods do the calculations more efficiently by evaluating the probabilities sort of in the right area so what do I mean by that so if you look at uh sort of frequency simulation methods for these discrete response models so you need to do this at lots of different values of the parameters including some that are far away from the truth that are in the end irrelevant where these probabilities may be hard to estimate because they're very close to zero and so taking logs could lead to a lot of variation these Bayesian methods move around sort of in the end given that the the draws correspond to draws from the posterior distribution they tend to be in area where where the prime around the true parameter values and so they're much more efficient in in that sense they don't they don't waste a lot of uh of draws on the relevant parts of the parameter space I submit it there there's some work um so I'm trying to think actually what the exact reference is but sort of there's this literature in the in the 90s looking at simulation methods for these discrete response models since there were these some work by kvk has a facilio keen and in the end I think the the sort of their eventual conclusion was that Gibb sampling was better than than the the frequency simulators they they developed I think the sort of there's a review of economic statistics paper by with Gary king among the authors that that makes that point in the end yes then Riders are yes I mean if yeah I mean in the end if the likelihood function is flat over a lot important part of the parameter space then it means we cannot say much so that no matter what what methods we use on the other hand if the the likelihood function sort of has multiple modes so at that point frequent this methods are going to have a lot of trouble because sort of with the standard thing would be to use in a quadratic approximation to the to the log likelihood function and that's not going to fit very well and sort of none of the methods it's none of the non-normal approximations tend tend to be very tend to be particularly helpful so in that case basic methods I think are are very useful but in the end it's sort of clear that um that good practice would be to uh to report these things and Report the sensitivity to the to the prior distribution and that that's not all that that difficult now um on this paper Susan actually one of the the editor complained that wow but these patients they never actually report the sensitivity to the prior distribution in some sense that's not so different from the fact that almost never do people report evidence on the quality of normal approximations and it really that is pretty much the same thing now that doesn't excuse the other part I mean it it clearly is important to report sensitivity to the prior distribution and I mean it and I'm not saying that that sort of being very formal about a Bayesian perspective is is clearly useful in all settings um I've had this discussion a couple of times with manski about uh what the the FDA should do whether they should sort of few drug approval from as a Bayesian decision problem and it clearly doesn't seem very sensible to allow the introduction there of Prior distributions but I think that's because that's a much more complicated problem where there's sort of a repeated game between drug companies and and the FDA where drug companies are trying to no use the rules to that to maximize profit subject to whatever the rules are and allowing the introduction there of subjective knowledge could make that much more complicated but in a lot of other cases it's I think it's very clear sort of in this case if it's relatively low dimensional parameter Space is having a flat prior is typically a reasonable starting point and combined with some evidence of the the amount or the lack of sensitivity to that choice I think it's a very sensible thing to do 