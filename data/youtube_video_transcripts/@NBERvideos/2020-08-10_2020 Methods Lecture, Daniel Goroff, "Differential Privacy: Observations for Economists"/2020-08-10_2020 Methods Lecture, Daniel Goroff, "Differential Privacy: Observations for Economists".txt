Daniel Goroff: It's my task
to present a brief overview and a little bit of context for economists on
differential privacy. As I hope many of you know, the Alfred P. Sloan
Foundation supports research on science,
technology, and economics. But the way I like
to think of it is that we're in the
public goods business. Here we are. So we're in
the public goods business. This gives you some examples of the things that
we've been funding. We do so because public goods, after all, are non-rival, they're non-excludable,
and that makes them very hard to finance, otherwise. There are some great
examples here, I would just call your
attention on the upper right to a very small icon
that says OpenDP. The DP there it stands
for differential privacy. It's a large community
effort to build suites of differentially private software
for use at large scales. That open software,
open datasets, and also, of course, fundamental basic
discoveries are all great examples
of public goods. We'll come back to this
issue of public goods. But the OpenDP is, at least, one of the
examples that Sloan has funded over the past
dozen years or so about researcher access to
private and sensitive data. I don't know about you, but I really don't want
to live in a world where all of your fuel
to tech companies and other institutions know
what's really going on. Once you start thinking about privacy protecting
research protocols, you realize that there's
an inevitable trade-off between privacy and accuracy. So you have to
obfuscate somewhere. You can do [inaudible]
at the input stage, at the computational stage,
or at the output stage. That gives you two to the three, otherwise known as eight
different examples of how you could make that
trade-off and where you could put it in
the obfuscation. There are examples of them
listed in this chart here. The first of them is open data where you don't
try obfuscating at all. The next three are
really interesting. We rely on them, data enclave, disclosure agreement, and so on. They mostly work, but they actually provide no guarantees. It depends on everyone's
discretion to make it work. As far as anonymization, if someone tells you that they've anonymized the dataset, my advice is that
you should laugh. It's really not
possible to do that. On the contrary, Cynthia Dwork, she said, "Sanitizing data doesn't, and de-identifying
data isn't." This is just isn't a matter of observations, or, to you it
seems pretty hard to do. These are theorems. In particular, I would point you towards in the database
reconstruction theorem, which says that,
regardless of any linking, regardless of anything else, that you can really
reconstruct datasets if you answer too many
questions too accurately. This is actually a
practical matter as well. It's not just a theorem
that's been written down. It turns out that, even
from the census tables, as I think we're going to
hear a lot more about, you can uniquely identify tens
of millions of Americans. From the census tables alone, you can determine that there's exactly one person
of a given age, gender, location, etc. Once you know all that, you can, of course, find lots of other information
about them these days. That might seem like
a surprising result. But if you think
about it this way, perhaps not so much so, the Census Bureau, after all, releases billions of numbers in those tables. But there's somewhere less
than 350 million people. So if you do enough
fancy linear algebra, it turns out that you can
uniquely identify people. So those ways of doing
privacy protocols are interesting. We rely on them,
they mostly work, but they don't give guarantees. Another one on this list, the fifth one, which is
randomized response, and I've done this with
my students before. If you want to ask an
embarrassing question, such as whether you cheated
on an exam ever, you can have the
students flip a coin. If they got heads, then they should
answer truthfully. Otherwise, they
should flip again and say yes if it's heads
and no if it's tails. You can see that you
can quickly back out of that unbiased estimate of how many people in the
class has cheated. This is interesting. We'll hear more
about it shortly, but it actually does give
some privacy guarantees. The next one on that chart that gives some
context for all of this, it's called
multiparty computing. It answers the
following question. It's a great party check. Ask people. If you have three of them together,
and you wanted to compute quite precisely
their average income, could you do that without anyone telling anyone
else their salary, without writing it down, without giving it
to a third party, without doing anything
else like that? Could you get their
average income without actually
revealing anything? That sounds pretty difficult at first until you
see how to do it, and I'll tell you
that all you need is some random numbers. Each person computes
two random numbers. Each one gives one
random number to each of the other
two people who are sitting there with them,
say, at the table. Then they take their own salary, they subtract the
net random numbers that they gave away, and they add the random
numbers that they got from their two compatriots. So now you have a new
number. Call it X. That number, it
no longer reveals anything because it has
all these random numbers added and subtracted. However, if you add up
all three of the X's, all those random
numbers just cancel, and so you'll get the
sum of all the salaries. If you want the average, you can divide it by three. This is an interesting
way of trying to do privacy protecting
data collection. It has some advantages
and disadvantages. I won't go in any detail here. You need rather
bespoke algorithms. Obviously, if you think
about the game theory, as this [inaudible]
collude with one another, then you can recover what
everyone's salary was. If two people collude, that is, you could recover the third. Also, it does deliver
an exact answer. That may seem like an advantage, but I'm going to argue that it's actually
a disadvantage. The next to the last
one on this chart, the last one being
differential privacy, of course, is fully
homomorphic encryption. Let me describe this again
in terms of trying to get your average salary with
mine. Here's what we can do. We could both to encrypt our salaries using our own keys. Mine encrypts as Qx93aW, yours encrypts as a2T5zN. We could give those
encrypted codes to a third party,
say, in the Cloud, and tell them that we'd want
to get the sum of the two. They do not have our keys, so they can never encrypt this. Nevertheless, if they
know what they're doing, they can evaluate those
encrypted values, come up with another
encrypted code, say 78AbC3. Then when they send
it back to us, and we decrypt it with our keys, we actually got
the right number. We got the average of the salaries, if
that's what we asked for. This should sound
a little crazy, and it should sound like magic. It was believed to be impossible until someone trying
to prove that it was impossible, actually
just a few years ago, showed that
you could do it. It's enormously
computationally intensive, but they are beginning to be actual applications for specific kinds of calculations
where you can do this. But that's at least another way of thinking about
privacy protection. Again, it gives you
an exact answer. Let me turn to the
last one on the chart, which is differential privacy. Here's the conceptual
framework for all of that. You should imagine
a curator who has a dataset that they're holding, and there are researchers
who ask questions. So when you ask a question, call the dataset x. There is a mechanism
that the curator applies to the dataset and
gives you an answer M(x), and that's what they do. As we mentioned before with
this reconstruction theorem, if the curator were to answer too many questions
too accurately, then an adversary
could reconstruct most of any dataset to a very
high degree of accuracy. They can't really answer too many questions
too accurately. So how do you handle this? What you want to do is to limit how much an adversary can learn about whether the answer comes. So an adversary is
getting this answer M(x), you want to limit how much
they can learn about whether the answer comes from a
database x equals to d, that actually contains
my own information or from a "neighboring
dataset" x equals d', which is exactly the same as d, except for it's missing the
row with my information. D has my information
in it, d' doesn't. If we're talking about
learning, of course, then we're going
to be [inaudible] about all of this
and assume that the adversary has prior beliefs over whether x is equal to d or whether x is equal to d', and we want to limit how much they
can learn from that. When they learn, of
course, they're going to learn using Bayes' rule. I'll just remind you
very quickly what Bayes' law looks like,
conditional probabilities. On the far right, we have the prior odds, and in
that fraction, on the far left, we have the posterior
odds, and in the middle, that fraction is
the Bayes' factor. If you want to say
that from going to the prior odds to the posterior odds that
you haven't learned much, the very easy way
of saying that is by requiring the Bayes factor
to be very close to one. That's how this works. That's Bayesian updating for it. Let's apply that
to this situation. We have these prior odds that x is equal to d, or
x is equal to d'.. You're going to get
some new information, which is whether M(x) is in some set in the image of
M. So it's just telling you that the average is between
9 and 10 or some other piece of information that the curator gives to you in
response to your query and that the query
mechanism M(x) generates. If you want to make sure
that that doesn't reveal too much about whether I am
even in the dataset or not, then I restrict the Bayes'
factor to be near one. So at the bottom of
that screen, you see the Bayes' factor.
The way I do that is I make it less than
or equal to e^Epsilon, where Epsilon is usually a pretty small number but there's a lot to
say about Epsilon. You'll also see why we do it in terms of the exponential
in a moment, but that's all this is. That's the definition
of what it means to be an Epsilon differentially
private mechanism. When you look at that in most of the papers
that I've ever seen, that definition is plunked
down, and people tend not to say that this has
anything to do with the decision problem, or with Bayesian updating, or anything. They just write down that ratio and say that it should
be less than or equal to e^Epsilon. I'm not sure how other
people think about it. Maybe the computer scientists
see something else. But now you know that this is just a Bayes factor that you want to be
very close to one. I'll point out, that because
you can switch d and d', that when you say that
it's less than or equal to e^Epsilon, you're also saying
that it's greater than or equal to e^minus Epsilon. So you really have bracketed it between one plus Epsilon
and one minus Epsilon, which is what you want, at
least, for a small Epsilon. So that's the definition.
That's the condition. One other comment about this is that the probability here, when I write PR, in the beginning, is
a joint probability. So it's over both the beliefs of the adversary and the randomness of this mechanism M. We said that M could be
a random mechanism, it could have coin
flipping in it, or something else like that that randomly determines
the answer M(x), and it's over both of them. But then the probability in
the definition collapses back down to just
the probability over the randomness
in that mechanism. I just wanted to point out that that's the way
the definition works. That's the idea.
It's a criteria. It's not a specific method
or anything else like that. So you have to ask yourself, do algorithms actually
exist that satisfy this? The answer is that if you try to do something that's
entirely deterministic, it will not satisfy this,
and that's easy to verify. Here's one example
of how you can implement a mechanism
that would. Suppose you have f is a
deterministic function. Say it's just counting
something in the datasets. Let capital Y be a Laplacian
random variable, and the mechanism would
be to compute f(x) and then add some of
this Laplacian noise. If you do that for a
carefully chosen value of the parameter that determines that Laplacian distribution, then it really does satisfy Epsilon differential privacy. That parameter depends on both Epsilon, of course,
but also Delta f, which is called the
sensitivity of f, which is the maximum of f(d)
minus f(d') absolute value over all the neighboring
datasets d and d prime.. So it's how much f can change if I remove a row from the dataset. That's really what
it's talking about. In particular for
counting queries, if you happen to work
at the Census Bureau, this might be
interesting to you. Delta f is always one
because if I remove a row, all I've done to the count
is change it by, at most, one or by exactly one. I would ask you to
think for a moment, if instead you were trying to compute a regression
coefficient, how much that could
change if a row in your dataset either
was there or not. We'll come back to
that in a moment. But the main thing
to notice for now is that if you take
a bigger Epsilon, you get less privacy and more accuracy, and
a smaller Epsilon, you get more privacy
and less accuracy. So it really does make
this trade-off idea very precise, so
that's how it works. Here's a few properties. You'll hear more about this, but I just wanted to
highlight a few of them. If my question is, should I allow my information
to be included in a study? Say, it's a study of some
very unusual disease or something else like that, I may really care not just if someone learns my
social security number but if somebody knows
that I was in the study, and so this precludes anyone finding out very
much about that. Because it's really looking at datasets and
comparing them where my row isn't even there at all, then in particular, if
you don t know if I'm there, then you don't know
anything about my social security
number either. That's the idea of this. It gives you that factor to the Epsilon that tells you by how much the base
factor can be changed. A very important thing is that this guarantee is immune
to post processing. So if it's either random
or deterministic, and you apply that to the output of a query
mechanism, it still satisfies Epsilon differential
privacy as long as M did. That means that whatever data is released in the future of the universe, it doesn't
matter. You still have that Epsilon differential
privacy guarantee. You can do a lot of mischief
with that G. If you work at this Census Bureau,
and I don't know why, but you might be
hesitant to release tables that have
negative numbers in them to the general public, so then you might have to do some things to post
process and everything. You will maintain the guarantees of differential privacy by
doing that post processing. But you may introduce
some biases or other problems with the data, and people are
working very hard on trying to minimize those biases and those issues right now. But it's very
important that this is immune to that post processing. It has this beautiful
sequential compensation law. So if you ask M1,
and then you ask or query M2, and get
those two answers, then if each one has differentially privacy,
then so is the sequence. Moreover, you can say
that it's going to be Epsilon 1 plus Epsilon
2 differentially private. This is easy enough
to see if you think about it in terms of the way we put the definition with the
prior odds on the right, the posterior odds on the
left. If you do M1 and M2, you just multiply the
two base factors. When you multiply those
two base factors, the bounds also multiply,
and since they're exponential, when the
exponents multiply, then guess what,
their exponents add. So that's it. There's no
other mystery to this. With any other privacy
protecting mechanisms that you might imagine,
it's very hard to get that composition. I'll just mention that if
you do parallel composition, if M1 and M2 are two
different mechanisms, and M1 only looks at part
of the dataset, and M2 only looks at another
part of the dataset, and you do both of them, then you still have something that's differentially private, but the Epsilon will
be the maximum of the two Epsilons. This
is one of the reasons why if you work in the Census department, you may have always really
liked histograms, but now you really
like histograms because you can get the
maximum like that because the mechanisms are
only asking about one portion of the
dataset at a time. Those are important and
useful laws about this. Let me just try to summarize a few of the challenges
and what this is telling us so it does provide a formal privacy
guarantee, and let me say it doesn't guarantee
you against harm. That's to say if the study
shows that smoking is bad for your health and your
insurance company finds out that you're a smoker
and wants to ding you, then obviously, you've
been harmed in some way. But it does protect
you from harm due to participating
in the study. In other words, your decision to have participated in
that study or not, to give over your data or not, could only change the
results of that study by a very small amount
and no one would know for sure whether you are in
that study or not if indeed it's handled in a
differentially private way. That's the sense in
which it's a guarantee. It provides a conceptual
framework. There really aren't very many others around
that can compete with that, gives you a clear sense of
the trade-offs, and it has this adjustable
parameter Epsilon that tells you how your
trading these things off. Now fixing that Epsilon is a policy question. It's
not a mathematical one. It's something that has
to be decided based on very specific ideas about the situation that you're in and the values that you have. But once you set
for a given dataset x, what you think that
Epsilon should be if researchers are going
to start asking the curator a whole series
of different queries. then each time you ask, you're
using up a little bit of that Epsilon. So
if you want it to stay less than or
equal to that Epsilon, the sum of the
different Epsilon sub i's that you've asked has to be less than
or equal to Epsilon. In other words, it's very
natural to think about this as a privacy loss budget that goes along with the dataset
and a given x. Even answering a very innocent
looking question about x without infusing noise blows that privacy loss
budget and voids the privacy guarantees so that if you are trying to cajole an agency or a business or some individuals into
submitting sensitive data, encrypted or otherwise,
and you still plan to release precise statistical
information about it, it's not so clear why
they should trust you to protect their privacy when you think about
privacy in this sense. Here's the bottom line. Every query that you
answer leak some privacy. You're using up some of
that privacy loss budget. The important thing about
differential privacy is it controls the
rate of doing that. Similarly, every query that
you ask leaks some validity because you're getting closer
and closer to p-hacking. Why would you ever want to sacrifice any privacy
for junk science? The amazing thing,
and it took me a while to understand this, is that differential
privacy also controls the rate
of overfitting. So whether you're dealing with data about people
that's very sensitive, or whether you're dealing
about data about rocks or stars or
something else like that, it still may be the
case that you want to use differential
privacy because it protects you against
that idea of having a huge change in the result when one line
in the dataset changes. Think about the outlier. Whether you're trying to do
a regression coefficient, if that lets you
change one line in the dataset and your answer completely changes
all over the place, then that's not a really
robust or reliable result, on the contrary, we would
call that overfitting. If you don't like using Epsilon differentially private
mechanisms for adaptive data analysis, then that means that you
don't mind overfitting. Again, if someone talks to
you about Bonferroni again, my advice is to laugh
in this context because this applies to adaptive data analysis,
where you're choosing the next hypothesis to test based on what you've
already done before, and I don't know
anyone who actually randomly takes
hypotheses out of a hat. If you don't like
overfitting, you should think about using
differential privacy whether it's private
data or not, and if you can't handle
noise in a model whose distribution you know, I will tell you the distribution and all of its parameters, then I'm going to take away your license to
do social science because I think this is something that we're all
supposed to know how to do, and the bottom
line with all of this is that we think
about open data as a public good, but
empirical evidence from a dataset is actually a
commodity that you use up. It's a rival good, and
this is a theorem. It's not something that's
fixable by new technologies. You actually use it up, but you can control
the rate by using differential privacy. So I
will just wrap up and say that, because it's a good that you use up, you have
to think carefully about the end-to-end solution. Sloan has placed some bets on different ways of doing that that involve differential
privacy, and fully homomorphic encryption,
and multi-party computing. Data linkage is very
hard to do privately, and we have some bets
on how to do that. Administrative data has its
own kinds of challenges. But one of the things that
economics has taught me anyway is that when
transaction costs are high, the solutions are
often institutional, and so there are
some administrative data research facilities
that we've been trying to set up and organize to try to
handle these things. There's also a National
Secure Data Service that people are talking
about these days, that would handle these things as a federal institution. Just as the last slide here, I just want to point out that many of these basic concepts are really very familiar
to economists. It's all about trade-offs, about base factors, about modeling with noisy data, and about an allocation
of scarce resources. Now that we know why it is that both privacy and validity are really precious commodities, so we need a lot
more ideas to help facilitate empirical economics
research in light of these kinds of discoveries.
So people with ideas, I would just
tell you that the Alfred P. Sloan Foundation is interested in
hearing from you. 