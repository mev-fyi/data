so our title uh today as you see is empirical-based methods theory and application i think ajayin gave an amazing overview of a lot of the theory in the first half and kind of the tools available in this area so i'm just a kind of humble lowly labor economist an applied person so i'm here to do the application part my goal is to sort of show you how these tools play out when applied in a couple of concrete examples and hopefully in doing that communicate that this is not only a set of tools but kind of a way of thinking about empirical problems that i think has broad applicability to a lot of the types of settings that economists are now after in practice these days so just to motivate that point a little bit um you know as jim said maybe because of the rise of large administrative data sets large-scale social experiments maybe just the forward march of science uh economists in a lot of different literatures these days are increasingly doing what you might call drilling down to study heterogeneity in fine-grained unit-specific parameters uh to be more specific for in my field of labor economics you know a few examples a classic question in labor is the return to education so the causal effect of additional schooling on your labor market outcomes your earnings the traditional way of thinking about that question is about the effect of going to school for an extra year but as that literature has progressed and become sort of refined we've now started to think about returns to particular types of education returns to say going to a more versus less selective college as in famous work by dalen krueger more recently returns to specific individual colleges as in work by mount joy and hickman example two another classic question is about industry wage premia so do some industries pay more for the same workers maybe because of efficiency wage considerations or rinse sharing or compensating differentials other types of reasons the modern incarnation of that type of idea is a large recent literature using matched employer employee data to look at premium associated with working for specific individual firms and what the distribution of those firms uh firm effects looks like uh final example you know it's long been of interest uh in labor economics and public economics what's the effect of the neighborhood that you grow up in on your future outcomes is it good for a kids say future earnings to grow up in a neighborhood with a low poverty rate versus a high poverty rate in recent influential work in this area raj chetty nathan hendren and co-authors have done a series of papers trying to drill down and look not just at low versus high poverty but effects of specific neighborhoods counties or even finer geographies like census tracts what's the effect of one census tract versus another and i think in all of those types of settings where we have these estimates of many unit specific parameters effects of individual educational institutions effects of individual firms effects of individual neighborhoods uh empirical bay's methods provide a very useful framework for thinking about answering those sorts of questions on several dimensions okay so first we could ask questions about the distribution of parameters across units how much better are the best neighborhoods than the worst neighborhoods what's the spread in neighborhood quality look like that's a question about a distribution of parameters empirical bays as a natural way to approach that second once we know something about that we can also use this to improve our estimates for individual units rather than thinking about each neighborhood or school in isolation we can kind of interpret the estimate for each individual unit in the context of coming from a distribution of many estimates that we know something about and we can derive estimators and decision rules for each individual unit that in some aggregate sense as ji ying said perform better than uh estimates that consider them in isolation and finally for making decisions so this includes both you know policy type decisions if i have some policy that i want to select a specific set of units maybe i want to recommend to people what neighborhood they should move to maybe i want to select schools as high or low performers empirical bays gives us a way to think about what functions of the data should we use to make those sorts of recommendations and then second more practically for us as scientists this can help us with decisions about what to report you know if i'm doing a study of firm effects i have an estimates of a million firm effects it's probably not a great idea to make table one of my paper just a printout of a million estimates okay i want to distill that information produce some kind of function of that information to communicate to the audience what's going on in this context i think eb the eb philosophy is a really useful way of trying to digest that information and decide what to report all right so with that as background the goal for my part of the talk i'm going to recap some base very basic theoretical elements that jiaying already covered through two concrete applications and see how they play out the first is an application to school value added in boston estimating the effectiveness of middle schools this is loosely based on the anguist at all 2017 paper on the reading list there i'll take what you might call a classic parametric eb approach just to see how the methods work and get comfortable with using them second application will be uh application to a large-scale resume correspondence experiment trying to study the distribution of discrimination across large employers in the u.s labor market from a recent klein rose-walters paper there we'll extend the classic parametric approach to try to leverage some recent tools in non-parametric and robust eb and see what kind of extra stuff we can get using those tools so i don't have photos of my co-authors but it goes without saying that the other names on this slide contributed a lot to this work and i've largely learned much of this from them okay so let's get into the first application just to set the stage i can go quickly through this because it's uh similar to some things you already talked about let's think concretely about a population of students in some district say boston that we're interested in studying index the students by eye each student is going to attend one of capital j schools in the district and i have in mind a very simple potential outcomes model where every student i has a potential outcome y i of j associated with the attending each of the possible schools in the district so why ifj tells me how i would do at j in terms of an outcome think of a test score okay i'm going to take a very simple model for those potential outcomes an additively separable model with a school effect beta j and a student effect epsilon i okay so beta j we're going to call that the value added of school j it's a causal parameter giving me the quality of that school epsilon i is everything else represents unobserved student heterogeneity family background ability whatever i'm going to normalize the mean of that to zero so you could think of the betas as mean potential outcomes this is a simple model because it's all constant effects if i take any student and move them from say school k to school j and this model the effect of that is beta j minus beta k so no heterogeneity no match effects we could obviously add those but for our purposes it's going to be simplest to keep things in terms of constant effects to learn the tools all right so even in that simple model there's several questions you could be interested in in this setting first you might be interested in the value added of a specific school say beta1 how good is the school that's in my neighborhood maybe i want to know the answer to that second i might be interested in features of the distribution of the betas across the schools how much does school quality vary are all the schools in this district kind of the same or some much better than others that's kind of a distributional question and third i might be interested in making a decision that depends on the betas maybe i have to decide which school should my child attend i have some option of which of these schools to go to maybe i'm a district policy maker i have to decide which schools to select for some kind of expansion or closure policy that's a question where i might have an eye on the value added of those schools and making that decision and as we'll see eb methods are going to be useful uh for helping us answer each of these types of questions although the first one there's some subtleties there and we'll uh get into that when we get there all right so how to operationalize this let's move towards what we get to see in the data of course that's a potential outcomes model i just laid out for each student i get to see the potential outcome associated with their actual treatment choice which is y i i've written that here as just a sum of school attendance dummies d times the value added parameters plus the student heterogeneity term in this literature we're usually worried about selection bias we don't think that students are unconditionally randomly assigned to schools there's a lot of sorting so first cut it trying to fix that we can control for some stuff and typically we a key control is last year's test score so try to control for lagged achievement hope that soaks up a lot of the selection let's call that vector of covariates x it's got last year's score maybe some other demographics in there project the student heterogeneity term epsilon onto x uh residual from that projection is u so that's uncorrelated with x by definition i've substituted that in to my equation here maybe i should use this little hand to show you what i'm talking about and then let's suppose everything works perfectly as we'd hope and that additive control for x is enough to kill all the selection bias in this environment the component of potential outcomes that's left over after i do that the u is uncorrelated with the school attendance dummies so everything's great and this causal equation remember the betas or causal parameters coincides with the population ols regression of y on d and x so i can get those coefficients from ols all right that's great i go into the sample of data i run my value-added model as we call this trying to estimate the value added of the schools at the end of the day what i get from that is a list of beta hats one for each school value-added estimates along with their standard errors and we'll call those sj all right so that's the starting point we have a beta hat j for each school and sj for each school a collection of those for the capital j schools and now we want to think about what to do with that all right and for a lot of what we do i'm going to make this assumption here which is again along the lines of what we saw in the first half for a lot of the discussion i'm going to assume that beta hat is approximately normally distributed centered at the true beta for that school with a sampling variance that's well described by its squared standard error so i'm thinking of this as a kind of loose asymptotic approximation center limit theorem has kicked in my estimates are normally distributed they're kind of centered in the right place i know something about their sampling variance they kind of behave like a normal i believe my standard errors we can talk later about what happens when my samples get small enough in each cell where maybe that's not a good approximation but for now let's work with that and see what happens all right all that's pretty standal standard all i've done so far is run some ols regressions now is where things get interesting okay so we move to the second level of the hierarchy and we think about a distribution of those betas across the schools in the population that we're studying i'm going to call that distribution following the same language jaiying was using g okay so i'm going to think of the betas as random they're draws from a distribution g each a draw from that same distribution that's my mixing distribution and that plays a key conceptual role in the eb framework okay why is it a mixing distribution well for each individual school what i see is a beta hat with normal noise and then the means of each of those draws are drawn from a second level distribution so i'm seeing like a mixture of normals and that's my mixing distribution it's important to emphasize that in this framework g is not like a subjective prior okay this looks sort of like a bayesian thing but g is an objective feature of the world in the way i'm thinking about this it in particular answers concrete questions about how value-added works in this district what about that first question i ask what's how does school quality vary how much variance is there g answers that that's the variance of g would be a sensible answer i could give to that question another way of thinking about it you know how much better are the best schools than the worst schools maybe 75th versus 25th percentile of value-added is the right answer to that question that's g inverse of 0.75 minus 0.25 okay difference in quantiles of that distribution and so the next step if i want to answer questions of that type is the eb deconvolution step okay where i'm going to use my noisy estimates along with what i know about their sampling variances the sj squareds to deconvolve the beta hats and extract an estimate g hat of the mixing distribution g that i can then use to answer these types of distributional questions about what's going on across all the units in this population all right that sounds good but i think it's worth stopping here a little bit to dwell on the philosophy of g so if you're kind of a very practical program evaluation sort of oriented economist you might say you might think i've tried to slip something past you with that assumption you might say what does it mean to say that the value added parameters are random draws from a distribution okay at the end of the day maybe i should take a fixed effects perspective there's capital j schools in this district they have value-addeds those are the betas each school has one but those are just unknown numbers they're out there in the world so what's this g thing that you're talking about why do i need some kind of distribution involved in this um this looks like some harmful econometrics or something some bayesian or random effects kind of stuff all right so one unsatisfying answer i could give to that is that maybe i should think of the observed schools as sampled from some larger super population of schools that could have existed but don't exist and g is the distribution of value-added in that super population there might be some applications where that makes sense i think for a lot that's going to be a lot of applications that's going to be unsatisfying defense of this type of assumption you know where are the missing hypothetical schools in boston that could have existed but don't exist and why would i want that to influence my view on what type of econometrics i should use so i'm going to kind of reject that way of thinking instead i think we can motivate this random effects uh perspective in a more instrumental type of way by the objectives that we have as econometric analysts in particular even with a finite population of schools in a pure kind of fixed effects perspective i can ask how the beta js are distributed across that finite population i can sort of do a thought experiment where i say what would happen if i kind of sampled from the finite population of betas what distribution is induced by that thought experiment there is an answer to that question and i can learn something about that from my data because i have an estimate of a bunch of the betas and because of all of the uh elegant theorems and so forth that jae ying laid out it turns out that if our loss function cares about average performance across schools we want to do something that involves all of the schools okay make a decision report an estimate it's valuable to incorporate that distributional information into our estimates for individuals okay and so the g is sort of an instrumental way to get us towards a tractable framework for doing that as you'll see in a second we often in practice use continuous or iid models for g i'm going to write down like g is a normal distribution or something in the pure fixed effects kind of finite population view of the world that strictly speaking can't totally make sense there's only capital j possible uh mass points in that distribution but i want you to think of that as sort of a parsimonious approximation to whatever's going on one lesson you could take away from the discussion today is i don't actually have to get everything about g right necessarily for the estimators and decision rules that come out motivated by this perspective to perform well even in purely frequentist type terms okay so you could think of this as an approximation and then we're going to go forward with that one last note on this philosophy slide one thing this discussion is not about is about whether the betas are correlated with the x's that i stuck in my value-added model so i think it's kind of an unfortunate quirk of econometric terminology that we often use random effects to refer to a kind of panel data estimation strategy that imposes an independence restriction between the betas and the distributions of the x's in that regression whereas fixed effects we're not doing that so you might kind of remember back to graduate graduate school something about that random effects means independence fixed effects means i don't assume that that's actually kind of a separate question you know whether i want to view the betas as fixed or random is a different question than whether i want to view them as independent of something else that varies in the population of schools so you could think of this as kind of a correlated random effects approach i haven't i haven't taken any stand on whether the x's are related to the betas in fact i probably think that they are that's why i wanted to control for x in the first place but i still want to view the betas as draws from a distribution all right so that's my philosophy of g hopefully you'll give that to me and we can move forward everybody's still on board and let's do a parametric version of this okay so let's suppose g is a normal distribution independent of the sampling error sj what does that mean okay we'll come back to that but let's just impose that for now then we have this very simple hierarchical type model where for every school j i have a beta hat j conditional on the beta for that school and it's standard error i just have a normal distribution centered at the truth with variance sj squared and then at the higher level of the hierarchy the betas are drawn from a normal distribution that doesn't depend on the standard error it's the same distribution for schools with different standard errors it just has two parameters mu beta and sigma beta squared we call those hyper parameters because they kind of live at the top level of the hierarchy they're parameters of a distribution of parameters so they're hyperparameters and under this normality assumption to do deconvolution all i have to do at the end of the day is estimate those two parameters because once i know those two things i know everything there is to know about g all right so how would i go about estimating those there's a variety of ways which again jang already touched on a very simple way you could think about estimating these i'm trying to estimate the mean of the betas well i'll just take the mean of the beta hats okay that's going to work that's fine to estimate the variance of the betas what i would typically do is compute this quantity right here or something along these lines this first part is the sample variance of the beta hats okay look in my data look at the beta hats how much did the beta hats vary and then i'm going to subtract off the average squared standard error from that okay why am i doing that well you could think of that as a bias correction okay that fixes the fact that because of the noise in the beta hats the beta hats are necessarily going to be over dispersed in my sample even if all the betas were the same so there's no dispersion in value added in the district i'm going to get some chance variation across the beta hash that i get to see how much will my standard errors tell me about that and i can take that out i subtract off the average square standard error if i get a positive number for sigma-hat beta squared in this expression that's what we call over dispersion meaning the beta hats are more dispersed in my sample than what i would expect from the sampling error alone okay so the sj's sort of tell me how much dispersion should you expect if i see a much wider spread in my value-added estimates they're over dispersed i've got over dispersion i'm going to infer a big random effects variance of g from that phenomenon of course there's other ways to do this estimation procedure i could have done maximum likelihood on the model i just wrote down um as we'll talk about later you know the variance estimator i wrote down here is not actually exactly unbiased but it's going to be consistent as j gets big client sergio salston have a kind of general framework for unbiased estimation of variance components we're going to apply that in the second half but for now let's stick with this simple kind of method moments looking type approach all right so that's my deconvolution and actually in this simple normal normal model once i've estimated those two hyper parameters i can answer all the distributional questions that we started with i know how much value added varies i know the difference in any two quantiles because there's only really two parameters of that distribution now what can i say about the individual schools okay well a natural thing to do as we've been discussing is to take that normal normal model and compute my posterior mean for each individual school's beta given all the information that i have in particular i know that schools beta is a draw from g and then i get another signal of the beta it's the beta hat i know its variance and i just do a normal updating problem on that and that leads to this simple linear shrinkage formula right here shrinkage because i'm pulling the beta hat towards the grand mean of g in proportion to the signal to noise ratio so if i have a huge standard error on beta hat i'm not really going to trust the beta hat very much i'm going to shrink that a lot if there's a lot of variance sigma beta squared is big in the population i'm not going to do as much shrinkage because i should expect there to be a lot of dispersion in the betas all right so this is the posterior mean in the normal normal case but like many things in life derived from normality it has some attractive robustness properties in particular i can also think of this as the fitted value from a linear regression of the latent beta on beta hat in a regression that lives in the population of schools and that sort of inherits all the usual minimum mean squared error linear approximation type properties that we're used to with linear regression so this is kind of an attractive uh thing to do with linear shrinkage i could think that might be a useful quantity to compute even if i'm not totally sure that g is a normal distribution all right as written i can't actually implement that formula that i just wrote down because it has unknown parameters in it i don't know the sigma beta i don't know the mu but of course i just showed you how to estimate those so the empirical bay's philosophy is to just stick a hat on those unknown parameters of g in my posterior formula and compute an eb posterior so that puts the e in eb it's now an empirical uh posterior eb beta hat j star so i put the hat on the beta to indicate that i've now used my g hat estimate to form the posterior mean so at the end of the day what i've done if i implement this is to shrink the estimate for school j using the hyper parameters that i estimated with the larger pool of schools in my sample i have many schools i use that to learn something about the distribution and then i use that distributional information to compute a posterior for each individual school it works out very simply in this linear shrinkage example but that's actually a much kind of broader idea in empirical base the empirical base philosophy is to use a deconvolution estimate g hat as prior when i form whatever posterior quantity is of interest to me about individual schools okay this has various names in the literature ephron and morris and morris 83 call this borrowing strength from the ensemble so i've used the ensemble of estimates across all the schools in the district to refine my estimate for each individual school jay ephron 2012 in his book calls this learning from the experience of others in some sense i have uh data on many different units that kind of have a shared experience there in the sense that they're in some way or other scientifically relevant in the same way that individual j is so i learned from the experience of those others how to interpret my estimate for any individual school and i use that to form my plug-in empirical based posterior all right so that's the three-step empirical bay's recipe that we follow in many examples step one you estimate some effects in some way or other we did ols but of course you could do all kinds of different things i get a beta hat at the end of the day and a standard error on that step two i do a deconvolution i use the beta hats and what i know about the noise in the beta hats to extract an estimate of the mixing distribution g i get my g hat and then i have step three posterior formation i treat that g hat as my prior and i update with beta hat and along with its standard error to form a posterior here in this case it was the posterior mean but i could think about other quantities and we'll talk about that in a second all right let me pause right here to ask if there's any questions about the cookbook that we've developed so far all right no questions we can keep going so let's um kind of switch back a little bit to philosophy here this is the cookbook empirical implementation but when would i actually want to do this so should i prefer the shrunk posterior mean to the unbiased estimate beta hat that i started with and the answer is that it's actually not obvious it depends on my goals as a researcher so one way to see that is to say let's say treat the value added of school j the beta j as a fixed parameter condition on that and compute mse for two estimators the unbiased estimate beta hat and the posterior mean beta star for the moment abstracting from the fact that i'm going to do eb rather than full b that's actually not going to affect the conclusions of what i'm about to say so the mean squared error of my unbiased estimate is of course just coming from the variance it's sj squared when i do shrinkage to form the posterior mean i've introduced some bias so i've got a variance term and a bias term in here the variance has gone down because i shrank it towards a constant so i'm removing some of the sampling variants but of course my bias has gone up and if for some schools it may have gone up a lot actually if i treat those as fixed parameters in particular if i'm only interested in one school beta one and i don't care at all about the other schools it's not at all obvious that by doing the shrinkage i made things better for school one in fact if that school is atypical so it it's very far from the mean it's different than the other schools in the ensemble that i have estimates for i've introduced a lot of bias and i could easily do worse on mean squared error grounds by doing shrinkage than by not doing shrinkage okay so shrinkage always reduces my variance but it might introduce substantial bias if the school is very different from average i think this point is even clearer if you think about a case where you have a collection of estimates that came from totally different contexts so you know maybe i'm a labor economist i'm sitting in my office i have an estimate of the returns to schooling i'm excited about that my macro and i o colleagues walk into my office and tell me estimates of their favorite parameters like the inflation rate or the elasticity of demand for frosted flakes or something like that whatever they happen to be studying should i take those estimates and shrink my estimate of the return to schooling towards the mean of those other two parameters that would be kind of a crazy thing to do why would that be crazy it's because i don't care about those other parameters for the purposes of my analysis and i'm not guaranteed to get a better estimate of the elasticity of labor supply by shrinking it towards this other stuff okay so if i'm only interested in one thing not obvious i should do this okay but in a lot of these contexts that we talked about on the first slide we're actually interested not in one thing but in many things in many things in the ensemble of estimates that we have suppose in the value-added case we're interested in all the schools we have some kind of compound decision that we're thinking about making and my loss function is averaging over the schools in this population it's not just about one school in that case the relevant notion of mse is going to integrate over the mixing distribution g rather than treating beta j as a fixed parameter i'm going to average i'm going to integrate that against g that doesn't doesn't change mse for the unbiased estimate beta hat i still get sj squared but if i work that out for the shrinkage uh formula beta j star my mse becomes this which is just the shrinkage factor times sj squared which is strictly smaller why does that work well it can't be the case that all the schools in the district are atypical for the district okay in some sense the average school must be average uh the typical school must be typical and so the biases across the different schools sort of average out and i'm guaranteed in an aggregate sense to do better across the ensemble and get lower mean squared error okay so the linear shrinkage estimator is superior if i want an estimator that on mean squared error grounds performs well on average across all the schools that i'm studying and i've done this calculation in the simple normal normal case but this actually holds as jiaying said much more generally it holds whether or not g is normal i don't even have to adopt a random effects sort of view of the world for shrinkage to dominate the maximum likelihood estimator on mean squared error grounds in the compound decision problem so if i wanted to do that but i'm not sure that g is normal i could also do a you know a kind of robust inference procedure that acknowledges that uncertainty along the lines of the armstrong at all paper as um as jung ying mentioned all right so let's implement this for the value added example now that we're comfortable with the tools here's my deconvolution step okay i've taken a list of beta hats and their standard errors for different value-added models over here i have an uncontrolled model which doesn't control for anything we think that's badly contaminated by uh selection bias the middle i have a lag score model where i've controlled for lagged y and then here i have a gains model where i've kind of differenced relative to lag y rather than controlled kind of fixed effects type intuition differencing out unobserved heterogeneity you can see there's massive overdue so this is my sigma beta hat estimate it's my estimate of the over dispersion in the beta hats how much signal variance is there in the underlying g distribution okay and you can see that there's massive over dispersion in the uncontrolled model that's of course mostly selection bias but there's still over dispersion in the lag score and gains models i get a sigma beta hat somewhere on the order of 0.2 uh student test score standard deviations so moving up one standard dev and the distribution of school quality i expect my students test scores to go up by something like 0.2 sigma on the right hand side i've done this within sector so taking out sector effects by which i mean different types of schools charter schools pilot schools and traditional boston public schools and you can see that brings down the within sector variance a little bit we'll talk about that again in a second all right so that's my deconvolution step i've now learned something about the distribution of value-added in this district the schools are very different some schools are better than others okay what about my posterior means here's the histogram of value-added estimates for boston in blue i have estimates for traditional bps schools those are like the schools within the district i've also put separately the histogram for boston's charter schools there's a large body of literature studying charter schools in boston suggesting uh their high value added relative to the district so it's useful to break them out in this way you can see the standard deviation of the unadjusted estimates the beta hats is 0.22 that's going to be too big my deconvolution estimate of the standard dev of the prior is 0.197 so that means there's actually a lot of signal in the beta hats it's mostly signal not that much noise if you take the ratio of those two numbers and square it that says about 80 percent of the variance in the beta hats is actually true school quality variation and not sampling error and if i'm working with this simple normal normal model my prior distribution looks like this it's just a normal distribution uh centered at the mean with that uh deconvolved uh sigma beta-hat variance all right and then uh what about my posterior means well i just shrink towards the mean in proportion to the signal-to-noise ratio those are uh so that's the the solid bars here you can see that by construction those are less dispersed than either the noisy beta hats or the prior distribution so i'm shrinking them down they're always going to have less variance than either of those two other quantities and i sort of pulled in the tails of this distribution towards zero there's some shrinkage going on but again because there's a lot of signal here not everybody's getting shrunk down to the mean there's still a lot of information about the quality of individual schools in these data one thing you may have noticed about that picture is that the charter histogram looked a lot different than the public traditional public school histogram the charters are all kind of over here on the right hand side the publics are further to the left you might think i should incorporate that type of information into my empirical base procedure and indeed we often want to do this so taking the kind of f brown phrasing uh you know we could call this learning from the experience of which others maybe i think there's some other characteristics that i get to see in my data that are in some sense relevant schools that share those characteristics are more similar have similar more similar experiences than schools that don't share those characteristics and i want to build that in to my procedure um in a parametric model a very simple way to do that is just to write down a conditional model for g uh simplest version is just to say i have a vector of characteristic c maybe it's got a dummy for charter status in it maybe it has some other stuff and say i'm building a conditional prior where the mean of that now depends on a linear index involving c here i've made it homoscodastic so the same residual variance around that doesn't depend on c i could obviously relax that too if i wanted to and then to estimate that i would follow the same type of procedure we did before i could estimate this coefficient mu by just regressing the beta hats on c i can deconvolve the residuals from that projection to get an estimate of the sigma r and i for my eb posterior mean it's just going to shrink me towards a linear index a linear fit from that procedure rather than a constant but otherwise everything works the same so here's what happens when i do that this is a prior distribution with a charter school location shift imposing common variants you can see it's shifted to the right the estimate of the charter effect is something like 0.3 sigma in this case so this does two things one it gives me a richer prior that has these two location points depending on the school's covariates second those covariates absorb some of the residual variation in school quality my residual standard dev is only about 0.14 instead of 0.19 that means i'm going to be able to do more shrinkage because i've built stuff into the model that actually explains a lot of the variants of school quality i'm going to get a more refined posterior out of that so here's what i get when i do that you can see if you kind of compare this to the previous picture i'm doing more shrinkage than i was before the other interesting thing you can see this actually goes back to jesse's question uh in in the break is i'm treating the charter schools and the non-charter schools differently actually in these two procedures uh in the unconditional shrinkage procedure i had this set of charter schools that had estimates kind of near zero a little bit below zero the unconditional prior thinks that those are basically typical schools uh shrinks them tricking them towards the mean essentially does nothing so they remain in the same location when i do that if i build charter status into my prior what i'm doing is shrinking towards the sector mean and actually those schools get moved up above the average because uh this prior thinks they're more likely to be close to the charter mean than the overall mean and i shrink them in that direction you could think depending on what your covariates mean that may or may not be a desirable thing to do maybe you have some horizontal equity concerns or something that uh influence your view on what should be built into the prior if all you care about is mean squared error of course this is what you should do all right a couple of extensions before we move to the second uh application uh first one uh one kind of extension that's often coming up increasingly empirical pract in empirical practice is a case where we have multiple estimates actually not just one of the same parameter and some of those are better than others in the sense of coming from more credible research designs okay so in the boston public schools context uh this is very natural we have some ols estimates like i just showed you which are just controlling for some stuff and hoping for the best and then we also have some lottery estimates that came from randomized admissions lotteries where we really believe the research design we think we have partial random assignment and we can get a good estimate of school quality out of that so to show you how this might work let me change notation instead of calling ols beta hat i'm going to demote it and call it alpha hat okay so it's just uh an estimate for each school j alpha hat and i'm no longer sure that that's an unbiased estimate that selection on observables holds instead i'm going to say that alpha hat is centered at something which is the true beta plus maybe a bias component bj that could vary across schools so maybe ols is overrating some schools over underrating other schools i have the standard error on the ols as before i'm going to again treat that as a normal distribution centered at that point but that might not be the right point anymore and let's suppose i have another estimate that's asymptotically unbiased call that beta hat maybe that's an iv estimate that i got out of a randomized lottery why asymptotically unbiased well again this is why this normality thing is an asymptotic approximation we know iv is never unbiased in finite samples but i'm hoping that my asymptotics have kind of kicked in and i can treat my iv estimate as centered at the truth uh it also has its own standard error i expect that to be actually much bigger than the ols standard error in general ols is precise but maybe it's biased iv is unbiased but maybe it's imprecise so one thing one kind of traditional thing i could do here is just a hausmann test compare ols to iv are they giving me the same answer probably not maybe my husband test rejects so should i just throw away the ols and report the iv that's something i could do uh in the empirical bay's view of the world there's actually quite natural to retain both and think about forming my best estimate of the quality of an individual school given all the available information including the biased ols and the unbiased lottery estimate okay so i would use the ensemble of both ols and iv estimates to estimate a joint distribution of truth and bias i can kind of get a handle on that by saying how similar are the ols and iv estimates what's kind of the distribution of the difference between those two things what does that look like is it correlated with the truth i could be more or less flexible with that and then i form my posterior which you might call a hybrid posterior beta hat star which uh in this norm uh in this normal model is going to end up just being a linear combination of everything i get to see it's putting some weight on the lottery estimate some weight on the ols estimate may be corrected for some mean bias and then weight on the prior mean and the weight i assigned to ols is going to depend on the relative precision of the two of course and also how biased it is so if i conclude that ols is not biased at all probably that's all i want to use if it's a little bit biased i put more weight on the lotteries and so forth so in the paper i'll refer you to the paper for other examples of this there we generalize this to the under identified case where actually all that's identified is certain linear combinations of value added not the beta hats themselves but you can kind of get a similar type of hybrid estimation procedure out of that this work by chetty and hendren is trying to do something similar with neighborhoods where you have estimates based on permanent residence and also some more quasi-experimental research design based on people that move between neighborhoods all right so here's just a picture from the 2017 paper on improvements that you get out of this hybrid strategy you can see a couple of things going on here one is that especially for the very badly biased ols you get huge improvements from using the lotteries that's coming both from the fact that you have a lottery estimate now to use and second the distribution of lottery estimates tells you how much to trust ols and it turns out that especially in this uncontrolled estimate you shouldn't really trust it you should put very little weight on it that brings down the bias a lot you can also see that the hybrid estimates that involve the quasi-experimental variations lottery variation tend to have higher variance than the by the biased ols that's kind of natural that's the bias variance trade-off that's being solved by this hybrid posterior all right last thing on schools so we've learned a lot now about empirical bay's posterior means this approach delivers estimates with low mse so if mse is your objective the posterior mean is your go-to okay but we often have other goals besides minimizing mse maybe we have other policy objectives other kind of things that we're trying to do uh jae ying also alluded to this a very simple toy example of that in the school value-added context is suppose i'm interested in a policy that's trying to select schools in the lower tail of performance so instead of just reporting a good estimate of value added for every school i'm trying to find the schools that are not doing so well formalize that by trying to find schools that are below some value-added cut-off c maybe that's a quantile of my estimated g distribution maybe it's some fixed number that i just started with let me write down a loss function that sort of formalizes that objective this is a very simple type 1 type 2 error loss so my loss function for a decision delta which is a dummy 0 or 1 along with the truth the beta is delta times indicator for beta is bigger than c so that's an error where i label the school school low performing but it's actually good uh plus one minus delta times beta less than c okay so that's an error where a school is actually not so good but i failed to label it as such and then there's a cost kappa associated with that a cost one associated with this error okay cost one of mistakenly selecting high performing schools cost kappa of failing to select low performing schools what should i do well if i care about this equally across all the j schools in the district i write down my risk function add it up over the schools integrate this over the distribution of the data and the prior you can probably immediately see that the right answer to that question is to instead of selecting on the posterior mean select schools that have sufficiently high posterior probability of being below the cutoff c okay and how sure i want to be before i label that school as low performing depends on my relative cost of type 1 or type 2 error how worried am i about accidentally labeling on failing to label a low performer versus accidentally selecting a high performer so this is in general the decision rule if you invert that cdf in that expression what this tells me is that i should be using posterior quantiles to make this type of decision instead of posterior means in other words i should select schools by the posterior one over one plus kappa quantile instead of the posterior amine in the normal normal case the decision rule looks like this right here and you can see that this is going to treat two schools that have the same posterior amine differently depending on which one has a noisier estimate this first piece right here that's the posterior mean that's the linear shrinkage part and then i have this part here that says multiply by a function of the standard error this inverse normal transform okay and that depends on my relative costs of each type of mistake and that thing is going to cross zero at one half okay so if i was equally worried about the two actually i should select on the posterior mean if i'm more worried about type 1 versus type 2 i should do something else and either reward or punish schools based on their standard errors okay conditional on their posterior means in this case and so of course as before my eb decision rule is going to plug in the estimated hyper parameters into that expression put a hat on the mu and on the sigma and i could do that in my data all right the the lesson here is different objectives call for using different functionals of the posterior for decision making so instead of just reporting the posterior mean all the time maybe we should think about exactly what we're trying to do maybe mse is not the right goal in our particular context in that case maybe we want to think about a different functional of the posterior uh goo and conquer in that their 2021 paper go into much more detail on these kind of eb tail selection problems this is just a very simple toy example of that all right um so that's it for uh schools i'm gonna take a kind of digression now before we move into the second application and try to connect some of the methods we've learned so far to other commonly used methods in the recent applied econometrics literature so we sort of motivated analysis here by this uh this idea that we have many estimates for many units a lot of beta hats okay many betas they're noisy the beta hats are noisy i want to do something about the noise to fix that and get a better estimator and you might say that actually sounds a lot like this thing i've heard about machine learning okay i thought machine learning was supposed to deal with problems where i have a ton of different parameters on trying to estimate and kind of fix the noise problem in that sense there's a reason that those two things sound similar they are similar and in fact in some cases depending on how you look at the problem they're the same thing so i think it's worth demonstrating that in a particular very simple example all right so let me make my model even simpler instead of this value-added thing instead of worrying about controls and different sample sizes and so forth let's just suppose we have a really simple normal means model n students per school all i'm trying to do is estimate the mean for each school i have homoscedastic normal noise mean 0 at every school and then my prior mean is also going to be zero so the betas are just drawn from normal 0 sigma squared in that case of course my unbiased estimator is just the mean y bar my variance of that is sigma squared over n and my posterior distribution since everything is normal is also going to be normal it has not the now familiar shrinkage formula for the posterior mean and then this variance right here common across the schools because everybody has the same sample size same standard error but let's suppose i didn't actually know what beta j star was yet and let's think about what the posterior distribution of the betas looks like in that problem okay in general my posterior density for beta j given the data for that school and the prior is just the likelihood of the by bayes rules just the likelihood of the data times the marginal for the prior over the integrated version of all that stuff okay i already cheated a little bit i told you i know the posterior distribution is going to end up being normal so in a normal distribution the mean and the mode coincide and therefore i can also think of my posterior means as the maximizers of the posterior density okay so i'm going to write the posterior means the beta j stars the things we've been talking about instead of as e of beta given y bar and so forth i'm going to write that as the arg max of the density in this in this kind of log likelihood formulation here which if i plug in the normals it's just going to look like this i've got a log of the likelihood of the data given the parameter plus log likelihood of coming from the prior plus constant that doesn't depend on anything i care about in general we call this kind of estimate the maximizer of the posterior density the map estimate maximum a posteriori in this case it's the same as the posterior mean because it's normal let's write down what that looks like okay let's use that formula for the posterior mean beta star because i'm using normals log of a normal density i'm just going to get a bunch of squares in there okay i've got squares squared deviations coming from the likelihood of the data i've got squares of the prior coming from the prior let me multiply through by the scale factor on this first thing minus 1 over 2 sigma epsilon to get rid of that that turns the max to a min because i multiplied by a minus and so what i've got here is a sum of squared residuals for the data okay plus a constant which is this variance ratio times the sum of squared betas and my beta stars are uh are the max the minimizers of that expression right there so that kind of looks like something this is a regression okay where i'm taking a sum of squared residuals plus a constant times what you might call a penalty term which is a quadratic in the betas it's just the sum squared betas so that's called something what is that somebody tell me quiz ridge regression that's ridge regression this is just regularized least squares with an l2 penalty uh p uh which is kind of one of the simpler you know basic uh so-called machine learning algorithms that you may have heard of so uh ridge regression is eb with normal uh priors y eb well um turning this from base to empirical bays i would be choosing the tuning parameter in this penalty function you can see that the tuning parameter that i kind of want to use in this bayesian framework is this variance ratio that essentially depends on the signal-to-noise ratio in my estimates right this thing is n times my standard error this thing is the variance of the prior those are the same hyper-parameter type things i was trying to estimate before all right so i can think of empirical base as kind of empirically choosing my tuning parameter doing this regularized least squares thing ridge regression instead of normal normal eb so the punch line of that is that these sorts of regularization ml procedures often have an eb type interpretation i could think of ridge as posterior means from a normal model if i wanted l1 penalization instead of l2 so i want an absolute value in my penalty term instead of a square so i want to do lasso instead of ridge i could have gotten that by just making uh the prior here have an absolute value in place of the square that would be a laplace distribution instead of a normal remember the laplace density just replaces the square with an absolute value so i can think of lasso as uh computing maps maximum a posteriori from a model with double exponential laplace priors on the betas that's all fine i guess the takeaway is when doing model selection or penalization via ml it's sometimes useful to think in this empirical base type of way and think about what implicit prior distribution you may have imposed on on the parameters and also the connection to your loss function so here you sort of baked in what functional of the posterior you're going to report that may or may not be the right idea depending on what type of decision you're trying to make as we just learned with the mean squared error versus selection rule type loss so i think this is a useful kind of connection there's a lot more on this in this paper by abiding casey if you're interested in reading about that all right any questions about any of that before i move to the final application okay um so application two now that we're all familiar with empirical bays we sort of know the philosophy we know how the tools work we're now gonna use some of the more recent like flexible eb type tools that jae ying taught us in the first half in a second application this is going to come from our paper klein rose walters forthcoming that applies these types of methods to try to study the distribution of labor market discrimination across large employers in the united states in particular the structure of the experiment underlying this paper is a large resume correspondent study okay so we sent fake resumes to a bunch of real job openings and we did that in a kind of hierarchical structured way so we selected 108 giant companies from the fortune 500 each of those companies we tried to sample up to 125 jobs each job in a different u.s county so spread across the whole country for each of the 125 sample sample jobs for each employer we send eight applications stratified for black applicants for white applicants how do we convey race we're following the sort of literature here you might be familiar with this bertrand and mulanathan paper which manipulates employer perceptions of race using race and sex distinctive names on the resumes we did the same thing so we signed four distinctively black names four distinctively white names at each job applied to those jobs 125 jobs for firm 108 firms in the experiment all right so that has this kind of natural hierarchical structure uh which makes it a a good test bed for these eb style methods all right let me develop some notation like we did before for thinking about what we're going to get to see out of this experiment let's start with potential outcomes there's a lot of hierarchy here so a bunch of subscripts let's call y i j j f of r which is a dummy variable a potential callback to applicant i at job j nested within firm f if that applicant is assigned race r which is either a b or a w uh and then i can use that to uh define the average treatment effect at this particular job and firm which is just the average difference in uh the white versus black potential outcomes at that job uh you might notice i've imposed a kind of suba assumption here already by writing these as functions of only your own race assignment you might be worried about that since there's you know maybe screening going on where all these applications are getting read at the same time for what we do next that actually doesn't matter because we're going to aggregate everything up and treat the jobs as observations but the notation is a lot more complicated to show you that so you can just think of this simple potential outcomes model to motivate what we're going to do of course what i get to see again is the observed the potential outcome associated with the realized treatment assignment which is either a black a distinctively black name or a distinctively white name and at the job level what i can compute is just the treatment control difference the mean difference in callback rates for black for white versus black applicants i'm going to call that treatment control difference delta hat jf at that job for that firm that's a very noisy estimate of course i only have four treated and four control observations in that particular cell of the data but i randomly assigned race so it's an unbiased estimate of the average treatment effect it's just super noisy all right so that's what's going on at the job level now let's aggregate up to the firm which is going to be the the level of interest in this experiment let's call delta f with no j subscript the average of those job specific treatment effects the delta j f's across all jobs in that firm so i'm abusing notation a little bit by putting a sub f on this expectation but hopefully you know what i mean think only about firm f average across all the jobs i could have sampled for that firm think about averaging the job level treatment effects and delta f is that firm level average okay so what i get to compute for that firm is the average of the jobs that i sampled the average treatment control difference across those jobs i'm going to call that delta hat f i randomly sample these jobs for each firm so random sampling of the jobs plus random assignment of the treatment means that this thing in the at the end of the day is going to be an unbiased estimate of the firm level average treatment effect the delta f and i'm going to use this simple standard error estimator sf squared which is just the familiar standard error it's an unbiased estimate of the sampling variance of this mean okay and those that's my step one of my eb cookbook okay that's my effect estimation at the end of the day i've collapsed all the data for this giant experiment down to two numbers for each of the 108 companies a treatment control difference and a standard error on that and those are my building blocks i'm now going to move on to step two okay remember step two of eb is the deconvolution step i'm thinking about my g distribution i'm going to call g the distribution of contact gaps that's my label for the delta f's across firms in the population so there's 108 of these firms there's some distribution of the treatment effect of a distinctively white name across those firms and g is telling me about that distribution it answers questions that i might be interested in about the concentration of discrimination for example if i see a difference in callback rates for white versus black applicants is that difference driven by a small number of firms that intensively describe discriminate in a population or otherwise unbiased companies or is everybody discriminate discriminating a little bit okay so that g distribution answers questions like that anything i want to know about the distribution that's a question about g i would like to estimate that i'm going to start by estimating its mean and variance uh non-parametrically the way i did before but then instead of just assuming everything is normal and i only need those two parameters i'm going to apply some of these more flexible deconvolution techniques to try to get a little bit more information about what's going on in the g distribution here are the means okay the mean is easy to estimate i just take the sample mean of the estimates in the end in this experiment we got a 23 callback rate for applicants with black names uh 25 callback grade for applicants with white names so that the average treatment control difference average white black callback difference across all the firms in the experiment is around two percentage points uh that is a standard error of 002 so t statistic over 10 highly statistically significant i'm also going to show you gender that's over here on the right this is male versus female that turns out on average to be an extremely precisely estimated zero minus 001 is the point estimate with a standard error of 003 okay so those are the means what about the variance of g i'm going to do something that looks very similar to the simple estimator i used before with a couple of embellishments okay so here's my sigma hat delta squared my estimator for the variance of the contact gaps across the firms in the distribution g this looks a lot like what i had before okay here's my sample variance of the delta hat i've done the usual degrees of freedom here i'm dividing by f minus 1 on that i'm subtracting off the average squared standard error from that and then i'm sticking another degrees of freedom correction f minus 1 over f out here in front of that why do i want to write it that way well it turns out that that particular estimator is a special case of the unbiased leave out variance component estimator of client-side geosolstin which gives me an exactly finite sample unbiased estimate of sigma squared even in a in a finite population where i'm not i'm not promising to get infinite firms even in the finite population of firms here i can think of this as a unbiased estimate of the quadratic form that i need to form the variance okay why does that work well one intuition for that is if i plug in delta bar i plug in that formula for the standard errors that i showed you before i can rewrite the variance estimator like this turns out to be just a function of a bunch of cross products of job specific estimates across jobs at the same firm so essentially what i've done is take all the possible cross products that i could construct for any two random jobs sampled at this particular firm i'm treating those as independent okay so those are basically firm effect plus uncorrelated noise so each of those is essentially an unbiased estimate of delta f squared and then i've got many many different ways to do that in fact i've got jf choose two of those at every firm so i'm averaging over all of those and i'm averaging over all the firms and then here's my other component of a covariance so i could think of this unbiased variance component estimator as essentially an average covariance that gives me the right answer for the variance in this population works in finite samples all right so what do i get when i do that here i'm showing you the bias corrected standard deviation of the contact gap so i took the unbiased variance estimate and i square rooted that what i get for race is a number around .019 around two percentage points so the mean and the variance are comparable you could think of that as a lot of heterogeneity there's as much spread you know one standard deviation move up in this distribution that's comparable to the mean white black difference in the experiment for gender interestingly the standard deviation is even bigger than that it's 2.7 percentage points maybe that's surprising because the mean is exactly zero so how could that be the mean is zero but there's a huge variance that must mean there's discrimination going on in both directions actually there's some firms that are discriminating against men some firms discriminating against women i think this is one way of illustrating the power of these eb type methods if i just ran my experiment took treatment minus control called it a day i would have said there's no gender discrimination that turns out to be way wrong all these firms are described many of these firms are discriminating on the basis of gender they're just doing it in different ways in different directions and that will come out even more clearly in the deconvolution that i'm about to do okay so that's mean and variance now let me move on and uh uh talk about uh uh learning more about g beyond the mean and variance so i'm going to think about this hierarchical structure okay i have a delta hat for every firm it's sent i'm going to now call that normally distributed i didn't need that for the thing i did in the last slide but now i'm going to say i have enough observations at each firm to get a normally distributed delta f it's standard error describes its sampling variance and then i want to know about this g distribution which is the marginal distribution of the deltas from the population that i drew from um an important note at this point which uh we also learned from jae ying is that in in doing this if i want to be very flexible in how i do it i need to account for possible dependence between the effect sizes the delta f's and the standard errors the sampling variance the sf squared so somehow from line one to line two in this picture i got rid of the sf i'm no longer conditioning on that i'm thinking of the marginal for delta how did i do that well i have to do something to either integrate out sf or assume it's independent or or something along those lines that's actually kind of substantively interesting so how would such dependence arise there's a couple of different ways you know what's the delta hat the delta hat is a treatment control difference so you know my usual ols standard error is sigma squared variance of the outcome over n times variance of the treatment variance of the treatment is the same for everybody because it's an experiment assigned 50 50. so any standard error heterogeneity that's going on here is coming from sigma squared variance of the outcome that's going to be driven by the overall callback rate at the firm in particular firms that never call you back actually have a standard error close to zero okay because that's p minus p times one minus p for a binary outcome firms that call you back around fifty percent of the time have a maximized variance and you might think actually that the level of discrimination is related to how often they call you back in general if a firm never calls you back they can't really discriminate in levels so there's a natural reason to think i might have this kind of heteroscedasticity and dependence built in another way that could happen is through the n that's in the denominator of my uh standard error we tried to get rid of that by sampling the same number of jobs and and so forth in this experiment so we're not as worried about that in this case in other applications you might be worried about that if there's n heterogeneity like in the teacher value added example that we talked about earlier all right so how do we deal with that well let's sidestep it in step one of the deconvolution and we'll come back to that let me sidestep the variance dependence issue by transforming everything into a z-score okay let's call z-f the z-score for firm f it's just delta hat divided by s okay just scale everything by its standard error and let's call mu the population counterpart of that okay so it's the real delta the latent delta measured without error divided by s for each firm i can write my hierarchical model in that world as z is distributed with mean mu and variance 1. since i've z-scored it i got rid of the variance heterogeneity and then the muse themselves the population z-scores have some distribution in the population of firms okay and that deconvolution problem is a bit easier i don't have to worry about the variance heterogeneity i can apply some of these tools to that immediately i'm going to start with the efron 2016 exponential approximation that jayang taught us which basically approximates this g mu thing with a distribution in a smooth but flexible exponential family in particular we parameterize the density with a flexible spline we estimate the parameters of that by penalized maximum likelihood and uh you can actually do this yourself if you ever face this type of problem automated in this nice deconvolve are our package described in this paper right here this does require you to choose a tuning parameter to decide how much to penalize when you're doing penalized mle i think a nice way to do that which is what i'm going to do oops in the next slide when i show you this is to calibrate that penalty term to match something else that you're pretty confident about about that distribution for example we're going to calibrate the penalty so that the g hat we get out of this has the same estimated variance as that unbiased variance estimate that i just showed you that i got from the kss client-side geosolstin type procedure that's one thing i could do the other thing i could do is the npmle which we also learned a lot about okay that picks the mixing distribution to maximize the likelihood of the observed data as we learned the solution to that is a discrete distribution that has as at most f mass points that's a known fact about this problem if you're an applied economist like me you may kind of remember this from graduate school from heckman and singer and be vaguely disturbed because this is notoriously difficult to compute but fortunately uh that's improved a lot in recent years as as xiaoying told us konikar and mizera developed this approach to computing this problem which is a lot easier than the em algorithm type things that we used to do so you can do this easily you could do it yourself off the shelf with the rebaze r package now as described uh in this conquering goo article comparing these two types of estimators is a very interesting thing when does one perform well when does the other perform well um i won't go into that but uh we learned something about that in the first half as well and this conquer paper does is a nice little like meditation on on which circumstances these different flexible deconvolution approaches perform well in all right so suppose i did one of those two things i have my g hat mu okay i have an estimate of the population uh distribution of z-scores and now what i care about is the effect sizes themselves the deltas not the muse i need to do something to go back to levels from z-scores in other words i need a change of variables so my delta that i care about is actually mu times s right it's the z score times the standard error and i have to do something to deal with that simplest thing i could do which might be a good approximation we'll come back to talk about this in two slides is just assume independence of mu and s suppose the population z-score is independent of the sampling variance okay that could be true if we have dependents in levels actually maybe we don't have dependents in z-scores that could happen we can test that let's just suppose that's true if that's true then i can just compute the distribution of the deltas through a simple change of varia variables formula that looks like this and then i put a hat on everything i have an estimate g hat mu from either ephron or npmle i stick that in here i integrate over the distribution of the standard errors we're going to do that by just averaging over the edf empirical distribution function of the standard errors and get an estimate of the the mixing distribution okay so here's the histogram of the uh bl the white black contact gap estimates across the firms in this experiment that we're summarizing that's the histogram you can already see it has a kind of interesting shape right it's it's sort of uh it's kind of high near zero but there's not very much mass at all to the left of zero and then sticks out to the right and that's going to emerge in the uh the mixing distribution estimate also so here's the basic ephron estimate that actually doesn't do a great job of mass matching the mean which remember was 2.1 when i um forced it to match the variance but something interesting a constraint you might want to impose on this is to notice that there's very little mass actually below zero in that distribution in the paper we do formal tests of that you can actually use some techniques from the moment inequalities literature to ask are all the deltas weakly positive so can i reject that every firm in this experiment weakly favors white applicants it turns out you can't reject that the p-value on that is one okay so the data actually really like that and uh if you want to impose that you get a little bit more kind of precision on the mixing distribution and it looks like this turns out to look kind of like a log normal okay so we've got some mass here near zero but a longer tale of discriminators that are discriminating a lot against black applicants now i'm doing a very good job of matching at both the mean and the variance that we showed you before so this is imposing that shape restriction that all the delt the support of that doesn't extend to the left of zero here's the npm in pmle if i do that again this is going to be a discrete distribution um so it's whoops it's spiky uh but it i think the substantive conclusion is kind of the same we're getting uh it wants to put weight that's pretty far out here in the tail so it's saying there's some firms out there in the population that are discriminating five percentage points more than twice the mean uh affecting the experiment or even more than that all right so there's kind of a tale of discriminators that are that are favoring whites by a large margin what about gender remember for uh the first two moments which we estimated before uh things look quite different the mean is zero and then we have a huge spread you can see that in the histogram things look different it's symmetric about zero but then we have some firms sitting out here way in the tails if i do my efron smooth deconvolution it looks like this it looks more like a laplace distribution that's very peaked at zero but then it's uh it's got kind of fat tails in both directions uh npmle agrees it wants to put mass away out here in the tails both against uh men and against women so this mixing distribution estimate is saying there's some firms out there that are going in both directions quite a bit actually and that's something that i couldn't have learned just from the means that highlights um i think the value of some of these um some of these methods okay something interesting you can do with that you know if you do these flexible non-parametric type deconvolution methods rather than assuming normality is compute other functionals of the distribution that are kind of interesting uh what we've done here is to compute the lorenz curves for discrimination implied by those efron 2016 g hat estimates so we might be familiar with lorenz curves from the public finance literature looking at income concentration of income distributions you can do that for any random variable here we're doing it for the delta f's the discrimination for gender i'm actually doing this for the absolute value of delta f because it goes in both directions so i'm kind of saying how what what is the concentration of the total gaps against either men or women look like and you can see that this is a pretty concentrated distribution uh one metric for that is how much of the total discrimination in the experiment is driven by the top 20 percent firms in the top quintile discriminators that's about 50 percent a little bit below 50 for race over 50 uh for gender all right um so let's return uh briefly to this precision dependence issue uh you know what i assume to get those distributions out of my z-score distribution estimates was to assume independence of the population muse and the standard errors f if that's true that actually imposes the assumption that the effect sizes themselves the deltas are proportional to the standard errors if i kind of do this simple calculation here it says delta is equal to mu bar times s if i do e of delta given s you might think that's a reasonable approximation actually because as we said if nobody calls you back they have a very small standard error and probably a small contact gap if they call you back more they have a bigger standard error maybe they have a bigger contact gap i can kind of test that assumption actually we look at that in the paper it seems to fit reasonably well there's other types of more flexible things you could do so you could treat sf as a covariate that shifts the location or the scale of g in the same way that we did with the um kind of charter school indicator in the in the earlier parametric analysis you could do a vst a variance stabilizing transform try to find a function t basically such that the variance of the transformed contact gap delta half conditional on delta but not the standard error is variance stabilized there's known vsts that work for certain data generating processes you could use the data to try to find something that works well here if you wanted to another thing you could try to do is estimate the bivariate distribution actually of both so you could say i have a marginal i have a marginal for delta i have a marginal for f i have some joint between those and i could actually try to estimate that from the data that's asking a lot of the data but you could do that for example with npmle and impose less restrictions on the way this heteroscedasticity interacts with the effect sizes all right the very first kind of poor man's approach to that is to just split your sample by sampling variability so i here i've just taken the above and below median firms in terms of their precision of their delta hats estimated my deconvolution separately for both you can see this kind of works the way that i'd hope given that assumption on the z-scores the firms with low standard errors they all have pretty small contact gaps because they're basically not calling anybody back okay so they have a very compressed distribution the higher standard error firms also have a wider distribution in that sense the approximation that we used seems to fit pretty well um you could also form the marginal uh from this kind of split for the the full distribution of the deltas it looks like this it's a little bit lumpier because it's combining these two distributions but for key functionals like the mean or the standard deviation or the top 20 share which we show in the paper it actually turns out to perform basically the same all right uh last step so i've done my first two steps of my eb cookbook i did my effect estimation i've done my deconvolution what about step three firm level posteriors okay well i have my estimate of g hat in hand let me move on to step three i can form my eb posterior amine okay one way to do that here is to take the z-score posterior amine from the g-hat mu and just multiply that by s to get my posterior mean for delta which i'm calling delta f star i can compare as we did before the distributions of the unadjusted contact gaps the mixing distribution and then the firm level posteriors i already showed you this picture here's the empirical base posterior amines layered on top of that again as always by construction those are less dispersed than either the unbiased noisy estimates or the mixing distribution itself but there's still some dispersion there that tells me that this experiment contains a fair bit of information about the individual firms all right kind of last topic i wanted to get to i guess i have around 15 minutes left um and i want to leave some time for questions so i'm going to go through this very quickly um sort of parallels what we did with schools you know all this is about minimizing mse so far if i want a good estimate that minimizes mse for each of these across the ensemble of these firms i should report the posterior mean okay but i might have other objectives and in the firm context there's interesting classification decisions that i might be specifically interested in in particular i might care about which firms are discriminating at all okay which firms have delta exactly equal to zero which firms are violating the civil rights act which firms are not in the discrimination context i might care about which firms are in that upper tail that seem to be driving a lot of the action in the experiment which firms are above the 80th percentile of the g distribution okay we saw kind of a toy example of how to do that there's various ways to approach these tail selection problems the approach i want to kind of offer here in closing is to link this eb state of mind with the multiple testing literature uh a bit like jia ying did at the end of her her part and uh by noting that these sorts of decisions are closely related to multiple testing problems so efron in his book calls this large-scale inference trying to do inference in this world of many estimates at the same time and let's think about some robust empirical bayes methods uh to approach this sort of multiple testing problem to decide which firms are discriminating at all okay so what i'm going to do is conduct a hypothesis test for each firm testing the null that delta f is equal to zero maybe i want to do a one-tailed test if i believe this result that everybody is going in one direction i do a one-tailed test of delta f equals zero against the alternative that it's bigger than zero my test statistic is of course just going to be the z-score p-value one minus phi of the z-score and then i'm gonna adopt a decision rule that says let's reject all hypotheses below some p-value threshold p-bar how many mistakes do i expect to make with that decision rule well i think the empirical base type of approach provides a natural way to think about that question how many mistakes do i expect to make one way of formalizing that question is to say what fraction of the firms that i label as discriminating with this decision rule p below p bar are actually not discriminating okay so what share of firms have delta f equals zero among those my decision will select as discriminators by bayes rule i can write that as the probability of getting a p p value below p bar given that you're not discriminating times the marginal for delta equals 0 over the marginal for p below p bar this first piece since under the null the p values are uniform that's p bar okay the denominator is the marginal cdf of the p values evaluated at p bar and then this first thing the kind of prior probability of the null being true in this case that's pi naught we'll call it and in the multiple testing literature this thing has a name this is the false discovery rate in a kind of empirical-based way of looking at the problem or fdr for our decision rule it tells me on average among the nulls that i select to reject how many should i what share should i expect are actually true all right if i can limit my fdr to q bar say five percent i should expect at most 100 q bar percent of those i classify as discriminating to actually not be discriminating one out of 20 say if it's a five percent threshold uh so let's see how we can control fdr okay this remember this first part is easy that's p bar because the p values are uniform under the null the denominator is something i could try to estimate that's the marginal cdf of p values i observe a bunch of p values from this population so i could estimate that somehow maybe the empirical share below p bar the hard part is the pie knot okay the hard part is knowing in this ensemble that i'm looking at how many of the nulls are actually true what share of the nulls in this population are true uh in this empirical bayes way of looking at the problem that's a feature of my g okay that's a feature of the g distribution it's the integral over g of indicator for delta exactly zero so in g how many deltas are exactly zero and in general we treat that as partially identified it's very hard to tell the difference between a world where a bunch of firms are exactly zero versus just vanishingly small contact gaps that are not exactly zero and depending on which of these deconvolution approaches i took i may have already taken a stand on this actually if i did the f bronze smooth approximation i've already imposed that pi naught hat is equal to exactly zero it's zero um firms discriminate uh at least a little bit with probability one so this is actually not even an interesting question in that world so maybe i want to do something else okay the most conservative thing i could do is to plug in the logical upper limit for pi naught which is one which is kind of the traditional approach to this that actually could still imply a low fdr if i have many p-values that are close to zero okay if a lot of p-values are close to zero the marginal cdf for the p-values is going to be concentrated a lot closer to zero but if you think about it for a second you can do better than that okay because it's logically inconsistent to have pi not equal one but a bunch of the p values very close to zero another way of saying that is we already know the mean and the variance of g are not zero in this case so pi naught can't be one somebody has to be discriminating and i can use that information to borrow from the ensemble and bound pinot in a non-trivial way the way this works maybe i won't go i guess i'll go very quickly through the details the way this works is basically at any point in the distribution of p-values if i'm thinking about the density the true nulls contribute height pi naught because they're uniform and then the alternative false nulls contribute something else f1 i don't know what that looks like exactly the alternative distribution but i know it's a density so it must be positive and therefore the height of the density at any point gives me an upper bound on pi naught and therefore the best upper bound is the minimum density of the p-values okay i expect that minimum to be achieved somewhere near one because my test is worth anything i expect the the false nulls to be concentrated towards zero and so what i'd like to do is know what is the density of the p values at one uh story story has this proposed tail density estimator in this 2002 paper basically this is saying look at the average height of the density above some threshold lambda there's sort of a trade-off there you know the higher lambda uh the better bound i'm going to get but the less data i'm using to estimate this so the noisier is my bound you can think about different ways to do that um story at all 2004 have a kind of bootstrap procedure they proposed to do this you could also uh use this nice paper armstrong 2015 to provide a non-parametric confidence interval for pi naught if you want to be conservative maybe take the upper limit of that or something like that however you do it you get your bound on pi not you plug that in that gives you a what's called a q value uh for discrimination uh for the for this uh um this null that we're testing the q value is essentially uh the fitted uh fdr hat for this firm plugging in whatever bound i chose for pi naught you could think of that as the eb equivalent of my p value rather than controlling the probability of type one error the probability of rejecting given the null is true i'm using bayes rule plus the ensemble of tests together to control the probability of the null being true given that i reject it so to invert that expression and control the expected share of true nulls among those selected for rejection by my by my decision rule and if i get a q value of 0.05 uh for a particular firm i should expect at most uh 5 uh of those i select for rejection to actually be true nulls on average okay in expectation so here's what happens if we do that in this experiment this is the distribution of p values you can see there's a bunch of discrimination going on a bunch of them are near zero the story procedure produces a lambda that's like 0.5 that gives me a estimated estimated upper bound and pi naught at 0.39 in other words at least 61 of the firms are discriminating against black applicants i plug that in i get a bunch of small q values for a bunch of these firms so 23 of the 108 firms in the experiment end up with q values below 0.05 that means i should expect roughly at most one firm on that list to not be discriminating uh if you kind of buy the the procedures that we've done to get here and i had a some other decision theory stuff at the end but i think in the interest of leaving time for questions i'm going to conclude right there and and thanks thanks everyone for listening [Applause] [Music] 