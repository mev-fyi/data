Daniel Kifer: The aim
of this talk is to explain some of the
intuition behind what differential privacy is and how it can protect
your confidentiality. Most people don't
realize the impact that differential privacy has on
their day-to-day experience so we're going to start
with some examples of real deployments of
differential privacy. First, there's the
US Census Bureau whose goal is to
collect information about businesses and
people in the US. This data can only be used
for statistical purposes, which means that the information being published cannot be used to re-identify
specific individuals. Over the years, this has become a challenging task
for many reasons. First, it's becomes
so easy to collect observational data about
millions of people. So there are many
datasets available for free or from commercial sources. A concern is that
someone may be able to link these datasets
to Census data to obtain additional
information about target individuals that
they didn't have before. So linking attacks can
be successful even if the Census only publishes
data in aggregated form. For example, in the past, the decennial census published billions of statistics about millions of people. So as Danny said, that's a simple linear algebra to
reconstruct some information. Reconstructing records from all this information
is fairly easy now. We have fancy data science
reconstruction algorithms, we have software for solving optimization problems at scale, and we also have cheap
Cloud computing, which makes all this possible. So in the face of such attacks, confidentiality needs to be protected for multiple reasons. First, it's ethical, and second, you need people to
trust the Census Bureau so that they'll provide
truthful responses. Another reason is Title
13 outlines penalties, such as up to five years in prison and a fine of up to $250,000 for disclosing
private information. In some cases, such as when the census data is
mixed with IRS data, the penalty increases to
up to 22 years in prison. Just for reference, a
first-degree felony in Pennsylvania carries a
shorter maximum sentence. Differential privacy has been in the Census toolkit
for a while now. In 2008, the OnTheMap
tool contained the very first large-scale
public deployment of differential privacy. As you probably heard, it's going to be used
in the 2020 census. The use of differential
privacy at the Census Bureau follows
something that is called the central model
of privacy. In this model, the data collector is
trusted by the respondents. The alternative is
the local model in which the data
collector is not trusted. Here's an example. Google is very interested in
information about you. This includes your
online behavior and your usage of
their technology. One of their earlier
goals was to understand what are common user settings
for the Chrome browser. These settings
included homepages and plug-ins that
you had installed. Just sending all of this
information to Google would be just a massive
breach of privacy. You need user
consent to do this, and very few users
actually choose to opt in to data
collection campaigns. So in 2014, Google added a differentially
private component called RAPPOR to
the Chrome browser. RAPPOR takes your
browser settings, converts them to bits, randomizes those bits, and then sends those noisy
bits back to Google. This noise protects
your information so Google doesn't know what
your true settings are. Anyone who hacks
their servers won't know what your true
settings are, and so this information
is technologically protected also from
warrants and subpoenas. When Google aggregates
these noisy bits from millions of users, they can actually figure out population statistics, such
as the most popular plugins. Again, this is called
the local model. The user doesn't trust
the data collector. The user's information is
safe from the data collector. But the data collector can still learn about properties
of the population. So one could argue that
differential privacy became main stream when Apple
started using it. Apple wanted to understand your mobile device
usage in quite detail. They wanted to
know, for example, what websites people visit through apps so that they
can recommend links, they want to know what
words people type so that they can make better
predictive keyboards, and of course, most importantly, they want to know about emoji usage so that they
can recommend emojis. Now the problem is
that software that collect your keystroke and sends them to a third party is a type of malware
called a keylogger, unless you use
differential privacy. Keystrokes and usage
information, again, can be converted to bits. The bits can be randomized so that Apple does not
know what you did. However, once Apple collects
all of these bits from millions of users and aggregates them
across the user base, they can start to see trends. Again, this is the local model, and it didn't stop there. Microsoft, Facebook,
Samsung, Uber, and many other companies have now invested in
differential privacy. The takeaway message is that differential privacy has an important selling point. It enables the
study of data that you would not be able
to access otherwise, even if you're a tech giant. As we saw from these examples, there are two trust models. Both of them will be
explored in this talk. The central model has a
trusted data collector who gets to see
the real data and all the perturbations
that happen after the data has
been collected. In contrast, in the local model, the users do not trust
the data collector, so the data are perturbed before being sent to the
data collector. This gives you more privacy. But as we'll see, this comes at a cost of lower accuracy. In both of these models,
differential privacy offers another benefit. Everyone is allowed to know exactly how the data
were perturbed. So this is tremendously
useful for data analysis, and it does not increase privacy
risk at all. As a result, you should
release the source code. There are many examples of source code implementations of differential
privacy algorithms so includes the
disclosure avoidance code from the census end-to-end test, includes the code that Google uses to analyze
browser settings, I think Danny mentioned OpenDP, which is an open
differential privacy library that implements many
of the algorithms in the literature, and there
are many, many more. Now, let's try to build up
some intuition about what differential privacy is and how it actually
protects privacy. Algorithms for
differential privacy are called mechanisms. So if I use the word mechanism, just think of it as an
algorithm that you run on your code to produce
privacy preserving outputs. Even though differential privacy was officially invented in 2006, mechanisms for
differential privacy existed much earlier. Let's go back in time
to just before 1965. In those days, meetings were held face-to-face
instead of through Zoom. There were also in-person
face-to-face surveys. The problem with these
surveys is that some of the questions just
ruined the mood. For example, suppose
you are researching insider trading. You ask the interviewee
if they ever did it, and they become more likely to lie to you or just throw you
out of their house. In 1965, Warner invented
randomized response. This is a technique that
relies on a spinner. Only the respondents
sees the spinner. It has a true region
that occupies a proportion, p, of the area,
and a false region that occupies a proportion,
one minus p. Here p is some number strictly
between 1/2 and one. The way this works is if
the spinner lands on true, the respondent
answers truthfully, and if the spinner
lands on false, the respondent is
supposed to lie. The interviewer does not see the spinner and does not know if the respondent
lied or not. But the interviewer does know
the probability of a lie, which is 1 minus p. For a respondent
using this mechanism, we'll see that their privacy guarantees are the following. With randomized response, their private
information is protected almost as well as the case where they just
lie whenever it suits them. So the protection they receive only depends on the
randomness in the spinner. It does not depend
on any prior beliefs that the attacker, or
interviewer, or anyone else might have about the data. This spinner is
completely public. The only thing that the
respondent needs to keep hidden is the randomness in it, which is basically the
outcome of the spin. Let's analyze the privacy
guarantees in more detail. Before Warner, this
is what happens. Suppose the respondent has
engaged in insider trading, the respondent has two choices. The first choice is
to answer truthfully. We call this the factual world. The second choice is to
always deny insider trading, which is what we
call the privacy preserving counterfactual world, because their response does not incriminate
them in this case. If we don't trust
the interviewer, we have a strong incentive
to avoid being factual. Now let's see what happens
with Warner's spinner. Again, a respondent is
guilty of insider trading. We can view the spinner
now as a machine. The respondent feeds
their response into the machine and
with probability p, the machine outputs the
response unchanged, and with probability 1 minus p, it flips the response. Now, in a factual world where
the respondent inputs Yes, the output is Yes
with probability p and No with
probability 1 minus p. Now let's consider the
privacy preserving counterfactual world where
the respondent inputs No. Now the output is Yes with probability 1 minus
p instead of p, and the output is No
with probability p. So the question is, now that we have this machine in-between the respondent
and the interviewer, is there still an
incentive to lie? Let's take a closer look. First, we summarize
what we know. If the respondent feeds
in Yes into the spinner, the output is Yes
with probability p. If the respondent feeds
No into the spinner, the output is Yes with
probability one minus p. We see that if the respondent decides to be factual
instead of lying, the probability of a Yes
goes from 1 minus p to p, which is a change
by a factor of p over 1 minus p. Similarly, the probability of
a No changes by a factor of 1 minus p over p. So as p gets closer to half, these factors get
closer to half, meaning that the
probabilities are almost unchanged no matter
what the input is, so no matter what
the respondent said. Hence, p close to 1/2
gives more privacy, and p far from 1/2
gives less privacy. We can summarize what we've
learned with one equation, which is this one right here. For any possible output, whether it's Yes or No, we can examine the probability
of seeing this output, E, if we feed in a true response into the mechanism, and we can compare it to the probability
of seeing E if we lied to the mechanism. We see that the
probability of any output changes up or
down by at most a factor of p over 1 minus
p. So we're going to take the natural
log of this quantity, so natural log of
p over 1 minus p, and we're going to
call it Epsilon. A spoiler alert, this is the Epsilon in
differential privacy, and this is one way
that you can use to interpret what it means. Let's see how this equation here translates into
meaningful privacy guarantees. Let's give the respondent
a name, call him Bob, to make it a little bit
less than personal. I'll also introduce
a new character now, which is the data snooper. Sometimes, in the
literature, it's called an attacker or an adversary. The data snooper
sees the output of the randomized response,
and instead of using it for statistical purposes like
you should, the snooper wants to infer a
specific information about Bob's response. A Bayesian snooper would have a prior probability that
Bob is an insider trader and could compute
these prior odds, the prior probability
a response equals Yes over prior probability
response equals no. If Bob is honest with the
randomized response mechanism, how would the snooper's
belief change? Let's suppose that the output of the randomized response was Yes. The snooper can use
this information to get a posterior distribution and compute the posterior
odds over here, which are the
updated probability that Bob is an insider trader divided by the updated
probability that he is not. How did the prior odds over here compare to the posterior odds? Randomized response guarantees that they increase or decrease by a factor of p over 1 minus p. If you remember that Epsilon is a log
of this quantity, it means that the change is by at most e^Epsilon or e
to the minus Epsilon. What if randomized
response produced the output No instead of Yes? We're going to get
the same guarantee. You work through the math, and basically the
change from prior odds to posterior odds is
again bounded between e to the minus Epsilon
and e^Epsilon. To summarize, no
matter what happens, the odd change by at
most a factor e^Epsilon. No matter what
happens, and no matter what the respondent does, that's the bound on the odds. We discussed this in the
context of randomized response. But the same guarantees
actually hold for any differentially private
algorithm in the local model, which is a setting
where you don't trust the data collector. So since e^Epsilon refers
to the change in odds, I just put in some typical
values that people use. So setting Epsilon
equals to 1 allows a change by a factor
of up to 2.72, setting Epsilon to 0.5
allows a change by up to 1.65, and setting Epsilon to 0.1 allows a change by a
factor of around 1.11. That's the Bayesian
interpretation. Let's go on to the
frequentist's interpretation. For concreteness,
let's suppose that the spinner
probability p is 0.55. So they tell the truth
with probability 0.55. So our magical quantity, p
over 1 minus p, becomes 1.22, and the Epsilon, the natural
log, of that is about 0.2. So our frequentist snooper
may have decided that the null hypothesis
is that Bob did not engage in insider trading, so the alternative hypothesis would be that Bob is guilty. Under the null hypothesis, the probability of observing
output Yes is 0.45, and the probability of observing
the output No is 0.55. So no matter what happens
with the spinner, the snooper does not gain enough evidence to reject the null hypothesis
at reasonable levels. But you might ask, what
if the snooper has other evidence about
Bob and wants to create a hypothesis
testing procedure that combines it with the output
of randomized response? The additional effect of
randomized response when combined with a prior evidence, so the change is a little bit harder to describe, but
basically, the ratio of the power to the Type
I error will change by, at most, e^Epsilon. Another way to
think of it is that if we keep the Type I error fixed, that power can increase by, at most, 2^Epsilon. Although again, we
discussed it in the context of randomized
response, the same holds for any differentially
private algorithm in the local model. There are other
interpretations that are possible that you can use
without getting too technical. If you want to
explain to someone without using a lot of math, you can give a
scenario like this. So suppose Bob is
participating in an insider trading survey
with 100 other people. Suppose Bob is the only
one who actually traded stocks based on
non-public information, Bob is the outlier
in the state of set, but you can convince him that he would still be
protected as follows. Suppose the randomized response
probably again is 0.55. If Bob answers truthfully, there's an almost even chance
that his response will be changed to No anyway,
0.45, close enough. If Bob's response doesn't
get randomly changed, so the mechanism
outputs Yes for him, there's still going to be
around 45 other people whose response was
changed to Yes. So Bob is now effectively hiding in a crowd of around
45 other people, and even if the interviewer knew that one person in the
survey was guilty, the interviewer won't
know which one it was because there are two
layers of uncertainty, whether or not the guilty
person's response was changed, and if it wasn't changed,
then which among these 45 people with Yes
outputs is the guilty one? In this interpretation, we used uncertainty
about the data, which is that one
person was guilty, combined with the
additional randomness in randomized response. But in the previous
interpretations, we didn't. For example, in the
Bayesian interpretation, the bounds of the change in the odds worked for any prior
the attacker might have, so it only depended on the randomization
and the spinner. You can strengthen the
randomized response guarantees by considering attacker priors, but you don't need to because the randomness in the spinner is strong enough by
itself to protect you. We've spent a lot of time
discussing the privacy of randomized response
and, actually, by extension of differential
privacy in the local model. Let's see what
information we can gain by applying this technique. Suppose there's this newly
introduced senate bill, and it's a bit controversial. But despite that, we
still want to know how many senators privately
approve of this bill, so not their public statements, but their private
beliefs about it. If you walk into each senator's office
and ask them directly, you might see some
response bias. The answer you get might be different from their true
beliefs because they might be worried about re-election, or they may have made prior deals related to other bills, or
there might be other reasons. So instead of being direct, you could bring a copy of Warner's spinner with you
to each senator's office. Now, as they listen
with rapt attention to your explanation
of the mathematics of randomized response
privacy guarantees, two things are going
to become clear. First, there's no longer
an incentive to lie. So the information leakage
is actually small. If they have an incentive
to lie, it's very small. But on the other hand, they might be interested
in the result. What did their colleagues
privately believe? So that's actually an
incentive to submit a true input to
randomized response. Overall, due to the possibility
of gaining information, that can be an incentive to truthfully participate
in the protocol. We're going to repeat this
with all of the senators and get a randomized response
report from each of them. So now we're interested in Pi_s, which is the proportion of senators who privately
support this bill. We don't know Pi_s. All we know is the number
of senators for which the output of randomized
response was Yes. We could call that number Y. From Y, we have
to estimate Pi_s. There's a wrong way to do that. The wrong way is to divide Y, the number of Yes responses from randomized response, by
the number of senators. This should be obvious,
but just in case, if the randomized
response parameter is 0.6 and no senator
supported the bill, we would still see
around 40 reports equal to Yes because of
randomness in the spinner. We can't just ignore the perturbations
applied to the data. This is an important
point, actually, when you're working with
differential privacy or any other disclosure
avoidance technique. The nice thing about
randomized response and differential privacy
is that you know exactly how the
data are randomized so that you can adjust
your inference. In the case of
randomized response, we can use this formula
over here to get an unbiased estimate
of the proportion of senators who privately
support this bill. The standard deviation of
your estimate would be this slightly long formula. If we plug in our numbers
from our numerical example, where the privacy
parameter was 0.6, we see that the standard
deviation we get here is around 0.25 for the proportion of
senators supporting the bill. Since we're estimating
proportion, basically a number
between zero and one, a standard deviation of
0.25 is unacceptably large. So this seems disappointing. We went through all this trouble to protect the privacy of the senators, and at the end, we got a fairly useless result. There are several
things we can do. The first thing is we can try to increase
the sample size. We know that there
are a 100 senators, but there are many
more representatives. There are 435 of them, so maybe we can
improve the accuracy by targeting a
larger population. Now, we can go to the office
of each representative. They'll listen again
to our explanation of randomized response, probably with even more
fascination than the senators. So we're trying to
estimate the proportion of representatives who privately think the
bill is a good idea. This will be Pi_r. After applying randomized
response, we get, again, Y outputs that equal Yes. We can get an
unbiased estimate of the proportion of
representatives supporting the bill
using this formula. It has this standard deviation. We plug in our numbers, the population size and
the privacy parameter, we get a standard
deviation of around 0.117. It's not great, but it's much better than our exercise
with the senate. Overall, what we would
see as we increase our population is that
the standard deviation of our estimate decreases at a rate proportional to 1 over
square root population size. Again, we're going
to ask the question, can we do better? For example, can we keep the
same nice privacy guarantee but improve on accuracy? Now, the answer is Yes, but we need to trust
the data collector. Previously, we were in
the local model where the respondents did not
trust the data collector. So let's consider what happens when the data collector
is actually trusted. The senators welcomed us back and told us truthfully
what they think. We tally up the Yes
responses, and we need to figure out a way to
publicize the result. So first, we note that we can't just reveal
the exact tally. The reason is, let's say that in a sudden outbreak
of bipartisanship, 99 senators decide
to collude and use the exact tally to figure out what was the response
of the 100th senator. The 100th senator would be in trouble if we reveal
the exact tally. We have to perturb it somehow. Since the senators were so excited about
randomized response, we should perturb
it in such a way that each senator gets the same privacy protection as they would have with
randomized response. The trick is we need to provide the same privacy but
better accuracy. Just as a reminder of the randomized
response guarantees, if one person
changes their input, the output probability of
any event increased or decreased by a factor of,
at most, p over 1 minus p, which is the same as e^Epsilon. The insight in the
paper that introduced differential privacy is that the data collector can actually, instead of perturbing
each record individually, you can add Laplace
noise with scale 1 over Epsilon to the number
of Yes responses. This process is called
the Laplace mechanism. The Laplace distribution, if you're not very
familiar with it, with scale 1 over Epsilon, it has the following
density function over here, and its variance
is 2 over Epsilon^2. It gives the same
privacy guarantee as randomized response
but more accuracy. So let's take a look at
this in more detail. We have 99 colluding senators
trying to determine how the 100th senator responded to the trusted data collector. These 99 senators get together and determine
that x of them responded Yes and 99
minus x responded No. Regarding Senator 100, there
are two possibilities. Case 1, the senator voted Yes, in which case the
trusted data collector would see x plus 1 Yes votes and adds Laplace noise to it. Then you get the following
density function with the peak at x plus 1. In Case 2, the senator votes No, and the data collector sees x instead of x plus 1 Yes votes and adds Laplace noise, with the scale 1
over Epsilon to it. Here we have the
output distribution with the peak at x
instead of x plus 1. The 99 colluding senators are going to observe a noisy count. They know it either
came from the top distribution or from the bottom distribution
and their job is to figure out which one it was. Let's plot the
densities together. The last is this plot where we just use the same point instead
of two different plots. On the right, it's
the same as the left, except I changed it to be
log scale on the y-axis. When we plug in the log scale, the ratio of the
densities is equivalent to a visual vertical
distance between them. This log scale plot shows that the ratio of the densities
is always bounded, and it's between e
to the minus Epsilon and e^Epsilon. What
does this mean? That means if Senator
100 had submitted a No response instead of a
Yes response or vice versa, the only effect
this would have is that the probability
density would increase or decrease by a factor of, at most, e^Epsilon. From this, it follows that the probability of any
output event would increase or decrease
by a factor of at most e^Epsilon. So the probability of seeing some event e if
senator responded Yes, it would be at most
e^Epsilon times the probability of that event had the senator responded
No, and vice versa. So these are the same
equations which gave us the randomized response
privacy guarantees. So they guarantee that any data
snooper, even with the help of the other 99
senators, would have low confidence in
trying to figure out whether Senator 100
voted Yes or No. This guarantee is not
specific to Senator 100. Any senator gets this guarantee. So the confidentiality
of their response is protected even if all of
the other senators collude and use the latest data
science inference techniques, or somehow they travel
into the future and get some future data
science techniques, none of this will
help the snooper. So differential privacy
is future-proof. So we've got the same
[inaudible] privacy guarantees, but let's examine accuracy. So when we use randomized
response with p equals 0.6, that corresponds to
an Epsilon of 0.405. When we try to estimate the
proportion of senators who privately supported the bill by using randomized response, we got a standard
deviation of about 0.25. If we have a trusted
data collector who uses the Laplace mechanism, the standard deviation
then decreases to 0.035. If we try the same exercise in the House of Representatives, which has a larger population, we see that the estimate under randomized response has a
standard deviation 0.117, but under the Laplace
mechanism, it's 0.008. If we worked out the
math and not just relied on a few
numerical examples, we'd see that under
randomized response, standard deviation
due to privacy is proportional to 1 over
square root population size. On the other hand, under
the Laplace mechanism, the standard deviation is
proportional to 1 over n. Aside from accuracy, an under appreciated
selling point is that, in both of these cases, we had full transparency. Full details about
the Laplace mechanism and randomized response can be made public, and none of this degrades the privacy guarantees. But it does help
the data analysts. So you can take the output of
randomized response or the Laplace mechanism. They can use it to estimate properties of the population, such as how many members of Congress supported
that particular issue, and the transparency
allows us to create estimates, compute
standard deviations, confidence intervals, and so on, which we wouldn't
be able to do if the mechanisms were hidden. Now let's examine differential privacy itself
more systematically. Differential privacy is built on the concept of
neighboring databases. For now, let's say
that two databases are neighbors if you can obtain D_2 by changing exactly
one record in D_1. You can think of this as the
difference between D_1 and D_2 is the response of
a single individual. That's where the
differential part of differential
privacy comes from. Note that if we change the
response of one person, then n, which is the
number of people in the dataset, it stays the same. We're going to revisit this
point a little bit later. Remember, a mechanism is something that we
feed data into. It performs some
data perturbations and produces an output. So we say that if a mechanism satisfies
differential privacy, then we get all of these
good privacy properties that we talked about with
randomized response. The Epsilon that
we've been talking about all this time
has a fancy name. It's called the
privacy loss budget, and we'll see why that is later. But to check if a mechanism satisfies Epsilon
differential privacy, what we need to do is
we need to consider all possible pairs of databases that are
neighbors to each other. We also have to consider
all possible sets E, and then we have to check
that E is essentially the randomized
response equations hold for all of those choices. These equations
say that if we run our mechanism on database
D_1 instead of D_2, all it does is change
the output probabilities by a factor of, at
most, e^ Epsilon. So if one person
changes their response, all it does is change the output probabilities
by a little bit. There are a few key
points to bear in mind. We have to look at
all possible pairs that are neighbors
of each other. We don't just look
at neighbors of the actual database
that we've assembled. The second point is that the mechanisms must
employ randomness. We saw that it was true
for randomized response. We saw that it was true
for the Laplace mechanism. We also saw that we
cannot just publish the true count of Yes votes. The true count can be attacked. In our previous examples, senators can collude and use the true count to determine
what other senators did. But in general, if you
publish true counts, it makes you
vulnerable to linking attacks and data reconstruction
attacks as well. The final point is
very important. The probability computation only involves randomness in
the mechanism itself. We don't use any probabilities
associated with the data. There are several
reasons for this. The first is it avoids
problems like losing privacy guarantees by misspecifying the
data distribution. The second reason is that we can gain useful properties
like composition, which is going to be
discussed in a later talk. Let's see how this
works in pictures. Here's one example of
neighbors D_1 and D_2. The only difference
here is how Senator X voted, either the blue
vote or the green vote. This pair of neighbors
models this situation where the rest of the
data has been collected, and Senator X is modeling
what response to give. Since D_1 is a neighbor of D_2, and D_2 is also a
neighbor of D_1, we can plug them into the differential
privacy equations, this one and this one. Together, what they say is that Senator X's decision
cannot change any output probability by a factor larger than e^Epsilon. Intuitively, these equations guarantee that the noise we add almost masks out to the
contribution of Senator X. Remember, we have to consider
all pairs of neighbors. The differential
privacy equations are applied to neighbors
that defer on Senator 1 to ensure that Senator
1 is protected, the differential
privacy equations also apply to neighbors
that defer for Senator 2 to ensure that Senator 2
is protected, and so on. If you actually counted
the set of neighbors, you'd see that there are 2^100
such possible neighbors. That's why it's important
to also know how to construct these algorithms which will be covered in
the later talk. But for now, we saw that
differential privacy protects the response
of any individual. But what if we want to hide
not just their response but whether they participated in the survey in the first place? For example, if
you're using IRS data for statistical purposes, you not only need to protect the contents of the tax
returns you have access to, but you also need to
protect information about whether someone or not even
filed a tax form at all. Similarly, if you
run a STD study, someone's participation in it could already be revealing, regardless of what the
information they provided. So to hide participation and any response
if there is one, we can simply change the
definition of neighbors. So instead of two
databases being neighbors, if they differ on the
value of one record, we can say two databases are neighbors if you can
obtain one from the other by adding or removing
a record like this. This version of neighbors is
called unbounded neighbors. The reason is because it changes the size of the dataset. Previous version is
called bounded neighbors because it didn't
change the size. Here the difference between
D_1 and D_2 is that D_1 has an additional person. Based on the output of a differentially
private mechanism, we won't be able to say
with any confidence whether the input
was D_1 or D_2, which is the same as saying, we don't know whether this person's data
was used or not. So regardless of whether the person actually
opts in or out, inference about whether the data was collected is protected, and inference about the contents of the record is also protected. So far we've talked
about bounded neighbors which are pairs of databases that differ on the
value of a record. If you use that in the definition of
differential privacy, it protects the contents of
the record from inference. Then we talked about
unbounded neighbors which differ on the presence
or absence of a record. If we use those
with a definition of differential privacy, we're protecting whether
someone participated or not. Then there's a third commonly used definition of neighbors, and these are called
action-level neighbors. The best way to
introduce this concept is to have an example. So here's an example
of two databases. Here every customer has a
record, and every record consists of a set of actions, such as the purchases
of a customer. These databases, they differ
on one purchase right here, one purchase for one customer. Since we equate an
action with a purchase, that means that
these two databases differ on the action
of one person. If you use this
definition of neighbors, what you're protecting
is a specific action. We won't know if Customer
1 bought the crab today. But we're not protecting our inferences based on many
actions about the customer. For example, using
action-level neighbors, we can reveal that
Customer 1 likes seafood, but we won't know that they bought crab on a specific day. In general, customers
with many actions get weaker privacy protections
than customers with fewer actions
in this setting. To summarize, you
can choose how to define the concept of neighbors in the definition of
differential privacy. Among the most popular choices, the first one we discussed
is bounded neighbors, where two databases differ
on the value of one record. Because all pairs of neighbors here have the
same number of respondents, we can release n, the total number of
respondents, without noise. But what this means is
that we should only use bounded neighbors if n, the number of respondents,
is already public. We shouldn't voluntarily
release this information. The reason we don't want
to release n in general, is because we want to avoid data fragmentation.
Here's an example. We have a data collector
with a large dataset, but the data collector
decides to split one big dataset into
many smaller datasets. For example, for
each combination of age, ethnicity, gender, disease, and geographic region, there is a new dataset. For example, there's one dataset exclusively about
32-year-old Hispanic women with cancer in a small
geographic region, and so on. If you reveal the number of respondents in each of
these tiny datasets, that degrades privacy
significantly because of the detailed selection
criteria for these datasets. As a general rule, it
is good to avoid using bounded neighbors so that
you don't leak n directly. Then we talked about
unbounded neighbors, where our neighboring
databases differ on the presence or absence
of one individual's data. This means that
neighboring databases here have different
numbers of respondents, and so we cannot release
the exact sample size n, but this is also the most recommended choice of neighbors. Then finally, we discussed
action-level neighbors which provide the weakest
protections among the three. To summarize the
last 40 minutes, this is differential privacy. This is the definition. In a nutshell, it
limits how much your information can change
output probabilities. It protects properties
of the individual, but it allows you to infer
properties of the population. So it resists linking attacks and other types of attacks. It's transparent so you can
release the source code, and privacy only depends on randomness in the
mechanism, not in the data. The main idea behind differential privacy is
that we add just enough noise too hide the
possible effect of any one individual. There are two main
models used today. One is the one where you
trust the data collector, and one is the model where you don't with
the local model. 