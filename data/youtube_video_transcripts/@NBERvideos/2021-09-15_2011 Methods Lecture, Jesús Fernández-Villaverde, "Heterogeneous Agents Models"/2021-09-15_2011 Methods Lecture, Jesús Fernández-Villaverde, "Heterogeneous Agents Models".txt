Jesus Fernandez Villaverde:
This is the last stage of this long journey, but hopefully, we will survive. We seem to be having a lot of attraction, but that's okay. Heterogenous agent models, makes us change a
little bit gears. This is slightly different
from what we have been doing during the first sessions. Everything that we learn was about how to compute all
these business cycle models that are either with a representative agent or with a very trivial form
of heterogeneity. But there are many situations where we really want to handle models with real heterogeneity, with non-trivial heterogeneity. The typical examples include, for instance,
heterogeneity in age. Take your favorite
Overlapping Generation model, there are going to be
younger people and there is going to be older people and there are people
in the middle. Imagine that you want to write a paper about social security. How can you have a paper about social security if you don't
have heterogeneity in age, they are not people retiring, they are younger people. There
are people in the middle. That's something that
you definitely need. Very interesting problem, is heterogeneity in preferences. When you talk with
microeconometricians, they will tell you that
there seemed to be a lot of heterogeneity in
preferences out there. That you see people
with absolutely the same observables and yet picking very
different things in life. This is interesting,
for example, because of issues
like risk-sharing. Think about the
following situation. Imagine that I'm
very risk-adverse and there are people who
are less risk-averse. We go through the
business cycle, the optimal allocation
will be for me who, I'm very little risk adverse to take a lot of the heat
of the recession. While those who are less risk adverse are the ones that are very
insured against this thing. You are going to have
a very rich set of predictions about how
to allocate risk, etcetera. Heterogeneity
and abilities. I think I can do a fair
job of teaching macro. I don't think I will do a
very good job teaching. I don't know Japanese
even if you asked because I don't speak
a single word of it. There is a lot of
heterogeneity in your market. This is particularly important, for example, if we want to think about issues like today. Some people have argued maybe the reason why that your
market is not working that well lately is because
people have different set of skills than the ones that
are needed right now. Well, I don't know if
this is true or not, but at least it's an
interesting assumption or an interesting hypothesis to
explore a little bit more. Heterogeneity in policies. I think here the clearest
possible example that we could think of is the idea of progressive
marginal tax rates. If we really want to think
about tax rates that look like the actual
tax schedules that we see in the data, you need to have some
type of heterogeneity. There needs to be
rich people that pay higher marginal tax rates, than poor people, otherwise, the whole thing, the whole issue is completely irrelevant. The issue is why in the
world we want to think about all these very
rich heterogeneity from this general
equilibrium perspective. Because in fact there
is a tradition in micro of thinking about models with a lot of rich heterogeneity. Yet they do it from a very partial
equilibrium perspective. Arguments that people
have presented over the years about why we want to do this is first of all, because we are going to
impose a lot of discipline. We will see that one of the interesting things
that is going to happen in general equilibrium is
that the relation between the discount factor and the interest rate is
going to be endogenous. There is actually going to be non-trivially endogenous
is not going to be the case at beta is
just the inverse of the interest rate or
the other way around. The interest rate
as the inverse of beta is going to be
something more going on. It's going to generate an
endogenous consumption and wealth distribution. Hence, we can go to
the data and see if what we observe in
the data looks like anything that the
model generates. We know that the wealth
distribution in the US has become more unequal over the
last 25 years, 30 years. What type of features
we need to introduce in the model to generate that was this technological change, was this change in policies? Was this just change in
abilities in the population? You have different theories,
different hypotheses, and you want to have a
model that will speak to those in a serious
and disciplined way. Finally, because it enables what I'm going to call meaningful
policy experiments. The end of the day, we want to say things like what happens. Imagine that we are really seriously interested
in fiscal policy. What will happen if I change
the marginal tax rates is, I change these days, it seems that Congress. One of the things of the agreement between
the President and Congress may be that they will change the marginal
tax rates a lot. What is going to be
the effect of that? We want to have a model where there are
different types of people that respond
to these things. We can come with
a number and say, I believe it or I
don't believe it, and so on and so forth. Before I get into more details, I only want to point
out that a lot of this material I borrow from my colleague Bill Krueger, who teaches a class on heterogeneous agent
models at Penn. I figure it out that, I may as well follow a
lot of his explanations Instead of basically re-cooking
the whole thing myself. Actually your
market paper was on heterogeneous agents and
it was with Bill Krueger. Everything stays at home anyway. This is my most cited
paper by the way, but which means that my life has only been
going down since then. But anyway, that's
a different issue. There are many ways to
think about heterogeneity. Many dimensions where
we can really think about how these heterogenous
agent models work. But I think that the best possible way to
get into this literature is to think about what
sometimes is known as Aiyagari model or Willie
Aiyagari for Rao Aiyagari, who used to be a professor at Minnesota and Rochester and unfortunately he
died very young. There is the following. We're going to have a
continuum of individuals between 01 and each of them is going to face
what I'm going to call an income
fluctuation problem. That is, they are
going to wake up in the morning and sometimes they are going to be very productive. Sometimes they are not going
to be very productive. But there is not going to be
any aggregate uncertainty. This basically means
that on the aggregate, everything is stationary, everything is constant, is
only at the individual level. This is going to be already a very rich and
interesting model to think about that is going to teach us
a lot of things. Later, in this
lecture today I will introduce sorry, this
aggregate uncertainty. Every person, so
the labor income is going to be equal to how productive you are
or your labor endowment. You can think about how many hours of work in
that day times the wage. The labor endowment of every one is going to be the same. It doesn't matter. The process, not
the realization. I am agent 0.329 and
you are agent 0.259. We follow the same process, except that I may get
high today and you low. Tomorrow is the
other way around. You get high and I get low. To make things easy, that process is
only going to take finite number of values so
n possible realizations, high or low will be the
simplest possible one. But you can really have very rich 10 or
20 possibilities. There is going to be a
stationary Markov chain. That is going to
tell you what is the probability that I
will be, for example, high if today is low or
the probability I will be low tomorrow,
if today I'm low. I'm going to follow
recursive notation. Y prime is the
realization tomorrow, conditional on the
realization today. Very important
thing, I'm going to assume there is a law
of large numbers. By this imagine that the probability that I
get high is 40 percent, then 40 percent of the
population will get high. This has a technical
difficulty, which is, it really forces all
these realizations not to be absolutely
independent. Because we need to
have a subtle way of dependence to ensure that we have this 40 percent of people getting the right shock. In comparison with
the 40 percent of the individual probability. There is a nice paper
by Haral Ulik in economic theory
where he actually shows all the math behind this. For our purpose, since we are going to do
this in the computer, it doesn't really
matter that much as long as we understand
that the law of large numbers and
we need to apply is not the regular law
of large numbers is a more subtle one. There is going to be a
stationary distribution. Pi of these associated with lowercase pi of these
different realizations of the labor endowment. Basically you're going
to have some metrics or Markov chain with some
movements up and down. This is going to
imply that at the end we may have like 30-40 and 30. Imagine that we have
high, low and medium. Then we are going
to have 30%, 40%, 30% and that's going to be the stationary distribution
associated with it. At period zero the
income of all agents, the labor endowment
is going to be given. The population since we are
going to be thinking about models without
aggregate uncertainty and without transitions yet, I'll talk about
transitions later on. Is going to be the case that the population
distribution happens to be the stationary distribution. That we start exactly where
we were supposed to start. The total labor endowment
in the economy is going to be a constant. What is that? What is just the stationary distribution times the amount of labor in each of the points of the
stationary distribution. Imagine that I have
high, medium and low. What is the total? The
stationary distribution is 30, 40, 30. What is the total amount
of labor in the economy? High times 30 plus medium
times 40 plus high times 30. That's the total
amount of the economy. Everything is stationary
so that labor is constant. Since we have a very nice and simple mark-off structure, our history, capital T remember the notation this morning is the whole history of events. Is just the multiplication
on the factor of all probabilities
period by period. We start at y0, the property of moving
to y1, blah, blah, blah, blah, until the probability of moving from yT minus 1 to yT. As I was saying before, this very simple setup
really allows us having a lot of
idiosyncratic uncertainty, but that is not going to
be aggregate uncertainty. Hence, there is at least a
little bit of hope of having a stationary equilibrium with constant interest rate and constant wages that we
can easily compute. Preferences, very simple. It's agent is going
to have expected discounted utility or the discount factor
is going to be Beta. The budget constraint
is going to be my consumption plus some asset is going to be equal to my labor endowment times my weight. That will be my labor income
plus the assets I carry from yesterday times the
interest rate on those. Very important thing, we don't have complete
markets over here. You only have access to
this uncontingent asset. That asset will not
pay you more or less if you have a low or a high realization
of your shock. Otherwise there is
no heterogeneity. The only thing we will do is
we will just buy insurance. If labor is high, I will pay back some
of my resources, if labor is low, I
will get back some. But everyone will be
consuming the same, they will be perfectly sharing. Key thing over here, to have heterogeneity is
not only that you have labor heterogeneity is
that you cannot fully insure against that
labor heterogeneity. Also we are going to have
some borrowing constraint. I'm not going to get into very careful discussion today about how we can specify it, where it can come from. Because the point is really
about computation more about pure data or economics. Let me just suppose that the
amount of assets that you can keep needs to be
strictly bigger than zero. That you can never borrow. A simple way to justify that is as Greece has done
in many of his papers, is you just get unemployed
from time to time, which will be one realization of this shock is zero and
you cannot pay back. Since you can be really unlucky and get unemployed
for the rest of your life, you will never ever be able to pay back and that
will be very bad. That's a simple
justification but there are many other
justifications. Maybe you can borrow
up to $100,000 or to $10,000 or to 50 percent
of your human capital. Who knows? That will really depend on the model you
are thinking about. We are going to have initial
conditions of agents. The assets at the
beginning of time and the labor endowment
at the beginning of time. This is the key thing
of this model because heterogeneous agents
that is going to create all the problems. We are going to have
a population measure. This is going to be a
distribution that is going to tell us how many people in the population has
this level of assets, and how many people in
the population have this level of labor income. Such that of course,
the integral over this distribution
will be one. Imagine that assets
can be high or low and productivity
can be high or low, we basically have
four possibilities so we'll have a 0.2, 0.4, 0.2, and 0.2. Something like that. This
is really the key because the whole point about
computing these models is to keep track efficiently
of this distribution. It's a very difficult
object to keep track of because it's an infinite
dimensional object. We will think about how to
keep track of it later on. The allocation, of course, is just to be a sequence of consumption and
asset holdings for each agent in the economy given the initial conditions and
some history of shocks. Technology. Various standard aggregate
production function with some standard assumptions. Capital depreciates at some
rate Delta and hence we have an aggregate resource
constraint where capital letters now are
going to be aggregates, total consumption plus
this is investment in the economy needs to
be equal to production. Something that is important
to remember is that the only asset in this economy is this physical capital stock. That the A's that I was
telling you before, they are really claims to
that physical capital. One constraint that
we will have to ensure holds in
equilibrium is that the total amount of A
integrating over all the agents in the economy is equal to the total amount of
capital in the economy. Of course, we don't have
any state contingent claims as I was
telling you before, we have this very stark
form of incomplete markets. However, we could have models where maybe
there are already some limited assets
and some limited amount of a state
contingent claims. But you know, let's
learn how to compute the basic model first before we get into more
complicated things. An equilibrium is a
very basic object. I include just the definition of equilibrium for you to see it, but I'm not going to go over it except for this slide over here. What this slide is
telling you is, this is total capital
in the economy. Actually total capital that
we carry into tomorrow. This needs to be equal to the total assets that each
individual wants to carry over given the history of each person and integrating over the
initial distribution. I need to be sure I have an initial
distribution of people, each person in that
initial distribution is going to evolve over time. Given the evolution of
the labor endowment, they are going to
decide how much assets they want to accumulate. Then when I integrate over all the people in the economy, I need to be sure that the
total amount of assets they have is exactly equal to the total capital
in the economy. Is just market-clearing
in the capital market. The same for labor, although we already
argued before that the total amount
of labor was constant. This is consumption plus
the total amount of capital needs to be equal to production plus undepreciated
capital from yesterday. The definition of equilibrium
is very, very standard, the only thing I need to be
sure is that I keep track of the right objects in terms of distributions to ensure market-clearing and
resource constraint. But maybe a better way to
think about this model is to think about situations in a reclusive
equilibrium framework. Instead of thinking
about this sequentially, let's think about this in
a reclusive framework. Where we are going to have the individual state is
going to be a and y, your asset and your income. We're going to have a
distribution period by period of agents in a and y. This is just the
definition of what all these different
objects are going to be, you don't need to
worry much about it. What is going to happen is
that we're going to have a transition function that is going to take one
distribution of people and make it into
another distribution of people tomorrow. Let me do a simple graph
to illustrate this. Imagine that we
just have assets. This is the distribution
of people today. That means that we
have all these guys with this very high level of assets and these guys with this very low
level of assets, and these guys with these
assets in the middle. This is the distribution today. This person is going to make some decisions about how many
assets to have tomorrow. This person will make decisions about how many assets
to have tomorrow, so on and so forth. We are going to have a new
distribution tomorrow. I don't know, maybe this one. What we are going to have
is a mapping between the distribution today into
the distribution tomorrow. Of course, what we're
going to look for is situations where we are in a fixed point of that mapping, where this distribution and this distribution are the same. The way to think about that is to bright a value
function for the agent, where now the value depends on the pure individualist states, which will be a and y, but also on the
aggregate distribution. Why is the aggregate
distribution I state for the household? Well, if this is an economy
with a lot of capital, what will happen with
the interest rate? Will be very low. Because remember, the interest
rate is just going to be the marginal
productivity of capital. If we have a lot of capital, because everyone has
a lot of capital, the interest rate will be small. I need to know not only
the amount of capital, but also the
distribution of capital. Because the distribution of
capital matters for me to understand how much capital that will be in the
economy tomorrow. Because the guy that have
little capital will have a different marginal
propensity to save than the guy that
has a lot of capital. That's why we have this
object over here as a state. This aggregated state, which is the whole
distribution in the economy. Now, on the right-hand side
of the Bellman equation, I will have my
utility today plus the discounted expected
utility tomorrow, which will be my value function
with my assets tomorrow, my productivity tomorrow, and
the distribution tomorrow. I integrate all the different
realizations of y prime. For me the
distribution is given, so there is no
uncertainty about that. Because there is no
aggregate uncertainty, so I don't need to do
anything about it. But an a prime is the same. I'm picking my amount
of assets today. The only uncertainties with respect to my
productivity tomorrow. What is my budget constraint? Well, my consumption plus
my assets tomorrow is equal to my weights that will depend on the amount of capital
in the economy, hence, it depends on the distribution, y, the income, and my labor endowment plus
1 plus r, the interest rate, which will depend on the distribution of
the economy times a. Then I will have
this law of motion from the distribution today into the distribution tomorrow that I take as given. For me is exogenous. I'm just an Epsilon person, so I just don't need to
do anything about it. You see the basic framework. You just have a
bunch of these guys, each of them takes the
distribution as given. They need to keep track of the distribution because
the distribution will determine their wages and the interest rate tomorrow,
and they say, well, given my assets, given
my labor endowment, and given the aggregate
distribution, what is my optimal choice? How do I evaluate
that given my payoff today and my continuation
payoff tomorrow? I just have a
definition of that, but that's not that interesting. Well, just a bunch of stuff. You don't need to know that. The issue is, how do I solve this problem
in the computer? I jumped quite a bit to page 17. How do I do this
in the computer? How do I actually
compute this thing? Well, the first thing that I would like to know is if there is in fact
an equilibrium, progressive competitive
equilibrium, which surprisingly enough, it's not so easy to show, you more or less can do it. Existence of
stationary recursive competitive equilibrium where we have a stationary
distribution usually boils down to show that there is an asset market
clearing condition where the total amount of capital
in the economy is equal to the total amount of
assets in the economy. By Walra's law, we
can't forget about the goods market and
the labor market is always indefinitely in
equilibrium by definition, such that, that asset
market is clear. Since the capital demand for firms is going to be defined by the traditional marginal
productivity of capital, we are going to have
all the nice properties that we need to show that there is an equilibrium,
at least one equilibrium. Is a little bit more
difficult to show uniqueness, and stability is even
less well understood. The stability of the sense
of what happens if we just, basically what I'm telling
you over here is that if we're starting the
stationary distribution, we stay in the
stationary distribution. But what if we just begin
an epsilon away from the stationary distribution?
We don't know that much. How do you compute this? It's actually
surprisingly simple in the case where you don't have aggregate uncertainty and
you are searching only for the stationary recursive
competitive equilibrium. What you do is the following. You say one, let me
pick an interest rate. The interest rate needs to
be between minus Delta y, because even if the
marginal productivity of capital is zero, then I just need to
still pay depreciation, so it cannot really
go below that, and then 1 minus Beta minus 1. In the standard model with only one
representative agent, the interest rate in the steady-state is
1 over Beta minus 1. That's comes directly from
the earlier equation. That will tell
you, I don't know. You will cancel marginal
utility over here, and Beta and R will
need to satisfy that. Well, I will argue in just one moment that in this model, agents are going to do a little bit of
precautionary behavior. They have incomplete markets, so they want to ensure
a little bit themselves against bad realizations, which basically means that
they are going to have, everyone in the economy
is going to have a little bit more capital that
they will otherwise have. Since there is going to be
a little bit more capital, the marginal productivity of capital is going
to be a little bit lower, hence, the interest rate is
going to be a little bit less that what the inverse of the discount factor
minus 1 will give you. If you have the
discount factor is 99, for example, 0.99, the interest rate is
not going to be one. It's going to be, I don't
know, maybe 0.8, 0.7, 0.95. That depends on parameters. But is what it's going to be. What we use the following,
it's very simple. You say, look, let me pick
a number between those two. That's going to be
my initial guess for the interest rate. In my particular case, I usually pick something quite close to 1 minus Beta minus 1, because for a
standard calibration, the precautionary behavior is not that amazing, not that big. Let's suppose that that is 0.01. Let me pick 0.008, like 20 percent less. Now, we're looking at a recursive competitive
equilibrium stationary version of it. Let me go back for a second to the problem of the household. This distribution and this distribution
are the same. We are in the
stationary situation. I don't really need to know the details of
that distribution. The only thing I need to know is how that distribution,
whatever it is, implies for w and for r. But r is the
guess I'm having now, and w is just going to come from the first-order
condition of the firm. Let me show you that. I
don't have it over here. Basically, I know that if at some interest rate the firm is going to operate
so much capital, and that's going to
operate so much weight, so much labor, so I can compute the marginal
productivity of labor, which is going to
be the weights. The only thing that I
really need to know of the aggregate distribution
is the interest rate. Then I'm just going
to solve this as a value function
iteration, plain vanilla, I'm facing these problems with my income fluctuation problems and I solve my value
function iteration. You can solve your value
function iteration with whatever method you fancy. If you are value function iteration guy dynamic
programming, dynamic programming. But if you want to apply
any of the projects, your methods we learned
in the previous section, that will be great as well. If you want to do
perturbation, well, probably it will also
work in many situations. Or you want to use
perturbation as an initial guess for
value function iteration. Basically, what you say is I have fixed the interest rate, I serve the household
recursive problem because now I don't really need to worry about this
aggregate distribution beyond the interest rate. That will give me a decision rule for every
agent in the economy. Basically we will have a
decision rule that tells me if you have this
much level of capital, how much capital you
want to have tomorrow. What I do is, given some initial guess of the
aggregate distribution, I just simulate over
time and I find the stationary
distribution implied by the decision rules
I just computed. I say, well, how much capital is accumulated in that
stationary distribution, and how much capital is
demanded at that interest rate r. If the
answer is the same, voila, I found my interest rate, sufficiently small differences below
some tolerance level. Walla, we found it. If not, I need to
change r and adjust it. If there is too much
demand for capital, I raise the interest rate, if there is too little
demand for capital, I lower the interest rate. In some sense, this computation
is very, very simple. I just say look, since we are in the stationary distribution, the only thing I
really care about is this total amount of
capital in the economy. I can keep track of all I need through the interest rates. I solve the problem
of the household. I simulated everyone
in the economy. I find the stationary
distribution. Sometimes you don't
even need to simulate, you can compute it directly. If you have discrete
asset values and I just adjust r until
I clear the market. Questions about this
basic algorithm? Is very simple. Problem,
takes a little bit of time. Why? Because basically you
are solving the problem of the value function iteration, and then you need to compute the stationary distribution, which may be computationally intensive and then
you need to come up with a new unit to check whether or not the interest
rate is the right one. If it is not the right
one you need to start again and you need to
do it again and again, and sometimes it may take
quite a few iterations of this whole thing before you actually converge
to the right one. Now, as I was
telling you before, in this model is going
to be the case that the interest rate is going to be a little bit less that one over Beta minus 1 because of this over-accumulation
of savings. It can be because of
liquidity constraints, because of prudence
or because of both. The question there, I guess
two different questions. One is how big of a
difference does this make? I'm not going to
review the literature. There have people that
have argue a lot or there are people that have
argue a little bit less. There is a non-trivial amount
of discussion over there. If we had a little
bit more time, I will be more than
pleased to review that, but there is also
policy implications. The main policy implication
is somewhat surprising one, which is this is
an economy where to a certain extent we
are saving too much. Anything that will lower a little bit saving
may actually be good. We know that in the model
with a representative agent, the optimal tax on capital
and there are many, many environments is just zero. Well, this is a model where you may want to have
a little bit of attacks on capital to lower the amount of
over-accumulation. That is a very simple
policy implication of this type of models. How many of the
results, for instance, in public finance, change, when we think about them in an heterogeneous
agent's context. I'm going to show you a
particular calibration. I will show you, doing a few experiments just to
show you how this works. Let me suppose that I pick a constant relative risk
aversion with values of the risk aversion of
the Sigma of 1, 3 and 5. I'm going to do
different combinations. The discount factor
is going to be 0.96, which imply that the
interest rate with complete markets will be 0.0416. Maybe not a bad approximation of the whole, an annual level. Cobb-Douglas production
function with Alpha, the coefficient on top
of capital of 0.36, and the depreciation factor
eight percent annual level. Then you need to specify
an earning profile. You go to the data and you will look at some sources of
micro data to try to understand how people's labor endowment or labor productivity
changes over time. Again, if this was a complete semester or a complete quarter on
intelligence agency model, believe me, I could talk about this for days and days and days. Having written more
papers that anyone ever suspected about
this and a few more. Without any type of endorsement, just let me put the
simplest possible I know, which is auto-regressive
in logs. We could have random walks and that will be
very interesting, but that will force us to
do a little bit more work. Just let me do
something stationary auto-regressive,
very, very simple. We have that the log of
my laboring productivity tomorrow is a function of my labor productivity
today and some shock. This thing, I'm just doing it
to normalize the variance. This is going to imply
some correlation over time and some
variances of shocks. I'm going to play
with situations where I have a lot of
correlation, 0.9, situations where I have
virtually no correlation, well, 0.3 no correlation at all, zero, and different
levels of volatility. Things with high volatility and things with low volatility. Then how can you do this? How can you actually put this stochastic process
in the computer? Well, something you can use is a method proposed by Tauchen, which I briefly
mentioned before. Which is just a
method that says, if I have an
autoregressive process, how do I come up with a simple discrete support
distribution mark off chain that reproduces a lot of the properties
of that process? Is very simple. For example, if you pick seven possible
events of the wall, what you do is you sub-divide the log of yt into seven intervals using
this rule over here. You pick mid-points
of those intervals. Well, this one is a little
bit crazy because it goes to minus infinity and this
one goes plus infinity. Yes, let me pick three standard
deviations minus three, and three standard deviations. What I'm going to do is just
going to apply a formula that you can find
in Tauchen's paper. You can find in any textbook in numerical methods like tangent, to tell me what is the
probability that I will jump from one
position to the other. Remember, basically
what we are having over here is we are going to have seven possible
realizations 1, 2, 3, 4, 5, 6, 7, and a matrix which is
going to be 7 by 7. Basically what we
need to do is fill each of these 49 probabilities. What you want is that the
probabilities are such that when you simulate
from this mark-off chain, that simulation from the
mark-off chain will look a lot like a simulation from the
autoregressive process. In particular, they will have more or less the same moments. By the way, the
Tauchen procedure is the one I have here
because it has been the most popular
in the literature. There is actually one
that I like a lot. I was recently the editor
for a paper about that Rouwenhorst proposed
many, many years ago. There is in this
book by Tim Cooley, a frontiers of business
cycle research. He has a chapter
on asset pricing in this type of RBC models. He proposed this, is an
alternative to Tauchen, I guess no one really paying
attention for 15, 20 years. Recently a bunch of people
have been looking at it and it really has a
very, very nice properties. I will invite you to
read the paper by Robin, and it's actually
extremely simple to call this, literally five lines. It seems to work really well. Basically, this is nothing
more than a system that will allow you to fill the different entries
of the 7 by 7 matrix. Then given that transition, you find the stationary
distribution. This is solved in this way. We know that the
stationary distribution needs to satisfy that when
you put the capital Pi, the transition matrix over here, and you multiply by the
stationary distribution, you get back the same thing. Yeah, fine. Then you compute the total amount
of labor implied by that distribution and you normalize such that in fact we have a total
amount of one. I just take this model, I compute it in
the way I computed before and I can compute
things like the savings rate, and the interest rate, etc. You can show a bunch of things. I have a very limited set of numerical results in
the interest of time. But for example,
the benchmark with complete markets is that you will have an
interest rate of 4.16 and our savings rate of 23.7. What you can show is that
for example, three things. One is keeping the risk
aversion and the variance, the standard deviation
of income shocks, fix and increase in
the persistence of the shock induces more
precautionary saving and more accumulation
of capital. Well, and that makes
a lot of sense. Remember, shocks
are good or bad. You can have either a
very good realization or a very bad realization. If you know that you can have a very
persistent bad realization, you are really afraid of that. You are going to
accumulate a lot in case that you have a
very bad realization. However, if it happens to be the case that the
persistence is very little, very small, you say, who cares
about a bad realization? Is going to be washed out
anyway in just one second. The same way, keeping the persistence fix
on the variance, the standard deviation
of the shock fixed, an increase in risk aversion
will make you safe more. Well again, because you face
the same amount of risk, but you hate risk more so you want to be
careful about it. Finally, the third
combination is that keeping the risk aversion and the persistence
fixed and increasing the volatility will lead
you to accumulate more. Again, because you are afraid
that you're going to have a lot of shocks. This was the benchmark algorithm that you use to solve
the basic model. You search for the
stationary distribution, no aggregate uncertainty. That's fine and
that's interesting. It allows you to address
really a lot of problems, but there are many other
things that we are interested that involves
some type of transition. For example, let's think
about Social Security reform. Let's imagine that
we go from having our Social Security system
where the retirement age is 67 to our Social Security system where the retirement age is 70. Or a Social Security with
lower replacement rate. Or we are going to change
the tax rate on capital. We're not only interested
in the final outcome, the newest stationary
distribution to which we convert, we are also interested in the whole evolution of
this thing over time. Because actually, as I will
argue in just one second, the welfare consequences
of this type of policies may crucially
depend on the transition. The best example is
social security. Is usually the case that
moving from a pay-as-you-go to a fully funded system if you are just comparing the
two steady states, the fully funded is better. But you need to finance
that transition. Once you take account of the welfare costs
along the transition, is not obvious anymore,
but it's a good idea. Because basically going
from pay-as-you-go to fully funded mind that they
want to do that right now. Well, I'm in a really
bad situation, I'm 38 years old. It basically means that now for the next 25 years I will need both to pay the
retired people right now and to accumulate
from my own saving. Very bad deal. If I'm 64, I said, why do I care I'm
about to retire anyway. If you are 21, you
may say, well, yes, it's true that I need to
pay for this transition, but I'm going to be
so many years under the new system that I
may be the better off. You may be in a situation where the old and
the young are in favor of the transition and the people in
the middle are not. This is the perfect example of a heterogeneity,
in situations. Or as I always say, the day I finished
paying my mortgage, I radically changed my
attitude towards inflation. Before when I had a large mortgage
inflation, great idea. Now that I don't
have any mortgage, but I have a nominal
income from Penn, inflation, very bad idea. This is the type of
things you can think about with heterogeneous
agent models. Let's think about the
following exercise. Let's suppose that in a
completely unexpected way, tomorrow I'm elected
president of the US and I change
Social Security, or I change taxes. How is the economy going
to react and how I want to keep track of the whole
distribution of things? In particular, just to
make things concrete, let me think about
the case where I introduce our capital
income tax at a rate Tau. What I'm going to do
is I'm just going to take the receipts of these taxes and I pay them back lump-sum
to everyone in the economy. What are the
consequences of that? Well, this is actually
very simple to compute. If you are ready to
accept one assumption, which is after T periods, you are in the newest
stationary equilibrium. T can be large,
can be 200 years, but you are in the nearest
stationary equilibrium. It's a hard assumption to make because we know that convergence is going to be asymptotic. Think about the
neoclassical growth model. When we converge we
quite never get there, even if we spend 2000 years. But you say look,
numerically, I don't know, after 50 years, we are close enough, it
doesn't really matter. Let's suppose that
we say in 50 years, we are in our newest
stationary equilibrium. What do we do? Well, first, I take the
algorithm I learned before, and I compute the
stationary equilibrium for the economy without
a tax on capital. I guess some interest rate, solve the problem
of the households, I find the stationary
distribution. Does it clear markets? Yes, no, if no, I change the interest rate
until I clear markets. Now, I do the same, but for the case where I already have the
tax on capital. Suddenly the same step. You can even use
the same algorithm , as in the first one, you put Tau equal to 0, and in the other one you
put Tau equal to 0.1. We are seriously
running out of paper. Let me see. Good. Think about time. We know where we start and we know where
we are going to end. In some sense, what we want to know is how to compute
this trajectory. What we're going to
do is the following. We are going to guess
a sequence of capital. I'm going to say capital
at period 1, period 2, period 3, until
capital in period 20. The good thing is, once
I know the capital, well, my guess,
computing the interests, the wage is trivial. It's just the marginal
productivity of labor. The capital is given,
labor is constant. The interest rate,
again the same. I know what the tax, this is a total capital, the interest rate, the tax rate, this is the lump sum transfer. As an agent, I know everything
that I need to know. I can compute my value
function in period 1 because I know what the interest rate is
going to be in period 1, I know what the wage is
going to be in period 1, but I also know what is
going to be in Period 2. When I'm in period 2, I know what is going
to be in period 2, but I also know how it's
going to be in period 3 and so on and so forth. I can compute the whole
path of value functions. It's actually even easier
because I can do it backward. Since I know what V of t is, it's very easy to compute
what V of t minus 1 is. It's just I go to the
Bellman equation, I know what V_t is. I just solve for the session
today that gives me V_t minus 1 and I do that
backward until period 1. Then what I can do
is I say, well, given those decision rules of the agents in this 20 periods, in each period there
is going to be some decision of
accumulation of capital. Given that accumulation
of capital, I can look at the
sequence that I guess of capital and the sequence
that I computed of capital. I can ask myself, did I clear the markets or not? Of course this is a big pain
because you need to clear, imagine that capital T is 20 in some sense you need
to clear 20 markets. The way you will probably
like to do this in real life is with some type of
Gauss-Seidel algorithm. The idea will be, so
you have 20 periods. You will try to clear
the market at period 1. Then you will move to
period 2 and change the capital in period 2 and so on and so forth until time t, until time 20, then start again. Until you have done
this sufficient number of times, that it converges. You don't want to change
the whole case at the same time because it will be a little
bit too difficult. You do it period by period, but then you cycle
again and again. The evolution of
capital over time, that will give you the value functions and we
have computed our transition, so we can start asking
relevant questions. What happens when we
introduce this capital tax? Not only I will know the
end, where we get to, but I will know the whole
transition to that. As I was saying before, this is particularly
important for welfare because you
really want to take account of the transition for
your welfare computations. Let me show you an example
to illustrate this. We are in time 0. What is my value function? What is the expected utility
flow I'll get in my life? Remember this introduction of the policy is a surprise thing. It is a zero probability event
that you never expected. While you say, I'm in the
stationary distribution, there is a Beta missing
over here, sorry. There should be a Beta to
the power of t over here. I'm in the stationary
distribution and given the stationary
distribution, I'm going to get some
consumption over time and that's going to
be my value function. V_0 is just my value function before the new policy
is introduced. Now, there is also
going to be a V_1, which is going to
be equal to V_0 and this g. Basically what is happening over here
is the following. B1 is the value
function that you get in period 1 after you
introduce the reform. You wake up in the morning, you open the newspaper
and you say,"Oh my God, they introduced this reform." You say, "Well, my new
value function is 22." I actually just computed
that in my iteration here. I was just doing that thing, so I computed that. Well, now I can ask you
the following question. How much money will
you be ready to pay to avoid that policy reform or how much money will
you be willing to pay to get that policy reform? Well, that's very simple. You just say, this is the
value that I get after the reform is going
to be equal to the value I'll will
get before the reform, and this g and basically
what this g is,t' is the compensation that
I will get period by period because of the reform, which can be positive or
which can be negative. Since it's constant
period by period, I can take it out of
this expectation. Again, there is a Beta
missing over here. But this thing over here, is just the value that I
had before the reform. What you have is that V_1, the value function
that you get in the first period right
after the reform, is the value function
you had before the reform times 1
plus Z_1 minus Sigma, where g is the
compensating differential. It's exactly, how much
money you will pay in consumption units to avoid waking up and reading
the newspaper, they introduce this
tax on capital. Then of course, I know this, we computed that from
our previous algorithm. I can just find g in
a very trivial way. That's the welfare
cost of the reform. The reason why this is
different from the welfare, the steady-state
welfare comparison is because in the welfare
state comparison, we will not use V_1, but V_t, which is the value function after the whole
transition has happened. Of course, this g and this
steady-state welfare, maybe very different objects. Now in the literature, you are going to find
that in many times people look at this number
instead of this number. The main reason is because computing the stationary
distribution under the new policy and under the old policy is
relatively straightforward. In some sense is solving
the model twice. Then you just use this
and you are done. But if you need to compute
the whole transition, that's really quite a bit of a pain because you need to go one after the other
period by period. People tend to be lazy and they don't want to compute this. They just compute this. The problem is that
this is really a very biased view of
what the reform is. As in the case of
Social Security, I was just showing you, it may give us a very bad
view of what the real issues are at hand. Questions? We learn how to think about models with heterogeneity when there is
no aggregate uncertainty. We thought about the
first move towards a more interesting situation where you have
these transitions. But this transition was an
unexpected shock happened once in a lifetime and we
never saw it ever again. But you can say, look, I really care, I really want to think
about models where there is aggregate uncertainty and
this is continuous over time. In the sense that happens every period they have an
aggregate shock. The typical example is
economic fluctuations, the business cycle, and their interaction with
idiosyncratic uncertainty. We really want to understand
how you can combine business cycle fluctuations
at the aggregate level with a lot of things that
happen at individual level. For example, my
favorite illustration of this is about job creation
and job destruction. We know that job creation
and job destruction behave very differently
over the business cycle. When we are at the
beginning of a downturn, there is a lot of
job destruction. There are a lot of factories close and a lot of
firms getting out. You are really
going to have this distribution of firms and jobs sending over time because we have an
aggregate shock. That really opens
the door to thinking about many super interesting
problems in macro. Problem is, of course, or the challenge is how to compute that equilibrium because now we need to keep track
of the distribution. The distribution as
a state variable. What we did before
was in some sense to get around that problem, just by imposing that we were
stationary distribution. Instead of keeping track
of the whole distribution, we only need to keep track of the interest rate and the weights that
distribution implied. But now every distribution is going to imply some
different interest rate, so different wages
and more importantly, a different
distribution tomorrow, and that makes this a
very difficult problem. As I was saying before, it basically gives us an infinitely dimensional
object which is very difficult to handle and we have very limited results
about existence, about uniqueness,
about the stability, about how good the
approximations are. I don't know, you
want me to continue. This is like when you go to
the doctor and he tells you, this is everything
that is wrong with you and I have three more pages. Really, our understanding
of this type of environment is very limited. Which is a pity because
we really think that this combination at least
my heart of hearts, I think that idiosyncratic
uncertainty, and aggregate uncertainty need to interact in an important way. Let me just do other tourists we are getting at
the end of the day. For one second we can at least just think
outside the box. There is a lot of
stories these days about the recovery have been very slow because people are really bad event by
very large debts. Households have very large debts and then they don't
want to consume and that has lower aggregate demand, blah, blah, blah,
blah. Okay, fine. Well, the core of this story is a story
about heterogeneity. There is a lot of people
with a lot of debt do not want to
consume and a lot of people with little
debt that do not want to consume either because
of the zero lower bound, the interest rate
is not sufficiently low to induce them
to consume today. If you really want
to think about this model and this
type of stories about balance sheet
recessions in a really serious
quantitative way, you will need to have a model
with heterogeneous agents, with aggregate uncertainty, or with a lot of
idiosyncratic uncertainty. I really think this
is a challenge, will be very interested
if someone could make progress on it and
having this story. I believe this story is
not that I am against it. I'm just saying, yes, I
believe it, but I don't know. I haven't really
seen quantitative models that tell me that my prayers are right and not just product of my
own prejudices. Let me show you this instead of in a very
general framework, in a very concrete example, what we're going to
have business cycles, and they are going to be the super boring business cycles, the most boring business
cycle you can imagine. The economy can either have
high or low productivity, so S_b is bad times
and S_g is good times. It's high or low productivity and there is a
transition probability. Sometimes we are high, we move too low, sometimes
we are low, we move to high. This is the most
boring, irrelevant, and absolutely an
interesting model ever of the business cycle. But it helps us to
understand what is going on. Also LED is going to be labor productivity and
labor productivity is also going to be
very uninteresting, is going to be only two
possible positions. You are either unemployed
or you are employed. Unemployed, you can
think about it as making single income or maybe some type of unemployment
in students, employed is you are
making some income. What is going to happen is that those probabilities are being correlated with the
business cycle. When you are in good times, there is going to be a lower probability
of being unemployed, and when you are in good times, there is going to be no,
there'll we are wrong. When you're in bad
times, you will have a higher probability
of being unemployed, and when you are in good times, you will have a higher
probability of being employed. Each agent is going to
face a transition matrix. This is going to be
their productivity tomorrow, whether
employer employed, this is going to be
the aggregate estate, which will depend on the aggregated state today
and the productivity today. This is going to be a
four-by-four matrix because there are four different combinations of things
that can happen. Again, by the law
of large numbers. I want to be sure that there is a consistency going
on over here. But if you have a 40 percent probability
of being unemployed, given that the states are good, 40 percent of people will be unemployed when
states are good, and this is just some algebra to ensure that these
probabilities are right. Over here, what I'm doing
is I'm integrating, summing over y primes, and over here I'm just doing
the base theorem to have the right conditional
distribution find, nothing very deep. Now, the individual, when we go to the
recursive formulation, the household will
need to keep track of the individualist
state variables, a and y, and the aggregate state
variables is under distribution. You can see over here how the value function will depend
on these four variables. This is a number,
this is a number, this is a number, but this
is a whole distribution. They are going to maximize, they are going to
choose consumption and asset given the
utility function. Given that tomorrow,
they will be in this value function
with this asset, this productivity,
this aggregated state, this aggregate distribution
and the probability of moving given the stage
today to this stage tomorrow, and we sum over all
different possibilities. What is the law of the
budget constraint? Consumption plus
assets tomorrow are the wages times y plus 1 plus
the interest rate times a, where all these things depend
on the aggregated states. Then of course we have
the law of motion for the aggregate distribution that says the distribution tomorrow, given the stage today and shop tomorrow
will be this thing. Because this is
something that is realized at the beginning
of next period. Questions about this? Very long definition of recursive competitive
equilibrium. We don't need to go
over there again. The interesting thing
is we will have a transition function
for the distribution, again, this is just notation. Let me not use much time on it. The problem is, as I was already advanced
in before that we really need to think about how to forecast that
distribution tomorrow. Because forecasting the
distribution tomorrow will tell us what are going
to be the future prices. But the problem is
that I need to know every single person and the situation of every single
person in this economy. Because, for example, if a prime where linear in a, if I will always save the same amount or the same proportion
regardless of my assets, I wouldn't need that because I will just
be able to say, well, everyone is saving 20
percent of their income, so I just look at the assets of everyone or the
income of everyone, I just compute 20
per cent of that. The problem is, for example, if you are very poor, you have very lowest sets, it may be the case
that you will save very little but if you
have a loss of assets, you may save a lot in proportion and not in absolute
levels, in proportion. If that case is true, if you don't have the same slope for all different things, then it will be the case
that you really need to know where everyone is in
the distribution today, and that makes this
really difficult. Per Krusell and Anthony Smith, I think that what I already
like 15, 16 years ago, came out with a very smart idea, which is the following. Instead of keeping track
of the whole distribution, which is something
extremely complicated, just keep track of a few
moments of the distribution. For example, the mean and
the standard deviation. Yes, this is not really
the whole distribution, but if the moments
are informative enough about the whole
shape of the distribution, this may be good enough. You see the idea? Let's think about m as the vector with the first n moments
of the asset distribution. What is going to happen
is that instead of having the exact law of motion
for the distribution, that is impossible or nearly
impossible to keep track of what ANS are just
going to have is a law of motion for this end moments. There are two
interpretations for this type of trick. One is that agents are
boundedly rational. Ourselves cannot keep track of the whole distribution
of agents in the US economy so we just
keep track of a few moments. Another interpretation of this is as a pure
computational trick. I think I'm a little bit more convinced by
the agents being boundedly rational seems
to me a little bit more coherent with the spirit
of the algorithm. Then, so the first thing is
the idea we are going just to keep track of some moments instead of the
whole distribution. The second thing that
we're going to do is we're going to parameterize. We're just literally to guess one functional form for
this law of motion. What Krusell and
Smith suggest is to have just this linear function
in the log of capital. They say, let's forget about all the moments
except the first one. The only thing we care about is the average amount of capital
and the average amount of capital tomorrow is going to be a linear function of
the log capital today. Where this A and this Bs are index by the aggregated state. They would be different. They need to let aggregate
states enter into some way so they
are going to let this beam either big or low. This will be the
household problem, which is the same as before, except that now, instead of having the whole
distribution over here, we have capital and capital tomorrow and instead of having
the whole law of motion, we just have this log evolution, law of motion for
the log capital. Given AS and BS, This is an extremely simple
value function problem. You can solve it
using any type of method that you are familiar with to solve dynamic
programming problems. For example, the
production methods we were talking
before, perturbation. Again, the only thing that is different from the
aggregate from a very plain vanilla problem is you need to keep track of this case and you know that the evolution of K
is given by this thing, which is exogenous
to you by the way, which makes it even simpler. You are still only maximizing over-consumption and assets. How do you guess or
how do you find this, of course works when you have
AS and BS. What do you do? Well, you guys remember that we have high and low so you need to guess two As and two Bs. You guess those four
numbers and you solve the problem of the
agents in the economy, then you simulate for a very
large number of periods, for a very large
number of households. Okay, so you say, let me
take one million households, let me give each of them the decision rules I
just solve and I'm going to get them operate
over 100 years and I will look at how the
aggregate capital in this economy will
work over time. Then what I'm going to do is I'm going to regress the
capital tomorrow, imperious in the next period on capital today, of course, conditioning on whether
or not we were in good or bad times
and I'm going to look at my estimates
of AS and BS. If the R-square for this regression is
high and in addition, the estimates Alpha and
Beta are very close to my As and Bs that I
guess before, I'm done. This is in some sense
a guess and verify. You say, I guess, that the law of motion for
capital gets determined by A and B. I'm just going
to simulate the economy. I'm going to run that regression
in the simulated data. I'm going to get
some Alpha and Beta if they are the same
that A and B, I'm done. If not, I will need to change and update those
guesses and it's absolutely not very easy to do that update because there's not a clear
rule of how to do it. It may be the case though, that the guesses
for A and B have converged but that the
R-squared is still low. We still do not have
a very good fit. Then what Krusell and
Smith suggest is that we increase the number of moments that we need
to keep track off. Questions about that Krusell
and Smith algorithm? Okay, so just let
me show you quickly a calibration of how
this thing will work. The period is going to be one-quarter constant
relative risk aversion utility, log utility. Just to simplify your life, discount factor is going
to be 0.99 Alpha 0.56. The discount factor, the depreciation is going to
be 9.6 aggregate and annual level is 0.25 at the
quarterly level. They are going to be
two aggregate estates. Remember, this is a
super boring model of the business cycle. It can be aggregate
productivity, it can be either 0.99 or 1.01, so very low volatility, just a little bit higher, a little bit low and it's going to be asymmetric
transition matrix. Again, this is probably not a good representation of
the data because remember this morning I was
telling you that expansions are long and
smooth and recessions are sharp and short. I guess we could do that. They are very easy to
introducing that theme but let me keep
things simple here. The way we are going
to say is, well, let's suppose that the
average recession lasts for eight quarters so that basically means that in
my transition matrix, remember the diagonal is the probability of a
stain where we are. Now, this will be seven-eighths and
the probability of getting out is one eighth. Okay, this is just an
exponential distribution. If I know that I'm going to
be eight periods on average, it means that in
every period I have one eighth probability
of getting out. Idiosyncratic component, okay, so remember we have
these two estates, employment and unemployment. If you are employed you make one, you have one unit. Well, it's not that you make one is that you have one unit of time and get one weight. If you are unemployed, you only get 25 percent. I think about this as you
know, unemployment in Sudan, maybe some moonlighting,
some jobs here and there, or maybe it's household
production by the way. If you are unemployed,
you may take the time to do some gardening. Now I need to specify
the transition matrices. Instead of a specified
a four-by-four matrix, I'm going to specify for two-by-two matrices, it's
a little bit easier. I'm going to do the following, I'm going to assume that
if we are in an expansion, you are going to be unemployed
1.5 quarters, six weeks. Which means that
the probability of being unemployed if you are
unemployed today is 1/3, and probability of being
employed if you are unemployed today is 2/3. You get out of
unemployment very fast. If it is in recession, is worse, is 2.5. That will give me 0.6 and 0.4. The other thing I need
to specify is what will happen with these probabilities where not only I'm switching, but also the aggregate
economy switching. Over here the probability of a state unemployment was 1/3 when the economy is
staying in a expansion, and over here it was 0.6 when the economy is staying
in a recession. But what happens when the
economy goes from an expansion to a recession or from a
recession to an expansion? Well, I'm going to assume that the probability of
remaining unemployed is 1.25 percent higher. When you go from good
times to bad times, the other way around
is 0.75 percent higher when you go from
bad times to good times. This is basically saying, worst moment to get unemployed when the economy
is getting into a recession. Best time to get a job, when the economy is getting
out of a recession. This captures the intuitive idea that when we are getting
into a recession, we are closing down
a lot of factories, a lot of people is being fired. When we're already
in the recession not that many people is fired, but you don't get
jobs that easily, and when we are getting out of the recession is when we are
opening a lot of factories, so it's very easy to get a job. We put all these
things together. I'm fine. You get all these different entries
of the probability matrix. Just I'm not that interesting
to discuss each of them. The model, we put
this in the computer. Since we don't have
that much time I didn't really bring
code to show you this, but it's actually not that deep. By the way, Tony Smith has all the code of this
stuff on his web page. If you go to his web
page at the URL, you can download the code. If you put Krusell and
Smith code on Google, you will find 20
different people that have versions of this. Then you are going to get the
aggregate laws of motion. You are going to get the
individual decision rules, and the time-varying cross-sectional
wealth distributions. Just to say a few more things, this will be the
converged laws of motion. In particular, what times
are good this will be like, this coefficient will be 0.962. When times are bad,
it will be 0.965. You can see basically in the intercept that you will
save a little bit more in good times than in
bad times because basically you want to do intertemporal
consumption smoothing. You want to save more
when income is high. We can ask the question, how irrational are agents? They are doing this
bounded rational thing we can compute the R-squared. The R-squared of this thing
is going to be 999,998. This is a very, very
good approximation of the law of motion that
we are having over there. I don't want to get
into the discussion. The problem about this thing is that it is not obvious
that a very very high R-squared can be such a good informative
statistic of goodness of fit. [inaudible] has examples where
extremely high R-squares actually imply huge
utility losses for the agents and
where the equilibrium, the exact equilibrium is
actually very far away from the approximated one. The R-squared is probably not
a terribly good statistic, but it's the one that
most people use. Well, when you look
at these two graphs, the main lesson
that you get away, and this is a little
bit of a disappointment of Krusell and Smith. It's a result that has worried people since
it first come out, is that you don't see
a lot of facts in between bad and good times. The coefficient, they are
a little bit different, but it's pretty much the same. Second, it doesn't seem that heterogeneity matter that much because we are doing
a very good job with just one moment. I set up this whole
heterogenous Eigen model class telling you about how
exciting these models are. Then at the end of the day doesn't really seem
to matter much. That's very disappointing. People have really tried to
think carefully about how to get around that because
there is really the prior, I think is still there, that this interaction between individual and aggregate
uncertainty matter a lot. The reason is very, very simple and it's
basically because, yes, it is true that not everyone has the same marginal propensity to save but at the end of the day, except the guys who
are extremely poor, every one else is going to have nearly the same marginal
propensity to save. The guys who are extremely poor, who cares about them because
they don't save anything. Remember in the US economy, the bottom 80 percent of the
population have nothing. The fact that they have very different money or
propensities to consume that mean
I own quite a bit. Absolutely irrelevant
because it doesn't matter. In some sense, you only
need to think about maybe the top 20 percent
of the distribution. In this type of environment, this top 20 percent
of the distribution, narrowly everyone
is going to have the same marginal
propensity to consume. You have this thing that is
called quasi-aggregation. As I was saying, this is a
little bit of a letdown, but I still believe
that there are many other situations where heterogeneity with
things like age, with things like entry, exit
into your market, entry, exit of firms, etc, will deliver very
interesting results. It doesn't seem the case that we have put our finger on them yet. 