foreign what I want to do is talk first a bit about multinomial and conditional loaded models uh essentially and so this is literature that goes back to the early 70s when uh McFadden was working on these models when it's helping uh the design of the mass transit in in the Bay Area um and so he developed a whole slew of of multinomial choice models first I'll talk about some of the basic ones then I'll talk in some detail about the the problems with some of the the simple models in particular with the the independence of irrelevant alternatives um and then I'll talk about uh three uh generalizations of this of the conditional logic models that don't have the independence of a relevant alternative properties and then I'll talk a little bit about a set of methods that has become very popular in industrial organization associated with the work by Barry Levinson and Pecos and I'll talk a little bit about models with multiple and observed Choice characteristics and hedonic models and so a lot a lot of the the empirical literature in the last decade or so here has been in the industrial organization which is not generally my my area of expertise so I hope you bear with me here um but nevertheless I'll try to give some comments and and try to put some of these things in perspective um so so I'll start with some of the the multinomial choice models uh that felon developed in the early 70s and I'll talk at length sort of about the issues with those models they're independent of the irrelevant Alternatives property and why that's that's particularly unappealing in some of the settings that have been considered in industrial organization where people have looked at these models in settings with with relatively large number of of choices I mean some of McFadden's work just looked at a cases where there were very few choices so he was looking at the the design of Bart and so people he was interested in whether people would commute to work by car by or by mass transit or in various forms so they were relatively few there were more than two choices but they were relatively few uh choices there's another paper using some of these models looking at the demand for for dryas where the the choices were not buying a dryer buying an electric dryer buying a gas dryer but it was almost all the the applications he was focused on has very few choices in contrast in uh in these industrial organizations where people look at the demand for different shaded products this is large number of choices uh two of the the now now the most prominent examples involve people buying cars so there's a huge number of choices there you can kind of narrow it down a little bit but in the end uh there's there's going to be on the order of uh of 50 or 100 uh different choices and the the problems with with some of the the properties of of these early models become much darker in in those settings and so when I look at three in the alternatives to the to multinomial logic models to conditional logic models they don't have this IIA property uh nested loaded models um sort of relatively unrestricted multinomial probate type models and random effects models and so I'll I'll argue that that the the random effects approach is a very attractive way of uh of dealing with with violations of with with concern about violations uh about independence of irrelevant Alternatives that either nested loaded or or these unrestricted multinomial probate models and then the second part of lecture I'll talk about this work by Barry Levinson and Pecos where they now use these random effects models but they generalize this in a in a number of ways uh in particular they allowed for for they developed a very specific model that uh that you could actually estimate using only aggregate Choice data in the in the case of uh demand for for different shaded products there would be market shares for this uh products in different markets in addition they allowed for for endogenous Choice characteristics as well as an observed Choice characteristics and so we'll look at some detail at their model and sort of how you would go about estimating that as well as sort of what what I find the the less attractive features of of those models in which way you may want to yeah do things differently from from the way they did it in their original 96 paper um so the the whole lecture is about multinomial choice but there's there's more than than two choices um so the the outcome is some um you know here but why it takes on why I take some non-negative unordered integer values between zero and J so this J plus one choices and so the classical examples are the the commuting choices uh that McFadden looked at last train or car um in labor it could be employment status people could be employed unemployed or out of the labor force and consider that I O the classical IO examples there's car choices where you can buy a whole different set of models as well as different brands and uh as well and various versions of of these things and so uh you know to to explain the the distribution of uh of these choices we're going to have two different types of uh of covariance available one set will be individual specific variables things associated with the the decision maker with the individual making the the choice uh denoted here by by CI and so those are things it's like AIDS income education those are fixed characteristics of the individual uh and the the important part of the being fixed is that they don't vary by choice uh no in addition we're going to have some variables that vary by choice uh and so these are these are characteristics of uh of the choice they may also vary by individual uh that's going to be allowed for it that's not going to be essential the key thing is that they vary by uh by choice and so one of the most important ones in in this literature on different on demand for different shaded products would be the price of uh of these choices which typically doesn't actually vary by the individual uh um but in some of the original McFadden examples if you think about commuting Choice a characteristic of a choice would be how the the price for a particular individual to commute to work by by car or By by Train and certainly there both the the actual the cost in terms the monetary cost would could vary by individual as well as by choice but in addition other aspects of a choice could vary by by the individual as well as by the the choice then with the commuting time and so even in the demand for differentiated products it could be that that there is some price variation by by individual by certainly by by groups of individuals so in general we're going to have this uh discoveries XI day for which it's essential that they vary by by choice so in general the choice will refer to them as Choice characteristics but they may also vary by uh by individual and so the um the starting point sort of there and this is in some sense a very direct generalization of just a simple uh binary loaded suppose we only have individual specific covariants um a natural way of modeling the choice would be to generalize the binary logit uh in in to this multinomial logic but you make the probability of uh of choice today a function of these individual variants uh in in this particular form it's for the probability for Choice day is e to the power C Prime uh gamma J over this divided by the sum of uh there's the e to the power C at the prime gamma l but you sum of all the other choices and so here the gamma Jace a choice specific parameters and the disease individual specific choice invariant covaries and so this works perfectly well if you generalize this by allowing the functions of of c in there then no matter what the actual uh joint distribution of of the data is no matter what these probabilities actually look like you can always approximate them arbitrarily well by some uh function of of this form where you may need in addition to disease entering linearly you may need to you need many higher order terms but in principle there's nothing very restrictive about the setup if if all you have is a is a set of of covariates that varies at the individual level then you could approximate things uh arbitrarily well using this this model and so in some sense this generalizes very easily to the case where you have Choice specific covers you could just Define the the sea to be the Stacked version of all these uh this Choice characteristics and so you could oh you can always model the the choice probabilities as some general function of all the the choice an individual characteristics and that would that would fit very that would in the end fit very well it wouldn't be a very attractive way to go it wouldn't be very easy to interpret the parameters but the point is that the dysfunctional form isn't really uh all that restrictive but it doesn't tie in very nicely to to economic theory if if you do this with disease consisting of these stacked versions of the the choice specific coverage and so um the second version of these models that McFadden developed it's a conditional loaded model uh it's a simple case suppose that we only have Choice specific coverage so we have we have you know capital J plus one factors of of covariates so think of this uh just prices for the the different choices as well as say quality of of these different choices then the implication of the conditional loading model is that the probability of of choice of the choice being of choice J is e to the power x i j Prime beta divided by the sum of all choices of of that expression and now the xia is a choice specific but the beta is a common uh it's common to all choices and sort of like the the multinomial loaded model this model is very easy to estimate it has a very well behaved likelihood function uh strictly concave it it works uh it's it's extremely easy to uh to implement now um the um first comment I want to make here relating these two models essentially sort of dismissing the multinomial loaded model uh from further discussion is that we can just view the multinomial loaded model as a special case of this we could just Define the choice specific characteristics in the following way where for for the first choice we put in the the individual characteristics in the first part of the the the fact of characteristics and put in zeros everywhere else do it for all the other choices and now even if we start off with a multinomial loaded model we can in the end write it as a conditional loading model so for any of the statistical issues we can we can just only focus on the conditional loading model and always think of the the choice characteristic potentially consisting of of individual characteristics interacted with the dummy for each of uh of the choices foreign comment I said that that's more interesting and so that that sort of gives much more motivation for thinking about the the conditional loading model this relates to the way McFadden developed these uh these models originally he said he was very interested in making sure that the the choice model was was consistent with individuals uh maximizing utility and so what he uh the point where he started from was to think of the utility Associated uh with a particular individual associated with a particular choice so he modeled that as as linear in this uh in these Choice characteristics x i j with at this point a common coefficient beta plus some idiosyncratic error term and then given given all these utilities for this uh this J plus one choices the individual individual eye chooses uh option a mixed Choice day if their choice gives the highest level of utility if that exceeds the utility for all the other choices Now sort of uh natural thing from certainly from the economics at that in in the 70s might have been to use the normal distribution for these Epsilon ideas that would make the choice probabilities very complicated functions of um of the parameters even if these absolute ideas were independent because now you would be looking at the maximum of of these utilities which essentially is going to involve other statistics of these normal distributions that doesn't work very well and instead it works much better if we use a extreme value distribution which is sort of a slightly funny distribution uh it's not symmetric uh around zero it does have a mode at zero but it the mean is positive uh it's it's slightly skewed to the right but if you use the distribution you get a thing simplify enormously and you get uh the implication is that the the choice y follows the the conditional loading model which ends up being extremely easy to estimate relative to uh this type of model with with normal errors so um so this is all is all very well um and so the so up to here these models are extremely easy to uh to estimate which obviously was very important uh at the time that McFadden was uh was working with these uh these models but it's also some concerns with these uh these models uh also pointed out at the time by by McFadden and so the key issue with this model is this independence of irrelevant uh Alternatives so I spent a little bit of time on that so if if you look at the conditional probability so given that we have J plus one choices if you look at a conditional probability that someone makes Choice J given that I tell you the making Choice uh either Choice J or l you can just figure it out from the The Joint probability of of all these choices uh and that that turns out to be a very simple function uh very similar to the the unconditional probability yeah but depending only on the and that that's the IIA property it depends only on the characteristics of the choices you're conditioning on J L in this case and not on any other choices of uh not on the the characteristics of any other Alternatives m so at some level that's sort of nah seems fine if uh I mean it's sort of partly coming from to one one simple way of of changing that that property would just be to uh I was part of that property comes from the fact that the utility for Choice J only depends on the the characteristics of choice day but that seems that seems very reasonable it doesn't there doesn't seem to be any reason why the utility for Choice J to depend on characteristics of any other any other Goods so at some level this may seem like a very reasonable uh property trouble though is that it if we take this model more seriously if you take it if we interpret this this really as a as a structural causal behavioral model that tells us what choices people would make if if we change the characteristics of the products you get into very awkward uh you find you get very awkward implications of of this uh this property so um let me sort of elaborate on that a little bit um and so what I want to show is that uh that these models the conditional loaded model even though it may fit very well as I said before the same way the multinomial logic model fits can be made to fit arbitrarily well it isn't necessarily very attractive as a structural model because it it generates very unrealistic implications so the example here is one for my my Berkeley days and so apparently it's a tribute to McFadden there so here the the choice that is three restaurants which is where the favorite of mcference and where if you go at the right time of the year at least it used to be that you would get uh figs from uh mcferran's Farm and so and then then there's the third place where the other econometricians would go but um it's um which wasn't quite as bad as the name suggested but it um it wasn't very McFadden we'd go so um so we have these three choices Japanese which I guess is well known enough which is also a very good restaurant and The Bongo Burger so say penis and lalims are very expensive bongobur was very cheap but it also was reflected in the in the quality of uh of this uh these three places so um here uh so I put in some numbers that was I put in some parameters for the utility function that we generate market shares for say penis of 10 for our leaves of 25 in The Bongo Burger of 65 percent uh and so at this point the data would be perfectly consistent with this model uh and just from this from this data there's nothing you could see that uh there is independence of irrelevant Alternatives uh would not be uh be satisfied the problem is if you think about the the thought experiment I think about the predictions of what would happen if you uh if you change the characteristics of one of the restaurants and so the the natural thing to think about is what would happen if if one if the price at one of these restaurants changed and said let's take this look at an extreme version of this maybe just take one of these restaurants out of business raising the the price to uh to Infinity yeah but the conditional login model predicts is that the the market share for the names would get divided by the two remaining restaurants proportional to their original market share and then um the implication is given that the market share for The Bongo Burger was much lighter than that for for Japanese most of the the people would have gone to La lame and I was still in business if it goes out of business would now go to the to the Bongo Burger rather than than say penis it just seems very implausible uh and see what you would expect well well I think the natural thing to expect would be that the the people we went to LA Lim's if that option was no longer available we'd go to a place that is similar in the end it would seem that these people would be much more likely to go to a similar restaurant than go to a restaurant that is very different even though that latter restaurant had a much bigger market share to begin with and so um so the the implication is uh that the um that if we want to use these models if you want to take these models seriously if you want to use them for predicting the effects of of policy changes of of changing the the characteristics and that is that is obviously in the end what makes these models most interesting just describing uh the the patterns in the data isn't all that they're relevant going back to what McFadden was interested in in the in the 70s he was trying to predict how many people would actually go by train and the various scenarios uh we want to we need to be able to take these models much more seriously and at that point the this IIA property is extremely unattractive uh one and so we need to think about what drives that property we need to think about how we can can generalize that and so one way to think about it is to go back to the the latent utility setup where we specify the utility uh of uh of choice day for individual is x i j Prime beta plus Epsilon ID and in order to make distractible uh the the conditional loading model assumed independence of this extreme value error terms Epsilon ID now it's really the independence what uh that drives this IIA property that's not completely right you don't get exactly independence of irrelevant Alternatives if you use the normal errors but the deviations of uh of that property you would get are not really large enough to to make these models uh more attractive than than the conditional loaded model and so it's really somehow this independence of of these era terms and think about it substantively why why do we think uh individuals we would have gone we had a preference for lalims would have gone to Japanese was was out of business we think that that the utility for those individuals that say penis would be much higher than what it would have been at the Bongo Burger we apparently we would have thought that the Epsilon IJ for for say penis would have been very similar to the Epsilon idea for uh for our limbs so somehow we need to allow for some correlation between these uh these Epsilon ideas and so um the next in a couple of minutes I want to talk about the three ways that uh of relaxing that that assumption and so um so there's three things I want to talk about it it's uh it's the nested loaded model where the researcher at priori groups together sets of choices that are viewed as uh as similar implicitly that's going to allow for for non-zero correlation between the unobserved components of those choices within within this uh sets within this nests and at the same time it maintains zero correlation between the unobserved components across nests so in terms of of that resonant example the idea would be to group a priori La Limbs and say penis together as sort of similar choices and think of individuals as making a choice first between the the two sets either going to uh to a cheap restaurant or going to an expensive restaurant and condition on that choice choosing between the the two expensive restaurants if if the choice is to go to an expensive restaurant um and so I'll give them a little more detail that in a in a minute a second approach is to just free up that um covariance Matrix for these Epsilon ideas directly they're slightly more Awkward to do in the extreme value case uh and so when when people started looking at those models instead of using extreme value errors they used these normal IRS in the end so this is sort of more of an aside in the end the choice between the between extreme value and uh Eris and and normal errors there is and and and should be completely driven by by convenience that there's not really anything particular to uh to choose between those uh substantively I remember a long time ago seeing it actually said let me make two comments there now I think there's a paper by David Cox um in the statistics at some point doing some showing that even in the binary case we need thousands of observations before you could you could tell um whether data came from a probate model or a loaded model because these things are just far too close uh same time when I when I teach this this stuff sort of often I've asked the students how should we choose between sort of just in the binary case how we should we choose between uh loaded or or probit models and the remarkable thing is that in most cases the the dominant answers that we we should uh look through economic theory to uh to to motivate that that choice and I'm I'm all for I mean I think here this discrete Choice literature is a very nice example of how economic theory can be extremely useful but the idea that that that theory would tell us that the unobserved components would be extreme value rather than than normal seems and uh seems surprising now but so so I mean now so so here I think that that is just another completely irrelevant uh concern and um the fact that the pro that the normal errors make it much easier to to uh to think about the correlation structure seems very sensible reason for for looking at multinomial probate sort of I'm respected montenom your probate models then I try a very convoluted way of of having an unrestricted covariance Matrix for for extreme value errors that separate from the nested loading model which is which is obviously a very elegant way of uh of allowing for the structure but it's a very different one where you impose a lot of structure on that that correlation the the unrestricted one would be very hard to uh to do in the with loaded errors um so I'll talk about uh the the unrestricted multinomial probably model for a while and a third way of of doing that is the the mix of random coefficient loads it model and there the idea is that going back to the utility representation having the utility for Choice day for individual I depend on the coverts for that choice interacted through multiplied by a set of coefficients that varies at the individual level and so here the the using this utility maximization maximizing Frameworks very useful these betas would vary at the individual level but it would have very clear interpretations it's just the marginal utilities and there's no particular reason why they should be the same for for individuals and in the end the the and this is so partly the reason I now I find that a very attractive way of generalizing the conditional uh loaded model it sort of seems very to fit in very naturally with the reason why you would think that these individuals we went to LA Lim's would have gone to uh to shape penis if that had not been available by the by revealing their preference for for La limps they reveal that they're relatively insensitive to price and have have strong preferences for Quality compared to individuals we went to The Bongo Burger so um no let me now move on to the um and give a little more detail about the nested loading model so here the idea is to uh to take the full set of choices uh partition that set into uh capital S subsets so that each choice is in one of these subsets within a choice with it sort of within a set within a nest and this nested loaded terminology the choice follows a conditional logic model with scaled set of parameters uh so instead of just with beta it's with beta divided by an additional parameter rho sub s where this rho sub s it's going to to essentially give us the correlation within the nest you can have that the common within each NASA you can have that beness specific here in this notation it's it's a nest specific correlation and then the probability of a particular Nest turns out to be to have a remarkably simple form still that also has a essentially has a conditional loaded form and so um if you just fix all the the rows at one we're back in the the conditional loaded model but if we allow it's slightly awkward that the row equal to one corresponds to uh to zero correlation between the the unobserved components and rho is a Serial corresponds approximately to perfect correlation um but that that's sort of the terminology yeah that's the notation of which used uh originally in the literature and so the another way of thinking about this is that what this uh this setup does is still allow for these uh these marginal extreme value uh errors Epsilon ID but instead of having them independent it has some uh structure on it it has some non-zero correlation allows for non-zero correlations with a particular structure on it that keeps the absolute IDs independent if they're indifference nests but it gives them a common correlation common non-negative correlation if they're within the the same nest and so the um to capture the sort of to go back to the example the way you would uh capture that example is to have two nests one of the expensive restaurants and one of the the cheap restaurants and individuals would first choose between the the expansive or cheap and then choose one of the the expensive uh restaurants now it turns out the sort of the conditional logic was extremely easy to estimate the likelihood function was strictly concave it was all very straightforward here the likelihood function is a mess it's I think it has multiple modes I don't actually think I've seen any paper that well people may have directly maximized the likelihood function certainly when McFadden was doing this this really wasn't uh feasible but it turns out you can exploit this uh this repeated conditional logic uh structure and so you can first estimate the scaled coefficients within each Nest by only using the choices within that nest and estimate beta over uh row using a conditional uh the loaded model then you can estimate the rows using the probability of a particular choice by by calculating what is known as these inclusive values so this looks considerably more complicated than than it is in the end this is twice using conditional logit and that gives you consistent wooden sort of really inconsistent asymptotically normal random estimators and so now there's also results on The covariance Matrix here but this this is a perfectly regular estimator you can in principle just bootstrap this and I would uh that would work fine because this would be very fast now um the so one one extensive this example was just for for two uh nests you can actually do this uh sort of much more generally and sort of here it's a figure from a paper by uh Penny Goldberg which is probably the most impressive example of the of this setup that I've seen but you see models that individuals choosing cars so there's as I mentioned before in the end there's a very large number of choices here you couldn't no well it's a very large number of choices here it's simple conditional loaded model would be extremely unattractive you clearly wouldn't think that individuals would choose a particular foreign luxury car would go to completely different type of car if that car got taken off of the market and so the way Penny Goldberg tries to get around the independence of a relevant alternative property is to have this whole sequence of nests where individuals first choose between not buying a car buying a car used or old actually used a new car then says a number of of classes of of cars than individuals choose between foreign or domestic cars then they choose a particular model of car and so now somebody remarkably detailed nesting structure but the same the same principles pretty much apply you can estimate these parameters you can so what does in the end allows for is having correlations at the lowest level for all the the models for all the cars for all the choices within the within a particular model it has additional correlation for for choices Within These higher level nests and so in the end this implies a very nah flexible structure on the The covariance Matrix of of these Epsilon ideas but still with an enormous amount of suction there is sort of there is a number of correlations that are that are restricted to be zero and lots of other correlations that are restricted to be the same but given that in principle sort of save it 100 choices there's a 100 by 100 dimensional covariance matrix it's clear that you need a lot of structure and and this is one way of of providing that that structure the um the key the concern really is is that things may be sensitive to the the choice of the nest the nesting structure and uh and at the same time this nesting structure has to be uh chosen uh at priori so even though the data are going to tell you is how much correlation there is between choices so back to the the the single Nest single layer of Nest the the the data and that will will allow you the Civil choose the value of of row will determine how much correlation there is between these choices but once you specify this Nest you're fixing a priority correlation between the the residual for the The Bongo Burger to be zero relative to the the residual for the for the other two restaurants and so um in in that particular example it's obviously a very natural uh nesting structure but in the in the Goldberg example it's clear that uh and even if the nesting structure she chose is a very reasonable one that would be others that would be uh that would be arguably reasonable as well and unless you try different uh different ones it's very very hard to see how sensitive uh things are to that this uh I think in this 1981 paper by McFadden in this uh um book on discrete Choice edited by by munchkin McFadden he uses this NASA loaded model in a much simpler case with just a few with relatively few choices and he actually estimates it for all different uh possibilities of uh of the nest or at least for a very large number of possibilities of the nest any you do find that that it can make some difference now it's sort of clear that some of the the nesting structures are less plausible than others but it's still somewhat awkward you have to choose those without uh and and then there's always going to be a concern that some other nursing structure may have lots of different results okay um moving on to the the next uh way of freeing up the the independence of relevant alternatives the property is to just directly free up the covariance Matrix of the error terms instead of using um no more errors we could just specify that the the factor of Epsilon I of the factor of apps on ideas for each individual has a normal distribution with a relatively unrestricted covariance Matrix um here are sort of a couple of issues one is that uh sort of with more than a couple of choices uh direct maximization to the direct calculation of the likelihood function is uh it's very difficult it involves uh High dimensional normal integrals uh you can't do that very accurately for the for more than two Dimensions there's been a huge literature on uh numerical methods for for Cal for doing this uh um so this the original work by McFadden and uh looking at simulation methods these have been much improved and fine-tuned for exactly for for this type of problem trying to calculate this High dimensional multinomial probate models those allow you to do this for much higher Dimensions I think sort of certainly up to uh to uh to 10. um so those I think those things work uh work fairly well um I think I think the sort of two leading uh ways of doing it now what is called the gay Wiki hasi facilio Keen simulator we sort of apparently simulates and partly calculates univariate normal integrals which which in itself is actually very fast um I think that it's still easier to do this from from Invasion perspective and it's a paper by the gay Wiki Keen I think and rankel and the review of economics and statistics right that actually Compares Gibb sampling here and this dhk simulate and they find that the Gib sampler sampling actually works uh considerably faster so there you would set it up by treating the the latent utilities as as an observed random variables as well simulate those in this one step and then draw from the posterior distribution of the parameters given the latent Utilities in the second step um and both of those things uh sort of combining those two steps the Gib samples is very straightforward and it uh it's it's more efficient than these these other methods and so there's a lot of it's a little detail for for some of these methods in in the work by various configurations of these authors of McCulloch Paulson and Rossi it's a book by Russian co-authors um and I think this is some of the stuff that the rosie has programmed up um that said I think see what what is the attraction of uh of this method uh compared to the to the National logic it's clearly attractive that now the resources doesn't have to commit our priori to where the the correlations are you can you can just in principle allow this covariance Matrix to be completely unrestricted other than some normalizations obviously it's a covariance matrix it's symmetric um even in the binary case you would in the binary case you would normalize the variance to one so there's still going to be some normalizations but in principle if there's a reasonable number of choices there's a large number of elements of this covariance Matrix and there's very little direct information on on these correlations remember that all we see is is what choice individuals actually make it's a very hard to see directly how much correlation there is between Epsilon I one and Epsilon I2 if you never get to see the Epsilon i1 and Epsilon I2 combined all you're going to see is is rankings that Epsilon I bonus is higher than than Epsilon I2 so you're going to need a lot of of data a lot of variation in in Choice characteristics in order to be able to estimate this uh to estimate this large number of covariance parameters very well so so in the end I don't think this is this is very attractive once once you've got more than than a couple of choices uh there's just no way you're going to be able to estimate all of these these accurately and so even though in principle you sort of you're safe against now you're not making all these restrictions you're going to end up with predictions about substitution patterns that that certainly would should incorporate realistic ones but that don't rule out a lot of values uh that that would have to be uh that would be fairly unrealistic and so in addition to still being competitionally uh feasible with with more than 20 or 30 choices uh even with with choices I think with four or five choices it's very hard to uh to get accurate uh inferences on on all these parameters and so I'll actually come back to that a little later when we're talking about an observed Choice characteristics because in some sense that that sort of explains that this multinomial probe with models without restrictions are far too flexible allowing for far more Journal substitution patterns than the data could possibly uh reveal okay um getting now to the the third alternative which is the the random effects models so what uh this approach does it allow for for the slope coefficients in the utility function to be individual specific and so in some sense that seems very natural in the example I gave before you could think that individuals who Express the preference uh for for the limbs now revealed by doing so that they're relatively priced and sensitive and they have relatively strong preferences for quality and that therefore they would be much more likely to go to chaperones rather than the Bongo Burger if uh if La limse was uh was closed so now how how can you capture that was supposed to be uh write the utility uh for Choice JS XI J Prime beta I we're now in the coefficient is indexed by the individual plus this Epsilon IJ which is still the same extreme value or or normal error independent across individuals as well as across choices then we can uh you can sort of rewrite that uh where you yeah replace beta eye by its average and the difference uh from its uh its average so you can write this as x i j Prime Veda Barbara beta virus the common uh part of the the marginal utility plus some disturbance but now even if the Epsilon days are independent the new ideas are not going to be uh uncorrelated across individuals because there's this component that depends on the the choice characteristics and these individual preferences and so now we're getting this covariance we're getting this correlation between uh between the unobserved components of the utility but it's not a it's a very structured correlation it's only choices that are very similar in in observed characteristics that are allowed to have they're allowed to be highly correlated so um so thinking back about the the nested loaded and the multinomial pro unrestricted multinomial Pro with and and this setup here in the nested loaded the researcher had to specify which choices we're going to uh to be highly correlated which were allowed to be correlated and then the data would tell you how much correlation actually was in the multinomial probate you could free that up completely and allow the data to tell you to to be determining uh all the the correlations here and the the random effects loaded you again imposing structure on the um on the correlation structure but the structure now is is coming from The observed characteristics of uh of the choices you allowing the the and so the data would still be consistent with no correlation but if there is any correlation it would have to come from uh from observed characteristics being relatively similar and so the idea would always be that if if a particular choice was not available if an individual no has to make a substitution they would substitute to uh they would be more likely than under the conditional logic to substitute to uh to choices that are very similar in terms of uh observed characteristics and in practice of course that is what you do in the nested loaded model as well other than you do it by hand in that case you group these choices together in a way that you think they're similar but typically that would be that would correspond to being similar in terms of observed characteristics already but you would do so by by picking essentially which which characteristics uh you're grouping them on now in terms of of implementing this uh sort of this different ways you can do this uh one way would be to uh um to use sort of a approach similar to that that use in duration models uh by Hackman and singer to think of the being a finite number of uh of types of individuals uh so in the rest of an example you could think of some individuals being uh they're very price sensitive and some having very low price sensitivity some individuals having strong preferences for quality or not but you could allow for a finite number of uh of types modeled the distribution of uh of types either as being independent of observed characteristics or or some function of absurd characteristics and then estimate these these models using sort of methods for finite mixtures these models would be relatively uh easy to estimate using the EM algorithm or sort of a host of other methods available for for uh for finite mixer models an alternative instead of this is more the way the i o literature has has gone is to model this uh the the types as following a continuous distribution and so given some observed characteristics you could uh model this as the beta ice as having a normal distribution with parameter with centered at CI Prime gamma and some covariance Matrix where we may still want to impose some restrictions on The covariance Matrix but where in principle you could you could allow for a fair amount of generality depending um on the the amount of data you've got so the choices between using these continuous mixes versus this uh this finite mixture distribution again is one I think uh should just slightly be driven by uh by convenience both both are actually fairly uh straightforward at least in principle to do and both both are fairly flexible so I don't think it's going to make a huge difference in in practice although I don't have any any I don't recall seeing any direct comparisons of of these uh um these two choices they still approaches okay um so now so having discussed these three uh alternatives to uh these three ways of uh of getting rid of the the independence of irrelevant Alternatives uh let me move on to the the very Levinson and Pecos and then approach uh I remember the days when at least in econometrics blp stood for best linear predictor that those days seem to have gone um so this has been a very influential uh approach in in IO and so what they do is they they take these random effects loaded models but instead of do a couple of additional things when they allow for unobserved product characteristics uh they also and this is obviously very important in these i o settings where one of the the key Choice characteristic that varies across choices as well as across markets this price they want to allow for endogenity of these Choice characteristics and also they they come up with a clever way that they're that implies that they only need aggregate Choice data so instead of having individual Choice data they can estimate their models using only aggregate data that's certainly extremely helpful at the same time this there's clearly huge cost to only have having aggregate Choice data in most cases having individual the level Choice data is going to make estimation of of all these models vastly more precise you know especially and especially if there's this variation in some of the choice characteristics at the individual level and so um these these models work sort of are also particularly well designed for dealing with uh with relatively large numbers of choices similar to the The Goldberg paper the original blp paper looks at choices for cars but there's there's a large number of uh of choices um so to set up there is uh again where we have a latent utility now that depends on the choice characteristics that are now indexed uh both by the choice and by the market um so um the idea is that we have we observe choices uh in in different markets where the characteristics of these choices yeah I think um so you hear the the in this blp approach the utilities indexed by individuals choices and markets but sort of in different markets the characteristics of a choice may change so in particular there is um very important for prices that that are allowed to to at some level you kind of wonder what a choice is if it changes its uh its characteristics but obviously for price you could still have have exactly the same object but have a different price in in different markets the the say that JT Harris an unobserved product characteristic and today it's important that they allow that to vary both my market and and by product and then there's absent on idat that has uh in has an extreme value distribution that sort of has this loaded form that's going to be independent across uh individuals products and uh and markets and I'll talk a little bit later about sort of the role of that uh why is extremely useful to uh to have that in there and then for the individual level coefficients uh they um yeah specify those to follow a normal distribution center that uh at CR Prime gamma plus plus the average value of the coefficients uh so just the same as in the in the random effect specification we used before and so here what are the the data that they're going to need they don't necessarily need individual level uh Choice data needs um market shares for all these choices but the proportions of these choices in each in each market they need observations from the the marginal distribution of the individual characteristics from from each market now then what do they do and so you can spend a couple of minutes sort of going through this they may now to at least give you some flavor of uh of the way this uh this works so you can think of the utility uh as uh consisting of two parts one the part that is uh choice and Market specific and then all the rest that is uh that is indexed by the individual but the by I and as independent of uh of the rest that is the center at uh at zero given in a particular market for a particular choice so the first part is this that was written here as Delta JT that consists of the systematic part of the choice characteristics beta prime x80 plus the unobserved Choice market specific component uh t and so the the Epsilon atheist is in the remaining part as well as the the random part of the individual preferences uh the CI Prime gamma plus Ada I for the CIS is CI is a normalized to a mean zero so now how do you go about uh estimating this it's supposed to be no all these parameters so we know Gamma Sigma and we know the the Delta JT's then we can figure out accident and then we can figure out what I would imply for the market share for a particular product how could you do that well if I tell you what Gamma Sigma and the Delta jts are you could just simulate for a bunch of individuals what a utility would be under each choice you can see which one is the highest utility you can figure out what choice individuals would make and you can figure out what what choice individuals would make on average you can figure out what the market share is in a particular market so if I tell you sort of both the gamma I tell you Sigma so you can draw the eight eyes I tell you what Delta JT is so you know the the systematic part of the utility you can just simulate the unobserved parts of the utility and you can figure out what the market shares for a particular choice in a particular Market and so the implication of that is that you can write the market share for a particular product as a function of these three objects Delta JT gamma and sigma in a very simple case if I actually fixed gamma at zero and sigma at zero the market share would be very simple would just be the ratio of this e to the power Delta JT's but no matter what the values are for gamma and uh and sigma you can you can in principle simulate this uh sort of be messy it would take it may take a while but it's sort of only computer time you could in principle it would be a straightforward way of estimating what the market share would be given the values of these parameters and so you get this implicit function uh of the market shares implied by values of Delta gamma and sigma then the um next step is to fix only the second the last two parts the gamma and the sigma and now look at that function and try figure out for what values of Delta you actually match the implied market share to The observed market share and sort of kind of discounting we have one exactly one Delta JT for each market share so you sort of have it equal the same number of parameters as equations you're trying to fix uh in fact from the structure of this this problem it's a very well defined problem you're always going to be able to find a set of Delta jts that fit exactly the market shares because in principle these these Delta JT's just tell you how attractive a systematic how this they tell you the systematic part of the attractiveness of a particular choice and if you make that go up you can make the market share go up and you do you can do that in a way that you balance all the the market shares sort of in uh blp this is yes a particular way of doing that but sort of in principle that's going to be a very straightforward problem to solve then the the last part and that that's that's really the part that's the most uh difficult now this is all for particular values of uh of gamma and sigma but we're actually trying to figure out what these values are and so what what we said so far is that irrespective of the values of gamma and sigma you can figure out a set of Deltas that fits the markets yes but now not only do we want to find a set of Deltas that fits the market share you want to find a set of Deltas such that the difference between Delta and beta Prime XT beta Prime xjt is uncorrelated with a bunch of other things and that's that's sort of why this is going to be really challenging and computationally this is very uh demanding but at least in principle now people have have used this a fair amount of and and gotten it to work now um that's it there's a whole bunch of references in there in the notes sort of about the discussions of uh of of how to make this uh this work uh some stuff I uh that's very readable and accessible uh um now let me just kind of as a final illustration of the spot supposed to be actually um we're in a case where we uh we had a conditional loaded model and we were doing this it's supposed to be actually fixed gamma and sigma at zero so we we didn't post the the load the conditional loads and conditions then what this is doing essentially is taking these markets as looking at the the relative different sort of the the difference in the logs for this uh this market shares and looking at the difference between that and beta Prime xdt and given some set of instruments you would essentially be doing uh two states this grass on these transformations of uh of the market shares and the thing that makes it much more complicated is that before doing so before doing that the the incidental variable spot you allow for these random coefficients uh and and this individual specific marginal utilities and that's the part that makes this uh makes this very challenging to do in uh in this with this aggregate data now going back a second uh them so with this specification alone if you actually had individual level data using give sampling and and this Markov chain Monte Carlo methods would actually be extremely straightforward I would still I mean it um it would be concept is straightforward it would be fairly easy to uh to uh to implement and it would likely work much faster than these numerical methods that use only aggregate data so even though it's obviously extremely useful to be able to do this only with aggregate data there clearly is a considerable price to pay in terms of of implementation as well as in terms of precision okay um and instead of this sort of partly relates to their next thing I want to talk a little bit about uh allowing for um for multiple unobserved choice characteristics so in the um because the Barry lavis and Pecos uses only aggregate data it's very important for them that it's just a single and observed Choice characteristic they need to feed it through this the single observation for each market namely the the market share if you actually have if you have individual level data and variation in the in the choice characteristics you can allow for for more than than one unobserved choice characteristic uh so there's been a bunch of papers doing that um including Albert and Keane and gatliner we actually allow for for more than to an observed Choice characteristics in some sense you can think think of this as uh of this unobserved Choice characteristics as freeing up the covariance Matrix in the the multinomial probability setting by having a choice having an unobserved Choice characteristic you're allowing for one additional free parameter in that um and that covariance Matrix but you're not immediately freeing the whole thing up completely and so and so here this is kind of described in in more detail in some of the work in some of this in this paper that it uh with Susan you can think another way of thinking about the multinomial probate model is that it implicitly allows for as many unobserved Choice characteristics as to our choices and so that is that's sort of at one level makes it obviously extremely flexible but on the other side it makes it it clearly frees up way more than you than the data could uh could ever tell you and and see one on the formal result in this paper with Susan is that in fact irrespective of what the data look like you can always explain the the choices in a way consistent with utility maximization by just having two unobserved product uh characteristics so so partly that is showing that uh having this completely unrestricted covariance Matrix relies very heavily on the functional form for identification and that that's in a non-paromatic sense you couldn't possibly identify that whole covariance Matrix now so the way we set things up in that paper sort of very similar to these random effects or these blp models we allow the the utility to depend on the choice characteristics uh this should obviously be a JT rather than it yeah um there's Choice specific characteristics then there's potentially up to two unobserved Choice characteristics uh say that day um as well as a idiosyncratic error epsilonid now once you allow for for more than one an observed Choice characteristic the likelihood function is going to be uh extremely uh awkward to deal with again it's certainly going to have multiple modes um it's uh it's going to be very difficult to uh to even evaluate it nah but this uh the basic numerical methods are very patient they're very convenient for this kind of thing and they allow you to do this uh sort of by breaking it up into a whole bunch of uh of essentially normal linear model steps it's relatively straightforward to do that okay so the final set of comments I want to make about hedonic models uh where um so recently a number of people have also looked at models that only now that right you sort of model the utility only as a function of the characteristics that's essentially getting rid of the the Epsilon ID T here which is some level is sort of a bit of a awkward uh component to have there sort of you'd think that the utility for a particular choice should in the end depend on the characteristics of their choice and should depend on on individual characteristics you may not observe all of these but there's sort of no reason to have a component that just varies randomly across all choices across individuals and so so in this literature people have tried to to um use only observed and unobserved choice characteristics to have a relatively small number of them uh instead of the sort of this multinomial probe at first thing where you have as many unobserved product characteristics as choices in the end and so to see if you could rationalize the data that way and and estimate these models and so um I want to make two arguments here about it why you actually do want to have this Epsilon idea in there if you want to take these these models uh to the data and so the main point is uh so there's two points to that um one I said not having these Epsilon ideas in there makes these models extremely sensitive to measurement error I suppose in fact these models that you have choice they that have generated by by one of these hedonic or pure characteristic models now then it's possible that it's one of the attractions of these models is that they can actually predict zero market share for some products if a product has characteristics that are dominated by some other choice if this if there is an Epsilon ID in there it would imply that there was still some market share for this Choice with dominated characteristics but without the um without this absolute night day the model could actually imply that a particular choice has has zero market share now suppose that that the data were generated by uh by this model uh but supposed to was one there was a single unit for whom we observed that choice due to some some measurement error then the the results would be extremely sensitive to that irrespective of how many observations you have the single mismatched observation would would force the parameters to be away from from the values that generated all the other values I said certified that's a very unusual characteristic typically you think of uh of econometric models that are well behaved that if there's some contamination that if some part of the data was not coming not generated by the by the same distribute joint distribution as the others if the fraction of of the data there were the the the fraction of contaminated data was small enough then you would you would get back to estimating the the parameters consistently here that would not be the case the irrespective of how many observations you have that were generated by this uh by the hennonic model the parameters would always be substantially away from uh from the two values even with a single uh mismatched variable and so clearly it would be nice to make these models more robust and you could kind of think of of just doing that directly we're saying well suppose that affected the the underlying Choice was based on uh on this pure characteristics model but there was some measurement error in the Nam you could do this sort of in a couple of different ways one uh way is to just say if there's measurement error you would get the one of one of the other choices uh you would see observe one of the other choices randomly chosen from uh from the overall set of choices alternative would be to uh to just add a at this Epsilon idea to the utility function and I would imply that you would get you would be more likely to see a close to Optimal choice if the the choice was mismeasured then one that is far away from from the optimal one see so the first argument is that by having the Epsilon Ida in there you would make you would essentially allow for measurement error in this uh in the pure characteristic model second part is then in the end the the model with the Epsilon Ida essentially nests the the pure characteristics model even though you fix the variance for the absolute ID by by having it be extreme value if the data were generated by the pure characteristics model you can rescale the systematic part of the utility function in a way that the choice probabilities from from the model with the Epsilon ID would be arbitrarily close to the the choice probabilities coming from the the pure characteristics model so to a point where that even if the data comes from a pure characteristics model you wouldn't lose much by having this uh this additional term because you would essentially be nesting that and the the big Advantage would be that you would make things much more robust that's going to be advantageous both computationally as well as as in the end for interpreting the parameters foreign this is it for for this lecture any uh any questions here unobserved product Ive um what happens if you have a new Goods that either has used existing Goods or you know the value yeah so and those yeah in these models with um let me say going back to the the conditional loaded models there the the implications for the interaction of new goods are typically very unattractive uh that you would uh potentially get very large market shares for for new products even if they were very close to existing ones uh these random effects models tend to generate much more realistic predictions for this these new Goods once you allow for the um for these models with either unobserved product characteristics but also these multinomial probate models you have to make a lot more choices for predict in order to predict the market share for for a new product and these models with unobserved Choice characteristics you need need to assume either postulate the values for unobserved Choice characteristics or you need to uh you can do this for different values and you can sort of get a whole range of of predictions but sort of once you allow for the possibility of the being an observed Choice characteristics it's clearly difficult to predict very precisely what the markets are for a new product is going to be but in some sense that's created very very realistic in a lot of cases you're not going to be able to predict very precisely what uh what new goods are going to do and this gives you by for these models by using the distribution of an observed Choice characteristic you see you can actually get a much more realistic range of of market shares for this for the new Goods then you would get under say the the conditional loaded model okay thanks 