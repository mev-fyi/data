um anyway so I will talk about some methods for dealing with missing data and um I have this listed under under four headings and you can read them um it's first useful I think to know when missing data can actually be ignored because then it suggests the sorts of Corrections that that might be needed or estimation methods that have some robustness properties that others don't so if we start with um the linear model and to just allow some generality consider estimating it by Instrumental variables so think of equation one as being a random draw from the population of interest and then we may or may not observe all of the data that we need in order to estimate this equation by um either two stagely squares or actually even some sort of generalized method of moments procedure actually for to illustrate the um the main idea just assume that the model is just identified so we have the same number of instruments as regressors so you can write the instrumental variables estimator as in equation two and then substitute into equation three and we we've seen indicators that indicate missing Data before one certainly is in the treatment effect literature where for the people who actually become treated we have missing data on them because we don't observe the counter factual in the state where they're not treated so I'll actually have a little bit to say about how some of these methods apply to that case Guido's covered it of course quite well but I'll have another couple of comments to make perhaps so the instrumental variables estimator written in terms of the errors and beta can be written like that and and the selection indicator S I simply just takes out the the data points that we actually observe and then the way to think about this is in general we should just think of s i as being random because we don't really know why a particular observation is observed or not this might be data that's just missing on why there might be a covariate that the data are missing on or or some combination it could even be that we're missing data on the instrumental variables okay so the question is using this simple representation when would using what they call in the statistics literature the the set of complete cases when would that produce a consistent estimate of beta and the the weakest condition is given in four that's just follows by looking at the final average in equation three and asking when would it have a probability limit of zero and then of course by the law of large numbers that's exactly when the expected value of the um a generic element is equal to zero and so this is um it's a kind of zero correlation assumption but it's not um it's not zero correlation in the population it um the the selected sample matter the the selection indicator matters here so a sufficient condition of course would be to assume that the error term has a zero mean conditional on the instruments and selection and then a special case of that is that in the underlying model the error term has a zero mean so this this is a an assumption purely on the population and um and then what would suffice is that selection is based on the instrumental variables okay so that that means a deterministic function of the instrumental variables notice of course in general that it can't be a function of the regressors because if if those regressors are endogenous then that's going to violate this condition so the general everybody pretty much knows this I think that the idea is that if this if the sample is selected on the basis of exogenous things then that doesn't cause any systematic bias in estimating the parameters in the in the population model but it it's a little important to be to be formal about that because notice that while this is sufficient along with selection being a deterministic function of the instruments it's actually not sufficient if we just impose the zero correlation assumption which is the one we often use to motivate instrumental variables estimation so in other words even if selection is a deterministic function of Z whether it's linear or not actually because this this s times Z is not going to be a linear function zero correlation is not generally going to be enough now the case where this would be enough is if selection is independent of everything right so if if selection s i is independent of the instruments and the errors then that expected value is is of course the product of the expectations and then we're done so these this distinction actually comes out in the statistics literature and I'll I'll talk about these definitions a little more formally later but if selection is independent of everything that's typically called missing completely at random and when selection depends on some of observable things that's uh in this case the exogenous instruments that's a case of missing at random in this case I think we would just call this exogenous selection and that that pretty much tells us what's going on by the way this this distinction between zero correlation in this case and zero conditional mean this this actually does have some implications about whether whether one wants to use uh so-called weighted estimators that we're going to talk about later on uh even in cases where the um the selection is a function of exogenous variables okay a special case of course is oops oops let's go there a special case is where we've modeled the conditional mean correctly it's a linear function of the x's and then of course selection can be a function of X in any way with the slight requirement of course that we have enough variation in the X's so that we can actually estimate the regression function now this this kind of derivation extends in a fairly straightforward way to most estimation methods that we deal with it's easiest to State the assumptions differently depending on what the Quant the the population object of interest is so in the case generally of non-linearly squares this would be the sufficient condition to ignore selection if we're doing quantile regression it would be that the median of one or at least absolute deviations which I'll talk about later today that would be the condition and then if we're doing maximum likelihood it's a conditional Independence assumption so in all these cases you can see that selection can be a function of the conditioning variables but what it rules out of course is selection that depends on the response variable that cannot be explained by the um the cover the conditioning variables by the way another showing each of these is easy in fact you do it pretty much in one one General framework because in each of these cases we know that the mean the median and the density or the log density to be more precise solve an optimization problem in the population the true parameters either maximize or minimize an objective function that's um that's leads to either least squares least absolute deviations or maximum likelihood and so basically the argument is shown in a few steps I work it through in in my textbook for example but anytime you can show that in the population you have an objective function that identifies the the population quantity of Interest well then multiplying it by a selection indicator and then taking the expected value and using iterated expectations gives you the result so there's nothing special about these three except that they happen to be common estimation methods that that we use but something like poisson regression which we know is fully robust as long as you have the conditional mean correctly specified would also fit into this category for exactly the same same reason that the mean maximizes the expected value of the quasi-likelhood function and it doesn't have to be that that light likely that log likelihood function is correct okay so clearly if we are interested in estimating unconditional moments like just the mean and we try to use the selected sample then that's going to require that the mean is the same in um in the selected subpopulation and the unselected subpopulation and Edo talked about how of course you can come up with bounds on these sorts of things if you have a bound on the um on the variable y itself okay now in with panel data exactly the same sorts of things hold just to consider the case of Maximum likelihood or I should say partial maximum likelihood estimation then again we can we can write this in a couple of different ways but the point is the conditional on whatever the X's are selection is independent of the outcome and again the way one goes about doing showing that this is sufficient is to look at the so-called pooled likelihood function now notice in this case this is this is not necessarily a true likelihood function because this might not actually represent a joint distribution across of the response variable across all time periods conditional on the X's at all time periods so this is applicable to lots of cases where we might be using panel data but we're interested in something like a static probit model say or we've put in some lags for a distributed lag model or even cases where we might have lag dependent variables but we also have explanatory variables that that might not be strictly exogenous in all of those cases you can't write down easily a joint likelihood function but all that matters is that we're interested in this conditional density we have it correctly specified and that being in the sample at time t for unit I is conditionally independent of whatever the response variable is okay intuitively this I hope makes makes pretty good sense and it again the argument about how that goes through is sketched out here and and covered more extensively in the notes but it um it does allow us for some flexibility of course the if we go back to this condition down at the in equation seven this the more that's in X the clearly the more likely this assumption is likely to hold because if x is empty then we're basically assuming that selection is independent of Y that is what the assumption is if x has say lag dependent variables and other observable variables in it or at least partially observable variables then this allows selection to depend on whatever is an X okay as long as it doesn't also depend on why at time T once you condition on X so as you go from Models such as static models to models with with Dynamics it it's more likely that you're going to be able to ignore the selection problem which in this case in a panel data case it might be attrition units dropping out it could be it could be attrition where units drop out and you never see anything about them or it could be people moving in and out of the labor force and you're trying to model labor earnings or something like that so at this level this is fairly um a fairly general statement but of course if what happens is that selection is a function of shocks that occur between time T minus 1 and T to YT then this condition is going to be violated and that's why we typically then worry about attrition say in panel data and we worry about people moving in and out of the labor force in a non-random way Okay so again that's so so that's a fairly General result and again you can see that this would work if we replace this log likelihood function with a squared residual function then you can easily write this down in terms of um expected values as well where you're using non-linearly squares or you could do the same thing with a dynamic say least absolute deviations regression okay um so as I said this is if if we do have a lag dependent variable then it does allow selection to depend on why we don't have to have that if we in any case where we think that it really is just a matter of you know say rotating people out of a panel exogenously then of course it shouldn't matter that we are getting different samples on people at different time periods we should be able to ignore that assuming of course they all come from the same population and that is is implied by this uh this General um very simple result now unfortunately it's much more difficult to write down conditions in general non-linear models when you have unobserved effects because of course if selection is truly independent of everything then then that's easy because then then anything you do whether it's maximum likelihood where you have some way of removing the heterogeneity such as in a so-called fixed effects logit or fixed effects poisson regression then you know in those cases actually selection can depend on the heterogeneity because you have a way of eliminating it basically but in cases where you don't have we where we don't have good ways of removing heterogeneity which is that entire class of models I talked about on Monday where you model the heterogeneity as some function of the um say the time averages of the covariates or other functions of the of the covariates you you would have to assume that basically selection is is a function of those observable things that what's left over doesn't cause selection and that's a little unsatisfying because that leads to basically selection on things that are exogenous and we don't necessarily think that's always a good characterization so in general although there are special cases where one can remove the unobserved heterogeneity and then allow selection to be a function of that heterogeneity in general that's complicated but I will work through the linear case with unobserved heterogeneity just so we can make sure we understand what is allowed and what's not going to be allowed in these cases okay so if we start with this model with again time effects and then the additive unobserved heterogeneity again thinking of cases with a large cross-section and small number of time periods and we want to allow for General correlation between well actually in this case instrumental variables and the heterogeneity then then it makes sense to remove the heterogeneity before using some sort of estimation method and so we can think of a couple of assumptions that we'd be willing to make maybe one is given in equation 11. now this this is never the weakest form of the assumption that you can write down but I think it's it's probably one of the easiest to interpret this says that the idiosyncratic error at time T and then this is just the usual conditioning on the exogenous variables in all time periods but then also conditioning on selection in all time periods that that's equal to zero okay so this is a very clear sense in which selection in any time period is not allowed to depend on the idiosyncratic errors in time period t so this this rules out for example the idiosyncratic errors reacting at time T to somebody dropping out at time T minus one it also rules out people dropping out at time t plus one based on what happened to them in time T at least if we don't observe the the things that happen which namely are in uit now what this doesn't restrict is this doesn't say anything about the relationship between selection and the unobserved heterogeneity CI okay this next assumption does make that restriction because it basically says in addition to the standard random effects assumption that heterogeneity is at least mean independent of the exogenous variables it's also mean and dependent of the selection indicators so this is more of a random effects type assumption this is more of a fixed effects type assumption actually random effects typically uses both of these well it does use both of these although as I said you can relax this a little bit and so this is certainly more likely to be true on its own than adding this along with it and so if if we use fixed effects on the unbalanced panel that means we can get by with this okay because what happens is the time demeaning now happens well it it still happens of course within each eye it's just that we can only subtract off means of the observations we actually have so in this notation T sub I here it's properly considered a random variable right it's the number of time periods we observe for unit I and since we're allowing selection to be random in any time period Then the number of times any unit is in the sample is Is Random so if you if you make these definitions then you can write down the so-called fixed effects IV estimator in this form and then if you do the simple algebra to get it in terms of the errors you can see that the weakest condition for consistency is this right here and again um it's not enough just to say that the zis are uncorrelated with the use hidden this notation I suppose is is a bit misleading because hidden in this z i t double dot is a function of the instruments in all time periods and of course that means that the selection indicator is hidden in there as well in a non-linear way actually because we're dividing by one over the number of time periods for each unit so this is a non-linear function of selection and the exogenous variables so that it's you could just assert this of course but then you're thinking about well when is this going to be true realistically and basically it's that the idiosyncratic errors are not correlated with selection in any time period okay and then it restricts the the exogenous variables as well Okay so um if we were to do let's see oh yeah one one important point is um that this all this all hinges on this uh structure where we have this additive unobserved heterogeneity so remember back to the first lecture I gave a set of conditions under which the fixed effects IV estimator on a balanced panel would actually be consistent for the average effect under you know what what is not a completely General assumption but is you know a somewhat reasonable assumption at least for for continuous endogenous variables well unfortunately one of the one of the problems with um with unbalanced panel is that if you just relax the the assumptions of the basic model a little bit and now let's say we add um we add random coefficients instead of assuming a constant beta then the condition that would be needed for consistency well you know you can write stuff down but the only thing that would make sense is this doesn't depend on the Z's and the selection in in any time period so if you are worried about that that this unbalanced panel would cause one not to be able to consistently estimate the average effect then it's easy to to come up with some sort of test for it this there's no claim that this is somehow the best test for it but consider this as an alternative where we don't worry about the exogenous variables because we might very well just be thinking of them as independent of the slope coefficients anyway and we're focusing on you know is there a selection problem as it relates to being correlated with these ran these individual specific slopes and so as a simple way to come up with a test it seems to make sense to specify that as an alternative and because we have a large cross-section we're assuming and relatively few time periods of course we should have we should have units with all time periods TI now we can't do anything with the units where we only see one time period because we don't have any way of of removing the heterogeneity for them anyway and and in the formula they just drop out okay so um one way to implement a test is to take the the set of individuals where we have a complete set of data as the base and then essentially Define interaction terms for all the other possibilities and then just add these as explanatory variables to the equation where of course these X's if they were endogenous before they're still endogenous and so this equation should be estimated by Instrumental variables but we have instruments for them because those would just be the zits interacted with the same the same sort of selection indicator so that's just one simple way to to come up with a test for for selection problems the same thing is um is true if we if we difference and we'll actually in the notes I discussed this case in more detail because for attrition problems especially differencing has some advantages because if you if attrition is an absorbing State then you know if you see a unit at time T then you also see that unit at time T minus one and so differencing becomes an attractive transformation relative to the within transformation and you can do some more things with that that you can't do with uh with so-called fixed effects okay so here here was the comment I guess I should have saved it but it's it's difficult to handle um General models with uh with non-linearities in the um in the unobserved heterogeneity and I think almost nothing is known or at least um not in any classical setting where you can deal with Dynamic non-linear models with with unobserved heterogeneity the methods that we have except in special cases really rely on having a balanced panel okay okay so if we think we need to do something about missing data what can we do well I'm going to start talking about inverse probability weighting and in the cross-sectional case first the um the idea of course is to try to re-weight the sample that we observe to make it reflect the population and the question is when does that sort of re-weighting work um well we already talked about what would happen if we tried to use the mean on the just on the data we observe and that would require selection to be independent of the Y's themselves the the general approach to this problem usually uses what's what's known as a selection on observables assumption this is also this is a special case of the so-called missing at random assumption where instead of saying that selection is independent of y when we want to estimate its mean the assumption is that selection is independent of why conditional on Z now in some you know this is this is not necessarily A convincing assumption in lots of cases so it's important to understand that a lot of methods actually rely on this assumption or even things that are more complicated than it inverse probability weighting relies on it um as as do imputation methods that we're going to talk about a little bit later and so you know it may be that this is just a non-starter and so when you get sampling weights and so on with your data set instead of just using them as if somehow they've they've come from some really um sophisticated underlying information that we don't have access to you might be a little suspicious actually that that one set of Weights is being offered to solve any kinds of kind of missing data or stratific stratified sampling problem but in any case this is this is a pop reasonably popular thing to do so so let's go ahead with it and see the kinds of problems that it's been applied to so the idea is to just this is I think the easiest way to think about what's being done that essentially shows immediately why this works notice that this average here is over all of the potential observations so in other words you think of drawing randomly from the population but if s i is equal to zero for whatever reason you can't use that observation so this the s i selects that out and then we wait by 1 over the probability of observing the observation conditional on whatever the zi's are and it's it's of course it's easy to show under this assumption that um what this what this weighting does is if you take the expected value of this you get mu sub y and it's it's an application not surprisingly of the law of iterated expectations and then using this condition here so it's actually quite simple to show that now occasionally occasionally you actually know what this probability is and one case where you know this is in variable probability stratified sampling schemes where ahead of time some sampling probabilities are decided and then as you say do a telephone survey and you keep an observation with a certain probability in that case you actually know these but in general they have to be estimated such as in any scheme where you just are trying to model why some data are missing as a function of data that you actually observe okay so in practice this is what you do where the P hats are estimates and writing the equation like this makes it um I think a little more relevant for practice because N1 then is just the number of data points that you actually have okay and then this ratio here this row hat is the fraction of observations that were kept overall in the sampling scheme and so this this is often the thing that's reported as the sampling weight okay where this is can be bigger or less than one depending on whether the observation had a higher or lower probability than a generic observation K4 actually studying the properties of the estimator this is a more convenient formulation okay um okay and of course the this is is just a consistent estimate of of uh that fraction of of kept observations you get a somewhat different answer by setting this up as a weightedly squares problem because now the sum of these things actually show up as in the weights but the um you know how one one analyzes these estimators is is quite similar this is actually the formulation that is used for doing regression analysis so we'll talk about how one does regression analysis in this this uh framework shortly now there are some problems with inverse probability weighting and I I alluded to them already but um there are some problems that are perhaps a little a little more subtle than than what I said which is that we're assuming that selection actually doesn't depend on the thing where where we're missing data on we're assuming it depends on other things that we're not missing data on um this actually comes up in a paper by Horowitz and manske where they studied a fairly simple problem which was estimating the mean of some function so whether you call this you could just redefine this to be y of course and instead of having it VG of Y but the key is that it's conditional on some other variable taking values in a certain set so call this that you can call this um I guess I didn't actually write that here I call this the dummy variable D which is one if x is in this set and zero otherwise so so X could just be something like a gender indicator and then you want to estimate the mean of something such as earnings for women versus men for example and the the caution that they that their paper is mainly um uh interested in providing bounds that you can find and you know talked about this actually but they they do mention a problem with with these so-called uh probability weighted estimators and that is that in your sample you usually find weights that are supposed to estimate this and in fact this is the row hat that I talked about and this is the P hat as a function of Z that I talked about so those are the weights that are reported with your data where sometimes we don't know what Z even was okay but the point is that if we don't know what Z was then we don't know that X was actually included in z and if you use this this the probability weights that you get you you actually might come up with an estimated mean that is out outside the range of possible values um that that any logical bounds analysis would come up with of course if y it's if this function G of Y itself is unbounded then that's not possible but in other cases where you know if if Y is a binary variable for example then you can come up with estimates that lie outside the bounds the Horowitz manske bounds that are based on no assumptions at all okay so the solution to this is to basically always make sure that whatever you're conditioning on here is used in calculating the weights okay so this would this should be conditional on this event and this should be conditional on this event as well okay then that that solves the problem however if you don't always observe X then you can't put it in the conditioning variable Z Okay so when you get these weights and you're missing data on some x's and you're missing data on some y's then the weights weren't constructed using those and you have to wonder you know whether they can be used for all kinds of um all kinds of econometric problems now there's a related issue here and in that um if the weights say depend again on variable Z and we want to apply regression analysis so we'd like to estimate the parameters say in this population regression okay and um we have missing data on either y or X or both but we want to use these these sampling weights in order to do what's it's it's called a weightedly squares estimation but it's with probability weights we're not we're not choosing weights that are one over the the estimated variance of Y given X okay so the problem is if we look at the solution to this so that's written out like this summed over all observations and then again using the same device to select out the The observed data points the problem is that if in fact selection is exogenous in this sense okay so suppose that selection is a function of the X's which may be you know includes age or gender or something like that and maybe we're missing data on those variables for some some fraction of the of the population but if this is true then as we just discussed consistent estimation would be obtained by just ignoring the problem and doing least squares on The observed data okay but if this is true and X cannot be included in Z because X is not always observed then this condition is probably very unlikely to be true because the selection in fact in a case where selection is a deterministic function of X it's clear that this can't be true if if x is actually not not contained in z so in other words if we specify some set of variable Z and we go ahead and we estimate a probability of selecting the sample based on Z and then go ahead and do this inverse probability weighting the weighted estimator will actually be inconsistent okay whereas doing nothing would have been consistent so it's not as if sometimes you hear people say well of course I should use the weights because that can't hurt right well it can hurt if the weights can't be a function of all of the right things um if in this regression context if you if you can always include x in Z then the case for waiting is much stronger because then it just becomes a functional form issue you know maybe you've thrown some things in Z that you don't think belong in the population regression model but but would help to explain you know the probability of being in the sample well if if only the X's matter then if you may you know with enough data in a flexible enough model you will pick that out and so in the end you won't have an inconsistent estimator but you know if you can't include x and z then that's that's a different issue okay um so so that this theme actually comes up unfortunately in all of the methods that that I'm going to talk about the same the same issue comes up in imputation methods as well because if the missing random assumption doesn't hold yet you're doing a regression and selection was largely on the basis of the x's and the regression then it would have been better to to not do the imputation because that'll actually be inconsistent okay I'll mention that again okay um now suppose that there's this issue of whether one should use weights if in fact we think that selection is what we would typically call exogenous in this case and let's say we're doing linear regression actually the the same sorts of issues come up if we're doing nonlinear regression or maximum likelihood too the question is should we use inverse probability weighting um well the answer is it depends the answer is no if we believe 21 is just the assumption that the conditional expectation is linear and then if we added the standard homoscedasticity assumption then clearly we shouldn't wait because OLS of course if that isn't true you just you can set it up to be true right you can always just Force the data set so that attrition is an absorbing state um so of course if we can observe a set of stuff in general call these very lump these variables together in something called wit again you can do maximum likelihood here this could be a variety of other problems least squares or at least absolute deviations if you can find variables RIT observed such that this is true then the weighty the the weighted what's usually called the weighted maximum likelihood estimator is this this is actually better called a weighted partial maximum likelihood estimator because again there's no presumption that this is that if we didn't have to do the weighting that this would be a joint density function over the entire set of time periods it's just modeling a set of conditional distributions um and and generally this works because if you write the objective function like that then what you can show is that its expected value the expected value of the weighted thing is equal to the expected value of the unweighted thing not selected in the population that's how these things all work okay well the question is how can this is a strong assumption right this because if we're going to estimate these probabilities the rits have to be observed or at least they have to be observed at time t if you have General missing data problems the question is how do you how do you choose these rits so that they're you have a chance of estimating a probability such that this is true in the attrition case the assumptions are strong but at least you see some sort of natural structure that leads to to to a fairly simple approach so the um the strategy that that's suggested in this robbinsky job paper is a sequential strategy so the idea is um to estimate a sequence of conditional probabilities where so for the sake of argument let's say we start off with a random sample in the first time period and then what we have to do is figure out estimate probabilities of why people are leaving at time T conditional on being in the sample at time T minus one and of course if we because of the way attrition works these zits they only have to be observed for everyone in the sample at time T minus one okay so we're moving along in time and actually the the zits are growing in dimension even though we're seeing fewer and fewer people because we are going back we every time we go one time period more we grab the person's um the units who have who are still in the sample at time T minus one we we get their Z's so the idea is to model these sequential probabilities estimate them and then to form the probability weights but these aren't the weights that you use at time T we we need to wait by the the if you will unconditional probability of being in the sample at time T not the conditional probability at being it in the sample at time T minus one well this is where the assumptions really get strong I'm I'm writing down the simplest version of the assumption that that makes it clear that the procedure works um the robins at all paper shows that some weaker set of assumptions do work they they focus on the regression case so if um if you let V at time T be everything that is either in the in the population model and then this extra stuff which generally would be lags of things that are in that model then the condition is this right here so that somehow we have good enough things at time T predicting attrition that none of the rest of the history of V matters it's a pretty it's a strong assumption okay so under this you can show that in fact if you believe this then the sequence the probability at time T in fact is just the product of these probabilities these conditional probabilities and so now the procedure is fairly fairly clear you just run a sequence of logits or probos or something like that basically throwing everything you can think of into the probeitter logit at time T That You observe for everyone at time T minus one you compute the product of these probabilities and then that these become the weights that you use in order to quote solve the attrition problem you have to be a little careful I have seen papers where I'm I'm pretty sure they're actually waiting by these conditional probabilities instead of the product of them okay this this is this is the thing that falls out of this kind of framework um this is uh there are a couple of things that that one should worry about first of all as a practical matter these can be quite small right you're now taking products of probabilities and so do you really want to give you know so much weight to observations where the conditional probability of being in the sample at each time period is is small well of course the theory tells you you should do that whether you should do it in practice is is a different matter there's this other um practical problem which is very similar to what I talked about in the cross-section case and that is if in fact it turned out that the selection is mainly due or let's say entirely due to the things you're conditioning on in your model then this could could just be a disaster it could lead to inconsistent estimates when you didn't have to do anything except just use the the treated sample as is and there's really no way to tell that's the right because because you can't always observe these x's and in the attrition cases is especially true because attrition means they're dropping out that means it's not just that you're not seeing the Y's you're not seeing the X's either so I I you know I I did some theoretical work on this and having thought about it more it's not not entirely clear that this is such a safe thing to do again the argument that well it can't do any harm is is not true actually it can do some harm a related point is that if you start by assuming that your model is is what some like to call dynamically complete so in other words if you have specified the distribution conditional on everything in the past or say a conditional meaning you're doing least squares this really reduces the role for waiting okay because what sorts of things could you include in those attrition probabilities as you're moving through time well they're things that you observe for people in previous time periods but if this is your model then selection by definition is a function of the things in the conditioning set now whether your model actually contains all the Y's back to the first time period that's not the issue the issue is whether it represents this conditional distribution and if it does then you can't really make a case for for doing the waiting unless you have some external information like on you know people who were doing the interviewing in different time periods that that information clearly wouldn't show up in your underlying structural model but may actually affect the probability of you know the person leaving the sample or whatever so that's in those cases you can make a stronger case but if all you're going to do is do a bunch of you know logits or probe it's conditional on things that could already be in your model then then it's it's not so attractive okay imputation so the the first thing we talked about was just when can you ignore the problem so that's the so-called complete case analysis then I gave you a quick run through on some some ways that inverse probability weighting is used but again that uses just the data that you have right but it waits it to try to make it representative of the population another approach is imputation where you actually try to fill in the missing data and then use it in some way in in subsequent econometric procedures so I recommend the book by by little and Reuben on this um it's it's quite accessible and um it it actually the in the end multiple imputation relies heavily on Bayesian arguments and so you know the the and in fact Edo talked about a particular example of that in his talk on sampling from a in the case of a sensored Tobit model um so I I think I'll just make a couple of points here the I suppose is with all methods such as inverse probability weighting you know it's better it seems like well you're not just doing least squares or maximum you know you're using these weights and that seems kind of neat and so on um and so you kind of forget maybe that you know what the assumptions underlying it actually are I think there's a bit of this in the imputation literature too because now you know sometimes you get data sets that have have five or six imputed samples and you go okay I know how to deal with this and some and you know there is a set of assumptions under which that the the methods that are have been worked out work but the problem is that this this missing at random assumption is is key in in making imputation work just as something like it is key in making the inverse probability stuff work um so it doesn't in in the case where say we have a variable y we don't always observe it we do always observe some x's then missing at random is basically this assumption here um you can Define it for General patterns of missing data but it gets complicated because you can only the the probability of a certain data point missing sorry a certain variable missing can't depend on its value it can only depend on the values of things You observe so if you think about General patterns of non-responsive surveys this becomes very complicated and that's why Bayesian methods are used to essentially do these um simulation methods like Monte Carlo um Markov chain Monte Carlo methods are are needed in order to essentially draw parameters and data at the same time so I'll just um well so for example this is just that the probability of say retaining observation one um let's see yeah okay so so here for example the probability that observed that the the data on why on W1 and W2 is both missing can't depend on what those values are they just have to be some constant so in a sense this is a that this really seems like a missing completely at random assumption on when when both variables are missing so right away you can see that if you're going to allow General patterns of missing data more more generally than um than this missing at random assumption becomes you know a little hard to to figure out and of course if you if you just think everything is missing completely at random then then you have missing at random so that that may be the best or the easiest way to think about this um with with monotone patterns of missing data it becomes easier to figure out what's going on but we basically just discussed the the attrition and panel data case which would be the natural case where you have a monotone missing data pattern that means that you know if you you can list the variables so that if wih is observed so is w i g where G is is less than h so in this case of course you can just use a sequence of conditional distributions in order to estimate parameters and then this sequence of conditional distributions can also be drawn from to generate values for the missing data so in other words if you start with a variable W1 that's always observed and then you can estimate the distribution of W2 conditional on W1 because the assumption is that the missing data are only a function of W1 values then you can draw the missing observations on W2 from that conditional density okay and that becomes essentially a single imputation for that value and then you can do it again and you can come up with you know five or six different imputed data sets in that way and the reason this sequential structure is useful is because then you can go on to the next variable or the next time period and do exactly the same procedure where now you use the imputed data that you just used in the previous period to draw for the um for for the current period okay so let me just show you a a simple way that imputation is done and the and the pros and cons of it um suppose that we actually want to estimate the unconditional mean of Y and we make them missing at random assumption where the X's are always observed and we actually know what the conditional mean of Y given X is so this parametric assumption well you know you can you could make it non-parametric but typically this is done in some flexible way and you know this is this is the cost of the missing data right is if you want to be able to to impute values for y um then if the selection really depends on just the X then you have to use the conditional mean in order to then generate the imputed values so we we already discussed why it is that now just using the observed data consistently estimates beta so so the first step would be non-linearly squares using the complete case data but because we always observe X we can then predict a value for Y for the Y's that aren't there so this scheme leads to so-called conditional mean imputation and that would be given by this right here so in other words this is so now how would we use the imputed values in order to estimate the population mean of Y well we would keep y if we observe it and then we impute the missing value using this estimate from the conditional mean if we don't observe it and um that's not right what should be written here is from pllim of beta hat equals beta clearly this is just assuming the conclusion that I'm going to state next so the fact that beta hat is consistent for beta means that we can replace this summation in the limit with this expected value and then it's just a matter of invoking iterated expectations to show that in fact the expected value of this is equal to the population mean okay so imputation in these kinds of contexts is fairly straightforward the danger of course is that we're going to ignore the fact that we estimated those betas in the first stage and that those um those observations on why their only uncertainty is in beta hat right that these don't reflect General Randomness that comes from the unobservables these are deterministic functions of this this Vector X and beta and then the fact that beta hat is in there that's just sampling error so as I said that's called the the method of conditional means um the they also little and Reuben prefer and I think does the the literature in general is the so-called method of what they call conditional draws which is to add Randomness to this imputation to the to the missing values because you know that the chance that the actual data is is equal to its conditional mean basically is is very small so what can you do well of course as you go as you layer on more stuff you have to make more assumptions so if we assume that the underlying errors are say normally distributed then we would simply draw an error from this estimated distribution and then go ahead and and impute a value in that way so of course we'd have to estimate the error variance but that's also consistent on the using the selected sample and then just draw from a normal distribution and then this becomes a single imputation for this particular problem now of course this is now there's this additional uncertainty that's been added that isn't just you can't just analyze this really in the context of the original data and this is where a Bayesian approach really comes in especially when you start looking at multiple imputations so it's it's hard to um it's difficult in general to quantify the uncertainty that comes about from this people have looked at what happens of course if you use this data say versus imputing only based on the conditional mean and what happens and but both of these lead to consistent estimates it's not a matter of does this give you a consistent estimate of the mean in the case that not adding this Randomness on does not that's not the issue the issue really is in terms of estimating variances and then being able to come up with a sensible standard error say for the for the estimate of the population mean that you have and if you if you treat this as if it's a real data point then you're going to ignore the fact that there really is an error on the end of it and tacking on an error that's been drawn from the distribution certainly does a lot better but it's limited because it only applies to you know fairly simple cases like this so let me just jump ahead to just give you briefly what the general idea is here um from a Bayesian perspective when you define parameters Theta what you want in the end is something like the posterior mean of the parameter conditional on the data that you actually observe what imputation does is it gives you this and if you do it several times then what you do is you get several posterior means but these are conditional on a set of imputed data okay so again this is this is not diff this is just iterated expectation so the question is how do you use the imputation that the imputed data sets to come up with this object when you want to basically integrate out the imputations at the end well you just basically obtain these things as the posterior means and you have D of them and you just average them in the end so that's that's what they recommend right if you use a a procedure with an imputed data set you do the procedure however many times you have data sets and then your estimate is the average the how you come up with the variance is a little more complicated because you have to account for these two sources um the variance in the in the posterior distribution conditional on the imputed data but then also the variance in the posterior mean this is of course a standard formula from probability and then when you get down to it you have two sources of variation this is the average of the variances from the post from the imputed data sets and then this is usually called the between imputation variance and it's but you know these things are are easy to obtain of course there's a finite sample adjustment a finite not sample a small imputation adjustment down here so of course once you're handed the imputed data set then this becomes all easy the the hard part is an and how the imputed data came about to begin with it's I guess it's kind of like making sausage as you you probably don't really want to know but somebody hands you this imputed data set and and the way you go so as I said like these waiting methods there are cases where they would clearly lead to some sort of systematic bias when just ignoring the data would not so you can't just say oh it's better to use the imputed data just like it's not always better to wait it's not better to always use the imputed data okay well let's travel back in time a little bit to um I just want to quickly talk about Heckman type sample selection Corrections because of course they apply to the missing data problem and I just want to highlight a couple of issues that come up and um both in cross-sectional cases and panel data cases and really they have to do with what happens if you have missing data on your explanatory variables or you don't have missing data on your explanatory variables but one or more of them is endogenous in the traditional sense that even if you had well okay endogeneity shouldn't when you when you write down a population model like this when we first start thinking about this model endogeneity should be defined in terms of the population right it shouldn't the the whether there's a sample selection problem is a separate issue so let's think of this model here with this endogenous explanatory variable Y2 the Z ones are exogenous and at this level y1 and Y2 at well presumably at least one of them is not observed all the time it might be both of them it might just be why one is not observed it might be that Y2 is only observed along with y1 okay so the question is how should we what kind of Heckman correction should we apply to this particular model well I'm okay so I'm thinking of let's just write down a linear projection in in the population like that okay this becomes more substantive because this is the selection mechanism and because I'm calling this a Heckman type correction this is going to lead to some sort of inverse Mills ratio correction okay so again here here are just the assumptions and then these assumptions down here well you can get rid of you can Encompass them all by just saying everything's jointly normal here and independent of disease okay but there of course a weaker set suffices so the key here is that the exogenous things are always observed okay or at least that's how we're defining that that's the that's how I'm defining the problem here so if this um if this setup is true then a good way to think about this what what we want to do is we'd like to estimate this equation here now this I've written this so that this holds in the population okay So eventually of course we're going to see which people are selected into the sample and which people are not but the the justification for the method that I'm going to talk about really hinges on having this equation here where because this is well when Y3 is equal to 1 this is just going to be a linear function of the inverse Mills ratio okay but there's also of course a formula when Y3 is equal to zero for that part of the population that we're not observing so in this equation the key thing is that this is actually exogenous the Z ones are exogenous Y2 is still endogenous okay I haven't tried to use a Control Function or anything like that to try to make Y2 exogenous here and there's a good reason for that because I want us to think of Y2 as as being anything it could be binary or continuous or you know count or whatever it doesn't matter and so by setting it up in this way I've intentionally left this to be an endogenous variable in this equation but this is an exogenous variable and so by the results that I mentioned earlier that's why just having this basic result that if you do instrumental variables and select on the basis of exogenous things everything's fine well that means I can Do instrumental variables on this equation and I can choose instruments that are this is its own instrument these are their own instruments and then presumably I have some exclusion restrictions that I have for Y2 um so here's here's the two-step estimator that comes out of this now we have to of course condition on Y3 being equal to one so the first step is to do a probate and in general of course you would do it on all of everything that you're claiming is exogenous to make this Supply you need you really want two things that are in Z that are not on Z1 intuitively you need one that moves Y2 around exogenously and you need something else that moves selection around exogenously okay it's pretty clear that you kind of need that these are always these models are always identified off of non-linearities but you're on on Shaky Ground I think if you if you rely on that okay and then in the second stage you do instrumental variables well two stage lease squares where you do this um regression you include the inverse Mills ratio and then specify your instruments this way now the couple of points I want to make about this procedure is that um this is much more robust than a procedure you might think is either the same thing or close to being the same thing okay suppose Y2 is always observed then one might think well why should I why should I only use the information on on Y2 in the selected sample and estimating the parameters so you might say well I'm going to first project Y2 on the instruments using the entire sample get the fitted values plug them in and then do an OLS regression where I add the inverse Mills ratio the reason that's a bad idea is because now you've suddenly taken something that works for General Y2 and basically made it only work for normally distributed Y2 because by plugging in the fitted values you've added the reduced form error to this error and now unless both of them were normally distributed to begin with the resulting thing is not normally distributed and so you won't get an inverse Mills ratio as the proper correction plus this is a lot easier because all of the standard errors are appropriate for testing whether there's actually a selection bias problem you can just use a standard t-test after doing two stage lease squares the other thing is a lot Messier so that's one point is that it's not a seemingly subtle change can actually change how robust this procedure really is the other point is that even if Y2 is something that you would think of as being exogenous in the underlying structural model if you don't observe it all the time you really should think of getting an instrument for it okay and the reason is is because if you don't then in the first stage of your Heckman procedure you have to leave Y2 out right you it's exogenous so you would like to put it in because you you think it could probably help predict selecting into the sample but practically you can't do that and so therefore to implement the Heckman procedure as is normally stated where you assume you have exogenous things you you are making an exclusion restriction on what we usually think of as a reduced form selection equation and that's always a dangerous thing to do as well right to say that well the selection probability depends on Z1 but not on Y2 because I don't have data on Y2 okay that's one way to identify the parameters in the case where Y2 is exogenous I think it's a fairly tenuous way to do it another way to do it which is of course easier said than done but is to treat Y2 now as an endogenous variable and try to come up with an instrument for it in addition to whatever instrument you're choosing to exogenously influence sample selection there's a discussion in the notes about how this is also important in panel data context where you have attrition if you kind if you just first difference and then try to apply you know a Heckman method to the resulting equation it winds up with exactly this sort of problem that any any selection probability you use necessarily leaves out the X's or the change in the x's and so the safe thing to do if you can is to find instruments for those things okay it looks like I've gone a little bit over but 