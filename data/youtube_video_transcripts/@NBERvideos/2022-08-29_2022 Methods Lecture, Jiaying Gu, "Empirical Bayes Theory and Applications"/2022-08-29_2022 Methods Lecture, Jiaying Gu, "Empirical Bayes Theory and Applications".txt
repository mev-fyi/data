it's a great honor to be given this opportunity to present at the method lecture so i'll be talking about empirical based method this is a topic very close to my heart and i would like to share with everyone here today so here you see a picture of roger conker so with whom i have a long standing collaboration on this topic we have had a very fun journey working through this i remember that we went to the library of the columbia university looking through the archive of herbert robbins it was great fun to to find a lot of treasure there so i encourage everyone if you had a chance to try it out there and i'm extremely grateful to have roger as my advisor who introduced me to the wonderful empirical based world so here is how the outline gonna look like so i'll be um starting off with some motivating examples with teacher value added and show how it has a connection to the so-called compound decision problem that is uh as a terminology used in the stats literature then i'll give much detail on the so-called normal mean problem and introduce what everybody probably is familiar with on the parametric shrinkage method as well as the nonparametric shrinkage method which probably is not as well known but is very powerful for large scale estimation and inference problem then i'll be talking about computation methods which really unlocks the possibility of going towards these non-parametric techniques then i'll deviate from the normal mean problem and talk about other compound decisions that has connections to multiple testing ranking and selections i'll end with empirical base inference which is uh relatively underdeveloped that hopefully today's talk gonna inspire more uh research in this area if time permits i'll go beyond normal models and talk about mixture models in general okay so let me start off with this uh motivating example it's probably simplified uh away from a lot of uh complications but would help us to get what's the key object that we have in mind so let the y i j tilde be denoted as the test outcomes of student j taught by teacher i right so in the literature of education economics we typically run this so-called value-added regression where the covariates xij captures a rich collection of information some student family background lack test scores teacher characteristics and and and all that and also the alpha i is the so-called value added of teacher i in the classical panel data literature this would be nothing but the teacher fixed effects our goal and that will be the focus of today's lecture is trying to estimate the whole vector of alpha 1 to alpha n now we might be interested in this because either we want a systematic way of evaluating teachers or we might be interested to know what is the heterogeneity of the teacher quality or there might be policy that requires us to select top or bottom quality teachers so all of that involves estimation of these fixed effects we would start off our analysis with the familiar fixed effects estimator for alpha i which is nothing but the teacher-specific sample mean once you purge away the effects of the covariates and j-i here denotes the number of students teacher i has taught okay when ji is reasonably large then it's reasonable to assume that we have a normal model on the teacher's specific sample mean with the location as the teacher quality alpha i and here the variance would depends on how many students this teacher has taught now naturally in this application we will be facing uh the complication of variance heterogeneity in the beginning of my lecture i would abstract away from that and then i'll bring it back just to kind of bring out some key intuitions okay so what is a key feature of this estimation problem right so essentially it can be summarized that we are facing an independent statistical decision problem we observe some data which leads to y i and our goal is to estimate alpha i for all the individual teachers now this n independent problem has common structure in the sense that given an alpha i they are all having the same model here i denoted as p in the teacher context they are all normally distributed with the location parameter being alpha i and n which is the total number of uh simultaneous problem we are considering is typically large in in an at least in modern application and most importantly we care about collective performance right in the sense that we want to come up with estimator for the vector of alpha that has good performance as a whole like considering n problems simultaneously so these features put together is precisely what defines a compound decision problem first be given the name in the in the 1951 paper of herbert robbins so to be to to put it in contrast with single decision problem right so here the key uh feature is that single decision problem would be ignoring the ensemble of n problems and just be coming up with estimators using using individualized data for individualized parameters whereas the compound decision problem would take into account that we are really estimating uh n parameters uh at the same time okay so now there are two type of assumption that one could consider imposing on a vector alpha one is the familiar fixed effects model where these vector alpha are just treated as some unknown parameters the other slightly tightened assumption which is the random effects model is to consider this vector alpha as a vector of random variable coming from some common distribution g for instance alpha i such as random iid jaws from the distribution g sometimes the literature reserves the name compound decision to correspond to the fixed effects model and reserve the name empirical base for the random effects model but that's really just terminologies okay so let's formulate this uh compound decision problem right so we have y i measuring the outcome we have some model to characterize the randomness of y i given this key parameter of interest alpha i we would like to estimate all these alpha i's now let's denote the vector of estimators that we could come up with for the vector of alpha as delta just to sort out some notation and this delta of course would be depending on the vector of data that we observe okay and now in order to evaluate different estimators we need to come up with the loss function so the loss function because we are considering n problems simultaneously naturally has this sample average kind of form where we are just adding up losses for each individual estimation and divided by how many problems we are considered as a whole now zooming in there are different type of loss functions one could consider the most commonly used is the squared error loss where we just are taking the usual euclidean distance we could also consider other form for instance the absolute error loss is also used in the literature what is risk okay risk is nothing but expected loss right so in our context a priori individual estimator for alpha i could potentially depends on the entire vector of y so therefore when we write out the expectation it's with respect to this product measure of all these ndgps now the key of the compound decision problem is estimation risk of each individual matters however they are being aggregated so that's going to be matter a lot for estimators that i'm going to introduce forward so let's zoom in to the so-called normal mean problem right so let's specialize the distribution p to be normal and here i have simplified the variance to be the same across individual so this is the homogeneous variance assumption which is strong right but i will relax that later and let's use the commonly used squared error loss for that our goal is to come up with estimator that tried to make the compound risk small there are two well-known estimators the first of course is the maximum likelihood estimator also called the fixed effect estimator so the fixed effect estimator is only using information from individual i which is kind of a priori natural because all these n problems are independent why would i want to use other people's information to estimate my own rfi however there is also this uh well i believe well known a linear shrinkage estimator first proposed by james and stein in 1961 which basically is a shrinkage estimator right so we are putting a shrinkage factor in front of each individual y i and this shrinkage factor is going to be depending on the entire data y 1 to y n so in the sense that what james stein is proposing is a data dependent shrinkage for each individual coordinates problem and there is this very powerful result in stein 1956 as well as in the james and stein paper in 1961 which shows that the maximum likelihood estimator is inadmissible in the sense that the risk of using mle is going to be dominated by the jamestown estimator as soon as sample size n is greater than equals to 3 and for any vector of alpha so this is a very powerful finite example frequent test result now let's try to understand where does it come from so first of all why are we doing shrinkage right so think about the extreme case where suppose all the alpha i's are zero then the y i are really just noise then to shrink them makes a lot of sense right because you want to get rid of those noise and coming up with a better estimate for the vector of alpha but the key catch is they come up with an estimator that dominates in any situations of and for any vector of alpha how does that go so now let's consider the class of linear shrinkage estimator so here i have specialized my estimated delta to just be a shrinkage estimator for a shrinkage factor 1 minus b where b is any fixed constant that is greater than equals to 0. and note that b equals to 0 would include the maximum likelihood estimator so we can easily write down what is the compound risk for these class of linear shrinkage estimator so these will take this form now a natural idea is to look for the optimal b that will minimize the compound risk within this class of estimator so if we do that we're going to find that the optimal b denoted as b star here going to take this form now let's take a look at that so that optimal b kind of depends on the entire vector of alpha so that's not very useful right because i don't know what alpha is then i wouldn't know what is the b star but the key observations that james and stein made is that the b star indeed depends on alpha but it only depends on alpha through the second moments of the data y i right so in this normal mean model it's easy to see that the second moment is equals to one plus alpha i square which is in this summons in the denominator that chip into the optimal shrinkage so then the idea is basically to come up with sample counterpart right that mimics the expectation so that's why these sum of y i squared terms and the n minus two factor is to adjust for the fact that we are having an inverse instead of just the sum itself right so this is in the denominator so indeed they then show that with this data dependent prior right now it depends on orderwise they show that the risk of the jamestown estimator can be upper bounded by the right hand side quantity when all the alpha i's are 0 that's where we see the biggest improvement in terms of risk in comparison to the mle is that we have a risk bound of 2 over n instead of 1 for the maximum likelihood estimator now it's also easy to see that as long as n is greater than equals to three we always have an improvement the idea of the jamestown estimator is essentially to use shrinkage to introduce some bias to each individual estimator but to improve the overall performance in terms of the compound risk now later on in the 70s efron and morris in the 1973 paper gives a bayesian interpretation of this frequency shrinkage so now let's consider the random effects assumption where all the alpha eyes are id draw from some distribution g and in their context let's specialize to the normal distribution with zero mean and variance a now with this we have a bayesian model where the g is just a subjective prior on these alpha ice so in the bayesian world a natural optimal principle is then to write down the base risk which i write here and to minimize that which will leads to the base estimator okay and with the squared error loss that comes back to be the familiar posterior mean for the normal mean problem this posterior mean gonna take this linear shrinkage form where the shrinkage factor gonna depends on the variance of a which appears in the prior now notice that the marginal distribution of y in the normal mean model is this multivariate normal with this variance covariance matrix so the sum of the square of them is going to follow a scaled chi-square then we can find an unbiased estimator for the shrinkage factor which is exactly is the shrinkage factor in jamestown estimator and minus 2 divided by the sum of squares so in essence the james science estimator basically replaced the unknown shrinkage right so here one over a plus one in the base estimator by an unbiased estimator that comes up from the data and this gives to the right writes the name empirical base because we are estimating some unknown quantity in the prior so when a prior is normal then jamestown estimator is mimicking the optimal base estimator but recall stein's result holds without any bayesian assumption on the vector of alpha any vector of alpha would have improvement in terms of mean squared error there are a couple of extensions on the jamestown estimator for instance we note that the shrinkage factor is always strictly positive so put a positive part on the shrinkage terms would provide some further improvement especially for small n we could also generalize the prior g to have an unknown center alpha 0 right then the optimal base estimator gonna shrink the individual y i towards that common prior mean alpha zero and the corresponding jamestown estimator is basically replacing the sample counterpart on the unknown pieces in the prior in our case here alpha 0 and a and it's easy to show that these estimator again would dominate the maximum likelihood estimator in terms of compound risk when n is greater than equals to 4. and there are many other variants that would arise for instance heterogeneous variance how would we work through to get the jamestown estimator and i'll refer to those variants in increased section let's see how much improvement we get by doing this so this is taken from evernote mars 1975 where we have 18 baseball players we observe their number of hits out of their first 45 at-bats in the season so ni here denotes the number of advents what we want to do is to predict for the remainder of the season how their hitting performance would look like a natural model on the hits would be the binomial with the number of at-bats as the number of trials as well as the success probability denoted as pi here measuring heating performance everyone morris suggested a variant stabilizing transformation which will bring us back to the normal mean model right so once you do this transformation we approximately have a normal distribution for this transformed quantity and our goal is to estimate alpha i which links back to the heating performance so for mean squared error comparison where we use the remainder remaining seasons realized outcomes as to calculate the true value of alpha i we see a big reduction in terms of mean squared error by just using the james stein shrinkage so this could be very very meaningful empirically now a natural question to ask is is there a room for further improvement right recall the intuition that under the fixed effects assumption james estimator is trying to mimic the optimal estimator in the class of linear shrinkage estimator can we do better if we enlarge this class using the random effect assumption or the bayesian context james estimator is mimicking the optimal base estimator when the prior is normal then a natural question becomes what if the prior is not normal what should we do so in order to bring that out let me bring you back to the compound decision problem and this is really teased out in the early paper of of robbings the 51 and 56 paper so let's go back to the fix effects model where the vector of alpha is just fixed parameters now let's suppose that y i given alpha i has this density denoted as p so it can be any well-defined density let's consider a slightly larger class of estimator right so instead of forcing the attention to the linear shrinkage estimator let's consider some function t that is applied on individual coordinates so these are the so-called separable estimator if we are focusing on this class of separable estimator then the following lines gonna leads to the fundamental theorem of compound decision so these are very important derivation i want to go slowly on them so on the left hand side we have the defined compound risk right so these are just expectation of the loss sorry because we are using separable estimator this allows me to replace my general delta to just be a t function applied on y i and now because of this the expectation can be applied on each individual coordinates that leads to the next line where i replace the expectation by the integral form with respect to the density p for each individual and integrate it out right and the last line here is to notice that because we have a sample average here and that's nothing but taking another integral with respect to the empirical measure of alpha right so this g n here are nothing but the empirical cdf of the vector of parameter alpha 1 to alpha n so what has this brought us this essentially connects the compound risk to the base risk of a single copy of the compound problem with a very specific prior so this prior gn makes the link back to the compound risk okay so what's going on is since we've connected compound risk to base risk with a specific prior now pretending that we know that prior we could work out the bayes rule which will optimize or minimize the base risk equivalently minimize the compound risk now but since we don't know gn why don't we learn it from the data so we have y 1 to y n containing information about alpha 1 to alpha n is there a way to let us estimate this empirical measure right so this distribution g n and these link or these results of connecting the compound risk to base risk holds for any density p and for any loss function so it's quite generic as as a machinery now let's think back to the james stein estimator right it did actually estimate the second moment of that distribution because that optimal b star thus connects back to the second moment of alpha i and that's enough if we consider linear shrinkage robbins being more ambitious would ask us to estimate the whole distribution of gn hence giving rise to the name nonparametric empirical base so we are really trying to learn not just parameters in some prior that are subjective we're trying to learn the prior as a whole like this gn measure so and robin's estimator is more ambitious than linear shrinkage because it's targeting the minimum of the compound risk right because of the connection back to compound risk as well as he's considering a larger class of estimators okay zooming back into the normal mean model we have the squared error loss for any gn that you pass on to me then the optimal base estimator is again posterior mean which would take this form okay so then let's ask the question is it feasible right to estimate this gn it sounds like a great plan but it is implementable so gn is unknown we can estimate it from the data i'm going to show you example why this is doable and the idea is to estimate that and plugging in to get a mimic rule for the optimal base estimator so this in the literature is called the g modeling because it's trying to target gn and it's applicable for any form of the conditional density right so normal poisson gamma anything that we have in the empirical context and also just to remark the estimation of g is a deconvolution problem right in the in the in the normal mean problem here we observe y i we know ui is normally distributed all we want is to back out what's the distribution of alpha i so this is a classical deconvolution in the normal mean problem we have something further specialized right so the base estimator in the normal mean where we replace this p by the gaussian density gonna leads to the well-known tweety formula okay so let's take a look at the 2d formula so the 2d formula says that the posterior mean in the normal mean problem gonna be y i the original coordinates of the information plus an additional term that depends on the marginal density of y right so here what f of y is this mixture model this is the marginal density of y as well as f as its derivative so at least this leads to an alternative strategy of proceeding to have a non-parametric estimate for the posterior mean is the so-called f modeling where why don't we just target on estimating the marginal density that sounds like a more natural and simpler problem because we have kernel methods and all that but i just want to point out here at least that this is specific to the 3d formula hence specific to the normal model but the g modeling would be more generic and further on later in computation section i'll comment on what's the drawback of f modeling so suppose we stick to the idea of g modeling and trying to ambitiously estimate these uh gn nonparametrically can let backfire right our target is to try to minimize the compound risk but in the past we need to non-parametrically estimate some quantity which can be hard can it actually leads to a risk that is substantially inflated and the good news is we have good theoretical guarantee for nonparametric empirical base estimators so consider just a plugging estimator for the posterior mean which is the optimal base estimator with the non-parametric maximum likelihood estimator for this probability measure gn and let's denote the minimum risk risk target which is the minimum of the base risk which is equivalently the minimum of the compound risk xiang and jung 2009 shows that the nonparametric empirical base estimator is asymptotically optimal among all separable estimators what does it mean it means that the regret which is the difference between the risk of using this plugging in nonparametric eb estimator versus the minimum risk target is going to be proportionally vanishing right so this is 01 multiplied by the minimum risk target uniformly for a wide collection of vector alpha so this suggests that estimating gn when we target these uh optimal base estimators does not incur a huge price so the as sample size goes large we are we're doing well now what about variance heterogeneity right so recall that i begin off with the teacher value-added example where inherently we have heterogeneous variants across teachers different teachers teach different number of students there are two ways to proceed to handle this one is to assume that the number of students each teacher teach is independent to his to his or her underlying quality with this assumption we can proceed similarly to deconvolve and estimate the distribution g of the alpha non-parametrically just do a deconvolution under a heterogeneous variance gaussian model so there are technical results that guarantees that that's identified and behaves well we could also consider a richer model where we leverage on the panel structure of the teacher context where each individual teacher i teach multiple students so this number of students play the role as the time dimension in the classical panel data and then we could uh introduce an additional unknown quantity theta i as the variance of u i j and pretend that we don't know about that and we can proceed to just estimate the joint distribution of both the location alpha i as well as the scale parameters theta i this is done um in in the paper with roger where we apply this technique to estimating income dynamics using psid data and there are some further theoretical literature follow-up along that lines okay so now comes the question we have two tools parametric shrinkage and nonparametric shrinkage which one should we use right so in final example the performance difference between the parametric and the nonparametric method gonna depends on the underlying g how does the distribution of the vector alpha look like as well as the sample size so the nonparametric methods offer an adaptive kind of approach right as long as n is reasonably large then it's not going to be much worse off if the g is really can be approximately well by a normal distribution but it can be much better for other shape of g that could be looking very different from the normal distribution and the you here i would give a common uh common wisdom kind of comments where if your n is really small then maybe using parametric method of a more passimonious uh estimation problems might be better off in practice okay but the typical assumption that we have in mind having a large end seems to unlock the benefits of using nonparametric methods so here is just a simulation exercise trying to compare this perspective so i'm generating data from the normal mean model where these there is built in variance heterogeneity so j here stands for 8 students or 16 students taught by this teacher i with equal probability okay so this is just a stylized example where i've tried a variety of different sample size starting from 100 all the way stepping up to 12k so 12k is roughly the size of the applications that we have in mind using these methods so on the left-hand side figure where i'm plotting the risk ratio of either the parametric or the non-parametric estimator versus the risk benchmark where we utilize the infeasible knowledge of the true dgp for g right so on the left figure the true dgp is indeed the g follows a normal distribution well on the right hand side it corresponds to a dgp where we have a mixed of normal and this mixture of normal has the feature that the majority of the mass is centered around zero and there there are two little piles of mass centered around -1 and 1 and this mixed normal is calibrated so that we have the same mean and variance as the first dgp so their mean and the variance will be matched exactly to the normal distribution in the first dgp so the risk ratio here is quite revealing we can see that if you have a problem as small as just n equals to 100 then you are better off using the parametric method right you do pay some price of estimating gn nonparametrically but these gonna converge down to the same performance of the parametric as well as the infeasible benchmark which is one here as sample size much on to be larger on the right hand side the parametric methods having this risk marked out as red never improves right because it's stuck with the gaussian prior and its estimate for its variance and the mean is just going to be as good when n is 100 versus n is 12k versus the num parametric methods really march on to the benchmark as sample size goes up okay and let's see where does that gain come from right so let me give you some visualization of how shrinkage is done differently between the parametric and non-parametric method in the second dgp so if the dgp of g is the mixed of normal on the left hand side i'm printing on the y-axis the data or the maximum likelihood estimator y and the one on the y-axis the different shrinkage method right so the solid black line is how the parametric shrinkage method would work it's going to be linear as a linear function of y it's going to shrink right so it deviates from the 45 degree line which is the dashed black line here the red curve corresponds to the non-parametric shrinkage now we see that it's a lot more complicated right so for instance there is a region of y values that it almost does no shrinkage as well as down below here well as there are regions of y values it does more aggressive shrinkage compared to the linear shrinkage method so on the right hand side here i'm just plotting you how for each different values of y for collection of representative points how shrinkage is being done so on the top here it's as if we pretend that we are always using the normal model as our prior so here we see all values of y gonna shrink back to the common mean zero and the further out you are the more shrinkage you get that's just a built-in feature of linear shrinkage for the non-parametric shrinkage right so it's a bit smarter as in values around minus one and one those are the two little masses of teachers that we had in that dgp is not going to be shrink much they're gonna stay around there well as values further out gonna be shrink less aggressively back to zero but shrink towards those two centers right which makes a lot of sense because if a teacher is really a good teacher which belongs to this mass one if his outcomes is further out in the tail why should we penalize it all the way back to zero so it's all built in automatically in this non-parametric shrinkage method okay so how does it work in real data right so all that is simulation so we did try this out that tried this method out on the los angeles uh school district data so these are primary school outcomes grade 3 and 5 where we have student and teacher matched outcomes and these rounds out to be about 11k teachers controlling for a rich set of covariates we calculate teacher specific mean which then leads to the stylized model that i begin off with we're going to compare the difference between using the parametric methods which is commonly done in this literature versus how does the nonparametric shrinkage would look like so first let me show you what is the estimate for g right so that's a g modeling key ingredients which chip into the num parametric parametric estimator so the black line here is where normality is imposed and the red line here is a nonparametric maximum likelihood estimator for the g on the vertical dotted line i mark out the bottom five percent as well as the top five percent quantile what we see here is at least for the point estimator right we should talk about inference later on of how to put confidence bands on this at least for the point estimator what we see is the num parametric estimator estimates an asymmetric distribution as well as a much smaller tail than the normal distribution right so he thinks that there are not as many bad teachers in the left tail as dictated by the normal assumption and these did influence how the posterior mean would look like so here i'm plotting the parametric and non-parametric estimate the empirical base estimator for the top and bottom five teachers that with risk with respect to their outcomes why i so the nonparametric method is more optimistic about teacher at the bottom right so it's try to shrink more back to zero versus the parametric ones are kind of giving them a worst outcomes top uh five percent not so much difference that's just uh what we see in the la data but the takeaway message is really the nonparametric method of offers an adaptive approach right whatever distribution you have is trying very hard to learn it from the data and then adapt shrinkage accordingly okay having already said i need to tell you how we compute all of these things right so for parametric models we have a variety of methods one can use maximum likelihood estimator or the sure estimators to estimate these unknown parameters in the prior how about the g modeling or the f modeling part so here is a an overview on uh what are the available tools in this at least for this normal me model so we have the f modeling which is proposed in braun and greenstein 2009 as well as a couple of variety of methods for the g modeling to non-parametrically estimate g we have first i will define the nonparametric mle right so we need to define it so what is our target this is done in the early paper by kefir wolfowitz who actually proved consistency of this nonparametric mle we have the well-known and popular method em algorithm which was introduced into the e-con literature by hackman and singer in 1984 who really have this non-parametric mle perspective in their paper we also have the modern approach which is the interior point algorithms proposed by conquer mizera 2014 and that is behind the ie based package that's the machinery that we use in that in that package in estimating nonparametric mle there is also the efference log spline method which is uh the machinery underlying this deconvolve r package so then i'll give some demonstration and then i'll make cautionary remarks on what if g is not point identified right so what should we do in that case okay so just very very briefly on f modeling since we have the 2d formula which suggests that the posterior mean really depends on the prior g only through the marginal density f right because g is kind of hidden in this marginal density f and f only f is the only thing that ships into the base so that leads to brown and greenstein suggesting kernel method to estimate f and f prime now the disadvantage of the f modeling is that kernel method usually does not respect the hierarchical structure of the model kernel method will not respect the fact that the marginal density f comes out from a mixture model as a consequence the resulting estimator using the f modeling in the 3d formula is not going to respect the fact that the bayes rule is supposed to be monotone in y it's easy to show that the first derivative of the posterior mean gonna be have to be strictly positive so using f method you're not going to have that built-in monotonicity in there and then a natural improvement which is suggested in concave misera is to impose shape constraints to enforce this monotonicity however if we have g modeling which will automatically respect all these features why don't we use that right so let's first define what is the non-parametric mle so the nonparametric mle denoted as gn star gn hat here is nothing but the maximum probability measure that will maximize the log likelihood right so here in the bracket i have the marginal density sitting there so that's the usual definition of the maximum likelihood estimator except that we have an infinite dimensional objects here so first for the normal mean model g is identified otherwise we need to worry about that first right that's just the argument in gaussian deconvolution where the distribution of alpha is identified so kevin wolfowitz proved consistency lindsey 1985 further characterized the feature of the nonparametric mle so he showed that the solution exists and is unique i think at least for the normal me model and it's going to be a discrete probability measure means a probability measure that has mass points okay and he also showed that the number of mass points gonna be upper bounded by n and where the mass point are going to be bounded in the interval of the data the minimum of y and the maximum of y there are further strengthening of these number of mass point results polanski and wu show that if the true g is sub gaussian then the number of mass point gonna be of log n so you will see a lot less mass point and that's actually what we experienced in in in usual application okay so and just to remark that this method or this definition of the nonparametric mle is amenable to be generalized to any base density that you would be plugging again right so in the package we implement poisson gamma and so on so forth as long as it belongs to an exponential family lindsay's result on characterizing the nonparametric mle would hold okay so let's first talk about a well-known algorithms to implement this which is also the algorithm proposed by hack to be used in heckman and singer for their viable mixtures in modeling the duration so how we go about it is we first pick a k which tells the em algorithms that i'm going to consider distribution with k mass point then the algorithm gonna iteratively optimize the location and the amount the weights put on this mass point then we're gonna slowly increase k right because i don't know how many mass point i should have i'm going to slowly increase k until the likelihood no longer improves and i'm going to settle down at that estimator to be called my nonparametric mle now this is tough problem anyone who has tried these would would know that this is not easy to do and the underlying reason of it is because this is not a convex problem right because the set of discrete distribution with k mass point is not a convex set if you do a convex combination of distribution with two mass points you might end up with a three mass point four mass point and so on so forth so this is not a convex set so the problem is not convex so you face all the trouble of local optima and all that then you have to randomly start in different places and so on so forth now a key observation is relaxing the finite mixture problem which is inherently built into the em algorithm to an infinite mixture problem actually restores convexity right because the whole collection of probability measure is indeed a convex set okay so it pays off to look at the dual problem which is usually a bit a bit faster to compute so the dual problem of the nonparametric mle you have to take my words for it takes this form it has n variables right v1 to vn but the trouble is it has infinite number of constraints because the dual constraints has to hold for all values of of alpha okay now the trick used in concave mizera 2014 is to suggest that we can take a fixed and the refined grid on for the support of alpha and enforce dual constraints only on the grid pretty much how you would bring an infinite dimensional problem to the computer right you take a grid and then the basically these will give an approximation of the nonparametric mle right so here we're optimized within this probability measure on the fixed grid and this turns out to be again a convex optimization problem since it's convex you're gonna get a unique solutions and you don't need to random start from different places and all that and modern interior point algorithms scales very well with sample size n and it's very efficient and that's what we use in the re based package and under the hook is basically the most sac machinery and this generalizes to a variety of base density in the package we include the different things okay what about uh efference method so the idea of affront is we again take a grid okay so take a u1 to ul as your grid for for the vector of alpha for the alpha let's assume that g has some smoothness let's assume that g has a density little g that belongs to the exponential family characterized by a finite vector mu the dimension of mu is p right so and then we can rewrite the log of the density g would take this parameterized form where the matrix q is of dimension l which is the size of the grid multiplied by p which is the parameter that characterized this exponential family distribution the else entry of the g is going to be nothing but the density g evaluated at the else grid point ephron suggested using natural spine to configure the matrix q okay and so here there are a couple of user choices that needs to be made in using efron's approach the first is how many mu do we have how do we parameterize this so the dimension of mu is a user choice larger p means more flexibility in this class of density function we consider and then we when we optimize for the vector mu ephron suggested a penalized likelihood approach right so here in the first term we have the usual likelihood sum up and then the second term is a penalty terms it's kind of like a square root of rich penalty where in front of it is another tuning parameter lambda controls how much shrinkage we have right so larger lambda would shrink the vector mu towards the origin hence penalize the resulting distribution towards the uniform distribution let's see how it works in practice right so very quickly on two examples first is we have a discrete g two mass point one at one the other at four with equal probability on the left hand side is what's produced by nonparametric mle almost spot on for these two mass points and the right hand side is the implied marginal density in comparison to the truth so the blue is the true density well as the black is the implied nonparametric estimator so they are again spot on so this discrete g environment is really favoring this non-parametric mle because it's it's discrete by nature right for the uh by lindsay's result now efron's approach by construction has some smooth smoothness built in right because he's targeting the density so here um i'm picking two type of tuning so p equals to 5 or 10 corresponds to different level of degrees of freedom and lambda is fixed at the same quantity and we can see that the red one is really trying to mimic these two mass point context and the right hand side you can see the corresponding implied density let's consider a continuous g right and we can see that okay npmle does produce this almost gaussian quadrature looking like mass points these four mass points here with a little bit of smoothing implied just the kernel smoothed on this nonparametric mle leads to the red curve here right so a little bit of smoothing you're going to recover something that is reasonably close to the truth of course that's again a tuning right how much bandwidth and all that and on the right hand side we have the implied marginal density what's revealing is that even though the black is very discrete the g is very discrete once you go to the marginal density it's extremely smooth and actually tracks the truths very very well so the the bait the mixture model really provides the smoothing that is uh doing the job of uh getting you a good estimator for the for the marginal density then this would be the corresponding effluent estimates which of course works uh a lot better here although we again have to choose the tuning parameters okay so just uh very quickly on the remark on what if g is not point identified right this often occurs when the outcome variables is discrete for instance consider the binomial mixture where the y i is binomial with k number of trials and success probability p i these arises as an application in the klein and waters 2021 paper where the y i's are number of job recalls so in this model all we can learn about g is its first k moments right so even if i tell you the population frequency of y we can only learn the k moments of g just knowing the k moments of g not gonna point at nfig we're gonna have a collection of g that are observationally equivalent and if our parameter of interest is a function of g like in their applications the posterior mean of p given y i for instance then it's only going to be partially identified and we are supposed to construct a bound on that so that's more advanced on how to leverage these nonparametric techniques to kind of come up with uh with with a bound okay so uh in in summary uh we recommend using empirical base estimator when collective performance matters not individual performance right so e for each individual estimators they are biased but as a whole they produce a good performance in terms of mean squared error and also we have tools to estimate g and num parametrically and it can be beneficial especially when n is moderately large and uh just a quick slides on probably some of us would have the notion on deconvolution being hard right so deconvolution usually have slow rates in fact in the normal mean model the rate is logarithmic so it is hard problems the reason is because even if for two different g1 g2 very different their corresponding marginal density could be very similar so you have a hard time distinguishing using the data which g it is so that's why it has you need extremely large and in order to kind of have the convergence right okay but oftentimes what we are interested is not the g itself it's some functional of g right so if the parameter of interest results from smoothing gn then performance can be good for instance the marginal density is smoothing gn and we indeed have encouraging challenging hellinger risk bound documented in a sequence of theoretical papers and in general for linear functionals i would defer the reader to to vendor gears he has a she has a chapter on discussing what are the assumptions that leads to good properties of linear functions of this nonparametric mle estimator okay and also learning this distribution g opens doors to other inquiries about the parameter alpha for instance we might be caring about heterogeneity which is measured as the variance of alpha we might be caring about tail probability which again is a functional of g the tail mean can be written as a function of g as well and also learning g opens the door to consider other compound decision problems where this g is going to again show up and as as an underlying machinery to construct nonparametric method so now let me bring you through another simple example on compound decision this is really the robbins 1951 paper so again almost a normal mean model except now we specialize the alpha i to just take two possible values minus one and one before it's situated on the real line now we only have two values in this context maybe absolute error loss is more natural because our decision gonna be minus one and one the true parameter gonna be minus one and one designing in this way we're gonna incur uh loss one if delta i makes a round estimates otherwise zero so again the compound risk gonna be the expectation of the aggregated loss and here the nonparametric distribution of the vector alpha can be characterized by just one number which is the empirical frequency of how many ones we have in this vector of length n okay so again consider compound risk for separable estimator we again have the fundamental theorem of compound decision arise where we connect the compound risk to the base risk where we have a prior pn showing up here where pn summarizes how many ones we have in the vector of alpha so then going through the same idea of we have the base risk let's work out what's the bayes rule and with that loss function the optimal base estimator for any pn gonna take this form and these forms suggest that if your true pn right what you kind of don't know as a oracle knowledge is larger than a half then you basically put a factor that shrink quote unquote upwards right so you you inflate y if the p n is greater than a half and otherwise it becomes a negative quantity so you shrink it downwards so again it has this shrinkage flavor built into the space rule not knowing pn robbins go ahead and suggest why don't we estimate it from the data right a natural method of moments estimator would just be counting how many uh the basically this is can be easily seen as the method of moment estimator for how many ones we have in our data and hanon and robbins 1955 further proved that as long as we have a consistent estimator for pn as n goes to infinity again the regret would vanish right so this is the minimal risk benchmark whereas the left-hand side is the risk of using the estimated non-parametric estimator so we see that when n pn if the truth is a half then the optimal base estimator corresponds to the mle so the maximum likelihood estimator would just tell you estimate one if sine of y is positive estimate minus one if y is negative okay now but using so in that context using the base rule we can show that it's just at most epsilon words off and it's epsilon smaller p1 but for any other value of pn we're going to be much better off using robbing's estimator than sticking to the maximum likelihood estimator which has constant risk and this is being plotted in this figure here where on the left-hand side i have sample size 20. the red curve here corresponds to the mean absolute loss for the maximum likelihood estimator so just the sine of y the black curve here is the robbins estimator where we try to estimate pn and mimics the base rule where the green one is the oracle where we know the truth of pn so we see that with n equals to 20 we have a battery of p values where we we are already much better off than a maximum likelihood estimator on the right hand side one sample size increased to 100 then even at the least favorable case where p is a half we're really mimicking the oracle rules we're not too much worse off compared to the maximum likelihood estimator but much better in other values of p and how does that single robbins compound decision problems connects to multiple testing right if i bring you back to the normal mean problem where alpha i is again on the real line following some distribution g where our interest is no longer the vector of alpha but we are interested to know which alpha i doesn't belongs to a set of value a okay and if we set this a set to just contain the zero value we are testing for significance right we want to know who has the alpha is not equal to zero that's just the testing context so here a key observation is that we transform our parameter of interest from alpha i to this binary parameter which is denoted as h i here which just tells me whether you are indeed in the set a or not right so then focusing on this the compound decision with the following loss which characterize how much loss you occur if you commit a type one error or a type two error so they are weighted by one minus tau and tell then the robin's context starts to kick in right we again have binary uh parameters we want to count come up with estimators that minimize the compound risk so here the distribution of the new parameter of interest is again characterized by one number p0 here which is just the probability of h i taking value 1 which links back to the g on g evaluated on the complement set of a so the optimal base estimator gonna take this form so it's gonna truncate right so we have a tau truncation on the posterior pro the posterior probability of alpha i belongs to the set a given the data evidence y i where the posterior probability is nothing but a function of g right so that leads us back to g if we learn g we would know how to come up with this posterior probability and then we're going to use that to rank individuals and do a thresholding rule to decide who are to be rejected and who are to be not rejected okay so then um the choice of tau right we still have that degrees of freedom the choice of tau gonna leads to different force discovery rate which is the proportion of false rejection out of all rejections so by tuning the tile you're going to have different targets of fdr how does that connect to ranking and selection so in this paper with roger we consider the empirical base method on ranking and selection so think about the teacher context suppose that now we want to estimate the top cue percent performers right we want to know who are the teachers that has their latent value added in the top q percent the parameter of interest then converts from alpha i to this binary h i just like before and then we're gonna rank teacher by the local force discovery rate which is nothing but this posterior probability or the posterior tail probability right looking at conditional on the data evidence y i what is the posterior probability of the teacher eye having a quality below certain quantile and we're going to threshold by given a tau we're going to threshold who are the teachers that we say are in the top cube percentile so the first a remark on the measure we use to rank teacher so if we are in the homogeneous variance context imagine all your teachers teach the same number of students then all their variants are the same then ranking with the mle estimator is going to be equivalent to ranking with any monotone transformation on that because ranking is just ordinal information and we probably have already seen that all these parametric non-parametric eb or local force discovery rate is all going to give you the same ranking because they are all monotone transformation on the mle which is y i however if the variance is different across individuals then different eb methods can produce different rankings so we documented these in that paper second remark on the row of tao so here tal could play two rows right tau controls the force discovery rate the proportion of force rejection out of all rejection tau also controls what's the proportion of selection we make how many teachers we're gonna say belongs to the top quantile so what we did in that paper is to basically design a loss function to achieve two goals one is what we call capacity constraints because we do only want to select q percent and second is fdr constraints because we we want to avoid making too many false discoveries among the teachers that we have selected so then leads to a data-driven way of selecting the towel and then everything holds up together as a ranking and selection approach okay so um very very briefly on empirical base inference this is as i mentioned in the beginning quite underdeveloped so if we look at the normal mean model and if we specialize to the normal prior which is done in the ephron and morris context then the base estimator gonna take this thing perform which we've seen before we can work out the bayesian credible set for alpha i with 95 percentage coverage right so the posterior mean is going to again follow normal then the 2.5 quantile and 97.5 quantile gonna give you the bayesian credible set for alpha i if we know everything about the prior right if we know the parameter a in that prior normal then we can use the bayesian credible sets as a confidence interval for alpha i now the eb context gonna say i don't know a i'm gonna estimate that right so it's gonna have a naive e b confidence interval which is just plugging in replace the a by the estimator a hat but that does not account for the fact that you have estimated a right so these could give poor coverage when n is small and morris in 1983 provides a finite example correction meaning that we add an additional correction terms to account for the fact that we have estimated a now if the prior is not normal then in the recent paper by team armstrong and co-authors provides a robustified eb confidence interval right so the idea is the usual naive ebci gonna have poor coverage if we deviate from the normal prior then the idea is to come up with a different cutoff value the critical value c tau hat here chosen in such a way that we have average coverage right so the average probability of alpha i in the confidence sets is guaranteed to be greater than equals to 1 minus tau now this is great because for people that were using the parametric eb method this is a very natural idea because the confidence interval centered at that linear shrinkage estimator and gonna be symmetric around that and they would guarantee at least in using that criteria of coverage however let me try to end my talk with a compound decision right so an alternative way of viewing the confidence interval construction is a proposal in this comment uh in young 2021 which is to really kind of write it write it down as a compound decision problem right so all we want to do is to come up with confidence interval here the left end gonna be a applied on the individual coordinate y i and right and going to be b a function b applied on y i what we want to do is we want to minimize the average width of the confidence interval subject to the same coverage criteria that is proposed in the armstrong paper right so given these we could write down the correspond we can design a corresponding loss function that would allow someone to find the function a and b which will give you a non-parametric way of constructing confidence interval which again had that coverage property okay so with all that said let me conclude by saying that empirical based methods can be useful for economics applications i just gave one chris gonna give more applications and as we becomes more interested in modeling unobserved heterogeneity in different contexts herbert robbins who is my hero of course is way ahead of his time in proposing the nonparametric empirical base method in the 50s now which with the availability of large-scale individualized data sets becoming increasingly available and modern computational methods facilitates uh the the estimation perspective really hopefully made it feasible to apply his method and of course this is still an ongoing research areas that a lot can be done especially on the ground on nonparametric methods that hopefully this uh this lecture would inspire some interest so thank you very much that's it [Applause] 