yeah just to be clear um there's both lecture notes out there as well as copies of the the slides so the slides are not in the in the big uh set of uh of nodes let me also make one disclaimer these these notes were put together fairly quickly this is not a complete uh literature review and I'm sure that this uh omissions and and mistakes in there though will correct those as we find them and and update um them on the the website well certainly this is a very different econometrics course from anything I've told before typically the first thing I say is there's plenty of room here at the front no need to uh so okay so um let me get started on the the first lecture so the first lecture I want to talk about uh the part of the program evaluation literature which has been uh let me see um this has been very active the last 15 20 years um I want to give a little bit of an introduction talk about what what kind of things uh we're interested in there uh talk about various methods and then talk about additional pieces of uh of analysis uh that you might want to do in in this case and I'll illustrate this uh some of these methods use some data that has been used many times actually in this literature and it was originally put together by uh by Bob Lalonde um this sort of an interesting area where there's been a lot of activity in the theoretical econometric literature but at the same time there's been a lot of uh demand for these methods in the empirical literature not just the uh academic literature but also the the sort of the non-academic Consulting literature but there's a lot of organizations nah such as Mathematica West that where they're doing evaluations of Economic and other programs using this type of uh of methods so the setting is one where we're interested in a binary program so the example I will use instead of a labor market program we're concerned that the individuals we take pilot labor market program may not be the same may not directly be comparable to the individuals we don't take part in the labor market program but we're going to assume that the biases that would be present in in simple comparison can be removed by adjusting for differences in observed covariance so we may see that these individuals would take part in the program have better labor market histories have had higher earnings in the past than the individuals who don't but we're going to assume that once we uh we adjust for those differences we can interpret differences in our in subsequent outcomes as as causal effects of of the program and so there's literature so there's a long tradition uh both in economics and econometrics as well in statistics uh sort of part of what is interesting about the recent literature is that it really brings together a lot of the insights uh from uh from both uh literatures and so that includes sort of a lot of the theoretical econometric uh methods that have been developed in the last uh 10 15 years a lot of semi-parametric methods associated with the work by Nui and so but what is remarkable is that in this case these a lot of these methods have actually been used in practice in fact many more methods have been used in practice than I would probably recommend and so I'll talk a little bit about sort of which I mean I think part of the literature has focused too much on on choosing a particular method of estimation rather than worrying about the addressing the conditions that lead to to differences in in the results from these different estimators and in particular there what I what is what I think has been found to be very important in in practice is that you need a fair amount of overlap in the in the two groups so in the end estimating the effect of one of these programs is going to be based on comparing individuals who went through the training program with individuals who didn't this the key assumption is going to be that uh if these individuals look similar in terms of a bunch of observable characteristics we can make that comparison but in practice we can never find individuals who look exactly the same and ensuring that there is sufficient overlap ensuring that these individuals are sufficiently comparable it's going to be the the key thing and there's going to be a number of things that the researchers ought to do prior to to actually estimating these effects in terms of making these covariate distributions more comparable and so I'll talk about a fair amount about that but once uh once there's overlap issues have been addressed bits particular estimator you use is actually yeah less important the ones that that seem to do best in practice are once they combine various methods of address of removing biases let's see that typically involves either matching or waiting in combination with regression adjustment but really the the one of the two key points uh that I'm going to make in this this lecture is that prior to using the outcome data probably possibly prior even to having the outcome data there's a fair amount of analysis to be done involving just the joint distribution of the indicator for the program and the the covariance so the variables that are traditionally thought of as just right-hand side variables but looking at their joint distribution is very important for assessing whether any of these methods is going to work and which particular methods are going to work well so um let me then set up the the notation which is by now a little bit sort of it's a standard way of setting this up in the literature since the early 90s so we have and units uh individuals in the the labor market program for each individual we think of there being two potential what we call potential outcomes an outcome that you would see if the individual were not to take part in the program uh denoted by y0 and Y one the outcome that you would see if the individual took part in the in the program and it's the difference between those two outcomes that is the the effect of uh of the program now just as an aside this um from sort of 70s 80s econometrics it seems a very unusual way of setting things up but in some sense this is very much the way economists have always thought about uh structural problems and causal questions uh demand function is exactly the same thing but with a potentially continuous argument you think of a demand function as telling you what demand would have been at each value of a price and here the Y 0 y 1 similarly the outcomes you would see at multiple values of a particular covariate um so in addition to these two potential outcomes uh we have we have a number of covariates uh denoted here by XI and the key thing is just that these are not affected by the treatment uh they're often involve lagged values of the outcome but it may also involve permanent individual characteristics in the labor market example these willing where we're looking at the effect of the program on earnings these will involve earnings prior to the program as well as individual characteristics such as age education marital status and so what we observe is the the triple Wy and X where Y is the realized outcome and the fundamental problem is we would like to uh make statements about y1 minus y zero but we only get to see one of those two the same way we may be interested in estimating a demand function but we only get to see demand at the realized price and not at any other price um two other pieces of notation that I'll I'll use later and for this reason it's sort of helpful for for people to to have the slides also for subsequent lectures so um e of X is what is known as the propensity score the conditional probability of getting the treatment given the the covariates I also Define the conditional expectation of the two potential outcomes this mu indexed by W of of x um now let me say something about uh what we're interested in here the the primary objects of Interest are going to be average effects of uh of the treatments and so we may at some point make assumptions that the treatment is constant for everybody that y1 minus y series sustained for its individual but we don't want to impose it in annotation and so we'll we'll only make that type of assumption if uh subsequently but not not that the stage of setting this up so what we will be interested in uh is either the population average of uh of the effect where we think of the units as a coming from some random sample from a large population or we will look at what is called here the sample average effect the average only over there the sample that we actually have in hand now that's in some sense it's a very subtle difference and it's a difference that doesn't matter when we look at estimation when we look at particular estimators they'll estimate both the sample average effect as well as the population average effect it is going to matter when we look at precision the variance for for the sample average effect it's typically going to be smaller than for the population average effect and so I don't necessarily want to make the claim that we should be interested in one rather than the other but I do want to make the point that when you're doing inference you have to commit to you have to look you have to estimate deference for either the population average or for the the sample average because the variances are going to be different in a way that doesn't disappear in in large samples and within General the sample average being easier to estimate being more precisely estimable than the population efforts um now let me talk about the the two key assumptions uh here in the first one I'll refer to as uncomfoundedness and sort of formulated this way um by Rosenbaum and Ruben which says that conditional on the covariates if we look at populations that are homogeneous in the covert the potential outcomes are independent of of the treatment indicator and this is really what uh what's what we mean informally by by exogenity and so to make a link uh more explicit suppose we actually have a linear model for y0 as opposed to the treatment effect is constant uh then we have a additive linear model for The observed outcome Y is equal to Alpha plus Tau times W plus X Prime beta plus Epsilon I and in that case given those additional assumptions and confoundedness is equivalent to Independence of the treatment indicator and the disturbance condition on X which is so we'll be informally uh mean by exogenity part of the reason I'm not I don't want to refer to this assumption in general is actually denied the uh I said it doesn't actually agree with the textbook definitions of exogenite if if you look at the the paper by the angle the Henry Bashar they Define a number of versions of exogenicity and this doesn't necessarily capture this uh this case in addition it is important but it's very useful that we can make the unconfoundedness Assumption without any functional form assumptions we can still transform the outcomes here by taking logs or anything and that doesn't affect the substance of the Assumption where it would do so if we combine it with the linearity and additivity assumptions um let me briefly give some motivation for uh for this assumption that there's been a large literature on this this last literature using this exploiting this assumption but it's also been a fair amount of discussion whether this is uh this is reasonable as you want to make three points uh one is that it just from a data descriptive point of view think again about the this labor market program um we have a number of individuals who volunteer for this program we have a number of individuals we choose not to take part in this program we're interested in estimating uh the effect of the program you could compare the starting point clearly would just be to compare average outcomes into two groups but suppose you see that on average the individuals will go through the program are much older than the individuals we don't it's clear that I would that you would be better off basing the comparison on individuals with the same age comparing individuals to the same age in both the treatment group and the control group even if that isn't necessarily going to be adequate even if that isn't necessarily going to be credible it clearly is a more sensible it is more sensible to adjust for those differences than not and if you were to find that adjusting for something as simple as age took away any evidence of an effect of of the program you'd certainly not think there was uh much credibility to any uh finding of of an effect so so first point let me summarize that first point is that even if this isn't necessarily uh sufficient even if the result isn't necessarily credible as an estimate of the average effect it certainly makes sense after just comparing the the simple averages to see how much that changes if you remove any differences uh in in background characteristics the second argument is uh it's closely related in there but the the point there is that if we're not willing to make this type of assumption then somehow we're going to have to find comparisons for individuals who went through the training program we look different from the individuals uh sorry we look different so suppose we have a woman we went through and the The Loan Data is an evaluation of a labor market program for women suppose we have a woman who went through this training program we had a particular labor market history it was 30 years old who has two kids we had a we dropped out of high school and we tried to figure out what the effect is of the training for that uh that person with no additional information available about this person it would be hard to see how you could do better than finding a woman in the control group with the same labor market history same age same number of of children in the same educational background again it may not be sufficient to uh to adjust for these things but it's hard to see how you could systematically do better by finding someone who looks different how you could systematically do better by using someone who's 36 years old and had three kids and didn't have a high school degree then looking for someone who's exactly the same and the only settings where the settings where we do use alternative analysis we have very strong assumptions about with very credible information about reasons why other comparisons are going to be more credible so in instrumental variable settings that I'll talk about later this afternoon there's going to be covirus that we really don't want to control for and similarly in regression discontinuity settings there's going to be covariance that we don't directly want to control for but it's because there's very specific information about those variables available in most cases like the labor market program I just described you would it would appear you would be much better off making these people comparable along observable dimensions and absent any of these assumptions you wouldn't be able to estimate the average effect you would only be able to get get bounds I'll actually talk about it on tomorrow um final argument is that Uncle founded this is certainly consistent with uh with optimizing Behavior by individuals if individuals choose to participate based on uh some expected benefit from it uh as long as that if involves some differences in incentives or differences in costs that I am related to the effect to the effect of the program then you may well find data where individuals will look the same in terms of observable characteristics make different choices in terms of program participation but they can be made comparable by conditioning on the on the covers so here the point is just that unconfoundedness is certainly consistent with some with optimizing economic behavior even though it obviously isn't uh consistently but all by such behaviors um second assumption that I'm going to make throughout this uh this lecture that I'll come back to this this one as well because this is one of this testable and may require some adjustment to the data is that we're going to assume that condition on the covarence the probability of receiving the treatment is strictly between zero and one this is this just means that for all values of the covariance there's going to be the individuals we will receive the treatment and the individuals who don't if you find that for some set of x's the probability of receiving the treatment is zero there's obviously very little we can do in terms of estimating the effect of the program for those individuals and unless we're making extrapolation type assumptions or assume that the effect is constant we cannot say much about the effect for for those subpopulations um given these two assumptions we can actually estimate the the average effect in principle yeah we can Define what's called a Tau of x then as the average effect of the treatment given the covariance by and confoundedness that can be related to uh The Joint distribution of the observable variables uh y x and w and by the overlap assumption we can actually estimate these conditional expectations because there will be individuals in in each of the the groups give an identification of Tau of X we can estimate the population average by just averaging that over the sample distribution of the coverts so with this uh what this leaves us is that we now have a well-defined statistical problem we can estimate Tau of P or we can identify Tau of P the question is what to do in in practice and this is really what what's been the subject of substantial literature both in statistics and econometrics what are sensible ways of estimating uh types of sub p now before now getting there let me make two more assumptions about two more comments on on the actual assumptions bonus in principle for most of what we are interested in uh namely these average effects we could weaken the um assumptions a little bit one way we can we can damage instead of assuming conditional uh Independence we could just assume that the conditional mean of the potential outcomes doesn't depend on uh on W that is clearly a weaker assumption but in practice my my view is that this is it's actually very rare that you can make a credible case for the weaker assumption without the case being just as strong for the strong assumption making the Assumption only in terms of conditional means really requires you to commit to a particular functional form if it holds in levels this assumption wouldn't wouldn't hold in in logs anymore unless it unless the full conditional Independence version of this uh this holds um second point and that that so so in my view this weakening of the Assumption isn't all that uh they're useful what is more useful is uh that if we're interested in the effect only for the treated we don't need to join Independence of the two potential outcomes it's sufficient to just assume that uh y zero is independent of uh of w given uh given the X's given that we observed by one for all the individuals that we care about that we only need this for the the potential outcome under the control treatment um one important result in this literature and one that analyze a lot of the the estimation methods that have been proposed and have been used it's uh the propensity score result uh if I'm confined in this whole so even subpopulations that are homogeneous in the coverts we have Independence between the potential outcomes and w then it's also true that if we condition only on the propensity score we get this in condition we get this Independence and in principle that's a very powerful result uh if you have a 20 dimensional uh factor of covarence and we often have a lot of covariates to make the uncle found in this assumption more plausible this says we don't actually have to adjust for the differences in all these covariates we don't have to look for individuals with the same along all these Dimensions it's sufficient that these individuals are the same on one particular measure on a scalar measure of of all these covariates so we don't have to to going back to this example we don't have to look for a woman who's 30 years old two kids as a high school degree and had a particular labor market history we just need to calculate the summary statistic summary measure of these characteristics and we need to find someone who looks the same along that Dimension even if they differ in terms of of all their characteristics separately and so one of the big challenges in this literature is that it's going to be difficult to do non-parametric adjustments in high dimensions and so in principle this seems like it gives you a way out because you only need to to adjust for a single variable the problem of course is that in practice we don't know what the propensity score is and so rather than solving the problem this really shifts the problem that it's still an extremely useful result there and it's it's going to help us motivate different strategies but it doesn't directly solve the problem unless we were to know the propensity score which is a case of very limited interest okay um now I'm going to give one technical result which which will be extremely useful later for comparing some of uh of these methods so making only this uh these two assumptions and confoundedness and and overlap we don't need any functional form assumptions we could make parametric assumptions on on the regression function or linearity assumptions but in principle we don't actually need any of those um what what the enhance showed us that you can estimate the average effect in the population at the standard wooden rate and you would say get a variance that is equal to uh this expression one on this uh this slide this depends on the conditional variances and the the propensity score and the main reason for for it giving this hairs because it will come back later and then when we look at these estimators but what you can see here already is that it's going to be very difficult to estimate the population average effect if the propensity score is close to zero or one what would happen in that case is that these two ratio one of these two ratios would would get large and no there would be no estimator sort of with a relatively small variance and so this is where the overlap assumption is going to be very important limited overlap in the covara distributions corresponds to the propensity score being close to zero or one okay so now um beginning to get uh um to more practical part I want to look at some of the estimators that have been proposed and I'm grouping them in four categories set estimated regression estimators that estimate these conditional mean functions matching estimators and estimate is the use of propensity score and various combinations of of these three things and in the end the I think the the recommended thing uh in the current state of the literature is to combine regression with either matching or or propensity score waiting and that leads to more robust results as I said at the the very beginning once there's sufficient overlap the differences between these estimators should be relatively small and I'll actually illustrate that with this lalon data um but let me just make a couple of comments about each of these uh this set class of estimators themselves so the regression estimate is uh what I mean by that is the estimators that are based on first uh estimating the regression function mu of w of X so what you would do is just take the treated observations estimate say a linear regression function predict the regression function at the sample value of of the covariance take the difference with the regression function for the controls and average that whole thing over all the the covariates a very simple ways of implementing that uh the consists of having linear specifications for these uh two regression functions possibly even with the constant treatment effect in this case you're back to just doing OLS and in the end that that's probably that's that's very likely the most common estimator for uh for this case we just try to remove biases coming from the coverts by including the the covirus in the regression uh linearly and additively you could generalize this by having uh different slopes for the two groups you could have higher order terms and so there's a lot of theory there's some work by uh Chen Hong and tarotsi looking at estimators where you slowly let the number of covariates increase and they show that during that gets you to the efficiency bound you can do the regression using kernel methods and again you can you can get consistency there a bit of principle all these methods work by estimating these two regression functions and really the the and the main concern with those estimators is that it can be very sensitive to differences in the covaria distributions in the end these estimators rely very heavily on on extrapolation right to think about it is we're going to use the regression function estimated on one group to predict what the outcomes would have been in the other group and so if so it's supposed to be just at a linear regression function as opposed to average value of x in the treatment group is X bar t now we would predict that that the average outcome in the control group plus the difference in the cover at times said estimated regression coefficient now if these two groups are very close if the covariance if the average is of the covariance in the two groups are very close it doesn't really matter how well you estimate beta or how accurate the regression specification of the regression function is but you get into trouble is uh if these covariates are very have very different means in the two groups at that point the regression you rely a lot on the correctness of the regression function in an area where you don't have a lot of observations and so as a result you may end up with estimates that look very precise even though they they're not necessarily very credible just to Assurance it's very easily um you can see this very easily um um here I plotted two covariate distributions the one for the the controls on the left the one for the treated on the right and I fitted two regression functions on the control observations and so you can see they fit both fit very well in the Distribution on the left but their predictions for what happens in the support of the covariate Distribution on the right are substantially different and so the point really is that if you're using these regression methods you should be very careful if the covariate distributions are very different um now let me move on to the next uh estimator so here the the notation so the formula listen is slightly more Messier but the idea is is very simple and that's the Simplicity of the idea is really what led to the popularity of of these methods in practice here what we do is uh if we're if we have a particular treated individual so going back to the example uh this 30 year old woman with two kids with a high school degree will be Lucas uh what we do is look in the control group in the control sample for someone who's most similar to that person you just look for for someone with with the most similar in terms of all these characteristics ideally we would like to find some of it exactly the same characteristics that is that that may be difficult if you have a lot of coverts and so we look for someone who's the most similar in terms of all these characteristics where we Define some distance measure in terms of of these covariates and now but in PR in practice the the choice of the distance measures now is the results are not all that sensitive to the choice of uh of the distance measure you can do that with one match you can do that uh with M matches uh so you look for the M closest uh matches then you impute the missing potential outcome with the average for those uh those matches and you average the difference between each pair so now for each observation we look for one or more close comparisons you take the average of the outcomes for that close comparison we use that to estimate for that particular individual the effect of the treatment and then we average that overall overall pairs so this is extremely simple uh idea and it's them and I think that's that's added considerably to its appeal promise is that it doesn't really get around the the complication of having a lot of covariates and the the formal way to think about it is that there's obviously going to be a bias Associated uh with uh with this estimator but the problem here is that this bias uh goes to zero only very slowly if you have a lot of coverts if K is the dimension of the covariance the biases of what of the order n to the minus one over K where n is the sample size which means that in large samples we don't get a normal distribution center that uh at the True Value we'll get a non-normal limiting distribution centered at some complicated expression I said well in practice what people do is ignore the the bias and just calculate the variance and hope that the bias is small but it's not always clear that that's that actually works well so rather than doing matching on its own now I think that the the sensible thing there is to do to combine it with some regression adjustment and I'll come back to that in a bit um final uh sort of single class of uh of estimators is based on the on the propensity score here I'm just looking at one way of implementing that the key uh idea here is that if we take treated observations and weight them by one over the propensity score you can get the marginal expectation of the outcome given the the treatment the marginal expectation of uh of y1 and we could use that by averaging the treated and control observations where we wait the treated observations by one of the propensity score and the the control observation by one over one minus the the propensity score so here we don't need to use we don't use the code virus at all the other than the way they come in through the the propensity score we remove any bias we remove all the biases associated with discoveries by just rewriting the sample in a way that the weighted sample has the same distribution of the cover the weighted treated sample has the same covariate distribution as the weighted control sample and so by the once you remove the differences between the covariance distributions it makes sense that you don't need to adjust for that anymore it's just like the old omitted variable bias result that if the omitted variable is uncorrelated with the the included one dropping it doesn't affect the the estimator and so um you can in fact get an efficient estimator doing this by estimating the the propensity score and implementing it in the following way problem still is that this estimator relies very heavily on getting the propensity score right and so in practice there is a that is a very difficult object to estimate for for this purpose and so again using this estimate on its own is not something that would really uh recommend Vada with combine it with the type of regression adjustment that I'll talk about in a second um then there's a there's a couple of other estimators that actually were actually the the properties have been less well established even though they have been used in in practice namely matching and regression on the propensity score um in particular with regression on the propensity score um I think that the serious issues with that remember that recall that the propensity score is sort of a purely statistical uh object here it's a conditional probability of uh of receiving the treatment uh given the covert something that's by its nature is between zero and one there's no particular reason why the outcome the conditional expectation of the outcome given that should have should be well approximated by a linear function in particular if the propensity score gets anywhere close to zero or one individuals with the propensity score say of 0.05 and 0.01 are clearly very different one of them has has twice the chance of being in the in the treatment than than the other that's a much bigger difference than between 0.55 and 0.5 and trading sort of putting this putting the propensity score and linearly or even doing it in a more flexible way really now analyzes the data as if the propensity score sort of has more substantive meaning uh than it than it has so I see very little reason for for using those uh those estimators and I haven't seen much evidence that they do particularly well over either okay let me then get discuss uh briefly two of the things that I I think are much man more likely to work well and practice bonus to think of the The Waiting estimate the Harvard stompson estimator as uh not so much in terms of taking a weighted average but in terms of uh doing a weighted linear regression in the end you can interpret this estimate as doing a linear regression of uh the outcome on a intercept and the treatment indicator with the the weights depending on the propensity score and the the treatment indicators once you have this weighted linear regression interpretation it's a very natural thing to also include the additional covers even if they're not needed for bias removal uh if you have the propensity score correct they may still help you get get more efficient and in particular more robust estimators if these covariants are highly correlated with the outcomes they can they can make things considerably uh more robust so a more sensible thing than just using the waiting uh is to uh to do the weighted linear regression but also functions of the covers are included and now there's some work by bobbins and a rid of showing that if either the regression function if the conditional expectation of Y given X and W has the form that is used in this regression or if the propensity score is specified correctly then these estimators are consistent so they're going to be much less reliant on the propensity score being correct or on the um the regression function being correct you just need to essentially you get two chances to get to get lucky here and in practice what it means is that these estimators are much more robust and tend to work better similarly you can combine the matching with regression but instead of one after you've constructed the matches instead of just taking the simple difference in averaging that overall coverts you adjust it for some for some of the remaining differences in the covarence by now by the very nature of the matching the co-parents should already be very close but if there's still some differences left removing that with regression is is likely to uh to improve things considerably and so in both of these cases with the waiting and the the matching the regression is asked to do a much more moderate sort of it's sort of given a much more modest task at this point these differences are already much smaller and so you're never going to rely as much on the extrapolation as you do when you do the regression on the on the full sample and that that's that's really what is extremely important in cases with uh with limited overlap in the in the coverage linear regression that's perfectly well in approximating most smooth functions over a limited range but it doesn't do necessarily do very well approximating things globally and or if the converters are very far apart okay um another practical uh concern um I'm going to make a couple of comments on on estimating the variance now so a bunch of these estimators are known to uh to reach the uh the efficiency Bound in that case estimating deference in principle is just sort of conceptually straightforward we just need to estimate the um all the components of that that various expression ideally non-parametrically and just plug them in fact for the estimators that are known to uh to be efficient so these these flexible regression estimators or the The Waiting estimators you can also use the bootstrap and that is very likely to work well the bootstrap doesn't work for for matching estimators because they they are non-smooth to uh to a relatively High degree and so they're matching that so bootstrapping doesn't uh doesn't work but even for these other estimators uh this is in principle somewhat uh complicated way of of calculating the variants and I want to talk rather than than do this I think in a lot of cases we can exploit the structure of this problem a little more and get a there easier estimator for the variance in the end all these estimators are going to have the feature that they're linear in the outcomes all the estimate is sort of clearly for any estimator to be sensible here they need to be of the form that it's a weighted average of the outcomes given the treatment minus the weighted average of the outcomes given the controls it's sort of very hard to think of anything anything else you could end up with uh given that we can write this estimator it's just a weighted average of of all the outcomes with the weights the way it's a function of all the covariates and all the treatment indicators so here denoted by this bold face X and and W but this function is something you know once you've committed to a particular estimator you can calculate what this function looks like then if you look at the conditional variance now conditional on on the coverage difference is just the sum of the squared weights times the conditional variances and so now there's only one component we don't know namely this this conditional variances and even though we now don't know them and estimating that function consistently would be very hard it's very easy to get a get a good estimate for that that you can plug in there so here there's one suggestion for doing that namely if you want to estimate the conditional variance for a particular observation we just look for the closest observation to that particular one look at the difference in the outcomes squared and divided by 2 and that gives us a very simple estimator for the conditional variance that you can plug in to that expression for V to get a consistent estimator for the the for the variance of the overall estimator irrespective of which type of estimator you actually use so this variance estimator would apply equally to the weighting estimators to the regression estimators to the the matching estimators or any of the the combination ones okay um now the next couple of uh slides I'm going to make um go back to the assumptions and suggest some ways we can we can see whether they're they're credible and whether in the end any of these analysis are going to give us incredible results the first thing to uh to keep in mind when we talk about and confoundedness is there is not a testable assumption it tells us that the distribution of the outcomes in the control group is the same as the distribution of the alcohol in the treatment uh sorry the conditional this for treated individuals the distribution of outcomes of control outcomes is the same as what it is for for people on the control group since we never get to see the control outcome in the treatment group there's nothing testable about that it's essentially imputing this the distribution we cannot learn anything about directly with one we can estimate directly nevertheless there's sort of two ways we can think about uh assessing whether this is uh it's plausible um both which involve having a particular type of data um so here here's is one uh method that sort of this this goes back to some work by uh by Hackman and hotz and uh by Rosenbaum the ideas that we may that sometimes we have multiple control groups and in the labor market program a prominent example of that is that we may have individuals were not eligible for the program and we may also have people who are eligible but we chose not to participate now if you're willing if you're willing to extend the uncomfort in this assumption to assume that both of those potential control groups are in fact valid both of those comparison groups are valid then what we could do is look at the two comparison groups and see whether any of these methods removed the differences between them if in fact it is true that conditional on the the coverts the potential outcomes are independent of whichever group these individuals are in then it's also the case that given that these individuals are in one of the control groups the control outcome should be independent of which of exact group they're in and that's what we should be able to do is compare outcomes in the two groups conditional on the coverts and the way we can do that is exactly the same way we estimate the average effects for uh for the program in the first place we can look at these groups apply one of these estimators and we should get get something close to zero if you get something far away from zero that suggests that these methods are either not working in removing these differences or the assumption is is not correct in both cases you don't want to just go ahead and apply this to the to the treatment group until you actually understand why you find differences there it's a given a sufficiently good specification well given a sufficiently good estimator if you still find differences it means that at most one of these comparison groups is is valid and unless you have you have prior information which one this uh no it's correct it still suggests that at this point you should uh just stop and say that you can't find um there's no credible way of uh of coming up with estimates um the second uh method and that has slightly I mean it doesn't rely as much on having additional data it's um is to use some of the covariates as uh as outcomes so again the idea is that you use these methods to estimate the effect of the program in a case where you know what the answer is so um and the most powerful example of that is in cases where we have lagged values of of the outcome and so I'll illustrate this later with this the alone data but to think of uh of having lagged earnings uh no what we could do is pretend the program started a year earlier than it did estimate the effect estimate the differences between the the treatment group and the control group in terms of this uh of this earnings one year prior to the program if we find that there's uh big differences in earnings one year prior to the program even after we adjust for all these other coverts the sort of two possibilities one is that is last year of earnings really is very important even after we adjust for all these other coverts possibly even after we just for other lacked outcomes but somehow this last year is still extremely uh important or the other alternative is that the unconfoundedness doesn't hold and so in cases with sufficiently Rich set of covariates if you find that any one of these covaries is still crucial for for removing uh biases you clearly uh in a somewhat uh difficult situation and the results would be much more credible if you find that taking away one or two of the covariants doesn't change things very much so the specific test here would be to just take the one of the lagged outcomes and estimate the effect of the program on that using only the uh the other covarets and finding you know precise estimates of zero would be encouraging that the assumption is is more credible than if you find big effects and again I'll illustrate that in a little bit with the lalon data see the the second assumption I want to uh look at and this is one that's actually extremely important in in practice and in particular in um in this land example um it's the the concern about uh lack of overlap in the in the covera distributions now if you just have a single covet we can do this uh directly you can just plot the the distribution of uh of the two covers you can see whether these are very close now in high dimensional cases that's sort of more difficult you could just do it uh one covered at a time but uh that's not necessarily sufficient it could well be that for each coverage separately the the two distributions have a lot of uh overlap but um if you look at the Joint distribution it could be complicated correlation patterns uh that lead to to lack of overlap in their in particular areas into leading to cases where to areas where the propensity score is very close to uh to zero and one so really the way to uh to look at the overlap is to look at the distribution of the propensity score lack of overlap corresponds to the propensity score being close to uh to zero and one so you don't even need to uh to plot to covariate distributions directly just looking at the distribution of the propensity score itself is separately or or jointly and checking whether there's a lot of mass in there for close to zero or one would suggest there's a lot there's there's problems if you find that there is a lot of this concern for for lack of overlap um this is this is the point where we're choosing these estimators becomes a Much More Much trickier thing and so I want to talk about one way of uh of dealing uh with that um and so but in practice it's actually first let me now make a very extreme case suppose you find that there's a large part of the covariant space where the propensity score is exactly zero May disputed the smart group that just doesn't take part in the in the program it's clear that you can't do anything for that group you can't estimate what the effect is is for that group and so in the end what you have to do is just put them aside estimate the effect for for the remainder of uh of the sample and just acknowledge that it's for for a particular sub sample you can describe what the sub sample is that this average refers to and acknowledge that the other group uh that you don't can't estimate very well what the effect is for the other group now you can take that a little further and you can think more generally about estimating the effect for some sub population had denoted by this this set a and you could think about just everything only the in over the individuals in that particular sub-sample and so if you look at a sub-sample uh you look at how well you can estimate the Tau for that particular sub sample by by inspecting the implied efficiency bound it'll look it looks something like the expression here on this uh this slide and so what you can see is that it may be very helpful to rule out observations when it propensity score is very close to zero and one that's what people typically do in practice already by just throwing out either a couple or all observations where the propensity score is is below some some cutoff point but so one way of formalizing that is by thinking about looking for the sub sample we get the most precise estimates and if you do that it turns out the optimal set has exactly the the form that corresponds to what people do in practice maybe throwing out the observations where the propensity score is very close to zero or very close to one effect it's the interval where you throw them out it's symmetric on both ends you throw out observations where the propensity score is less than Alpha as well as the observations where the propensity score is greater than the minus Alpha 90 now you can take this little further you can figure out what the optimal value is uh for Alpha for for optimizing the Precision uh that depends on what the marginal distribution is of the propensity score but in practice that's actually not all that important uh and it turns out that uh that just using Alpha is 0.1 throwing out observations where the propensity score is less than 0.1 or greater than 0.9 gives you typically a much more precise estimate and one that's very close to the most precisely estimable object so there's a couple of comments on this uh as I said before this does more or less correspond to what people do in practice but it does in principle change what you're estimating sort of after looking at part of the data after looking at the this is without looking at the outcome data after looking at the coverage and the treatment indicator now this suggests that you're going to be able to do much better in terms of variance by focusing only on a particular sub-population because the other part of the the sample is not very helpful for estimating average effects it's in a very extreme cases where part of the the population is only in one of the two groups if you find that uh there are men are only in the the control group not in the treatment group it clearly doesn't make any sense to try to estimate the average effect for men but this is just even if uh there's a non-zero proportion of men in the treatment group but it's very very close it's very close to zero you may still be better off by putting them aside and just estimating the effect only for the the group where there is uh overlap okay so now let me uh spend the remaining time illustrating some of these methods using uh the loans data that the Lalonde originally used in a paper in the the mid 80s this particular sample is uh is from a paper by uh Baba and so this sample is actually um it's on the HS website so you can look at it look at some of these methods it's been used in a large number of papers including Heckman holds some papers by Smith and Todd as well as the Asia Baba paper so what is the setup here are the randomized experiment very data from a randomized evaluation of a job training program a fairly small experiment 260 controls 185 trainees in this particular sample it was a randomized experiment there so it was very easy to evaluate the effect of of the program at least to get an unbiased estimate of that effect but what he was interested in uh what makes it this is a very important paper in this literature he uh wanted to use that as a way of evaluating how well various of these evaluation methods worked so he took he said he put aside the control group from the experiment he got a comparison group from the current population survey as well as one from the PSID that I'm not looking at here so he took this very large comparison group from the CPS he said well let's see how well the various methods work in in replicating the results from the experiment so here essentially we know the truth or at least we have a good estimate of that let's see how well these these methods work if we actually apply them in a setting but we don't know how well they work they may differ but we can't tell we can't rank them here we know what the truth is let's see how well they they work and so um and sort of his conclusion was that none of this methods methods worked uh very well and this just sort of led to a lot more emphasis on that on experimental methods since then sort of there's been more the literature has has progressed and there should have been more nuanced versions of uh of the conclusion the the basic conclusion that it's very useful to have experiments I think still stands for a but let me mainly want to use this sort of partly to show what the differences are between what what I think would be now a lot of people who do now and support will stand in there in that time and so the first thing yeah I would do look at this data it's in addition to uh looking at the averages for the for the coherents look at the difference in the efforts scaled by the the standard deviation so not so much looking at the T statistic for whether the the comparison group differs from the the experimental group that but looking at the the difference in the means scaled by the standard deviation uh the trouble with looking at the T statistic is that if the sample sizes get large and larger the T statistic we always get large and that's not really the issue the issue is how different these uh these two groups are and what you see is if you look at the the experimental groups the scale differences and the means and never more than than point two yeah in fact this this will be zero but obviously we're estimating these things but they're all they're all fairly small if you look at the scale differences between the trainees and the CPS comparison group you see these are very large now a lot of them are more than than one there's one as big as 2.8 um but so for the earnings variables they're also well over uh one if if you find differences more than half a standard deviation just using linear regression with putting by putting in these coefficients linearly is potentially very sensitive to the to the exact specification so if you see this at this point now the conclusion should be that there's no that just doing linear regression is never going to work there's no point in even looking at the results and again this is before we see any of the outcome data but just doing linear regression with such big differences uh would rely so much on extrapolation that it's just very hard to uh to believe that this uh could lead to Credible results now we can illustrate that another way if we take take this full sample and we do the we do the tests for assessing uh unconfoundedness by using earnings in 75 as the outcome so the actual outcome here is earnings the 78 which which I haven't actually shown yet but it's still uh matches of uh of earnings prior to the program earnings in 75 and earnings is 74. there's actually not quite yearly earnings it's I think it's a little more complicated but I'll call them earnings to 74 and earnings is 75 for here and so I'm using earnings in 75 as the pseudo outcome I'm going to look take for that tree that is the outcome use all the other use the the seven other covariates as uh as controls and I'm going to use a variety of of these methods that I talked about before and see how big a difference we find for the effect of uh of the program on earnings in 75 which I know which we know has to be zero this is earnings imagine in thousands of of dollars and in the end the the overall effect of the program is on the order of two thousand uh so there would be two on this uh this scale um so if you do this for the experimental uh data using the experimental controls and the the trainees looking at this uh it's uh 11 different estimators they're all on the order of uh between zero and uh and 270 none of them have a t-satistic uh greater than than one so this includes just taking the simple difference which would work because we just have a randomized experiment doing OLS but just a treatment dummy and coherence doing all or less with separate regressions for the two groups doing propane Cisco waiting during propensity score blocking which I didn't really talk about but this is another way of exploiting the propensity score by dividing the sample into strata depending on the value of the propensity score and just taking the differences within there propensity score of aggression so where I just put in the propensity score as an additional regressor as a regressor in addition to the treatment indicator matching on the propensity score matching on the coverts waiting and regressing blocking and regressing and matching and regression and so here this is sort of this illustrating the point that made the beginning if things are well behaved these things should all work fine and here they all do work fine now this looks this is very different if we look at the if you use a CPS comparison group so same set of covariants without taking the sample of 16 000 individuals in the the CPS we're sort of going back here clearly are very different but now if we make this assumption what we find is for all these estimators we find big differences we find big effects on earnings in 75. with the T statistics all in the other and now we are interested in the test statistic we're interested in seeing whether we find statistically and economically significant effects on this uh the pseudo outcome and we do both statistically significant and the the is substantively Meaningful they're on the same order of magnitude as the actual effect of the program and so doing this in some the the doing this would suggest that just using the CPS comparison group with any of these estimators would be very unlikely to lead to Credible estimates so again this is before looking at any of the outcomes before looking at any of the estimates of the actual fact of the program if you see this you should be very concerned and you shouldn't just apply these estimators to the to the actual outcome data this in some sense this this is not surprising given the sort of the normalized differences from the previous slide looking at the last column here we see these uh huge differences we should already be concerned there now for the pseudo outcome we see uh big effects we should be even even more concerned so um the next thing I I didn't have this data is uh is try to to construct a sample that had more overlap in the in the covarence and so estimating the propensity score and I thought I had a transparency with the the profanity score distributions but um that's not in here I think it's in the it is in the notes I'm pretty sure um you see that for the controls the propensity score is is very heavily concentrated at zero it's not surprising um let me just go back for a second that's not surprising most of these individuals in the CPS are very very different from the the Target for this labor market program the people in the we went through the labor market program now had low levels of Education had very low earnings in the Years prior to the program but mostly unemployed if you look at the CPS individuals who are making uh vastly larger sums of money every year most of these individuals can't you can't imagine them being in these uh these programs and they clearly are not helpful for estimating the effect of uh of the program and the way that shows up in the statistical analysis is that for most of these individuals in the CPS the propensity scores are extremely small they're they're essentially zero and they're only positive because of the loading model that is used to estimate the the propensity score so um estimating the propensity score then and looking at the the observations in the Tails we find there's very there's no individuals with the propensity score greater than 0.9 but there's a vast number of individuals with the propensity score greater than less than 0.1 in fact out of the 15992 controls the 15979 have a propensity score less than than one and only 313 have a propensity score greater than 0.1 there's also a couple of trainees with the propensity score that small but so if we we follow this recommendation of uh throwing out the individuals whose propensity scores is less than 0.9 in this case we're throwing out a very large proportion of of the data but what we are left with is a sample that's actually comparable the sample where you can actually where you have some chance of of getting incredible estimates so let me then go back to the the first table but now for this modified sample if you look at this uh the selected sample and again the important thing to keep in mind here is that the selection here is purely based on the on the covaris there's nothing so far that has involved the outcome data and so there's none of the the pre-test bias issues that we often need to to worry about but here for the selected sample we get a much greater amount of overlap now the normalized differences between the two groups and here again it's important that we look at these normalized differences rather than a decent it's the T statistics obviously would be much smaller because we have a much smaller sample but the normalized difference is when we normalize better standard deviation and now all below 0.5 with most of them being on the order of of 0.2 or to between point one and point uh 0.3 and so now we have a sample where a lot of these estimators have a much better chance of actually uh giving us a credible estimates uh now we're not extrapolating uh way out of sample to predict the outcomes uh before the outcomes for the other group whereas before when you're using the full CPS sample you're essentially using these individuals with twenty thirty thousand dollars worth of earnings and you're trying to predict what the earnings for the treatment group would have been given zero prior earnings and so it's the extrapolation there that makes that makes that analysis uh very uh of very low credibility but now here and sort of doing this the same analysis for the 75 outcome as well as the analysis for the the for the full outcome you see that for for the 75 pseudo outcome pretty much all these estimated all these estimators have t statistics less than uh than 1.4 all these estimates are substantively fairly close uh to zero and and give reasonable uh and so they look uh very reasonable here Now using that same sample and applying the same 11 estimators to the to the 78 outcome even the simple difference here gives a reasonable number but certainly you looking at the waiting and regressing estimate or the matching and regression estimator you get numbers that are very credible and that are reasonably precise but the main thing here is a lot of these estimators now work fairly well once uh once you have a sample that is uh but they they have a reasonable chance of of working so um summarizing the the um no that's the same thing summarizing the the points here now what I think is key in in doing this analysis is to do a large part of the analysis prior to to looking at the outcome data checking the joint distribution of the treatment indicator and the covariance to see what uh to see whether any of these methods are going to have a chance to work that involves both looking at the overlap and the the covariant distributions as well as in assessing and confoundedness uh how well you can assess down co-founders is very context specific often in these labor market programs we have reasonably long labor market histories and so there these methods have these assessments have much more credibility but if you didn't don't have any uh liked outcomes that part is not as as valuable but only once you've established that that these things are going to have some credibility so you look at the outcome data and then the choice of of the particular estimator is uh is of uh of considerably less importance don't practice using the regression in combination with uh with matching or waiting to improve the robustness it's probably the most practically useful way of estimating these effects okay that's that's another question there yes um yeah so what I was looking at there was the the ratio of the difference it means to the standard deviation so so here this was um just calculated here as the 25.8 minus 25.1 divided by 7 standard deviation is pretty much the same and so the exact way already able to do the standard deviation on the full sample or the sub sample doesn't really matter the the key thing is to it's not so important whether these distributions are statistically different looking at the T statistic isn't very helpful what you want to see is get a sense of whether the methods for adjusting for these differences are likely to work and when these differences are really big you need to be sensitive to the the peculiarities of the estimators and once these differences are big you want to make sure to in the the Lalonde example with the the CPS being so different from the experimental group you want to make sure you use an estimator that doesn't that essentially ignores these observations with very high prior earnings you can do that through waiting now you can do that through matching you can do that in particular ways but you need to be cognizant of the fact that you need to to ensure that and simple linear regression that that's the part of a simple linear regression uh performs very poorly yes overlap you mentioned that a lot of these estimated from the propensity score and that compensity scores are difficult it's a typical object estimate estimate or how do you choose yeah there's a couple of things in there sort of the so yeah I see the the question was about uh the role of the propensity score here and the fact that a lot of these these procedures in some place or another rely on having an estimate for the propensity score and how sensitive things uh things are to that so the first thing you want to do is make a distinction uh between estimators like this the simple waiting estimator that relies very heavily on on having the correct propensity score or at least a good estimator for the propensity score and this there at other places where the propensity score is uh is used like the selecting the sample um when we sort of when it's used for selecting the sample or when it's used for waiting in combination with the regression the sensitivity is uh is much less I mean in principle the the method I described sort of would give this optimal cutoff point but given that that you know optimal cutoff point the derivative of zero essentially and so small deviations are not going to are going to be second order so so they're the specification there isn't all that uh that critical now that doesn't take away from the fact that you do need to estimate it uh in the end for the purposes it's being used for our hair this is mainly trying to reduce sensitivity and trying to to remove bias rather than than primarily for efficiency I think you want to use a relatively flexible specification which in practice means to for the for what I did for the estimators I reported here I just put in all these coverts uh linearly that what I now and even then it didn't matter all that much if I put it put in some quadratic terms even though these groups were very uh very different um in practice you might want to play around with that a little bit by by putting in some of the high order terms I wouldn't be all that concerned about uh biases arising from from this sort of playing around with the specification there as long as you do it sort of in all in this analysis prior to involving the the outcome data so um um Susan for for now in this case I might also have have tried to put in all quadratic terms I think there were too many for for that to work very well but I would be happy doing step wise type procedures where you keep including coverts till the T statistic uh for the next month is uh it's less than some some cutoff point so I think I think two is a reasonable number to use um there but um but I wouldn't be that concerned with the sensitivity there unless you're using the the simple uh waiting estimator then you I think you would be much more concerned I know that's partly the reason I would not recommend uh using that in practice yes but is it a propensity score is sort of something that once you have to data it's sort of it's a it's a statistical object it's not I mean it I think the you would like to get the biases come from uh from covariance that affect both the the probability of uh participation and affect the outcomes so you want to get to coverts that are that are highly correlated uh with both or with at least with either of them and it um I mean if you actually so you're you're talking about a situation where you're designing the study but the the treatment you have no control over the over the actual assignment then then the best advice I think it's just trying to collect as many coverts as possible that that affect uh either the the probability the participation decision or the or the outcomes and a particular try to think hard about what affects participation what sort of what changes incentives uh for for participation is the better you can explain that the the easier it is going to be to remove biases from from these comparisons yes you just weren't estimated because um so the question is so here we're also throwing out some of the the trainees um and so at what point would I be worried that I was actually estimating the propensity score really uh badly suddenly um so so that but when I did this uh this sort of these particular calculations um and I'm trying to recall exactly how I did this but so pretty much I put in the the covariance linearly and um and went with that specification if I was doing a more substantive analysis I would look harder at at higher order terms and see if they if they mattered a lot in particular here given a be concern about lastly about the individuals making having very low earnings uh I would probably put in um dummy certified for being unemployed in both both periods in the end sort of after some specification now we just check whether the Tech the adequacy of the specification by looking at various covariants and see whether they're different between the the two groups but I'm I'm not sure that um all right I've I've not seen a lot of sensitivity to to that and the overall results really I think where this is going to matter a lot so changing the specification of the propensity score slightly is going to change the values sort of the ratios considerably in the very tale so um for these CPS individuals that were very low values of the propensity score maybe 0.01 so maybe 0.001 or maybe 0.0001 that shows up a lot in estimators that that put in these weights that depend on one over the propensity score if those guys are still in if those individuals are still in the sample clearly a weight of of 100 was a weight of a thousand is going to make a big difference but once you you exclude these individuals by by dropping everybody with a propensity score less than 0.1 moving the cutoff points slightly or changing the specification of the propensity score seems to have very little effect on the on the overall results I mean in some sense maybe what I should have done is sort of look at these tables as well and instead of using the 0.01 we kind of use 0.05 but my sort of from from having done that in a different uh setting then I would strongly expect the results to be extremely similar oh yes um yeah so I mean that's really so that's why where the point came from that in the end once once you're careful about the overlap the differences between these uh these estimators is much much smaller now they said in the end this is one example and so I would still be inclined if you do the the regression waiting the observations is still a sensible thing too but me remember what the we call what the weights look like they're one over the propensity score before with all these individuals with the propensity score extremely small these weights get very large Now by throwing out these individuals with the propensity score less than 0.1 the largest sort of the the ratio of the largest to the smallest weight is now less than ten to one where before it was was I think ten thousand uh to one I clearly don't really want to have estimated where where some of the observations get 10 000 times the weight that that observations get and so the hair you ensure that that's not the case but having a little bit of waiting may still make things uh things more robust but the difference I clearly strikingly small between the various estimators compared to what uh what we found before 