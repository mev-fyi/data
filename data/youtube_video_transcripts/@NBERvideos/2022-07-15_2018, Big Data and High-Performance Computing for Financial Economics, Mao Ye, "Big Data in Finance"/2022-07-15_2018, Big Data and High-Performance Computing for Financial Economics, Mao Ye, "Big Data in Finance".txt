today my topic will be big data in finance so it's a very challenging topic because finally i realized i need to talk about something which does not have a clear definition okay so the for the sake of this talk i want to define a big data in three dimensions first one is like if we think about big data it needs to be large size that's a minimum right but only size is not enough usually big data also have high dimension so what does high dimension mean high dimension means like you have lots lots of variables sometimes the number of variables can even be bigger than the number of observations this is called high dimensional problem and the third dimension is complex structure so since i was a student even now most time i work with panel data it's like raw column flat format but there are lots of data are unstructured so for example it's like satellite image social media credit card transactions and today we are taking a video that's the unstructured data so we are creating the data in this process okay so uh here is a roadmap my talk so i will start from large size and that then i will move to high dimension then complex structure but i want to really add one thing so i hope you guys will not think about it's like a big data is a collection of empirical facts so later i want to show you i i will try to convince you big data also motivate new economic theories okay so let's start with large size okay so the first question is like uh we why we have smaller data set okay some data sets are naturally and small but some others data sets are small it's because we have some selection process to reduce the size of the data okay for example we can have smaller sample size and sometimes we have less variable we can collect more variables but we collect fewer variables and sometimes we aggregate the economic activity and sometimes we take snapshots all these things reduce the size of data it creates data we can manage right but there's a natural question to ask are there any selection buyers when we create smaller data so i want to show you one example so this is a new york stock exchange trading code data okay it's a small data set i want to use that as example as a small data set so uh yeah seriously it's a small data set compared to another one so it includes all trades and quotes reported to the consolidated tape so it's 25 gigabytes but that's per day so why is this a small data center because there's an even larger data set than in tech singapore if you submit orders sometimes you cancel orders some orders remain unexecuted it's seeing the older level data but not in the trader level data so this is an older level data one of the older level data from nasdaq so basically it tears you when people add order when people cancel order okay so technically we want to ask a question are there any selection buyers intact data so it's something we try to compare large data center with a larger data set okay so what do we find uh before i say what do we find it's very data intensive okay so then i finally exceed okay so we use high performance computing i mean to basically to make this one work why it works because majority of data we work with is still a panel data right there are two level of natural parallels the first one is day by day you can do the first parallelization a day by day so it will reduce the data size to about 100 gigabytes per day it's still large if you want to do some manipulations so the second parallel is across the stocks across stocks is a little bit tricky because some stocks like apple they trade so much it's equivalent to like 500 small stocks so if we want like 7000 parallels that's a waste of resources so what we do is something simple we just parallelize based on the sample size okay so what do we find we find actually tech data have selection buyers and this selection bias was created by regulation why because in the past if you trade less than 100 years you don't need to report why because think about that it's like people most times think like small trees come from small traders for example my neighbor buy one share of disney for his kid is that interesting should that be regulated probably no but when we compare the large data set in a larger data set we finally realize all the laws actually are missing trees less than 100 years so lots of people told me it's like a tech data include all the trades before 2013 that's not true small trades disappear but is that a big deal it's a big deal 20 5 of observations is truncated especially for high priced stocks some well-known stuff google is like a majority of observations are truncated 53 so apple is 38 okay it's a huge truncation but why is this truncation are these coming from retail traders you can look at this pattern so this is a series of 111 trades each one of them is unreported okay but everything happened within one millisecond so we find we pick a smaller sample from nasdaq 120 stocks but can retail traders do that i don't know whether you can do that i cannot do that trade 111 times in one millisecond okay so finally we realized here's one thing we find is machines challenge existing regulations we find that these trades are more likely to come from a computer why they do that because if you have a large order let's say you want to trade a million shares okay you can slice and dice your orders into many many pieces each one of them is not reported why you want to do that you try to hide the information right this is surprising it's like all the laws are mostly informal trades actually anything below 100 they're most informative it's like they're informed guys who slice and dice orders so basically it's a truncation caused by regulation and then people's behavior change if you have supervised computers you can send lots of small trades to the market and nobody's using consolidated tape is able to see you so this one actually has policy impact finite regulators see our paper then they reduce the threshold to one year okay but then the question is like the regulation change where this cut-off affects the results before even before computers finally realize sometimes you can have tiny tiny truncations but when it's combined with other truncations it can be a big deal why because uh there's a difficulty to find a a long time series of retail traders there's terrorists or links data but if you want a really long time series you need to find some proxy and the best proxy proxy is one designed by charles lee and his co-authors it's like let's say many many years ago it's like still small trees are more likely coming from small traders let's set a cut off let's say five thousand dollars okay that's the best uh weekend dead and we did like a minute uh many many years ago but finally when we realized uh how your shares cut off realize when you combine two truncations that cause problems why because singapore if you have a stock with price about 50. the minimum trade size is 100. what's the conclusion you will see territories don't trade these stocks right and this is a truncation based directly on price does not even depend the market share of all lots because this this is a new truncation rule it's like okay any stock with price above 50 is truncated so how big is the issue if you look at this is the number of stock truncated it seems like not a big deal it's like about 10 okay but think about most high-priced stocks are very large stocks think about google right and also this is a pattern fluctuates with business cycle right if you look at here so this is dot-com bubble period there are lots lots of high-priced stocks at that point right so if you do this five thousand dollar cut-off you will find about seventy percent of market cap is truncated right and you can make a conclusion it's like if you use that as a example of retail trade you said retail trades do not trade dot com stocks in tech bubble period so it's a mechanical pattern right so here is like i just just want to use this paper as example to motivate you to think about two things number one is techniques techniques i want to say like exceed helps to solve the science challenge but i want to talk slightly more broadly about two economic intuitions number one is like there's open questions for public policy because existing regulations are designed for humans machine learning and big data actually bring machine players into the market so should we update some or revise some regulation useful design for humans for machines this is one question and the second question is like are there select sample selection bias in any other small data sets we know it's possible but i think it will be very interesting question where we can collect more data or large size data maybe some of the conclusions we draw from previous literature can change so next i want to talk about high dimension okay so there are large number of variables relative to the sample size so let's start from a motivating example so i mean big data and machine learning are two buzzwords in wall street there are lots of famous firms let's say renaissance technologies and worst features says okay they use machine learning techniques to make investment decision and their horizon ranging from a few minutes to a few months okay then the question is okay let's think about the farthest guy in this spectrum minute by minute guys do they track any economic meaningful signal okay so here we met a high dimensional challenge at a minute by minute horizon so the basic idea is like okay let's uh start with a simple example you can use lag return of other stocks to predict the return of city okay uh let's see other stocks the universe is new york stock exchange listed stocks they're about like 2000 of them okay each minute you get one observation if you run ors regression you need two thousand observations so that's six trading days there are too many right hand side variables to make the predictions you technically you cannot run os regression especially when the signal are short-lived so how to solve this issue machine learning techniques okay so i will not go deep into machine learning techniques but i just want to summarize about the differences in approach so traditional approach is like we use economic reasoning to use like a predictor let's go into x okay and then we use statistics to estimate whether x is a good estimator we can do sorting we can run little regression so machine learning is different it's like they use like statistic method to both select and estimate x so it can handle a large number of x and sometimes it has more flexible functional form okay so let me summarize machine learning techniques also using one slide is okay there are two common features of machine learning techniques number one is the focus more on like out of sample predictions so this is something called cross validation so the goal is to maximize out of sample predictions so they focus less emphasize on the causal inference that's number one feature number two is like they impose regularization what is that it's penalty for complex models okay and there are two variations about different types of machine learning techniques number one is functional form okay you can have linear functional form you can have regression trees or you can have neural networks and the second dimension is the type of regularization you have okay so then i use uh one of my paper with alex tinkle what do we do is like we use lasso okay so what's a functional form of lasso it's linear functional form okay it's like an oil so we try to minimize this term but y lasso is different lasso's difference is like there's some penalty term so the penalty was imposed on beta okay so the main idea is how can loss do variable selection so first we need to normalize the variable uh before regression and if you have small beta lasso theta to zero if you have large beta lasso basically shrink that so that means if you have beta which is too small then lasso ignore this predictor okay and next is what is cross-validation it's like we use standard like 10-fold cross-validation what does it do because in the previous regression there's one free parameter which is num which is a penalty term right if we set num not equal to zero that's ors right if we set a ridiculous large number we get nothing okay and then how to pick number so here's what we do it's like we divided the sample into 10 chunks we call 90 of them as training example and the other 10 percent is called test example so we use the training sample 90 to calculate the loss of estimator and then we use the testing sample to calculate a mean square error and this is the term it depends on k also it depends on lambda and then we repeat the step 2 to 3 10 times to get the average and then we pick the number with the best like overall performance there's there can be some twists but the basic idea is like we try to find the optimum number okay so what do we find we find lots of implied strategy work really well think about sharp ratio during our sample period is 2005 to 2012. so the shop ratio of sp 500 is 0.123 but the sharp ratio of lasso implied strategy okay it's about 1.8 and we have alpha of 2.8 okay so if we're on a hedge fund that's the last slide okay but we are writing papers we need to find again the most important thing to it's like we're economics we are academics we try to find an economic interpretation of this result so they alex adam and i actually established four results so number one is like it's unexpected because there's many many like well-known factors small stocks uh large stocks to predict small stocks some kind of a long-term relationship like a weekly a month we find it does not work well at short horizon why it's related to the number we find in the num now and it's like at least lasso typically ignore any predictor weaker than 2.5 per month so many weekly or monthly predictor cannot generate this high return but then the other question is okay will you have a ridiculous high return where i become super rich something like that uh no the answer is there's a trade-off 95 percent of loss of predictors we pick at minute by minute horizon disappear within 14.2 minutes and it's also sparse lasso use only 12.7 predictors on average and this is the most significant result it's like and also surprising he's like we thought lasso are more likely to pick a stock as a predictor before again it's before this dog have a news announcement even if we pick the best news announcement the feedback data feeds so then the question is is that insider trading here's finally we figure out what happens it's like big data incorporating information faster than news announcement so sometimes we talk about public information we use news as an example of public information but think about who writes the news sometimes the machine can write a news but you need a news reporter to write some story right it takes time so here i want to pick one example actually it's a story with torben so we invite you for a seminar but you arrived late i remember you come to semi room and say that you apologize because your chance hit a truck and then we googled the news we didn't find any news unfortunately later once you arrive you send us an email you tell us here is the news so basically your idea is like it's unscheduled news you saw the news reporter come later so then there's a lag finally realized oh actually what topic method is something called unscheduled news it takes reporters sometimes to write a new story so then we did another empirical test this is one of the main results it's like okay if you have scheduled news lasso will pick that in the same minute if it's unscathed news then you see lasso pick the stock as a predictor more likely to see lots of pictures stock as a particular first and then you see the news so here's my interpretation it's like something happened some guys maybe on site or something that they they can trade and machine learning recognizes this pattern and machine learning follows this pattern and later news comes so that's the economic interpretation and they let me move slightly broader even in terms of trading there are at least three open questions along three lines number one it's like alexander and i we started minute by minute horizon but you can apply lasso to other horizons there have already been three nice papers on monthly horizon but again there's a huge spectrum between one minute to one month what kind of economic signal does lasso if any they capture this is i called allocarizer the other dimension is other regularizations so kozak nego and santosh have a nice paper using rigid regression so what is regression it's again linear functional form what's the difference it's a penalty term it's the square of beta so lastly using absolute value of beta and the third dimension is like do you have other functional forms so uh google kelly and shu has a paper showing this later okay other functional forms for example regression tree neural networks can capture important nonlinearities and interactions so later i want to have a titanic example it's not in finance but it's an example i really like so hard variant has a paper so basically here's the pro you try to predict the survival rate of people in titanic you can run a logistic regression what's the result result age doesn't matter but then you have another philosophy then he ran another regression using regression trees regression engineering is highly non-linear basically it's split into different notes and then the main result he finally is like okay if you are less than 8.5 years old you are much more likely to survive beyond that the pattern becomes much more complex so then the economic intuition is it's a children first this matches the economic reality we know the reality but if you only have the data you look at the pattern you find that you find this rule cheering first okay so what's the main takeaway from high dimension techniques we have already have machine learning techniques to deal with high dimensional data but i think it's a tool and for economics and finance we are more interested in economic insights it's like determine the economic interpretation of some results sometimes have even higher hurdles so this is high dimension okay so then i want to talk about complex structure so it's not in traditional raw column format so here's a nice summary uh it's two authors from jp morgan they write a manual and they divided unstructured data into three types so the first type is generated by individuals for example social medias product reviews web searches right and the second type is generated by business transactions for example supermarket scanner data sec filings and the third type is something created by sensors what are the sensors satellite think about pollution and weather sensors they also create lots of lots of data they can help you to ask interesting economic questions so let me start with the first one is individuals here is the data of twitter okay this is unstructured data so when we saw this data there are two challenges number one is like how to extract information from this unstructured data that's a technical problem uh there are two ways uh the first way actually is surprisingly simple is like find a data vendor so there are in jp morgan's manual they have 77 pages of lists of alternative data vendors i mean you would be amazed look at all kinds of data they create so what do they do let me summarize majority of them do one thing they transform unstructured data to structured data to the panel data which we know uh so if you look at this is summary statistic of that manual this is the frequency of wars majority the buzzword in that field is satellites and there are lots of firms you don't need to i don't know how to analyze satellite data these firms are using many of them using machine learning techniques and they generate structural data for you okay and the second solution is like uh work with guys from other fields i have one paper uh for that uh but then what can we contribute so here's a question it's like a lot of unstructured data does that create a unique measure of economic activity we don't know before so i want to give you one example is my people with gida natish and she okay so think about uh information diffusion is very very important for economics so there are lots of like theories and empiricals that suggest like word-of-mouth communication is very very important for economic activity but then the question who document world of mouse communication before it's hard but it's such an important economic question so a lot of smart people find proxies for word-of-mouth communication for example you can based on whether we are neighbors whether we are close to each other or whether we go to the same school so usually we assume if if you and i go to the same school we probably know each other we probably talk more right but still we these are smart ideas but still we don't see information diffusion and recently there are two nice papers they're interesting criminal investigations why it's important because if you bring something to a court and their documents tears you who tell whom what that's a unique case you can see information diffusion right so there are papers about insider tweeting their papers about ponzi schemes and then they're legal documents it's a small sample but it also provides additional insights but what is a big data solution big data solution is like singapore tweets you can sort of see information diffusion right so here is the motivating example so let's say a g my course has 10 000 followers and if you say twitter data unstructured and then this is my second question natish and g told the story to natish and then more people know and the teacher then retweets that to gen okay this is information diffusion and then how to capture that so this is the tweet data we first need to figure out the id of the tweet when it is created and how many followers the person has or the news outlet has and they we can follow one retweet so if we look at one routine we know something is like we know the the tweet was transmitted to one person and then the other person retweet okay we just follow the same tweet id and we also follow how many people follow this retweet guy so what's the purpose it's like we want to create an empirical measure of information diffusion so what is information diffusion information diffusion basically tells you through time how many people know the information okay uh tweet it's not perfect measure but at least it tears the basal number of followers we can construct the speed of uh information diffusion for example this is a fifth percentile this is a medium this is 95 high how many people the information rich so what do we find finally realize like there are lots of functions of social media but we find one function it's like social media sometimes spread old news what does it mean it's like okay if you get a tweet let's say okay you get a tweet from one of your friends some firm is cool something like that that's all news uh so we use so the stair is 10 minutes after the initial release of the news outlet but retail traders lots of them maybe not you still get excited and what's the pattern it's like it creates temporal price pressure price move away for a very short period of time and as they revert within next day so what does that mean that means okay smart trader should tweet against the twitter sentiments if you see good news you should sell and then buy it back quickly so but here's one thing it's like a paul title has a nice paper about the stair news across traditional news media and the reversion happens much much slowly so then here's an open question we have a conjunction we haven't do that it's like are these smart traders machines because we know lots of machines say okay people writing descriptions saying okay we try follow social media so originally i thought oh these guys trade in the direction of sentiment they say good news they should buy they follow finally we realize since they are machines they might do the opposite they trade against the sentiment but then that opens broader question is number one do machine trade against human behavior buyers after these questions here if the answer is yes then it's like if they trade against human behavior bias do they intentionally do that or it's because they follow certain decision rules they do that without even knowing they're doing that and the next question is are markets more efficient due to the rise of machines that's the economic interesting question so let me summarize about structure challenges so for techniques you can analyze satellite-like data by yourself but i strongly recommend you either find a data vendor or work with experts in other fields but i hope i can convince you big data i mean unstructured data can create unique measures of economic activity i just show you one example but i'm pretty sure they're more and it helps us uh to financial economics to test the economic theory okay then the next question is like we talked about how to test the economic theory but does big data create any new theory so the rest of time i want to come ask you yes okay so it starts with an empirical project it's like you guys probably know high frequency trading michael lewis like flash boy books this guy's super fast nanoseconds microseconds but then your natural question why their arms raising speed so we start with an empirical project at the beginning economic intuition is uh pretty straightforward it's like think about majority of models we learn it's like a vibration equilibrium there's implicit but important assumption it is price is continuous i still remember when i have my phd quantification exam it's like i need to solve the price level and then the price level is square root of two do you think that in reality probably not right that's one reason number two is like zero regulation so for example if you trade in your u.s stock exchange you want to list the quotes there's one set of minimum price variation that was imposed by sec rule 612. okay and then you probably say okay how can trade happen let me tell you one thing it's like a problem when we're using our trading it's like there's market maker specialists most of these guys disappear so this currently stock market is something called a limit order book and it's voluntary liquidity provision but then you guys say how can trade happen let's say tony want to buy hundred shares at a hundred dollars and they totally see okay this is empty book how can tony do that he said okay i become liquidity provider so basically tony said i list the offer to buy a hundred dollars and then tony is called liquidity provider okay and then how can trader happens you uh tony needs someone to accept his limit order so then modeling arrive i accept your limit order and they are 000 shares at 100 a transaction happens first guy limit holder liquidity supplier second guy market order liquidity demander but then let's say i realize our liquidity provider is cool i also want to provide liquidity but then you have two guys provided liquidity there need to be some rule to break the tie so there are two rules the two rules first one is called price priority so that means at a given price if you quote a better price you become the winner so in this case tony wing because you post a sale order of lower price okay but let's say okay in this case we quote the same price and then the second priority coming is called time priority first come first serve so in this case you also win okay so here's hypothesis finally we think about oh why there is an arms race in speed it's because they are constrained price competition okay but then i mean because the uniform tick size means like okay that means hft are more actively provided liquidity for low price stocks because once they anticlimacy is more binding okay but then for empirical projects the first question people always asked in the seminar is identification identification and identification right but people can say okay low price stocks are different so they how can find identification okay if i don't realize we can find the etfs okay there are two etfs to track the same index sometimes one etf split and reduce the price and we can find its twin brother which is a control group is the etf does not spread but why it becomes a big data problem final realize because etf split is rare so we need to go through like four years of data to find 64 splits and reverse plants it's tens of terabytes of data because remember each day is large you need to search for four years to make this mechanism work it's a big data project what do we find so here's a mechanism so let's say longer hp equal to better price than hfd and there's a split to reduce the price by half and the encoder order seems equal now hft will move to 50 dollars and 1.5 cents and hft will move here but then tick size kicking and then now hrt has to quote the same price as hft and then they lose time priority so that's one of the origin of high frequency trading but you probably asked me where is the theory in this example actually with hands making one thing so finally publishes a paper but there are more it raises more puzzles number one who are these non-hfts are they humans probably not number two questions why they quote a better price than hfts they must have a reason why hft do not compete more aggressively in price they are now hft so that's the theory part so we start with analysis of big data then we have new theory and then we put a new analysis of big data finally uh the model is complex but let me summarize like using one slide it's like here's a simple thing it's like including me many researchers in this world think things like black and white either you're a computer or you are human but finally realize i mean their third type is called human half computer so you guys say oh terminator uh not terminator it's like finally we include a model with a new type of traders actually they're really really important it's called buy side algorithmic traders so why call them half machine half human because there are lots of portfolio managers so they make investment division now they are machine learning investors but still lots lots of funds are using human to make investment decisions but the investment deduction what is the investment decision buy one million share of google that's the investment deletion but now the market is so complex they need to use computer algorithms to execute orders for performing managers and what's the decision for the algorithm it's like whether you demand equity or whether you supply liquidity what's the purpose minimize transaction cost right so these traders bats are faster than humans but they're slower than hfts you guys probably ask me what uh because they don't need to the traders need it's not like hft actually need to like a consistent monitor market to find any opportunity so let's start with a benchmark there's a nice paper by buddhist kramden and shin it was presented in afa aea joint launching keynote last year very nice model so it's a continuous time continuous price model with two types of traders again it's two so who are these two types hfts hfts what does hft do hft just consistently more like marketers to find any opportune pricing profit opportunity the demand will supply liquidity when never there is a profit opportunity so in their model non-hfts has an inelasticity demand to buy or sell one unit and the zero arrival intensity is lambda i and the only demander credit team okay in their model so there's a security with value called vt so the value evolves as a compound person process it's public information everybody knows that but it can jump with intensity number j and with equal probability it will jump up or jump down let's say the size is d so let's first look at their model i will summarize their model i mean in two slides uh so let's say here's hft hft cl at is asked suppose a null http arrived that's good news for this hft because the hft will make half the bit-ass spread that's good news that's a revenue of hft but what is the cost the cost is the value can jump there's one hft sale at a dollar but remember there are other hfds and other hft will become snipers what do they do they look at oh there is a snare quote here why not snipe that and if this guy this guy can't escape but with certain probability this guy gets sniped and liquidity providers lose money snipers makes money so the main takeaway from a buddhist criminal machine is like even if it's public information because of this sniping risk the beta spread is not zero okay but then let's suppose we just at first add one layer to buddhist crime editing model we split the trailer so now we introduce half machine half human here they are bats so first let's look at the original strategy in buddhist criminal machine the only demand liquidity so then they pay half the spread which is s star divided by two if you're smart enough will you do that i want to show you no you never do that if price is continuous why because there's an easy strategy simply beats the previous strategy it's like okay suppose i'm a bad i can post a limited order here at wii t plus epsilon what will happen because i i mentioned to you is like vt is public information so fundamental value is here so all these hfts look at oh there's a profit opportunity here which is epsilon and what will this hfts do is they will immediately come to demand liquidity at a microsecond or nanoseconds there's a profit and the transaction cost for the battery is plus epsilon it's much smaller than half the spread okay so what's the takeaway the takeaway is like this is a model of machine intact with another machine a faster guy you back with a faster guy so then the strategy becomes very interesting number one is like you may ask why bats on the condition use benchmark always provide liquidity there's an important one opportunity cost bads have lower opportunity cost of private liquidity because they have to buy or sell so that's the reason whenever there's a price competition they can do that okay because they have different outside options opportunity cost but then what simple hfd strategy fee strategy have one economic intuition is like hft has two prices one is prices they offer i list no offer there's a price and is another price i accept if i am hft what's the difference it's like when you list no offer it's subject to sniping risk so if you sell you want to see it at a higher price but you are going to a set a price which is lower so that's a that's the reason okay what i showed in the previous slide but what's the main takeaway is like machine and machine impaction are fascinating because it blurs lots of traditional definition for example in the previous example i show you based on traditional definition uh like limit order in limit order book literature the first guy is bad bata arrived first so bad becomes a liquidity provider but battle will generate immediate response from hfts so they the question is like who price liquidity becomes open question right so it's because it's machine and machining hashing so so then we generate a lot of predictions and policy implications i would not go to details but the original purpose uh by this paper with xinhuang and styli is like uh we want to explain why not actually quote better price okay the reason is opportunity cost but finally realize we write a model it generates lots of new predictions especially when we're adding discrete price which is another layer so there are four types of equilibrium and it works extremely well in predicting who provides liquidity and when on a different condition sometimes hit provide liquidity sometimes best provided liquidity so what's the take away from that the takeaway from that is like i believe machine and machining action probably provide one of the most one of the best environment to develop economic theory because they follow some deletion rules because everything is coding if you find the economic mechanism of their behavior you will make great predictions because they're i don't think they're less subject to sentiments all kinds of variables they don't know which is not in the code and it also has policy implications for example scc recently have a tick size pilot program they increase the tick size which is a really a good natural experiment if you work in corporate finance it's like randomly pick a thousand than 200 stocks and increase the tick size from one cent to five cents okay so implication for the model it's consistent with the queuing more accurate equilibrium in our model it's like i predict this policy initiative can potentially increase the number of activities so later so i want to talk about the financial ecosystem so so when i was a student i worked with 13th data so what is a short-term trader inserting of data there are some people say a trader with horizon below four months why because it's for each quarter you get one observation you do not know what happened below a quarter and there's a reason the proliferation of literature starting the other end is microseconds and then nanoseconds okay it's high frequency trading literature but there's under explored territory are there any guys in between i'm pretty sure yes the answer is yes at least i show you two examples first one is bats bats is slightly slower than hft they're leaving executing horizon that's let's say a or seconds and machine learning guys they are even slower it's like their horizon is anywhere from a few minutes to a few months but they will meet a very big challenge because u.s data usually the best uh us data usually you don't see trader's identity at this horizon right so then it's uh it's we almost have no data so then this is a working progress it's alex jingle and i it's like we try to solve this problem using public available data with those they're guys leaving different horizons horizon maybe one way to differentiate traders so at least the trading volume data is available tech data the small data set i just mentioned but it's really big so let's do one thing we first aggregate the trading wallet at a minute by minute horizon okay so each minute we get our observation and then we can measure the variance of one minute horizon right and then we use wavelet estimator what do we do is like we decompose each stock trading volume variance into time-scale specific components using wavelet estimator so what's the main idea i just show you the main idea so let's look at a period example beming the series so it's like first period zero volume here is a hundred here is minus 100 okay so you have a graph representation here so then a period example it's straightforward what happens in different horizons so in a period we have low frequency medium frequency and a high frequency in this simple example so low frequency variation is compare the variance of the the change in the first half and the second half let's call that a low frequency okay in reality maybe it's first two weeks to second two weeks okay and the medium frequency there's one to two minutes compared to three to four minutes there's another another wavelength here okay and high frequency is minute by minute variation okay i will not go to into details but i did provide you with two examples look at here so in this time series all the variation comes from difference between the first half and the second half so this one we call it's a low frequency movement 100 percent is at a low frequency okay and this is another extreme is that everything happens at very high frequency then we see everything is high frequency so i will not go detail into details but here i want to give you one takeaway it's like here's my two the two cents i got through this process it's like we start with big deal and then we have a theory and then theory discussed some underexported territories and it motivates empirical predictions it motivates policy implications and sometimes it motivates new empirical tools okay so let me conclude so i want to build several points so big data provides lots of challenges but it also offers lots of opportunities for new research questions for techniques i want to summarize high performance computing will help us with the size challenge and machine learning can help with dimension challenge right i mean alternative data vendors there are lots of alternative dynamic vendors provide unique data but sometimes they do not provide the data you exactly want like information diffusion and you can work with experts from other fields okay but what is more important is like i want to i hope i have already convinced you it's like big data opens doors for new research questions so once you have computing power you can document new empirical regularities and you can change the conclusion from the previous results based on smaller data sets that's the first thing document fax is the first thing and also it also motivates us to find economic interpretation of the data okay machine learning i mean it's predictive but we still can find economic intuitions on some cases okay and also it can create unique measures to test the existing series okay and it also motivates to construct new series so uh because my uh my error is on trading but at least i show you one example i hope i can um let you think more it's like think about what is this like a big data machine learning is very popular words but think about that in trading industry i want to let you think about one thing think about behavioral finance it's like what's the foundation of behavior fine it's psychology right a lot of things was derived from psychology and now it's like 85 percent of trading volume coming from machines it is possible it's like big data and machine learning will be the foundation for the next generation of series i call that algorithmic behavioral finance thank you 