Susan Athey: Just briefly on
propensity score estimation. Those of you from more
of the treatment effects literature side know that propensity scores are
incredibly widely used. They're actually really widely used across all disciplines, and there's a subset of machine learning that also uses propensity scores and uses propensity score
weighting and so on. But actually, nobody has yet answered the question
of exactly what's the best way to do
propensity score weighting for the purpose of using
it in a causal model. We know a lot about the best methods for minimizing mean squared error
of our prediction, but we want to think more about what the properties
are if it's going to be used as a component of
a second stage estimation. That as far as I know, is an open question. Now let's talk a little bit about using machine-learning for model specification if we're
doing causal inference, but we have a selection
on observables setting. That's the setting where
conditional all my covariates, my treatment variable
is exogenous. That's usually what the
people who we run OLS, we're hoping that by sticking a lot of Xs on the
right-hand side, our treatment
variable is agonist. Victor and co-authors have
a very nice discussion in The Journal of Economic
Perspectives, that is, I think a great example of why it's not enough to listen to the first two hours
of these lectures and go home and just run Lasso. Because it actually matters
whether you're trying to do causal estimation
or just do prediction. If we were naive and
we just went home from the first two hours and
went to catch our plane, then you would miss this to say, I'm just going to
run, I think I have a selection on
observables setting. Now, it's even better.
I'm going to throw an even more Xs
and I'll let Lasso tell me which
actually should be in and I should be done. As Victor points
out, the problem is that in that setting, of course, you're
going to force your treatment effect and you're not going to take the chance
of having that go away. If some some
covariates are going to have coefficients
that are forced to zero, the treatment effect
coefficient will then pick up those effects and
will thus be biased. They say, there's actually
a better approach, which is not something
you ever would have thought of if you were
just doing prediction. You need to do variable
selection via Lasso in a selection equation and an outcome equation separately. For IV, we always do two-stage estimation, for
selection on observables, we usually don't, we just control for stuff on
the right-hand side, control for our Xs. He's saying actually
we need to estimate a selection equation even in the selection on
observables case. Then any variables that were important for predicting
your treatment, which means they're correlated
for your treatment, those should be in the
right-hand side as well. It's not the outcome
equation that tells you what Xs solve your selection
and observable problem. Lasso was not intended to solve a selection on
observables problem. Lasso was intended to solve
a prediction problem. Just throwing everything
into the Lasso, you shouldn't hope that
it's going to really do a good job solving
your selection on observables problem. To solve that problem, you regress your treatment
on the covariates, and that's the regression that's going to tell
you what variables are important for solving your selection on
observables problem. You want to have both on
your right-hand side, things that are important
for predicting your why, and things that are important
for predicting your W, and that's a better
way to do it. The insight is so simple,
you could write it up in JP. So I would say like in some
sense, we want to show it to your students as like why you don't take machine-learning
off the shelf, this is a great thing to read. He shows just how you
get different answers. You get biased answers if
you do the single selection, this is bad, that's good. Now, we can talk about using machine learning for
the first page of IV, and actually, I gave
a talk at Harvard a couple of months ago
in department seminar. Surprisingly enough, I had
a whole morning full of student meetings that
people trying to use ML in economics, and a couple of them had
papers on this topic. The first stage, IV is an economics problem that a machine learning problem, and they're trying to use machine learning for the
first-stage regression. Chernozhukov and
Hansen already have a couple of papers about that. Again, in the spirit
of being able to use these techniques and not worry about getting your
paper rejected, you can cite lovely theorems and Econometrica that tell
you it's all okay, you can do lastly on your
first page and you can do OLS on your second
stage, and report your standard errors
and it's all good. Maybe not quite all good, there's some
conditions and so on, but again, hopefully,
nobody will read those. For future discussion, maybe we should move beyond Lasso. Lasso is nice because
it's familiar, but maybe there's other
things that perform better but also preserve
interpretability. As Quito mentioned, some of the other methods,
people haven't really focused on their
asymptotic properties, so we just don't really know if we can get good results
about other things. When economists
want to actually do hypothesis tests and
report standard errors, there's still some
more work to do to figure out what the best
performing methods are. Now, I'm going to go through a paper that [inaudible] and
I are working on and that's available on the web that looks at
heterogeneous effects. Again, I'm going to go
through this in some detail, both because I think
everybody should use it, but also because I
actually want to use it as an illustration
for two things. First of all, it's just
going to help you get comfortable with
all the components of a machine learning model, the in-sample fitting, the out-of-sample
cross-validation, and I'm going to
change those things. It's going to help
you understand what their roles are because
I'm going to change them. But then maybe you'll see that they're
actually tangible, and so we can think about modifying methods and it's
really not that hard. The first motivation is this concern about
ex-post data mining. Michael Kramer told
me the story about this norovirus treatment in the pre-analysis treatment
plan that they've filed, they said they
were going to look for the effect of the drug, but they failed to say that they were going
to look for the effect of the drug on people with
the most severe diarrhea. It is against FDA policy to look at your data
after the experiment, find the group for
whom it worked, and then expose to make
recommendations to use the drug. Bunch of people are dying because they didn't specify the correct pre analysis plan. On the other hand, experimental economists
and various folks in the treatment
effect literature are actually trying to push, everybody should file
their pre analysis plans before they do
field experiments, so we should be
more like the FDA. Why are they saying that? Well, by running an experiment, I'm going to find some group
67-68 year-olds in Alabama, my drug looked great, so
let's recommend it for them. Of course, that's
bad data mining. Not data mining is a good word, we say it, our deans love us, but among ourselves, we always
saw data mining was bad, and what we meant by that
was like searching out for obscure things in our data and pretending that they
are a real effect. What we're going to do is propose a machine learning
method that will allow you to systematically
evaluate the results of a large-scale
randomized trial, find those groups and get correct standard errors
about their effects. I could go back, and actually, some drug companies have
already asked me to do this, to go back and look at
their large-scale data and find the groups for whom
their drug really work, so then they could try to, then they still have to
do a new experiment, but at least they could do
it in a cost-effective way to get their drugs approved. But ultimately, you could do this and you could
change FDA policy. The second thing is
that we want to think about treatment effect
heterogeneity for policy. I might like to understand the structural form
of the function. The drug works really
well for these people, not so well for these people. Later on if the cost falls, I might give it to more people. I just want to know that
structural functions so I can make optimal decisions. I just want to say there's two very closely related problems. First of all, is estimating the treatment effect is a
function of your covariates. I want to know how
well does aspirin help you if you tell
me your covariates, I'm going to give you
a treatment effect. There's a very
closely-related problem, which is optimal policy. I'm going to tell you if
you should take an aspirin. If the costs are zero, in some sense, those
are the same thing. If in some applications there's a benefit function
that we're going to measure and the cost
might change later, or cost function and the benefit might change later, whatever. We want to just estimate that structural function and
get the magnitudes right. I'm going to focus on
that first, later on, I'm going to show you can modify these techniques
just a little bit to compute optimal
policies as well. It's a slightly
different method, but very closely related. Just to preview, we're
going to distinguish being causal effects
and attributes. We're going to estimate
treatment effect heterogeneity. The methodological
contribution is that we're going to have different cross-validation
approaches that are conventionally used in the
literature on prediction because we're estimating
a causal effect and that's what we want
to cross validate. We're also going to
enable inference. We're going to use trees. I hope I'm going to convince
you out of this as well, that using trees to segment models is a
great thing to do. I love it. I was doing this long before we wrote the
paper, I just didn't have a theoretical
justification. We've already talked
about regression trees. It's just partitioning a
covariates base for prediction. Let me now summarize on a single slide what
[inaudible] said before. There's three components
of these models. These are the three components
I'm going to change. When you do estimation
with a tree, the estimator itself is going to be the sample mean of your outcome
within a leaf. Just like a kernel is
a weighted average of nearby units or a nearest
neighbor matching is like the near neighbor. Well, here, the
estimator of a tree is just going to be the
sample mean of those guys. It's very simple predictor. When I decide whether
to split the tree, I'm going to use mean squared
error as my criteria. I'm going to say a good split is something that reduces
the mean squared error. I could split by people
under 60 and over 60 or under 50 and over 50
or under 40 and over 40. I'm going to choose
among those by finding the thing that minimizes
the mean squared error. Finally, out-of-sample, I'm going to use that
exact same criteria. I'm going to use mean
squared error as well. Now, I'm going to
compare my prediction to the actual outcome
out-of-sample. Those are the components of
the model and in some sense, so you want to
change this model, you can twiddle
with those things. I'm going to suggest
that economists should be twiddling with
all of these things. We're going to use trees to divide up my covariate space. I'm going to estimate
something different, I'm going to choose
different ways to split, and I'm going to use different
ways to cross-validate. I'm going to show you
one, but there are lots of them that you could do. If I wanted to use regression trees to
estimate causal effects, again, just to say what
am I trying to do? I can say there's
a mean function which is your expected outcome as a function of your
treatment and your attributes. The treatment effect is
the average difference in outcomes for being
treated and not treated as a function
of your covariates. This is the effective
of an aspirin as a function of your age
and medical history. If I wanted to just go off the shelf to use machine-learning
methods to do this, there's a few obvious
things you would do. It turns out a whole bunch of people have done
these obvious things. What are the obvious
thing? Not a whole bunch. There's a very small literature and machine learning that cares about this but
those people who have, have done these things. First of all, you could analyze the two
groups separately, I'll take my treatment
group and I'll build a big tree and I'll see how your health outcomes are determined by
your covariates. I can do that for
the control group. I have two separate
estimators and I can take the
difference. That works. But remember that what
these models are all trying to do is make the very most efficient use of your data. You're going to keep adding parameters until you
can't add them anymore. If you add them to do one thing, you can't add them to
do something else. What this is going to do is
use up all your degrees of freedom predicting the
level of your health, but it's not going to use them predicting the treatment effect. If you think about any dataset you've ever analyzed, outcomes, whether it's health or height or test scores or whatever, are highly variable as a function of your
characteristics. But treatment effects, those are finding treatment
effect heterogeneity as much harder and we can be tall
and we can be short, but the effect of an aspirin
may not depend on that. That's an issue. The second thing you
could do is you can say, all right, fine, let's
just build one big tree. I'm going to throw in my
treatment variable along with all my other Xs and treat
them exactly the same. The problem with that,
is that you're still not asking the model
to really measure treatment effect
heterogeneity specifically, it's still going to build
whole parts of the tree explaining things that aren't related to treatment effects, and you're going to have some parts of the
covariate space where you don't split on the
treatment variable at all, so you're going to
have a mechanically zero estimated treatment effect. That may not be ideal either. Both of these are lame. What you'd like to do
is actually if we're doing models where were designing an algorithm
to solve a problem, our algorithm should
actually solve that problem. Why was this not an
obvious problem to solve? We don't actually observe the
ground truth for anybody. I don't know how to
cross validate my model. The first approach we have
is a very simple approach. I'm going to criticize this
approach in a lot of ways, but I want to start
with it because A, it works and B, it's very simple to code. You can run it in
like two seconds in R. All you have
to do is write one line of code and
then run a tree in R to one line of code is I'm going to
transform my outcome. Let me just explain
this really briefly. Suppose we have
50/50 randomization. You guys are treated
your control. Then I'm going to estimate
your treatment effect. You're treated, my estimate of your treatment effect
is your health. That's my estimate of
your treatment effect. Greg's treatment effect, he's the control group.
It's minus your health. That's my estimate. Times two. That's a crazy estimate. Pretty high-variance. But actually, on
average, it's right. Why is it right on average? Well, the expected value of this is just, there's
the two here. Half of the people
will be treated, half of the people
will be control. I've got a minus one
for the control. If I add them all up, I'm going to get the expected value of
the treatment effect. Actually, with just one person, I can get an unbiased estimate
of your treatment effect. It just happens to be
very high-variance. I can do that with unequal probabilities and I can do that with selection
on observables by using propensity scores. This first procedure,
which again, it's one line of code to
transform your outcome and then one line of code
to say r part Y, X_1, X_2, X_3, and boom, you're done. You're going to get an estimate
of the treatment effect. What is the effective
thing that we're doing here after we
transform the outcome, my estimator is going
to be the sample mean of Y_i star my transform
variable within a leaf, which in expectation is equal
to the treatment effect. As I just argued, my
in-sample goodness of fit will be the mean squared error for this
transformed outcome and same for my out-of-sample
goodness of it. Basically, it's just the
standard algorithm with the transformed
variables so I don't actually have to change
any lines of code. What are some
problems with this? Well, the advantage
is it's easy to code. But the problems are, that actually, if I look what
happens within the leaf, like say I have 55, 45 within a leaf, then what I'm actually
going to do for my estimator is
I'm going to take the sum of the treated people and divide by 0.5
instead of 0.55. I'm essentially
going to estimate the treatment effect
with a group that doesn't have a 50-50 population, except we will weigh them 50-50. It's bizarre, you would never ever do that if you were writing the
code from scratch, we'd never estimate a
treatment effect and not just average over the number of treated people in the
number of control people. But you don't really think about it if you're just using an off-the-shelf method, but in fact, it's not
the best thing to do. If we don't mind writing
a few lines of code. The great thing about R is
its open source software, it's just got a little function that says, how do you estimate, and you can just substitute
in another function and so we substitute
in another function, which is the actual sample average treatment
effect instead of this crazy in a particular sample wrong
high-variance estimate. Then when we do splitting
and cross-validation, it's also the case of this high-variance estimator is going of the treatment effect is
possible to improve upon. In sample, for splitting, given that we know
by construction in-sample, we have an unbiased estimate
of the treatment effect. Then we know we're going
to have a better predictor if our prediction has
a higher variance. In other words, if I start
with all of you and I say your average
temperature is 98.5, then I can split
between men and women. Maybe the men are a
little hotter because it's a crowded room so then I could split and I would see the variance of my
prediction would go up. I might say the men are 99
and the women are 98.2. That increases the variance of my prediction, that's
a better predictor. If you have an unbiased
estimator, that's the same as improving mean squared error
so we're going to use that, the variance of the predictors
are in-sample splitting. Then out-of-sample,
we're going to do something that's more exotic. We're going to use matching. We're going to use nearest
neighbor matching. There's other things
you could do, so this could be
future research. Why did we choose nearest
neighbor matching? Again, a sense is the beauty of this literature is you don't
have to prove any theorems, so you can just do it. What is it you're looking for in an out-of-sample criteria? Well, the whole
reason you're doing out-of-sample cross-validation, is it you're worried
that you overfit. These three guys in Alabama
had particular outcomes and so you're going to create
a branch of the tree for them but that was just
spurious Epsilons. What you're trying to do is to test whether
you've done that, whether your estimator
has gone too far. You really just need
an unbiased estimator out-of-sample that makes sure that what you've done
in-sample isn't biased. Using the very nearest
neighbor one match, we're using the single closest nearest neighbor is a match, is going to give us an estimator that minimizes the bias among
all matching estimators, but it's going to give us an estimate of the
treatment effect. If you got these two
different estimates, one is the Y_i star, your treatment effect is two times your health versus your health minus your health. The advantage of using the matching estimator
is if there's any predictable components of your health outcomes that depend on your
characteristics, the matching estimator
controls for that. The variance of the
matching estimator is basically the variance of the treatment effect
conditional on X while this Y_i star doesn't control for
any of the covariates. Also, the scaling factor which just averages out when you take the mean actually gets 1/p squared when
you go to the variance, you multiply by p
and you end up with these crazy terms in the denominator so it actually
increases the variance. That can actually make a
substantial difference. We modify the algorithm and practically that means we just change a few components, the subroutines of the R-code, we're going to use
a sample average treatment effect
as our estimator. We're going to use the variance of the estimator
to decide when to split and we're going to
use the matching estimator. We're going to
essentially estimate our cross-validation criteria. It's a funny thing. Instead of comparing to the ground truth, I'm comparing to an estimate
of the ground truth, which again, we
haven't really seen. If we compare the causal
tree to say using a single tree, they're going to be similar
treatment effects and levels are highly correlated. But if things that
affect treatment effects aren't the things
that affect levels, then the conventional machine learning methods
won't do very well. The transformed
outcome will do badly if there's a lot of observable heterogenenian
outcome because the matching estimator
controls for that. We do a bunch of
simulations and what we find is just as
you would expect, the transformed outcome trees
end up being less complex. Because when we test
to see if they work, we keep thinking they don't work because we have really high
variance and so we say, don't build a complicated tree, because every time you try to get more complex,
you make mistakes. But actually, it's the evaluation criteria
that's making a mistake, not the tree itself. We find that the
causal tree journaling does better, although
I should say, you can find datasets where
any of these does best. There's no single
dominant criteria. Now, let's talk about inference. Again, this is not the subject
of the machine learning literature at all and
you might have thought, I even have many
questions when I present this to machine
learning audiences, they say, well, how
can you talk about doing standard errors
when you're doing trees? Because we know
that the prediction of a tree is not
asymptotically normal. It's not just that
we don't know if it is, we know that it's not. Trees are highly discontinuous. But my goal is not to do
inference on the prediction. I am using the machine learning methods to
segment my sample. Just like it's perfectly
fine for you to split your sample and run
the regression on old people and young
people separately. It's perfectly fine
for me to build the tree and then estimate treatment effects
at the bottom of the tree. I want to do inference on the treatment effect,
not on the tree. Actually, for doing inference on the treatment
effects the tree is actually easier to work
with than a continuous model. Because at the end of the day, it's just splitting your sample and we all know that's fine. Actually, here I would say, you should be able to publish
this too, no problem. Our papers are not
quite accepted yet. We've submitted to a specialist, you a PNAS because
they had a machine learning special issue that
we presented the paper at. Hopefully, it will be
better when it's published for using it. But I think that
actually the logic of this is clear enough that
you should be able to explain this to your editor so there shouldn't
be any problem. There is actually that
one important issue. When I split my tree, I need to separate the process of building my tree and
doing my estimates. Because it is true that I might still end
up with a part of the tree that I need
more than one person to mess up the tree because I'm doing
cross-validation and so on. But if I have a few
people in Alabama who have funny outcomes and
then I split on Alabama, on average, the treatment
effect within that dataset, the same dataset, that split is going to be higher
than it should be. When I build the tree
and estimate my effects, I am going to have biased estimates and we see that in applications
and simulations. But my proposal to
you is use part of your dataset to build the tree. That part of your dataset
is the one that's going to tell you
how to split and then use the other
part of the dataset to estimate your effects,
and at that point, you can do whatever
the hell you want. My tree told you
to split by state and pick out the South
and do old people, and then over here, you don't even have to
use the treat estimate. You could just put in dummies, you can do other things. If you end up splitting the
way the tree suggested, you could run a whole model according to those splits and so that you're really
not constrained. That's the idea. If you were really
mostly wanting to run a model and just wanted
to find a few splits, you might use less of your
data to estimate the tree and save most of your data to get precise estimates
for your models. On the other hand, if you were mostly
concerned about finding the heterogeneity
and not as concerned about inference, you could
do something different what we just proposed in the
paper is splitting 50-50. Let me just give a
little application here. Estimating position
effects in search. As you know, the
Google's getting investigated by the European
Commission and they admitted in the FTC documents
that they have been made public that they manipulated
the search results. They put Google Finance on top and put other things lower. So you can read all about
that on the Internet, The Wall Street Journal
leaks the documents. Here what we did was it in a randomized
controlled experiment where I actually
took a couple of million users on
Bing and I re-rank their results to see the
causal effect of position. Of course, it's easy to look
out in the data and say, the thing at the top gets
clicked more than the thing in the second position.
That's true. In our data, the thing that the top position gets clicked
25 percent of the time. The thing in the
third position gets clicked seven
percent of the time. But of course, you
know that the position is not exogenous. The whole point of
a search engine is put the best thing on top. We don't know whether
it got clicked a lot because Bing did
a really good job, or if the position caused
people to get clicked a lot. You can decompose that with a randomized controlled
experiment so when we take the thing
in the first position and put it into position 3, it turns out that it gets clicked 12 percent of the time, which is more than the
typical seven percent for the lousy thing
that's usually in the third position. But it's a heck of
a lot less than the 25 percent it
would have gotten. The causal effect of
decreasing position is you lose about
half your clicks. You can see that even though you might think that having everything on a
webpage is not important, that intermediators, that rank things actually
have a huge impact, but the effect is
quite heterogeneous. We built a tree
that tries to see exactly what causes the effect to be big or what
causes it to be small. We can see that for celebrity queries that
likely to have pictures, the position doesn't
matter very much because if you search for Paris Hilton, then you're just going to click on a picture and you don't really care about the
informational length. On the other hand, things
that are more informational, that are coded say
to be likely to be from Wikipedia
reference or so on, there's actually a
much larger effect for losing a 0.2 clicks. Then what we can do, what I've got here is
the all the leaves, the treatment effects
and the standard errors. There's a treatment sample and the training sample
and the test sample. The training sample, it turns out, if you do
some calculations, has a much higher variance
than the test sample. That's exactly because
of this overfitting. The training sample
estimates are biased because I
found a leaf exactly because in that
particular sample the treatment effects
were more extreme. Then if I take that same tree
and go to the test sample, the data that wasn't
used to create the tree, I get effects that
are more moderate. But then as you can see, it's fairly straightforward to think about reporting this. You can say, look, I've got
these exogenous variables. I use this other dataset
to build a tree. Now I've just segmented my analysis and there's
a standard errors, they're fine. No problem. The conclusions that the
key to the approaches we want to distinguish
between causal and predicted parts
of the model. We want to think about
having best of both worlds. We want to combine what we know from the treatment
effect literature with what we've got from the
machine learning literature to get better answers. I do think actually for the randomized controlled
trial literature, this could be
important, especially. This is just the
literature slide. There are bunch of people who have looked at similar things. A few people actually turned out independently while we
were writing the paper, introduced the transformed
outcome into Lasso. The same critiques I said, of the transformed
outcome for the trees also apply if you
use them in Lasso. We think there are
better ways to do it. We actually also apply it
to instrumental variables. Now we're going to build a tree where we have two
components, the numerator and denominator,
and estimate them separately. I'm going to skip that. I think about some
other next steps. Once you see this general idea, you actually could think about estimating a demand model at the bottom of the tree and
doing various other things. Really, that was actually
my original motivation estimating elasticities. I talked to people at
eBay and Amazon who also agreed that this was a
really common problem. A lot of the other economists who have been looking
at this data, we're also struggling
with the same issues. Just have a few minutes left. Let me move on. How
much we have got? About 20 minutes. I want to talk now about optimal
decision policies. As I said, this is a very
closely linked problem. If I know your treatment effect is a function of
your covariates, I also have some ideas about what policy I should
assign to you. But it turns out that the loss function from estimating a treatment
effect is a little bit different than the loss function from estimating optimal policy because I'm trying to
by just wanting to get the right answer, I don't care about the
magnitudes in the same way. Mean squared error isn't
the right criteria. It's more of a classification
error problem. I should also say that the contexts may govern which
one you're interested in. In the health study, I think I want to estimate the average treatment effects. But if I'm Amazon and I'm
making online recommendations, or if I'm a webpage, I'm trying to decide what news to show you or what
content to show you, then all I care about is
the environment today. I don't care about the
environment in the future. I don't care about an
underlying perimeter. I just want to make
good decisions now. I think whatever
I'm measuring is the full cost benefit analysis. I'm not really going to care or save the magnitude of
the treatment effect. I just want to basically
give you the right thing. Examples are offers
and marketing, web page optimization,
customized prices, etc. Same old model. We're going to maintain the selection
observables assumption. Now, we can just say
there's an optimal policy which the notation
isn't perfectly ideal. Maybe I shouldn't call
this W star of X, but Pi star of X is the
optimal W given your Xs. I should say that this
problem also gets more interesting when it's
more than a binary choice. I'm going to show you the math mostly for binary choices, but it's more general. There's actually a
small literature in machine learning that
tackles exactly this problem. It's interesting that they've gotten there before
economists in some sense. I should have said all this
stuff about how they don't care about decision-making
and we do. But for this specific problem, they actually have been
working on it for some time, and it does have a close link
to what I just described. There's about 10 different
names for this problem. Contextual bandits, partial labeled problem,
all sorts of names. What they do is they apply another literature called
cost-sensitive classification. All of the classifier
models can also be run waiting
classification error. You can say some errors
are bigger than others. That was how it was
originally motivated. But generally, you're just
going to wait observations. The policy problem
is to minimize the regret from a
suboptimal policy. Again, it's a little bit different than mean
squared error, close but not the same. You want to minimize the mistakes and some mistakes are more costly than others. For the two choice case, it turns out there's this
really cool result they have that makes it very easy to solve the personalized
medicine problem or the personalized
offer problem. The basic thing is I'm
going to transform the outcome similar
to what I did before. If there's a propensity score, I'm going to divide by
the propensity score. It's a 50/50 experiment,
that doesn't matter. Just pretend for a second 50/50 experiment and ignore that part. I'm going to train
a classifier as if the observed
treatment is optimal. I'm going to do a
50/50 experiment. I'm going to assign
you aspirin randomly. Then I'm going to get an
optimal policy by doing a discrete choice model that predicts as if that
was an optimal choice. Seems a little counter-intuitive
at first, doesn't it? The reason it's right is that, well, I'm not exactly
going to do that. I'm going to wait you
buy your outcome. If you happen to be assigned an aspirin and you had a
really good health outcome, you're going to be
weighted really heavily in saying that your
X's should predict a one. You would really
lousy health outcome. You're going to be
weighted only a little bit in predicting whether your Xs
should get you an aspirin. You might think this
is okay approximately. But the loss from my cost weighted
classifier that's doing misclassification error
minimization is actually exactly the same in expectation
as the policy regret. In principle, just
like I said before, you can transform your outcome, do weighted minimization, and just throw it into our
routine and out will pop the optimal aspirin policy for all people as a function
of their covariate. You can do personalized
medicine in two lines of code. Brilliant, so that's very nice. But actually, in some sense, that literature stopped
it little too soon. They got very
excited about that, that they could use these
off-the-shelf methods. But they didn't actually
ask whether these were the most effective
methods in practice. It turned out all of
the critiques we had of the transformed outcome for the heterogeneous
effects also apply here. It's just a little less obvious. But you just have to do a little algebra to figure it out. Actually, how excited you guys are about algebra
at this late hour. Maybe I'll give just a little bit of it
because I think it's helps you understand
how they work and also how classifiers work. I'm going to compare
two policies. I suppose I estimate
two classifiers maybe one has a variable in it
and one doesn't have it. More complicated tree and
a less complicated tree. I want to compare them and
see which ones do better. I'm going to compare the
classification error. Of course, any place where
these two classifiers agree, there's no difference in
classification error. There's only two regions that matter for comparing
the classifiers. The region where a classifier A says treat and B says no treat, and the region where A says
no treat and B says treat, those are the only two
regions that matter for comparing two classifiers. In this method, if in the
region where A says to treat, the loss function A has no
misclassification error for the treated units because we're assuming
those were optimum. On the other hand, for
the control units, classify array does have
a loss and it's equal to the expected outcomes
of the control unit. They average out from
the control unit. For the classifier B. B says don't treat, but these were the units
who were actually treated. I'm going to have the negative. The classification error is
that I didn't treat them. The classification error will be the negative of their outcome. If I add all this
up in expectation, the loss function or the
classification error is the expected value of the negative of the
expected value of the treatment effect
for the first region, and a positive of it
for the second region. Again, these are the
differences between A and B. These are the object. But of course in some
sense it's like if this is what you're
trying to estimate, again, these objects
are very high-variance. What we would suggest
to do instead is to estimate just using
the treatment effects. Let's use a matching estimator or out-of-sample or in-sample. Let's use the sample
average treatment effect. Let's not make the stupid
mistake of pretending that we're 50/50 when we're
55,45 basically. That seems like a
simple correction, but it can make a
difference in practice. Here I just put them
both side-by-side, but let me move past them. This slide is just
saying we're going to use matching for
cross-validation. For inference, actually, again, this isn't considered in the machine
learning literature, but the same discussion
holds as before. As before, we're going
to split the sample. We're going to estimate
the classification tree on one part. Then I can do inference
on whether I've got the optimal policy
on the other part. Just summing up,
there is actually this existing machine learning
literature that says, as a function of your covariates,
should you get a drug, should you get an offer, should you get a voting
mailing, whatever. You can apply that and it works. But we can do better by improving it for the purpose
at hand and customizing it. I mentioned that for
multiple treatment options, what they suggest to do is to do a combination of
binary classifiers. You take all your data, you run option 1 versus 2, 3 versus 4, 5 versus 6, and then you go
region by region of the data and you
run a horse race of the winners and then
you run a horse race of the winners until you've
got an optimal policy. They show that that
actually is better in terms of prediction than trying to do
them all at once. Other related topics on the
machine learning literature, actually the directions
they've gone that are more interesting for them is
trying to do this online. I hope people might remember
like two years ago, I actually brought in
John Langford and he was supposed to tell everybody
about machine learning. What he did was he
actually opened up his computer,
went to a prompt, typed in a command and
all these numbers went through the screen and
he said, I just learned. I forgot what he was
predicting like, "I just did this right now." We was like, "What did
you just do?" No idea. But what he was trying to show everybody within he had
such a fast learner, it could read in
data in real-time and make very complicated
predictions very fast. Everybody in the IO was like, why would I want to do that? But if you're running a website, you absolutely want to do
that, that's really cool. Some other directions,
actually I was just chatting with
Alberto about this, there's actually some
work in economics about double robust estimation
and also this branch of literature has all about
that dealing with problems with propensity scores
being too small and so on. I'm going to skip over this. Here there's a slide
about the paper with Alberto and [inaudible] , it's an NBR working
paper, you can read it. Let me say a few words about some proceedings paper that [inaudible] and I wrote. Here even more than before, I just want to emphasize I don't think we have the final answer. But I want to inspire
you to think in this direction because I think it's a
promising direction. Here, we want to think, now that we've gotten all
excited about model selection, maybe we should
also start thinking about robustness of models. The typical thing we all do in all our applied papers is, God gave us our model and it was decided by economic theory, but just in case we were wrong, here's four more columns
that show what happens if we add and take away
fixed effects and some other things and that
makes us feel better. Then what makes us feel really good is if the Beta coefficient is the same in each of the
five columns. That's awesome. Now, which 100 columns we didn't put it, it's a little unclear. Also, of course,
the way we got to that pretty table where the
coefficients were stable was by taking stuff in and out earlier on and seeing
where we weren't robust. That process is unsystematic. It's not that we're trying to be bad. We're trying to be good. We're trying to tell the
reader how robust we are, but it's awfully
unsystematic and it just goes to hell if you
have hundreds of covariance. I'd like to be more
systematic about robustness. You can like it or not like it, I'm going to be inspired by the engineering approach of my machine-learning colleagues. They don't let the fact that
this is hard to stop them, they just write something
down and see if it works. I'm going to be a little
bit in that spirit. Why have we not done
robustness before? It's been pointed out, Let's Take the Con Out of
Econometrics by Ed Leamer. But it never really caught on because we never found
a good way to do it. One problem, it
could be a Bayesian, we should have a
prior over models, but what's the prior? It's just hard to
know what to do. We're going to propose
something that you could do. Could there be
something better? Yes, but I'd like to get that
conversation restarted, and what do I think
has changed is that now we have lots of
bigger data sets, more data and we can think about using machine learning methods to be more systematic. But I should say I'm not directly applying any
machine learning method, I'm just inspired by them. What is our proposed approach? We're going to use a
series of tree models to partition the sample by the
covariates, the attributes. Simple case, we're going to take each attribute one by one. We're going to re-estimate our model within each partition. Once I'm going to
split my sample, estimate my model, then I'm going to
re-average it back up. That is a different estimate
of my original estimate. I had an original estimate of a parameter for
my whole dataset. I'm going to get two estimates
on different partitions, but I'm not asking how different are those two estimates. That's heterogeneity. I'm going to average them
back up and see how does estimating the model
to essentially allow everything to interact
with the thing I split on, change my overall
average effect. That's going to yield a set
of sample average effects, and we're going to propose
the standard deviation of the effect as a
robustness measure. This thing is a problem
and I'm going to talk about the problems with it, but it's a starting point. What are some nice
things about this? Well, I'm asking
people to do this, but I'm not asking you to
write any more code, really. Whatever you're very complicated structural model estimation is, don't change a thing, just split your sample and run
it on a smaller sample. I'm going to hold everything about how you estimated fixed. I'm not going to tinker
inside your code, I'm just going to split your sample and let
you re-estimate it. I think that'll make it
convenient for people to do, even if they're doing
nonlinear structural models. Then we apply this method
to four applications, when the randomly assigned
training program, one the non-random Lalonde data, a lottery data that had exogenous assignment
of lottery winnings, and one is a straight regression
of earnings from NLSY. What we're going to do
first, this is just within the Lalonde data, is
we're going to look, the base model has
an estimate of the treatment effect
and a standard error, and this is the
experimental data. The non experimental data has a similar magnitude
treatment effect and a similar standard error. Our robustness metric however, is very tight for the experimental data and very big for the non
experimental data. If we go systematically
through the covariates, in each case, split the data by the covariate and then
combine them again, which again is like allowing interaction effects with each of the variables essentially, we get a whole bunch of
different estimates. They're tighter for
the experimental data. For the non-experimental data, if you had written down this specification,
you get negative 4.26 and that's perfectly reasonable because
you just allowed one more interaction effect
in the regression and suddenly the training
program is terrible. This metric is going to pick
that up and it's going to just provide you some discipline besides the five column. I'm sorry, I shouldn't
have used that example. This is a splitting on the treatment
variable, bad example. Putting on unemployed
and 74 gives you negative 2.44. Sorry about that. Then we look at
the four examples and we look at our
robustness metric, and the ratio of
the standard error to the robustness metric, it's very low in the
experimental data. It's high in the
non-experimental data and it's medium for some of
these other ideas. Let me just say again, I don't think this
is the final word, but I think it's
provocative that we can and could be
more systematic. Some of the nice
things about this, is that it's invariant
to the scaling of the right hand
side variables. My method is going to
decide where to split. If I take income and multiply
it by 2, or square it, or log it, I'm going to get exactly the same
robustness metric. It's invariant to transforming
the vector. I'm sorry. That's a good desiderata. Then there's two
desiderata that we fail. If we added interactions or did a matrix multiplication to
transform the whole vector, our metric is going
through one by one. It's not considering
interactions and so if I specify differently, I would get a different answer. The worst problem with it, is if I add in irrelevant
variables, then I'm not going to change my answer and so that's going to allow me to gain the robustness metric by putting in a bunch of
irrelevant variables. That's a problem that
should be fixed. The bigger thing is we want
to create a class of models. Each model has to be
distinct to create variance, but we also want
to allow a lot of interactions which tends to
make models more similar. We want to think about
a well-grounded way to make model. Something we're working
on that we haven't actually totally written up yet, and this is a complicated
slide so let me just say a few words
verbally about it, we actually want to think about maybe doing worst-case analysis. So think about a bounds
analysis or a set of models and looking
at the worst-case rather than looking at
the standard deviation. The advantage of
looking at bounds is that I can keep throwing
in irrelevant models, I can throw as many
irrelevant models as I want and I can't
gain my outcome. The disadvantage of this is if I try a whole bunch
of different models, some of them are
going to be crazy. These bounds may be very
sensitive to outliers. There's actually a fix for that, that has a whole
decision theory. Tomas [inaudible] from Harvard has a bunch of work
on this and so does Tom [inaudible] about
variational preferences, that basically do worst-case
analysis except for, if they deem a
particular model bad, and I'll say what's
bad in a minute, then it doesn't contribute
as much to the bounds. I can add something
back onto it to make it less likely to
actually be the bound. In [inaudible] 's work, he's thinking about macro
models and so he says a model is bad if it's not
consistent with the data. Here, my idea is that a good model might be one that does a good
job in prediction. I could think of a whole
bunch of different models, but if some of them do very badly at predicting
outcomes out of sample, then those models
shouldn't determine the bounds of my
parameter estimates. Now, this works great for
exogenous covariates. It doesn't work very well
for IV because we just told you that IV doesn't
maximize predictive power. That's really an
open question more broadly for causal models, how you would say
this model is so bad that I don't want it to be entered into
my robustness. But these models are all
plausible models and so they should be part of
my robustness measure. I think that's an open question. But anyways, I think the big takeaway
I'd like you to have, is that these machine
learning methods have really systematic ways to generate models and
get the outcomes. We now have another
tool for measuring robustness that can
systematically search our data, generate different estimates, and the open question
is really then, how do I aggregate those up? Do I want to take the
standard deviation of them? Do I want to take the bounds? If I take the
standard deviation, I have to have some
way maybe to throw some of them out
that are identical, so I would want to look
at different models. That's one way to go is to say I'm only going to enter
different models, I'll take the
standard deviation. Another way to go is to
say I'm only going to take reasonable models
and take the bounds. This is ongoing work
and I hope that some people get excited
about it and work with this. 