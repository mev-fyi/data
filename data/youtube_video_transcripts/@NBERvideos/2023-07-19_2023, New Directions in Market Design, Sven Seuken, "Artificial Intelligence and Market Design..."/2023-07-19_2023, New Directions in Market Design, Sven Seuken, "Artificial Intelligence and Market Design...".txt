thank you so much and first thanks to the organizers for putting together a wonderful program and for giving me the opportunity to talk today um I will talk about machine learning Market design which is based on roughly eight years of work coming out of my research group and I will focus on the community trial assignment domain where the goal is efficient assignment of indivisible items and to agents n and the applications I will study today are Spectrum auctions like what Kevin talked about as well as course allocation and in both domains the agents have complex values for bundles of items be it a bundle of licenses or bundle of courses and this gives rise to value functions that are exponentially sized and so this is the well-known curse of dimensionality where the agents can't possibly reason or report a value for all of these bundles and the main thesis of my talk today is that a key Market design challenge in many domains but in particular in this domain is getting the preferences out of the agents and into the mechanism and the key idea I want to put forward today is that using machine learning can really help to improve preference list station by helping the agents Focus their attention on the most useful parts of the value space and in my talk I'm going to talk about these two different applications first iterative combinator auctions where I will give you like six different vignettes of a lot of work we did over the last couple of years and then in the end I will talk on about one specific paper on course allocation that we have done recently so the starting point for our work on governmental auctions is the seminal paper by Ausable Crampton and Milgram on the common control clock archery which is this two-stage auction design and the first stage you have a dynamic clock auction where prices are going up every round until um demand is less than Supply and then in the second stage you have a sealed bit supplementary stage uh this is a hugely practical and successful auction design more than 20 Spectrum auctions have been conducted worldwide and generating more than 20 billion dollars in Revenue however there are also a couple of challenges one is that the clock stage can take many rounds and the allocation after the clock stage is often far far efficient and second there's a question that the bidders have which bids to submit in the supplementary round and there's some experimental evidence by Chevrolet I'll showing that the efficiency at the end May often be uh quite far away from uh fully efficient so about um seven eight years ago together with my former PhD student John Luc Cabrero and Ben Lubin who's also in the absence in the audience today uh we went out to design a completely different kind of auction um which is based on value queries where the auction mechanism interacts with the bidders we are a value query protocol and we equipped the auction mechanism with the machine learning algorithm and the idea here is that on the micro level we're using machine learning to learn the bitter's preferences during the running of the auction and on the macro level we're using an optimization algorithm to iteratively compute useful queries after every round of the auction so zooming in a little bit on the key component of this mechanism we are equipping every individual bidder with their own machine learning model and the role of that machine learning model is to learn the bitter's preferences so as an input the machine learning algorithm gets the previous reports of the agents and learns a value function VI tilde here and then we feed all of these learned value functions into a machine learning based winner determination algorithm we can solve that to optimality and that gives us an allocation Which maximizes social welfare based on these learned value functions and the key idea here is that this allocation now is a feasible allocation and we can take the bundles in this allocation and treat them as queries that we feed back to the bidders and the idea is that these will be very useful queries because they form a feasible allocation that has high predicted social welfare so this will likely be very useful information to get from the videos now there's some questions here one is for the learning part we need a machine learning algorithm with two properties one is it needs to be good from an economic perspective being able to predict these highly complex non-linear values and it needs to work with a small amount of data because we're learning on a per agent level second it needs to be good from a computational perspective because we need to be able to integrate the machine learning algorithm into the winner determination problem and it needs to be remained computationally feasible and so one of our main technical contributions back then was that we figured out how to do that with support Vector regression with fluoride kernels now in terms of incentives um we designed a mechanism back then where the main idea is kind of a textbook idea where we iteratively call this query module that I just um showed you once for the main economy with all the bidders and then also once for each marginal economy leaving one bidder out and this allowed us to in the end when we calculate the final allocation and the payments we're not using any machine learning at the end of the mechanism but instead we're just collecting all of the elicited bits throughout the auction to compute the final allocation and the final payments either BCG payments or core payments now we used a similar approach as well Kevin I described in his talk to do a computational experiments here we use the simulator that was tuned to the Spectrum auction in Canada and um in these results we found that using our approach we could beat the community clock auction by about two percentage points in efficiency now one shortcoming of that approach was or one main limitation that these value queries going to the bidders required exact answers so if I ask perfect what's your value for a bundle a b he had to answer two million dollars and that's very costly to figure out for a bundle and so one Improvement we did in a paper we published at ECE 21 is we developed mlca with interval bidding where the beta could respond with an interval so for example you could say my values between 1 million and three million dollars this significantly decreases the informational requirements from the bills on the technical side our main contributions there were to combine the machine learning algorithms we had previously developed with a modified revealed preference based activity rule that made sure that as the rounds progressed the bounds around the values were being tightened so that over time the mechanism was eliciting enough information to converge towards the um to the to the efficient allocation we ran similar simulations as before and the positive result here is that even though imlca is getting much less information than mlca it's achieving almost the same efficiency now one shot coming of the original machine learning based approach that we used um the support Vector regression algorithm was that it has limited expressiveness and it was kind of stretching the computational feasibility so one of my new PhD students who has by now graduated Jacob weinsteiner when he joined he said why don't we use neural networks because they are highly um expressive and we can figure out the computational feasibility so here we now used a fully connected feed-forward neural network so the idea is on the on the first layer you're just import encoding the bundles and then on the last layer you're just predicting the value of the bundle and we're learning using neural networks to learn the value function and then the key question was well how do we formulate the optimization step using these neural networks to still solve the winner determination problem and this we figured out by using a real low activation function in this neural network allowing us to reformulate the neural network based winner determination problem into a neural network based mixed integer linear program keeping this problem tractable and here we found that using neural networks instead of support Vector regression algorithms we could further improve the efficiency compared to the prior work now one shortcoming of all of the work I showed you so far is that it was ignoring any kind of prior knowledge about this domain that we are looking at here turns out in this community assignment domain be it auctions be it course allocation two properties usually hold true one is monotonicity meaning that an additional item can only weakly improve your value and second that the empty bundle has zero value so the challenge we are facing here is that we wanted to design a new neural network architecture that captures this prior knowledge without limiting the expressiveness and so this we did by designing a new neural network architecture called monotone value neural networks and we could prove a universality theorem about this new architecture showing that any value function that is monotone can be expressed in our architecture and at the same time we found that our nvnans can still be encoded in a success way as a as a mean and then we ran for the experiments and we showed that the efficiency can further be improved if we use these monotone value neural networks which makes sense because they code the prior knowledge that we have in this domain another shortcoming that I haven't told you about so far is that all of the approaches are kind of myopic in the sense that they completely ignore the mechanism's knowledge about the uncertainty about the bitter's preferences so it's always just looking one step ahead and maximizing in a myopic way but ideally the mechanism should think about how much how uncertain am I about the Builder's preferences in this space of this bundle space or in this part of the button space however there was no approach that we could use to form a model about these uh this uncertainty that we could then optimize over so we started out by developing a new approach and this was challenging because we are in a high dimensional space with very scarce data and we had to make sure that any model we come up with allows us to at the end um fit it into a machine learning based when a determination problem that we can still solve and so the solution we found here is we developed a specific neural network architecture that actually consists of two connected neural network one upper neural network that is responsible for the valid prediction and one lower neural network that is responsible for the uncertainty prediction and then we designed a special loss function based on five patient is a director that makes sure that the uncertainty predictions satisfies the model uncertainty prediction that we want this work then allowed us to kind of go the final step and design a Bayesian optimization based communal auction so here we could then finally incorporate this model uncertainty into the running of an iterative combinatorial auction and this we do by designing an upper uncertainty bound based on our previous work of the monotone value neural networks as well as the model uncertainty neural networks and then we can optimally control this the well-known exploration exploitation trade-off and this gives rise to a new activation function so in the optimization step where we've previously optimized over the neural network representations we can now optimize over the activation functions that come out of this upper uncertainty mount and then similar simulations as before show that we can even further improve the efficiency compared to the plane neural network approach and compared to the mbn approach sp4 okay so the final point I want to make about the common control auction work stream here is some ongoing research we're going we're doing at the moment and here we're kind of going back to demand queries because obviously value queries are not the only way to do this we want to Leverage The Power of prices and the idea that we're currently exploring is designing a CCA but where we are enhancing the clock face of the CCA with a machine learning based approach the idea here is to train each with this machine learning based model based on their demand query answer so which are tuples of the form bundle and price at which this bundle was optimal and then in every round we're going to find an approximate clearing price Vector P star that minimize this function W which is the sum over the um indirect utility functions of the bidder and the indirect Revenue function and this is an idea that was previously put forward by Bray right now and here we see in this chart that with our approach we can have the very few rounds in in a domain called lsem we can achieve efficiency around 96 after roughly 20 rounds while the CCA is at that point still around 88 and later it plateaus around 91 so this is just after the cloud phase without the supplementary mode so this is the end of the commercial auction work screen and now I want to tell you about one paper called the course allocation paper and here I have to thank the organizers of NBR because roughly five years ago uh when I presented the first version of the paper in the previous work stream after the presentation um Eric approached me and said hey can't we apply the same ideas that you just showed about the options um to the Corsa location problem and then you know only five years later we have another paper um yes so in case you're not familiar with the course allocation problem the problem is we are we want to assign university courses to students we have a limited number of seats and no monetary transfers are allowed in this domain in contrast to the previous domain uh the dimensions of Interest here are efficiency fairness and good incentives and our goal is to design a practical mechanism that is good in all three dimensions so the state of the art is the course match uh mechanism designed by Eric it's a practical implementation of his approximate competitive equilibrium from equal incomes mechanism that he put forward in his 2011 paper and the course match mechanism makes a good trade-off between efficiency fairness and incentives um compromising a little bit on all three dimensions but achieving a good trade-off that works in practice and this mechanism course match has been adopted in many top Business Schools including at Wharton and Colombia um here's a quick overview of the reporting language that of course match uses first the students for every course that they're interested in they can report a base value so for example I can enter value 50 for the first course a value of 100 for the second course and second for some courses I can additionally report an adjustment for example for two courses I can say I have a minus one or a plus 50 value for the pair of the two courses if I get both of them and then you the utility of a schedule at the end is simply computed as the sum of all the base values of courses contained in the schedule plus any adjustments if the student reported any pairwise adjustments now boots and Tesla did a very nice lab experiment before uh course match was deployed in practice and while in on average the student really benefited from this new mechanism compared to the previously used BPA mechanism they also identified several shortcuts one is that the students showed significant cognitive limitations they only made use in a very limited way of this very nice reporting language reporting only a few base values on average and most importantly only reporting one adjustment on average and actually the median number of adjustments was Zero um you might also be concerned about the limited expressiveness of the language The Language by Design can only capture at most quadratic preferences and we don't know if that's enough for all students or not but I think most importantly uh they showed that the students made significant reporting mistakes um exports showing them alternative bundles they could have gotten and the students reported that they would have preferred another schedule and this significantly harmed the efficiency of the mechanisms so this is the motivation for our work uh using machine learning to kind of fix these reporting mistakes leading to better efficiency and here so one way to understand our mechanisms to contrast it with the course management mechanism and the course match mechanism essentially just has two phases in the first phase all the students enter their reports into the graphical user interface and then you run the course match mechanism which has three stages and then the final allocation is computed so what our mechanism does is it essentially inserts itself into the middle between the first and last phase which gives rise to this iterative machine learning based preference incitation phase and the idea is similar to the auction based work that we're going to maintain a separate machine learning model for every student the input to the machine learning model is a course schedule and the output is going to be the predicted utility for that schedule and then at the end we're going to run the original course match mechanism but instead of the GUI preferences we're going to use the trained machine learning models as the input and so in phase two of the mechanism we're going to first create an initial data set to have an initial machine learning model based on the GUI reports then in phase three we got a compute an ACI based on this initial machine learning models and then the interesting thing happens in phase four this is the iterative preference elicitation phase we're going to ask every student a pairwise comparison query and after answering one query we're gonna immediately retrain the machine learning model and repeat and this stakes in practice less than two seconds so this can be done in real time and the student can answer zero or as many of these pairwise comparison queries as they want and this can be done completely asynchronously between the students and we use these very simple pairwise comparison queries where the student just looks at two schedules and then says I like this one better than this one there's no need to enter any values or any adjustment numbers or anything else so to evaluate this evaluate the two mechanisms we designed a new simulator using a similar approach as what Kevin described we paid particular attention how to model the students reporting mistakes and we allow students to forgot forget a base value forget an adjustment make mistakes when reporting a base value or make mistakes when reporting an adjustment and then we fitted the simulator based on the real data that Buddhism Kessler reported in their 2022 paper and here's some of the empirical results from the simulation so on the x-axis you see the number of comparison queries that the students offset 1 5 10 15 20 and on the y-axis you see efficiency here measured as average student utility and in yellow I'm showing the efficiency achieved by course match of course it doesn't increase with the number of comparison queries because they are non-in course match and in blue we see our mechanism end with 10 comparison queries we can increase efficiency by seven percent and with 20 by more than 11 percent and this increase is even larger if we look at minimum student utility so these are the worst students in the population so here it goes up by more than 15 or by more than 24 percent now the last point I want to make is in the spirit of what the typhoon presented yesterday uh in terms of minimally invasive Market design throughout this whole project we um we designed the mechanism to be as easy as possible to deploy and practice uh if a university wanted to do that so first mlcms design is Plug and Play compatible with the existing implementation because you can just you know use the existing GUI then use this new process in between and then at the end feed it if you want to into the course match algorithm second it allows for asynchronous interaction between the students there's no synchronous interaction needed as in an auction third the participation is completely optional for the students you can completely opt out if you don't like machine learning and fourth opting in the into the machine learning component is beneficial even for a single student so even if nobody else does it you benefit from it um yourself because it will help you elicit your preferences better and capturing it better and this is shown here in this table if you are the only one who participates and nobody else does then you will prefer the outcome outcome under mlcm and 72 of the cases you would prefer the outcome under course mesh in six percent of the cases and on average your your utility would be increased by 11. okay let me conclude so the key challenge that I've put forward here is to get the agent's preferences out of their head and into the mechanism and the main idea I want to put forward is to support the agents via an ml based iterative preference elicitation mechanism there are many things to consider uh sufficiently expressive machine learning algorithms for example neural networks making sure it's computational feasible often that means being able to encode it as a mixed integer linear program incorporating prior knowledge if you have it in your domain modeling uncertainty if that's important and then making sure you have a simple user interface in the course allocation domain for us that meant using pairwise comparison queries and of course aligning the incentives of the agents thank you very much 