Guido Imbens: This is going
to be the last lecture, and so let me talk in
the remaining time about generalized method of moments and empirical
likelihood methods. What I'm going to do is just
after the introduction, talk a little bit
about GMM methods. Instead have been around in pretty much the same form
since Hansen's '82 paper, and then I'll talk
about empirical likelihood which is
something that's starting in the
early to mid '90s. I'll talk about some
computational issues there and illustrate this with some panel data model type that Jeff was talking
about on Monday. GMM has clearly been incredibly influential
service or general framework for estimation and inference
since Hansen's paper rates, it can show the setup, the general framework for that. One obvious reason for that
is that many estimators, many models we use fit into
that framework so this gives a very nice
unified framework for analyzing a lot of methods. You can make a distinction
between two cases, one the just identified cases where there's the number
of moment conditions is equal to the number
of unknown parameters in which case you just
set the average of the moments equal to 0, average over the sample
of the observations. Then second,
over-identified case, where Hansen's proposal was to use a two-step
method where first, you get a consistent estimator, you use that to estimate the
optimal weight matrix for the moment and then get an efficient estimator
in the second step. For this over-identified case, empirical likelihood
provides an alternative instead on theoretical grounds, really seems much more
attractive properties than this two-step methods. Some sense, this is
a little bit like the difference between
two-stage least squares. In the end, all the
theoretical advantages are in favor of the empirical
likelihood methods relative to this
two-step method. Let me take a comparison with
two-stage least squares, you can clarify some of
the comments I made. In the end, my condolence, that all the
theoretical advantages where there are any differences, there seemed to be
in favor of Lemma. Certainly, there may well be practical advantages to
two-stage least squares. There certainly are
a lot of settings where it's not going
to matter very much in cases with the
instruments are very strong. Where it may not make all
that much difference. But my claim was more
precisely that I didn't see any theoretical reasons for ever using two-stage least squares. But someone pointed out that
instead of, for example, the clustering was not
implemented with limo, but also with two-stage
least squares. There's certainly seems to vary. Practical reason
in a setting where you would expect it to make
very little difference. I can certainly see the
argument for using that. But that is distinct
from the point I was trying to make the
least theoretically, I didn't see any reason for using two-stage
least squares. As similarly here, I'll make
the case that I didn't see any theoretical reason for using these two-step estimators. It's a fairly safe
claim in the sense that these two-step
estimators just have a light arbitrary
component to them. In the end, they rely on using some consistent estimator to get an estimate of the
optimal weight matrix. But the implication of that
is that two researchers with the same data and the same moment conditions may validly end up with
different point estimates, they use a different
initial weight matrix, end up with a different
consistent estimator. In the first stage, as a result, end up with a slightly
different estimate of the optimal weight matrix, and you end up with a
different eventual estimate. It's clear, that it's
much more attractive to have a procedure that is
invariant to starting values. They have a procedure like
limo that is based on the likelihood function
where two researchers with the same data should end
up with the same numbers. Similarly here, empirical
likelihood, essentially, it's going to set up
an objective function that doesn't require an arbitrary
choice in the beginning. Even though in lots
of cases that may not make much of a difference taking away these arbitrary choices where we have no
theoretical reason to make one rather than the other, and we have no way
of communicating an universal way
of choosing this always gives the advantage
to somebody like empirical likelihoods
or limo type things. Then it turns out
there's actually various versions of this
empirical likelihood type estimators. The differences of a very
different qualitative nature than choosing a first-stage
survey weighting matrix. There's a couple of different empirical
likelihood estimators that people have looked at. There's less clarity over
which one should be preferred. But my sense is that
the choice between them is much less important than the choice between the generalized empirical
likelihood class. As Newey and Smith
called in two-step GMM. A little bit like the difference between limo and
two-stage least squares. These empirical
likelihood estimators can be computationally
more demanding. I think there's some
programs out there that have implemented
these things. But it's certainly clear
what the most effective ways are of implementing
these methods and practices tend to be
computationally any more complicated than standard GMM. Let me set up the
general problem. We're interested in estimating a vector of parameters, Theta. We know what the value of
the true value Theta stars. We have a random factor
C, and we have a moment, we have a moment function that is a function of the random variable and
the unknown parameter, then we know has expectation 0 at the true value
of the parameter. It has expectation
different from 0 at any other value of some
random sample of this C's. I'm just looking at the
cross-section case, the independent case. But this could include panel
data where the factor C's includes lagged values
of some outcome. Just to support the claim earlier that this is very
general setup that fit, that a lot of standard
things fit in to that. Maximum likelihood
fits into that. Where you take as the
moment condition, the score function, the derivative of the log
of the density function. But standard likely theory, if you look at the
score function, it has expectation 0
at the true value of the parameter and shouldn't have expectations 0 anywhere else. This is actually useful. It can be useful to interpret this inner method
of moments setting. It's one way of deriving the distribution of
the maximum likelihood estimator under
misspecification, which was done for this
particular case in widespread, which would now be immediate result of
standard GMM theory. Now the examples. Linear
instrumental variables, you have a set of instruments that is uncorrelated
with the residual. If the dimension of Z is greater than the
dimension of X, where we are in the
identified case where the more moments than
unknown parameters. Last example, and this is example I'm going
to come back to and look at some simulations for is panel data model with the individual permanent
component ADI. A lagged dependent
variable Theta and Epsilon ITS mean 0
given lagged values. In this case, we can construct moment conditions of the following type
where we difference the data and then use the subsequent lags to
construct moment conditions. If you also assume that the initial condition is drawn from the stationary
longer and distribution, you get an additional set
of moment conditions. In principle, you can have
a fairly large degree of overidentification here with only a single
parameter if you have a reasonably large
number of time periods. Having motivated the
general setup in this case, what did we do in the setting? If the dimension of Psi, the vector of moment conditions is equal to the dimension of the unknown parameter Theta, you can typically
estimate Theta by just setting the average
moments equal to zero. The standard regularity
conditions that are estimated will be unique in large samples and
consistent for Theta. That's a little
different if we have more moment conditions
than unknown parameters, then you can in general solve this set of equations because we will have more
equations than unknowns. Hansen's solution to
that was to minimize this quadratic form in
the average moments. Take some positive
definite matrix C, take the inner product of the average moments with
that quadratic form in the form of that
quadratic form and then minimize that. If we do that for an arbitrary positive
definite matrix C, you get a consistent
estimator for Theta star. You get consistency asymptotic
normal distribution within a very messy
variance covariance matrix consisting of this product
of these 11 matrices, where Delta is the
expected value of the outer product and Gamma is the expected
value of the derivatives. If you actually
didn't indicate where the model was just identified, then Gamma would be a square matrix and it would
actually be invertible. The whole thing
collapses, C drops out. You would get back to the
inverse of Gamma prime, Delta inverse Gamma, irrespective of the choice of C. That's a little different in the case where over identification
at that point, the choice of the weight matrix
C is actually important. The optimal choice
for C is the inverse of the covariance matrix of the moments which
makes perfect sense. He gave more weight
to moments that have a low variance than to the moments that
have high variance. If you do that, if you use the optimal weight matrix in that quadratic
formula minimizing, you get the previous formula for the variance simplifies. Now in general, it's the inverse of Gamma prime,
Delta inverse Gamma. That is not directly
feasible because in practice we don't
know what Delta is. We can minimize a
quadratic form, I say Hansen solution in the spirit of
three-stage least-squares. DLS type methods was to estimate the
optimal weight matrix. Here, that is slightly
more complicated because the weight matrix depends on unknown parameters. The specific feasible
solution is to first get a consistent estimator by using some arbitrary
weight matrix, the identity matrix, and just
take the first couple of moments that we get a
consistent estimator. Use that to estimate the optimal weight matrix by averaging the outer
product of that. Use the estimated
covariance matrix as the weight matrix, then minimize the
quadratic form. With that, it turns out that the resulting estimator has the same first-order
asymptotic distribution as the one that uses the
optimal weight matrix, namely this Gamma prime Delta
inverse, Gamma inverted. Irrespective of the way you're estimating
that weight matrix, as long as you have a
consistent estimator for that. Same way, two-stage
least squares, you get the same asymptotic
distribution as you get if you had optimal linear
combination of the instruments. Now, at the same time, it's clear that in
finite samples this may well be an issue. Much the same way, there's issues with
two-stage least squares if we have a high degree
of overidentification. This particular
first-order approximation, may not work very
well if there's a large degree of
overidentification. To add comments here, in addition to doing
estimation here, we can actually test
the moment conditions if there's overidentification by just looking at
the quadratic form. The way, given the
normalization I used, you can just directly look at the quadratic form evaluated at the GMM estimates
under the null, that all these moments
have expectations zero. This should have a
chi-squared distribution with degrees of freedom equal to the number of
identifying restrictions, n minus k in this notation. If you get a very
large value for that, you would reject the
null hypothesis that all the moments have
expectation zero. Now, to the second
point and there may be slightly less well-known. Even when you think about
this two-step estimator, you can be right that as just identified GMM estimator by augmenting the parameter vector. We can fix an arbitrary
fix this matrix C. Then you can think of the
initial estimate here as estimating a parameter Beta. Estimating the variance
covariance matrix of the moments Delta and estimating the derivative
of the moment Gamma. Stacking this whole set
of moment conditions here in combination with this
whole set of parameters. We're now in addition to Theta. We have this matrix Gamma, this matrix Delta, matrix Lambda, and
additional parameter Beta. We have a set of moment conditions
consisting of four parts. But in the end, this gives us a just identified system
where all the methods we can apply to just identified systems can now be used to apply to
over identified systems. In particular, you could get the asymptotic
variance-covariance matrix for Theta and the
general misspecification of the moment conditions. But just writing it in
this way and looking at the Gamma prime Delta
inverse Gamma matrix for this augmented moment factor in terms of the augmented
parameter vector. There's two points of this.
One is to understand that any result therefore, is established for the
just identified case, can directly be translated
into the overhead fight case. For example, the validity of bootstrapping and other methods. But also sort of shows you
where the arbitrary nature is of the estimator and
this two-step estimator. If we change the matrix C here, we're not going to change the asymptotic
distribution of Theta under the condition that all
these moments are valid. But in general,
changing C is going to change the value for Beta. It's going to change
the value for Lambda. Working through all
the other parameters, it's going to change
the value for the limiting value for Theta. Final comment on that and that's the point that will lead us into a empirical likelihoods. Gary Chamberlain
showed that this is two-step GMM estimator
is in fact efficient, so not just within the class of estimators that minimize
a quadratic form, so that it's not just the weight matrix is the
optimal one in the class. But in fact, it's efficient
within the class of all estimators that exploit the full set of
moment conditions. The Chamberlain argument is a very interesting one there. That's really the length
to empirical likelihood. Well, let's suppose
that we actually have a parametric distribution for
the random variables here. Let's suppose that we
have a very flexible parametric distribution,
namely, discrete one with a very large number
of points of support, L, which we know these
points of support. The only thing we
don't know is the Pis. Then given that
parametric setup, we can estimate the Pis by maximum likelihood
namely just calculate the proportion of the
sample observations at each of these points. We can calculate the implied maximum
likelihood for Theta, namely by the
invariance of maximum. That is defined implicitly
in terms of these despise. That estimator for Theta
must be efficient. Is it's a maximum
likelihood estimator. Is by the efficiency results
for maximum likelihood, there must be the efficient
estimator for Theta. What he showed then, if you look at the Cramer-Rao bound for Theta in
this parametric model, you get in fact back the variance covariance
matrix of the GMM estimator, this Gamma Delta inverse
Gamma invert matrix. Given that we know the maximum likelihood estimator
is efficient, the fact that it has the same variance covariance matrix, as the GMM estimator means the latter must
be efficient too. That's a very important
result in its own right. But it's even more interesting in the light of the subsequent empirical
likelihood literature. Let's take this
Chamberlain argument a step a little further. Let's look at a very
simple case here. Actually no unknown parameters. Suppose we have a random sample from some unknown distribution. If you're interested
in estimating the distribution
function there for z, the natural, instead
of pretty much the only choice is just to use the empirical
distribution function. What is the empirical
distribution function correspond to? It's essentially a
distribution corresponding to a discrete random
variable that puts mass 1/n on each of
the n sample points. That's what the empirical
distribution function is. That's the most sensible
way of estimating that. But now suppose we also know that the expected
value of z is equal to 0. The empirical
distribution function doesn't satisfy
that restriction. Putting weight 1/n on each of the n sample observations
doesn't give you a distribution that
has expectation 0 unless the sample average
of z is exactly equal to 0. But in general, the empirical
distribution function wouldn't satisfy that condition. The idea behind
empirical likelihood is to modify these weights a little bit to ensure that the estimated
distribution functions satisfies in this case, the restriction that
the expected value of z is equal to 0. It's essentially
saying, if you don't know anything about
this distribution of z, our best way of estimating
that is just putting mass 1/n on each of the
sample observations. But if we know that the
expectation of z is 0, we've got to be able to
do better than that, because we should only
look in the class of distributions that actually
has exactly expectation 0. We're still going to end up pretty much through the same mechanism as the way we've got the
empirical distribution. We're still going to end up
with a distribution that only puts mass on
the sample points. But now we're going to modify
these weights a little bit to keep them as close
to 1/n as possible. But to ensure that the mean of that implied distribution
is exactly equal to 0. One mechanism for doing that is to write down what is known as the
empirical likelihoods. It's just a product for
all observations of Pi_i defined for all
Pis between 0 and 1, such the Pis add up to
1 and then maximize that empirical likelihood
function subject to the restriction, that the expected value of c and that a distribution is 0, subject to the
restriction that the Pis multiplied by c_i add up to 0. As you this one typo here. Here, I took the log of the empirical
likelihood function. I should have ended up with
the log of the Pi_i's should be maximizing the sum of all observations
of the log of Pi_i, subject to the restriction
that these Pis add up to 1. That their proper
distribution subject to the restriction that the Pis multiplying c_i add up to 0. Now if we didn't have
that second restriction, we will just get 1/n for each of the Pis and we would end up with the empirical
distribution. But the second restriction
for us is away from 1/n. But typically a very minor way, but in a way that makes sure that adding up the
restriction is satisfied, you can actually
concentrate out the pie. Actually, that's not
quite right either. Here the solution N times Pi_i should be equal to
1/1 plus T times c_i. Should be N in the
denominator there. Where the T times
c_i is how much you move away from the 1/n, where this t is the Lagrange multiplier
from that restriction. In this case, where
there's actually no unknown parameters set. The idea is that we look for a distribution
function that puts mass on all the sample
observations in such a way that all the
restrictions are satisfied. This case, the
restriction is the sum of the Pi_i times c_i
is equal to 0. In general, the way
you would do that, at least here, the
longest there for the Pi. But now the subscript
i is missing. This is one of the
latest sets of notes I was finishing here. In general, when we have a
set of moment conditions, we now maximize this
empirical likelihood function which is not affected
by this GMM setup, subject to two sets
of restrictions. The first still the
adding up rstriction and the second now
saying that there has to be a value of Theta such that Pi times Psi evaluated at c I
and Theta adds up to zero. We're going to look for the distribution
function as close as possible to the empirical
distribution function such that all the restrictions
are actually satisfied, such as there is a Theta. It turns out that the key results are
that if you do this, if you maximize this
over Theta and Pi, you end up with an
estimator for Theta that is equivalent
to first order to the two-step GMM estimator. Moreover, what you gain. That's just back
to where we were but the gain is that
for many purposes, the likelihood function here, which is not quite a regular likelihood functions
with more observations, we're going to get more
parameters in there. But for many purposes, the empirical
likelihood function has the same properties as a parametric likelihood
function there. We're going to be
able to do likelihood ratio tests, score tests, Wald test, and effect as higher order
corrections as possible. There's a lot of machinery known from maximum likelihood
estimation that we can actually bring to bear on the empirical likelihoods setup. Now, let me generalize this a little bit, that was the empirical
likelihood estimator. This is a more general class. In fact, there's two
more general classes of estimators that are
useful to consider as, here's one way of doing that based on something
that is known as the Cressie-Read
discrepancy statistic care. This defines the distance between two distributions there. If you have two vectors, p and q, both of
the same dimension, and if these factors
are the same, irrespective of the
value of Lambda, this will give you a value
for this function I of zero. If these functions
differ, this gives you a way of measuring how
different they are. For any choice of Lambda, we can minimize the discrepancy between the empirical
distribution and the estimated
distribution subject to the same
restrictions as before. What this is doing
is it's changing the metric a little bit
for maximum likelihood by allowing different
forms for this metric. Now, why is this
useful? One, I said, empirical likelihood is actually
a special case of this, where we let Lambda go to zero. Actually, let me first name two other estimators
in this class. There's two other estimators in this class that
are actually very useful if we take Lambda is two in this Cressie-Read
test statistic. Then we end up with
an estimator that independently originally
was proposed by Hansen, Heaton, and Yaron, looks very much
like the standard GMM setup where instead of minimizing the
quadratic form, given a weight matrix, they simultaneously
minimize the quadratic form over Theta and the
average moments n over Theta in
the weight matrix. That's another way
of getting around this awkwardness that you need an initial estimate and
depending on where you start, partly determines
where you end up. Here you update the weight
matrix as you go along, as you change the
value of Theta, which is why they call it the continuously updating estimator. But it turns out, it fits into this class of empirical
likelihood estimators. Now, going back to the
original order of the slides. There's another
way of setting up this general framework following work by Richard Smith
and in particular, this is a paper by
Newey and Smith that shows that they obtained some results for
this overall class. But you write the
general problem as a saddle point problem that includes both the
empirical likelihood and the continuous updating
estimators as special cases, namely corresponding to
particular functions g. It's actually here, it's
for another estimator that has run off to three that's actually been
used a fair amount. If you take Lambda going to minus 1 and the
Cressie-Read statistic, you end up minimizing
something that looks more like the
kullback-Leibler criterion than the likelihood function. The advance of that
is that it has some computational advantages where the implicit probabilities are guaranteed to
be non-negative. As I said in the introduction, the much bigger
difference is between the two-step estimators and the whole class of generalized empirical
likelihood estimators. But within that class, there tend to be relatively
minor differences. The main theoretical issue being that empirical likelihoods coming from its close ties to likely theory has higher
order bias properties. It has some theoretical
advantages. The continuously updating
estimator tends to have more computational
difficulties and the exponential tilting
estimator tends to be computationally
relatively stable. But in practice, these
differences are fairly minor. More points on this
issue I raised. Earlier I made a point that the empirical likelihood
actually has a lot of the properties of a
parametric likelihood. Some of these properties
are reflected in the options for testing
restrictions here. One way of doing that
is just interpreting the empirical likelihood as
a likelihood function and doing a likelihood ratio test comparing the value of the likelihood at the
unrestricted estimates, where you just use the
empirical distribution function comparing that to the value of the log empirical likelihood function at the
restricted estimates, at the maximum likelihood
estimates for the Pis. That's going to
have a chi-square distribution with degrees of freedom equal to the number
of identifying restrictions. You can also do a Wald test by directly comparing
the average value of the moments that are very
similar to the GMM setup. You can do a Lagrange
multiplier test for the restrictions
which corresponds to the Lagrange multipliers for these restrictions. When we actually do
this, along the way, you get Lagrange multipliers for all these restrictions and they feed into that
Lagrange multiplier test. Just as in the standard
parametric likelihood setting, all three of these
tests are going to be first-order equivalent and have the same chi-square distribution
with degrees of freedom equal to the number of
identifying restrictions. Setting it up the
way I did initially. They involve lots
more parameters than just the unknown Theta. In principle, this introduced for an additional parameter
for each observation, namely this pie I. Now with N observations, we have an additional
parameter set. The silly, that
would seem to raise a big concern about
computational issues. You're going to want to maximize a function with N parameters. It turns out that is not quite as big a problem
as it may seem. You can first of
all concentrate out the Pis in terms of the
Lagrange multipliers. Given the Lagrange multipliers, there's going to be an
explicit solution for the Pis. That reduces the
computational problem to one where the dimension is the same as the
dimension of Theta plus the number of
moment conditions, which gives you the number
of of Lagrange multipliers. At that point, just solving
the first order conditions, like in this generalized
empirical likelihood setup, trying to solve the
first-order conditions for T and Theta doesn't
work very well. Some point there was
some suggestion using penalty functions that doesn't work particularly well either. What seems to work best is a suggestion by Mittelhammer
Judge and Schoenberg, where we suggest concentrating the Lagrange multipliers and then optimizing over Theta
without any constraints. Partly the reason
that works out very well is that solving for the Lagrange multipliers is typically a
very easy problem. This is where the
particular case of exponential tilting
is particularly easy. For a fixed value of Theta, minimizing the
objective function over the Lagrange multipliers is just a strictly convex
problem as it is very easy first and
second derivatives, you can do this very fast. It's for a given value of Theta. You can solve for
T very quickly. Then you can maximize over Theta having concentrated out set T and I particular
value of Theta. In fact, instead of going beyond the Mittelhammer Judge
and Schoenberg paper, you can actually calculate
the derivatives of this objective function with respect to Theta analytically. Because even though t is
defined implicitly here, you can still get an analytic derivative
for that function. You end up with something where the first derivatives are fairly straightforward
to calculate. This actually seems to work
very well in practice. I think there's actually
some packages out there that implement these things. The thing to mainly Gauss
and MATLAB packages. Let me show you some of
the implications of this in the context of this dynamic panel data
model that I gave before. We have the individual
permanent component, fixed effect Theta I. We're interested
in the coefficient on the lag dependent variable. We get two sets of
moment conditions. One coming from the correlation
between the lags and the difference in outcomes
and the outcomes. The second set coming from
the stationarity assumption. Then what I did there to see
how well these things work, I took some data from a paper by John Abowd
and David Card. But it had earnings for 1400
individuals for 11 years. But they actually used the
minimum distance estimator. First estimating the
covariances unrestrictedly. But I was interested
in comparing this method of
moments estimators and empirical
likelihood estimators, and seeing how well they did in cases with lights degrees
of overidentification. Once we have a fair
number of years, you get a high degree
of overidentification. So I estimated the data-generating
all the variances of the different components on the Abowd Card data with the variance of the
idiosyncratic term around 0.3, as well as the variance of
the fixed effect about 0.3. Then I simulated this data both for Thetas 0.5
and Thetas 0.9. I think to 0.5 is very close to what you get on
Abowd and Card data. Here I also varied the number of time periods to
see how things would deteriorate with the degree
of overidentification. First here are some results for the case where Theta is 0.5. What you see here is that these things don't
make much difference. Both in a two-step GMM and the exponential tilting
estimate I calculate here, both do very well. Coverage rates for the 90 and 95 percent
confidence intervals are very close to nominal ones. The median absolute error is
very similar in both cases. That changes considerably
if you look at a case where the degree of
persistence is much higher. If you look at the case
where Theta is 0.9. Now you see, now in
some sense that's going to weaken the effect of the moment conditions. You get closer to settings where the moment conditions have
very little influence. Now you see the two-step
GMM estimator deteriorating considerably when you use large number of
moment conditions. By the time we use
11 years of data, the coverage rate for the 90 percent confidence
interval is 76 percent whereas for the exponential
tilting estimator, the coverage rates stay
much higher both for the 90 and 95 percent
confidence interval. In the end, consistent with all the instrumental variables instead of the week
instrument findings once consistent with
the theoretical results in Newey and Smith, David larger group of
overidentification. These two-step estimators have considerably poor
properties than this whole class of empirical
likelihood estimator. From a theoretical point
of view, in particular, based partly on the
Newey-Smith results, where they establish
formal bias properties for these empirical likelihood
estimators that are much better than for two-step GMM. But partly based on the fact that is this arbitrariness in the two-step GMM estimator
that can just be avoided by using these empirical
likelihood estimators. On theoretical grounds, there's clear reason to prefer these empirical
likelihood estimators. Especially given that in
combination with the fact that the computational
differences are not all that big anymore. That is what my recommendation
would be there. I think this is it. I think we do
actually have to stop on time here because they're using the room for
different function later. But are there any
questions here? 