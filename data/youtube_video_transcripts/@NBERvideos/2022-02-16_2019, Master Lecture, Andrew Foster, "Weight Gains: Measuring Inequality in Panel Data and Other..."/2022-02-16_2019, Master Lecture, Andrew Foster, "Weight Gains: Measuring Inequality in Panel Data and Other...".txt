uh it's pleasure to see so many of you here and um i wanted to talk about something that i was never really trained to talk about and uh which i never really meant to study uh because i think many of you are in the same boat uh but uh you know most i'll talk about sort of four different projects um that all sort of led to led me to think about this issue of weight gains and just in case anybody's wondering this is really not about anthropometry and it's not about my anthropometry uh it's about samples all right so i mean the starting point and this you know even has come up and you know mark in my most recent paper that sort of stratified sampling is it really has played an important role in the collection of development data um some of you who've been out there in the field and you know done randomized trousers and so forth may not have thought very much about stratification but it's it's kind of at our core i think among the most prominent data sets that uh that you know we examined at least um uh you know when i was uh assistant professor and so forth um and continue to use uh sort of are stratified by income or land holdings so stratification is is quite important the original sat surveys they had 40 houses per village 10 from each land category and that was not representative of course of the land distribution those villages the aris reds data which which i've helped to sort of collect and work with and worked a great deal with over the last years was also initially stratified by income there was a process of updating weights as the kind of keeper of those data i'm always getting questions about what the weights are i have some answers and and even trying to figure out what the original people did is a little bit confusing but the bottom line is that there was stratification um one might argue it's important um and we don't get taught very much about that okay um as i you know as i say i sort of as i go around i distribute data talk to assistant professor or even tell people i happen to be working on weights i get sort of a sense that people think of the waves as a nuisance they come with your data there's something your referees can tell you is wrong uh but nobody can tell you what is right and i think that's basically sort of at the heart of the issue that dealing with weights there's rarely a mechanical solution to the problem it depends on what question you're asking what issues you're pursuing and so forth and so the way i'm going to focus this is not to give you a laundry list of how to sort of think about uh think about weights from a sort of a recipe perspective but i'ma talk about four projects that along the way i realized i had to think about weights and um and maybe inductively we'll get you'll get a sense of why i think it's important at least why it's something that we should that we should uh at least think about and why it's kind of fun actually maybe you won't believe that but but i really think of these as adventures because indeed i didn't start off um thinking about weights and along the way i realized i had to think about it seriously okay so the four topic papers are going to talk about the first is measuring changes in equality from panel data okay constructing the second will be constructing estimates of economic mobility over multiple generations again using panel data a third won't be evaluating changes in spatial distribution and well-being as development proceeds and the fourth really looking at the effects of democratization so some of you may have clues about which of my papers are these all involved with but i don't expect anybody's looked at all of them okay so the first two are work uh that i've done uh with militia uh with uh sveta milicevic now at the bank um and the the starting point is this uh issue of how do you construct representative samples from panel data okay um so sort of this you know we collect these panel data sets we typically collect samples of households and we have some there's a reason for that there's sort of a sense of how households are important for resources we can talk about how households uh that sort of intra households is also very important but to some extent households play a primary role in the allocation of a lot of resources and it's just easier to collect data that way most of the time okay but when you've worked with but you know that's like the first time you're going to collect a household survey data you collect the households but then you sort of go try and go back to those individuals later and you realize that the households have recombined and this is something that you know duncan has confronted any of us who have collected panel surveys have had to deal with and i've sort of called this they recombine these households kind of mix in various ways over time okay and so one of the things that sort of arose and and as in sort of thinking about this is i was have long been interested in the process of household formation dissolution and then i've recognized that weights come from are based on households and we sample households in some deliberate way and so one of the kind of questions uh that sort of arose that i that maybe some of you think you know have an answer to but i never really knew the answer to until i sort of started working on this is that is is the question about whether the fact that the weights are responding some endogenous variable means we have to worry about the endogenous weights in our analyses right it's not something that none of us uh ever does and yet we all use household survey data many of us use household survey data with weights and uh so i needed a way to think about that okay so all of this pro this work comes from another sort of uh you know long-term data sets i've been involved with um in in some respects from the early 70s when i was in bangladesh as a child but although i wasn't collecting any data i did see the data sites um the the uh these these are vital registration data from bangladesh you have the demographics for surveillance system which has 40 years of vital registration data um with uh detailed censuses in 1974 and 1982. what makes this i think interesting is the comprehensive it's sort of everybody in the in the population um over time now i in 1996 and in 2014 uh we have collected panel uh we've done a panel survey the sort of first economic data that sort of integrated into the demographic surveillance system the demographic surveillance system like many such systems that expressed around the world was really there for just tracking bodies the basic idea was in low-income countries the only you can't measure mortality unless you know what a denominator is and so you have to have a basic vital registration system in place in order to look at mortality rates and vaccine trials and so forth so there wasn't much economic data collected until the surveys that we did in 96 and then most recently in 2014 one feature of this and this has been it turned out to be in retrospect the really dumb idea is we had this idea well the sort of social organization in bangladesh households come in baris which are sort of groups of houses are related typically we decided to sample in that way it turned out that that made things a lot more difficult when we actually started thinking about uh the the sort of agenda i'm going to pursue for the in the moment okay one of the reasons for doing the 96 survey and the 2014 survey was in fact that matlab is as some of you know the sort of site in 1978 of a a trial of a family planning and maternal health program that was introduced in 78 and was really from my perspective the first program that really showed that that providing uh high quality services in a low income setting could result in reductions in fertility uh it was not of course a randomized trial it was in 1978 uh but uh anyway that was you know it was the one of the reasons for collecting the original data and the subsequent data is to look at that and i'll talk a little bit about that but here's the issue that i want to address first is how do we handle the fact that we have a representative survey in 1996 okay but we can think about the antecedents of those households in 1974 the antecedents of the 96 households are not necessarily a representative sample of 1974 and similarly the descendants of the 1996 households that we would go often interview are not necessarily a representative sample of 2014 and yet if we want to look at changes in inequality over time we need to actually turn the panel data into uh into representative samples so we need to figure out what sampling weights are okay another thing that some of you may have noticed is we did something a little funny here at least from the perspective of modern sort of trial literature we were interested in studying an invention in 1978 and we drew a random sample in 1996. okay you know that in retrospect was kind of dumb we should have taken the random sample in 1974 which we could have done because of the vital registration data and then followed the households forward um so we would not be so worried about the fact that the council composition in 96 might actually be affected by the family planning program and that's where this issue that i talked about uh we have this these households that are industrially formed as a result in part of the family planning program we've got ways for 96 like does that matter okay so hopefully you'll be convinced i know the answer to that but i'm not going to tell you yet okay okay so you know um i didn't even really when we started this didn't really know how to even think about this problem i just wrote a grant proposal or it was written in a grant proposal that i was part of said andrew foster is going to solve the problem of weight okay so i realized along the way that that you know the the key um that i needed to kee to take the 1974 antecedents and turn it into a into a representative sample and this kind of this this um there's this sort of general issue in any of these households that are panel surveys if you want to take a cross-sectional take a given round of the survey and make it cross-sectional so you can compare sort of cross-sectional differences over time time now it turns out that exactly how this works um or the the sort of sampling problems that arise depend on the rules okay so when the the red survey was initially done that was really the error survey in the 67 the 6971 period then there was decision made in 1982 to follow up the descendants of those households okay the rule was we were going to only follow those households in the event that either the head had not died or the household had not split okay so in a case like that assigning weights to the subsequent set of descendants is not hard okay because there is at most one descendant from every 71 household okay uh because we don't we don't have any splitting happening because as soon as the split happens um then we're going to either lose one half of the household or if the head died we're going to lose that household entirely and then the rest of the 1982 sample came from a random sample was backfilled so you had you had sort of sampling weights from 82 that came from the original survey and then you had a representative sample of everybody else okay in the matlab health and socioeconomic survey as is i think more typical uh is you would um you would do a survey um and then you would try to follow everybody who was in the original household into their subsequent households 10 to 20 years maybe people who are migrants you'd want to follow them that's certainly perceived as best practice but certainly people that were in the same area you would want to sort of follow all the different groups of people there's a good reason to do that because it maximizes recontact rates and so you have the maximum number of individuals in both surveys and for some of our analyses those differences are going to be really important but it does sort of complicate this issue as waiting as i hope you'll understand in just a bit okay but the main point is in you know sort of doing these kinds of resurveys you're sort of setting setting in motion a different procedure for how to think about panel weights in a cross-sectional sample okay now there's nothing on this slide that i that i knew um uh up until about a year ago okay so we were kind of working through trying to figure out what the right thing to do when we knew what the right thing to do was we actually went into the statistical literature and found that indeed at least to some extent some of the ideas that we were working on were ones that were in place not all of them but some of them but i wanted to introduce you to this these sort of two ideas because they're really important and to some extent they're surprising okay at least i found them surprising okay the first is perhaps not but it's sort of how it's constructed it's important to see okay so the idea is that what we're talking about is something that's known in the literature as indirect sampling where you do a sample of a population and then you reconfigure that population in some way and so then you have this new sample and the sort of the probability of observing anything in the new sample depends on the original sampling distribution plus the process of reconfiguration okay and we sort of all know what to do if we do a you know if we just do a cross-sectional survey and we have probability weights there's some some notion that we kind of use as a weight the inverse of the probability and one way of thinking about is is that where that's where this comes from this is the horowitz thompson estimate okay and the the basic point is imagine you're interested in a quantity sub the sum of y i in a population okay and you have a sample okay well then you can have an indicator i which tells you who's in the sample and who's not and for every person in the population in principle but you don't really need that you have pi i which is the probability that that household was sampled okay and so you construct this thing the sum of y i times big i over pi i okay and the whole point of the orbits thompson estimator is simply if you take expectation of that you can bring the expectation inside so you've got expectation of i i what is the expectation of ii well that's simply pi i and so and so you're going to get pi i or pi i so that's going to give you some of y i okay and so what's sort of neat about that is because you have this i i in the first expression you only need the y's for the people in the sample you don't need to worry about the y's for everybody else and yet you can construct an estimate of the population level okay and notice this does exactly what you think you would want to do which is take the probability of sampling and take and invert it and and sort of add it up to get weights okay the only complexity is well then how do you calculate pi i in the case of indirect sampling okay well basically you just need to kind of work through and say okay well i had some original sample there's a probability that people were sampled and then i can follow up people later on and every if we think about these households and i'll go through an exercise in just a minute every household is composed of people initial sample so the probability any subsequent household is in your sample is going to be some combination of all the probabilities of the people who live in that household at that point in time okay so that was one thing now the the second thing and again i'll illustrate this with an example in just a bit a bit is is is that there's this fundamental problem and this the best way to think about this if you're familiar with something like the psid in the psid right there's an initial sample of people we the the houses were interviewed subsequently if they were descendants of the initial samples that were there okay the probability of sampling of that household depends on a bunch of stuff that isn't known okay because the probability that that psid household was picked depends not only on the particular sample that was drawn on in 1965 or whenever the first sample was drawn but on all possible samples that could have drawn in 61 because there's some sample that it would have brought in the wife instead of the husband okay so it would seem like to construct the pij for indirect sampling you would actually need to know the probability that every single person in the household is a descendant of someone who could have been in the original psid sample okay and that is something we could never know maybe in sweden you could know that but you could never know in the us okay but it turns out that there's a little bit it uses the same kind of trick and this is this that that um i just wanted to show that i probably won't uh explain it give do it quite justice um is that you have um a set of people who are in some uh who belong to the descent in the um sorry you have you have a uh household eye and a particular period you have a set of antecedents of that households um and and basically you can construct this thing which is gamma j times the times i j over pi j which is the probability of any particular individuals in the sample okay and as long as these gamma is sum to one and this will be clear in just a moment you can still construct an unbiased estimate of the sum of y eyes okay so let me just show you how this works so here's an example of what we faced in the context of matlab okay so as i said what we did was we had an original sample of people okay um and we had this bari level sampling okay just for the for this stylized example uh we had a 50 chance of sampling this bar you know 50 chance of sampling this bar and we sampled one household for bars so if we got this household if we got this bar we would pick one of those three so the original sampling probabilities of a b c and d are 0.167 uh up to point five okay then if we think about the 2014 sam uh help us sample okay there's a series of households that are all descendants of the blue households okay and the question that we would like to ask the pie ij that we would want if we want to turn this into a representative sample is we need to know the probability that this household ends up in our descendant sample and the question is what is that probability okay well the probability this guy's in the sample depends on the probability that any of these three links was chosen which is simply the probability the first bar it was chosen so that's got to be 0.5 okay similarly the probability this guy's just is going to be 0.833 and so forth okay so the probability of a particular household being picked depends on the full set of links and the full set of the full probability distribution over the initial population okay the problem is that in a normal data set you only picked one of these households let's say this guy okay so you know 0.167 you follow that household and you end up with this household here okay now you might be tempted to say well this is probably the assembly this helpful is 0.167 so maybe the probability of sampling this health is 0.167 but that would be wrong okay in order to get the right number here you actually need to know if i picked that household what's the uh would that household be picked and the answer is yes and if i picked the third household would i would i think a household would be that and so forth right to get to the point five okay so what i want to convince you of is that it would seem like this is an impossible problem okay in a normal panel data set because we only know about this held in this house but we don't know about these links at all because the puzzle b wasn't sampled at all okay well this weight share estimator sort of uses a little bit of a sort of a funny thing a little bit of a trick because now think for a moment if in one state of the world i pick a and so i might assign a particular weight to this thing of 0.167 okay on the other hand in another state of the world i might get .167 which is i would also assign it 0.67 and or i could get this state of the world and i'd also assign 0.167 okay in other words in different original samples i'm going to get different households so i can ask what is the expected weight in other words each weight will be wrong but the question is on average am i right okay and the shared estimator says yes okay so how does that work i mean this in this case is kind of an interesting one because it's 0.167 times 0.167 and so forth okay the only trick is that the weight you want to assign here okay is the weight which is one over 5.167 divided by the number of lengths which is in this case okay how is that going to work well so what's so yeah one over point one six seven eighty nine okay we divide by three like two okay um but we're going to take that see .167 is the denominator from that calculation what's the probability of that weight being realized it's this 0.167 to the point where 7's cancel out okay and so it turns out that that happens in each state of the world and so it turns out it's what you do is if you if you figure out what the probability of this thing is because you 0.167 you picked up this household and you divided by the number of links then on average your estimate of the weight would be right okay and that's the shared estimator and it doesn't actually have to be 50 50. there are other there's other tricks and we've done some other work but that's the basic idea it's a little bit remarkable but in order to better understand yeah go ahead because the loops that you're dividing by are in the second period yes these three now it's a good point because it does suggest you need more information you typically have in the typical panel survey right most of our panel surveys you can't actually ask our 2006 households or we don't usually ask our 2006 or 2012 households like of your descendants which ones could have been in the sample that we did in 1996 and you would ask them they would probably laugh at you right but in principle that information will be available and in vital registration data it is available okay well it's worth sort of thinking about this a little more concretely by talking about the psid okay because for a long time we couldn't figure out what the psid was doing and what they're doing is a lot like what i just said they have this when they put it together they have this idea we're going to use the like mary's like solution the basic assumption is you have a you have this descended household you have a man and a woman let's say the man is the guy with a psi dna okay he's the guy who makes the household become a psip sampler okay we know what his weight is oh sorry yes you're recording okay um i guess i'll have to i never do this but is there a pointer or i can i'll point with a mouse if i need to okay so then the idea will be what we're going to do is going to assume the husband and the wife are the same okay now that we know the probability that the husband was included in the sample because he comes from the psid lineage and the people have been calculating statistics about that guy since he was you know 12 right um and we're going to assume he married someone just like him again that seems like a crazy assumption okay because yeah we kind of marry people like us but we often marry people a little different from us too right a little boring to marry someone's just like you okay anyway that's the psid some assumption okay and what i sort of work through and say well under what conditions does that actually work okay and the sort of the the whole point of this little exercise okay is that we can that it that this works as long as the probability of being picked is really small and that's a really good assumption for the psid it's like four thousand people out of the entire population the united states okay so the idea is you can kind of figure out what the sample weight is by essentially saying you know if the husband is p and what is and the wife is is q if we actually knew the wife and the husband's probability of being in the original sample or their antecedence being the original sample okay we can write down an expression we can approximate that it's something like 1 over q plus p p plus q and then it has this other term okay so this would be if we did it right that is we actually knew the husband's probability and the y's probability and they were different okay the other alternative however okay um is to essentially make the assumption that these that the husband and wife um are going to be the same okay and basically the expression here works that out what if you make the assumption that the husband and wife are the same you assign the wife the probability that the husband has and vice versa and you realize that sometimes you get the husband and sometimes you get the wife over different draws of the psid sample and it turns out you get this expression if you do it the wrong way assuming like mary's like and you get this expression in if you actually had the information and the point is at least to first order these things are pretty close as long as the sample size is small you can use the life mary's like assumption and not worry about it too much and it and the reason it works is your weight is wrong for every household but on average it's right overdraws of the sample okay now it turns out that that didn't work for us for a number of reasons the first is that we wanted to go from 96 and go backwards okay now there's an advantage in going forward because we all have two parents okay but we don't all have two children a lot of people do but not everybody okay so if we all know that you know that i had two parents and my wife had two parents and so we can all we can kind of figure out that the sort of number the number of links in our household is going to be at least if it's a simple nuclear household is exactly two it's always two but if you go the other way you have to deal with the fact that some people have three kids and some have one kids and so it's not nearly so obvious how to how to proceed so that doesn't work okay our sampling is not necessarily independent which is sort of an assumption in there our sample is not a small fraction and it also there's also a sticky issue and this arises the psi t2 a substantial fraction of people are not in the frame that is the calculations the assumption i was making only made sense if your wife's antecedents could have been in the original psid but in fact if your wife is an immigrant there was no way that she could have been in the original psid and so you shouldn't be dividing by two in that case and we have a lot of those because we're dealing with marriages that go across borders okay so now let me show you what sort of what we did in fact okay the first is to understand the data we have this longitudinal data we have data at every point in time on who's living with whom okay that's a sort of us an understructure for the household survey data we have at in effect three points in time okay so what we're going to do is we're going to create links between households based on people okay so the idea is that if we have a household in 74 and somebody in that household is also living in a household in 96 then those households are clearly linked okay but we can also imagine a case where someone in 74 lived with somebody who then is in a 96 household and that's what we're going to call a first order link and so forth okay so we're going to create links based on residents so all of this linking that we're doing is based on resonance patterns not on relationship data and we can talk a little bit about whether that's the right thing to do uh certain it's what we did and i think there's good reasons to do it okay now it turns out there's a that because we did it that way there's a nice little analytic structure that we can use okay so so we have a set of households in period one and a set of households in period two the set of pills households in period one um our p we can take the people a b c d and period one and partitioning them into two households alpha and beta okay that's what this matrix is and in period two we have some of the same people and some new people and they're partitioned into households okay and we have another matrix which maps the people at time one into the people at time two okay that's really all we need in order to create these links these links the first matrix we need is we just take our household matrix our partition of how people into households and multiply by itself and we get something like this this says a and b live together and c and d live together and there's a comparable one at time two okay and then we can construct this c1p1 c2 matrix okay so those the co-resonance matrix the sort of matrix that match people into people and the co-residence matrix again and now we get a set of links between people in one period and people and two and the thing that i want you to recognize is that mr a is now linked to mr e okay why are they linked a and b live together and a disappeared by the second period but a lived with b and b later lived with e right so this mathematics works out so this now describes uh where people how people are linked across time and then if we pre and post multiply by our household variable again we end up with this matrix that relates households at different points in time so now we have a matrix that describes that links households at one period of time to households at another period time accounting for all the possible resonance patterns as they happened over time okay this is what it looks like in practice okay this is from the matlab data and the thing that was very striking to me about this is 74 to 96 and it's one of the reasons we didn't want to just use relationship data link households these households tend to be they're not that complic complex okay you know maybe there's a mother-in-law uh and maybe a couple of brothers living together occasionally but it's not like you they're not huge households nonetheless there's a lot of shifting of co-residents that happens over time so if you want to take a set of 74 households sorry just yell at me i'll go back it's like my dog all right i'll learn eventually um all right so so the the sort of point is that this 74 household has a lot of 96 links okay now why is why is the density of this matrix going to matter because the denser the matrix the more the 74 and 96 samples the more you're going to need to re-weight in order to get a representative sample why is that well take this household you know do1 he has 11 households in 96. therefore in a simple representative sample he has 11 chances to be picked okay so do1 in the 74 sample is going to be way over represented relative to everyone else okay so if this was just an identity matrix you know we'd all be fine we could say okay we just take the 96 weights or even if it was just random sample we take just one and we apply it to 74. but because different households have different number of connections to other households when you do this process of linking 96 to 74 households to get the set of 74 antecedent households you're going to get very heterogeneous weights across households based on the number of things in a row and the same thing is going to happen when you go from 1996 to 2012. okay if you have a lot of co-residents going back and forth you're going to get very different weights in 2012 then then what you would just get by taking the 96 weight and moving it forward okay so this so then the question is well this pi ij you know this is this the weight we the weight we need in order to construct this 74 probability of being sampled in the 2012 estimate now the question is how do we get it okay so and the answer it turns out to be fairly straightforward because we're working with demographic surveillance data okay and because we were the ones who actually drew the sample in the first place okay because what we can do is we drew the sample in 96 and that yields a certain set of 74 anti-seiden households but we can rerun that survey that sample a million times it's just a computer exercise and each time you run it you get a different set of 74 households you can count up how many times each household appears in the and household and that's your estimate of pi i j okay and the point is you could never do this analytically because of all the dependents different people are likely to be living together or not living together and so forth these are not independent draws but because you have access to the full set of links and you have access uh to the sampling frame of the original survey one can actually construct the antecedent probability weights and then construct representative weights and we can do the same thing going forward uh to the later sample okay now um one thing that i said or that i sort of raised and maybe some of you have the answer okay is well these weights they're clearly endogenous to all these co-resonance decisions can we take them as given okay well first i want to show you during using a little this is you know some of you will sort of recognize this problem immediately but let me just show you a little state of simulation okay so i'm going to generate a random variable generated an independent x and generate a y okay hopefully you've all done this before we're going to construct a sampling probability that's based on the endogenous variable okay and i'm going to show you that in fact the endogenous variable is correlated with the sampling probability okay so here's our problem right and the question is is it a problem well first i'm going to regress y on x in the sample okay and remember the true coefficients were one on x was one and the other one was i think zero yeah zero okay and obviously if you don't use weights you get biased estimates okay you regress y on x only in the sample data you don't get the right estimate of the coefficient on x okay but if you do use weights that's the bottom panel you get it bang on okay so the fact that the weight is endogenous with respect to the outcome variable does not present a problem as long as you use weight in the estimation okay now some people know may know this result sort there's a literature on choice-based sampling it fits exactly into that context why does it work well the reason it works is because the randomness associated with whether someone's inside the sample condition on their probability which is observed is all about sampling noise and that's true random noise right it's not like the choices that people make it's true random noise and therefore that noise whether you got included on the sample conditional on your probability okay that noise is uncorrelated with anything right the sample is being drawn on top of all this behavior stuff but the sampling probabilities are not correlated with them at all and that's basically what the formalism says if you just think about the sort of the relationship the the x i e i what you would need to construct in order to find the bias of an ols estimator okay well you want to multiply by the i over pi i that we've seen before you can take expectations through and you get exactly the population and estimate of x i e i so as long as the population x and y are not correlated when you use the sampling weights they're not correlated as well okay so this thing that worried about me that i worried about for 30 years turns out to be not true okay hopefully none of you had to worry about that okay does it matter okay well here is just so here's um one variable distribution of household size with various kinds of weights okay since this is a vital registration data we have household data for the entire population 74 we know what the truth is but we also know what our sample of antecedents looks like okay and so we can actually see how things line up so the actual population is the black okay and the population weighted is what i would call the right thing to do okay which is to do it the way that we did it by simulating these links and so forth and you see that the red and the black are pretty much on top of each other okay now all is not lost the green curve is doing another thing that you can do if you don't have proper weights which is basically to run uh you know a logit if you have the if you we you have the 74 census population you know which were in the households and you have observable attributes that are both in the census and in the household you can do a predicted probability of being in the survey okay and that's the green line it also works pretty well okay but the other things that you do if you just use it unweighted you're obviously biased okay you have two big households why is that big households are more likely split so they're likely overrepresented in the subsequent sample and if you do kind of the standard thing of assigning the weights based on the original household it doesn't fix things at all okay so this is just my graph to say all that work maybe was worth it okay so i think oh and then this is you know birth data okay again in some ways there's no reason to use the weight data for this i mean the burst series is published by icrb but there is a problem which is icrb the the that has the matlab data they have vital registration data they know everything about everybody for 40 years they will not release that to anyone no matter what kind of clearance you have but they allowed us to link data that was linked to the people in our survey okay so the question i'm asking here is imagine you only had our survey and you wanted to construct a birth series could you could you replicate uh what actually comes out of the data and the answer is yes if you don't use weights you get the green curve you're way off but you use these weights the the black r curve is the blue one and the black is the truth they're pretty pretty close with a little more noise okay um and then this goes back to the original question i started with okay and this you know in the paper itself there's a whole motivation of what we're actually looking for here but this then takes our measure of the distribution of educational z scores at three points in time in matlab over a period of tremendous economic and social change uh between you know 74 when you know george harrison had a concert and everybody thought bangladesh was a disaster to uh to 2012 where it's sort of considered a place where has done been extraordinarily effective at least in basic human resources and the economic growth is plugging along despite the political mess okay um and so you see not surprisingly a shift up in sort of educational uh uh you know the educational performance of the kids but the interesting thing that we sort of point to in this paper is that the variance doesn't change very much okay and this relates to sort of the paper that the fundamental question kind of drive this paper is that to what extent does fundamental investment in human resources which matlab in particular and bangladesh in general has been very good to good at lead to a compression in outcomes and i think the answer from this graph is no okay and then i'm not going to be here to kind of debate through all that but anyway that was that's a substantive point of that paper which i refer you to okay so that's sort of what you have to do to cross-sectional estimates okay and it's just you know it's it's required us to think a lot and provide us some new insights in how weights work okay now what about mobility okay so we have this we have panel data so in principle we should be able to look at mobility but we have a problem it's a little bit of our own making okay which is that we did the survey in 96 we want to go back and create a 74 representative sample and then we would like to know how well the this the the people who started in the given 74 were doing in 96 and later in 2012 okay now if we'd actually started with a 74 sample that would have told us who to interview in 96 and therefore who did interview in 2012 but because we went backwards and forwards okay not every household in the 96 sample that's a descendant of any given 74 households is going to be in the survey okay so we have another problem to fix okay which is the fact what do you do if you only have a sample of the descendants from the original sample and that sample in our case was being driven by the fact that our original survey was done in 96 okay now this leads to questions to a sort of a fundamental set of questions that i think we all should be interested in about like if we're measuring economic mobility over time in a developing country like what are we interested in okay what is the right concept so you know when you're looking at mobilities or economic mobility over time it's kind of nice to think about individuals because that's not very complicated you have an individual one point in time see how they're doing look at them eight to ten years later see how they're doing did they go up did they go down it's a well-defined question but when you're talking about 40 years it's a silly question right we probably don't really care so much about whether someone you know uh they're earning when they're 20 versus the earning when they're 70 or whatever 60 if i do the math right okay so individual well-being is not really a great way of thinking about 40 years of economic mobility and perhaps how it's affected by access to health and family planning services a more standard thing to do is biological descendants okay and this is very natural thing you take the 74 given sample and you look at their kids and you look at their grandkids and you know my colleague john friedman is doing that uh using you know the wage date in the u.s and i you know i don't want to be too critical as a result however i think there is a fundamental problem it's it's not well suited to what we're we're trying to do because the problem with descendant surveys is they're asynchronous right we all know somebody who's like uh who uh where an uncle is younger than their nephew right that people get out of sync so if we start with a given set of households in 74 and then we ask like what's the state of their sons in 2012 their sons will be lots of different ages and therefore experiencing lots of different things okay so then you have to kind of figure out how to factor in age or assume that age doesn't matter you have to do something to kind of finesse that okay it also from my perspective doesn't work very well when there's a lot of by people living with you who are not sort of direct lineal biological descendants your brother uh daughter-in-law and so forth okay so what i what i wanted to think about in terms of sort of dynamic change is the population of households in a particular region over time okay so we have a given survey in 74 we have another survey in 2012. we have a set of links at link 74 to 2012. i would like to know a given household in 74. what is the distribution of outcomes of their descendants in 2012 based on a particular set of rules having to do with co-residents that i've talked about before so that's what we're doing you may like the other ones but that's your problem not mine okay um and so this is what i was saying that in fact we're we're going to have this sort of uh a a non-random a problem of non-random sampling of descendants we have to kind of figure out what to do okay now to give you some sense of this okay here's a little stylized a you know a model okay so we have two descendants and if we if we observe both of them we would say their average income is y1 plus y2 over 2 right and that would be our measure of mobility my income relative to my the average income of my two sons okay and if the sampling is independent that's going to work fine because sometimes we'll get y1 sometimes we'll get y2 sometimes we'll get both but the average across all states of the world is going to be still y1 plus y2 over 2. obviously it'll be noisier if i only get one sun but on average we'll get one sun or or the other with equal probability okay but because if the sampling isn't done independently which is not in our case okay it may well be that maybe two states of the world observe either you observe y one and y two or you only observe y two okay so in that world this the average of my descendants in the sample is not going to be y1 plus y2 over 2. okay it's gonna be a quarter y1 plus three quarters over y2 and if the sampling is correlated with the income my kids that's going to be a problem okay and so we have to kind of figure out well what are we going to do in a case like that where we are not going to have necessarily a random sample of our descendants and we have to fix the problem okay well this is another i'm going to call it adventure maybe you aren't convinced the last one's adventure maybe this one will strike you as an adventure and it became quickly clear to me that there's lots of lots of things you could do and so i want to see well is there something that you could do that's basically right okay so in other words like how are we going to pick the weights in different states of the world on my two sons so that on average i get the right income of my sons and which i want that result to be fairly robust in some sense okay so there's one equation that basically says the expected value of whatever i do in terms of weighting my two sons has to equal y bar that would be a necessary condition and the other thing is we'd like the variance to be small okay and so i wrote down the variance and it became pretty quickly clear that there's no way i could like we could solve this thing for w1 and w2 because it would depend on y1 and y2 and if you knew what y1 and y2 were you wouldn't have a problem to begin with okay so i sort of mucked around a little bit took a couple of derivatives thought maybe a derivative would be a good thing because we want our results to be not very fairly robust to the different values of income so i ended up with this criterion function which is that the derivative doesn't change very much the second derivative is is is not very large respect to the income of the first and the second sun and that the expected values work out okay so i did this maximization and much to our surprise we ended up with this very simple formula which the weights you should should apply should depend on the original probability the 96 probability of sampling a household probability of i which is the probability of the original household which we've calculated in ways that i've talked about before and n which is in this case the number of suns okay so this is what i got okay this should look familiar to you or should ring a bell this is actually the weight share method okay i was you know i came up with it but of course then i realized almost everybody knew that okay at least within a certain class of people so so in any case we came up with a sort of estimate and then we then were able to sort of uh formalize it and then use that to look at look at various uh definitions of mobility weights now one of the things we were kind of interested in is well all right when is it a good thing to use this procedure the weight share method which we know has some potential problems because we're basically sort of uh you know we know that on average we're right but in any particular case we're wrong versus doing something else like just taking the average income of my kids recognizing the occasional i get a biased estimate so we set up the little simulation exercise we have three states of the world and the three states of the world are sort of generated by this thing and the parameter delta tells you how correlated the probability of sampling is with income okay so we ran these simulations to get some sense okay uh well what would happen to our estimator based on as the correlation between sampling probabilities and incomes changed okay and the green is our weighted estimate and the good thing which is what i built into my maximization is that it's fairly robust okay the green line doesn't move very much as the sort of the correlation between income and sampling probability changes okay the red curve is if you use no weights you just do what you might normally do which is let's find all the descendants we have and to calculate their income and and use that that works pretty well that works great if delta is very small there's no correlation between sampling probably the income but it gets really bad if there's a high correlation okay so here's the graph gives us some sense now you might ask well where is your data okay and this is one of the most disappointing answers to questions that i've ever had to give which is it's right here it turns out it turns out that that that the sort of correlation between there is a correlation between incomes and sampling probably in our data set but it's exactly where it doesn't make a heck of make any difference at all of course we didn't know that until we did it right so i a little bit justified but anyway it's just sort of a fact now the the other point that's worth saying is that as i suggested there is a real problem with using the weight share and this is something we all know about using weighted data it tends to increase variances and indeed when you use the weight share methods the variance is much much higher than if you just do the other okay all right so how so we now have a procedure where we can actually see what happens if we use population again we have to use household size of dependent variable because that's something we can actually take from the vital registration data so we can check and see if our approach works this is a regression of household size change between 74 and 96 and a series of initial conditions i wouldn't worry too much about the specification or why it's there and so forth the main thing is we can use our formal approach and that you know these estimates end up being pretty close so in the previous graph we just taken it out that's kind of here right and so yeah we're not doing a mean squared error here this is just the variance of the estimate across draws but yeah you'd want to put them both together i think is what you're getting at um but you'd have to combine sometimes two graphs yes okay even simpler so if you go back to the future there's still a level bias oh there yes yes there is still a level um let's see uh yes and i think yeah i can't right off my head remember what that source is or where that came from right because yes you can remove it yeah yeah um okay so anyway the main point is that the sort of formalism does mimic the population which is after all what we're after right we're trying to see if we have a if we have a rep we have a um we have a survey data that's a sample we'd like to replicate what we do if we had all the data for the population okay so so you know here's an important point because replicating what you get for the population is not the same thing as replicating the population of any particular coefficient okay so there's a you know paper that some of you may know in the jhr what are we waiting for and it makes the point that using weights when you're asking many regressions doesn't always regulate replicate the population average or particular quantity because it depends on essentially the variance of the x in different uh in different populations okay and so i haven't solved that problem i've said well that's not what we care about what we care about is replicating what you would get if you had the population data and this we can do okay so that's idea happy to talk about that later if anybody wants to understand more subtly what that's all about but there is a difference between replicating what you'd have if you had population data and getting the population estimate of a structural coefficient if there's heterogeneous heterogeneity in the population okay well here is just sort of the payoff we're able to construct these educational mobility graphs between 74 96 and 74 and 2012. i've stratified here between villages with high education in the first period and low education just to see if sort of village characteristics matter and so you know this looks like probably any other uh most other regressions you've looked at with sort of outcomes at one period on outcomes of the other which is what this is so that shouldn't be a big surprise you know the key substantive question is is sort of how flat this curve is which is a little misleading because you have to look at the axes right um and the basic point is that sort of a one standard deviation difference in educational attainment in the first period is translating 14 14 years later or so or 12 years later into about half a standard deviation so there is sort of you know some mobility would say some compression of the educational district or not compression of the educational distribution some mobility where the relatively well-off people are moving down relative to the poor people except everybody's moving up it's just a matter of the poor people catching up a little bit interesting if you look 74 to 2012 um the sort of original village conditions sort of disappear and you see even more sort of compression now about a once a deviation in 74 is uh connected to basically a quarter of a standard deviation period later so this this is a period in which in bangladesh over this period 74 to 2012 at least in terms of educational attainment there seems to be a lot of mobility okay in the sense that that the the sort of the outcomes in 2012 are being drawn not so much from a stratified distribution but from sort of an overall distribution that's consistent with the idea that this is a place that was very successful in implementing health and family planning programs and education okay so what i did is i asked the right question okay um and it's important right because if you you know so so what i was thinking about is let's think about house of mobility within the matlab area and so the only houses that count are the households in 74 and the households in 2012. okay why is that if you think about from a natural perspective we do that all the time right we're interested in the effect of a program on our country and if people leave or people come in maybe you know whatever we care about the citizens our country or if we're thinking about voting right the people who vote for us are the people within our country and so we care about mobility within our particular geographical region obviously that's kind of a rogue's answer right the problem is trying to think about these issues of constructing the correct sampling weights for the out migrants is a real headache because for the people inside we actually know you know who their antecedent households were to the extent they were within the area but at least we know who's in was out when somebody leaves the antecedent households come from everywhere we have no idea how to think about those sampling probabilities right or perhaps we just say it's the sampling probability when they were in the region we just apply it to that because the other people couldn't have been in the sample at all but there's always a possibility that somebody somebody else from the motlub area moved to daca and then they got married and now there's actually two chances that house will be in the sample so it's just a mess to think about it that way and i'd prefer not to but i agree that if you really want to you know evaluate the effect of a program on people you need to do that if you want to look value the effect of a program on space it's not so much a problem okay now this is matlab so just just to show you about the treatment comparison everything else was stratified by education but here's looks at the treatment area of the comparison area and interestingly if you look at 96 you see the treatment area there was higher education right this is something that uh that we've we've known and it's gone over some time if you look at the same mobility graph by 2012 it's gone okay it turned out that the educational differences that are crude to the areas that had access to health and family planning programs in the in the mid 70s showed no differential benefit in the latest round of surveys this is not the point to get into that but again just to show you that we actually used this stuff and the reason that i got into all these weights is because i wanted to know the answer to that question okay all right so i think i have 15 minutes is that about right okay um well so let me just rush i'll rush through the last two just to give you a sense of what matters i can do it fairly quickly this one here is about segregation now this is i have to apologize this is actually not a developed developing country except you think that you think of the us is now a developing country uh however i'm i am going to use it for a development purpose in just a minute so i need to set the stage a little bit but the point is that when you're messing income segregation it turns out that sample sizes matter and weights matter and i want to make that point okay so there's a sort of a bit of a debate in sociology about whether segregation income segregation in u.s cities increased uh over the 2000s okay and my co-author sort of thought there might be a problem because over that period we went from using the long form to the acs as a basis for measuring income segregation and that meant that the sampling rates went from five percent uh from seventeen percent or so to five percent and the idea is when you're measuring segregation sample sizes matter for bias okay let me help you understand this we're talking about income segregation so matter you have a city and every census tract has exactly the same income distribution okay but you only sample one person per tract okay and now you try to measure income segregation okay you will find out that this is a really terribly segregated city right because you'll draw a different from a different point of income division these tracks so these tracks will look really really different the more people you sample for track the closer you will be to finding out that in fact all these tracks are exactly the same okay and it turns out on the order of five to 17 percent actually makes a big difference and so at least the possibility arises that the perception that income segregation in u.s cities is rising turns out to be an artifact of something that happened at the census bureau okay so i've sort of already gone through that i i won't take you through all the details given given the time there are a couple of different measures it turns out that one of the favorite ones to use is called the entropy index and it turns out that for correcting sampling bias that that there's something really nice about that which i'll show you in just a minute which is this okay so the entropy is just this way of transforming the problem you know the proportion of people who live in tract a that are below or um that are below the sort of 10th percentile of people in the city right you put all that stuff together you get a measure of income segregation okay so it turns out that if you just think about counts and you ignore weights that the bias correction turns out to be just one over n one over the sample size in the tract okay and it turns out that the track sizes in the u.s um in the acs uh you know on the order of 50 or so so the bias ends up being about two points and that turns out to be about the level of the change that you see when you look at income segregation now we did all this before we went to the rdc when we went into the rdc okay here's the adventure part we realized that this correction for sample size wasn't enough why because we'd ignored the fact that the sample sizes were used in a weighted way to construct the grouped estimates okay when we get grouped estimates of the income distribution a given tract okay those are based on a tracked sample of a certain size but also those households are differently weighted okay and one thing we also probably know or you probably know that in more variances weights is a lot like reducing your sample size in the extreme case where one one household gets a thousand and every other weight the other hustle gets away to one it's basically like you have a sample size of one okay so it turns out that we could actually work out a bias correction that works with weighted data which is the sum of w i squared which is going to be the same as 1 over n if the weights are all the same okay so anyway here's another case where thinking about the fact that the underlying data are weighted really mattered okay um we were able to show that our bias correction on this is data from 1940 the advantage of 1940 is we actually have menus we have data on everybody okay here's the estimate if you don't correct here's if you correct only for counts and here's if you correct for weights and if you graph for weights you're unbiased okay and this is you know substantively this is the u.s it says if you looked at the raw white estimate it's marginally increasing at least over the relevant period over 2000 to 2010 but if you look at the white the true estimates it actually is going down it's even worse for blacks there's a big increase and a very small increase when you look at the data why is that well because the number of blocks we're sampling attract is even smaller than the population so these measures of income segregation among racial minorities are even worse than the ones for whites as a whole so anyway this kind of tells the story it basically says you know as you go from using group data to using count correct to using rdc data using count corrected data using weight corrected data what happens to your estimates and the answer is essentially that everything seems to oh the bias always seems to be in the positive direction okay now where is the um where's the the the development payout so the development payout is going to be okay well another question we might think about is development this is matlab over 30 40 years how does the income segregation change so we might have one view of development that even within a region that in the process of development some areas will become more uh have higher incomes or higher consumption other areas not so much they'll be catching up to sort of notions of cousins that creep in here that is this idea of spatial variation income is exactly the same as income segregation in the u.s context so i took our estimates the raw estimates the count corrected and the weight corrected estimates for this entropy and mex of education across villages in matlab over time okay and you get a really different answer if you deal with the weights or and even with the counts with the raw data there's a increase through 96 and then a decrease in the degree of segregation and this is education the extent to which there's sort of variation across villages villages in education but if you do the corrected estimates was pretty flat and the segregation only started to rise when you got to the last round of the survey okay so weighted estimates really matter here in terms of an answer to a question that i don't think has been asked enough in the context of sort of development but which which if we're going to use household survey survey data to look at segregation or to look at spatial distribution of income growth which is in some sense the same thing we're going to have to account for weights we're going to have to effect for sample counts we can't use the sort of general idea that we often rely on that well sample sizes the expected values are right here the expected values are wrong if you don't think about the underlying distributions okay this is programming let me not talk about that okay don't really have time to talk about the last example but just just to tell you it's another example of weights that crept into my life without me wanting to okay uh peter delbo and myself and louis petterman have a paper in a lab looking at democratization the sort of idea was can we show whether it's a democracy effect can we establish a democracy effect accounting for the fact that when you give somebody democracy they choose different policies and so we needed effective democracy net of this election into policies and we devised a procedure in 2010 and then we were thinking about doing some more work and we realized no we can just solve this problem with weights and and so we did here's just the bottom outcome i'm not going to take the time to explain it but the basic idea is the original data the idea is to what extent does sort of democracy give you a different answer to sort of cooperative behavior under these uh under sort of the initial versus the modified payments if you just and in one story would be you just look at sort of how cooperative people are when they vote for a rule versus versus if the rule was imposed exogenously and you get this difference here but the other is to re-weight in this case you re-weight by the population of averages of how people vote and you get something that's different okay so again weights came in and allowed us to solve a problem and it turns out to be a convenient problem this case because in the previous paper we we even people who had random who had random rules imposed on them had to vote and in practice that's kind of a strange thing to have to have happen so we can now use them this method even in cases where people who have a random rule don't have to vote okay there may be place in the world where people uh get to vote and then then the government does whatever the heck it wants but i'm not sure that's the most likely scenario okay so let me just finish up remind you what we we are but then i'll give a few minutes if there any other any questions okay so sorry one of the key insights from this is that the idiosyncratic processes uh that affect the process of household formation and the pure randoms from sampling matters right our ability to solve this problem in the first case and the fourth case which i didn't really talk about has everything to do with this issue okay that the sampling issue is a different problem from the endogenous sort of shocks that we normally think about and that turns out to be quite helpful with regard to a couple of different questions okay collection of construction of cross-sectional waste and panel data is going to be really hard without vital registration data or without questions that allow you to figure out how many links you have linking one set of houses to another okay um follow-up strategies matter we have this idea we're going to go and interview as many people as we can and that's in some sense for some purpose it's very good but there are trade-offs that one has to think about and so i would suggest you know we can at least be deliberate about what we're doing there okay and sample sizes weights matter for spatial hydrogenating if you're going to look at spatial hydrogen 80 you need to realize that sample sizes are a first order concern not the usual second order concern so let me stop there and i think maybe five minutes is there any questions or thoughts or reactions or 