in particular the paper proposes to relax the standard assumption made in monetary analysis using dsg models a uh particularly strong aspect of the standard assumption which is to relax the assumption that the agents in the model all formulate complete infinite horizon state contingent plans that are optimal given the situation they're in before they take um any action even in the short run and i think that's obviously um an unrealistic assumption if taken literally it's it's surely not something that we think is really feasible even if we think that the people in the economy are often highly motivated to make good decisions and may even have a lot of experience with the kind of decisions they're making one reason why we should realize that it's it's unlikely to be feasible in practice is that even in artificial environments where the set of possibilities are are easy to list uh say games like chess or go where there's a finite set of feasible moves in any position that you're in and you can easily enumerate what are the things that you have to consider possibly doing we know that even the best professional players and despite the fact that they're people who spend a lot of time studying the problem a lot of artificial intelligence researchers who've spent many years on the problem the best human or artificial players don't actually solve the game they they can't solve the game by backward induction and know the optimal action they do something else and the paper uh proposes an approach to bounded rationality that's based on looking at what the best artificial intelligence programs do with intertemporal decision problems of this kind pro problems like our programs like ibm's deep blue that beat the world chess champion or more recently uh google's alphago that has beaten one of the uh world's best go players instead of reasoning by backward induction from all the possible final states in the game when the game is over what they do is start with a description of the current position that you're in in the game and look forward from that current position one doesn't look forward all the way to the end of the game but look forward some finite number of steps using a process of tree search deductively calculating the positions that you could get to through some finite sequence of moves then those positions that you could get to after a finite sequence of moves at the point where the tree search is truncated those positions have to be evaluated even though you can't just look at them and see if you've won the game or lost the game at that point so there has to be a value function that the program uses uh to value those possible positions say in terms of probability of winning from the position given the valuation of the nodes where you've truncated the tree search you can then do backward induction from those nodes use that to assign a value to each of your possible initial moves from your current position then select the current move that has the highest estimated value the next time you have an opportunity to move you repeat the process you don't continue to execute the plan that you chose previously but you do the finite number of steps ahead tree search again from whatever the new position is that you're in now an observation about this kind of strategy is it does depend on this value function and the value function that's used in practice can't just be the correct or optimal value function it's of necessity even in these um in these games where the set of possibilities can in fact be completely enumerated it's in practice not going to be possible to learn the correct value of every possible say position that the pieces could be in on the chessboard you have to classify possible positions on the basis of some smaller number of features and try to learn the average values of positions that are described in this more coarse way rather than learning the value of a complete description of the state because you would need a tremendously large database to try to learn empirically the value of every possible fully described position that fact that the value function can't be an exact value function is why forward planning is actually important to the algorithms if you could learn the correct value function you could just always move to the next the position you can move to from where you are that has the highest value and you'll be playing optimally uh forward planning is valuable exactly because the value function that you evaluate positions that you get to uses only a smaller number of features so the goal of this paper is to try to show how we can use this idea in modeling decision makers in macro models model them as sophisticated or rational but in the sense that say the best artificial intelligence programs might be viewed as as rational not assuming that they just know the truly optimal action in the environment that they're in the goal of the paper is to see to what extent we'll get similar conclusions or different conclusions from doing more traditional rational expectations equilibrium analysis i'm going to illustrate the method in the context of a basic new keynesian dsg model and the application that i'll specifically talk about is considering what the predicted effects should be of forward guidance by the central bank statements made in advance about the monetary policy rule that's going to be followed some distance out into the future so the important difference from the standard model is to introduce this idea that people in the model optimize but they do forward planning only some finite number of steps ahead to illustrate that i show on the slide here the description of the planning problem of a household in some period t in the model it's reached some exogenous state in period t at that point in time this decision maker is going to choose a forward spending plan this ci sub tau is planned spending in some period tau in some state s sub tau reached at that point in time but instead of choosing a spending plan off into the indefinite future it only chooses it uh for periods t through t plus k in each of the possible states that could be realized between t and t plus k it chooses that finite horizon plan to maximize an objective which is its calculation of expected discounted utility from consumption in those periods t through t plus k and then there's a terminal term which is the evaluation of the household's continuation problem given where it gets to at the uh by period t plus k it's valuing that continuation problem as some function of real wealth carried into period t plus k plus one and it might also depend on state variables that are anticipated being the case in period t plus k the planning is subject to a series of flow budget constraints for how the wealth of the household evolves as a function of its spending over these periods from t to t plus k i have to say something about the expectations that are going to be used in the forward planning exercise these are expectations about the periods t through t plus k i assume that people are able to use deductive reasoning given their knowledge of how the economy works over that finite horizon that's like in the case of the chess programs using knowledge of the rules of chess to deductively conclude what kind of things would happen or can happen following different moves so here it's assumed that deductions from the structural equations are used for the forward planning consideration of what are feasible evolutions of your wealth under different spending plans for periods t through t plus k in particular what's important about this is i'm going to assume in the forward guidance analysis that in this deductive reasoning the households will take account of any announcements of the central bank about its changes in policy if they imply things about policy over that planning horizon so for the planning horizon where the explicit forward planning is being done announcements about some novel monetary policy will be being taken into account i'm assuming though there's not going to be any thinking about what ought to happen beyond the horizon t plus k and that means in particular that when the household reasons about what economic conditions should be in different states in some period t plus j which is further in the future but not beyond the horizon t plus k the household is going to assume that prices and its income are being determined by optimizing decisions of other people in the economy but it's going to model those other people as planning only k minus j periods ahead and that's necessary so that it can think about what they should do without having to think about what anyone should expect to happen beyond the horizon t plus k so the household is modeling its own behavior in the future period t plus j as if it will be optimizing then but with a horizon that's only k minus j periods long it's similarly going to model everyone else in the economy as optimizing than but only having horizons of length k minus j another crucial assumption is what i assume about the nature of the value function as i pointed out realistically the value function can't be the correct value function it has to depend only on some course description of the state of the economy i am allowing it to depend on your level of wealth to assume that households are sophisticated enough to realize that it's valuable to have more wealth rather than less wealth the important thing about the coarseness of the description of the state for the forward guidance analysis is that i'm going to assume that if some novel monetary policy is announced people will not take that into account in the value function they will not have learned the value of being in a state where that unusual monetary policy is expected uh to be in force in the specific results that i'll be talking about here i further simplify by actually assuming that this value function v of b doesn't depend on any other state variables at all although that's not the critical thing the important thing for what i'll show you is this assumption that it isn't affected by monetary policy announcements where does it come from the value function is learned from past experience and here's how i model the learning there's this finite horizon planning exercise that the household engages in in each period in doing that exercise using some current estimate of the value function it can compute the value of that finite horizon objective it computes the value of its objective for the level of initial wealth that it comes into the period with but it can also counter factually compute what it would do and what the value of its objective would be if it had come into the period with any other level of real financial wealth b so it can compute an estimate of how its objective would be different for any level of b i call that the v estimated estimated value function as a function of financial wealth it uses that then to update the value function that it's going to use in future forward planning exercises with this with this simple error correction rule so the the new estimated value function is compared to its existing value function and if they're different it adjusts its value function for each level of financial wealth in proportion to that discrepancy the gain parameter gamma determines how fast the updating occurs a consequence of this is that in a constant environment the value function v of b will converge to the optimal one it'll converge to the solution to the bellman equation because this constant gain learning algorithm is in fact an iterative algorithm for solving the bellman equation through value function iteration so it only gets you to sub-optimal behavior to the extent that you're in a more complicated environment and the fact that the value function isn't conditioned on additional state variables could then lead to not learning uh the correct value function to describe dynamics under different macro policies i log linearize then the household's decision rule i log linearize it around a perfect foresight steady state with constant values of the real disturbances a constant inflation rate determined by the central bank's normal inflation target and i assume that in this steady state the correct value function for that steady state has been learned and so then i get log linear decision rules for the household that can or for aggregate demand aggregating the decisions of the different households that can be described by this kind of equation that's at the bottom of the slide i use the notation here y with a superscript j is aggregate real spending and aggregate real income uh in some period if all of the households in the economy have a planning horizon j and i can calculate that yj for each possible horizon starting with horizon planning horizon of zero similarly there's a pi j that's the inflation rate in the economy of all the firms that are setting prices optimized with planning horizons of length j i j is the nominal interest rate if the central bank is reacting to inflation and output determined by people with planning horizons of length j then i get an equation for each j relating aggregate demand if the planning horizon is j to the true model consistent expectation that e sub t is a model consistent expectation of aggregate demand and period t plus one if people had planning horizons not of j but of j minus one and similarly inflation expectations enter over there but their expectations of what inflation would be in the next period if firms had planning horizons of length j minus one there's a order zero version of this equation that involves no expectations at all at the bottom uh of the slide these equations can be solved recursively the solution for aggregate demand if people have zero length horizons comes off the equation at the bottom of the slide once you know that you can find the aggregate demand if people have horizons of length one using the equation above given that you can solve for aggregate demand if people have horizons of length 2 and so on this equation is similar to what's sometimes called the new keynesian is curve in the model with rational expectations it has exactly the same form except for those superscripts the js and the j minus ones and the thing that makes the results with the finite horizon different is that the js and the superscripts are not the same on all of the terms except for that it looks like this standard equation i do a similar analysis to the decisions of price setting firms when a firm re-optimizes its price it has a finite planning horizon it has to use a value function v tilde for its continuation problem the value function is learned by error correction log linearize the decision rules of the firms i get an equation for inflation that looks a lot like the standard new keynesian phillips curve but again there are those superscripts the js and the j minus ones that are different on different terms that's what makes the predictions not equivalent to the model with with rational expectations close the model with a reaction function for interest rates i want to say something about the results with forward guidance than about future monetary policies so suppose that we have a policy experiment we start out in this steady state with the normal inflation rate pi bar value functions have been learned that are optimal for that environment and suppose the central bank announces that monetary policy will be determined by some other reaction function up till some finite date capital t but after date capital t policy expected to revert to the normal reaction function which is a taylor rule consistent with that normal inflation target pi bar what i show in the paper is that it's possible to uniquely solve for the model's predictions for dynamics under an experiment of this kind you can solve uniquely for the horizon j equals zero solutions then you solve uniquely for j equals one from them and so on recursively uh regardless of the level of the finite planning horizons we get unique solutions for all of the variables if the planning horizons of all of the households and firms are long enough specifically long enough relative to the length that the new policy is going to last the model's predictions are the same as irrational expectations equilibrium but it's a specific rational expectations equilibrium the rational expectations equilibrium in which the economy is expected to return to that previous state after d capital t you can further show that if the new temporary monetary policy reaction function conforms to the taylor principle then that particular selection of a rational expectations equilibrium converges as you make the length of the new policy unboundedly long and that means that even with a permanent change in policy the solution to this model with finite horizon planning will be similar to the rational expectations solution and it will approx approach the rational expectation solution if enough people in the economy have sufficiently long planning horizons so the finite horizon analysis provides a justification for standard rational expectations analysis as a simplifying approximation even if we don't think that people literally formulate infinite horizon plans it also solves the equilibrium selection problem for the rational expectations analysis that's been a source of puzzlement to many people who probably teach those methods and have have their students struggle with the question of how you know which of the possible rational expectations solutions is the right one this would justify a particular one but that simple and maybe reassuring conclusion is not what you get if you assume the taylor principle is not satisfied by this temporary new policy and that's particularly relevant to the case of a temporary interest rate peg where it would not be satisfied if we had a temporary commitment to keep the nominal interest rate say fixed it's lower bound and we ask what the rational expectation solution is as as many of you know you would get a prediction that effects on output and inflation should grow explosively as the length of that commitment to the interest rate peg is made longer that implies forward guidance should not just be effective but extremely effective even explosively effective if it's credible a lot of people would find that implausible given what we've seen happen it's sometimes called the forward guidance puzzle in this model if we assume some finite horizon k for people in the economy the commitment to this low rate interest rate peg is predicted to stimulate output and inflation but the effects of that are bounded no matter how long the commitment is is for so there is an upper bound to how effective forward guidance can be even if you commit to maintain the peg forever that upper bound could be quite modest if the length of people's planning horizons are not very long a further and i think particularly interesting result is what the model says about a permanent interest rate peg in the rational expectations analysis if you have a permanent peg there are non-explosive rational expectation solutions they all converge in the long run to an inflation rate consistent with the fischer equation which means it's a lower inflation rate if you commit to a lower nominal interest rate and so that might seem to imply that committing to maintain a low nominal interest rate should give you lower inflation rather than higher in the finite horizon analysis that doesn't happen commitment to the lower rate even permanently necessarily increases inflation and that means that none of the rational expectation solutions are similar to what this model implies the realistic answer should be if people have finite horizon planning eve and it's regardless of what the distribution of planning horizons are so i think the rational expectations analysis uh can in some circumstances uh be quite misleading and i hope that people will think more about deviations from personal uh perfect rationality of this kind in uh in doing monetary policy analysis thanks 