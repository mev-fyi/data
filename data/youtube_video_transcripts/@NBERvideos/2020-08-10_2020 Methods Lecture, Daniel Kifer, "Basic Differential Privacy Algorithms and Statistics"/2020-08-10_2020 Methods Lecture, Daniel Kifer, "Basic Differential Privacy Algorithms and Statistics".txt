Daniel Kifer: This talk,
we're going to discuss some basic techniques for creating algorithms that satisfy this differential privacy with applications to statistics. Let's start with the basics. What we've learned so far. The privacy aspects of
differential privacy, we've a privacy loss budget and a bunch of
restrictions on what a randomized algorithm can do. Specifically for
every possible pair of databases that are
neighbors of each other, and for any possible outputs, e, the following
equation has to hold. For an algorithm satisfies all those restrictions
we call it a mechanism for
differential privacy or mechanism for short. As we saw earlier, mechanisms can protect the confidentiality
of our responses. The burning question is, how do we create
such mechanisms? You notice that the definition
is non-constructive. It just says that for all pairs of databases and all outputs, we need to check that a corresponding
equation is satisfied, and for all practical purposes, this represents an
infinite set of equations that must be checked. But who has that time? Unfortunately,
differential privacy has some unique properties out of all disclosure
avoidance techniques. Differential privacy is modular, which means that
complex mechanisms can be built from simpler ones in a similar way to how
intricate Lego castles can be built from a few
basic building blocks. All you need is a bit of
artistry and creativity. Almost everything that you need consists of
three basic tools, consisting of the
Laplace mechanism, post-processing,
and composition. There are a few more advanced techniques that we won't get to, but they reference towards
the end of those slides. First, let's take a look at the Laplace mechanism and a related concept
called sensitivity. Recall that differential privacy relies on the concept
of neighbors. Neighbors are pairs
databases for which the differential privacy
equations need to be checked. You can define neighbors in different ways depending on what information you're
trying to protect. The two most commonly
used ones are bounded neighbors and
unbounded neighbors. Bounded neighbors
differ on the value of one person's information, as shown in this
picture right here. We use this definition
when only wanted to protect the contents
of someone's response. Unbounded neighbors
are preferable when we want to protect, not just the contents, but also whether they responded or not, we want to protect
their participation. Intuitively, the goal of differential privacy is to hide differences
between neighbors. Another interpretation is
that differential privacy tries to hide the effect of
any one person's action. For example, it can hide the
effect of participation, hide the effect of
non-participation, hide the effect of lying
on a survey response or hide the effect of being
truthful on the response. The concept of sensitivity measures the effect that
one person can have. To make this concrete, let's consider a simple problem. We want to know
the average age of voters and the average age of
non-voters in our dataset. Let f be this function that takes a dataset and returns a vector with two components. The first one is the
average age of voters, and the second is the
average age of non-voters. We want to create a
privacy-preserving version of f that gives approximate but
accurate average ages in such a way that privacy is protected even in the face of sophisticated attacks that
leverage external datasets. The goal is to inject just
enough noise into each of these two answers so that the effect of any
one person is hidden. In other words, for any pair
of neighbors D_1 and D_2, the noise should mask the difference between D_1 and D_2. It's important to note
that I said inject noise, not necessarily to add noise. Differential privacy is often
used with additive noise, but we're not limited
to additive noise. In the simple case, let's say that we do want to add noise and how would
we go about it? A common noise
distribution we can use is the Laplace distribution. Ian mentioned the
geometric mechanism, which is a two-sided
discrete distribution. We can do that if all of our
statistics are integers. But in a more general case, we could use the Laplace
distribution instead. Sensitivity basically tells us the scale that we need to use with the Laplace
distribution. Given a function f, its L_1 sensitivity is going
to be denoted by Delta f. It's the largest
possible impact of one person on
f. Mathematically, we look at all
pairs of databases, D_1 and D_2 that are
neighbors of each other. The difference between
f of D_1 and f of D_2 is the effect
of one person. The L_1 norm of this
difference measures its size. We compute this difference
for all pairs of neighbors and select the largest one and that's the
L_1 sensitivity. We'll later see that
the sensitivity divided by Epsilon is the
scale of the noise that we would need to add to
the value of f of D. If we want to create
mechanisms with additive noise, it's best to do
this for functions that have low sensitivity. Let's try some examples
of computing sensitivity. Suppose we have a dataset where the maximum allowable
age is 115, that is if someone
marks their age as 120, it gets automatically change
to 115 by the database. Suppose we're interested
in the sum of ages of the people
in the database, what is the sensitivity
of this function f? We know that we can
create neighbors by adding or removing a
record to some database. We know that adding or
removing one person to any database can
change the sum of ages by at most
plus or minus 115. I'm taking the absolute value, get 115, which is
the sensitivity. Now if you didn't have
this cap on age over here, or if you didn't know what
value to set the cap at, or if you didn't
want to add noise with the scale of
115 over Epsilon, there are other more complex techniques that are available. The main point for those
techniques is that do not look directly into your data to determine a good
value for your cap. For example, don't look
at the maximum age in the dataset and say that's the cap because if you do that, you'd be leaking the age
of the oldest person. Let's look at another example. Now we're interested in
the number of people who are 18 years or older. Adding or removing a record
from any database will just change this count
by plus or minus 1. The sensitivity for this
function is just one. Now, here is a case where f returns a two-dimensional vector corresponding to the
number of people living in group quarters and the number
of Asians in the database. Again, to compute sensitivity, we can look at the
largest possible change in f we can achieve by adding or removing one
person from any database. In this case, the largest
change is achieved by adding or removing one Asian
individual in group quarters. This will cause the number
of people in group quarters to change by plus or
minus one and the number of Asians to change by
plus or minus one for a total change of two and
so the sensitivity is two. The more complex example
is the following; for each age from one to 100, we want to know how many
people there are of that age. We can see that if we add or remove a record
from any database, the count in one of
these age groups will change by plus or minus one and the rest are going
to stay the same. The total change will be one and so the sensitivity
is therefore one. Finally, let's consider
our motivating example, where we wanted to know
the average age of voters and average
age of non-voters. Again, for simplicity, we'll just assume
that all the ages are apriori kept at 115. Also for simplicity, just
to avoid division by zero, we're going to define the average age in an
empty dataset to be 0. Let's figure out
the sensitivity. What is the largest impact
that one person can have on the results of f? Let's consider these two
databases, D_1 and D_2. D1 is completely
empty and D_2 has only one person in it
and they happen to be 115 years old. If we evaluate our
function f on D_1, we get zero as the
average age of voters and zero as the
average age of non-voters. If we evaluate on D_2, we get zero as the
average age of voters, 115 as the average
age of non-voters. These two datasets
actually happened to be the neighbors that exhibit
the largest difference in f, and so the sensitivity is 115. We went through a series of examples computing sensitivity, let's put it to use. The Laplace mechanism is very simple, differentially
private mechanism. What it says is if you have a function f, you can compute its sensitivity, and then you compute the
value of f of D, like this, and then you add
Laplace noise with scale sensitivity
over epsilon to each component of f of D. If f is the function that tells
us how many one-year-olds, two-year-olds,
three-year-olds, and so on, its sensitivity is one and the Laplace mechanism
says that if we add Laplace noise with scale one over Epsilon to each
of these counts, then the results satisfies
differential privacy. It satisfies Epsilon
differential privacy with that specific
privacy parameter. The Laplace mechanism
is a simple technique, but it's not always the
best in terms of accuracy. It's a useful building block, but it's often not
sufficient by itself. Let's take a look at
the next main idea is post-processing. We just learned that this function over here, this mechanism satisfies
differential privacy. It gives us noisy counts
in different age groups, but often this is
not our final goal. Our goal is not just to know the approximate numbers per se, but we want to understand
something about them. For example, we
might be interested in how the age distribution of this database differs from the age distribution in
the entire population. This means that we
haven't wanted to run an analysis on the output
of M. For example, we might want to run
a Chi-square goodness of fit test to compare these approximate age counts to the age distribution
in the 2010 census. Now we have two pieces of code, so the mechanism M, it gives us the noisy counts, and let's say g is
a piece of code that implements the
chi-squared test. We can create a new piece
of code that combines them, so let's call this
new piece of code g circle M. What g circle M does is it runs M
on the database, runs g on the result, and returns the output
of g. The question is, does this new piece of code satisfy the differential
privacy equations? It turns out that yes it does, it satisfies Epsilon
differential privacy with the same exact Epsilon parameter that M does, the
original mechanism. There's nothing specific to
chi-square testing here. Suppose h is a piece of code that links to external data. We can create a new
piece of code called H circle M and what it does is it runs M on the
differentially private mechanism, M on the data and produces an
output, some noisy output, and then it runs h to try to link this noisy output
to external data. Again, by the
post-processing theorem, h circle M satisfies the Epsilon differential
privacy with the same Epsilon, and the fact that
we tried to run a linking attack on
the output of M does not degrade the
privacy protections for the people in our database. In general, if Phi
is any function that does not look directly
at our collected data, then Phi circle M satisfies differential privacy
with the same privacy parameter as M. This Phi could look at
external data sets and those data sets could have information about the
people in our database. But the idea is that
since you don't look directly into our dataset, you won't learn anything
new about those people. What these functions g, h, and Phi all have in
common is we run them on the output of M and we
call this post-processing. What we're seeing is that
post-processing does not degrade the privacy guarantees. Another way of
saying this is that differential privacy
is closed under post-processing and very
few other disclosure of avoidance techniques
actually have this property. Finally, let's talk
about composition the third tool that's going to allow us to create more
complex mechanisms for differential privacy. If you are tuned in
for the earlier talk, we had an example where we use differential privacy to collect private information
from senators. Given yes or no question, a trust the data collector, might be a person or machine, but it collects the responses, tabulates the votes, and
releases the number of yes answers plus Laplace noise with a scale 1 over Epsilon_1. This satisfies Epsilon_1
differential privacy. Now let's suppose
this works so well that we came back
the following week, but this time we wanted more detailed information broken down by party affiliation. The trust, the data collector, counts the number of yes
votes from Democrats, counts the number of yes
votes from Republicans, adds independent Laplace
noise with scale 1 over Epsilon_2 to each of
these two counts, and releases the total. So the mechanism in
week 2 satisfies Epsilon_2 differential privacy. The week 1 mechanism satisfied Epsilon_1
differential privacy. It leaks a little bit
of information which is controlled by this
parameter Epsilon_1. In week 2, we used Epsilon_2 differentially
private mechanism. It leaks a little bit
of information which is controlled by the
parameter Epsilon_2. But the important question is, what is the combined effect of these information leakages? When you combine information from multiple releases of data, the combined effect of the information leakages is called composition and
differential privacy allows us to quantify it. In week 1, we used Epsilon_1 differentially
private mechanism. If we had only released
the output of week 1, we could say the privacy
loss was Epsilon_1. In week 2, we use an Epsilon_2
differentially private mechanism. If we only released week
2 and nothing else, we could say that the
privacy loss was Epsilon_2. But we actually released
the output from both of these mechanisms
from both weeks, so the combined release of information from
both of these weeks satisfies Epsilon_1 plus Epsilon_2 in
differential privacy. In other words, the privacy
parameters just add up, so this additive property is why the Epsilon parameter is called
the privacy loss budget. We can think of it as spending Epsilon_1 of our privacy
budget in week 1, and then additional Epsilon_2
of our privacy budget in week 2 for a total cost of
Epsilon_1 plus Epsilon_2. The general case, suppose we have a bunch of mechanisms,
M_1, M_2 through M_k. M_1 satisfies Epsilon_1
differential privacy, M_2 satisfies Epsilon_2
differential privacy, and so on. From these, we can create
a new piece of code M. What M does is it goes through the
mechanisms sequentially. It applies M_1 to the
data releases the result, applies M_2 to the data
releases the result, and so on. This mechanism that runs
all of them satisfies the Epsilon differential privacy where the Epsilon parameter
is the sum of all of these. In other words, when M
releases the output of M_1, it pays a privacy
cost of Epsilon_1, when it releases
the output of M_2, it pays a privacy cost
of M_2, and so on. We've learned about
the Laplace mechanism, post-processing, and composition, and now
let's apply what we know. Going back to our
motivating example, we want to release a
privacy-preserving version of the average age of voters and the average age of
non-voters and our database, the ages are kept at 115 and the average age of an empty
data set is set to 0. The first thing we can try
is the Laplace mechanism. Earlier we computed
the sensitivity of this specific
function to be 115, so if we use the Laplace
mechanism directly, we compute the exact
average ages of voters and non-voters, add Laplace noise
to scale 115 over Epsilon and then
release the result. Then disaster strikes because the average age is
a number 0-115. But the noise that we added
had standard deviation of around 163 over Epsilon. For most reasonable
settings of Epsilon, the noisy average is
completely useless. This brings us to the
first main point. The moral of the story
is that you can easily waste your privacy budget
if you're not careful. Let's try a different approach. Here's a high-level strategy
for attempt number 2, we're going to split our
privacy budget half. We're going to use
the first half to get the sum of the ages, not the average, but the sum of the ages of voters
and non-voters. These will be the numerators. Then we're going to reuse the remaining privacy budget
just to get the counts, number of voters,
number of non-voters. This will give us
the denominator of the average and then the third step is we divide to get our privacy-preserving
averages. In more detail, we set
Epsilon_1 to be half of our privacy budget and we use it for this first function,
F_1 over here. We know that adding, or removing one person can change
the total change, and here is going to be a 115. Either the change to
the sum of ages of voters is going to
be at most 115 or the change to the
sum of the ages of non-voters is going
to be at most 115. The sensitivity is 115. We can use the Laplace
mechanism for function F_1 by adding independent Laplace noise with the scale to these sums. The remainder of
the privacy budget, we can use it for this function F_2 that computes the counts. We know that these two
are disjoint populations. Adding or removing
one person will have a total change of one, so the sensitivity is one and so we're going to use the Laplace mechanism to
create these noisy counts. We're going to take
the true exact values and add Laplace noise of
scale 1 over Epsilon_2. Finally, we divide, so
we take the noisy sum of ages of voters
that we got from the Laplace mechanism in step 1 and divide by the noisy number of voters for the Laplace
mechanism in step 2, we do the same thing
for non-voters, and this gives us the
noisy average ages. It turns out that the
standard deviation here is approximately 325 divided
by the number of voters. That's better than
our first attempt because here at least the standard deviation decreases with the population size. This result is not
particularly stunning, but certainly better
than the first attempt. In any case, we can
always improve on this by using more informative
statistics about the age distribution. I'll mention some of the more advanced techniques
a little bit later. But first let's
take a look at what we did using pictures. We had one mechanism that's satisfied Epsilon 1
differential privacy, we had another one
that satisfied Epsilon 2 differential privacy. There come together,
they satisfy Epsilon 1 plus Epsilon
2 differential privacy by composition, we post-process the
result by dividing. This doesn't affect the
privacy parameters. Then we release the
results to the public. The total privacy cost of this entire mechanism is
Epsilon 1 plus Epsilon 2. But here's something better.
The noisy sum of ages and the noisy counts that we
produced as intermediate steps. Those are called
noisy measurements. We already paid the
privacy cost for them, so they are safe to release. That means that they
should be released. Instead of just
releasing the average at a privacy cost of
Epsilon 1 plus Epsilon 2, we can release the
noisy measurements, noisy sum and noise counts as well without increasing
the privacy cost. The lesson here is, when dealing with
differential privacy, don't just settle for the
post-processed result. You should actually
demand that you get access to the noisy
measurements as well. What we've learned so far, we can build differentially
private mechanisms using the plus noise
composition in post-processing. But you should plan how
you do that carefully. Just like with a real budget, it's very easy to waste it if
you don't spend it wisely. When designing differentially
private mechanisms, think carefully about
where to inject noise and how to
inject the noise. In our example of
computing the averages, it may be better
to, for example, instead of doing what we did previously with attempts
1 and attempt 2, you might be more
interested in getting differentially private
quantiles instead. Or you might want
to get, let's say, a fancy or histogram approach that will
simultaneously give you accurate approximate counts
for all possible age ranges. If you're interested
in those techniques. Here are the papers
that are referenced. Now let's take a look
at another example, a simple way of performing linear regression using
differential privacy. The tools we're going
to use are again, Laplace mechanism,
post-processing and composition. To fix the notation
and the assumptions, we have a dataset of records. Each record contains a feature
vector which is the same as all of your
predictor variables and a target variable y. For reasons that
will be clear later, we're going to require
the L1 norm of each feature vector
to be bounded by C1. Similarly, the absolute value of each target to be bounded by C2. These requirements are
going to allow us to bound the sensitivity of
intermediate results, and that will allow us to
use the Laplace mechanism. The classical linear
regression model is that the target is equal to a linear combination of the predictor variables
plus an error term. For simplicity, I'm omitting the usual intercept
term that is there. Here in matrix notation, y is a column vector
consisting of the target for each record and x is
the design matrix. Every row corresponds to a
record and the goal is to learn the coefficients
Beta in this model. In the classical case, the classical solution
when you have access to the original data, this would be the
estimate for Beta. Now let's make this
differentially private. Our strategy is going
to be first to take the privacy budget
and split it in half. I'm going to call the two pieces Epsilon 1 and Epsilon 2. Using the first piece, we're going to try to
get a noisy estimate of this x transpose
x inverse term. Then using the other piece
of the privacy budget, we're going to try to
estimate x transpose y. Once we get these two
noisy quantities, we're just going to multiply
them together to get an estimate of our noisy
model coefficients. Let's examine the details. First, we're going to create a differentially
private version of x transpose x inverse
in the following way, we're actually going to create a differentiated
product version of x transpose to x
because it's easier. The sensitivity of this
turns out to be C1 squared, where C1 was our
priority bound on the L1 norm of the feature vectors or
the predictor variables. The Laplace mechanism
says we can achieve Epsilon 1
differential privacy by adding this
amount of noise to each element of this matrix. Then finally, once we
have this noisy version, we compute the inverse to
get a noisy version of this. Computing the inverse we use
the post-processing result. Now let's create a differentially
private version of this vector x transpose y using our remaining
privacy budget, it turns out its
sensitivity is C1 times C2, where C1 was our bound
on the feature vector, C2 was our bound on the target. According to the
Laplace mechanism, we can add this much
noise to each element of this vector to satisfy Epsilon
2 differential privacy. The total privacy costs
of these two steps, is Epsilon 1 plus Epsilon
2 equals Epsilon. Now we use
post-processing again, we multiply our noisy version of this term and
the noisy version of this term to get noisy
model coefficients. Then finally, instead of just releasing the noisy
model coefficients, we can release our intermediate
noisy measurements and that's a simple way
to do linear regression. Up to now, we discussed
some simple examples of how to construct mechanisms with differential privacy. We saw that careful planning was necessary to avoid wasting
our privacy budget. Let's examine this
idea in more depth. First it's worth noting that differential privacy is a
very flexible framework. Using it, we can do all of the following and much more in
a privacy preserving way. First, we can estimate
the number of people in different
demographic subgroups. We can also build a
generalized linear models and even get confidence
intervals for them. We can train deep
learning models and even create synthetic data. All this is possible
by carefully planning where to add the
noise and how much. As an illustration, let's
take the simplest example. In a given region,
suppose we want to know how many Hispanic
individuals there are. Let's call it X. Let's suppose we
also want to know how many voting age
individuals there are and let's call it Y. We want to get
privacy preserving estimates of these two. The question is, what
do we add noise to? In our first attempt, let's
just add noise to X and Y. It seems fairly straightforward. Let's take a look
at the sensitivity. For any database,
adding or removing one person can change X, number of Hispanic individuals
by plus or minus 1, and it can change
Y the number of voting age individuals
by plus or minus 1, the total change is two, and so the sensitivity
Delta is two. We can satisfy Epsilon
differential privacy by taking our exact counts, adding independent
Laplace noise of scale 2 over Epsilon to
each one of them. These measurements
are unbiased and the noise variance is 8
over Epsilon squared. Thus, these X Tilde
and Y tilde are fairly accurate approximation
to the exact number of Hispanic individuals and
number of voting individuals. But as with all
things related to differential privacy and in general research and statistics, the question is,
can we do better? Let's try something else. This might seem silly
in the beginning, but let's define
these new variables. Here S is the sum of Hispanic population plus
the voting age population. Notice that these are not
necessarily disjoint and D, D is for difference, D is the difference between
these two quantities. The sum and
difference, these are not very intuitive quantities. They don't really have
much physical meaning, but let's see what happens
if we add noise to them. The sensitivity computation is a little bit more involved. But let's see what
happens when we add or remove individuals
from any database. If we add someone who's neither
Hispanic nor voting age, the sum and the difference
remain unchanged. If we add someone who is
Hispanic but not voting age then the sum S changes
by plus or minus 1. The difference also changes by plus or minus 1 for
a total change of 2. If we add someone
who is not Hispanic, but is voting age, then S changes by
plus or minus 1 and D also changes
by plus or minus 1, for again, total change of 2. If we add someone or
remove someone who is both Hispanic and voting age, here we see that S
changes by plus or minus 2 because both
terms in the sum change, but D is unchanged
because adding this person cancels out when
we take the difference. Again, we see that
the change is two, to follow these situations, the maximum change is two, and so the sensitivity is two. What this means is we can add Laplace noise with
scale 2 over Epsilon to S and similarly to D and we get these noisy measurements. But this is not what
we wanted initially. We wanted estimates for X and Y, so we can reconstruct them. Using noisy S and noisy D, we can get a noisy
version of the number of Hispanic individuals
and noisy version of number of voting
age individuals, simply by either averaging S and D or taking their difference
and dividing by 2. Now the question is, did we gain anything by taking this
more roundabout route? The answer is yes. If we do some simple
calculations, we notice that now our
variance for X tilde is 4 over Epsilon squared and our
variance for Y tilde is also 4 over Epsilon squared. Let's just summarize and
compare our two approaches. In attempt 1, we added noise directly to X and
Y, in attempts 2, we added noise to linear
combinations of X and Y, even though they didn't have any physical interpretation. If we added noise directly to the
quantities of interest, we would have variants
eight over Epsilon squared. But if we added noise
a little bit more cleverly in Attempt 2, we get noisy versions of these quantities but
with half the variance. This process, the first figuring out what we want
to add noise to, then adding noise, and then reconstructing
the quantities of interest is what
we call the select measure reconstruction
paradigm and it's very useful and
it's actually one of the techniques being
used in the 2020 census. This also illustrates the
point that what you want is not always what you
should be adding noise to. So far the common theme
has been to think carefully about the
noise and let's examine this in the basic
statistical setting of Chi-squared
hypothesis testing. Statistics work better with Gaussian noise and
so this is perhaps an appropriate time to mention that there are variants of differential privacy that are compatible with Gaussian noise. They're a bit more
difficult to explain, I just left some references
here that you can explore, maybe after the talk. But the idea is
when adding noise, the scale of the Gaussian should depend on the
L_2 sensitivity. Previously we were
dealing with the L_1 sensitivity where we look at the difference of
the function and two neighbors and compute
the L_1 norm. If we change it to the L_2 norm, that'll tell us the scale of the Gaussian noise
that we need to use. For the rest of
the talk actually, all we need to
remember is that we can add Gaussian noise to protect privacy with slightly
different semantics. To see how we can create
differentially private version of a Chi-squared test, let's quickly review the
classical Chi-squared test. Chi-squared testing is
a general framework that allows you to
perform hypothesis tests. Testing for various hypotheses such as whether data came from a pre-specified distribution and this would be the
goodness of fit test. Or whether two samples came
from the same distribution, which is often
called the test of two-sample proportions. You can test whether two categorical variables
are independent, which is called the
independence test and there are many more
additional applications. But the general idea
behind all of them is you compute this test statistic
which has a quadratic form. Here, the X_i are the number of people of
type i in your database, and E_i is the expected number under the null hypothesis. The values that we plug in for E_i depend on the type of
hypothesis we're testing. Statistical theory tells us that under the null hypothesis, the distribution of this test statistic t is asymptotically Chi-squared with Tau
degrees of freedom and Tau depends on the type of
hypothesis being tested. In the classical setting,
we would compute the test statistic
from the data, see where it falls in the tails of the
Chi-squared distribution. If the area under this
tail is small enough, we reject the null hypothesis, and if not, we fail to reject. Now let's add
differential privacy. We had a data set. Our X_i is the number of people of type i and
suppose someone adds Gaussian noise to each of these counts in order to protect privacy and then gives
us the noisy values. Instead of the exact number
of people of Type 1, we get a noisy version
of it called tilde x 1, instead of the exact number
of people of Type 2, we get the noisy version
tilde x 2, and so on. How do we perform a hypothesis test on the
noisy version of the data? Let's start with the
most naive attempt. Let's pretend that these noisy
values are the real data. We have our test statistic. We plug in the noisy
data in place of the real data and then we run it through our
favorite software package. We would just tell it
its the real data. It will give us a p-value and if this p-value is below
some cutoff Alpha, let's say 0.01, we would
reject the null hypothesis. This approach, as you may
have guessed, is very naive. It does not account for
the extra privacy noise. Basically, it ignores it
and hopes that this noise is ignorable and this
approach is doomed to fail. We can run simulations
of how these naive, let's call them "p-values,"
how they behave, and we can compare it
to the ideal behavior. Ideally, p-values should be uniformly distributed
under the null hypothesis. But if we do a Q-Q plot of the uniform distribution against simulated sampling distributions
of this naive approach, we're going to see
that the naive, "p-values" are actually
smaller than they should be. Just because we added
noise to our data, it didn't change any of the underlying phenomenon
in our data. If we make reject or
fail to reject decisions based on these naive p-values, which are biased downwards, we are going to have
many false discoveries. Or in other words,
our Type 1 error will be larger
than it should be. Let's try something less naive. We can still compute our Chi-squared
statistic by plugging in noisy data in the
place of the real data. But now we know that
our test statistic, when computed from
these noisy data, is not going to have an asymptotic
Chi-squared distribution under the null hypothesis and that's okay because we
can actually estimate the sampling distribution
under the null hypothesis. We can estimate it fairly
accurately and then we can use this sampling
distribution rather than the Chi-squared distribution
to compute the p-values. As you probably expected, these p-values will behave exactly like the
p-values should. We can compare the
uniform distribution versus the p-values in simulations where we accurately estimate the sampling
distribution or we're going to
see a good fit. This is a valid approach. In other words, if we reject the null hypothesis
when these p-values, these better ones, are
below the threshold Alpha, then our Type 1 error
will be Alpha and so we have a valid Chi-squared test under differential privacy. Whenever we do this, the next question we have
to ask is, are we done? Well, from an
aesthetic perspective, there's still something a
little bit unsatisfying. If we compute the standard
Chi-squared test statistic based on the noisy data, the sampling distribution under the null hypothesis is no longer approximately
Chi-squared. One question we could
ask is maybe there's a different test
statistic and we plug in the noisy data into that different
test statistic and its sampling distribution maybe is Chi-squared,
is that possible? The answer is yes. This result is presented in this paper and we call it
the projected statistics. The formula is a bit too large to fit in the
remaining space here, you can look in the
paper for more details. But the nice thing
about it is that this projected test statistic appears to have much more power than in the previous attempts. Here's a chart from that paper. There have been other
test statistics or other purchase prior
to it and this plot is how much power do
they lose compared to the projected statistic and in some cases, it's
quite significant. It seems that we've managed to improve our tests through
a series of extensions. Now the question is,
are we done or is it possible to improve
the test even more? The answer, here again, is yes, because if you noticed,
we didn't play around with the
noise distribution, but we could go back and re-examine how we
should be adding noise to improve
the test as well. Overall, let's just recap
the takeaway messages. We covered a lot of material illustrating
different aspects of mechanism design under
differential privacy and we have some
main highlights. First, differential privacy
is like building with Legos. First, it's fun, and second, you can make complex mechanisms
from simpler ones using concepts like post-processing
and composition. Differential privacy is
also like spending money, which a lot of people realize
a little bit too late. It's very easy to waste your privacy budget
by being naive. Differential privacy requires a certain amount of
financial planning. But fortunately,
different strategies have emerged in the literature. The important questions to
ask when planning it is, where do you add the noise just like we had when we were
looking for demographic totals? What do you do after the noise, like in the Chi-squared example? Then there's finally
a third component which is a bit more advanced, but you can look in these
papers for more details. But this is actually a
more accurate tracking of the privacy cost than the composition
equations I showed you. It's actually possible
to improve over those. Then finally, takeaway
messages for an end-user of differentially
private data products is that you should be aware you can ask for many additional pieces
of information, not just the output. You can get access to the intermediate
noisy measurements. In most cases, they're safe to release and they're very
useful to work with. Sometimes the algorithms
create micro-data as an intermediate step
and that would also be safe to release
and then finally, the source code is always
safe to release as long as it used a good
source of randomness. One of the nice things
about asking for the noisy measurements
is often that they're just counts plus noise, meaning that they are unbiased. They have known variants, they have known distribution
and that allows you to adjust your inference. If the distribution
is complicated, you can just take the source
code and run simulations through the source code
as another way to examine the behavior of
differential privacy and how different hypotheses of your data are affected
by the mechanism. 