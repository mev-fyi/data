Jesús Fernández-Villaverde:
Before we talk about the motivation for why we are interested in non-linear and
non-Gaussian models, now let me get into a little bit more concrete details of how we solve these models
by perturbation. Yesterday, Larry
Christiana introduced some basic notation and some basic ideas and
I'm not going to go over all of them yet again. Basically, just to
remind you of what we are looking for
is we are going to have some type of
functional equation; h of d equal to 0, where zero over here
is my functional zero. This functional
equation comes from the equilibrium conditions
of the model or from many other type of problems
that we can think of. This is going to be the
decision rule or policy function of the agents and that's what we're looking for. The basic idea of
perturbation is to solve the problem by specifying
our approximation, a polynomial of this form that is going to depend on Theta, which are going to
be the coefficients or for approximation. I think that Larry yesterday
call it parameters. I slightly prefer to call them coefficients because
I prefer to use the word parameter to focus only on the parameters
of the economic model, things like a discount factor. These are coefficients
because they are just numbers in
the approximation that we are looking for. As probably you also
learned yesterday, perturbation is going to be an inherently local
approximation. We are going to find a
non-linear approximation, yes, but only locally
around some point. That will go back to some of the questions before
about whether or not we want to do this
around the steady-state. However, in many occasions, we have accumulated
evidence that this local approximation
has good global properties. The problem is, of course, we don't know for sure in every single case and there are cases where it
doesn't do so great but it's good to remember
that in many situations, the global properties are
actually quite interesting. Let me motivate a
little bit perturbation and I'm going to do it in a complimentary way from Larry. I think that Larry yesterday
probably emphasized the idea of the implicit
function theorem. I always like to think about
this more from the idea of finding a slight variation of the problem that has
an easier solution. The idea is we can search for either a particular case or a related problem to the
problem we are actually interested in that is
much easier to solve. We can use the solution of
this simpler problem to build an approximation to the general solution
that we are looking for. People in physics use
this all the time. If you go to any
physics department, you will see that the graduate students have all these books on
perturbation theory. Sometimes perturbation theory is known as asymptotic methods. I think it's terrible
notation because it confuses with asymptotics
in econometrics but the idea is basically
because we are going to do these Taylor order
approximations that asymptotically
will converge to the right decision rule
under certain circumstances. I'm going to show you the
wall simplest perturbation. Imagine that I asked you what
is the square root of 26. Of course, you need to do
this without your iPhone. Some of you may remember back in middle school that we learn, you remember that
algorithm where you will take things
in squares and you will find for the closest square and do something
with the remainder, it will probably take
me one hour to remember the whole algorithm and then another hour to get
an approximation. A very bad idea. You
don't want to do that but I can say the following, 26 is pretty close to 25. I know that the square
root of 25 is 5, so the square root of 26
would be five plus something. Actually this is the type of computation I will
do in my head. Twenty-six, I can write it
as 25 times 1 plus 004. I can take the 25 out as a 5. The square root of 1.04 is going to be
more or less 1.02, so 5 times 1.02 is 5.1. The exact solution is 5.099. We did pretty well, except
that this thing didn't force me to remember my
times in middle school. It took me like
what? One second. Look, we actually do this
absolutely all the time. We go to the restaurant tonight and they get the
bill and we need to put a 15 percent tip,
we always round it. See, if it is 59, you say, fine, this is 60, 15
percent nine dollars. We don't go and
say 59 15 percent. We are just perturbing or
deep-deciding problem. This is exactly what
we are going to do. The more general
notation will be that we are searching for
the square root of y. We will find the x that is the biggest integer such that
the square is closer to y. One plus Epsilon,
where Epsilon will be the parturition parameter
and then our solution. This will be x is
the solution to the close related
problem and this will be the type of extra element
we need to put to get this. Then it's called a perturbation
because it's basically changing y by a little
bit to get this x. This is the simplest
perturbation you can imagine. How would we apply
this to economics? Well, it was
pioneered by Ken Jab and Guu in a paper in 1993. Recently, perturbation methods have
become really popular. The main reason they have become extremely popular is because you can run them with the NR. I'll show you later in just
one second the NR code, and you can do second and
third-order approximation. Something that
basically requires half-an-hour work is that of three days work is bound to become extremely
popular very fast. In particular, second and
third-order approximations, as I was saying, are both
very easy to compute. They notably improve
accuracy and allow us to think about all the types of problems I was telling
you guys before. In addition to it and that's
another beautiful result, our first-order perturbation and linearization deliver
exactly the same output. Everything that you learn
about linearization of standard models is really a
first-order perturbation. That means that we can
really use a lot of what we already know in
this particular situation. There are two types of perturbations that
you can think about and the best way to think about them is with a physics example. It's the last time I'm
going to use physics today. This pen, regular perturbation is higher as you
slightly move the pen or whether it has isolite move. Singular perturbation, a slight change in
the situation gives you a discontinuity,
a huge change. Typical examples of regular
perturbations in economics, your excess demand function. You change the excess
demand by an epsilon, the price that will
clear the market, in most cases will just
change by an epsilon. Examples of singularities. Well, for example, introducing a new asset in an incomplete market
model because suddenly you are spanning
a much bigger set of things and a lot
of stuff can happen. You're going to have bifurcations
and things like that. This is a little bit
more advanced materials, so I'm not going to get into it. Today let's think about all
the regular perturbations. Basically, the type of perturbation I'm going to
think about is the following. You take your
business cycle model and you know how to
find the steady state, either by paper and
pencil or by using a computer but
that's going to be just a system of
equations, it's very easy. Then less perturb this by introducing a little
bit of a stochasticity, a little bit of randomness. We use the steady state solution to find an approximation to the exact solution in the nonlinear case
with random shocks. For those of you who may want
to learn more about this, a very simple and easy book
is the second one actually, I really like that one,
Advanced Mathematical Methods for Scientists and Engineers, Asymptotic Methods and
Perturbation Theory. Despite the fact that it
says advanced in the title, it's actually easy to read. Don't get nervous but the great thing about
having this type of books in your office is that your colleagues will
be very impressed. I always skip that one and
another one that is called Problems in Advance
Operator Theory, which I have never opened. Every time I call it come, they get away thinking I know something that
I don't really know. Anyway, in economics, Kenjiatt has written a
lot about these type of things and he's really
the leader in the area but there is a very nice
and gentle introduction by Martino Ribe and Stephanie Smith Krohe that came out in general of
economic dynamics and control, which is a good way to
start learning about this. Because they don't try to
solve the more general case, they don't try to handle
very abstract situations and that gives you a lot of concreteness and really
understanding what is going on. Let me do a baby example which will be very similar to
what Larry did yesterday. This is the Mickey Mouse
real business cycle model. We are going to have a representative
agent which is going to maximize the expected
discounted utility, where Beta is a discount factor, utility function is just log, and this is going to
be null issue so, let's make things easier. The production function
is just capital to the power of Alpha
and productivity. You can use the production
for capital for investment. I'm just writing here the capital accumulation
directly in terms of the discount factor
and we have some law of motion for productivity. Equilibrium conditions, this is something very well understood. We have a linear equation, marginal utility today,
marginal utility tomorrow, marginal productivity
of capital. Market clearing and
well, this case, resource constraint and
the law of motion for productivity happens to be the case that this does not
have a closed form solution, at least we don't know it, except in the unrealistic case where Delta is equal to 1. It's unrealistic, but hey, we are here to learn not to do anything deeper than that. The reason for that is because when
Delta is equal to 1, the income and substitution
effect cancel each other. The income and the
substitution effect of a productivity shock. By guess somebody Phi, you can show that the
policy functions, the decision rules, they have this very simple form where it's basically you
are going to invest it. Remember that precision
is equal to 1, so your capital tomorrow and investment at the same object. You are just going to invest Alpha times Beta fraction of your output and you are going to consume 1 minus Alpha Beta. Why is this guess unverified? Because if you plug these
things into these equations, you will see that all
the right hand sides and all the left hand
sides are the same. Well, but let's suppose that you miss the lecture in
your first year macro, where it gets some
verify was explained. It was at 8:30 in the morning and that particular
morning you felt it was way too cold to go to class and to listen to me
explain you this stuff. Now it happens to be the case that you are writing
a paper and you actually need to compute
the RVC and you say, well, how do I do it? Well, very soon you realize that really
what you are doing is looking for a decision
rule for consumption and another decision
rule for capital that will depend on the
two states of the economy. Capital today and productivity or D, in this abstract
notation that we had before, is just the stock, the vector that puts
a decision rule for capital and the
decision rule for consumption and the operator H is as these three equations. Well, it's actually just
these two because this will be an exogenous
driving force. Beautiful. What we do
now is the following. We go to our decision, to our equilibrium conditions and wherever we see a city, we substitute it by a decision rule and
whenever we see a K, we substitute it by
the decision rule for capital and the only
kerat we need to do, we need to take,
to put into this, is that consumption
tomorrow will depend on capital tomorrow, which is a function
of capital today. Here we have two decision
rules nested into each other. This is a system of
functional equations and yesterday you learn that is not easy to
solve functional equations. What we're going to do
is perturb this system. Something that is important
to remember about perturbation is that this
step is usually ambiguous. There are many different
ways in which you can perturb a problem and in fact, what people do in physics, what a lot of the
game is sometimes in physics is about coming
up with new ways to perturb the system that are better than previous ways
that already existed. For example, people
will say, well, "If I reparameterize the
problem in this particular way, there is this other perturbation that works much better." I will actually put an
example of that later on but for the moment, let me stick with this. What you do is once you have
this alternative problem, you solve that problem
and you use it to approximate the solution
that you are looking for. A natural perturbation
parameter in the type of business cycle models
that we are dealing today with is the
standard deviation. By the way, this is the case because we are
using discrete time. If you were using
continuous time, you would like to
use the variance, not the standard deviation. There is some reason for
that, let me not get into it. The reason for it is because when we set the
variance to zero, the standard deviation to zero, then we're back into a
deterministic model and we know how to solve these models in a very efficient way. What I'm going to do is
I'm going to rewrite my decision rules as a function of the states
as I was doing before, but also as a function of
this perturbation parameter. We're now on using
this semicolon to emphasize that is
not really a state, is really the
perturbation parameter. Again, these are my equilibrium conditions now as a function of Sigma. What I'm going to do
is I'm going to take derivatives of all these objects with respect to capital, with respect to productivity, and with respect to the
perturbation parameter. The interesting thing is that, I'm going to have a
system of equations that I'm going to be able to solve for the type of objects I'm looking for. What do I mean by that? First of all, let me get the asymptotic Taylor
expansion of consumption. We have a bunch of
terms over there. We're going to do the expansion around the capital
in the steady state, zero which is the as the productivity and
the steady-state, and zero which is setting the perturbation
parameter to zero. Later on, I will tell you a few more things about why
we want to do it over there, but stay with me for a second. Which terms do we have? I know it's a very
complicated expression, but if you actually look
at it for a second, you realize there is
nothing very deep on it. First of all, we
have consumption in the steady-state,
this term over here. Second, we have
the linear terms. The derivative of consumption
with respect to capital. Capital times the capital, the difference of
capital respect to the steady-state level
of capital, the same. The derivative of consumption with respect to productivity, and the derivative
of consumption with respect to the
perturbation parameter. Then we will have the
second-order terms. We will have second-order
derivatives. The only problem is we
have a lot of them, but it's nothing deep. It's just that we need
to keep track of all of them and then we will
have higher-order terms. This is a point that is going to be important about perturbation. Conceptually, perturbation
is very simple. It is just notationally challenging because you have a lot of terms to
keep track off. Sometimes people who are
unfamiliar with perturbation, feel a little bit of instant rejection
of the whole idea because they have too many terms and they get a little bit lost. Well, try to get over that. Try to understand what
is really going on and you will see later
that this is some algebra. It's nothing more than
a bunch of terms. We will have exactly
the same for capital. Of course, I'm not going to
go over every single term, but it's just the
asymptotic expansion. As I was saying before, this is going to be
notationally challenging, so instead of writing every single equation
and taking derivatives, which will be impossible. It will take me 50 slides just to put
all the derivatives. Let me use the function f that depends on capital
and productivity and the perturbation parameter as the vector of the two
equilibrium conditions that need to be equal to zero. I can also write it in
terms of consumption today, consumption tomorrow,
capital today, capital tomorrow, zd and Sigma. My notation is going
to be very simple, H_i will be the partial derivative with respect
to the i component. I will drop the evaluation at the steady-state when there
is no ambiguity in both. The first thing we
are going to do, is what I'm going to call the Zeroth-Order Approximation. First, you evaluate the function f at the Sigma equal to zero. This is just these
simple two equations. That will give you capital and consumption in
the steady-state, which will be this
expression and it will be this expression.
This is very simple. Then you will take derivatives of f with
respect to capital, with respect to productivity, and with respect to Sigma. You will make them
equal to zero. Why? This is a point that is extremely
simple to understand, but the first time it will
surprise you a little bit. Let me go to over here. When we take basic equations in high school or
something like that, they will tell you that
you have the problem. Alpha x plus y equal
to 0. I don't know. Gamma x plus Sigma y equals, let me do it some other things, three, so it's not that. It's easier to see. Anyway. Over here, what
you are looking at for x and y's such that these two equations
are equal to what? Let me put this
minus 3 equal to 0. That these two equations
are equal to zero. Equilibrium conditions
are different. Equilibrium conditions is
not give me a capital and a productivity such that the Euler equation
is equal to zero. Well, when you put
the Euler equation, the left-hand side and the
right-hand side together. No. This is for any capital
and any productivity, the Euler equation
is going to hold. Why, because you
are going to have endogenous choices that are going to change to ensure
that this is the case. If I plot f with
respect to capital. Remember, f is just
the Euler equation, like the left-hand side
minus the right-hand side. If I plot this function, f over kt, it's always equal to
zero. You see why? Because the equilibrium
condition is for any capital, the left-hand side and the right-hand side of the Euler equation needs to be equal. Function f flat. Now, if the function is flat
with respect to capital, what will happen with the first derivative of this function with
respect to capital? Better be zero. What will be the
second derivative of this function with
respect to capital, zero. What will be the
third derivative, and so on and so forth. They will always be zero. I know that the first derivative with respect to capital, with respect to productivity, and with respect to the
perturbation parameter of my equilibrium
conditions will be zero for every value
of the states. What is going to happen is
that this is a system of equations that I can solve
for my unknown coefficients. Basically, what is
going to happen, let me skip this for a second. Let me go over here. When I evaluate
this derivative of the equilibrium
conditions at capital in the steady state zero
productivity and zero, a standard deviation is
going to depend on H_1, the derivative of the function H with
respect to capital. C of k, h to c of k, k, k, H_3, H_4 and so on and so forth. I know what these H_1s, H_2s, H_3s, and H_4, 5, and 6 are going to be. If we we were doing
the whole algebra, all these terms will only depend on a steady-state values. The only thing that I don't know is CK, KK, what are the things I have? CC, KC, C Sigma, and K Sigma. There are six things I don't know and I have six equations. It looks like I
only have three but remember that F is the stack of two equations so I have six equations and six unknowns. Given that F is zero for all estates and all
perturbation parameters, and given that the first
derivative needs to be zero, the only way this
can happen is if all these partial
derivatives are such that this thing
is also equal to zero. I solve this system and I have all these theorems that
I need in my asymptotic, remember my asymptotic
expansion before? This is the term. I'm
going to get this term, this term, and this term. Before, in the C
zeroth-order approximation, I got my C. Again, if you're not very
familiar with this, looks like a lot of notation but when you go back
home and read over it for 10 minutes you will realize that
it is really nothing. This is very trivial. You can actually decompose
the system in two parts. The first two equations
were the first four in reality is going to be
a quadratic system. You can solve this quadratic
system with any of the algorithms that
are out there to solve quadratic matrix systems
like Blanchard Kahn, Harald Uhlig, Chris Sims, Pete Klein, all of those
algorithms are a standard. There are dozens of references
for them out there. All of them should give
you the same answer. That's why you have
this equality between this first-order
perturbation and the traditional
linearization because actually these four
equations over here are the same four equations
you will get if you linearize your
first-order conditions. Now, one thing that is interesting is that this
is a quadratic system. The reason why this is a
quadratic system is because rational expectation models are always going to have these
two possible trajectories. One trajectory is, let
me go over that again. When we were studying
the Ramsey growth model, and there is always
a stable manifold. If we have this level of capital we will jump in
merely over there and then just walk through the stable manifold and
another one that is unstable. This is basically a
trajectory that satisfy the first-order
conditions period by period but that violates the
transversality constraint. Think about this as grease. If you keep borrowing you can make the manual
utility today equal to tomorrow but eventually
something bad happens. We say, look, we
eliminate that thin. Since linearization, actually, instead of being like this
will be like this but it doesn't really matter because it's linear approximation. We are always going to have these two possible solutions. The way this pops out is in the fact that this
is a quadratic problem. Basically, where
this shows up as a quadratic problem
was over here. I don't want to
overshoot. When we were substituting
consumption tomorrow, consumption tomorrow will be a function of capital tomorrow, which will be a function
of capital today. When you take derivatives, the chain rule will
give you two terms, the derivative of consumption with respect to capital and the derivative of capital
with respect to capital and that's why we
have a quadratic system. By the way, for many years
people thought, well, when we do a first-order
approximation we have a quadratic system, when we do a second-order
approximation we should have a cubic system. It's really bad, no? It's one of these paradoxes, that's why mathematics
is absolutely beautiful. When you go to second
order the system, we will see in a second, reverse to be linear again. The reason is that once
we have eliminated in the first-order
approximation then unstable manifold we don't have
that problem anymore. The only thing that
we are going to do is refinements in the
right manifold. Those systems are
inherently with only one solution and that's why everything
is going to be linear. That's something that most economists didn't understand for a long time until [inaudible]
explained that to us. The last two equations, number 5 and 6, is the one related with the derivative with
respect to Sigma, which is a linear a
normal union system, which implies that
these two are zero. When you go to the
asymptotic expansion before, this term over here is zero and this term
over here is zero, the derivative with respect to the perturbation parameter. What did we just do over there? Well, we just show that a first-order
approximation is certainty equivalent, which is the same
thing that you got with a standard
linearization because, again, standard linearization should give you the
same over here. Basically, what
this is telling you is this type of a stuff I
was telling you before. Linearization of first-order
approximation is equivalent to a
second-order approximation of the utility function, which implies that you are not going to have any type
of precautionary behavior. That comes directly from
that equation over there. In a very beautiful way,
everything fits together. Well, let me escape this. We can go to the
second-order approximation. Sorry, I'm skipping a few slides because there's a
lot of material and time is limited so I'm
maximizing as we go along. We take second-order
derivatives. Remember, exactly the way
we did before, of course, there is going to be
the Young's theorem, which tells you that a lot
of this cross derivatives are going to be the same. We are going to have now a linear system of 12
equations on 12 unknowns, which are basically,
let me go back to the expansion before. They will be like this term, this term, and this term. Let's recap for a
second what we did, and I know I went fast, but it's not that complicated. We will write the equilibrium
conditions of the model. Well first, we find
the steady-state, then we take first-order
derivatives. Those first-order
derivatives will be solved, they need to be 0
for any value of the states that those will
give us CK, CZ and C Sigma. We take second-order
derivatives, those will give us
all the other terms. We can take first-order
derivatives or the cubic terms, and
so on and so forth. You can do this until
you get bored or until, most likely, you run out of
memory in your computer. You see how we are proceeding? In some sense it is very simple. Also, when we compute
the solution, we will find that a lot of interesting things
happen over here. Instead of going over the
math and the slides on it, I'm going to switch
for a second to Dynare because it will be much easier to see it in
real time as we do this, because all these
different things have a very nice and simple
interpretation. Let me see what I
have in MatLab. I need to do escape MatLab. The first example we did as motivation is a model with
Epstein-Zin preferences. How can we handle a model
with Epstein-Zin preference? Is using Dynare. It's
actually surprisingly simple. You can look over
there at my code, I always try to document
my code very carefully. If I have learned something
in this profession is that you send out the
paper to the editor, comes back one year later. This is a stochastic process
with only two realizations. The first realization is; after reading your paper, I have never realized how
stupid a human being could be, which happens to me quite often. The other realization can
be after I read your paper, I couldn't realize how stupid
a human being could be, but out of the
kindness of my heart, if you do this 20 pages of
revisions, I will publish it. In both occasions, you
basically need to do the same, which is going back
to your code and try to figure out what the heck
you did two years ago. If it is not well-documented, forget about being able to
do a successful revision. I do this, you can
see very basic and a straightforward
explanation. The key over here, I guess that most of you are familiar with Dynare syntax. If not, there are plenty
of Dynare introductions, gentle introductions
to Dynare out there, and they're very easy to follow. In this block bar, what you do is you define
the different variables. The thing I'm going to do is I'm going to introduce this
variable over here, E-V, which is going to be
my expected value function. It's an auxiliary variable. That's one great
thing about Dynare is a lot of things
that look difficult, actually become very easy when you define an
auxiliary variable. You will see what this
auxiliary variable is. Then, remember that this
was the paper about all these bond yields
so that's why I need to have our 20 different, I'm going to have one quarter, two quarter, three quarters, four quarter up to 20 quarters. Yield actually is a bad
name for a variable, it should be the yield spread but fine, I'll
just call it yield. Inflation and the shock. This is just a calibration. In the real paper,
this is actually nested into an estimation, but to teach, I can
calibrate the model. Let me show you first one theme, and then we go back to some
stuff I was doing over here. Remember that the main block of any Dynare code
is going to be the block model where you define the equilibrium
conditions of your model. What I'm going to do over here is to define my
auxiliary variable. Remember that in
Epstein-Zin preferences, we had this risk
adjustment operator. That was the expectation of v of the utility function to
the power of 1 minus Gamma. Dynare needs to be told that
this has an expectation with respect to t
because otherwise, if you just don't tell
anything to Dynare, it will just take the 1
minus Gamma and cancel with the 1 minus Gamma that were
outside the expectation. What I do is I call
expectation of v as v plus 1^1 minus Gamma and then in my definition
of the utility function, I will plug that ev. Surprise. This is the only
thing you need to do to your model to have
Epstein-Zin preferences. You see why it's such a
beautiful as it's more in deviation with respect
to the basic framework. It literally is a variety
in one more line of code. Don't get impressed by people using Epstein-Zin preferences. Once you understand them,
it's actually very simple. Now, I define another
variable that I defined, is my value function, which is just u^1 minus
Gamma divided by Theta, and you can see over
here how I have this ev, this expected value. There are a few terms over here that look a little bit crazy, maybe I should have had a
simpler version of this code, well, too late now. In the paper, if you go to my webpage and you
download the paper, we have growth over there. We need to re-scale and
refill to have growth. This is just the
term that re-scales the value function
to take account of growth so nothing very deep. Also, u is just the
utility kernel, which is going to be c^u, 1 minus l ^1 minus t so also that is
very straightforward. Then, all the other
equilibrium conditions of the model are
extremely simple. You are going to have the standard static
leisure consumption. You are going to have
the pricing kernel, which is a little bit
more different than the pricing kernel in
the standard model, because with Epstein-Zin
preferences, you will have more
terms floating around. The earlier equation
for capital and then all the equations
for the nominal bonds. This is just the pricing kernel. 1 plus inflation plus 1, that will give me the inverse of the yield of the
one-quarter bond, and then I just recursively
define everything else because the
expectation theory of the interest rates
will hold over here and that's what
I was telling you, it makes this computation slow. This is just defining this
recursively period by period, but there is nothing
deeper over there. Then, the yield is just going to be the difference
between the 20 periods, the 20 quarter bond and
the four-quarter bond. The production function, the rental rate of
capital, wages, the resource constraint, the Law of Motion for
productivity, and the Taylor law. Now, a couple of things
I'm going to do. These are a couple
of very nice tricks which are the following, well, this is just preferences,
nothing over there. You can ask Dynare to compute the steadiest
state numerically, can be quite unstable. Solving these
numerical system of equations may be difficult if you don't have a
good initial guess. The great thing is, you can solve for the
deterministic steady-state of nearly every single
business cycle model that I know if you already
know what l is. L is labor in the steady-state. I actually estimated
a very large model for the economic office of
the Prime Minister in Spain, we had 85 endogenous variables, and I was able to find
the steadiest state analytically with paper
and pencil for the other 84 given l. The
result is very simple, it's usually in your standard
business cycle model, the big pain is that
you're going to have a static first-order
conditions are a form like C divide by 1
over l is equal to wages. Marginal utility is equal to marginal rate of transformation and
this will depend on l. This will be a function
of l. We are going to have a 1 minus l over here and an l over here and
that's a pain to solve. Well, and there will be
a parameter over here. What I do, and this is a very nice trick,
is the following. Fix l. You can say my steady state l is going to be 1/3 so we are going to work eight hours
a day on average. I solve all the other system of equations of
the steady state, except this one because I'm
already fixing one thing. I solve all of those. Again, in all the cases
I have worked with, I was always able to do
it with paper and pencil. Then I go back. Consumption now is an
endogenous object. Capital and labor will be well, capital will be
endogenous. Labor I fix. I just use my last equation
to find whatever parameter in the utility function will
justify l equal to 1/3. You see how simple this is? The beautiful thing
about this is that for most business
cycle models, the average level of L
is just a normalization. What you care about
is how this theme fluctuates over time,
not about its level. Because if I tell you that we work eight hours on average, that means that
we work one-third of the time of the day, or you are going to subtract
the sleeping time and then you are going to work 50
percent of the wake-up time. It's the same model. It's
just a normalization of what you consider is the
amount of time available. That's what I do in this model. I just fix L. Well, in this case it's even simpler because I don't
even need to fix L, I can actually solve absolutely everything
with paper and pencil, and I will have that over there. Another thing I'm going to do, is I'm going to normalize my value function such that in the steady-state
has a value of one. The reason I want to do that
is because in Epstein seen, the trick of Epstein seen is
put large risk aversions, like 100, 200 and
see what happens. Remember that I'm taking that value function to
the power of minus Gamma. I don't want the Gamma to
be 100 and V to be 30, because 30^100 very
difficult number to handle. If I normalize V^1, such that the steady
state is one, the fluctuation and we'll
fluctuate a little bit, like 1.01, 0.99, but 1.01^100 is
something like 100. Do you see that? What I'm going to
do is I'm going to find consumption and labor
and blah, blah, blah, and then I'm going to find a constant is this norm
constant over here, and then I get this CTE constant such that my utility function
is pre-multiplied by that. This is just an affine
transformation, so it doesn't matter, it
doesn't affect any decision. Then my value function
fluctuates around one. Really computing a model
like this one with Epstein seen is once you
have a little bit of experience it's very easy. Now let me check that, in fact, we are going to find a second-order
approximation. The command that runs the simulations in Dynare
is a stock symbol. Of course what happens
is that over here, you can tell the order
you want to run. You can put here either one, which will be the
first-order approximation, two or three. Today I'm going to run
two and not three. The reason for that
is because for three, you still need to install a
C compiler in your computer. You will not need to use it, but Dynare will need to call the compiler and do
some stuff over there. So I just want to
show you how to do these things with
plain by need a Dynare. The great thing
about Dynare is you go to Dynare web page, you download it, you
install it in MATLAB, in five minutes you
are up and running. Let's run this thing. This is Dynare, easy. Tailor 11, you call
it? Level tailor. It's going to do stuff. Now it's doing a few of the
computations. We are done. You can see really the level of computational at the
end of the day is literally a couple of seconds. Let's look at the output
of what this thing did and we can look at it. First of all, this is the
steady-state results. You can see this will be
the same as that if we were doing linearization it
doesn't really matter, but you can see how I
normalize V^1 and in fact, it spits out that
V is equal to 1. What else over there? The interest rate, of course, in deterministic
a steady-state, the yield per period
of each one must be constant because the
interest rate is constant. The spread is zero. I did not talk about
the stochastic process for inflation, it
doesn't matter. It's not really very
important over here, but you're going to
have 50 basis points of inflation per watt. This is the type of
problems you may find when you do
things with very, very large risk aversions. You may find matrices that
Dynare needs to handle. They are going to be
close to singular. Actually these ones
over here, I check out. We could talk a little bit more about that. It doesn't
really matter. They get numbers
but they're fine. But actually, let me maybe reduce the problem
by changing this. Gamma, wow, I have 79.54. Let me put two, and elasticity of
[inaudible] substitution let me put 0.5. No, because that's Epstein seen. Sorry, standard 0.7. Now I will not
have that product. Sorry. Come on. I must have changed something
without noticing it. How can this be not? Fine. Come on. Don't do this to me. What do we have over
here, 20 I think. This may not work. Sorry about that guys. Let me go to the
backup [inaudible]. I want to copy this. It's funny because this is actually pseudo-random. Yes, literally, I am safe. I think that that
was the problem. Fine. Sorry about that. We just wasted two minutes
in a very similar way. Now let's look at the output
that this is giving us. Let me go over here so
I can point it out. Let me forget for a second
about what it says, constant and correction.
We'll come back to that. You can see first the type of objects that you will get from the linear
approximation. You will have in this model, utility yesterday is going
to matter for inflation. The expected utility
consumption yesterday, capital city, R1, all this happens to be
states for inflation. You don't need to
worry much about those but basically like the first 1, 2, 3, 4, 5, 6, 7, 8, 9 terms will be the linear terms
that you will get from your linear approximation. Then you are going to have all the quadratic
terms over here. You see how we have
a lot of them? But the great thing
about binaries just spits all of them. By the way, not only
puts it on the screen, they also stores it in a
matrix so it's easy to access if you want to
manipulate them in some other way later on. You can see well, we
still have a lot of them just because we
have many states. What is going to happen with a constant is the following thing. Let me go back to this
equation for a second. Look at this term. This is a coefficient so this
is a number 3, whatever, but this term we are going to evaluate at a given value
of the standard deviation. Along the equilibrium
path of the model, capital is going to be endogenous and Z_t is
going to be endogenous, but sigma, once we have fixed the calibration
or if we are estimating once we have
fixed the parameter values for that brand of the
likelihood function is fixed. This is a constant
times a constant so this whole term over
here is a constant. Yes. I have a slide
about that later on. Let me wait for a second but you see why this
thing is a constant? This is actually the
constant which is going to induce the
precautionary behavior we were referring before. This constant is exactly what precautionary
behavior is all about. You can see those. Let's go back to
MATLAB over here where it says no constant. Constant is just the value of the variable deterministic
steady state. You can see the correction induced by the second-order
approximation. Then you will see, for example, the theoretical
moments of the model, and wow you are going to find things like the yield spread
is actually negative. That's why you are unhappy
because on the data, the nominal yield cliff is
our sloping on average. This is something that
happens all the time with business cycle models, production models, we're
getting narrative. The research say if
we had more time, we could actually go over the details of
why that's the case. This model, even once we put
Epstein sing preferences, doesn't do a very good job at capturing the slope
of the yield curve. It does good things about
how it moves over time and stuff like that but not
about capturing that slope. What I was really trying
to emphasize over here is even a relatively
complicated model with Epstein seen preferences, a lot of yields and you want to do a
second-order perturbation, it really takes
nearly no effort. Let me show you another example in diner with a
stochastic volatility. This is an even simpler code so it will be even
easier to see. Maybe I should have
started with that one. This is the following thing. I'm going to take
the basic RBC model. Chapter one is Cooley Prescott
and I'm going to have a stochastic volatility on
the process for productivity. Let me write it over there
so we are on the same page. I don't remember which the function I use, doesn't matter. The productivity is
going to be subject to that process that has a T over here and then the log of
this thing, it's an AR1. May not be that your
essays and AR1. How do we compute
that? Very easy. First of all, I always like
to write all my models in terms of the pricing kernel because all the other equation will depend on the
pricing kernel. This is very convenient
because it allows you to change the utility function
in your model very easily. You only need to
change the equation that controls the
pricing kernel. You will see, for example, at the pricing kernel over here, it's going to be Beta and the ratio of
marginal utilities. I'm going to use year charts utility functions and the
reasons I'm going to do that is again to eliminate
this wealth effect which complicates the
interpretation of results but it's really nothing
very deep about it. This is just the fraction of the two utility of money
and utility functions and then the Euler
equation is one. It's the pricing
kernel two model times 1 plus R plus 1 minus Delta. You are also going to have
another equation return on risk-free bonds that
is going to be one equal to the pricing kernel
and the return on the bonds. Obviously, by nonarbitrage, you need to be
indifferent between putting in one and in the other. These first three equations, they will be exactly the
same in the basic RBC model. The static condition between
leisure and consumption will also be exactly the
same between the two models. The rental rate of capital is just Alpha Y over K minus 1, again, will be exactly the same, wages exactly the same, law of motion for
private capital, again, exactly the same, the
production function, exactly the same, resource
constraint exactly the same. The only thing that
is different is when I get to 10 and I write
my productivity process. Z will be yes,
autoregressive component. The shock innovation is Z but what is going
to happen is that now it's going to depend
on this exponent while I brought it over very levels but over here is an exponent,
doesn't matter. Is the exponent of Z sigma c which is the level of volatility in
that particular moment. By the way, I'm taking the
mean out, it's easier. C sigma Z mean. In that way, these
autoregressive process they don't have a mean. This is the equation
I need to add. I need to show how volatility evolves over
time just as AR1 process. If I erase 11 and I erase
this exponent over here, this will be exactly
the basic RBC. I've written it in a little
bit more detail because I have had this
pricing kernel and all that but is the basic RBC, nothing
whatsoever different. I have already
introduced two things. This change in volatility over here and this law of
motion for volatility. You see what is going on? This is really what I
was trying to emphasize. This is really a
very small deviation with respect to standard things. This is really why is
very easy to do this. Then nothing. I will just
compute the whole thing. Look, remember what
I was telling you, this will impose that output is equal to 1 in
the steady state, I will just speak, are constant in the Cobb-Douglas production function
for that to be true. LS will be equal to
1/3 and then it's straightforward to compute a
lot of different arguments. Then a yes, find
the parameter in the utility function
and the parameter in the production function
such that it is in fact the case that Ys and Ls are what they are
supposed to be. Then let's run this thing. Let me clean the screen. How did I call
this RVC as V Low. By the way, the reason I call this RBC as V is for a
stochastic volatility. A low is because I
calibrate it to the type of fluctuations in volatility you will observe
in the US economy over the last 50 years. Have another one RVC as Vi that deals with if you are Argentina or
something like that. Let's look at the results. Again, now is beautiful. We don't have any
numerical problem. We have y is equal to 1, l is exactly one-third, perfect. c sigma z. Remember, is normalized because I already
took the mean out, so this is 0. Now we have many, many
less state variable, so it's easy to see
what is going on. You can see why it fluctuates
around one. That's good. Then you have all
the linear terms. It only depends on
capital and productivity. That's good. That's what
we were looking for. The reasons c an l
appear over here is because they are
the state variables for the pricing kernel, but not for output or the
decision rules in this period, and we have the innovations. Notice, volatility
and innovation to volatility do not show up in
the linear approximation. Well, they show up with
that term equal to 0. This is the certainty
equivalent result. When we go to the second-order, you will see that we have quadratic terms like
k minus 1, k minus 1. Of course, we know that l minus 1 is not even a
state in the linear, so it will not be as state in the second-order
on the radar, but we have k minus
1, k minus 1, z minus 1, k minus 1, c minus 1, c minus 1, and now we see volatility and the
shocks to volatility playing a role over
here, and over here. It says small role, you have two zeros over here
and three zeros over here, because for the US economy, the fluctuations induced by volatility shocks on productivity are
not very important. Because productivity hasn't fluctuated that much in the US, the volatility of productivity but you can see how
you can solve this. Literally what you do is, you take your standard model, you've write the Diner code, and then you just specify this extra law of motion for volatilities and
you run this thing. Now, things I wanted to
say over here is, well, these are the type
of results I was mentioning before
about what appears and what doesn't appear
in the first and the second order
approximation, and what else? Let me see if I
have some windows. These are the shocks
to the levels, oh, I know what I wanted to say. These are the shocks
to the volatility. Now, in this models
with volatility shocks, you are going to have
two types of shocks. You have shocks to the levels and shocks to volatilities. You need to be careful
remembering in which one you are
referring each moment. So shocks to the levels is like a shock to the
productivity level. Shock to volatilities
are shocked to the dispersion of
volatility over time, and they are going to
have different effects and that's why they have
different economics, and it's interesting
to think about them. The great thing about
perturbation is it really allows you to
compute these models in a very easy and fast way, and Diner is going to do all the heavy lifting for you of computing
these derivatives. Now, what do you do if you
do not want to use Diner? Well, first of all, you can ask me why you may not want to use
Diner to begin with. Well, unfortunately, Diner does not have same estimation
for second order problems. They can estimate, it can do Bayesian estimation
for first-order, but it cannot do for second-order
because they haven't called the diversion of the
sequential Monte Carlo. So you may say, not only I
want to compute the model, I also want to estimate it. So you will need to go beyond
Diner and come up with some method to
solve what Diner is doing that is taking
all these derivatives and solving for them. How can you do that? Well, when you take a derivative,
there is two ways, well, they're
actually three, but let me forget about
the third one. There are basically two ways in which you can
take a derivative. The first one is to take what is called a
numerical derivative. Numerical derivative
is literally what the intuition would tell kids the first day
when we explain them, what a derivative is? You say, well, I
know what f of x is. Let me make f of x plus 0.001 divide by 0.0001,
and that's the derivative. First-order numerical
derivatives were fine. Second or higher order
numerical derivatives, they accumulate a lot
of error very fast. If you only had to take one
second-order derivative, it could be okay,
but in practice, you need to take 25, or 50, or 100 second-order
numerical derivatives because you have a
lot of equations with a lot of states, and a little bit of error here, and a little bit of error here, and a little bit of error here and a little bit of error here, suddenly we are talking
about weak errors. Second-order numerical
derivatives for this type of problems,
very bad idea. What can you do? Well, you
can go to Mathematica. Mathematica or some other
type of symbolic language is going to allow you to compute these derivatives
symbolically. Which means I can
pick a stream of analytic expressions
that you don't even want to open
and look at them but the great thing about
Mathematica is that you will specify the
first-order conditions and tell her to compute
it ends in a, so Spanish speaking, I'm going
to assume it's feminine. So it's going to take
all these numerical, all these symbolic
derivatives for you, and then it will
solve the problem. If you want to see how you
can write mathematical code, go to my webpage,
that does this thing. Go to my webpage. In the paper, I was mentioning before this one in 2005 with Boragan Aruoba and Juan Rubio
has a companion webpage. Just browse through it in
research and you will find it. Over there, we set up all the code that we
use for that paper, and in particular, we have
all the mathematical code. You can download that code
and look at how it is done. When we want to do estimation, when we estimate
all these models with second and third
order approximations, we asked Mathematica to compute all the
symbolic derivatives, and the great thing
about doing it in this way is you only
need to do this once. We estimate all these models with second and third
order approximations, we ask Mathematica to compute all the
symbolic derivatives, and the great thing
about doing it in this way is you only
need to do this once. Because symbolic derivatives by definition depend on Beta. They don't depend on 0.99. So you are just going to have
a gigantic expression is going to have literally
thousands of lines but you don't care because the computer is doing it for you. So you're going to have
this gigantic expression, and then you are going to
say for Beta equal to 0.99, evaluate that expression, and that's something that computers can do in one second, and then you can nest these into estimation algorithm and
actually do estimation. To summarize, if you want to
do this perturbation thing, just put in some numbers,
see how it looks. What type of results you get. Diner, great alternative. Can do this second-order,
can do third-order. There is still a unit
value with more work, but it's really
half-an-hour investment in time or learning
how to do it, but it cannot do estimation. If you want to do estimation, you will need to do these
things in Mathematica or some other equivalent
symbolic language to have the analytic expressions
for your derivatives. You see all this? How can you do all these type of things? Now, as I was talking, I was remembering things
that I had to say. Let me say a few more things
as we go along, and maybe later they will
come back to my mind. One question that you
will face a lot of times when you are doing
this type of exercises is, we got too high get
order perturbation. You convince me that
second, third order, fourth order terms are good, but how many are enough?
Do we want to do 20? We want to do 30, do
we want to do 60? Or we want to stop at three? Now, often a few
iterations will be enough for most of
the things we want to do second or third
order perturbation will be good enough. What we do in this paper
I was mentioning before, is we just did a lot
of perturbation going high-grade hybrid
orbitals and we found that after five,
nothing changes. Even with three, I
will be blown away. If you can show me a model where the fourth order at a lot with respect
to the third-order. The great thing is
second and third-order, very easy to do. Danielle will do them for you. Of course, there are situations where being really careful about this high good
order is very important. I put two of them over there. Maybe, perhaps the
most famous one is a very nice paper
by Kim and Kim, and it's about welfare analysis. The argument is, even in the standard real
business cycle model, it is true that if you are interested in our things like, such as business
cycle and statistics, linearization is good enough, but if you are
looking for welfare, linearization is not
going to be good enough. The reason is because welfare is a very non-linear
transformation of quantities. It goes through the
utility function. You may get very good
answers for business cycles, the type of things that
Larry argued yesterday, and I argue today is that
business cycle statistics, correlations of consumption
with investment. Linearization for the US will
probably be good enough. Welfare analysis, it
will not be good enough. The example of Kim
and Kim is very nice. They show a simple
international economics macro where there are
two countries that they have a barrier to trade and they eliminate the
barrier to trade. Suddenly we go from
something that is not Pareto optimal to something
that is Pareto optimal, our welfare should
go up hopefully. They saw that in the linearized
version of the model, welfare doesn't go up, but when you go to
second and high-grade or their welfare indeed goes up. It is the question that
you are trying to answer, the one that should dictate which type of procedure you use, not the procedure that dictates which question you
are going to ask. You understand what I mean. At the end of the day
we are economists, we are not numerical scientists, we are not numerical analysts, and believe me, mathematicians
are very smart, we are not going
to beat them into coming up with fancy methods. You should always keep in
mind what we want to ask, what do we want to understand, and that will give us how high in the approximation
you need to go, but again, this is
just another example. The great thing about
this numerical example I can show you now. With that, we will wrap
up the second session. Is that this is, remember the example before
about delta equal to 1. The great thing
about this example is that I know the
exact solution. By the way, when I'm
talking about these things, I always try to say exact and
not true solution because through rings a little bit about the true model of the wall
or something like that. What I mean by exact is that is the solution we will have if we have access to the best computer ever from Star Trek or
something like that, or we could compute perturbation with thanking the helium terms. The good thing about
the example with delta equal to one is that
we have a solution, and we can plot
the exact solution against the approximation. Doing by paper and pencil, I'm actually able to find
the first-order terms, the second-order terms
and so on and so forth. This will be just the
comparison between the exact and the
approximated function but probably you will
understand this better if you look at the graphs. Blue line is the exact solution. Over the x-axis, I have capital today, and over the y-axis, I have a concern. Now what is this thing? Oh, no, I'm plotting
it with respect to productivity, sorry. I'm plotting
productivity over there. Then on the vertical line, I'm plotting how much capital
you are going to have. Of course, when
you do the set or that approximation
is just a constant, it doesn't depend
on productivity. When you do the
first-order approximation, you can see how the red line, which is the approximation is linear because it's a
linear approximation. It does a very good job
around the steady-state, around the center of
the approximation but it does a much worse
job, on the tails. But when we go
with second-order, now it's nearly impossible
to tell them apart. If I did that out of there, it will be nearly the same. That's what I was trying to say, that this is the
graph that a guy who does linearization will
show you resell look in this area which I care
about in business cycles, linearization is good enough. What I have tried to
argue today is there are cases where we
may care about this, or situations where
this decision rule is much more cure of. What second-order
is going to do, is going to introduce Cuba to third-order in the
decision rules, so you're actually going
to do much better. Well, this is what
I was saying before about ways to
compute this thing. Let me stop here. We go for lunch,
and after lunch, I'm going to talk about some
properties of the solution, and I'm going to make
a few more remarks and in particular
talk about accuracy, and we will try to talk
a little bit about projection and how projects and compares with parturition. When we want to take in
all these derivatives that system work and we
could find the coefficients that we were looking for
because we knew how to evaluate a lot of these
functions in the steady-state. We don't know what
these functions look like at that particular point. What do people who like to do expansions on this point do? Well, they first
solve the model, expanding around this
capital in the steady-state, they simulate the model, they find the new
ergodic distribution, they evaluate everything
at that point, they simulate again, and so on and so forth
until convergence. The problem of that is one
that is absolutely not theorem whatsoever
that tells you that this should
converge to anything. It's a very complicated
fixed point and we don't know anything
about that problem. Second, it is trivial to build examples where it
actually diverges. I think that the best
thing that one can do is actually do the expansion
around the steady-state. Because first of all,
we have theorems that tells us that this is the
right thing to do, and second, because numerically we
know that it converges, we don't know what happens
when we do it outside. The fact that they
compute that, remember, the problem with cold is we're always going to
converge to something. The code is going to end up and it's going
to tell you 32. That doesn't imply that you actually converge to
anything meaningful. It just means that the
whole loop is stop. That's one important
thing to remember. The other important thing
to remember is that we need to have methods to judge the
quality of approximation. The problem for that
enterprise is that of course we don't know
what the exact solution is. Before when I was
doing the example with this depreciation
equal to one, we knew the exact
solution so we could measure how far away we
were but in real life, we are not in that situation. There have been several methods proposed in the literature, one that I liked and I have used a lot in many of my papers, is what is called the earlier equation error by Karl Pearson. Basically the idea is, remember this is just
the earlier equation, marginal utility today, equal to expected money on utility tomorrow rate of return. If we add the exact solution, the left-hand side and the right-hand side
should be the same. But they are not
going to be exact. With an approximated solution. There is going to
be a little bit of a difference between the two. The way I'm writing this
is already undoing. Over here, I'm already
multiplying this C over here. Really, I'm doing
the manual utility, I'm taking the inverse
of the marginal utility, and the reason for that is to express it in consumption terms. We don't want a measure of accuracy that depends on units. Because then instead
of using euros, we start to use these new
currency that is coming called drachma and things
will be very different. We really want to get
rid of these units. Also, people usually
report the log in decimal terms of
the absolute value of the Euler equation error. The very beautiful thing about that measure
is that it has a very simple interpretation,
which is the following. An error, for example, over here of minus five, means that you are making a $1 mistake for each $100,000
that you are spending. Minus five, is five zeros
because we have decimal logs. The best way to think about this is the following. You are using your
approximate decision rule. You are the household,
the family, and I come to you
and I tell you, how much money will you pay me to have the exact
policy function? I have it here in this box, and I can open the box and tell you the exact policy function. You're going to
basically tell me, well, I spent $100,000 a year, I'm going to pay you $1. Which tells you, anyone, who has ever tried to
maximize in real life, that minus 5 may be already
very, very good accuracy. How do you report these
earlier equation errors? There are two ways to report them or people in the
[inaudible] have tried to do it in two ways:
one is to plot what I'm going to call here a transversal cut of the
Euler equation error. What I'm doing, this
graph should be really in three dimensions because
you will have capital, you will have productivity. Then the Euler equation error, which is a function because it depends on which
point you are but since I cannot really
do three dimensions, what I'm doing is I'm facing
productivity at zero, I'm making a transversal cut. I'm coming with a knife
and cutting those graphs. So a bunch of lines. What is this green line? This is capital, the
steady-state over here was 23. What is this green line? This is the Euler equation error when you log-linearize
the model, which is you
linearize the model, except that you do it in
logs instead of levels. You can see how very close to the steady-state,
it does very well. Euler equation errors
of around minus 4. So it's $1 for each $10,000 but when you move away, that's a little bit
worse, around minus 3. So $1 for each $1,000. The next line, the blue line, is with linear, or if you want to
think about it, with first-order perturbation. Actually, surprisingly enough, except in these
two small regions, it does better than log-linear. This is actually something that you find in many other
papers where they really, even if it is more popular to log linearize
than to linearize, I think that everyone
that has looked at this in a little
bit more detail, they tend to find
that linearization does better than
log-linearization. I will say a few more
things about that later. The great advantage of log-linearization is
that a lot of times you can do algebra and get insights
about what is going on, but linearization seems to
work a little bit better. You see this, what
is this color? Like purple, [inaudible] line? That's 2nd order approximation. You see how we actually gain
two orders of magnitude. We go from minus 4
to minus 6 or 5.5. Even the simple second-order
approximation that takes 1 second in [inaudible]
to compute, actually improves
the thing a lot. I'm very, very close
to the steady-state. Look, it actually
goes to minus 8. Let me go now to this
yellow line over here. This is what happens when we have a 5th-order approximation. You can see how it goes to Euler equation
errors of minus 8, which is really great. Minus 8 is like $1 for
each $100 million. So this is as accurate as
you may want in real life. However, notice what
is happening over here is very quickly going up. This is actually for
high capital like 2627 but if I keep doing this, it will deteriorate a
lot and very, very fast. The reason is because
if you go to my paper, you will see it over there. By the way, this graph
is taken from the paper that I have mentioned
several times, 2005 JDC. When you actually plot the
policy function, we know, this is the neoclassical
growth model, the RBC, so I know that the policy function is concave on capital. When you plot the 5th
order approximation. This is the steady state. When you move
sufficiently far away, in some moment it
becomes convex. The reason for that
is because you are having these higher-order terms; 2nd, 3rd, 4th, and 5th order terms. When you get away,
this is basically telling you that the
radius of convergence stops here and that you
actually have a convexity and the whole thing is taking you to a very,
very bad scenario. That's something
that is easy to tell from these Euler
equation errors. What do I conclude from here? Second-order approximation
does quite well, much better than
linear recession. So it may be useful to
do it in some cases, not always, remember. Then we have 5th order
that does really, really well, at least
in a reasonable range. Over here, this black line is value function iteration
with something like three million points or five
million points in the grid. The reason is we know that value function iteration is
always going to converge. It's always going to
deliver something that asymptotically goes
to the exact solution. What I wanted to show
with this graph is that 2nd order perturbation is perfectly competitive with
value function iteration, even with three million points. This is the RBC, three
million points is a lot of points in
a value function. This red line is
finite elements, which is one of the
projects on methods. Hopefully, I will say
a few words later on. This blue line is going to
be Chebyshev polynomials. Clearly, hands down, what wins in this exercise
is Chebyshev polynomials. The problem of
Chebyshev polynomials, I will say more later when
we talk about perturbation, is that they're a little
bit of a pain to code, but if they work and
you are able to do it, then it's going to
beat everything, and it's going to do it along the whole state space. You can see how this is local, as we move far away it
iterates how this is global. It does very well
all about replace. Now, I have a bunch of
slides where I develop all the arguments I just went
over in the general case. I'm not going to go
over them because it's very heavy notation. I only want to mention
a couple of things. The first one is when
you write now x2 is going to be the exogenous
stochastic processes and it's going to be maybe you have
10 stochastic processes. I'm going to extract all of
them in that representation. You only need one
perturbation parameter. It's just a parameter
in front of the matrix of variances,
co-variances. Going back, remember that before when I was making the
introduction to this thing, I have that [inaudible], that's why I needed
the [inaudible]. Because I wanted to have
this for several processes. Imagine that I
have productivity. I have also a demand shock. If you were a little bit naive, you would say, "No, I need
two perturbation parameters. I need a perturbation
parameter here, and I need a perturbation
parameter there." Say, "No, you don't need to do that." Just do the following. Stack those in a vector. Draw 0, 0, Lambda, plus, and then you put a perturbation
parameter over there, and then you will
have your metrics of variances, covariances. This is the only perturbation parameter that we need because when you make this equal to 0, you are backing the steady-state in the deterministic case. When this is positive, for example, equal to 1, you are in the stochastic case. Even if you have 27 million
stochastic processes, you only need one
perturbation parameter. Through the matrix of
variance is covariances. Let me see if there's
anything else that I need to talk about. Well, the fact that we
need bounded support for the shocks, I already mentioned
that, and nothing. This is just a bunch of math. This is just the
general case of before. Remember what I was
telling you before about whether or not these higher-order
derivatives exist, whether or not these higher-order
value functions exist, the problems of
numerical instabilities, and the computational cost. I have computed models. Some of these
Neo-Keynesian models, like the type of things
that Larry does, some of my versions
have like 27, 30 state variables, so you need to take
first derivatives with respect to this 30. You will have 30
first derivatives. Imagine that you have 35
endogenous variables, so will be 35 times 30, will be 900 first derivatives, but there will be 900 times
900 second derivatives. Even if you eliminate a lot of them because
they are the same, we're already talking
about 45,000, and then you take
third-order derivatives. Well, you end up having a few million
derivatives very soon. Then you want to be Dynare. We'll just have a
very serious problem handling all these things
after a third-order, that's why they
haven't really have any release that goes
beyond than third-order. If you do it with Mathematica,
that's what we did, we had to expand
quite a bit of time in managing memory in an efficient way
because literally, you will run out of memory. Basically, what we do is we generate the analytic
derivatives from Mathematica, and we flush them
into a Fortran file. One of the great things
about Mathematica is it can generate automatic
Fortran files. Then we will compile
the Fortran file, and nest this into
a particle filter, into a sequential Monte Carlo. We had so many derivatives
and so big expressions that the Intel Fortran Compiler
could not handle it. Juan Rubio, my co-author, call Intel and say, "We
can not compile it." They guy at Intel obviously say, "You are an economist,
you are not that smart. That's the reason you
cannot compile this." Juan comes to my
office and said, "They didn't really
give me any solution." I say, "Well, let me call." I call and the guy said, "You're an economist,
you are not that smart." I say, "Yes, I don't know
if I'm smart or not, but I know I pay $500
for technical support. Unless you want to
hear from my lawyers, you need to take
a look at this." The guy say, "Let me call you half an hour to show you
how stupid you are." One day passes, two days passes, and we get a call from the Director of
Compilers Divisions from Intel telling us everyone has been trying to compile this for two days, we couldn't. The reason is because we had some many big expressions that literally the parser of the compiler could
not handle them. They actually ask us to sign a release form where they could use our code as a test case for future versions
of the compiler. That has been my only
impact in the real world. I don't know if that push TFP on the right
direction or not, but it certainly changed TFP. Anyway, so you are
going to run out of memory if you go to very
higher-order perturbations but again, if you want
to do third-order, this is not going
to be an issue. I don't think it will be a
concern in your real life. Before we wrap up perturbation, I want to talk about
two small things. The first one is something that actually physicists
understand very well, but that most economists I think don't appreciate enough. Erik Eady, is a very
famous numerical analyst. He says, "It is
not the process of linearization that
limits insight, it is the nature of the state that we choose
to linearize about." Basically, the argument
is there may be very simple changes of
variables that will make you get a much
better approximation without having to go to
higher-order perturbations. In fact, we already have
seen one example of this, which is to do perturbation in levels versus doing it in logs. Why logs? Why not
in the exponent of the variable to the
power of 3 minus 4? That's another
exchange of variables. The very interesting
thing is that Kenje had noticed that once you have the solution to the perturbation in levels, is a very simple formula to find the corresponding
coefficients for the perturbation in
transformed variables. Is just a simple application
of the chain rule. Because it has the derivative of the transformation
that you are making. It means you can go
to your binary code, get the filling in
here in levels, and then apply this very
simple formula that you can code in MATLAB
in two minutes, and find approximation in other transformation
of the variables. We did that in a paper that
also came out in JDBC, which I think makes a very nice case of
how well this works. The general transformation
that we searched for, this is for consumption
and capital, was to some power over here, and to some power over here. This will depend on
three parameters. We then raise productivity
to any power because some preliminary exploration suggested that didn't
really matter. If you pick these three
parameters equal to 1, you are back in the
linear representation. As all of them go to zero, you get the log
linear approximation, and things in the middle will be just powers that may work. I'm going to skip the
next couple of slides because you can go to my paper and just
look at the formulas. Is just basically a formula that is applying the chain rule. Again, it's very easy to code, and I think that
the code is online. If you go to my web page, I think that the code is online, so you can either borrow it. This is the particular example. For this particular
parameterization, these are the formulas. As you can see, it's literally
four lines of MATLAB. What we did was the following. We don't know what the
optimal solution is, but we had started with 1, 1, 1, 1, and we did a little bit
of an exploration around 1 to see if we could reduce
the Euler equation error. The great thing about what
I'm just going to tell you is that even if you don't get
to the global optimum, is okay because you're just
trying to improve accuracy. The best possible transformation
ever doesn't have any particular usefulness
beyond just being the best but if you're something
that is 0.99 as good, you are probably happy as well. By the way, instead of searching for the Euler equation error, since the Euler equation
error is a function, what we do is we integrate
it over to give us an idea of how big is this Euler equation
error on average. There are different ways
to do that integration, but let me not get into detail, is not very interesting. We found that values for Gamma and the first two
parameters are very close to 1. Work perfectly fine, so we may as well just do one. The only one that
seems to matter is Mu, which is the
parameter over here. You can perfectly do the
expansion linear for capital, and have a transformation of consumption. This
is in practice. Really what you are
doing is making the utility function
much more linear. Because instead of searching for a solution and consumption, you are searching for a solution in a function of consumption. Look at the Euler
equation error. It went from 0.085 on
average, to 0.027. This is still a linear model. Is just that instead of
doing it in consumption, we are doing it in consumption
to the power of Mu but in principle, you can
even apply the Kalman filter. In the paper, we
applied estimation. We estimate the model
with the Kalman filter, except that c is the finner
c to the power of Mu. You can see over here
the Euler equation error in levers, and with the optimal
change I just show you. This is by the way, exactly
the same parameterization that I show you before. How we go from minus
4.53 to ranges of 8, 7. Unfortunately, people don't seem to have picked
up on this idea, but to me it's something that is very interesting to
think more about. That simple transformation of
your variables may get you a long way towards improving the accuracy without
having to do much work. The second topic I want to briefly mention is that today we have emphasized that we would perturb in the
equilibrium conditions like the Euler equation, the resource constraint, etc., but it may be the
case that you may want to perturb the
value function directly. Why you may want to do that? Perhaps, is because for some reason the economics of the problem leaves you more naturally to the value function, not to the Euler equation. That will help you
to gain insight. There are many
situations, for example, where especially
things where you have discrete choices or discrete
choices, the best example. Imagine that you can either participate in the labor market, are not participate
in the labor market. These are discrete
choice is yes or no. The earlier equation, you cannot really write another equation, you're not going to
take derivatives. However, under some
regularity conditions, the value function is still
going to be continuous and differentiable because there is going to be a point that, whether or not you participate
in the labor market depends on the
probability of finding a job in the next period. Right at the point
where you jump, the derivative of the payoff
from one direction and the derivative of
the payoff from the other direction
need to be the same. Actually, the value function is going to smooth
non-differential one. You cannot solve perturbation on these labor participation
problems on the Euler equation because you're going to be on
radiolaria question, but you can do it on
the value function. Other situations
where this can be the case is where
it's very difficult, for example, to bribe
equilibrium conditions. Many times when you
write, for example, models with this type of non-standard preferences that I was mentioning
earlier this morning, what is going to happen is
that you're going to have terms that pop out in
the Euler equations. Things like if you have regret, that was losses and
things like that, that you don't really know
what to do with them, how to evaluate them, but they don't show up on
the value function. They are much easier to
handle in the value function, to evaluate welfare. Something I always do
in nearly all my code, I always put the value function
as one extra equation in the equilibrium
conditions because it gives me an approximation
to the welfare. This works very well as an initial guess for the
value function iteration. Imagine that for
whatever the reason you want to do value
function iteration, all fashion dynamic programming, we know that dynamic programming
works very well if you feed in a very good initial
guess, you can say, look, even if a third order
perturbation of the value function
may suffer from some things like maybe
it's not fully concave, but it should be,
doesn't matter. Let me compute it. I put it as an initial guess for my value function iteration,
it works beautifully. Working on the value function, perturbing the value of
function integration is not very difficult. Basically, the only thing
you need to do, again, is taking the value function, rewrite it in this way with
this perturbation parameter. This will be the
basic situation, this will be with a perturbation parameter and you will do exactly the same that we
have been doing before. We will take derivatives
of the value function. We will have syncing
the value of function, we are going to have consumption inside the
evaluation of the terms. We're also going to have
derivatives of the consumption, and that's going to
be all the set of equations we need to have to evaluate or approximations to the value function
and the consumption. This is just going to be the asymptotic expansion
of the value function. By certainty, the
good thing about this is that by
certainty equivalents, a lot of the terms are
going to disappear. For example, this term
is going to disappear, but also all the terms
like 1,3 and 2,3, this is with respect to the
three variables 1,2,1,3. When you eliminate all the terms that disappeared because
of certainty equivalents, you have the following:
You have that the value function is equal to the value function evaluated at the steady state
plus the derivative, the first derivative capital, second derivative productivity, second derivative
respect to capital, capital, square root of capital, second derivative z with
respect to C a square. The cross term and the constant, the constant that
shows up because of the perturbation parameter. Look at how beautiful
is the following, imagine that we happen to
be in the steady-state. We are moving in our
ergodic distribution, minding our own business and
in this particular period, we happen to hit
the steady-state. This term disappears, this term disappears because in the steady-state CT is zero. This term disappears, this term disappears, this term disappears,
but this one does not. What is that? I think I
have a slide about this. I may have it later,
but let me do it here. We have that the value
function in steady-state, see a steady state. One is equal, approximation. You see what we have over here. This is the value of function in the steady-state,
but in addition, there is no shock, is very different to be in
the steady-state because it happens to be the
case that we are going to be under
steady-state today. But who knows what will be tomorrow can do when
the steady state in the deterministic case
and then you know that tomorrow you are still going
to be in the steady-state. This is what we are
computing there. This is not only, I
mean, the steady-state, I'm always going to be
in the steady-state. I'm in the steady state today, but who knows what I'm
going to be tomorrow? Because I have
shocks in the model. What is this term over here? This is the correction to the value function in use by the fact that you
have aggregate fluctuations. Is that correction up to
fair or that actually. You see how beautiful when I was talking about gaining
economic insight. You have the value function at the steady-state
happens to be the deterministic
value function at the steady-state plus this term that depends on
your risk aversion. Once you have computed
the value function, this term from dinar comes
directly from dinar, you express it in terms of consumption
units and that's the welfare cost of business cycle fluctuations
in your economy. This is actually quite cute. This is only for those
of you who remember linear quadratic
approximations because a linear quadratic
approximations when you evaluate the welfare, we don't have terms like this, so you need to correct
it in complicated ways, but that's a different thing. Perturbation is
actually great to do all these welfare comparisons. I don't want to say much
more about perturbation. This is just how to do all
this in detail, I know. This is just an example of, I think I took as an example of the value function I will
get for this perturbation and the consumption
that I will get with this perturbation of
the value function for some particular
parameter values. Basically what I tried
to argue over there that this is a very
good approximation, is a great guest for value function
iteration and is also, and this is interesting, a model that delivers much smaller welfare costs of the business cycle that
the arena Lucas computation. For those of you who do not remember Lucas in his models of business cycles little book has this very powerful
arguments and look, you take the standard utility
function for a US economy, you plug-in our
consumption process that is similar to the consumption process
in the US economy, the welfare cost of
that is nearly nothing. It happens to be the case
that in production economies, the welfare costs
is even smaller because you can use capital. In the basic Lucas
model consumption is given to you so you can
not do anything about it, but in the production,
economic consumption is an endogenous decision. In general, you
tend to have even a smaller welfare costs. In fact, if you have leisure, they are parameterizations
for which the welfare costs of the
business cycle is negative, that is, you prefer to have shocks than not to harm them. Which maybe an argument
that the model is crazy, but it's something that
the model delivers. Just to wrap up,
perturbation is very simple, it's notationally challenging, has a lot of notation, but once you really think
about it carefully, there is really
not much going on, it's just setting up the equilibrium conditions
of the model, taking a bunch of derivatives, solving for the
unknown coefficients, and using those to build
Taylor expansions of your decision rules of your value functions,
whatever you need it. Dinar and do its
second, third order. If you want to have the analytic expressions for whatever reason like
estimating the model, then you probably want to use something like Mathematica. 