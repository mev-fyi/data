um the uh my contribution is going to be to tell you guys a little bit about what big data is and give you a foundation of the types of tools people use to analyze information of this type uh I know how to time this by training I'm an engineer um statistician in my background and to me part of the strength of that term is the fact that it's not very well defined that's part of the reason that's very popular these days but to me Big Data means um a couple of things the engineer in me knows that it means what Matt's going to talk about briefly which is Big Data is data that's so big you can't fit it on your computer you can't just pull it into stata and start inverting matrices and fitting lines through things you have to do multiple little analyzes on slices of your data thinking about splitting up your sample thinking about splitting up your dimension think about projecting into different dimensions things like that so from an engineering perspective big means distributed and that's a lot of the stuff that I do but we're not going to start with that today we're going to talk with big which is just High dimensional in the sense that it's tough to measure the types of models that we like to use in statistics and econometrics so big data for me today for the initial introduction is going to be basically data mining how do we fit patterns in high dimensions and how do we do it well so that we're not doing bad data mining but that we're doing good data mine the outline for my entire morning so I have like three talks this morning you guys are going to get very tired of me um the the first uh sort of thing that we're gonna go through is we're going to talk about just generally overview what is good data mining okay so I'm going to Define for you the goals of my three lectures today which is a narrow piece of big data but like I said it's a foundation for everything else you're going to do and what I think that is is Discovery without overfit and I'll tell you precisely what that means in a second and for most of today Discovery and model selection and all of these kind of high fluent terms mean variable selection the thing that everybody here knows how to do which is setting betas to zero or setting beta is not equal to zero so there's three main ingredients to this this is how I'm going to break it down the first one is how do we evaluate what is a good model and what is a bad model OKAY moreover even before we evaluate it what are the metrics the the sort of yardsticks upon which we want to evaluate these models because we want to measure whether they're good or bad we're talking about things called false Discovery proportion talk about out of sample deviance out of sample predictive performance we're going to talk about model probabilities now these are idealized things and we're going to talk about how in the real world faced with data we estimate how well our model is doing upon each of these yard sales and then finally we're going to talk about well how do we come up with sets of models that we can compare upon our estimated measure of these model metrics up here and I really don't know how long these things are going to take so what we're going to do depending upon how much we have in terms of questions at the end of each section uh if there's time and depending upon how much time we have we'll go into some discussion of factor models principal components regression some of these other techniques that that come to play the I'm probably messing with the videographer by moving over here but the um the way I want it to start out is with an example uh there's a lot of vague terms thrown around like big data like Discovery and like overfit and here's a concrete example we're going to depart from it a little bit at the beginning and then we'll get back to it at the end about what I mean by a high dimensional data mining example so here's some comscore web data I think pretty much everybody here is familiar with what comscore does if you're not what they do is they have a panel of households every year about a hundred thousand households I think it's up to now they put a little black box on top of your computer and they track all the websites that you go to and all the things that you purchase online um so you have very very detailed information about browsing history and purchasing Behavior it's about a hundred thousand households about 60 000 of those there's any money spent so these are people that were actually active in online consumption this is back in 2006 this is an older data set what I've done is uh for an example we'll look through a little bit today I've extracted info for about the 8 000 websites that were visited by at least five percent of people okay so you have 60 000 households that bought anything 8 000 of these uh or sorry sixty thousand households that bought anything and the dimension kind of of the space that you're looking at the the covariate space the websites they might have visited is about eight thousand so a why would we care about data like this okay well as the kind of the engineer or somebody who works at a business school I might just want to predict consumption from browser history right I want might want to know given you know the cookies that are associated with your website when you land on my website which AD should I show you okay this is most of what goes into digital marketing right these ad firms they're saying that they have targeted strategies Etc they're doing things like that sorry I'll stop moving on you um the uh uh so that's the kind of the Baseline I think everybody gets that okay as econometricians as economists that might be a little bit of a harder sell Economist I've learned in my few years at the business school are not necessarily so worried about prediction okay you're more worried about figuring out parameters and structural models that explain scientifically the way the world works okay so why would we care about something like this this big high dimensional prediction problem well you you might have for example some study where you get to observe people and some behavior of those people as a reaction to some treatment for example we see whether or not they click on an online advertisement okay the treatment there would be the fact that we've shown them an advertisement the effect would be the fact that they've clicked on the advertisement or maybe even purchased something that's probably what you're really interested in and the modeling question would be one of the efficacy of advertisement which is hey what's the causal effect of the fact that I showed these people an advertisement how did that change their consumption Behavior well in anything like that you're going to want to control on a lot of stuff but one thing you're definitely going to want to control on is kind of how much do these people spend in general what's their aggregate level of spending okay is this like Mr Moneybags clicking on my ad or is this someone who never never buys anything online okay well you're typically not going to see that so what you could do is you could take the map from browser history to what that says about how much you spend online and use those variables use that map to control for the information the confounding variable that you don't have which is the amount of money that people spend online okay so in this setting you want to solve for an effect where you have a treatment and an effect and this is what Chris is really going to be talking about tomorrow okay and prediction of each of those two things is key to getting the job done the potential regressions that we could have here we could have logistic regression I'll get into exactly what all these mean in a second with equations and such the probability that people spend at all given X and there's a little one of the little causal things I've done in here is looking at hey does Broadband affect whether or not their internet speed is fast does that affect whether they spend anything online or just given that they have spent some money what's the probability that they've they've they've uh what's the distribution for how much money that they've spent okay um X here okay to get back into big picture example of what we're looking at x here like I said is just websites okay in raw form it's just counts for the fact that you visit a website so X1 is I've gone to google.com I don't know seven times X2 is I went to bing.com four times okay um in particular beyond that what I've done is I've changed the X to proportion of time that you spend online so say one percent on on Google one percent on Bing something like that okay but beyond that there's much much more that you could do here and when I'm faced with things like this we often do this we follow the websites and we would look at hey what are the texts what are the images on these websites okay maybe two domain names look exactly the same but we could go out there and we could grab the text the images on these websites and start to regress onto that information which would give us a richer picture of this person's browsing history or if you're someone like Facebook or if you're someone who pays for the information from nip you can go in there and you could grab all of this person's uh Facebook's post whatever is public all their friends Facebook's posts okay so there's a lot lot more that you can do and this afternoon Matt and Jesse are going to talk a little bit about text mining which obviously plays a role in almost all of this big data stuff just because that's the way the data is coming at us so that's kind of the motivating example that I want you guys to have in your mind when you're trying to consider hey is the stuff that he's telling me going to work or is it is it not a very good idea now what is data mining okay DM data mining it's discovering patterns in high dimensional data and that's going to be the narrow piece of big data that we're going to focus on today what do I mean by high dimensional this is a bit of a vague term I'll try and make everything precise data that's usually big in both the number of observations and in the number of variables okay so number of observations statistician always n number of variables always P okay that's big data when both those are big but really your life gets tough and it becomes what I would refer to as high dimensional whenever p is anywhere close to N okay so whenever the number of variables you have is anywhere close to some significant portion of the number of observations that you have in scientific applications right what everybody here wants to do social science applications we want to summarize the high dimensional data in order to relate it to some sort of structural model of Interest right so I want to say hey here's my big high dimensional web browsing history I need to boil this down to information about confounding variables that affect the regression I'm interested in okay that's kind of a structural model projection there we also okay those of us that work in kind of reduce form world we also just sometimes want to predict we want to build machines that tell you what book you want to buy or what movie you want to rent or what the Market's going to do Etc okay so these are the two big goals and we're going to look at both of these goals today because as I've mentioned they're very closely linked the goals overlap and they have one overarching aim and that is dimension reduction okay so what do I mean by Dimension reduction okay well first we have to think about a game these two slightly separate but very closely related goals those of prediction and inference okay in some sense and this is going to be what I use to justify for the rest of the day just talking about prediction in some sense it's all about prediction okay in a predictive model where we want to forecast like I said just hey what is this person going to buy I'm going to use this to say you should buy this book over here you like this book you should buy this movie whatever we're just trying to predict y from X where both are drawn from some joint distribution so things are not changing over time we're not making policy decisions that change the relationship between X and Y or the joint distribution between X and Y we just have this aggregate world that's moving through and we say hey if we capture enough signal from that aggregate world that uncertain aggregate world then we can make a prediction about what's going to happen next again that's probably not what everybody here is looking at you're looking at causal models and you want to forecast why given some gain axes some covariates out there but one of them special and you call it D and that thing is special because it's a treatment and you want to understand causally what that treatment means okay the problem is the same it's still one of prediction it's just now you want to predict for y given D and X when D changes but X is held constant so they're no longer just moving randomly together D is something you get to move maybe you're a central banker and it's the interest rate or maybe it's the advertising thing and you're the person that gets to show the ad in either case Dimension reduction here the key goal that I'm talking about is either in the direction of Y okay so Dimension reduction that's relevant for y or Dimension reduction that's relevant for both D and four uh and for y okay we will learn to walk before we can run which is what we why we're going to do this prediction stuff first we're going to learn about predicting y for Max or D for Max whatever you can consider it to be and then tomorrow the way this is going to split up is Victor and Chris are going to tell you a lot about hey if you have a good model for predicting treatment for Max and if you have a good model for predicting uh response from X how do you combine that information and to do in order to do good causal modeling okay so today we're going to learn to walk tomorrow you're going to learn how to run here's an example of what I mean by Dimension reduction and I want to you know maybe give you guys some motivation by telling you that economists already do very good Data Mining and very good dementia reduction I know that in economics data mining is a bit of a pejorative term and has a negative connotation um that's because often you guys have seen bad data mining it turns out that economists also do good Data Mining and here's an example of it so this is a this is bad dynamite and this would just be a fancy plot these are these are stock returns um for I don't know I think it's about 50 30 stocks um 1992 to 1996 the S P 500 is in bold okay this is like when you look on TV you see an ad for Ameritrade or E-Trade or one of those sites they have a dashboard up and the guy's got the dashboard it's got a plot like this on it and it shows the stocks moving you say off if only I had that dashboard I'd be rich um but but really you look at this and you're not getting that much information out of it it okay the sort of good data mining good Dimension reduction version of this one of the good Dimension reduction version of this is a factor model OKAY in particular the simplest of those is going to be your cap M your Capital asset pricing model what we do here is we just regress these returns onto the market we're regressing the dark line onto all of the colorful lines okay and what we get out of that is a relationship between the individual stocks and the market okay we get a two-dimensional key that tells us a ton of information about how these different companies move how these equities move um and and what we can do to predict their movements in the future because it's going to be easier to predict the aggregate Market than it is going to be to predict a single stock so if we reduce dimension in the direction of something that's easier to predict well then it becomes easier to predict the other thing as well it's also so it's good predictive data mine okay so these guys were doing good predictive data mining okay you guys all know the history of this stuff it comes from an economic story as well right you have the efficient market story that gets laid on top and that's how you interpret these factors okay and in that sense because it comes from this falsifiable testable sort of you know story this narrative around the things that you're measuring the things that you're reducing Dimension towards it's also good structural data mind okay so this is the type of of of work that we would like to replicate in higher Dimensions uh perhaps with less of a story to start with foreign so data mining which for me today is going to be big data is the discovery of patterns in high dimension okay like I mentioned a couple slides ago data mining is a bad word in in economics I didn't know that when I joined the business school and I signed up to teach a course on Data Mining and I learned that uh it's a bit of a dirty word okay um that's because when you talk about discovery of patterns in high dimensions that's easy to do right I can look at anything High dimensional and draw a line through some things and then say that there's something there right so that's not data mining okay so I had to kind of to me that was not Data Mining and I had to narrow and I had to say okay well what I do is not data mining I guess it's good data mining okay and and good data mining is is Discovery okay of these patterns but avoiding false Discovery okay so good data mining is on Constant Guard against false discussion recovery okay I don't know if people are familiar with that term if you've heard the term false Discovery before you probably have false Discovery is synonymous with or or will cause I guess is the probably a better way of saying it overfit okay when you have false Discovery or over fit in a data mining model or any statistics model for that matter what you're doing is you're modeling idiosyncratic noise and the resulting predictor whether this is a predictor for treatment those sorts of things or just predictor in general whatever you want to predict with this model your predictor is going to do very poorly on new data in other words when you overfit when you have false Discovery your model is not portable you can't pick it up from the little unique data set that you work with and move it over to a new setting okay and have it work it only works on this narrow little cherry pick setting that you've you fit it to okay so the you the the search for patterns without false Discovery the search for real useful patterns true patterns we might even want to call them it's called Model building now in linear models which I think everybody's familiar with but I got a slide just in case um this is a question of variable selection okay so the idea of model building obviously I can choose a bunch of things my struct my functional form whether I want to use factors you know some other crazy things but for us today we're going to focus on linear models which means we're going to focus on variable selection okay so we're going to talk about things where you have an x times beta in there and as you guys know that's far less limiting than it might initially seem because your X can include some crazy expansion of a bunch of different variables that you're interested in you can have time Trends in there you can have things interacting with each other you can have x squared and X cubed and all this sort of stuff okay so this is less limiting than it might at first seem so here's everything that you need to know about linear models for today as I mentioned I believe everybody here is familiar with linear models but maybe you don't know the lingo the way I use the lingo okay a linear model is always just expected value of y given axis some link on x times beta okay it's always what it is the gaussian linear model the one that you guys fit with least squares like seven days a week is um Y is distributed as normal okay around a mean that's written as a linear function of x x times beta with some variance Sigma Square okay obviously there's departures from this and you can have Sigma squared and all this sort of fun stuff depend upon X but this is the basic linear regression model the binomial or logistic regression model OKAY is the probability that Y is equal to 1 which is of course the expected value of y when Y is a binary variable the probability that Y is equal to 1 is e to the expect e to X beta over top of one plus e to the X beta is just the logistic link function and then the last one um we won't actually talk about this but this is something that comes up a lot in data mining so it's just good to remind yourself that there's more models out there than linear and logistic as a poisson or a log count model okay this guy comes out all the time because often when you're data mining you're modeling counts of stuff number of clicks on a website counts on a accounts or a word things like that and that's just the expected value of y is equal to the variance of Y which is the exponentiated x times beta okay so those are our linear models we're going to talk about the first two of them today and I'm really not going to explain much more about them the way that we estimate beta is commonly through maximum likelihood okay so your M estimator you're maximizing things and the thing that you're maximizing uh is likelihood uh is a statistician as an engineer I much prefer to minimize things so what we do is we multiply it by negative minus 2 and take the log and we minimize the deviance okay so here are those two terms because those are key terms we'll be messing around with those a few a bunch today the likelihood is just the probability of what happened okay given a model so if I have a a coin toss right and let's say my model is that it's a fair coin so it's a 50 50. well then the likelihood when I flip the coin once is one half the likelihood when I flip the coin twice is one half times one-half is 0.25 okay and on and on and on the deviance is minus two times the log likelihood okay so that's that's all there is to it what we're doing is we're making it negative right so it's something that we can minimize and we're taking the log of it so that it lives on an additive scale right so that when you add an observation instead of getting multiplied in it gets added in okay so minus and log theoretically speaking it's also it's actually two times log of the saturated likelihood okay so the likelihood for a model where you have as many parameters as you have observations okay for linear and logistic regression that saturated likelihood is zero so it disappears for poisson regression it's not zero so it pops up but we can ignore it for today so for today you can just think about the deviance as minus two times the log likelihood and if likelihood measures how well our data or sorry how well our model fits the data think of deviance as a distance it's the distance between the model that we have and the data that we've observed fit is summarized by R squareds and there's no kind of clear definition of R squareds in general but here's the one that I'm going to use r squared is 1 minus the deviance of your fitted model divided by the deviance of the null model the model where all of your betas are set to zero for a linear model deviance is just the sum of squares so this is your standard r squared for the logistic regression model it's not the sum of squares it's the binomial deviance but the idea is exactly the same okay r squared is the proportion of deviance explained by Beta Hat by your fitted model so that's what's going on there and a a statement that might come as a little bit of a surprise to you guys is that in data mining the only r squared we really care about that actually matters to us is out of sample R square okay and so now I'm going to explain what out of sample r squared means via an example and uh hopefully this will motivate much of the other stuff that I'm going to be talking about so here's here's a non-economics example I passed this by Jesse and Matt because I said can I use engineering examples um I won't get egged and they said no yeah you can so if you're if you get annoyed just talk to them um so this is about making semiconductor chips it's actually a really common data set that people in machine learning use because it was made public by one of the big uh semiconductor companies uh years ago and so it's been a nice teaching data set that people use um so what it is is uh information on 200 diagnostic signals I'm about a chip while it's being made and to back it up at SEC let's just make sure we all understand what's going on here okay what we're doing is we're making the tiny little processors the little silicon chips that go inside our phone or a computer or whatever okay this process is very very difficult it's one of the things that you know is still kind of a high value added manufacturing process that people out in the bay spend a lot of time trying to do right it's complicated there's little margin for error okay and it's also not something where you can just get in there and look at the chip and say hey it's a good chipper it's a bad chip right so you got to go through some pretty extensive testing to figure out whether one's good or whether it's bad and that testing is expensive and it takes time Etc so what they have is while the chip is being made they have a bunch of kind of feedbacks on hey when I shot this thing or when I put this thing together this is the sort of resistance I got or this is was the reflectivity of this that I got right a bunch of sort of automatic uh Diagnostics that track the process of the chip being made and we have about 200 here and we have this data set which is uh about 1500 chips and 100 of those were bad chips and 1400 of those were good chips and the idea is hey can we figure out a way to narrow these Diagnostics to predict failure of the chip from these Diagnostics so we can come up with some sort of bell and whistle for the engineer the people who are watching the chip manufacturing process to look at and say hey this looks like a bad chip it's going to be worth our time to go and test it out and actually look at it in detail okay um so the model that you would use here is a logistic regression model the probability that my chip fails is some function of X and in particular I'm going to use the logit link on x times beta okay so logit regression here that's what we're doing something I'll mention because it comes up later these x i j inputs are actually orthogonal from each other or independent they're they're in Sample independent from each other and that's because they're principal components from a larger set if you know what that means you'll know what that means if you don't we'll cover it later on okay but these X's are independent signals about the chip so let's do an experiment to explain and illustrate what I mean by in Sample versus out of sample R Squared so the in Sample r squared in other words the thing that when we fit this model that I had on the previous page by maximum likelihood or by many minimum deviants the thing that that uh beta hat was fit to maximize it's about 56 percent for the full regression model okay so if I throw in all 200 signals okay and my 1500 observations I get about 56 r squared now go back here I have only 100 of 1500 failures right which means that for 200 signals okay I have one observation of a failure for every two signals okay so it's not just that n is small here it's that the signal amongst n that we're trying to measure is very very rare okay or relatively rare so this model we might suspect is over fit so since we suspect it's over fit we might try and fix a more limited model and an ad hoc thing that you could do is just take the 25 signals that are most highly correlated absolute correlation with the response of Interest okay I can look at just you know rank correlations because these X's are independent okay so how correlated it is it's a measure of how much it's going to influence the model fit so I did that I took the top 25 signals of highest absolute correlation and uh the r squared for this model was 0.24 okay so it's less than half okay 24 still pretty good 25 signals 24 percent so how do we evaluate which is the correct model okay or how do we test how these models would do out of sample here's the following experiment what we're going to do is we're going to split the data into 10 random subsets folds is what we call them in the business I'll describe why that is later and 10 times what we do is we take one of those subsets out okay and we fit the model on the left out ninety percent of data and then we predict on the left out subset okay we're going to do that 10 times in a row and what we're going to record is the r squared for the model fit on the training data on the left out subset and recall r squared here for logistic regression it's going to be 1 minus the deviance of the fitted model over top of the deviance of the null model the null model here is just Y Bar these out of sample or oos are squared gives us a sense of how well each model can predict data that it has not already seen and here's what you get here is our out of sample R Squared and I like this example because it it uh forces you to realize that r squared does not need to live between 0 and 1. okay what we have here are it's actually it's it's the full model here does so badly it becomes hard to see what the cut model is actually doing so I'll tell you what the cut model has the cut model is concentrated around 10 or 12 percent r squared okay so it's tightly around there the cut model is explaining about 10 percent of the out of sample variation okay so that's that now the full model what is this box over here this is a box plot of these out of sample R squares there's 10 of them what is the full model doing well it's got negative r squared how do you get a negative r squared let's go back to the equation well that's going to be if the deviance for your fitted model is greater than the deviance for your null model in other words if we're doing worse than Y Bar okay so the variability of the residuals if you want to think about a linear model the variability of the residuals for our fit okay is massively bigger like 50 times bigger or no 10 to 15 times bigger then the variability of the residuals when you just use Y Bar as a printer okay to the engineers this would mean hey okay instead of using all these 200 signals just throw out every 15th chip just toss it okay regardless doesn't matter what it looks like just toss it that will be better quality control than using all 200 signals okay um this is a little bit of an extreme example which is why I like it but negative R squareds are actually super common okay they're very very common you should go back and just so that you're the first person that catches it look at some of the complicated models you fit and see how they do out of sample and maybe don't be super surprised if you do worse than Y Bar okay it's not something to be embarrassed about it happens um so the cut model like I said has an out of sample R Squared of about 12 percent which is about half its in Sample r squared so even for that simpler model which I don't think is super over fit although we'll see later on maybe it is um out of sample R Squared is lower than in Sample r squared so how do we avoid false Discovery how do we avoid overfit okay so I've told you that game now the game is we're going to try and sorry we're going to try and discover patterns and we're going to try and do it in such a way that we don't get those big negative r squared so that we don't get over fit the standard tool that everybody here knows for avoiding over fit is called hypothesis testing okay I probably don't need to tell people about it chances are there's many people in this room who know more about hypothesis testing than I do but I'm going to go through a couple examples from the beginning just so we can fit it into the framework of what we're doing okay so hypothesis testing let's recall how it works it is an inherently one at a time procedure which gives you a bit of foreshadowing as to what we're going to need to correct for um and what you get is the p-value and a p-value is the probability of seeing a test statistic farther into the distribution tail than what you observe okay the null distribution tail okay so something that looks more rare under the null hypothesis than what you've calculated and the testing procedure this one at a time hypothesis testing procedure is you choose a cut off this cutoff is always called Alpha for your p-value which is called P confusingly given Dimension and then you conclude significance in other words variable Association some sort of pattern a beta not equal to zero for any P that is less than this cutoff Alpha okay that's how hypothesis testing works um the reason that this works the way that this is Justified is if you follow this rule your risk of a false positive the risk of concluding that there is variable Association when there was not the risk of concluding that the null is false when the null is actually true is less than Alpha okay so Alpha is like an upper bound on your false positive probability so um in practice what you do while and practice what everybody does is they just choose Alpha equal 0.05 but theoretically in practice what you should be doing is having some sort of acceptable risk of a false discovery that you through I don't know some magic have have magic cost function of dreamed up and then you're going to only conclude say for example in regression that a beta is not equal to zero uh if its p-value is smaller than this acceptable risk of a false positive and that's the mechanics of it the example that I'm going to run through to to talk about how these Mechanics Work in high Dimensions we're going to look at some contingency tables okay so the idea of a contingency table um I probably don't need this is the first thing everybody learns in stats but we'll we'll go through it very quickly the idea and Analysis of a contingency table is we want to know if the factors on the rows and the columns are correlated with each other so here's an example from the norc uh it's an old one 1998 is belief in an afterlife gender dependent okay so these guys ask everything about all sorts of opinions and then we want to know hey does this matter here so this tabulates the cross-classification if the factors are independent which is going to be our null hypothesis then any level of one should have the same probability for each level of the other Factor okay so in other words the cell counts across Dimensions should be independent from each other how does this work as a test statistic well you use that assumption and then you say given that assumption here's something that I know the distribution for given that assumption okay well consider a table like this here's how the test statistic Works what you do is you look at the difference between the actual cell counts okay so my uh 509 females who believe in an afterlife actual cell count minus expected cell counts tell you what that is in a second squared divided by the expected cell count you sum that all up and you know the Distribution on that thing okay expected sell count here is the row total times the column total divided by n you could just think of this as the the row proportion female times the column proportion belief in afterlife divided by total so it's the expected count if those two factors were independent um this thing if they are independent has something called a chi-squared distribution which everybody knows and loves and it has a certain number of degrees of freedom okay in this case it has one degree of freedom this is the basic formula stata I think does a correction of some sort so does Matlab whatever but this is the standard formula so here's how it looks for our table over here I've added up all of those things this is the exercise the Z value that we get back the chi-squared value that we get back is 0.8 we look at the distribution of null hypothesis Z values what we would expect to get if the factors were actually independent and you see that we have a a that that our Z the blue line up there Falls right in the fat bit so it doesn't look that rare and Alpha equal 0.05 cut off is out on the other end there the 95th quantile so what we would say is hey there's no evidence of of association between gender and afterlife believes hey that's the way hypothesis testing works now the problems okay the problems I think everybody here is probably fairly familiar with um the problem is that this is Alpha for a single test okay if you repeat many tests then what happened is Alpha times 100 percent of the null test should erroneously pop up as significant so if I test 20 times I should expect on average one of those things that comes up as uh uh one thing to come up every 20 tests just randomly spuriously by chance okay this is something that people are familiar with often they haven't internalized how big a deal that can be when you're working with rare signals okay and so I'll describe what I mean by Rare signal here let's imagine actually it's not even that rare but let's imagine that we have a hundred regression coefficients and five out of 100 of them actually matter okay so I I am all seeing and I know that five of my 100 coefficients actually matter and 95 of them are are junk okay and let's be super generous to me the statistician let's say that I find all five so for all of those five my P values are super tiny and I have my cut off and I reject and I I've managed to nail the five signals okay so these are like five strong signals I'm assuming now of the other 95 signals out there okay five percent of those are going to pop up as significant if I test at the alpha equal 0.05 level okay which means that spuriously about 4.75 or let's just say five of those junk covariates are going to be included in the model if I do a hypothesis testing type thing which means that in the total what I'm going to have is five coefficients that are true okay five coefficients that are useful that actually matter okay out of a total of five true plus five null coefficients that I've included in my model to put this another way half of the stuff that I'm putting in my regression model is pure junk okay and why is junk bad well it's going to make our model over fit and it's going to make our model less portable and it's going to give us negative R squareds right because all that junk does is it adds to the variability of our predictions that junk is what gives you the negative out of sample r squared okay so this happened because I said hey only five of my 100 coefficients are signif are actually valuable are actually real okay that's how I got a high false Discovery proportion here that's what that's called now five percent of coefficients actually mattering okay that's that's fantastic in many data mining settings you think about words right so you have a million words something like that you want to find the words that are actually correlated with some rare market movement or something like that okay not going to be five percent it's not going to be one percent it's going to be one one thousandth of one percent or something like that you're gonna find a few that really pick it up rare event type stuff okay web browsing click-through rates most ads most things that people click on don't matter if you talk to anybody that works in digital advertising they'll say it it's very much a needle in a haystack setting so anytime you have a needle and a haystack that you're looking for okay you run into this problem where your false Discovery proportions can get very very high okay so the false Discovery proportion here it is it's the number of false positives divided by the number of tests that you've called significant okay and this is what we're going to use and actually what I hope everybody here uses from now on whenever they talk about p-values okay the moment you have more than one p-value you should be talking about this rather than Alpha because you're not making decisions in aggregate or sorry decisions independently whenever you're looking at a group of p-values you're making an aggregate decision okay so it's no cost to you conceptually this should be the way that you're evaluating your aggregate decision it's not a data mining thing that's just good stats okay but it's especially important when your dimension when the number of p-values you're looking at is quite big and when you're in this rare signal setting okay so your false Discovery proportion is the number of false positives divided by the number of things that you call significant it's a property of our fitted model okay it's not something we know right if we knew it well then we wouldn't have to worry about estimation because we would just know what is true and what is false okay but just like Alpha okay we can derive its expectation under some conditions and its expectation is called the false Discovery rate okay and we can derive its expectation we can use algorithms to make sure that the false Discovery rate is below some upper bound and that's called false Discovery rate control okay um when you want to understand what this is like I said just think of it as the multivariate analog of alpha it's what you should be thinking about whenever you're talking about multiple p-values so false Discovery rate control this is actually a pretty old algorithm now it's very very common in the biostatistics literature and the medical literature um it hasn't really made a Big Splash in economics yet but maybe you guys will change that but it's kind of a Workhorse way to evaluate many many p-values okay and here's the way it works I'll just describe the algorithm then we'll talk about why it works you rank your NP values Capital NP values here from smallest to largest okay so whenever you put a little bracket around the subscript that means it's now a rank statistic so P1 is the smallest P2 is the next smallest all the way up to PN is your biggest p-value you choose a cut off Q value so it's no longer an alpha value it's a q value where you want the false Discovery rate to be less than or equal to Q so let's say 10 percent right so I want my false Discovery rate my expected false Discovery proportion should be less than 10 percent you set the p-value cut off then as a function of that Q as the maximum p-value such that the p-value is less than Q times its rank over the total number of p-values okay I'll explain why that works in a second but the magic is that if you do this if you set your p-value cut off in this way the false Discovery rate is less than or equal to Q assuming a bunch of stuff okay um so that's the magic of it that's why it's a very appealing algorithm that's why it's used all the time the weakness of it is the big assumption benjamini and hoshberg they assume Independence between tests okay which we know is probably not true right in fact it's never true they assume Independence between tests this condition can be weakened because this is such a nice popular heavily used algorithm a lot of effort has gone into weakening this assumption okay so getting away with false Discovery rates when you don't have Independence um there's uh fairly old result that says if your P values are only positively correlated under some slightly a couple other weak assumptions and then this result still holds okay there's also a literature on replacing the independence assumptions with other modeling Frameworks so Brad Ephron has kind of an empirical Bayes interpretation of these things and Jonathan's story has a series of of papers on full base interpretations of these things okay and those have been very successful in coming up with hey settings where I don't have Independence but this thing still works why is that okay so you can think of these guys that's answering those with that question but regardless Independence between tests you might not have been thinking about it previously this is an issue whenever you look at marginal P values okay so unless you're like Chris over there and you're getting you know joint P values on large High dimensional sets Okay most of us are looking at pointwise P values and this Independence thing was lurking everywhere there whenever we did that whether we realized it or not but here it's explicit so here's the motivation for false Discovery rate control okay so remember the algorithm is we just pick the P such that the Mac it's the maximum rank p-value less than or equal to Q times its rank over top of the number of p-values where does this come from one p values are uniformly distributed under the null okay I think everybody here should understand that and have internalized that but that's a big point right that's like the point of testing you need to really understand that okay and why is that well I have the picture of justification up here p-value is just under a null hypothesis okay it's the probability of being greater than or equal to something drawn from that distribution okay sorry greater than yeah greater than or equal to something drawn from that distribution that's what we call in statistics the probability integral transform okay so the probability of being greater than something where that something was drawn from that same distribution is uniform okay that's something that everybody should know whenever they do a hypothesis test um the next step is kind of the line below my my um beautiful curve thing there which is that rank statistics from a uniform have expectation K Over N okay so the ranked statistic function for uniform distribution is a straight line that shouldn't be too hard to convince yourself of if you want to just simulate from a uniform and plot their ranks you'll see pretty quickly okay what that means is if all of the p-values came from the null distribution we would expect that they should line up when ranked on this kind of straight line okay so what false Discovery rate does geometrically is it says how far below this expected straight line do your P values have to drop before I get to call them significant okay that's the geometrically what's going into it how does that actually work okay um here's the proof the proof is eminently simple if you assume Independence uh the people who have tried to relax Independence have put a lot more work into proving this but if you assume Independence it's really really easy um let's just say that R of U is the total uh uh number of P values less than or equal to some U so U is a uniform probability it's just P between zero and one and if we choose a certain P between 0 and 1 or U between zero and one okay then there's going to be a number of P values below that point okay and those are going to be the ones that we call significant another way to rewrite the benjaminian hostberg cut off is just that U star that P star is the maximum U such that U is less than or equal to Q times the number of P values at that point divided by n okay if you want to convince yourself of that just think about what it would be for p okay so if U was p i our Max p i then U would be Pi there and R the number of P values less than that point would be its rank I so it's just p i less than or equal to Q times I over n so what that means is that we have one over the number of P values at any cutoff point is less than or equal to Q divided by n times that cutoff point and our false Discovery rate if we say that uh R little r of U star is the uh the number of null less than that cutoff Point our false Discovery rate is the number of null less than that cutoff Point divided by the total number less than that cutoff point which from my little derivation above there we can write as Q times the number null less than that cutoff Point divided by n times U okay and then r u over U always has expectation and not okay it always has expectation number of null P values uh in your whole sample okay and that's where the independence comes in okay so if you want to see that the number of null P values less than a certain point divided by that point on a uniform okay let's put this in other words how many P values below every point from a null hypothesis divided by that point on the universe form distribution do I expect to be uh do I do I expect to find what is the expectation of that ratio it's always n not the total number of p-values in your uh in your sample okay and to see that you just need to scribble it out but all you have to do is you have to do it for one right and the the expectation of this thing is the probability of a null p-value you do it for two if they're independent it's the probability of one null plus the probability of another null you do it for three you add them all up you get the number null okay so there that's there for you if you want to go and do it yourself oh sorry I forgot the punch line the punch line is when you replace that little ratio with n naught then your false Discovery proportion has expectation Q times the number null divided by the total number n okay the number null can't be less or can't be greater than the total number of P values which means that your false Discovery rate is less than or equal to the uh less than or equal to Q this last Point actually here it's worth mentioning the bound even though it's not usually mentioned this way is actually Q times the number null divided by the total number n okay so as you have fewer null in your sample your false Discovery uh rate is is bounded by a lower and lower number however since you never know the number null you you don't get to leverage that in any way so here's an example of false Discovery rate control this is one of its its uh its settings this is where false Discovery rate control lives and that's in genetics okay so it's very popular it's it's used less and less these days because of the independence assumption but it's still bread and butter method um we're going to look at a gwas example so gwas means genome-wide Association study okay and these guys are pretty common um what we do is kind of given the fact that we have some understanding of what changes in the genome and how that might affect biology but not a great understanding of the mechanisms there's these big data mining exercises that go on which is where you take the entire uh large set of locations on the genome on the DNA or whatever and you look at how expression at those locations changing amongst individuals correlates with phenotypes with characteristics of the individuals whether or not they have uh diabetes is going to be the one that we look at single nucleotide polymorphisms Snips is what everybody calls them are just locations where things change between individuals okay we don't look at the entire genome because most of us share basically everything okay so there's a few locations that actually change between people and that's what people look at when they're doing these types of studies they narrow it down to those few locations and then you ask the question very often at these locations what changes depending upon some other thing that I'm interested in and the thing that we're going to look at changing today is frequency of the minor allele minor and major allele right so you have big a and little a at each location major allele and minor allele and you have one of each and you can either have big a and big a little a and little a or little a and big a or being a a little B okay so here's how that summary works here a common way of aggregating these things is just to count the minor allele frequency that's MAF so you have ever see MAF and snip you guys now know what that means um big a and little a uh um sorry we're counting the number of little A's okay so two major alleles is zero major and minor is one two minor alleles is two and the question that's asked here is which minor allele frequency distributions vary with disease status so in other words if we see a location and this person has a high expression a an extra minor allele here okay well then maybe we should monitor them for diabetes risk or maybe we should monitor them for this sort of uh disease over here that's the way the exercise goes um if we want to answer that question what we come up with is a massive number of contingency tables where you have disease status okay case versus control against counts for this minor allele okay so this gets back to the gender and and belief in the afterlife here's a more scientific version of the exact same thing and so here's what one of these tables looks like this is for good old snip rs6577 Etc and this is actually the most significant uh uh location when you compare to type 2 diabetes so this is a big pseudo-public data set on on genetic information and type 2 diabetes and what we have here let me just I don't think I say how many I have I do in the next one um this person or this sorry this location of the people that had type 2 diabetes 27 of them had a minor allele frequency of two and of the people that did not have type 2 diabetes only one of them had a mildele frequency of two this is actually a nice design experiment as well they controlled for a bunch of other stuff so these are pretty solid kind of results so this is the most significant of the tables they don't all look like that here are the p-values for all of the the contingency tables that we have here the data set has about a thousand individuals one half of them are diabetic okay and that leaves us with a little bit under 8 000 snips um 2700 of the tables have really small cell proportions in other words minor alleles almost never pop up for this snip and we just ignore those okay that's we don't have a theory for what the null hypothesis is when those expected cell counts get too small and so this is kind of a lesson I think I've noticed this in economics literature people using chi-squared stuff where the cell counts are very very small just don't do it null hypothesis is not a good approximation that setting you need other techniques here instead of using other techniques I'm just going to ignore them so so we do that and what that leaves us with is about 5 000 P values okay and remember rare signal we think most genes probably don't cause diabetes or I should be careful with cause probably aren't super correlated with diabetes okay um we have 5 000 of them how are we going to do something here and get a low false Discovery rate Well what we'll do is we'll use that FDR control algorithm and here it is visually okay if you want code to do this I mean the algorithm is eminently simple so you can just code it up yourself in stata there's code to do this in r on my website but it's very easy um what we do is we find the cutoff the maximum p-value the who's whose uh p-value is less than Q times its rank Over N okay this actually looks like a line through the ranked P values with slope Q Over N right through the ranks okay the line I times Q Over N okay is a is a line through I with slope Q Over N so that's what I've plotted here everything that lives below that line is significant okay past the suit point below that line is significant and everything above that line is called insignificance the red here are significant the gray here are insignificant and so this is the way it works you get a plot of your P values like this you look at them in aggregate you choose some cut off okay and then given that cut off you know the false Discovery rate properties of your or of your test okay so actually here we add a ton of action okay so 183 significant P values even at an FDR of of .001 and zero zero zero one that's pretty rare this was a big study randomized control experiment so these things were actually and these were targeted p-values these were places doctors wanted to look for biological reasons okay this is quite rare most often in g-was stuff you don't see anything this nice uh in the social sciences literature Jesse pointed this out to me there's this shabrese at all paper and psych science from this year they looked at a meta study of all of the G was or sorry a g wash meta study of all of the genetic locations that have been found to be correlated with measures of intelligence IQ type stuff okay and they found I think there is 14 that had really become stylized facts or at least accepted and published and big in the literature they looked at 14 these 14 things in a meta study where they got three new data sets and tested for each and those three new data sets Okay of those three new data set tests of those three or of those 14 locations one popped up significant okay so when you control for everything right which means that one and even that means that it came up only one out of three times okay so basically none of the things that were found to be significant were um so that's why you have to be careful whenever you're working in high dimension okay so check out that paper it's a great example false Discovery rate power calculations things like that here we are back to the semiconductors that I talked about okay so remember our semiconductors here's what the p-values look like for those guys um some of them are clustered right down around zero the rest sprawl all of the way out to one the question is which of these come from a null hypothesis and which are significant there's actually something to point out plotting your P values is not a bad idea because it gives you some intuition if I plotted these and there was no Spike down next to zero it would probably be time to break for lunch right so if you plot that and you see something with just a flat uniform distribution of p-values it looks like your P values all came from the null either you have nothing to discover or you shouldn't be using p-values okay in this case we don't have that it looks like we do have a little bit of a mode going up towards one towards zero so it looks like we do have some uh significant uh significant values here how do we choose that cut off well we can do FDR the FDR Q equal 0.1 line gives me 25 values okay because the X inputs are independent these are actually the 25 values that I came up with for the first slide okay so these were the 25 that I used when I cut it down to 25 okay so because the X's are independent the correlation and the p-value is basically the same thing here um there is not really an industry standard on what Cue to use okay the closest thing we have is 0.1 okay but for the same reason that people shrug their shoulders whenever they ask why Alpha equal 0.05 it's the same thing with yq equal 0.1 just seems to be the consensus that people use but there's no there's no real reason so now the problems with false Discovery rate control um a big problem and the problem that we highlighted from the very beginning is dependence okay that's the issue and that's the issue whenever you use marginal P values okay I can't emphasize that enough if all of your X's are super highly correlated with each other you don't have say 10 P values you have one right and you don't know its distribution because they're all correlated with each other okay so so this is an issue whenever you test but here it's explicit um here in these tests FDR is okay because the things that we're evaluating are plausibly independent I use plausibly in kind of a sketchy sense there the Snips were far apart they were chosen to be far apart they chose a location they targeted there the principal components are orthogonal and the semiconductors example however you won't usually have this and in fact we don't have Independence here right clearly genes are related even if I chose them to be far apart and my semiconductor signals my factors are orthogonal in Sample but those are estimated so their sampling distribution is correlated the absolute values are negatively correlated so dependence is always going to be an issue of particular issue for everybody in this room is multi-collinearity when you're doing regression and your X's are correlated with each other what this tends to do is inflate your P values right it can work in other nefarious ways but what it will tend to do is inflate your p-values which means that you end up with instead of a picture that looks like this with the spike down at zero you'll get a picture with a spike up at one because you'll get some really big p values and while what that means is that hey I have collinearity I have two signals that do the exact same thing okay I have two I don't know say I've measured the size of your house in meters and I've measured the size of your house in feet okay well the p-value on those is going to be very very big because we don't know whether one needs to be set to zero and the other given it's mle or the other one set to zero and the other given it's mle okay so if you have two that are highly correlated they both get high P values which means that anything based on marginal P values is going to miss both of them okay now chances are in a rare signal environment the thing you're looking for is correlated with a bunch of other stuff because it's important and you're looking for it right which means that you're probably going to miss the few things that you very much care about uh in multi-collinearity based settings when you're working with p-values even with Independence though even ignoring this Independence assumption which is by far the worst thing that we have to deal with FDR control is often impractical okay uh one of these reasons is that you're regressing I'll start with the second reason p-values only exist for p less than n right I mean there's ways to get model P values when you have more observations and you have dimensions of your model but that's that's pretty tough business okay so in a lot of the settings that we're looking at and that you guys will be looking at you could have P greater than n certainly in genetics they often have like 12 individuals and 8 000 genes which makes you question a little bit what they're doing sometimes I forgot I'm on video um so but uh but so that's one problem okay and for p anywhere near to n okay even if you don't have P greater than n even if you have P like one half n you're working with very few degrees of freedom in the full model okay which gets back to the other point all of these the first bullet there all of these p-values are being calculated conditional and everybody else being in the model conditional and all of the other X's being in the model okay for the reasons I just described with P close to N that's probably a terrible model which means you're basing everything on calculations based upon a probably terrible model nobody would start with the full model in most data mining scenarios so that's the reason that Beyond dependence that p-values are are tough to do work with and why FDR can fail however if you are tied to p-values if you don't have any other choice if you're in a forest and and you know you don't have any data and somebody just gives you a p-value false Discovery rate is the best thing you can do okay so that's testing with p-values um that's a bit of a natural break so just to kind of rehash what we did there um I've brought you up into how do we do testing in high dimensions and I kind of brought it crashing down a little bit actually it started out down because we discussed the Assumption of Independence and how that's going to be tough to satisfy and how that's going to get in the way of false Discovery rate control being a a method that we can use in practice so now we're going to talk about different ways that we can use to evaluate and select models amongst a bunch of high dimensional covariants and that is really the core this is the bread and butter so this next section between now and to lunch is really the core of what goes on day to day and modern data mining this is the the we're going to talk about things like the lasso and penalized estimation and cross-validation and that's going to be the the real meat of what you guys are going to need to know if you want to try and work at this scale so the first thing that we need to let me get rid of this guy the first thing that we're going to talk about are different ways to evaluate models you recall that when I started out I said hey here's some ways some metrics that we're going to use to say whether one model is better than or worse than another model and the one that we started out with was false Discovery okay false proportion proportion of false discoveries Etc that was just one the other metrics that I mentioned were both very closely related to prediction okay so what is the difference between Discovery or hypothesis testing and a prediction or prediction evaluation type of of model building process so hypothesis testing is all about evidence what can we conclude okay often that's not quite the question you're asking so the what can we conclude question it's like your your data is on trial it's like you are on trial right and and all of your conclusions are null until proven significant okay and that's the way you work because you really don't want to say something that's invalid okay now that's a great way when you're talking about one special parameter that you have a stylistic interpretation for that you're going to go start like changing interest rates based upon and like you want to make sure it doesn't plunge the world into recession that's great for that one parameter for everything else out there all of those thousands of other parameters that you're trying to get a pretty good model for okay you don't want to put each of those individuals on trial you just want a good sense of how they work in a predictive model for predicting the things you know to put this another way like I said I'm engineered let's imagine that people designed plain and Automobiles just by hypothesis testing okay the only people here would be people that walked here right none of us would be here from out of town um so so false Discovery rate control is a very valid way to evaluate appropriateness of a model however it's overused because it's not always the key to what you want out of a model and it's often far too cautious okay and in many settings being cautious is not even conservative right so we'll we'll see what that means so false Discovery proportions just one possible model metric an alternative it's predictive performance and this is usually easier to measure okay for one so that makes it nice in high Dimensions two it's going to be more suited to the application at hand often and that's even true in causal inference because let's go back to the very beginning and what Chris and Victor are going to talk about tomorrow when you want to control for many variables and say a treatment effect setting the whole Enterprise works on predicting treatment you know the whether or not somebody had a treatment applied to them from your exes and predicting response whether or not they clicked on an ad say for example from all of the X's doing a good job of predicting those two things and then looking at the I don't know say the residuals or the correlation and the residuals left over between those two predictions and interpreting that causally so all of these causal exercises boil down to really good prediction at very high dimensions and then at the very end a hypothesis testing evidence-based type evaluation for one parameter or two parameters some low dimensional thing that you really want to tell a story about okay so prediction is King so here's my prediction based model building recipe the way that we're going to do things is one find a manageable set of candidate models okay what do I mean by manageable I mean that you can fit all models in your set pretty quickly okay and what does pretty quickly mean well it's how much time you have and that depends upon how much you value your time um so so so so so fast once we have this set of candidate models and we can fit all of them in a reasonable amount of time then we're going to choose amongst these candidates the one with best predictive performance on unseen data okay so one it's actually hard but we'll discuss how it's done so coming up with sets of good candidate models so it can be fit quickly that is the lasso and related methods so we're going to talk about how that's done two seems flat out impossible how am I going to predict how well a model can do or evaluate how well a model will do on predicting that data that it has not seen okay um it's not impossible uh like many things that seem impossible in statistics we're going to estimate and come up with ways to estimate this thing so before we do that though we're going to have to basically decide upon or devise a couple metrics of model performance of predictive ability and then we'll come up with estimators for those things okay so when we're talking about item two here which is evaluating how well our model does on predicting data it has not seen there are two ways of evaluating how well in that scenario the first is the classic crystal ball and I think is old terminology I think Brian popularized the crystal ball terminology what is the error rate for a given model fitting routine okay so we're not talking about a model but a way of estimating a model on data on new observations from the same data generating process as my data okay so we're assuming X and Y are coming from the same distribution that generated my X and Y what is the out of sample error rate of my model on that new data and then the other one is the Bayesian way of thinking about these things what is the posterior probability of a given model those of you that aren't familiar with Bates in like I don't know three sentences your your parameters are random variables you have a prior distribution over the value of those parameters those random variables before you see data after you see data you have a posterior an updated uncertainty distribution for those variables um the point of it is is that it's a theoretical framework for motivating a lot of the ways that we'll talk about model evaluation so we're going to need it a little bit um so the Bayesian asks what is the posterior probability in other words what is the probability that the data came from this model okay these might seem as though they're different but mechanically they're actually very closely linked uh because posteriors are integrated likelihoods a likelihood is the probability of the data okay and integrating over parameter uncertainty means hey How likely or how predictable was this data given any of the possible values I could have estimated for the parameters okay over averaging over all the possible values so they're actually very closely linked for some nice deep reasons but we'll see in practice they also end up being giving the same results Crystal Ball performance so let's start with that Crystal Ball performance is something that we can estimate through cross-validation now we already did that you remember when we started out this was where I convinced you that you need to do uh uh you need to control for false Discovery when we had the out of sample testing for the semiconductor values and on your left out 10 percent of data you got a negative r squared okay that was an out of sample prediction experiment and implicitly in that experiment we were estimating crystal ball type uncertainty right so the uncertainty as to how well is my model fitting procedure going to do in that case maximum likelihood on all of the parameters versus maximum likelihood on 25 chosen parameters and the question was how well do those two possible model fitting procedures do at predicting left out observations okay and that's the way that we'll evaluate and measure and attempt to to estimate out of sample crystal ball predictive uh predictive uncertainty okay so the process of you using these out of sample or oos experiments and it's always oos in quotes because of course you can only you can't actually predict data you have not seen so so it's kind of a stylized little experiment the process of using these experiments in order to choose a model is called cross-validation probably a term that many people have heard before there it is rigorously that's all it is you split your data up into a bunch of little chunks you see how well your candidate models predict when fit on some of the data on the left out data and you choose the model that is best upon some measure at doing that at predicting the left out data that's called cross validation okay um how do we measure performance out of sample it depends upon the problem that you're working with if you are working in a vacuum you don't really know you know you're just trying to get a good model then deviance is probably going to be what you want to evaluate out of sample deviance is what you minimize when you fit the model so presumably deviance is a good measure of distance between model and data and it's a good measure of what you're trying to do in other settings though you might have different out of sample metrics for example people often work with misclassification rates for classification area underneath the ROC curve so if you want to have kind of a the receiver operating characteristic curve so how sensitive versus specific can I make my test air quantiles right so people in finance will cross validate uh high risk events rather than looking at how well it does on average which is what the deviants measures and the last thing I'll say here because it'll come up again when we look you care about both the average and the spread of out of sample error okay so a model who has massively wide so it can do very very well out of sample or can do absolutely terribly out of sample but on average it has it I don't know an r squared of 10 out of sample that is not as good as a model that has an average r squared of about 10 percent and is range is between 11 and 9 now it's not going to be they're the same so often you'll have to make some decision we'll talk about how that's done [Music] uh the lingo here when I talked about those left out samples and cross-validation I call them folds okay so a fold is as follows what you do is you take the pieces of you take your data in advance and you split it up into if you're doing tenfold cross validation 10 pieces okay and you use those pieces those pieces that are fixed in advance as your left out validation sample so you go through like I said already you're going to take leave out that first 10 percent you're going to fit the model on the 90 and you're going to predict on that left out 10 then you put that 10 percent back in and you take out the next 10 percent you fit to this ninety percent of data and you predict on the game that left out ten percent okay this is as opposed to the naive cross validation procedure which would just be 10 times I draw a random sample of size 10 of my data and predict on that as the left out data okay the the difference is is that here we're making sure that each observation is left out exactly once whereas if you were just running this and drawing random samples each time you're not guaranteed that each observation is left out exactly once it's just a way to lower the variability of the method and it's a Monte Carlo method so you inevitably want to lower the variability of this method okay so that's k-fold cross validation K or n depending upon who's talking um one very popular theoretically way of doing cross-validation is enfold or n little and fold or leave one out cross validation it's where the size of your left out sample fold is one so you repeatedly go through the data you take one observation out you fit uh on on N minus one observations and you see how well you predict the left out one then you do it over and over and over again okay this is nice for a couple reasons one it is for a fixed data set non-random okay so you've gotten rid of that Monte Carlo variation right so your result is going to be the same every time um and it's you know the sample size of your left out uh or sorry the sample size of your training samples are as close as possible to your actual sample size which is nice for replicating performance of the model that you're actually going to fit however it takes too long in most settings so you know you have twenty thousand sixty thousand observations this is Impractical so you often you don't do this generally what you do is you choose the biggest K that you have time for there's reasons to use large smaller K but basically just choose the biggest one that you're willing to wait for problems with cross-validation okay so so there's what it is here's what's wrong with it um it's time consuming like I just said estimation is not instant when we're working with data mining models and fitting K times is unfeasible uh infeasible if if even fitting once is expensive and you know if your K gets too small the typical smallest value that people use is k equal five that's the default on most software I don't know if stata does cross validation but if it did it would probably use a default of five um you know and that's kind of the smallest people are willing to go right so you're using 80 of your data for training at each point um getting smaller than that for computational reasons is probably unwise so it's time consuming it can be unstable and this is actually among statisticians I think pretty well understood but amongst practitioners of cross-validation often not well understood cross-validation can have a very high sampling variance okay so it's a high variance technique what do I mean by that imagine cross-validating cross-validation okay so if you had a bunch of data sets okay and you cross-validated on each of those data sets to choose a model the model that you choose via cross validation on each of those data sets can change dramatically okay so in other words if you Jitter your data cross validation can give you very different results even for the leave one out procedure where the cross validation itself is actually uh deterministic even for the leave one out if you imagine a sampling distribution for the leave one out model Choice it can change dramatically across samples so it's unstable in that setting especially when you combine it with unstable models which is something we'll learn about it about 45 minutes it's also hard not to cheat so most of the Cross validation I see people have cheated somehow okay um it's hard not to cheat unless you're doing something like the last or something like that where it's designed for cross validation for example we did cross validation today with those 25 signals how did I choose those 25 variables I looked at my entire data set boom that's where I cheated and I chose the 25 variables that were most highly correlated with the response and then I refit maximum likelihood with just those 25 variables yeah sure I didn't use all the data but it's not surprising that those 25 do well predicting left out stuff because I Cherry Picked them right because I went through all of the data I said hey these are the ones that matter for everything going on here who's to say that if I had for each little training sample chosen the 25 within that training sample that are most correlated with response okay who's to say it would do as well it probably would not have done as well and even cherry picking them we only had 10 percent out of sample R Squared so it makes me a little bit worried about the quality of that model there still despite these weaknesses cross-validation is seen as a pretty objective pretty impartial judge it's used in most data mining applications in some form even when it takes way too long there will be at least one validation set that somebody will say hey I ran it here and I trained it on my left out observations over here that's like one kind of a one-fold cross-validation type exercise um alternatives to cross validation Okay so we've seen cross-validation it makes a ton of sense it's used all the time it does have some weaknesses and the biggest weakness is time okay although Chris will talk about probably theoretical weaknesses as well um there are alternatives to cross-validation there are many of them uh the biggest one the ones that you guys are probably familiar with are information criteria okay uh so let's just talk about what an information criteria is and then I'll fit it into our Paradigm of models and model evaluation so there are many information criteria out there you guys are probably familiar with the AIC Bic there's d-i-c-t-i-c there's lots of them these IC are there are a lot of things there's a few different ways to interpret them I should say first of all okay one way that you can interpret these in these information criteria are as approximations to Bayesian log model probabilities okay so actually minus log knot model probabilities they're on the scale of deviance okay so approximations to posteriors for models they're on the minus log scale so that the lowest IC okay the value with the model with lowest AIC or lowest Bic corresponds to the Val to the model with highest probability highest posterior probability okay and where does that posterior probability come from well you have you have over here uh what is the probability of a model well this is that Bayesian thing that I told you what it was right for those of you who aren't bayesians and aren't steeped in that literature this will seem pretty sketchy what does it mean to talk about the probability of a model given some data what we'll do is we'll break it down what's the probability of the data and the model still a little bit sketchy divided by the probability of the data average over all possible models and well this thing here is obviously going to be pretty hard to get right um so I do what all good bayesians do is just ignore it it's a little proportional over here because this is the same for all possible models so if we want to find the model that makes this thing as big as possible well then we don't need to know what this is because everything's just proportional right you kind of got to make sure it's not infinite but that's better um no you do have to make sure so this thing here just through simple probability rules probability of two things is the probability of one thing given the other thing times the probability of the other thing okay so now the sketchiness is isolated and this thing over here called a prime because this is just our likelihood right likelihood it should be something that people are fairly comfortable with and what I've said is that this posterior construct over here is just proportional to the product of the data likelihood times the data prior and so these model these information criteria that we come up with are approximating this thing or can be interpreted as approximating this thing which means that the differences between them are due to different priors of models okay so that's the way to think about how different information criteria change from each other so that's it back it up a little bit the um remember I said that we're going to try and talk about uh uh ways to evaluate models for prediction and I separated that into metrics of model goodness and estimation of those metrics of model goodness the metric of model goodness here is kind of the subjective Bayesian posterior and the estimation of that metric of model goodness is the information criteria and the difference between different methods comes down to that subjective choice of a prior and the way that we choose it prior they all generally have this property that they put more weight on simple models and they put less weight on complicated models so a priori before you've seen any data due to Occam's razor you're going to say I favor the simple model and I have a lower prior on the complicated model okay from the Bayesian perspective that's an alternative to cross validation and an alternative to Crystal Ball uh cross-validation crystal ball ways of evaluating models and it's useful well if you're kind of a subjective Bayesian I don't have to to justify it in any way whatsoever but for the rest of us it's useful because it's faster because cross validation takes time and these things are asking very very similar questions okay so I see in model priors information Criterion model priors information criteria are distinguished like I said by what prior they use the two most common are akaikes and Bayes information criteria both of these and basically everything out there is proportional to deviance plus some penalty K times the number of parameters in your model and you guys are probably familiar with these you know that K is equal to 2 for AIC and K is equal to log n for the Bic these values here they come from the following approximation to the model probability so you might not have seen this before and some people don't like deriving information criteria this way but I find it the most sensible way to do it remember we had the model probabilities on the previous page over here and I said we ignore the probability of data so the thing we really want to integrate over or the thing we really want to find is the probability on the right hand side what's the probability of data and a model well it's the probability of data and all the parameters in that model integrated over the uncertainty about the parameters in that model that's what you get over here so in a regression context this thing that I was talking about the bottle probability is the integral of this likely good guy over here probability of Y given betas in my model little B here maybe this is like 25 betas or zero or non-zero integrated over the prior metric on Theta so this is the integral we want these models these information criteria come from a LaPlace approximation to that integral okay you don't know what a LaPlace approximation is it's just the second order Taylor series to the function then you integrate it from a statistician's perspective you assume that the posterior looks basically normal you you know you take the the the normal around the mle and you imagine that instead of whatever approximate shape your posterior could take or sorry exact same shape your posterior could tape you fit a normal curve over it and then we know the integrals of normal curves because we have lookup tables for multi library of normals Etc okay so that's where all of these come from they come from this approximation here let's get back to our original Point all the changes between these things because the likelihoods are the same between model information criteria what changes is the measure that we're integrating over what changes is the prior okay and different priors correspond to different information criteria AIC is two unless you have really small n is going to put a lower penalty on complexity than Bic in other words the AIC is going to select more complicated models and its prior works that way if you're curious at the end there there are the actual priors that they correspond to bic is this thing called a unit information prior where the prior is databased it's normal with center-up beta hat and um uh variance the the information Matrix for that the expected information for that aic's prior because it was not derived in this way it's prior doesn't make as much sense but it integrates to this thing over here which is just a p over 2 times log n which removes the log n here's the AIC and the Bic for the semiconductors okay remember we had about 1500 observations 100 signals P was equal to 200 in the full model cut was equal to 25 in the cut model or sorry P was equal to 25 in the cut model both of the model evaluation information criteria here favor the cut model so they both agree with what we saw in out of sample experimentation which was that using 25 variables is way better than using 100 or sorry 200 variables interestingly the Bic value for the full model remember these are on the scale of deviance so e to the minus of these guys are relative model probabilities are proportional to relative model probabilities the Bic of the full model the difference between the Bic of the full model and the Bic the cut model is much larger than the difference between the AIC and the full model and the AIC of the cup Model A game that boils down to AIC favoring more complicated models okay so here information criteria information criteria approximate this Bayesian posterior for a model which is one way of evaluating appropriateness of a model or performance of a model at predicting unseen data which one do you choose it's controversial people argue way too much about it you'll find out which one you prefer generally in different Industries or in different areas of Academia one's been preferred over the other I think in economics often you see Bic more often I talk about micro there in finance you see AIC all the time because those guys are trying to discover uh much weaker signals perhaps I don't know my take is I like the Bic conservative guy I work in very high Dimensions so generally a simpler model is going to be better and it also tends to mirror the out of sample error better which is something that we'll see in a second but you know again that's controversial and you'll find within your field whichever one works better about these two information criteria that we're not on there why would you ever use information criteria um as opposed to cross validation they're useful in conjunction with the each other if they really disagree with each other then you should question what's going on you should probably go in there and make sure that there's not something really weird about what you're doing if they so if they disagree with each other there's an issue the other reason you would want to use an information criteria is if cross-validation just takes too long okay which is going to be pretty often okay especially when you're working through the sort of development phase when you're building up models no you know not the final model that you run you can go away for launch and come back and then put it in econometrica but that all of the models that you've done on process to that okay you might not want to take the time to cross validate everything on Route okay if you need answers fast and information criteria will be what you go with okay so that's ways of evaluating models and how good they are at prediction and we've seen how that works for the semiconductors and for the uh and CV for the semiconductors and the information criteria for the semiconductors now both of the methods and this actually gets to a question it was asked um at the break both of these method methods require a set of candidate models so everything I talked about here was comparing models and that gets back to bullet point one and my recipe for building for building models how do we come up with a manageable set of candidate models and manageable means we can fit them in the amount of time that we have they're the way that we do this and the way that I'm going to teach you to do this today is through forward stepwise procedures okay you start from a simple null model okay something that it has all the parameters set to zero or has all the parameters other than the ones that you definitely want in there okay regardless of what the data tells you sometimes you have parameters that you want in there okay it has everything else set to zero then what you do is you gradually add complexity to the model okay it's like little paste on a little bit of extra signal a little bit of extra estimation over here okay and build your model up let it get more and more complicated and since we're maximizing likelihood here let it get closer and closer to the true data okay to the data that you observe okay in other words make your fit Tighter and Tighter and Tighter your candidate set of models are then all of the models along that path as you've added complexity from the simple model to the complicated model every stage along that path every segment of that path is a candidate model and you can take those candidate models and evaluate them against each other using information criteria and using cross-validation okay that's the basic recipe Why do we do forward step wise okay why is it better than backwards selection well on very high Dimensions often backward selection is impossible if p is greater than n okay and we want to do some sort of least squares type thing I just can't fit the full model and work backwards from that moreover even if I can fit the full model often p is big enough that the full model is garbage right so why would you start from something that is you know kind of acknowledged as terrible um it's also going to be expensive to fit those big models right fitting the big full you know inverting X Prime X could take a lot of time if x is really really big so you whereas the null model is just a null model it's available in closed form okay so you're going to know exactly what that is it's Pro it's usually very straightforward to get your hands on the other reason that you don't want to start from the top is that backwards selection procedures are less stable why are they less stable because where you start from is unstable okay and what I mean by stability I use that lingo a lot I just mean sampling variance right so if you start an algorithm from a place that has higher sampling variance in other words the full model has a lot of sampling variance because you have very few degrees of freedom because p is close to N that's going to move all over the place where you start has a big impact on where you end with the stepwise selection procedure okay so if you have a lot of variability in where you start where you end has a lot of variability and the path along there has a lot of variability as opposed to the null which is stable because it just set everything equal to zero it's the same regardless of what data you get okay the stepwise approaches are greedy okay the engineers call these greedy algorithms what they do is they're a little bit myopic they go forward and they say okay I have a set of constraints the set of constraints is kind of my previous model and whatever extra bit of complexity I'm allowing myself and I solve for that set of constraints then I step forward again I say okay now my my existing model becomes my previous model and my constraint becomes a little bit more complexity allowed in here and I solve again it's myopic because it's being greedy and solving at each point for exactly those constraints at each point it's not thinking about any Global properties of the path okay so that's why you have to be careful about where you start these things many many this is you know kind of an aside but many many of the procedures that work well that people use in machine learning that people use for you know teaching robots to ride bicycles and things like that predicting what book you want to buy many of those algorithms can be seen in a greedy framework they often work this way it's a useful way to to build predictive models regression this is probably the stepwise regression that most people here are familiar with and it works as follows okay so in R there's a function just called Step that does this and it executes this very common routine you put all univariate models in and you fit them okay so I've all models for y regressed on X okay where X is just one variable so y regressed on X1 y regressed on x2 y regress to X3 et cetera down to y regress.xp i take of those models the one with highest r squared so the single X that explains the most in linear regression is going to be the single X most correlated with Y and I put that in the model and then I look I step forward and I say okay my bivariate regression model is going to include that model that I selected at the first stage plus any of the other P minus 1 variables that I did not select at that first base so you're going to regress on to X1 plus X2 X1 plus X3 X1 plus X4 all the way through down and then you're going to choose the bivariate model the highest r squared okay you're going to put that bivariate model and then you go to the trivariate model Etc you keep going okay you could keep going all the way up to uh the minimum number of P or n okay so you could keep going until you have a completely saturated or the most complicated model available to you often because this takes time what you do is you evaluate model of value information criteria the in the AIC or the Bic for all of these models that you're fitting and you stop when the model that you have has a lower IC value than any of the extra models the next level of complexity models that you could add in so if my bivariate model has a Bic and that Bic value is lower than the Bic value for any of the possible three variable models then I'll stop the algorithm and stick with that Bic okay so that's the way it works that's naive stepwise regression I call it naive probably indicates to you it's not what we're going to use there's two big problems one big problem is purely practical and that's cost okay even with the tiny data set that is the semiconductors 1500 observations right it's it's Tiddly it takes three minutes to get out to aic's stopping value AIC wants to stop at P equals 68. okay Bic stops really quickly but that's because Bic only wants a 10 variable model so it stops very quickly why does it take a lot of time while you're refitting all these models to get all the way out to P you're going to fit P factorial models that's a lot of models that's a lot of X Prime X inversing to be doing the other problem with this routine is again stability okay so the the I'll mention a little bit later on where the theory on this is coming from but you're gonna have to take me at face value for now the sampling variance of the model you get through stepwise selection is quite high if you cross-validated it right if you took a bunch of data sets you fit a stepwise model on each of them they would stop at very different places each time it's an unstable model selection technique it's more stable than backward selection but that's not saying much I'll just mention here actually something that I didn't mention at the beginning um if you go to my website there's code for today our code for today that goes through all the examples that I've been doing using the semiconductor data okay so it's a small data set so it runs really quickly but I just wanted you guys to have an example so it's on my teaching page it says NBR and then there's code and then there's data okay so just so you have examples it's in our because that's what I use so now we're going to get into the real core of of what I hope you guys are able to take away from this which is penalized estimation stepwise regression or stepwise routines in general are a great way to build candidate sets the null model is available in closed form there's a bunch of theoretically fantastic properties about going from small to big greedy algorithms or like the engineer's best friend they're all over the place okay so there's reasons to go stepwise there's reasons that we like stepwise but the naive algorithm is slow and unstable those are two things that are not very good um in particular why does r and every other software routine that works with stepwise regression why do all of those routines use information criteria for stopping rather than cross-validation it's because they're slow they're so slow it makes cross-validation impractical what you would have to do is cross validate every of those P minus one models that you would want to add in at each point you have to cross validate at each point it would take forever foreign so the alternative um the way that we do modern stepwise regression is through use of deviance penalties and that's the way I like to think about uh penalized regression some people here have probably seen this stuff before some people here haven't to fit penalize regression into the way I'm talking to you about model building today penalized regression is a way to come up with sets of candidate models and so here's what penalized regression is instead of fitting the mle for a regression instead of fitting the OLS estimate of beta hat instead of minimizing the deviance what we're going to do is we're going to minimize the deviance plus a penalty term okay so the minus log likelihood here plus some penalty and the penalty looks as follows it's got a land over here and this Lambda is greater than zero that land is called your penalization weight or your penalty rate and a little C function over here okay C of the beta J this is a little C cost function C for cost if you apply to each of your individual regression coefficients and C has the function as the property that it's that it's uh positive and it's going to have its minimum at zero we'll explain why that is in a second for your intuition right here you could imagine cost function of beta being something like some Norm some l0 L1 L2 Norm of beta so the absolute value of beta of raised to I there's a power of zero which is just one if beta is greater than zero and zero otherwise one would just be the absolute value of theta that's the last of it 2 would be Theta squared which is rich we'll talk more about what these arms Lambda here the cost function is constant Lambda is this penalty greater than zero that we have to choose what does Lambda do it indexes candidate models and this is why I like presenting penalized estimation uh in the context of this building candidate models really oh yeah um you want a major point no I'll just I'll just gesture like this it's more for my benefit than anybody else's I think um so Lambda in uh Lambda is what indexes the models okay and forward stepwise algorithms are going to move from a big Lambda to a small Lambda what happens at a big Lambda well when the cost that we put on betas is really high well then we're going to not want to pay much penalty which means we're going to have really small betas if we make Lambda big enough all of our betas are going to be zero okay if if Lambda think about if Lambda was infinite okay well then for setting a beta anything other than zero you pay infinite penalty which means you're going to set all your betas to zero that's the null model okay so Lambda equals infinity indexes the null model turns out a bunch of finite Lambda values are also going to index the null model and then what you do is you find that Lambda that indexes the null model OKAY the the the soup Lambda the index is the null model and then what you're going to do are inflameda sorry what you're going to do is you're going to gradually add a little bit of complexity to the model by gradually making Lambda a bit smaller making the penalty that you pay on estimating betas that penalty that you pay for absolute values of betas a little bit smaller and smaller and smaller and you're going to do that along a path and that's going to give you a candidate set of models that does not choose the model for you right what you then do is you take that candidate set of models and you evaluate them out of cross validation or using an information criteria and the combination of those two things a model evaluation plus a a penalty path the combination of those two things is what's going to give you the model selection choice at the end so that's everything there is to know about penalization now we're going to go through it more step by step one why are penalty functions minimum at zero our cost functions minimum at zero and bigger away from zero let's think a little bit about what goes into the decision theory of estimation because that's really what we're doing here the moment I start putting costs on things that I'm minimizing I've stepped into the world of decision Theory okay and as you guys I think that's actually one of the ways that economists can quickly gain access to the theory and the way of thinking about these types of models you guys tend to be good at decision Theory and utility and things like that and thinking about costs the fact that decisions have costs okay so use that as your access to understanding what's going on here in estimation and testing what are the costs of our decisions well estimation the cost is pretty obvious because we've been minimizing it all along it's the deviants okay so you know just based upon inferring from our actions the fact that we minimize this thing when we estimate that means probably that's the thing that we want to minimize that's our cost okay testing is a little bit less clear but it works as follows all of these testing algorithms all of these evidence-based algorithms are going to stay at beta equals zero in absence of significant evidence otherwise that's the whole testing framework in other words if I wanted to try and shoehorn that it doesn't fit very well but if I wanted to shoehorn that idea into decision Theory what I'm saying is I'm paying a big price for setting beta not equal to zero so beta equal to zero is safe and I'm forcing myself to pay a price a cost if I want beta B not equal to zero and the only way I overwhelm that cost is if the deviance the likelihood side of my joint cost function okay has a big enough effect from that beta that it overwhelms the price that I'm paying for moving away from my safe null of beta equals zero so implicitly that's the type of decision theory that you guys and I and anybody hypothesis testing has already been working with what we're going to do now is instead of kind of having that ad hoc definition of or trying shoehorn hypothesis testing into decision Theory we're going to talk about penalties right from the beginning and say here are the various penalties they're going to have the same form that we talked about hypothesis testing have in other words they're going to be lowest you're going to pay the lowest price price of zero when beta is equal to zero because that's safe that's the null that's Simplicity that's Occam's razor and then you're going to pay more price more cost as you move away from zero the shapes that these things have we already talked about this a little bit they generally can be set up as something close to an L Alpha Norm of beta the common one is beta squared that's the ridge the more common one is the nowadays is the absolute value of beta okay so it's a straight line coming out of zero okay that's the lasso and then there's a bunch of variations that are far less common the elastic net is a popular cost function that popular uh popular amongst the remaining things that are very unpopular uh but it it's a cost function that moves between the l0 and the L1 Norm okay uh the the or sorry L2 and L1 Norm so it's something between Ridge and the lasso we'll talk about the properties of these things and then you have these things called concave penalties and the log penalty is an example of that that's where your penalty is no longer um uh absolute value of beta to some Alpha it's log of some function of beta this thing here is equivalent to um a a an L Alpha Norm where Alpha is somewhere be between zero and one okay so you can get these things to match up I could take a question I think yeah so all of these things are going to be sensitive yeah yeah you can think of it as either normalizing thank you Jesse's my director um so it you can either think about so the question was hey these things are sensitive it's actually on this slide over here um but I'll leave it for a second you can think about these things as either being sorry to camera guy you can think about these things as either being uh um uh let me back it up the question was X scale matters because we're going to penalize the size of beta you can either think of standardizing your x's in advance or you can think of scaling your lambdas to account for the size of X okay but yeah you want to standardize your x's in advance it could have been easier okay here's the big point about variable selection penalization can yield automatic variable selection here's the way it works you have a big sort of swoopy parabolic type thing and that's your deviance the minus of the log likelihood is going to be some nice convex function it's going to be smooth unless you work with crazy models and what we're doing is we're adding to that nice smooth convex thing something sharp for example if you have like the lasso let's say that's the penalty that I have in the picture there and you add that sharp thing to that smooth thing and once you get out of that at the end of the day is a sharp thing plus a smooth thing and if the gradient of the sharp thing overwhelms the rate of change and the smooth thing then you're going to have a sharp point at zero which is where the sharp thing has its point so in other words when you want to find the minimum of the sharp thing plus the smooth thing it's possible that your Minima will lie at exactly zero and so that's why penalized variable selection minimizing the deviance plus a penalty can give you a solution at exactly zero which is why people say that techniques like the lasso do automatic variable selection anything that has an absolute value in it anything that has a sharp Point okay is going to do this so the lasso does this the elastic net does this the log penalty does this all of those penalty functions do this there are many many penalty options there there there's way too many out there and far too much theory for it and I am also guilty of adding to that bloated literature um but you can think of the lasso as a baseline okay the lasso is for many reasons a very reasonable Baseline and that's not to say it's going to be appropriate for every setting but you can think of the lasso as a baseline And if you move away from that Baseline you can ask yourself why what are the properties of it Etc um here's the point that was the question penalization means that scale matters most software that you run will actually do the scaling for you it will scale the penalty to account for the standard deviation of X so that every beta that you're working with is measured in the scale of unit change or sorry change per one standard deviation change in X okay so expect change in the expected value of y per one standard deviation change in the value of x cost function properties there's as I said a ton of theory on these things um Victor and Chris tomorrow will talk a little bit about the theory of the different penalty shapes in the context of the problem that they're interested in here's just kind of a pointer as to where you can look for things Ridge is the oldest it's kind of the great grandfather of these penalty functions and it's been along far longer than any sort of path estimation or stepwise regression or cross-validation or that sort of jazz okay so Ridge goes way back and it is what we call a James Stein estimator which is just where you regularize something that you're trying to estimate uh with squared air loss or squared error penalty and there's a big literature in the 50s 60s 70s 80s and Beyond on kind of optimality of estimators that have this type of shrinkage under different contexts the original Stein paper is just purely for normal random variables but there's it's it's gotten far wider since then um it works well for dense signals okay so of those penalties that we looked at the only one without a sharp kink in it was the ridge right which means the only one that's not going to set things to exactly zero when you're minimizing the penalized deviances The Ridge and so it's useful for a setting where you don't want to set things to exactly zero where you think that all of your coefficients matter sorry all of your covariates matter but you think that none of the matter that much you want them to kind of shrink towards a small value to avoid overfit not by setting things to zero but by stopping individual coefficients from being overfit what it does is it moves it has this nice property which is one of the things people really like about it theoretically is that it shrinks the coefficients of correlated variables towards each other so my multicollinearity example before if I put in meters my size of my house in meters and the size of my house in feet some jittered version of that otherwise it would be tough but you know meters and feet in there okay what Ridge is going to do is it's going to shrink the coefficients on meters and feet towards each other so that each of them will get one half of the effect effect of the size of my house on whatever it is I'm interested in concave penalty penalty on convex penalties all those penalties that have that sort of spiky shape that flattens out to the edges like the log penalty these have this function called The Oracle property okay the Oracle property sounds pretty cool um it has been a big object of interest in the literature and here's what it means uh usually there's a few different Oracle properties out there but usually when you're talking about coefficient Oracle property what it means is that as n gets big and for the more recent literature as n gets big n p gets big the estimates that you get of the new the true non-zero betas will converge to the maximum likelihood estimates of those non-zero Vedas if you knew exactly which betas were non-zero okay so the Oracle is in the sense that you know which betas are non-zero and these penalty these things apply under there's a massive literature so I chicken fan from 2000 2000 basically to 2010 there's a big or 2008 there's a big literature of him and others uh coming up with Oracle properties in different settings and and under different constraints whatever basically it boils down to if you have sparsity okay so if the actual coefficients are big and then most of them are zero okay and the the so a large portion of them are zero or very very close to zero then you can get these sorts of properties for um penalty functions that are concave okay so for penalty functions they flatten out in the end and why is that well let's think about the way that penalty function works okay so that penalty function if it goes up like the log penalty and it moves way out towards you know it flattens out as you get towards Big values or big negative values what that means is that the cost of moving from zero to something a little bit away from zero is Big because the penalty you pay at that point is the differential is massive however if you have a big signal okay and your big signal is over here well then the cost of moving from that big signal to a little bit bigger signal is quite small so what that gives you is a property referred to as unbiasedness for large signals okay so if you have something who's kind of the space that your deviance wants it to be out in that penalty function is out in the flat part of the penalty function then you're essentially going to go to the mle because you're not paying any differential in price for moving it more or less for those large values out of the side so those concave penalties give you unbiasedness for large signals which is the essential ingredient getting Oracle properties is the middle ground between these two it tends to choose a single input amongst correlated signals so for example the size of my house in meters size of my house and feet okay what lasso will do is it will set the coefficient on one of those to zero and use the other one in your model okay that tends to be what it does um but it also has this non-diminishing bias so it does not have the property I just described so the lasso penalty is just increasing at the same rate out to Infinity okay so even if I have a really strong signal it's going to be biased just as much as a signal down close to zero okay so it's distance away from the mle value is going to be the same distance away from the MLD value it would be for a small signal okay so it has those two things it does not for that latter reason the bias of large signals reason it does not have these types of Oracle properties that I just described that's actually an often misunderstood thing people think the lasso does have Oracle properties it doesn't however it can gain them pretty easily okay so one way that you can make the lasso in Oracle is you scale the Lambda for each beta depending upon some measure of signal of that beta right so the Adaptive lasso I have in here Zoo 2000 and six that stuff what it does is it uses the least squares least squares estimate of beta as a way to scale the size of the penalty that is in effect a non-concave penalty uh sorry a non-convex penalty which gives you this unbiasedness for large coefficients more subtly if you're not worried about Oracle for beta okay all of your regression coefficients if you're just worried about predicting as well as the model where you knew the actual non-zero betas okay then all you need to do is cross validate the uh the Lambda parameter which in a sense does kind of make it non-convex because the size of your Lambda is becoming a function of the data okay you just cross validate this that thing and recent results will show you that you have a an oracle you will predict as well as the mle model that knew the actual um knew the actual non-zeros as and gets big so that's a little bit of theory for you guys regularization okay so let's get back to this idea of what we're doing here so back to the Practical World um these pads that were fitting so where you decrease Lambda a little bit and you fit a beta along that path it's called a regularization path and that lingo comes from engineering where this idea of penalizing things that you want to minimize is uh is quite all okay so it's not not new tinkenov regularization is uh Ridge regression just with a different name in the operations literature from Far earlier um and the idea behind regularization is a common one in process and and Structural Engineering which is you depart from an optimal system or so you depart from optimality in exchange for some stability okay so think about building a bridge uh you could build the optimal bridge to minimize some cost function over the amount of I don't know cost of materials the constraints that you've dreamed up for how much uh pressure that bridge is going to be coming under from wind from the cars that go over it the maximum load number of cars Etc and you could build the exactly optimal bridge for that okay I would not want to drive on an optimal Bridge because there's probably something that the engineers didn't fit into their stylized model when they were optimizing okay that I would like to you know worry about and and have uh have cautioned for so generally what they do is instead of building the lightest fastest whatever possible bridge that you can have out there what they do is they move away from the sort of constraints that you would have on that in some sort of safer Direction now for us in the regularized in the estimation World safe is always towards beta equals zero which is why we regularize towards beta equals zero seeing Lambda is called a regularization path the lasso in particular is a candidate for the new least squares so this is what people do now as a first stage OLS is no longer really the first thing that people think to do and it's actually for big p it is as fast to fit a lasso once the entire regularization path from null model up to OLS like fully specified unpenalized model Lambda equals zero it is as fast to fit that once as it is to do a single OLS in other words to invert X Prime x to the minus one I think the P where that starts to kick in is you know a few hundred okay even for the semiconductors here it says four I think it's actually around three it takes about three seconds to fit the entire lasso path for the semiconductors takes about one and a half seconds to invert X Prime X okay so even for that small P You're basically on the same scale now why is that it's the genius of these things okay so what you're doing is you're you're moving along a path we've come up with a set of candidate models and remember one of the constraints I said when I was fitting candidate models one of the things I need is the ability to fit all of these model models quickly because I want to cross validate them and stuff so what happens here is when you move along the path if you look at that picture there that's a regularization path for the semiconductors as I change Lambda the x-axis there is Lambda as I change Lambda a little bit what happens to my Beta coefficients okay so the y-axis on this picture is the beta coefficients that you have okay and the x-axis is the Lambda the Lambda penalty and so the path is in how those beta hats move as I change the amount of penalty I pay and as the penalty I pay moves a little bit the coefficients only change a little bit which means that updating my model estimates for a new Lambda that's a little bit smaller than my previous Lambda is near instant we basically just project out and align from where we were previously and then correct a little bit and we're going to do a pretty good job okay so it's very very fast to move through that way almost as fast as just inverting the X Prime X in the first place that is the July's estimation okay it is not a way to select models as we already said it's just a way to come up with candidate models it's a way to very quickly come up with a good large detailed set of candidate models it's too good to be true yes because you need to choose Lambda however we know how to choose Lambda so it is too good to be true we have these methods out there like cross validation and information criteria that we can use to evaluate a bunch of different models okay so Lambda here is like our signal to noise ratio right and or a signal to noise filter and I like to think about it like a a squelch on a radio like on a VHF radio so if you if you talk on a vhref radio those of you that don't use VHF radio it's like the noise cancellation on your cell phone right um so so if you turn the squelch on a radio up all the way you hear nothing you get no signal as you turn the squelch down you start to hear uh Transmissions until you turn it all the way off and you just got you just get static okay and the trick to actually communicating with people on a radio is finding the point in the middle there where you just hear the person's voice and you don't hear any other static that's what Lambda does here okay so what we do is through our model evaluation tools that we have information criteria and out of sample cross validation what we do is we move the squelch we move Lambda and we see how well that squelch that Lambda value does in predicting the Unseen data in predicting the left out data so the way it works here and this is actually a key point to understand is you fit regularization pass for each training sample so let's go back a set cross validation is I have a i i let's say 10 full cross validation I fit the model to 90 and I predict on left out 10 then I fit the model to 90 and I predict on left out 10 over and over again 10 times what you do when you cross validate a lasso or any regularized regression or penalized estimation routine is you fit the entire path for the 90 percent and then you use the beta calculations from that path fit to the 90 percent on the left out ten percent okay so you're not cheating here you're not kind of fitting the betas using all of the data no you're using Lambda to index a candidate set of models on a training set and so then what you do at the end of the day is you choose the best Lambda I'll tell you what that means a second you choose the best Lambda and then you fit a path to the entire data set and use the estimates for Lambda that was best in your cross validation for the lasso or for the penalized estimator fit to the entire data set so that's mechanically what goes on here there's two ways that you can decide what is the best Lambda one is just what has the minimum out of sample error okay so what is the minimum out of sample deviance it's a called the naive or or CV Min is something that people call it an Evidence way of doing this that stabilizes the procedure that leads to lowering sampling variance and the complexity of the model that you're going to get it's conservative in some sense it's that it moves you towards simpler models the rule of thumb called the 1sc rule of thumb here is to take the model that has the the largest Lambda who's out of sample average error is no more than one standard error away from the minimum out of sample standard error okay so there's two lines on this picture up here this bit line over here the one that shows put nothing in the bottle that's the 1s here it's hard to see here because we don't have error bounds and this is the minimum cross validation error this is for the semiconductor data okay so what I have here is on the y-axis I have binomial deviance on the x-axis it's the exact same thing that I had on the previous side of those coefficient plots paths I have the log penalty and you can see around these dots there's some error bars out on the left hand side okay those are one standard error for the out of sample mean of the Cross validation error right so it's just the 10 x bar out of sample error um divided by the square root of 10 right because that's our standard error for the mean out of sample error and so that's our 1 plus or minus one standard error because of the scale in the plot you don't see those out on the right hand side but they're there the line on the right hand side is the largest Lambda whose Blue Dot is no more than one standard error away from the minimum Blue Dot which is on the leftmost dashed line okay so that's called the one standard error rule any serious software out there will do this for you and do the cross validation okay these for example were fit in R using actually these ones were fit using the Gambler package for R these ones were fit using the glmnet package for R they do essentially the same thing at a lasso um and it's as simple as I have the code down there if you have X and you have y I told it family equal binomial because this is for my semiconductor so I'm doing logistic regression it's as easy as that what that does is it's cv.glmnet it's going to fit for uh the I think it does tenfold cross validation or five-fold cross-validation by default it does that it fits the pass for all of the left out and it's going to return to you a picture of how well it did on all of the left out stuff as well as the coefficients for the full model OKAY fit to all of the data and which Lambda did best out of sample by the minimum rule or by the 1se rule so basically does everything for you in One clicking the button and I don't know I think for this data set here it takes um it takes like 20 seconds or something like that there 15 seconds so very fast very easy there's really no excuse for not including this type of stuff as one of the things that you do if you're doing pattern detection in high Dimensions it's very mainstream now here is kind of a key idea about um the lasso in particular so not penalized estimation in general but about the lasso in particular actually let me just pause right there because it's been a little bit is there any questions that everybody wants Yeah question uh um with the lasso yes for the reason I'm going to talk sorry oh yeah thank you um so the question was can I translate well the question was actually if I have a nice cross validation plot like what I have there where it goes smoothly down to a point where it does best out of sample and then it increases up uh and does poorly do I need to worry less about stability of cross-validation as a model selection routine and the answer is yes you can worry less about stability and that's actually not something just as a function of this plot that's a function of the lasso okay and think about we'll talk about this more in a second but the lasso is far more stable than subset selection forward stepwise selection because when you move from one penalty to the next the beta coefficients are only allowed to move a little bit so if you Jitter the data kind of the Dual of that problem is if you did or the data while the cost function everything means you're not going to move very far on that path there okay so that introduces stability into the candidate set of models and when you cross validate a stable set of talented set of models cross validation itself reap some benefits and becomes a more stable method so we'll see theoretically a little bit more what that means in a second but your intuition is 100 correct other question yeah I mean do we believe that if we have modeled as more parameters but like yeah so so there's there's plenty of really good theory about how well cross-validation approximates actual out-of-sample prediction error and it the it's a tough tough thing because you're inevitably talking about how well does this in Sample experiment do at predicting data I have not seen and so you're going to have to have some modeling constraints and whatever in order to come up with some Theory there there's good work so um you know Brad Ephron has stuff in the early 2000s and late 90s and those guys Advocate this 1se rule as actually being a better uh giving you better true out of sample prediction errors so to go meta on you again if you cross validate it cross-validation the 1sc rule tends to do better than the CV dot mineral okay so you can keep kind of building that Russian doll of cross validation up conceptually so yeah so it is for prediction reasons um you know here it's a little bit interesting because the 1se rule here chooses no uh semiconductors uh or sorry no Diagnostics it actually says to the people at the semiconductor manufacturing firm you should just be tossing out every 15th chip that's what the 1se rule says um you know when we cherry-picked the best 25 signals okay we only got a 10 out of sample right when we Cherry Picked on the full sample so I'm not actually sure that that's a bad result there is some non-linearity here that we're not accounting for but yeah so anyway so it might be that you want to conclude something right just for you need something so then you'll go with the Min but in general the one SE rule is what you want to go with I find my mbas often go for the men because they want more story to tell but you guys won't do that so information criteria and the lasso this is you know so all of these techniques are penalized estimation we'll talk a little bit more about what's good what's bad in particular the the non-convex penalties the lasso has a bunch of advantages one of them is that stability right so the fact that the betas change very little as you move along the regularization path is is is a big deal moreover the lasso has this super um I would think surprising and fantastic result about its degrees of freedom and that works as follows so we could ask ourselves if we wanted to use a an information criteria the Bic or the AIC things like that what would the number of parameters be well we called them number of parameters but you guys all know number of parameters is really degrees of freedom here okay so it's what kind of what is the effective number of parameters in other words could I translate to a different space either via penalization or whatever and this is the number of dimensions in that translated space that I've actually had free when I wanted to estimate my predictor um we could ask ourselves what are the degrees of freedom in a penalized estimation routine well it's not clear think about l0 cost so l0 is just subset selection you pay a price for every extra beta that you add to the model it's a fixed cost okay this is like what the naive stepwise algorithm was trying to optimize okay parameters in a model and that's actually what the naive stepwise regression uses right if I stop at 60 parameters like AIC did or 68 parameters well then when it calculated when our step function calculated the information criteria used 68 as the degrees of freedom but that's clearly not true right there's a big difference between cherry picking as I go along the 68 parameters that are most useful for predicting the data I have and being given a set 68 parameters so the degrees of freedom that we've used up are yes the number of parameters in our model plus a certain number of degrees of freedom that we've used in searching in order to find those optimal parameters to put into our regression equation a fantastic and surprising result about the lasso is as follows intuitively what happens actually I'll tell you the result first of all it's just that the number of degree the number of variables that you fit the number of non-zero uh beta is an unbiased estimate of the degrees of freedom okay so you can use to put this in purely practical terms degrees of freedom here is the kind of the Stein degrees of freedom if you're into that type of thing sure Stein's unbiased risk assessment um it's the covariance between your predicted values and why okay but you guys can just think about it degrees of freedom the same way that you always have the number of non-zero betas can be used as the degrees of freedom for a given lasso fit okay that's pretty fantastic and pretty surprising I think when you compare it to the intuition of subset selection where I told you hey degrees of freedom has got to be some number greater than the number of parameters in our model but it's the bias kicking it right so what's happening is yes because you're choosing from a bunch of variables you're paying the price in extra degrees of freedom there however for each variable in our model we're biasing it down towards zero so we're not letting that variable fully enter the model okay we're we're attenuating the signal that we get from each individual from each individual variable that's in the model okay so imagine if you just had a fixed number of parameters and you fit you didn't do any model selection but you just kind of fit a penalized deviance for that fixed number of parameters well then you would be using less than P degrees of freedom because you're obviously not going to get as tight a fit as the mle it turns out that the number of degrees of freedom you give away by penalizing by biasing your coefficients moves with the number of degrees of freedom that you're adding by searching amongst covariates okay really fantastic stuff if you're curious the papers the zoo Hasty tip sharani 2007 one it builds heavily on stuff by Ephron and and yay actually in in 2000 or 1998 what this means is that we can use information criteria for the lasso okay so we have a degree of Freedom parameter which is what you need for the AIC and the Bic to work okay it's what you need for those approximations those lapse approximations to the integral to make any sense at all and so what we do is we just take the deviance plus uh the number of parameters which is an unbiased finite sample unbiased estimate for the degrees of freedom and plug that into our information criteria okay so the bic is going to be deviance plus number of non-zero parameters times the log of n um as I said the reason you use information criteria is really because cross validation takes too long I don't think very many practitioners would say I love the theory of information criteria and Bayesian posterior model probability so that's why I use it I'll just ignore whatever out of sample stuff says nobody says that but the reason they use it is that it's way faster okay so it works in cases where cross validation does not work and we can feel confident using the lasso in those information criteria because of this degrees of freedom result um I was talking with Chris about this at the break it's Eerie how well the Bic agrees with the 1se rule okay so the difference between the Bic and the 1sc rule the IC Min and the 1se rule is usually less than the difference between the 1se rule far less than the difference between the 1sc rule and the mid okay so it's usually within one standard error to put it another way there it is for the calm score data so this data was at the very beginning I used it to motivate now it's going to pop back up we're going to work with it for a little bit because it's a bit of a more High dimensional more realistic example so just to recall uh web browsing we had uh we had why was some measure of what people spent online it's actually divided up into a bunch of different categories you can do some cool multinomial regression stuff here but for us we're just going to look at among the people that bought something okay the 60 000 consumers we're going to look at their total log consumption the total amount that they spent online and we're going to model that as a logistic or sorry as a the log of that total consumption okay that's just a linear regression model that's a function of X where X is the sorry X is the percent of your time spent on each of eight thousand websites so these were the 8 000 websites that people spent more than or that more than five percent of the machines in our sample actually visited okay so it's just linear regression log amount of money you spend online regressed on to uh percent of time that you've spent on 8 000 websites so X here is going to be a big Vector of percents of times on websites most of it's going to be zero for every machine because most machines most people visit fewer websites than they do visit okay and then it's going to have these little uh one percent two percent four percent Etc in there so the lasso path just to bring it all together right let's bring this into a least Square set it the lasso path is minimizing the deviance plus the L1 penalty times Lambda okay the deviance for a linear regression model is we're going to we actually remove so when you do linear regression you just pull Sigma squared out from everything here so we're minimizing this deviance here it's the sum of squares or one-half the sum of squares to make it a log likelihood plus Lambda times the sum of the absolute values of the beta so we're just doing least squares like you guys always did except what we're doing is we have over on the side a penalty on beta and here's what you get out of sample so the picture on the left I did this with cv.glmnet it takes about five minutes to run for a lasso right so sixty thousand eight thousand it's still pretty quick the picture on the left is your out of sample deviance it's the same as mean squared error here and you see the Lambda 1sc rule is pretty close it's just below or sorry just above minus four so it's the log Lambda chosen there is about minus three something High threes and the Min is is just above to the right I've calculated the Bic for each of the models along this path and you see it looks pretty similar right this is why practitioners like the Bic okay and in particular if you look it's actually just shifted a little bit to the right the Bic curve relative to the Cross validation curve and if you look at the minute.bic curve and that one SE line the the rightmost line on the leftmost plot you see that they match up pretty closely and they do here just to bring us finally back to some actual data and some coefficients I thought this would be a nice vacation from abstract ideas Bic chooses a model with about 400 non-zero beta hats here are some of the big absolute values of those beta hats these are on uh remember this is on the scale of one percent extra time spent on one of these websites translated into log of total consumption the coefficients are massive because spending one percent of your time on shoppingsnap.com means you're spending a lot of your time on shoppingsnap.com right these are not normalized by standard deviation the amount of time spent on everything okay but you kind of see you get what you might expect if you spend a lot of time on bellagio.com we anticipate you spend more money online okay or you have spent more money online this is you know people could have been spending money on these websites this is very much like reduced form type stuff here okay no structure to this the gambling thing is interesting because it turns out we think you're a big spender if you're on Bellagio but if you're on scratch to cash um which is also kind of a gambling type website but maybe a lower brow one um then we think you spend less money online okay so there it is just to give you some coefficients of what you get out of these things but you know this is the type of stuff you'd expect I don't think there's maybe a causal relationship between scratch for cash and less money online in fact it's probably the other way because you're probably spending money when you're on scratch to cash right but it's correlated for some reason okay and that's what pops up here it was probably correlated with 5000 other websites and scratch2cash.com is the one that the lasso selected okay so this is useful for you going in there and figuring out penalties um it's not necessarily going to pick up those causal relationships and Chris and Victor will tell you tomorrow how to do that type of thing if you scale by standard deviation you see the big that the places you might expect pop up you know united.com orbits whatever okay if you're on those websites I was just on united.com you're probably booking a flight if you're on that website it's not that much fun okay um so there's some data okay so now what we can do is first of all any questions about that right there okay so now let's go a little bit deeper into the idea of how you choose your penalty shape okay and in particular I'm going to talk about something that I think appeals and applies to a lot of economists because you guys are very used to fitting things with mle and sucking all possible signal out of an X right so getting the beta that's optimal in some sense right we're not necessarily people here haven't been building airplanes so which is not a bad thing right uh so so the concave penalties that I mentioned the ones that have the shape like this I think are appealing and I've found appealing to economists because they let you if the variables in the model it really gets to be in the model you estimate it at its mle and that's appealing because we like mles we want to you know given we're going to have it in our model we want to really nail it and give it the best coefficient value possible okay um it's tempting we got to be careful with those okay and the reason was actually brought up earlier with your question which is you know bias right we we yes we want to get away from bias we like unbiased estimators maybe however when you get away from bias you introduce instability you increase your sampling variability that's true in any sort of model estimation technique even for fixed betas but what we've built here is kind of this edifice of model building we you know we fit paths from the beginning to the end and then we do another estimation thing so we first estimate candidate models then we estimate how well those candidate models predict out of sample and then we estimate the estimate the best estimate of the one that did well predicting out of sample either 1sc or Min so a lot of steps going into this there's a lot of places for sampling variants to really explode and when you move to unbiased estimators you can have that sampling variance explode Okay so so the classic there's tons of literature on this and I think it's something that people that work in high dimension could really read up on and internalize and the literature is fantastic there's stuff in the kind of standard frequency statistics sampling variance literature there's a ton of stuff in the Bayesian literature about why bias is good right you know it's good because I am biased um and uh and then there's I think a great paper is this bryman 1996 paper it was really kind of the pushing out of cross-validation techniques and he's a little bit agnostic about what it means and he just goes into hey you know what's going to work well with cross validation okay so if I have a model that is this stable is it gonna is cross-validation gonna be a good way of selecting that model okay if I make that model less stable where does cross validation fail as a way of selecting models so just kind of as a lit pointer I think that's a nice reference history as well that's that's where the idea of random forests come from for those of you that work with those things they are seen as a way of stabilizing trees so that you could actually do cross-validation on them so for us in our world bias is what makes the regularization path continuous so bias is what gives those coefficient plots which are useful for understanding what's going on here right log penalty on the x-axis beta on the y-axis bias is what makes those coefficient plots continuous okay so not only does moving away from bias have big implications for sampling variants for stability it also has big computational cost implications right because I motivated lasso being super fast because as you move along that path your coefficient changes very little if your coefficient changes very little the algorithm is quite quick without bias your coefficient is going to jump all over the place the algorithm gets much slower we're going to see exactly what that means in terms of numbers in a second here's one way to think about what's going on here there is breathing room between subset selection and kind of complete exploding instability and the lasso which is a constant bias as you go out towards Infinity it turns out that those jumps in your solution path those jumps As you move Lambda down a little bit they happen if your cost function has the following property okay there it is in terms of numbers and derivatives but basically what it is is if your cost function the curvature of your non-con convex cost function at zero if that curvature the acceleration down of that curve is faster than the negative curvature of your deviance at that point okay well then you're going to have a jump in your cost function the picture is here are what it looks like so there's it's a little bit hard to see the dashed line I think maybe but I have two penalty functions one of them has steeper curvature one of them is more concave than the other one and I add a little bit of signal here so V is the thing I'm trying to predict this is just you know I'm minimizing a least squares thing for one random variable Exon V okay so as x times V gets higher the correlation between X and B is higher or I've added data to the model whatever it's just my signal the Deviant side starts to weigh more and as I add in that signal you can see what happens to the sum of the two things instead of being like lasso where it was a nice steep thing with one minimum if it's too curved if it's like the solid line then you get a bimodality in other words you're estimating a thing that has two uh uh uh two Minima and what happens is as you gradually increase the penalty weight or decrease the penalty weight that bimodality is going to be creeping down you'll be at zero zero zero and then all of a sudden that bimodality over here the second mode okay is lower than the spike at zero you're not going to smoothly move away from zero Your solution for beta jumps out to the bottom of that second mode okay so this is I think a useful way geometrically to understand the idea of instability because it's one thing for me to stand up here and say Hey you know you get too much uh too concave a penalty or you know you go to subset selection and everything gets unstable it's another thing to see it and what actually happens and imagine jumping from the spiky bit out to the rounded bit okay now the nice thing about the log penalty okay so this over here I should mention that sorry this over here is the log penalty right so this is that log of S Plus or the penalty is some positive s or Lambda Times log of R plus the absolute value of beta okay so that was the curvy penalty of my picture a few slides back and the log penalty is nice because for certain parameters you can have uh something that basically gets as close to infinite curvature right so where you have subset selection right so where you pay no price and then the moment you move infinitesimally away from zero you play a big price so subset selection and it can go all the way out to the lasso okay so it can go kind of between the l0 and the L1 norms and that's some of the things that I work on um this curvature here and this is I think another useful way to understand these things the curvature of this model for the log penalty is s over r squared okay so the log penalty is s Times log of R plus absolute value of beta okay and the curvature here I call it the variance of Lambda and that might seem strange until I tell you that all of these penalized maximum likelihood estimation methods all of these regularization paths can be interpreted as a Bayesian model where the cost function comes out of a prior that you've placed on the betas okay the classics are Ridge regression corresponds to a gaussian prior on beta lasso is a LaPlace prior if you haven't heard of the LaPlace distribution you can look that up on Wikipedia um other ones have funnier names okay uh it turns out that the log penalty okay that nice concave penalty that goes from l0 to L1 depending upon the parameters you use corresponds to a hyper prior on Lambda on the L1 cost that's unique to every beta that's conjugate to the LaPlace distribution to the distribution that corresponds to the lasso and it's a gamma okay so if what I said makes no sense at all but you think you have a good grasp on base what I'm doing when you when you go with the log penalty is the same as being a good Bayesian and sitting down and saying I don't know what Lambda is for every coefficient so instead I'm going to put a gamma distribution prior on the Lambda independent independent lambdas for each coefficient and that gamma is going to have shape s and rate R and that pops you out the uh log penalty if you solve jointly for both beta and Lambda okay so that's neither here nor there there's there's this um this literature on coming up with Bayesian interpretations of these things it's useful if you have that in your background it's useful I notice people nodding so maybe it's useful maybe people do understand kind of have a bit of a Bayesian background uh find um variances a useful way to think about the world or variances of penalties I like it because I can call S over r squared the curvature you know the second derivative of my cost function it's just the variance of Lambda which if I talk to someone I can say hey here's my variance of Lambda you might have a sense that you understand what that is maybe you don't um so there's literature out there on that in many ways this is a nice useful way to think about uh penalization okay um you can think about as a relaxation of a lasso okay and the joint problem so this is my little hobby horse one of them the joint problem of optimizing the the sorry the problem of optimizing a you know deviance plus a non-convex cost function has that property that it's multimodal which means that as an optimization guy stuff doesn't work right it's just really hard to go to the bottom of something when that something has multiple bottoms okay turns out that the joint problem of when you set it up as a Bayesian model finding the optimal beta and the optimal Lambda which is the same as finding the bottom of that bimodal thing is unimodal has a single bottom and these are a couple pictures that explain that okay but the point of this is neither you know not so much to push and say hey you need to understand the optimization details of these things it's just to say we can move away from L1 away from the lasso towards subset selection which is what everybody here probably thought they wanted to do before I talked to you today you can move along that spectrum and more and more concave penalties less and less stability and you can see what happens and when you're moving along that Spectrum you can parameterize in terms of the variance of Lambda and that has this Bayesian interpretation so here is a is a plot that Jesse requested so um so here here's a plot of just the Practical issue of instability so remember let's wrap that let's just refresh a little bit what I've told you is that lasso is good because it's stable because your solution paths are stable I then said well that's not just lasso any penalty whose curvature has absolute value less than the absolute value of the curvature of the deviance is going to have that property that it's going to be stable okay um now let's talk about what stability means stability means lower sampling variance all these good things you should read the bryman paper and the others out there that's a big deal it also means big things in terms of compute time and here's what happens for fitting that comscore data a lasso path through the entire data set as I change the curvature and I've parametrized here in terms of the variance of Lambda and what you see is as I increase basically everything's below a minute right so um The Dot and the line at the very far left look like they're different that's just plot and R basically the values were all around 45 seconds or so for a single a single path through the data at the lasso and for those non-convex penalties four small values of the variance of Lambda as the variance of Lambda gets big all of a sudden the compute time shoots up okay and that's because of that instability all of a sudden we've reached a point where the paths are no longer smooth where we have that bimodality thing going on and all of a sudden things shoot up now these are still reasonable compute times four or five minutes whatever subset selection here which is like not our subset selection but a greedy algorithm where I just choose the covariate with the largest gradient with respect to the current model at each time okay so it's kind of like a very fast subset selection that takes about a half hour the full OLS path here is single just to calibrate these times the single full OS path takes about three hours or greater than three hours on a pretty fast machine okay now these paths all go out to about 5000 degrees of freedom and then I stop them the full OLS goes out to 8 000 degrees of freedom okay but you know whether that's worth your time depends upon how much your time is worth so that's a picture of exploding time costs just to internalize what's going on here right here's so the the variance values are on a weird scale because it's multiplied by n the curvature okay but a variance of of 1000 here everything is still smooth okay because the curvature of my penalty has not overwhelmed the curvature of my deviance everything's still smooth that was one of the things that took less than a minute we moved things up two orders of magnitude all of a sudden the penalty is no longer uh that's where that's where computation cost exploded and you can see why is right here it's because these are just for a few this is not all the covariates but just for a few when you add something to the model it jumps into the model and everything else has to be adjusted at the far right hand side is if I plot subset selection and it's again all over the place right so every time I change my penalization a little bit every time I change the thing that indexes my candid set of models the model fit that I get jumps all over the place to go back to the original question about stability the Dual to this is if I Jitter the data a little bit the plot on the left is not going to change much the plot on the right is going to change dramatically okay so algorithm cost and um computational complexity so that is the uh second section so back after uh after lunch um with the the last section was a doozy right as as uh Jesse was just saying we warmed up a little slowly starting with hypothesis testing and things people understood and I'd seen before and then we really kind of went through the wall with some new techniques and talking about how they work and when they work and stability Etc so um I think that the basic recipe is a really useful Paradigm through which to evaluate a lot of what goes on in in Data Mining and in good high dimensional statistics which is you come up with a way to get a set of candidate models and the way that I said that's a really good way to come up with sets of candidate models is to lasso or other some some other sort of regularized past path algorithm and then once you have that set of candidate models you're going to find some Metric for evaluating and choosing amongst those models and you have to understand that those metrics or measures that you are using are approximations or estimations of something else either be that crystal ball out of sample variance or Bayesian model probabilities or some combination of the two but that kind of mix of hey candidate set estimation of some sort of predictive thing and then actually apply the one onto the other is I think a really nice way of thinking about model building we're gonna or for the last section here the last hour we're going to talk about something that people in the room are probably more or less familiar with uh which is Factor modeling and this is a big ingredient into high dimensional variable analysis or high dimensional statistics and it fits in quite nicely with the things that we've already described okay so so this is not orthogonal to uh the material that I've already gone through it's it's a natural complement okay so the types of model selection when I said model selection it was really just variable selection we were setting betas to zero the types of things that we did in the first stretch was all that and that's just one way to reduce Dimension it's kind of the simplest way to reduce Dimensions just set some things to zero and say they don't matter there's there's a vast number of other ways you can reduce Dimension and many of them are best understood within this framework of a factor model so I've written a factor model up there I have the expected value of x x here is my big high dimensional Vector of stuff covariance right maybe these are my 8 000 websites that I'm looking at traffic on your cookies for or maybe these are my 200 signals for my semiconductors or whatever and what I'm going to say is that the expected value of x okay what I would think I should see for an individual X is actually a just a linear function of of some underlying factors that I call V okay and V here is let's say x is 8 000 dimensional say it's my website thing and let's say I think that yeah there's there's a bunch of different counts that you would have for or percentages of time that you would have spent at at all these various websites but I think there's really only 25 different types of browsers out there different types of people as far as I'm concerned and and the variation Beyond those 25 is not of interest to me and so what I say is hey the expected counts or the expected frequency of time spent at each of these websites is some upscaling of what I would expect for each of those 25 different browser types and you could be a mix of one type of browser and another type of browser maybe it's a shared machine my wife's one type of browser I'm another type of browser and so the info that comscore sees is going to be some comparison of some combination of the two of us people here are generally familiar with the idea of what a factor model is that's one of the things that economists have actually been very good at is is among many things uh very good at is coming up with uh Factor models and useful Factor models and stories behind Factor models so the the equation is just this linear algebra x is p dimensional V is K dimensional where K is much smaller than P so it makes the the whole thing useful and Phi here is let's see if I'm going uh Phi here is going to be a k by P Matrix okay does that make sense I might have got my tildes wrong but anyways 5 is K by P in some Dimension okay um we were actually already working with Factor models in the semiconductor example uh the the um the the X's that we were working with were actually Factor scores they were V's from a fit to a much larger set of x's okay so those were principal components uh for a larger set of signals that's why they were independent not independent in the statistical sense but independent in the linear algebra sense they were orthogonal to each other when you do Factor modeling of this sort and if you want to use it for prediction of stuff which as we've discussed is really the name of the game what you do is you just regress once you have your factors your V's instead of regressing onto your big high dimensional X instead you regress onto V and that's way easier to do because V is of smaller Dimension than x and so this is kind of something you can just do with your usual least squares type routine or whatever you want in this Enterprise it is much better to have Y and X both in form estimation of B okay so where you just come up with the factor model for X like principal components which we'll discuss more in a second that's called unsupervised modeling okay it's unsupervised because you're just finding the factors that explain the dominant sources of variation in X whereas supervised as if you want to explain the dominant sources of variation of X that are relevant for y and this might seem like a very subtle little distinction but it matters in data mining context typically with really big data kind of beyond what we've talked about today but the stuff where you need you know clusters of machines to store things in the cloud we presume the reason you're storing so much data is not because you want to get a really really low Precision or high Precision estimate of X bar or something like that the reason you're storing that much data is you're going to try and pick up some sort of rare signal or outliers or something like that okay which means that probably the thing that explains the dominant sources of variation in x is not the thing you're looking for because almost by definition of the problem you're looking for a rare signal that's not going to be the dominant source of variation okay so just to have in the back of your mind as stuff Beyond Today supervised Factor models I don't think we'll talk about it at all but supervised Factor models is where you both estimate the factors as explaining X and Y jointly okay that's kind of a Step Beyond first what we're going to talk about today which is still very valuable because it underlies these other methods or where you come up with an unsupervised factorization for x and you're going to use that in regression for y and it does still work very well especially on the scale problems that I think most economists are facing the most common way of doing factorization in fact for many people it's synonymous with factorization it's principal components analysis okay and principal components analysis is I'll explain it three different ways there's I mean it is quite simply just this what you do is you have the previous page your Phi there your Phi end up being the eigenvectors The covariance Matrix The observed covariance Matrix so for those of you all rusty on your linear algebra the eigenvectors are the independent Subspace okay or transformation the Subspace that's independent that spans directions of variation in X Prime X so X Prime X is our information or sorry our covariance Matrix so these are independent directions of variation okay in the high dimensional space of X here and if we project from X through Phi then what we have is from a linear algebra perspection of a projection from X onto the independent Subspace that underlies X Prime X okay so it projection into independent uh uh basis where things are independent from each other so we have no more multicollinearity or anything like that and the reason this thing is actually useful the reason people work with it this way is that they'll calculate 5 for the whole spectral decomposition in which case Phi is p times P dimensional it's a full Subspace okay but then they'll only use the biggest few principal components and big here is measured in the size of the eigenvalue corresponding to those eigenvectors okay so that's I mean that's principal component modeling right if there's a few things that are confusing here one if you don't know what an eigenvector and eigenvalue is even if you know what that is but you don't really understand what it is then this can be tough um also the idea that instead of working with v itself we're projecting onto V okay that's one of the most confusing things about principal components when I explain them to my mbas and the way I get around that is I say basically you can think of the principal component as V just you know you won't get yourself in trouble if you equate the two things okay but here's a couple other ways to think about principal components and again I think everybody here has seen this stuff so I'm not talking to a total group of total novices but you know maybe here are some other ways to understand what's going on with principal components okay one way to think about what's going on with principal components is to think about the two-dimensional example okay so what would a principal component look like where I have just X1 and X2 okay well imagine X1 and X2 and I fit a line through them this is my OLS line my least squares line okay then in X1 versus X2 I think I'd probably need to draw on this in here in X1 versus X2 I have this line through here for every observation there's a point on that line that is closest to that point okay for every observation there's a point on the line that is closest to that observation and the principal component score the Z right that we get out that we throw in our regression models when we do principal component modeling when we plot when we're doing kind of a perception plot or something like that the Z your principal component is that point on the line if you stretch a line through the points the point on the line that's closest to your observation is the principal component score for that observation and why is that useful this gets to the intuition of why principal components are useful well if I want to know both X1 and x2 in a picture like this where the line is pretty tight The Cloud of points is pretty tight around the line all I really need to know is where that point is closest to along the line once I know where it lives along the line I have a pretty good sense of the neighborhood where that observation lives in the higher dimensional X1 X2 space okay so that's geometrically what's going on with principal components when you move just to two dimensions and it's the same thing in higher Dimensions which is harder for me to draw impossible for me to draw now many principal components that was just the first one here you recall I said you have as many principal components Columns of Phi potentially as you have Dimensions that you're trying to principally compose um and what you have here is a picture of what the next the second principle component would be so the red line was the first principal component okay it was the line that fit through the two of them if you imagine taking the residuals from that first principle component and fitting a line through the residuals for the first principal component well I would have slope zero okay so what defines that second principle component in this case is just that it's orthogonal to the first one okay and so that's what I plotted here if you wanted to imagine the second principle component here's what it would look like and the reason that in general we worry about the first principal components the variance is the length of this line okay so the the size of the eigenvalue corresponding to the eigenvector is the length of that line you can imagine this point here is 2 point here down here is two so the length of that line is two square root of the length of line is the square root of 16. um and this line is obviously shorter I'm not going to embarrass myself so this line is obviously shorter this line is also far less useful okay if I know where you live on this line here if I know where you live on that line there it tells me very little information about what your original X1 and X2 are whereas where if I knew where you lived on the red line I knew everything about where your X1 and your X2 are so that's principal components in two dimensions here's the other way to understand what's going on with principal components and it's a greedy algorithm okay so this fits in with what we were doing with subset selection um imagine you wanted to find a set of factors okay you wanted to find a set of underlying factors that explained variation in all of your high dimensional X's okay well you would and these are latent factors it's not like V exists it's not like Y where we can measure it okay V is out there so what we try and do is find the single V okay univariate V one of them for every Dimension that maximizes the average correlation between that V and all of the X's okay so imagine we were fitting a line for each X on V doing a bunch of independent linear regressions least squares okay we try to find the the v such that our average r squared across all of the lines for each individual X is as high as possible what you do is you find that V okay the thing that gives you the average highest correlation with each of the x's and you call that your first principal component okay then you do you take the residuals from that first set of regressions and you do the same thing okay except now you're working with residuals residuals on X's okay so you have if you have P X's you have p sets of of residuals okay and you try and find the new V2 let's call it getting creative call V2 the latent variable that you find that has highest average correlation with all of those x's and you do this over and over again you just creating these factors out of thin air because they're just numbers that are highly correlated on average so that's that's the way that's the algorithm this algorithm could be used let me say it that way this algorithm could be used to get principal components it's not because there's faster algorithms do it you decompose Matrix and such but you know this is an easier way to understand it yeah foreign well that's because you're taking residuals so these are least squares fit so they're residuals but yeah good point very good point the question was how do I constrain these factors to be orthogonal and the answer was because we're doing least squares fits we're maximizing our squareds in a line uh and then we're looking at residuals the residuals are by definition orthogonal so that's that's a little bit I don't know maybe if you if you had never heard of principal components before you're still lost because you know that they're not something you can learn in two slides but if you had heard of them before perhaps that gives you a little slightly different intuition for them here's an application of them back to the Snips this is uh a few guys who are actually all now at Chicago I think they weren't when they did this but um it's it's a principle principle component decomposition of uh genetic expression data so go back to the Snips that we had in the very first example the G wasp thing this is a similar thing except there's no phenotype there's no why that they're trying to predict they were just trying to understand what drives variability and expression of I'm not sure what they did but you guys can imagine it being the minor allele frequency okay and what they do use this big high dimensional 8 000 or or 16 000 dimensional thing which is expression at all of these various locations they fit a factor model a principal component analysis model to it and figure out you know what is my first dimension of variation what is my first principle component my dominant principle component was my second principal component orthogonal to that and I got those out and these are for people in in Europe and and the picture here each dot on that picture not the big circles the big circles are Aggregates by country but each dot is an individual and that individual Gene okay so that person's DNA information their score in the principal component decomposition of DNA on the first two axes the first two dominant sources of variation and they got this out and John and looked at it and they they kind of turned it around and they realized that it looks like a map of Europe okay so if you kind of turn it around and you color by country you have the Iberian Peninsula coming off the side Britain and Scandinavia are kind of messed up together or so that tells you a little bit about migration patterns up there but generally if you shift it and you do the Magic Eye thing it looks as though we're reproducing a map of Europe so the people's location in a reduced form factorization of their genetic information looks as though if you slant it Maps up with where they're from okay which to put that in scientific terms means that the dominant source is a variation in this European gene pool is geographic okay so that's just like a nice stylized example but it's good to have in the back your mind the power of these sorts of things and also what they're going towards they're going towards these dominant sources of variation you probably already knew this right but there it is it's in the data here's an example um that's big in social science okay and here's an example of factorization um that I think is useful for understanding really getting a sense of the intuition as to what goes on with principal components fitting these things is really easy I think even stata could probably fit principal components very very quickly um but but understanding what you get once you fit the principal components often they're not going to line up with a map of Europe so often you're going to have to do some investigative work and do some some some put some effort in to figure out what we're working with because just working with principal components in a vacuum is a pretty dangerous exercise because when things go wrong you can't diagnose what went wrong in your model Etc so here's an example Congress and Roll Hall voting so um arrayer probably knows this but I'll go through it roll call votes are votes where they take attendance so people are there and we record who was there and how they voted okay where they vote yay nay or whether they were absent or abstained the website voteview.com maintains a massive record database of all of these votes okay for congress going back to long long time ago I don't know how far back they have they have at least the mid-1800s and there's a of course an r package that just will download whatever you want from this but you can also just go to the website and get them okay um we're going to look at the 111th which I'm sure Jesse could tell me which one that was but I kind of forget it's pretty recent I think it's like four years ago um three three to four years ago and it's 45 members in the U.S house uh 1647 votes the Nays are coded as minus one and the yays are coded as plus one and the missing are coded as zero so what my X is here is 445 individuals by 1600 votes and each X entry is a plus one if it was a yay for that person a minus one it was a for that person and a zero if they weren't there and factorization of this boating behavior is a very common exercise in political science it's really one of the like key exercises in quantitative political science in the U.S and the idea is that you figure out dominant sources of variation in voting behavior and that those dominant sources of variation and voting behavior correspond to Notions of political science and political economy that you understand and would like to interpret when you run so I've put it up there just so you guys have it again I think it really is helpful to remember what model this is fitting the expected value of x here is some uh function linear function of the underlying factors those v's and those V's are the things that we don't know we're going to estimate we also don't know the fives but we get them for free basically and the the V here just so that everybody's on the same page that little vi1 is univariate it's a single number and that Phi one there is p dimensional because it's the thing that translates from a univariate factor up to the high dimensional uh vote space so go back to my line my original point on my line in two Dimensions okay V would have been the score along that line that red line and Phi would have been the bivariate thing that says okay while four is score on that line I think your X is here your X1 is here and I think your X2 is over here okay so it's the key that translates back and forth between dimensions the plot I have here is the variance of the principal components okay so the variance of the Z's which is x times Phi which you can just think of as V if that's confusing um this is the variance of this principal components which is really related to the length of that line that you stretch through the data and for the linear algebra people it's the square root of those eigenvalues and there's a huge drop in variance from the first to the second which means that most of the variability in X is play explained by the first principal component okay and then there's more drops after that so political science literature or what I understand of it which is a little tiny bit holds that the principal component is usually enough to explain voting behavior in Congress okay stylistically there's that there's two periods when people said hey there's a meaningful second uh uh second principle component here that makes sense to us as a notion uh as a meaningful political concept and that's in the 1860s and the 1960s so what is it probably it's race okay so that's what's mattered the rest of the time here there is still variability in the second principle component but it's not uh it's not generally seen as that key here they are plotted okay so here's the principal component directions in the 111th house um I've colored them by party right so the x-axis is the first principal component and the y-axis is the second principle component and I've colored them by party here the Sines the plus minus on a principal component is completely arbitrary so inconveniently here um I guess the people on the right of the image are Democrats and the people on the left of the image are Republicans since it's arbitrary I guess I should have just multiplied them but hey there you go there's clear Separation on that first principle component okay um the second one and the first one is basically party right it looks as though that gives us a nice cleavage between parties the second one is not so clear in fact while almost by definition it's orthogonal to party because party is explained in the first one um and it's really kind of this weird heavy-tailed sprawl towards negative values okay interpreting the principal components you can look at so what I have here these are actually principal component scores for individuals so these are really big negative ones the big negative ones were out towards the left in our thing which turns out from this to be the right in the political Spectrum the way we think about where people sit in in France and um the right out here is kind of these Conservative Republican guys same thing the left the ones that get big positive values um the left very liberal they are on the right of my picture and you you uh can look those guys up they're all fairly liberal members the third the second principle component is really unclear there's a mix of people here you probably recognize some names it's not clear to me that if you put these say six people here in a room they would ever agree on anything so it's not really clear what they're voting on also try to interpret principal components not by looking at who scores what and by mapping that up to something that you do know okay so that's what I just did by the way to back it up a sec what I just did is I had principal components and I compared those to something that I knew something about okay party and I use that as a way to try and build a story for and interpret my principal components because that's a useful thing to do that's one way of building intuition about principal components the second way that you can build intuition about principal components is by looking at the projections themselves so from an individual X where does that X make me live in principal component space okay so what are the fives for these individuals if I vote yay on this issue over here what does that say about where I live in principle component one space or actually for us here because we're trying to Sherlock Holmes this thing what does that mean about principal component two space okay so here are the loadings for the first principal component um positive and negative okay uh most of the votes don't matter so this is a histogram of all of the loadings okay you notice that there's a hump in the middle those are votes with low loading so whether you voted yay or nay or you were missing it didn't move you much in any direction on the first principle component its loading was about zero because it didn't really matter for ideology or partisanship which is what we figured out that first component is the ones out at the edge those spikes at the edge those are votes that say a lot about your partisanship what you believe what part of your member of depending upon how you want to interpret these things and in particular the votes that have really big loadings here are votes that were associated with one party or another but had a lot of cross-party voting okay so one of the big ones I have up here is tarp tarp something that was you know seen as a liberal uh Democrat uh Democratic issue but a lot of Democrats did not vote for it okay on the other end over here we have the Affordable Health Care Amendment this particular amendment I forget what it is I think it's like 11 47 of the votes was basically don't do it and never do it okay so it was a very right way it was very Republican uh very Republican vote okay and it was actually so far in that direction that many Republicans voted against it so it had this cross party so This lends a little bit of credence to the political science interpretation of these components of these factors at not just party as not just party but actually ideology they give us some sense of not just what party you belong to but whether you're to the right of your party or to the left your party Etc okay so that's the last political science that I'll do it's really not what I do so trying to figure out that second principle component we can do the same thing okay and let's look at the largest loadings so that's what I did I just printed out the largest biggest absolute loadings for um for this principal component okay for the second principle component then I went and I looked and I looked up the votes what was that vote well it turns out that all of these things are like the one with the biggest loading which is 429 legislators voted for resolution 1146 which is supporting the goals and ideals of a cold war Veterans Day okay you don't even have to vote for a cold war Veterans Day here you just got to say you like the idea and the things that a cold war Veterans Day is about so if you did not vote for this you were either not a politician and all these people are politicians or you weren't there and bingo that's it the second principle component here is just attendance okay so it's people who were not voting for things that everybody else voted for all of these people here were by-elections or like uh Pelosi they had other important jobs going on at the same time okay so that's the sort of of exercise that I want people I would like to see people go through whenever they try and use these factorization methods is put some effort into figuring out what you've actually done when you factorized how do we use these things getting back to our key goal today which is really prediction how do we use these things in prediction well the concept is is is insanely simple you just put the factors in your regression instead of the original X's it could not be easier if you have a two Factor uh decomposition you just do y onto Z1 and Z2 and you're done okay so it's very very straightforward and it works well for a couple different reasons one it reduces Dimension the principal components reduce dimension of a prediction problem they help you stop over fit which we know is a big problem with high Dimensions it's still a high dimensional model because you're using all of the X's however those X's when we're doing unsupervised Dimension reduction we're reduced you know without any attention to why okay they were just reduced in and of themselves they were found they explained X and then we're saying okay well how does y relate to this summarization of X okay so you avoid overfitting that matter when you have supervised factorizations which are good because you can pick up rare events it's less simple because you can over fit the factors themselves in which case you can overfit even when you're regressing onto one factor okay because that factor has been sort of super optimized to help you predict why um we'll do this uh oh sorry the second thing that's nice about them is that they're independent now obviously the sampling distribution of these guys is not independent right they're actually the absolute value of the principal component is negatively correlated with each other okay but within a sample they are independent okay and so from the kind of if you're if you think about them as just being dropped on you and you're going to do some regression onto these principal components all of the nice things that come from orthogonal X's that really only exist in Theory actually exist in practice here at least conditional on the X's not unconditionally uh so if we want to do a factor regression for calm score it's as simple as saying the expected value of y given X is not a linear function of x per se it is still a linear function of X but it's a function of Z times beta okay where Z here are principal components Vector regressions are very popular in social science okay so finance and uh political science these are the way you do things okay macroeconomics as well quite often their big emphasis on Factor regressions they're not always fit via principal components in social science in fact in the the political science literature the vote view stuff they don't okay but the results are very very similar to what you get if you use principal components it's just that there's a story that goes into the way that they've decided to fit factors you can think of it as principal components okay so people in social science like the factors for the reason we just went through they tell stories right so not only are they a good predictive model but you can get into the business of interpreting your factors and coming up with stories for your factors and latent underlying things like ideology or uh uh momentum type stuff okay so there's nice nice social science work that can be done there and helpful Theory building you do have to be careful though hey I see very often that people trying to use Factor regressions where there really is no Factor structure in X in other words where the X's are pretty much independent from each other imagine if your X's are independent from each other there's no Factor structure because Factor structure induces dependence between the X's okay or it could be that you have a factor structure in X but that it's not related to the dominant sources of variate or sorry that Y is not related to the dominant sources of variation and so principal component regression becomes a bad way of doing things okay how do you choose the number of factors this is tough actually so um without supervision okay so choosing the number of factors just to explain X well is a very tough game um their information criteria for it okay but in general the approximations are are weaker and not nearly as tight as the approximations that go into a Bic or an AIC for a regression model and the reason for that is that your parameter space is different instead of just having beta which is the size of X okay the size of the dimension of X now you have uh phi's which is like your beta as well as V and the tricky thing about V is that this is a parameter whose dimension whose dimension on one side of the Matrix is the same as n you have as many V's as you have n so any approximation like the LaPlace approximation to that integral that I said goes into the information criteria that's going to work well as n gets big well that's as n gets big not as n and p get big okay so those approximations don't work nearly as well for Factor models people still use them but you just got to be aware that as you're getting two large numbers of factors you don't believe what's going on it once you have principal component regression however now we're completely within the Wheelhouse of cross validation right so we fit this thing and we're not even really cheating when we fit a principal component analysis first because we're not using Y there's a little bit of theoretical argument as to whether you know it's cheating to First calculate principal components whether you should do that within your cross validation Loop or not okay but in any case it's not that bad a thing to do calculate your principal components outside of the Cross validation Loop and then like we did with the semiconductor data just go through combinations of principal components and see how well different combinations of principal components predict y out of sample okay so that's that's not a bad thing to do in fact it's a very good trick for choosing the number of factor models especially when you have regression there's two ways that people do this one of them is you build a candidate set of models okay so sorry let me back it up whatever we want to evaluate to use cross validation that does not select models for us it just allows us to compare models we need to come up with a candidate set of models there's two ways that people come up with candidate sets of models for principal component regression one is that they go include all of the K largest principal components K largest various variance principal components so I would have a one principal component model then I would have a model that includes principal components one and two then a model includes principal components one two and three okay so that's a nice easy way to come up with the set of candidate models or you can do what we did with the semiconductor data and you can actually just throw in all principal components and regress onto these guys using some path estimation technique then there's no guarantee that you're going to get you know the first three principal components you could get the first the 23rd and the 43rd or something like that okay the standard in the industry is to do the first one there's really no reason to do it that way that I can think of or that I've seen so if you're used to working with lasso stuff you can feel free to mix and match your techniques and apply lasso to apply lasso after you've done a principal component factorization if you're curious about how the approximation breaks down there's a lot of literature this Larry Wasserman and Kath Broder have it for mixture models okay so how Bic breaks down for mixture models a mixture model is just a principal component where you force V to be one and is zero elsewhere okay so you can only have one of those things and so the theory pulls through and the intuition is is the same so when I fit principal components to the comscore data okay so these are my x's here our website uh uh frequencies sorry the proportion of time you spent on different websites and something else I should mention that came up earlier in this context was that the the the X's here principal components they are not scale independent so in other words the size of X matters so almost always unless you have a really weird scenario what you are factorizing is x divided by the standard deviation of X okay because you want everything to be unit free and so that's what I've done here these are the frequencies that people have spent on different websites or proportion of time that people spent on different websites divided by the standard deviation for that website the first one is all ad trackers and providers so maybe this is going to be useful for actually predicting what people spend online the second one's porn so I won't list the domain names okay so we can fit through a uh do the lasso so I only fit 100 here okay um and actually I should say that there are techniques for fitting principal components on really big data sets Okay um generally these add up to fast distributed ways which Matt's going to talk about a little bit later of calculating X Prime X okay and then doing an eigenvalue or approximate eigenvalue decomposition of X Prime X but there's a big literature out there there's there there's lots of stuff out there I fit the first 100 here just because I wanted to finish pretty quickly and here's the out of sample picture that we get okay the outer sample picture on the right okay that's my that's my coefficient path right so this is my my how my betas on the uh uh print 100 principal components change as I move Lambda okay the left is the picture that's far more useful the right picture is useful for learning about the lasso it's yes less useful for evaluating it the picture on the left is the uh out of sample performance the five-fold out of sample performance for those different levels of Lambda penalization in other words those different penalizations or combinations of principal components that we include there it looks in this case like we need a pretty complicated model okay so the the curve really flattens out and does not start to go back up again okay if I had fit all the way out to 8 000 principal components which I could have done I could have grabbed all the way out to that many principal components it would have started to go back up again but it might have taken a while that in other words it looks like we might not have a really nice low dimensional Factor structure here or at least not one that's easy to measure or at least not one that's directly related to spending okay so to put that in one other way okay if I look at this and I realize that estimating once I get out past a certain number of principal components I know that the variability and the principal component estimation starts to get big once you are estimating principal components with really low variance that correspond to the small eigenvalues I'm going to get worried about that estimation variance overwhelming any sort of predictive ability that I have once I start to have to include too many principal components so I might worry here that I don't have a factor structure even more simple than not back it up I think often people forget the big picture here let's look at the y-axis here which is the gaussian deviance okay the gaussian deviance here is my about 2.65 this is our out of sample predictive performance on log consumption uh um on estimated log consumption okay remember that number camera guy you don't have to follow me I'm going to flip back through a bunch of slides so last time we saw out of sample evaluation for comscore okay and the low Point here is about 2.55 okay these are comparable right there's no magic in Cross validation I'm just predicting why five-fold left out with stuff that uh uh models fit to data uh uh training sample and predicting on the left out validation sample here the best model when I use all the x's and I'm doing variable selection on them is about 2.5 something back up here the best we can do is 2.6 something okay so none of the factor models that I could come up with here did better than the variable selection method which indicates probably principal component regression is not what you would want in this setting okay and that's a useful thing to know right so that's a useful thing to understand about web browsing and I actually thought it would work better because I imagine there were factors of this sort of thing and maybe there are but my ability to measure them versus my ability to look at a few websites and set things to zero means that the lasso on on Raw website counts did better one point to remember about this question sure [Music] [Music] they can choose as many as it wants um yes right so so the the question was hey would it ever work better to do principal component regression uh and the answer is definitely yes it does very commonly uh work better um and the reason is that you are the reason is actually the last Point down here this is a dense model okay this is not a model that has set the coefficients on any individual website to zero right we've selected on the principal components but even if we include one principal component then the loading on an individual X can be backed out so it's going to be beta on that principal component okay times the rotation or the loading on X for that principal component so this is a dense model okay if you rewrote this model as a regression onto all of your X's it would include all of the x's and so the shrinkage is different than just setting things equal to zero the shrink is is this linear Subspace explanation for X first and then second stage selecting how many linear bases you actually need okay so the difference is whether that's a good model whether the dense Factor structure is a good model or whether the sparse variable selection is a good model and that's something that will change depending upon on the context you work in and we see that actually in these these coefficients that you pull out here okay so these are large absolute value scaled coefficients if I back them out and again this is dense they're way way smaller okay so these are these are um much much much smaller than the uh values that we saw earlier that's because it's dense and everything's correlated with each other so if you spend an extra percent of time on victoriassecret.com then you also spend an extra percent of time on 100 other websites that have a similar beta which increases my expectation of how much you buy I have the raw ones in here as well but so you can see what's big on on each there's Mandalay Bay again so it turns out if you want to predict people spending you should look at how much time they spend on Casino websites it's not bad okay so again the principal component regression another way to realize it might not be working that well is if you look in here the big betas in that principal component regression are not on the big principal components they're on like numbers 18 35 and 22 which means that y really is not related to the dominant sources of variation in X which means I might do better with a variable selection technique alternative Factor models and I think I might see how much time I have okay so we might end a little bit early this will set up what Matt is going to talk about in the next two worlds uh next two hours um so let's think about what I just did and this is important thing to do uh what was the model that we fit when we factorized with principal components well it was fitting a line through stuff okay that algorithm that I told you about either the greedy algorithm or the two-dimensional cartoon that I gave you of principal components or like I said if you're up on your linear algebra the idea of what an eigenvector is okay all of those are minimizing least squares objective functions okay and when you minimize a least Square objective function you're fitting a line through stuff and we know that it's generally not a good idea to fit a line through things that don't look like a line is a good expected value for the data in particular what we have here is 99 or more than 99 of the data is zero okay so browsing on websites most the time you spent zero time on any of these websites okay so what we're doing here when we do principal components is analogous to you if you had a y that was 99 zero with a couple little x's and you fit a line through that and you reported the slope of that line in your paper okay might do that probably not the best idea usually functional form really doesn't matter but when it's this extreme it does start to matter and so there are a bunch of alternative Factor models out there for this type of data okay you can think about text counts you can think about website counts you can think about counts for various dictionaries that people use to decompose images and things like that a lot of the things that people work with in machine learning and in data mining are counts of stuff so a very productive area of high dimensional research and data mining research has been coming up with models and Factor models four counts of stuff okay so that's the last thing I'm going to talk about here which is what would be a factor model for counts of stuff and Matt will talk about this a little bit more later on but remember our principal component factorization it's that equation up there the expected value of x is some linear function of the v's okay and really what we assume around this when we fit principal components is that it's a normal with that mean and a variance Sigma squared It's actually an independent variance and that's what we do that's where our model estimates come from is deviance minimization for that normal gaussian multivariate model instead for count data the way we usually model count data big high dimensional count data is with a multinomial okay so a multinomial is just you imagine you have your bucket of balls you have your yellow balls your red balls and your green balls and there's a probability distribution okay in that there's a a certain number of red balls certain number of green balls certain number of yellow balls and if you're going to pull out 10 from there and after you pull out each one you put the ball back in and then you pull it out and then you put it back in and you record how many you get the distribution for the number of balls you get that are red green or yellow that distribution of that Vector is called a multinomial that's our standard distribution that we use for modeling count data and so a factor model for count data just factorizes the mean of that distribution okay it factorizes the probability over balls and so what it does here is it has X is distributed as a multinomial okay my big high dimensional X and now X is count so X is no longer percent of time you spent on websites for example X is going to be count number of times you visited this website number of times you visited that website number of times you visited that website account vector drawn from a vector of a set of probabilities okay and a probability specific for that person and the factorization just says that the probability for that person is a weighted combination of some underlying factors okay and those underlying factors are probabilities over websites so say for example we have someone that goes to a lot of shopping websites their Factor would have a lot of high probability on those ad click those those Google Syndicate type stuff because those people are getting exposed to a lot of advertisement and Factor two you had the porn people and so they would have heavy weights on all the porn sites okay and then if you're someone who shops a lot and surfs a lot of porn you would have kind of a 50 50 waiting between those two underlying factors okay and so that's what it is that's a topic model I'm not going to talk any much more about it because we'll mention it's a little bit too much for the very end of the day but it's just good to understand that the wide world Beyond everything we talked about today a lot of it has to do with better models for functional form like this it's also called latent dirichlet allocation that's the kind of the original name but blind and all do better than factorizations and I used to be a little bit skeptical of this but I've encountered scenarios in Empirical research where something works I have some idea that works on a factorization and it works when I do the factorization with the topic model and it does not work when I do the factorization through principal components so this is kind of the the site up there is one of the papers it's a it's an applied paper I have where we're looking at we have sentiment on we have Twitter so a bunch of tweets coming in online and we have all of these tweets have been scored for whether people are happy or whether they're sad and in particular whether they're happy or supportive of a particular politician that they're talking about in their tweet or not support of a particular politician they're talking about in their tweet and the way we decided as to go out there and gather tweets that we would actually pay someone to read and say hey this is thumbs up for Rick Santorum or this is thumbs down for for Mitt Romney because that costs money the way that we selected those tweets was by factorizing all of the tweets out there about these individuals and then doing some experiment design choosing the tweets to be well spread out in that reduced Dimension space of what people are talking about when we did that with the factor model a PCA sorry when we did that with principal component actually I'll start with the good when we did that with topic models it works great out of sample fantastic turns out we were grabbing good tweets for learning about what people were talking about online when I did the exact same thing with principal components it failed miserably okay and that's where functional form really matters in a tweet the number of words that you say is much much larger than the number of words you or sorry member of words you didn't say is much much larger the words that you did say so it really does matter these functional form thinks Beyond this this is kind of big picture Beyond this stuff the functional form stuff really can matter and that's where people could go if they want to learn more about these methods you can move further in that way whenever you deviate from linearity though it becomes more expensive to fit these things and they take longer okay and so there's ways out there to you know I don't know I have stuff on approximating the information criteria okay quickly so that you don't have to use cross-validation other people take the very reasonable approach of just fitting them faster so that you can do cross validation and stuff like that and there's a bunch of stuff out there a lot of machine learning these days is concentrated on approximations to these nice functional forms that you can fit in a reasonable amount of time and the equation is always that you give up a little bit or sometimes a lot in Precision in order to work with a functional form that better represents the data you're working with it's non-linear okay you give up some Precision though because these things are going to be tougher to fit so you're going to have to use approximate methods to fit the things you're not just going to get to minimize deviance you're going to have to get close to the minimum of deviance and that trade-off is there so just be aware of that as something that plays into whether a machine learning method would be appropriate for what you're working on the first few topics you know you can go through these and interpret these these are things websites that have high probability within that topic and not only do they have high probability they have high probability lift Within These various topics okay and what that means is that the word or sorry the website is much more probable within that topic than it is in general browsing Behavior so I don't know the last one there is like a gaming one Fleet freeslots.com is not a very common website that people go to but it has high probability in this gaming topic and so it's lift is quite high and it helps us identify what that topic is getting at unfortunately the punch line is a bit of a letdown here the Precision that we've lost in using an approximate method to fit these things I use the topics function in r the Precision that we lose there is is worse than any sort of benefit we get by using the fancier functional form which is a good lesson I think and in this case what happens is the uh the out of sample deviance is now 2.87 something okay so now it's higher than we got either with principal components or that we got with uh or that we got with um just straight up variable selection this is for the comp score regression again here's the recipe okay the recipe is as follows you build a forward stepwise stepwise path of candidate regularized models okay that's step one then you choose amongst that forward stepwise path of candidate regularized models using cross validation or if you don't have enough time to cross validate information criteria the best is to do both and ask yourself some soul searching questions if they disagree um if you have a factor structure or if you think you have a factor structure try doing that as well just because you think you have a factor structure doesn't mean it's useful for predicting why and so you want to include its out of sample deviance as something that you try and evaluate and compare that to the variable selection techniques a ton more to data science which is kind of like big data just a new term that got made up but um you know one of the things that we really if I had you know thankfully I don't if I had you know a few more hours I would talk a lot about supervised Factor models that's something I work on a lot and that's important because for picking up rare signals again you don't want to just factorize X in general you want to find the underlying dimensions of variation in X they're related to your really rare signal of Interest there's a lot going on there the terminology of inverse regression is what people like myself like to use when talking about those things supervised Factor models there's a lot of stuff there okay other ways you can go are non-linear models that we haven't really talked about at all and the best of those this relates to a couple questions I got at breaks the best of those I think are things based on trees or decision trees which most people are familiar with a very very simple model for explaining the world and what you do is you kind of shrink towards that simple model and then look at combinations of decision trees mixtures of decision trees to build in interaction to build in non-linearity um more generally a lot of what I said today can be replaced with sorry the the task select a model can be replaced with weight models in which case your prediction ceases to be just a single model prediction it becomes a weighted average of a bunch of model predictions that's beneficial for a few different reasons one of them is it introduces stability it lowers the sampling variance of your prediction because you're not choosing just one model you're not putting all your eggs in one basket you're averaging over a few candidate models okay so stability is much better it also has kind of in general expected optimality as well so you will do better on average in prediction as well um so those are called Model averaging if you're a Bayesian Ensemble methods if you're not they're very very popular they're a key part of a lot of machine learning then there's all the distributed stuff that Matt's going to talk about and that's it for me so thank you guys very much [Music] 