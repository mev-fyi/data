Guido Imbens: This lecture
is about weak instruments and many instruments. I'll talk a bit about the motivation for this literature. This is actually partly
a very old literature going back to the '60s. But the recent interests
really comes from one paper by Anderson-Rubin where they had a lot of weak instruments, and they created a bunch of statistical
econometric issues. I'll talk a little
bit about that and the issue we've
with that paper. I'll talk a little bit about the recent theoretical
literature on weak instruments and as well as the literature on settings where there's
many instruments. By way of introduction,
the standard normal asymptotic
approximation to the sampling distribution
for instrumental variables, they're estimators, including two-stage least squares
and a whole bunch of ADA relies very heavily on the fact that the instrument
is actually relevant. There's a non-zero
correlation between the instrument and
endogenous regressors. As long as the correlation is non-zero, then asymptotically, instead of using the standard asymptotics you let the number of observations get large and keep the parameters fixed
implies that you would get limiting normal distribution and the normal approximation
would actually work. But if in fact this
correlation is close to zero, this approximations
may not be very accurate even in
fairly large samples. In the end, the big
surprising thing was that in this
Anderson-Rubin example, even though they
use census data. Particular, they use of sample is about 300,000 observations. It turns out that the
normal approximation still was very
poor in that case. Now I want to make
a sharp distinction in the subsequent
literature between the just identified
case or the case with low degrees of over
identification that this is still true, the normal approximation
is still going to be very poor if the instrument
is very weak. But it's not nearly
as bigger problem in practice even if the
normal approximation is not particularly good. If you calculate the standards, confidence in defaults, even if they're based on
asymptotic normality, which is not a good
approximation, they will still tend
to be fairly wide. Even if they may not be
completely accurate, it won't really matter because you'll see that you're not learning anything
from those data. Now in somewhat special cases, it'll be a bit more
precise about that later. It could be that you could get informative inferences and
the two-stage least squares or LIMO would have been
misleading if applied. Then there's some alternative
methods available. But mostly those cases correspond to cases where
the degree of endogeneity is so high that those are not very relevant for most empirical
work in cross-sections. In the just identified case, I'm largely going
to discuss some of these alternatives that a little more precise inferences. But there's not all
that bigger concern because LIMO or
two-stage least squares, essentially going to tell
you that learning much if the instruments
are very weak. That's very different from the case with a large degree
of over identification. Part of what Anderson-Rubin did, I'll come back to
that in some detail, but they use a large number of instruments by interacting
the basic instrument, which is three indicators
for cutoff birth. They interact today's dose
with stayed dummies and year dummies ending up with something like
180 instruments. That turns out to have very strong implications for the quality of the
normal approximation of two-stage least squares. In that case, two-stage least
squares is very biased. They're very biased towards OLS and the conventional two-stage least square standard errors are very substantially under estimate the variance
and the standard errors. LIMO is much better
in that case. It's still centered pretty much at the true value out of them, and the instrument is completely irrelevant obviously
because then we're just dividing something by
zero and not really clear what the estimator couldn't possibly be centered
at the true value. But if the correlation
is not completely zero, LIMO still stands to be
centered at the true value. But if the degree of over
identification is very high, its standard error
is not correct. In the end, there's some very important and
it's not always appreciated work by
Paul Becker deriving the asymptotic
distribution that allows the number of instruments to increase with
the sample size. That's suggests in
the simple case where it's just a single
endogenous regressor, very simple adjustment to the LIMO standard errors and multiplicative
adjustment that is just based on the explanatory
power of the instruments. That actually works pretty well. Overall, if pretty
much irrespective of whether there's
any concern about the weakness of the instruments, there's no reason to use the two-stage least squares just identify case obviously
it's the same as LIMO. But in cases where there's
any over identification, I know of no systematic
reason why you would use two-stage least
squares rather than LIMO. Moreover, if you're using LIMO, I generally be inclined to apply that adjustment
from Becker's work. It's also related to random
effects specification. I would apply that adjustment to the LIMO standard errors. If in fact the
instruments are not weak, that adjustment will just vanish and won't
make any difference. But we generally make inferences much more robust
in a systematic way. Part of the attraction of that adjustment is under
these Becker asymptotic, you still get
asymptotic normality. All the standard methods about the standard interpretation of confidence intervals are
still going to work. The relation with the random
effects interpretation suggests that in fact, you can also still interpret these confidence intervals as Bayesian
probability intervals. I would actually have fairly
strong recommendations that we would just never use two-stage least squares
and we always use LIMO, but adjust the
standard errors in this relatively
straightforward way. Let me start at the
Anderson-Rubin paper. I'm sure lots of people
have seen the paper. They were interested
in estimating the returns to education. They were concerned with endogeneity of the
years of education. They thought of exploiting variation in schooling
levels coming from compulsory schooling laws. Compulsory schooling laws affect people differently
depending on when they're born with the idea
that if you're born before October 1st, you would go to school
almost as year earlier than people born after October 1st if the October
1st is the cut off date. Perhaps somewhat
surprisingly, that actually leads to variation in average education levels
by quarter of birth. Then decided to
use the indicator, therefore quarter of
birth as an instrument. There may very well be some issues with the
validity of that. For example, there could
be concerns that starting the former education early at a younger age may have
differential impacts. You should go back to
that paper and look at all the concerns they raise and why they think they may
or may not be relevant. That's not the topic I
want to discuss here. But given that they
use quarter of birth if they just use, here I just did a very
simple version of that, using only data from the
first and fourth quarter. Then the IV estimator, which is the same as
two-stage least squares, which is same as LIML, it's just the ratio of the difference in average
earnings for people born in the fourth minus average earning for people born
in the first quarter divided by the difference in average levels of education
for those groups. You do that using asymptotic normality,
using the Delta method. For the standard errors,
you get point estimate of 8.9 percent versus
standard error of 1.1, so that's all fine. There's no serious problem with the effect of the
quarter of birth, the correlation
between quarter of birth and years of education. It's small but with
300,000 observations, it's very precisely
estimated and asymptotic normality
here is not an issue. The issue arise in some of their later estimates where
they interact the 3/4 of birth dummies with 50 states of birth and nine year
of birth dummies. In order to try to get more precise estimates
that are based on the conventional wisdom and the correct wisdom based
on first-order asymptotics that by having
additional instruments, you would not hurt your
estimates but you could potentially get more
precise estimates by soaking up
additional variation. Here I'll look at some
estimates instead of just interacting with
these 50 statement nine-year of birth dummies. Actually interacted it with states times the year
of birth dummies to get 500 instruments to make some of these comparisons
a little starker. But also this state times year of birth dummies as exogenous
variables should have a linear instrumental
variables setup with these 500 instruments and correlated with an
observed component in the regression function. I'm still interested just in this one coefficient
years of education. If you look at the
two-stage least squares estimates, that looks fine. In fact, that looks as if it justifies getting
these interactions. You see if you compare the
estimate to the one here, here we get standard
error of 1.1 percent. Here using two-stage
least squares, you get a standard
error of 0.8 percent, maybe that's not a big
deal but suddenly it doesn't seem to hurt you. There's nothing in the two-stage
least squares estimates that would seem to
raise any concerns. If you do LIML so that you get a
slightly different picture. In fact, the standard
error that goes up from using all those additional
estimates as well as being surprisingly different from the two-stage
least squares estimate. But if you just look at the two-stage least
squares estimate, I remember when Anderson-Rubin were doing some of this stuff. Seeing these estimates
and certainly, nothing would have suggest to me that there
was a problem here. But it turned out there was, and so did this
very clever paper by bound Jaeger and Baker. I said the implication of
that is that despite the fact that the Anderson-Rubin
are huge samples, this asymptotic
normal approximations may not work very well because the instruments
are only very weakly correlated with the
endogenous regressor. The most compelling
way of seeing that is based on
the following idea. If you take the
Anderson-Rubin data and we do this calculations after taking out
the actual quarter of the birth dummies we're just replacing them with
random binary indicators, but the exact same
marginal distribution, and then see what
these estimates and the standard
errors look like. Then principle, would
you would like? Obviously, if you
do that there's no information in the instrument
is completely random. You trying to estimate a ratio of population covariances, both of which is zero. There's no particular
reason to expect it to be centered in a
particular place but you'd certainly like the
standard error there to be big. Asymptotically it
should be big because it's somewhere in
the standard error. Appears the square is one over the denominator,
which is zero. A principle, that's
what you would expect. That you would hope to find
even though in principle the normal approximation
obviously wouldn't work because you're
dividing two things. You're dividing two objects, both of which are
estimating zero and so for both the numerator
and the denominator, the normal approximation
works fairly well. But so you would get a
Cauchy random variable rather than a normal
random variable. But in principle, that shouldn't necessarily give
you the bigger problem, you would expect it to still find a fairly wide
confidence interval. But the surprising thing from the bound Jaeger Baker paper is that if you do that not so much with a single
instrument where report these numbers here for the day I just using for these
earlier calculations. If you do this for a
single instrument case, now you get an estimate of
the returns to education of minus 196 percent. So obviously a
ridiculous number. There you goes to get a
very large standard error, so there's no particular
concern with that. If the instrument
had been so weak, had been completely uncorrelated with the years of education, getting a number like
this would not have misled anyone into believing that they were
learning something. But the concern is in the case
with the 500 instruments. Even if these instruments
are completely random, two-stage least
squares looks as if it's giving you a very
precise estimate, looks like a very
reasonable estimate. Here the point
estimate is 5.9 with the standard error
of 0.9 percent. That's really the basis for the whole subsequent literature. Because if you did this without being
aware of this problem, if in fact you had
instruments that were completely uncorrelated
with anything, you might still have
got the numbers that you would be willing to report and that
look very reasonable. That's obviously
not a great thing to the subsequent literature that set out to try
to understand that and come up with
better procedures. In this case, already you can see that LIMO does much better. LIMO gives you a nonsensical
point estimate here of minus 33 percent
for the returns to an additional
year of education. It would have been
very unlikely that that number would have
been taken very seriously. But it's still a
bit of a concern that the standard error is actually 0.1 here, suggesting that this estimate is actually relatively precise. Even though LIMO
doesn't do nearly as bad as two stage least squares, standard error here is 10 times that of the two-stage
least squares one. It's still not doing
particularly well. In order to illustrate that
problem a little further, I did some simulations to investigate particular
aspects of this. What I did here was take care of 10,000 artificial datasets, all of the same size
as this subset of Anderson-Rubin data that
I was using a 160,000. In each case, half of the individuals would have
cut off birth equal to zero, half would have cut of
birth equal to one. I sampled the reduced from disturbances from a joint normal distribution centered at zero with marginal
variances equal to the variances estimated on the Anderson-Rubin
data and correlations. I'm going to play around
with the correlation there. Correlation equal to Rho. Then I impute then I randomly
drew years of education and log earnings using linear model estimated on
the Anderson-Rubin data. Education is equal
to 12.688 plus 0.151 times a quarter of
birth plus this normal term. This data is just going to look like the Anderson-Rubin data other than they are continuous, they're normally distributed. But that isn't really
the issue with a 160,000 observations as normality's
for simple objects, like averages is going
to be perfectly fine. It's all going to
come off the concern about the ratio of these things. But it allows us to focus on what are the determinants
of this problem. What I then did was for each
of these 10,000 datasets, calculated the standard IV estimator and its
standard error, either using the actual
quarter of birth or the random quarter of birth
variable as the instrument. What I was interested
in was how misleading standard inference would have been in all of these cases. The one thing I'm going
to vary across some of these experiments is
the correlation between the reduced form errors
essentially amounts to varying the degree
of the originality. The test is just going to be the standard based
on t-statistic. We'd reject the
null if the ratio of the point estimate
minus the truth, which is 0.089 here, and divided by the standard
error is greater than 1.96. I did this for a bunch
of different values of the reduced form
error correlation. Here in this table, based on these for each column, this is 10,000 datasets. Look at the coverage rates for these confidence intervals, as well as the width of
the confidence interval, to both the medium width
and the 0.1 quantile. We would be concerned with is if you use the
random quarter of birth, if the coverage rate was low, as well as if the width of the confidence
interval was small. Here we said, we can interpret these numbers
somewhat substantively. We know what the
returns to education as somewhere between zero
and 20 percent per year. It's probably this people here will know
more about this than I do, but I think most
people would agree that it's probably
somewhere between zero and 0.2 given the monthly
balance we saw yesterday. That's the perspective. I'm going to put those numbers. If we do this for real
quarter of birth data, the fact that you have
160,000 observations, this gives you
accurate estimates, accurate confidence intervals. Confidence intervals say I have 95 percent coverage irrespective of the degree of endogeneity. If you use the random
quarter of birth, you actually have coverage
rates much higher than 95 percent for most
of the values of Rho. It's only when you
get two values of Rho, they're extremely high, 0.9 and higher especially by
the time you get to 0.99, that the coverage rate goes
below the nominal one. Even in that case,
the medium width of the confidence interval
is still pretty high, but now occasionally, you'll get confidence intervals
that are much narrower. But the point I
want to make here, I think this hasn't
always been very clear in some of the
theoretical literature, is that in a case with
just an identification, even though normality
doesn't necessarily hold, you need it enormously, you need arguably unrealistically high
degree of endogeneity. Before standard
confidence intervals will really be misleading. You may not get a confidence
interval that has the right coverage rate
but it's not going to suggest that you get a precise estimates
of something when in fact you're not learning
anything from the data. Related to some of the
technical literature. One way of describing this problem is that
if you think of this problem is that conventional
confidence intervals based on that point estimate plus 1.96 times the
standard error. There's conventional
intervals cannot be valid uniformly over
the parameter space. There's going to be parts of the parameter space where those are going to have
coverage that is much lower than the nominal wants. That's been established
theoretically some work by the first and work in the statistical literature, it's not possible to construct confidence intervals that
are valid uniformly if these confidence
intervals are of the form the standard point estimate plus 1.96 times the
standard error. However, that doesn't mean these things are going to be particularly misleading. We may not care
about all parts of the parameter space equally. In practice, I think in the just identified case, I have not seen
any examples where the standard inference would have been substantially
misleading. That is in sharp contrast to the case
with many instruments where the simple example
based on Anderson-Rubin data, they'd already showed that
inference would have been misleading and there
really is a concern. I think now, you should know people probably
would not and certainly should not do two-staged
least squares and completely ignore
these problems. Let me still pursue a single
digits identified cases. I'm only going to
look at the case with a single endogenous regressor as this will be a
single instrument, single endogenous
regressor case. Let me pursue that case a little further to get an
understanding of what can be done and that
will have a setup. Here, it's just linear model for y in terms of the
endogenous regressor, linear model for the
endogenous regressor in terms of the single instrument. You're going to use
asymptotic normality and that's nobody issues are especially if you have
reasonably large sample set. Normality for the IRS doesn't really make care all
that much difference. Reduced form for y in
terms of C is also there, the parameter we're
interested in this is ratio of this reduced
form coefficients. Omega the reduced form
covariance matrix. That's most of the
theoretical literature that's actually taken as narrow as known because that simplifies the theoretical
calculations enormously. That's not a big deal
in practice either, because with a
reasonably large sample you can estimate as well. It's very different from
the covariance matrix of structure errors that's
very hard to estimate because they actually relates
directly to the degree of endogeneity but the reduced from covariance matrix
is easy to estimate. In this case, the
standard IV estimator is just this ratio of sample covariance is another
object that is going to be important here
as well it's known as the concentration
parameter square off the reduced form coefficient times the sum of the
squared deviations of the instrument from its mean scaled by the
reduced form, the variance. Another way of saying
that the instrument is weak is said the
concentration parameter is small if in fact the Pi
one differs from zero, then enlarge enough samples
is concentration parameter goes to infinity and
everything's going to be fine. In finite samples Lambda
may be relatively small, even if Pi is not
completely equal to zero. If you just look
at the numerator and the denominator separately, we're essentially
thinking about cases where both of those separately, or in fact even jointly, well approximated by a
normal distribution. It's not that
there's any concern that the denominator itself is not normally distributed, the concern is going to be that the denominator is small and therefore the ratio is not well approximated by the
normal distribution. If the denominator is centered away from zero you could
just use the Delta method. The derivative in the
Delta method formula gets very large and we get a distribution that
is much better approximated by Cauchy
distribution there. How can we think about this? One way that's been
suggested by staying stuck is to look at the distribution of the
standard IV estimator. An approximation that the focus is on the weakness of the
instrument so instead of doing this under asymptotics
by the sample size gets large and fixed set of
parameters the approximate this distribution of the IV estimator
as if the instrument is going to zero implying that the concentration parameter
converges to a constant. In order to interpret
the behavior of some of the standard
confidence intervals, [inaudible] stock look at
the coverage properties of those confidence intervals and this particular sequence. In some sense the relevance of that
process is not so much about that particular
asymptotic sequence is the most interesting one through what the key
insight from that is, that it shows if you get
a difference between the coverage rates
and sequence than for the standard
asymptotics that then it implies that these
values of these pies such that standard confidence
intervals are going to have coverage that
is substantially away from the normal coverage. It's in another way
of thinking about it and that's in line with
some of the very recent work, is to think of standard
confidence intervals not having coverage uniformly
in the parameter space. As I said before, we may not care about
uniform coverage everywhere but you
can argue that at least in some cases, you won't uniform coverage of a particular part of the
parameter space and here, we can think about looking
for methods that have uniform coverage around the area where Pi is equal to zero. Some recent work by an
Amicus Shaver looking at this week instrument problems from that perspective. You can only do that for confidence intervals, you
can do that for estimate. This is if Pi 1
is equal to zero, there's not going to
be any good estimators but you can in fact find
confidence intervals. They work well here even
if Pi is equal to zero. The next thing I'm going
to do is look at some of these proposals for confidence
intervals that work well. They work well uniformly
around Pi is one. This is partly because there are certainly cases
where this is useful although there are
some reservations that I think for
this desk identified case standard confidence in the false and very rarely going
to be very misleading, but it will also lead into the case with multiple
weak instruments. For the identified case there's a very clear way of getting a confidence interval
that is fairly uniformly in the
parameter space. This is based on work
by Anderson-Rubin, the key paper is that 1949. The remarkable thing
is that Anderson is still actively writing
things and doing things. It's not always the most
interesting work is doing, a couple of years ago we had
a paper and this was for a fast drift for
tile and they showed that in fact it was
entitled but Anderson, who had come up with
two-stage least squares. It seemed remarkable thing to be doing by the time you're in your 90s especially given that in the current literature, two-stage least
squares is probably something nobody should be doing and so it's not
clear why you would want to claim credit for that. But this 49 paper is a
remarkable paper really and has some very
useful results in it. Their idea is that to take the instrument
deviations from its mean, look at this statistic, look at the correlation between the instrument and the
deviation of the outcome, the difference between the
outcome minus Beta times xi and looking at statistics
as a function of Beta 1. If you have the true
value of Beta 1, you should have two objects
that are uncorrelated, they should have the mean 0 and in fact condition
on the instruments, this is just the
product of C and the residual so we'd have an exact normal distribution there. If you have the correct value of Beta plugged into
that statistic, you get something that has an
exact normal distribution. Then you can use
that for testing the null hypothesis that
Beta is equal to Beta node. In the end you can base
a confidence interval on the set of values of Beta that don't get rejected by this test. If you assume the epsilons have normal distribution
then irrespective of the power of the instrument, irrespective of the reduced
form coefficient Pi 1, this statistic is going to have a Chi-squared
distribution and so the confidence interval on
this is going to have the right coverage irrespective
of the value of Pi 1. If in fact Pi 1
is close to zero, this is obviously not a very powerful test because there is not going to be any
power to any test and then this confidence interval can be equal to the
whole real line. That's in some sense as it
should be in that case, because there's just no
information in the data. This gets closer to the case with
many instruments. But suppose we do this in the over-identified case
with just a few instruments, you can still do
exactly the same. You can still look at
the statistic which is now normally distributed
factor rather than a scalar. You have a quadratic form and that is going to have a
Chi-squared distribution. That's all fine but the problem is if you try to basic
confidence interval on this, it may be that in fact the confidence interval is empty for a particular dataset. Even if the model is
correctly specified, it may well be that
there is no value of Beta that is in that confidence set as I referred to that earlier yesterday when I was discussing Bayesian methods. I find it extremely
objectionable to have procedures and the
correct specification that could actually lead to confidence intervals
that are empty. That to me seems a somewhat
meaningless exercise. It suggests maybe that
this is not a good way of constructing confidence
intervals in this case. Note this cannot happen in
the just identified case that the IV estimate itself is always going to
be in the confidence set. This really is
something coming from the degree of
over-identification. It's a problem that gets much worse if we have many
instruments that 0.5 percent of the time you're going to have an
empty confidence set. Let me be clearer, it's not that the confidence interval is
not valid in that sense. It does give you
confidence 95 percent of the time but it
will, in the end, look like 95 percent of the time you get
the whole real line, and five percent of the
time you get nothing. That does include the truth
95 percent of the time, it just isn't very informative. it's been a number of ways of addressing that problem
one as suggested by Frank Kleibergen to modify
the Anderson-Rubin statistic. Instead of looking at
the correlation between the instruments
and the residual, he looks at the
linear combination of the instruments using the estimated first stage
coefficient that is essentially looking
at the correlation between the residuals
and a single instrument. Even though there is
this estimated Pi in there that doesn't really matter for the asymptotic distribution. We lose exactness but you get approximate Chi-squared
one distribution. You can use that to construct a confidence
interval and that's going to be valid asymptotically uniformly in the
parameter space. Then there's some work
by Moreira who's focused on improving these tests. The key idea is a
technical notion that's called similarity
where you want to ensure they test at the same rate
rejection probability for all values of the nuisance parameter
which in this case is the Pis by adjusting
the critical values. Thinking back about
Kleibergen statistic which would have a
Chi-squared distribution. If you want to make this
into a similar test, you would adjust
the critical values in a way that asymptotically
they would look like Chi-squared quantiles
but infinite samples that will give you
better coverage. Another place you could
do this is by using a likelihood ratio test and adjust the
critical values there. It looks somewhat more
complicated than actually is and whereas in their program
this up pens status. I think actually on his website
you can find the link to the data program
and implement this. Final comment on the
just identified case or the case with low degrees
of over-identification. That's more conceptual points
and somewhat of a concern. All these confidence
intervals and what these confidence intervals
set out to do is be valid uniformly over
the parameter space uniformly with respect
to the value of Pi 1. They achieved the
goal but part of that means that it's not valid conditional
on the first stage. What do I mean by that? Suppose what you actually
do is do the first stage, look at the coefficient, look at the F statistic there. Then if you see that the
first stage is very weak, you decide to just abandon the project and do
something else and only continue with the
second stage if you get a sufficiently high
F statistic here. Then the large sample
coverage probabilities wouldn't be the same
as the nominal months. Now the question then is, what do people actually
do in practice? My guess is, probably judging from the
Ingres and Krueger paper where they spent I think about five pages
discussing the first stage, partly to substantiate
the argument that there is validity to their story that
compulsory schooling laws affected the average
education levels. I think in a very
sensible way where you have it in theory why the instruments would actually affect the endogenous regressor. Part of the analysis involves checking whether the story
is consistent with the data. But if you do that then in these weak instrument
premise cases you would still have a problem with these alternative
confidence intervals. There has been some work in their developing methods that would be valid conditional
on the first stage. There's some very interesting
work by Kyode and Michael Johnson
but there is still somewhat difficult
to apply I think. I mean, he would have raised
that here as a concern that it's not always easy to. There is still some concern with these confidence intervals. The last part on many
weak instruments. Here, this is in
my view actually the most serious concern
with these weak instruments. As I had showed earlier, the example of two-stage least squares with completely
irrelevant instruments leading to what would look like precise estimates
far away from zero. The first point is that
there's no simple fix. Bootstrapping doesn't work. If in fact, you look at
Bootstrap standard errors, they're very similar to what two-stage least squares
standard errors give you. In this case, the
theoretical literature has gone a couple of
different directions. Some of the literature
has focused on getting better inferences about
the standard errors for conventional estimators or alternative
confidence intervals. In addition, there's a whole
bunch of new estimators. People have looked
at various members of the K-class estimators, these various versions
of fuller estimators, bias-corrected for
instance of that. I was thinking about what to say about that and I was
talking to Jeff about that. He told me the story that when he was in graduate school, Rob Angle asked at some point in his econometrics class that, "Have you guys heard
of the K-class?" When nobody said yes he
said, "Well now you have." Then he moved on there. I think this is one of these cases but there's
many more estimators in the theoretical
literature that have been used in the applied literature. I don't really see a lot of reason why that should change. I think one of the most important results is from this Bekker paper. Bekker derives formalized
sample approximations for some of the
standard estimators, in particular, two-stage
least squares and LIML. He lets the number of instruments increase
proportionally to the sample size of K. The number of
instruments here is assumed to be equal to
Alpha times the sample size. It shows that the sequence
two-stage least squares is not consistent, and that's one way
of demonstrating why two-stage least squares is going to have
poor properties. What is more remarkable
it says that LIML is consistent in that case. But moreover, it shows that the conventional LIML standard errors are not valid. He derives a correction to that based on that
asymptotic sequence. It turns out, even with a relatively small
number of instruments, that adjustment can
be substantial. Here it's in the paper that is a more complicated version
of the standard errors. But if you simplify
this to the case, but it's just a single
endogenous regressor. In the end for the
adjustment looks like is something like this. Obviously this depends on a
bunch of unknown parameters. The coefficient of interest
is in here, Beta 1. The reduced form
variance-covariance matrix Omega is in here. But using estimated versions of these things gives you an
adjustment that I think in practice actually
takes care of most of the problems with LIML and substantially
improve the properties of the associated
confidence intervals. You can see what
can go wrong here. There's a couple of components to this adjustment factor. There's a number of
instruments itself. Even if the number of
instruments is relatively small, if that ratio to the sample
size is relatively small, it can still be large, if one of two things happen. One is if the power of the
instruments is very small, if the sum of the
Pi prime C squared, this is very small. Second thing that can happen
is if this last factor, before we take the inverse of that last factor
is close to zero. That can happen if you
have very large degrees of endogeneity. That's where the dependence of the degree of
endogeneity comes in. But so even if the instruments
are not putting in this adjustment factor is not going to affect
things all that much because at that point, the adjustment would
be very minor. This is something I would actually seriously recommend
people to do in practice. Irrespective of the
number of instruments, use LIML rather than
two-stage least squares and calculate this
adjustment factor. Either use those standard errors or if you find that the
adjustment factor is big, you may want to go further, but certainly that
would save you against a lot of trouble. That's another way of motivating some part of the
Bekker adjustment. I talked a little bit about
this worker Chamberlain. In the Bayesian part we
looked at this problem by modeling the reduced
form coefficients as coming from a random effects
specification, so just setting this up, assuming that the Pis come from a normal
distribution with mean Mu and Pi variance
sigma squared Pi, you can just get the
likelihood function. In contrast to the
likelihood function in terms of the original
large parameter space, now there's a
likelihood function with relatively few parameters. This is likely to work to me much better
behaved in terms of being able to approximate
it by a quadratic function. Quadratic approximation
is much more likely to be accurate in terms of inferences. The interesting thing is that if you look at this random
effects specification, if you look at this
likelihood function based on the random
effects specification, if you fix the variance of the first states coefficients, you fix sigma Pi squared
at a large value, the resulting concentrated maximum likelihood estimator for Beta 1 turns out to essentially be two-stage
least squares. It's implicitly two-stage
least squares is based on the presumption that the
first stage coefficients collectively are very important, are far from zero, that
the variance is large. I interpreted it yesterday from a Bayesian perspective as saying that the two-stage
least squares put a lot of prior mass
on this variance of the first stage
coefficients being large despite the fact that what we are
actually concerned with, these first-stage coefficients
are all very small. If in fact you use
another special case of this random effects estimator
and you fix the mean of the random effects
distribution at zero, you assume omega is known. Then we showed that the random effects squares and maximum likelihood
estimator is in fact identical to LIML.. But again, just like
the Bekker analysis, the random effects
calculation suggests that the conventional LIML standard errors are too small. He suggests adjustment that is very similar to the
Bekker adjustment, other than it replaces the average square parts of the first stage by an
estimate of Sigma Pi squared. That'll tend to be smaller. The adjustment we suggested is actually ends up being even bigger than the Bekker one, even though that's probably less important than the fact that
it's being adjusted at all. Two more points in a
very different approach. It's a very interesting
approach by Donald and Newey. They look at the question of choosing a subset
of the instruments. The way they formulas that
is by setting up a sequence of instruments and deciding
where to cut things off. Instead of using all
the 180 instruments, if you're willing to order them, you could imagine that
you decide that using all 180 of them is not an
attractive option. But you may want to use more
than just the first three. They suggest that
procedure for deciding how many instruments
to use based on an approximation to the expected squared error of the estimator. That's a very general idea
and one that's very promising and in other settings as well. For example, in
settings where you have generalized method
of moments settings, as many moments, but it's still somewhat difficult
to actually implement. Finally, how much evidence is there of how well
these things work? Very extensive set
of simulation is done by Alfonso Flores, where he can pass a whole
bunch of these estimators, some of the ones that
I mentioned before as never really
having been used. In fact, he doesn't include the LIML with the
Bekker standard error, which is the one that
probably should have been included in there as well. It looks at coverage
rate and bias, and he finds that this
random effects estimator works very well. But in the cases he looks at, it's generally very
close to LIML. But in these
simulations it hasn't matched by the
standard error because it has essentially this
adjusted standard error where the LIML version
that Flores uses as the conventional
standard errors and they're far too small even in the settings that
the Flores looks at. Again, let me stress the
point that in practice I think there's very clear now that if you're using
a lot of instruments, you don't want to use two-stage least squares,
and in in my view, there's pretty much
no reason to use two-stage least squares rather than LIML other than just
identified case are the same. If you do have over
identification, I would encourage people to adjust the LIML standard errors and the simplest way of doing that is just using that
Bekker adjustment. Yes? MALE_1: [inaudible]. Guido Imbens: No. You would just plug in the
estimated values here. Here, first, you get the point estimate,
you get Beta hat, you get estimates for the reduced form
variance covariance and you just plug it in here. The only two components
you don't know here are Beta-1 and Omega. You just plug this in
into that formula. I don't think that affects the properties of the
confidence intervals very much. 