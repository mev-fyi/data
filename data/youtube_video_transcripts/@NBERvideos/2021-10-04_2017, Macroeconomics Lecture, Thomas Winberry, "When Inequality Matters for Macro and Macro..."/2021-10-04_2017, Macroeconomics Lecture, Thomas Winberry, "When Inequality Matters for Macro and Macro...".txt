this project is about which is the interaction between macro and inequality as we now all know over the last 30 years there's been a lot of work in developing heterogeneous agent macro models in order to allow us to think about exactly this interaction and furthermore a lot of recent work has shown that these models can have very different implications for the effects of monetary and fiscal policies on the economy relative to what standard representative agent models have to say but despite this sort of promise we think that heterogeneous agent models have yet to make their way into sort of standard macro toolkit that policy makers use to think about the effects of their policies on the macro economy and broadly speaking there are two main excuses given for this this abstraction the first is that heterogeneous agent models are just difficult to solve because the distribution of these agents is a state variable the second is that even if you're willing to overcome those difficulties there's kind of a perception out there among some that aggregate dynamics with heterogeneity just aren't all that different from aggregate dynamics in a representative agent model so why should we even bother the main point of our paper is that both of these excuses sort of are less valid than you may have thought okay and that heterogeneous agent models deserve a sort of central place in this macro toolkit so to this end we make two main contributions in the paper the first is to develop an efficient computational method to solve a wide class of heterogeneous agent macro models with aggregate shocks and to make the method as accessible as possible we've posted an open source suite of matlab codes a toolbox online that we think is easy to use the second contribution is then to put that toolbox to work to illustrate exactly this interaction between inequality and macro in our title in our first application we're going to start from a model that is a model of micro behavior and one that matches key features of the distributions of income and wealth and npcs in the micro data and so this model also matches features of dynamics of aggregate consumption and aggregate income and the macro data we're going to be thinking about these features of aggregate dynamics that were first put on the table in in a macro annual paper in 1989 by campbell and mancu also discussed by cristiano so we're going to return to that then in our second application we're going to think about how aggregate shocks themselves shape the dynamics of these distributions and the dynamics of inequality moving forward over time so what i'm going to do today is just go through these two main contributions in turn starting with a computational methodology so at a broad level our computational method just involves two main steps let me lay them out now and then i'll get into details very quickly so the first step of our method is to solve for the steady state of any given model in which there are no aggregate shocks but there are still idiosyncratic shocks driving heterogeneity among individual agents we solve for the steady state using global nonlinear approximations people who work with heterogeneous agent models do this kind of thing all the time then the second step of our method is to compute the aggregate dynamics of the model using a local linear approximation with respect to aggregate state variables so the main sort of advantage of this local linear approximation is that we can include the entire distribution in the aggregate state i'll show you how we do that i just want to note that this idea of mixing non-linear approximations at the individual level where heterogeneity is important linear approximation at the aggregate level where aggregate shocks are small is also in a paper by michael ryder and goes even back before that okay so what exactly do i mean by this sort of mix of global mix of nonlinear and linear approximations i think in the time that we have here today it's easy to easiest to explain what we do by just sort of direct analogy with how you would linearize a very simple representative agent model which is the stochastic growth model so here's the stochastic growth model in continuous time so our methodology requires the model to be specified in continuous time that's not for any deep conceptual reason we found that continuous time has a number of numerical advantages that are very useful in solving large-scale models like in our application so there are three equilibrium conditions of the stochastic growth model in continuous time there's an euler equation for consumption evolution of the aggregate capital stock and the evolution of aggregate productivity okay so the three variables in this model just to be very pedantic are aggregate consumption ct which is the control variable aggregate capital kt which is an endogenous state variable aggregate productivity zt which is an exogenous state variable so how would you linearize this model you would just first compute the steady state values of each of these three variables use that to compute a first order taylor expansion of the equilibrium conditions around steady state so that gives you a linear system of this form here where the hats denote deviations from steady state and then you would solve that linear system using standard methods like so for example uh eigenvalue eigenvector decomposition that's exactly what we do with heterogeneous agent models as well but as i said things get a little more complicated because of individual heterogeneity so in particular instead of aggregate consumption being our only control variable that has to satisfy some aggregate euler equation now our control variable at any point in time is an entire value function of individuals over their individual state variables let's call those individual state variables little s at any point in time we're going to approximate this value function over a grid of individual states s1 through sn so the value function is represented as a big vector over that discrete size grid space so our control variable is a value function that has to satisfy some hamilton jacobi bellman equation similarly instead of aggregate capital being the only endogenous state variable now the endogenous state is the entire distribution of agents over this grid of individual states s1 through sn so let's call that distribution g and again that's just going to be represented by a big vector over the grid of individual states then the aggregate shock is the same because that's not affected by the presence of heterogeneity right so it's more complicated but we can linearize this model again following sort of same steps as before first we compute the steady state of this model which there are no aggregate shocks but here they're idiosyncratic shock so this involves solving for the stationary distribution and value function then second we can linearize the equilibrium conditions around that steady state so this gives a linear system of the form on the top of the slide and then we can solve that linear system using eigenvalue eigenvector decompositions same as before and in small models such as the original kusel smith model which i'm going to talk about in a second the size of this individual state space that n is something like two grid points for individual idiosyncratic shocks maybe 100 grid points for wealth so the value function is a vector with 200 entries the distribution is 200 entries that linear system has something like 400 equations it's very feasible and in fact fast to solve that using eigenvalue eigenvector decomposition in bigger models like in the applications i'm going to talk about we have more individual states that in is going to be something like 66 000. so the total linear system is something like 132 000 equations it's just not feasible to solve a linear system of that size so in the paper we develop what we call a model free reduction method to reduce the size of that linear system in a way that still preserves accuracy okay so we reduced the value function using just by approximating it with quadratic splines that's really relatively straightforward i don't really want to talk about it today but i do want to talk about how we reduce the distribution on this slide and the key insight that we use to reduce the distribution is kind of building off of crew and crusade smith's original paper but even though we have this complicated distribution moving around in potentially complicated ways a lot of that complication is unnecessary for what agents are actually using the distribution to do which is the forecast prices so crucial and smith suggested let's ex ante guess a set of moments which approximate the distribution and check x post those actually accurately forecast prices what our toolbox does is instead allow the computer to choose which features of the distribution that it needs in such a way to guarantee accuracy of those forecasts okay so specifically what we do is we take that big in-dimensional distribution g that big 66 000 size vector and projected onto a smaller k dimensional subspace spanned by these basis vectors x1 through xk so this reduces the size of the distribution because now all you need to know to know the distribution are those coefficients gamma 1 through gamma k so now the name of the game is how do you choose those basis vectors to get as accurate an approximation of the dynamics of prices for as small of a k as possible so to do that we went to the engineering literature and used some state-space reduction tools in particular use what they know as what's known as the observability criterion which in economics language just says choose the basis vectors so that the impulse response function of the prices in your model what you care about and the reduced model match the impulse response function of those prices in the full model up to some order k now the engineering literature is all about backward looking systems because this is an economics problem we also have forward-looking decisions which impact the evolution of the distribution we talk in the paper about how we adapt the tools to the forward-looking case so since the distribution reduction kind of builds on this krusell and smith idea it's kind of a nice reality check that it recovers their approximate aggregation result in the original model that they were looking at okay so this is showing that result here we plotted the aggregate impulse responses of output consumption and investment to a tfp shock there are actually two lines in each of these graphs there is a solid blue line which is the full model where we have the full distribution and the dash red line where we reduce the distribution by projecting it onto a k equals one size subspace you can show in fact that at k equals one we're just picking up exactly the aggregate capital stock so the result is that these lines are on top of each other dynamics of the model when you only keep track of the aggregate capital stock just the mean are exactly the same as when you have the entire distribution again i want to emphasize that there's our methodology doesn't rely on this approximate aggregation to work but it's a nice reality check that we actually get it when we know that we should and in fact in the application i'm going to show you in just a second approximate aggregation does not hold in exactly this sense we need something like 300 basis vectors instead of just one to get an accurate approximation of the dynamics of the model all right before going to the application i just want to show you that this methodology is very very good at solving the crew cell and smith model so to be able to compare it to sort of other methods in the literature we calibrated the model as in the special issue of the jedc which was comparing a bunch of different algorithms for solving this crucial and smith model the first table here on the slide shows uh how long it takes for us to solve our toolbox to solve the model we don't reduce the distribution at all it takes about a quarter of a second to solve the model if we reduce the distribution it speeds up a lot and we only need about a tenth of a second to solve the model in comparison the fastest algorithm in this comparison project was about seven minutes so our methodology is i did the math last night something like 3 500 times faster and that's important for solving these large scale models in the applications it's also important for thinking about eventually estimating these models going forward okay so the methodology is fast in terms of accuracy the key sort of approximation that we're making is this local linear approximation around the steady state without aggregate shocks and so the quality of that approximation is going to be very good for small aggregate shocks but deteriorate for large aggregate shocks so that's what i'm showing here in this table and plotting the error statistic they're doing hon air statistic is kind of one of the the main error metrics in the literature the error statistic is very small for small shocks and grows for large shocks the column there in blue is for the calibration in this jedc comparison project the denhan error in our in our method is about three times smaller than the most accurate algorithm in this comparison project okay so our methodology is very good at solving the crew cell and smith model but that's not why we started this project we started this project in order to think about this interaction between inequality and macro so now i want to put the methodology to work and to do that we're going to extend the krusell smith model a little bit in order to match more features or key features of the distributions of income and wealth and npcs in the micro data so i don't have time to get into all the details there in the paper but the spirit of the model is on the household side to build a two acid and complete markets model like in kaplan biollante or kaplan mall violante the households in this model are hit by idiosyncratic productivity shocks and they trade in two assets to buffer against these shocks a liquid asset which earns a low return but is relatively costless to adjust an illiquid asset that earns a high return but is relatively costly to adjust production side of the model is going to be simpler there's just an aggregate production function cobb douglas and capital and labor tfp we're going to assume is non-stationary and in particular the growth rate of tfp follows a continuous time ar1 process and this is important because we're going to match the growth rate of income to think about the data and finally market clearing is such that aggregate capital is just the sum of all the illiquid assets in the economy and the sum of all the liquid assets are in fixed supply okay so now we want to use the first application we want to use this model in order to think about how inequality in matching distributions is important for thinking about aggregate dynamics and to do that we're going to return to this question raised by campbell and mancu in the 89 macro annual as i said was discussed by christiano as well how can we account for certain features of the dynamics of aggregate consumption and income and i'll come back to what those features are in just a second first let me explain how we calibrate them all and how we're going to address this question so kind of calibrate the model in two steps so first we calibrate the household side of the model to match as i've been saying these key features of distributions of income and wealth and npcs the important thing that comes out of here is there's going to be a substantial fraction of households who choose to hold all of their wealth in illiquid assets because it earns a high return and none of their wealth in liquid assets because it earns a low return these saltos are going to have high npcs because they in response to shocks they don't have liquid assets to adjust and they don't want to pay the transaction cost to adjust their illiquid assets so this is plotting the calibrated sort of mpc function in our uh the steady state of our model these are the npcs out of a one-time unexpected 500 increase like 500 cash windfall and this is how much these households consume of that windfall but over a quarter so the households which have zero liquid assets have much higher mpcs than the other households in the economy okay so that's the sense in which the household side matches micro consumption behavior we then calibrate the tfp shock process on the firm side of the model in order to match the autocorrelation and the standard deviation of aggregate income growth so the spirit of this exercise is to have a good model of micro behavior a good model of aggregate income dynamic and ask how does the model do on the joint dynamics of consumption with income so we think about those joint dynamics through the lens of two sets of statistics the first are the sensitivity of consumption to predictable changes in income so we look at a bunch of different statistics about the sensitivity in the paper for the presentation today we're just going to focus on the instrumental variables coefficient of a regression of consumption growth on income growth instrumenting income growth with past income growth okay so it's predictable to the extent that this past infra this past observations are in information sets in the data that coefficient is about 0.5 so a lot of these predictable income changes pass through the consumption the reason campbell and manc and others in the literature think this is an interesting statistic to look at is because this kind of predictability or this kind of sensitivity is not consistent with permanent income behavior permanent income consumers will already build in these predictable income changes into their consumption plans and consistent with that the representative agent version of our model which is not exactly permanent income but similar has much less sensitivity than in the data in contrast our heterogeneous agent two asset model features substantial sensitivity as in the data and this is because there's this substantial fraction of households who are not permanent income who have no liquid assets and are essentially handsome now so the model generates sensitivity the second set of statistics that we look at are the smoothness of consumption relative to income just the fact that consumption is less volatile than income so here i'm reporting the standard deviation of consumption growth relative to income growth and the data consumption is about half as volatile as income which is the fact that our two asset model hits quite well surprisingly well and this is because and this is in spite of the fact that we have this sort of ex sensitivity over short horizons as households are hand to mouth over longer horizons they still want to smooth their consumption there's still permanent income over long horizons so they're going to generate this smoothness okay so our model matches this these two statistics that campbell and monkey are thinking about sensitivity and smoothness and in fact it's competitive or it's sim the behavior can be compared to a spender saver style model in the spirit of campbell and mancus uh macro annual paper so here we just took the representative agent version of our model and imposed that uh an exogenous fraction of households in that model are permanently hand to mouth calibrated the fraction of permanent hand to mouth consumers in order to match exactly the sensitivity component and that's the performance of the model there as well so in some sense this model has been reverse engineered in order to hit these aggregate facts the strength of our models instead that it was calibrated to match micro behavior and it endogenously generates this sort of sensitivity and smoothness behavior okay so that's our first application how matching distributions and our inequality can matter for aggregate dynamics our second application is how aggregate dynamics can themselves shape these these distributions going forward turns out that the cobb douglas production function that we've been using so far is not particularly well suited to study that because laboring the distribution of labor income is basically exogenous in that model it only comes from the distribution of labor productivity shocks which are exogenous so for this application we modified the production structure a little bit we assume that the aggregate production function is there in the in the slide so in particular we've seen that they're going to be two groups of households low-skill households which are the ones with sufficiently low labor productivity shocks and high-skilled households which have sufficiently high productivity shocks these households are not perfect substitutes with each other or with capital we calibrate these elasticities to generate capital skill complementarity like in crusal ohanian realtor violante and now we're going to look at what is the effect of a negative shock to unskilled specific labor productivity's etu we're going to look at a recession which disproportionately hurts unskilled workers relative to the high skill workers so kind of used to thinking about aggregate impulse responses and how aggregates respond to these shocks what we're really interested in here is how the whole distribution responds to these shocks i'm going to look at some distributional impulse responses so on the top slide is the response of the wage weight for unskilled and skilled labor the blue line is showing you the un the unskilled wage falls by quite a lot in response to this this negative shock to unskilled specific productivity because these workers are not perfectly substitutable the skilled wage doesn't fall by as much so the skilled premium opens up a lot in this recession this generates a big increase in labor income inequality across households the low-skilled households are hit particularly hard it also generates an increase in consumption and equality across these households as well these increases in equality are very persistent they're much more persistent than the aggregates okay so in that sense this is a recession that hits low-skill households particularly hard this in turn generates a very sharp drop in aggregate consumption that's what we're plotting here in the blue line because this negative shock hits the low-skilled households these low-skilled households are more likely to be hand to mouth they can't use borrowing to buffer against the shock they have to cut their consumption the red line is the effect of the shock in the representative agent version of this model that abstracts from all of this sort of heterogeneity and inequality there the consumption response is is much is much smaller than in the heterogeneous agent model okay so there's this is sort of again the spirit of this application is to show you how the dynamics of inequality and getting these differential responses across the distribution are important and matter for aggregate dynamics okay so let me just let me conclude there and remind you what our we think our two main contributions in this paper are so first we developed this very efficient methodology for solving a wide class of heterogeneous agent models and then put it to work in these two applications to think about the interaction between inequality and macro so now that we have this methodology up and running there are a bunch of things we want to do one thing that we're particularly interested in is thinking about estimating macro models using micro data and micro behavior 