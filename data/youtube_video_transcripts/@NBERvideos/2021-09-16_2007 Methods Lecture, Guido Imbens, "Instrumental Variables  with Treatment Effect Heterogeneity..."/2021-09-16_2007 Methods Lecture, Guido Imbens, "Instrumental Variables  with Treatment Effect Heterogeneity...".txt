Guido Imbens: I'm
going to talk about instrumental variables. This is the third lecture on the evaluation type problem set. I'm going to talk about
instrumental variables instead of duties in
settings where we want to liken the enkephalin in this case allow for a
lot of heterogeneity. Here's the outline. Now give some introduction, talk a bit about
the basics that are quite local average
treatment effects. Whether the key result
is going to be that even if the key assumptions are satisfied in some
mental variables, is very rarely going to give you the average effect for
the whole population. It will essentially only
do so in two cases. One, when the effect of the treatment is constant,
that's a very strong, very difficult to
believe assumption, as well as in settings where the instrument is
extremely powerful. This is a result known as sometimes as identification
at infinity, where you have an
instrument that is so powerful that it moves the probability of receiving the treatment all the
way from zero to one, essentially means that
for some sub-population, now, you have a
randomized experiment. That's clearly also
very rare occurrence. In most cases at best, if we can estimate the effects for some sub-populations and we'll talk a little
bit about that and characterize
that sub-population and see what we can
learn about that. I'll talk a little bit
about when there's at least some credibility to extrapolating that
to the population. More practical
problems of how to incorporate covariates,
what to do. It's sad things about us. Multi-valued instruments are multiple instruments
and I'll talk a little bit about the case where the endogenous regressor, it's not just binary, but it takes some more values and how some of these
results extend. Here, let me summarize
the main points. The key results in the recent literature is that if we allow for heterogeneity, which both has a lot of empirical evidence that
there is heterogeneity. As well as their concern that the assumption of
having a constant effect, this is very dubious. If you're outside
of that setting, then instrumental variables still estimates average effects. But the specific average depends on the choice
of instruments. Key theme is going to
be that if you were to do this using two
different instruments, you might get different results. Not so much because
the instruments, that, because one of the
instruments is not valid, even if they're both valid, they may just be estimating averages for different
sub-populations. Doing the type of
specification tests where you, based on overidentification
coming from multiple instruments
doesn't really work in this setting because one of the assumptions
that it's being maintained if you want to test the validity
of the instruments, they said the
effect is constant. Absent that assumption to different instruments
can just estimate different objects. In general, the terminology for the sub-population
where we can estimate the average effect is that
these are the compliers coming from this
canonical example. We have a randomized experiment
with non-compliance. Some of the individuals we are assigned to the
treatment don't take it. Some of the individuals were
not assigned to treatment, still managed to take it and the ones in the middle
will do as they're told. Effects it will always
do as they're told, will comply with after
their assignment is I will be referred
to as the compliers and that's going to be
the subpopulation that instrumental variables
is estimating the average effect for. There's nothing in
this simple setting with a binary instrument that a binary
endogenous variable, there's nothing much
you can do about it. That's the only population
that you get and if that's not the one you want, you may have to
learn the one you get to like the one you get there and we'll think a little bit about
it the better we can extrapolate from there. But the first part is
really about thinking what the population
looks like and how you can characterize it. Instead of the setup like in the ankle
found in this case, it's good to keep very separate. The substantive
assumptions that theory may help us with that are all in the form of a perimeter, in the form of conditional
independence assumptions from functional form assumptions and as well as extrapolation. That's really the large
part of the progress that's been made in this
literature or in the 90s. Let me just briefly remind people the standard setup so we're going to do
this in the case. For the time being, we're going to ignore the covariates. Ignore any additional
exogenous covariance. We're going to look
at a case with a single endogenous
regressor that is binary. In a single binary instrument, in this case, is the standard instrumental
variables estimator. You can do that two-stage
least squares or liminal. Everything is all
going to give you the same because we're in
the identified case. Estimators look like
it's the ratio of sample covariance is
in the limit where they're estimating is the ratio of population that covariances. You can get the proximate
large sample distribution from this by using Delta method, you're going to get asymptotic normality
for both numerator and denominator and the ratio will be reasonably well
approximated by a normal distribution as long as the denominator
is not too small. Now, that's a little quick. Denominator not
being small will be the part of one off to
the Wednesday lectures, we talk about weak instruments considerable concern
with the properties of these methods when
the instruments are relatively weak in
the sense that they are only weakly correlated with
the endogenous regressor. I'm not going to talk about
it now as party on Friday. I will also make the point
that in there just identified case that is actually not
not that bigger concern, that's a much bigger concern
in over-identify case. What I just described was the standard setup, the
standard estimator. I'm going to go back to the
potential outcome framework for setting this up in order to formulate the
critical assumptions. Let's go and start again with the two potential outcomes, Y_0, Y_1 month for each
value of the treatment. W is the value of the
treatment and what we observe, the outcome we observe Y is 1 if W is one
and Y_0, w is 0. But now the first
innovation is that we do the same thing for the
level of the treatment. We got to think about there being two potential outcomes for the level of the treatment, W_i0, W_i1, representing
the values of the treatment given
the two values for the binary instrument. One example that I'll use repeating anesthetic give
some numbers for it in the end is anguish
draft lottery example. See that the instrument is
ready your draft eligible, whether you have a, meaning, you have a low draft
lottery number. We'll make that binary so you either have your
either draft eligible, you have a low lottery number or you have a high lottery number. W is an indicator for whether
you serve in the military so wi0 tells you for
a particular person, whether that person
would have served had a high lottery number. If this person had had
a high lottery number, they would not have
been eligible. They would not
have been drafted. W_i0 tells you whether
that person would have served in the military and W_i1 tells you the better
that person would have survived had they had
a low lottery number, had it been drafted and so
will the extra observe own. It's only better an individual served in the military and what their draft eligibility
status while we observe Wi, which is W_1 if c is
1 and W_0, if C is 0. It will be observed
here and there's still abstracting
from any covariates. Is an instrument, it's
a binary instrument. Ci, binary treatment, W_i which is W_i
evaluated at CI. And the outcome Y_i, which is Y_i evaluated. W_I evaluated at NCI. What probably going
to be interested in is comparisons
of Y_i1 and Y_i0. The key assumption,
and I actually want to do it in two ways. First, let me set
it up this way. One way of formulating
the assumption is that the instrument is independent of all these potential outcomes. These four potential outcomes, in this case, Y_i (0), Y_i (1), W_i (0), and W_i (1). This really captures two
parts of the assumption. That's what I'm going to
break it up in a second. First is that it requires that the instrument is as good
as randomly assigned. That it's, in a lot of examples, like a randomized
experiment with non-compliance and
draft lottery example that very credible in those cases actually was
physical randomization. I see that part of the
assumption is very credible. But this way of
formulating the assumption already embodies the
second assumption that is substantively very
different namely that the instrument doesn't
directly affect the outcome. In that sense, it's actually
useful to split that up a little bit and in effect extend the
notation a little bit. Instead of starting with these two potential
outcomes for Y_1, Y_ 0, let's think of there
being four outcomes, Y_i indexed both by the
instrument and the treatment, so Y_i of W_0 corresponds to the outcome that
you would see if this individual was not draft
eligible and didn't serve, and the other three
combinations similarly. Now, formulating the
assumptions this way, it's a little easier. But there's some costs. Let me actually firstly the assumptions and
then come back to that. Now, we can split this
assumption up in two parts. One is that we have
random assignment. The instrument is independent of all the six potential outcomes. That's very credible here if you actually have physical
randomization there. But now that's not enough. In addition, we need what is called the exclusion
restriction that Y_i of C_n W is the same as Y_i
of C_n for all values C, C prime, and W. What this is really saying is
that for any individual, changing their C doesn't
change the outcome if we keep them at the same
level of the treatment, and so the reason that the awkwardness in formulating the
assumption this way is that it's a little hard
to think about how you keep the treatment constant if
you change the instrument. But that's partly reflecting the fact that we didn't need this assumption to
be quite so strong, we going to need is
only to hold for some sub-populations and
for those sub-populations. This awkwardness doesn't arise
as we'll see in a second. But the big advantage of separating the
assumption in this way, it is very clear what comes
from the randomization, namely the guarantees
assumption too here, and what is meant by saying that the instrument doesn't
directly affect the outcome. But it really refers to is the effect of a manipulation that change is only
the instrument, but doesn't change the
level of the treatment, and the key assumption
there is that doesn't affect the outcome
for any units. Randomization per se doesn't have any bearing on
that assumption there. That is a substantive one that isn't necessarily satisfied in this non-compliance examples or even in the draft
lottery example. I'll come back to arguments why that assumption
may not be satisfied there in a little bit when
we'll look at those data. But given this setup, a key feature is that
it's important to think about individuals' responses
to the instrument, and the way to capture that
is to think about a pair of values W_i(0), and W_i(1). Both of these are binary, so that gives us four
different groups. In the draft lottery example, the top-left group, and this is described here as
never take us individuals, we wouldn't serve
in the military irrespective of what their
draft lottery number there would be, irrespective of whether they
were draft eligible or not, they wouldn't serve
in the military, so you can imagine this could
be individuals who were medically unfit to serve in the military where irrespective of what the lottery number was, their lives wouldn't
really be affected and suddenly their veteran status
wouldn't have been affected. The bottom right was described
as the always takers, individuals who serve
in the military irrespective of their
lottery number. Again, instead of there, you can think of that in the English draft lottery example as volunteers, people who were willing to
serve in the military and didn't really need to pay any attention to the
draft lottery number. We would just go
ahead and sign up. Last two groups, and the
most interesting ones there, the W_0 is 0, W_1 is one group described
here as the compliers. These individuals
would not serve if they had a high
enough lottery number and we're not draft eligible. But they would serve
in the military if they go the low lottery
number and they get drafted. For them, clearly, the draft is going to have a big effect. If you have a high
enough number, you get out okay. If you get a low enough, if you've got a low enough number, you would end up serving
in the military. The last group is what is
called here the defiers. These individuals would
do exactly the opposite. If they had a low
lottery number, they would get drafted. They would manage to get out of it if they had a
high lottery number, and they wouldn't get drafted.
They would volunteer. In this particular example, this group will
play very important role in the sense that we going to rule out the
presence of such units, or such individuals, and very credible here. There's not necessarily so
credible in other cases. One example would be if you have a random sample of
individuals will get screened for a
particular program, we have two different sets of people doing the screening. You could imagine
that one group is more strict and the
other more lenient, and so the stricter group
leads to a lower rate of take-up of the program
than the other group. But that doesn't mean that there would not be
some individuals who would be taken in if they were screened
by the strict group, but not by the other group
and the other way round. In that case, there might well be individuals of this type. But here, it seems
very plausible that if individuals were willing to serve even if they
didn't get drafted, that it would also serve
if they did get drafted. A lot of these examples, it's very credible to rule
out the presence of defiers, and it's also very
extremely useful. Before doing that, now let me make one more comment on that. We can directly tell what
the type of unit is. We can rule out some types for a particular individual based on what we see about them, based on the pair of
CIN and W_i but we won't be able to rule out all possible
alternative types, and so without making any
additional assumptions, we can for each individual tell that they belong
to one of two groups. If you see someone who wasn't draft eligible
and who didn't serve, you don't know whether
they would have survived had they been drafted, so you don't know
whether their behavior is consistent with
being a complier, but it's also consistent
with being an effort taker, is consistent with them not serving had they gotten drafted. Same for the other three groups. You can rule out two
of the four types, but not more than that. To make some progress there, we're going to make this assumption that
there are no defiers. Originally it was called the
monotonicity assumption. Here written I said W1
greater than or equal to W0. But it really, what it
means is that we rule out this defier group. There's nobody always doing the opposite of what
they're assigned to. This doesn't really
impose any restrictions on the joint distribution of W and Z. It just rules out some of the configurations of the
latent variables in that case. It's going to help us infer what type particular
individuals are. Now, as described for the
draft let's say example. This assumption
makes lot of sense, said there makes a lot of sense in a lot of other applications. It's also directly implied by a lot of latent
index selection models. If you have a model for participation that
postulates that individuals look at
some thresholds. If that's positive, they'll
participate in, if not, they won't here, but
the linear threshold, that's a linear
function of C plus an additive independent and
observed component Epsilon I, then that immediately satisfies this
monotonicity condition. Especially with additional
covariates in this equation. It's very hard to see what
else is being assumed there. In the end, the only
thing we really need is this ranking of W1 and W0. But I just want to
make the point that this assumption has
implicitly been made in a lot of work predating the recent instrumental
variables literature by using these selection models. Now, what is the implication of monotonicity assumption
for these compliance types? It implies that for
some individuals, we can now infer
what type they are. You look at the individuals for whom you see not
serving in the military, even though they
were draft eligible, they would also not
have served had they not been drafted, they must be never takers. Similarly, if you
see individuals volunteer for the military even though they
didn't get drafted, you know, they would also have served had they been drafted. So they must be always takers. It's only the other two
groups where we still can't tell what type
an individual is, there's still two
possibilities for their types. The second implication is more subtle implication that given this monotonicity
assumption, we can estimate the
population proportions of each type given it. Here we use the random
assignment assumption as well. But if you look among the sub-sample that
was not drafted, they had a high lottery number. If you look at the proportion that still
serves in the military, there are always takers. You can estimate the
proportion of always takers, but a conditional proportion, but a conditional frequency. You can do the same
for never takers, you can infer the proportion
of always takers, of compliers given that the three proportions
have to add up to one. The point being given
these two assumptions, we can now estimate the probability of
the different types. Then the second and this is
different ways I'm making this point and this is not necessarily the
most elegant one, but it's a very
simple one in a way. If you look at average
outcomes in the four groups, remember what we see is not directly the compliance types, but we see the observed
treatment status, and the observed instrument. We can estimate
average outcomes in these groups, two of the groups. That's the average
for just one type, either for never takers or for always takers in two
of these groups. In the other two groups, that's a mixture of
average outcomes for compliers and never takers. Given that we can estimate the population proportions of these three different groups. We can then disentangle
these mixtures and infer what the average
value of the outcome is for compliance
given the treatment Y1 and average value
of the outcome for compliance without serving
in the military by zero. Given that, we can estimate both of these averages separately, we can estimate that difference. That's what we call the local
average treatment effect. The average effect
for compliance. The other part of that incident that's less clear from
this calculation, but instead of fairly
simple algebra, you can show that that's
the same object as what is being estimated by the standard instrumental
variables estimator, this ratio of sample covariance. Let me step back into that, summarize these two points. But I said, we can
actually estimate an average effect here
under these assumptions, namely the average
effect for compliance. Second is that standard
instrumental variables actually delivers that object. Traditionally, people looked at the instrumental variables estimator in this
linear model setup where they assumed that
the effect was constant, then obviously that the
point is for a immediate. But even if you're not willing to make that
constant effect assumption, which clearly has very
little credibility, then it turns out you still get a very well-defined average, namely the average for
the compliers exposed. It's also very clear that that's the only thing you could
possibly hope to get. The defires have been
ruled out by assumption. Always takers are only observed
receiving the treatment. It's very hard to imagine that any credible way of estimating the average effect for
always takers that doesn't rely on extrapolation. Some sense, you could do
the thought experiment. Suppose we actually could
observe directly what the compliance status
was of each individual. At that point these
assumptions would correspond to having uncle found in this given
the compliance type. But you'd have some
groups for whom the probability of treatment
was either zero or one. There would still
not be anything you could do other than
putting them aside and acknowledging that
you couldn't estimate the effect for these groups. The final comment here is that there's no
particular reason why this would have been the effect of the most
substantive interest. This sub-population is
determined by the instrument, but it is the only one you can get without extrapolation, in this case, the
binary instrument. What else could you do here other than reporting
the effect for compliers? Even though you can't
directly estimate the effect for never-takers
or always-takers, you can estimate what the
average outcome is for these other groups at
least under one level of the treatment. One thing that is useful to him, I'll illustrate that
in a little bit using the Angrist draft
lottery example. One thing you can do is compare these outcomes for these
non-compliance groups with the corresponding
outcomes for the compliers. You can compare how
different never-takers are from compliers
without the treatment. You can compare the
average of Y(0) for never-takers with the average
of Y(0) for compliers. Similarly, you can
compare the average of Y(1) for always-takers with the average of
Y(1) for compliers. To clear that, the more
different these groups are the less credible it is to extrapolate the effect for compliance to these
other groups. The other side isn't
necessarily that if these averages are the same that you can necessarily
extrapolate things. But if you find
that systematically the average outcome for never-takers is the same as the expected value of
Y(0) for compliers, you find that both overall as well as for various subpopulations
at some point it clearly becomes more credible
to interpret the effect for compliers as that for the overall population
or at least for the population and also
including never-takers. We'll look at that with this draft lottery
data in a minute. Second set of
extensions I want to discuss here is how to
use covariates here. Traditionally, standard
instrumental variables is two-stage least
squares setup, you would just include
additional covariates additively and linearly in the regression and then combine whatever the
original instrument was with these
exogenous covariates. At some level, that's the
reasonable thing to do, especially if the
instrument is independent of the other covariates anyway. However, it seems slightly
at odds with this focus on allowing for a lot
of heterogeneity in the effect of the treatment. If you want to
take that further, you may also want to have
more heterogeneity in the correlations
between the effects of the covariance
on the treatment. I'm going to look at two
ways of doing that ones with the Heckman selection type setup where instead this
is going to put it in this potential
outcome framework here. You can think of the
selection equation describing the participation
in the program where you add the covariates into this
latent index model as well as an outcome equation
where you add the covariates linearly as well. The traditional version
of these models would have joined normality and you can relax various
parts of this model. In some sense, what I find most concerning in
this type of models is it really imposes a lot of restrictions that are
more difficult to free up on the relationship between the compliance types
and the outcomes. One way of seeing it here is the heterogeneity
of the types is really captured by this single scalar unobserved
variable data high where individuals with very low values of data never participate
and this would correspond to never-takers with intermediate values of
the covariates with correspond to compliers
with high values of data high would
correspond to always-takers. Combined here with this
linear constant correlation between the two
unobserved components, it's going to imply
that the compliers are in between never-takers
and always-takers. There's no particular reason
why that should be true in the data and there's no
particular reason to impose that in the data. In the draft lottery
application, in fact, if you look at this, you'll see that compliers do better
than never-takers. Under the null, the
compliers also do better than always-takers
given the treatment. That's very credible
that compliers are relatively more highly educated
group where we may have higher earnings
potential than either of these other groups
and this ranking of the groups in terms
of participation, this setup where the compliance types
are always ranked in this particular way
wouldn't allow for that type of correlation. An alternative way of
setting this up that doesn't impose any restrictions beyond the exclusion restriction and the random
assignment assumption, is just think directly modeling the potential outcomes
given the covariates. Here, we have these three different types never-takers compliers,
always-takers, never-takers are only
observed in W series state, always-takers are only
observed in the W S1 state, and compliers are
observed in both states. In principle, you have four
outcome distributions for these four pairs of W and T. One way of approaching this is to set up a reasonably
flexible model for these four outcome distributions
and combine that with a discrete response model
for the compliance type. Then here, I just use a
simple trinomial logic model, but you can make it as
flexible as you want. In principle, there are
no restrictions being imposed other than
the smoothness you could just do maximum likelihood here and get potentially a
much richer set of estimates coming out of this where
you would be able to get separate estimates
depending on the covariates as
well as you would be able to get separate effects of the treatment depending on the covariates in a
very unrestricted way. Even though this
type of likelihood function may look intimidating, there's actually
a fair amount of structure on it that'll allow us for fairly straightforward
implementation of this using the EM
algorithm type methods or Bayesian numerical methods, which I'll be talking
about tomorrow. Summarizing this discussion, so traditionally, covariates
in these models have been added just additively,
and linearly. With a reasonable
amount of data, you might want to do this in
a much more flexible way, taking account of the
mixer structure implied by the theoretical model namely
the three compliance types, and free up the distributions for the potential outcomes. Let me illustrate some of these things using Angrist
draft floating example. I'm taking some of these data. Here, the interest is in the effect of serving
in the military. This is data from
the Vietnam War. The outcome of
interests here is log earnings at some
subsequent date. If you look at simple
OLS estimates, you get the numbers there. At the top of the slide, you get a negative but
insignificant effect of serving in the military. Angrist looked at
the lottery number as an instrument for a
veteran status here. If you look at the number
of individuals with high and low lottery
number as well as individuals who served and
didn't serve in the military, you see that individuals
that we see as one were draft eligible were much more likely to
serve in the military. That's the 865,
relative to 1915, were much more
likely to serve than individuals who
were not drafted. Under the monotonicity
assumption, you can estimate
the proportions of the different types
ruling out defiers, so you end up estimating
that the proportion of compliers is about 12 percent compared with 69
percent never-takers and 31 percent always-takers. This slide gives you
the average outcomes in the four groups by treatment
and instrument status. Here in the 01 cell, we know that these individuals
are always-takers. So we can estimate directly from there what the
average outcome is for always-takers given a
treatment that's 5.4076. Similarly, in the 01 cell, we know that all these
individuals are never-takers. We can estimate what
the average outcome is for never-takers
without the treatment. The other two groups
are these mixtures of compliance and never-takers. In the 00 group are mixes of compliers and always-takers
into 11 group. But now, given that we
know the proportions of the different types, we can disentangle these mixes, and we can get the following estimated
average outcomes by compliance type. There's no definers. Then we got the average
outcome for always-takers and never takers directly
from two of the cells, we disentangle the mixture and we get the
average outcome for compliers and the two
different levels of the treatment with the
difference equal to minus 23 percent and you would have gotten
this number directly by just doing a
two-stage least squares. Today the part of what
you get out of doing this seemingly more indirect way is that you also
get to compare, and actually let me just
go back one second, you can compare what the
average outcome is without the treatment for
never-takers and compliance. You get to see what the
average outcome is given the treatment for compliers
and always-takers. The first thing you see here
is that the compliers have much higher earnings than the never-takers without
serving in the military. The compliers are actually
higher by 29 percent. These groups are
clearly very different. Extrapolating the
effect for compliers to that for never-takers is clearly not particularly
credible here. Compliers and always-takers
are much closer. But even there, you see
that compliers have considerably higher earnings
than always takers even there the extrapolation
may not be very credible. The other point, and I
said I made this earlier, is that if you use a standard selection model
using normality and activity, the implication would
have been that if compliers have high average
outcomes than never-takers, they should have lower
average outcomes than always-takers and the
other treatment status. Here you see the opposite. You see that compliers
actually do better than never-takers when they're
comparable to them. They do better than always-takers when they're
comparable to them. Here, the compliers earn more than any of the other groups when
they're comparable. More generally, that may
not be a restriction. You really want to
impose a priori. Let me make a
couple of comments. Let me talk about two extenses. First one is the
case where we have more than one minimum
binary instrument. We can do that in two ways.
We can just think of having a single instrument that
takes more than two values. Now in principle,
we can estimate, we can define this local
average treatment effect for any pair of instruments, we have two values, z_1, z_0, we can look at the
average effect for individuals who would
participate given z_1, but not participate given z_0. If you have more than
two of these pairs, we may get a different effect for any particular combination. At least for awhile it was popular to test the
validity of instruments by implicitly just comparing
different estimates based on different combinations of the instruments that essentially test whether these objects are equal for different pairs. If you feel very strongly that the effects
should be constant, that's a perfectly fair
interpretation there. But if you're concerned
that there may be variation in these effects, then that leads to a different interpretation of
rejections of those tests. The other thing
you can do is now combine all these different
instruments in various ways. One way is thinking about
what would actually happen if you use a
particular function, g of z as a instrument in a linear instrumental
variables estimator, it turns out what you get
it is a weighted average of all these local average
treatment effects with a particular set of weights. Choosing a single function of the instrument
essentially corresponds to choosing these weights. You may try to be more sophisticated about
choosing these weights. But in the end, there's nothing you can do in terms of going beyond these particular local
average treatment effects. The only thing you can do is just choose their
weights differently. But you're not changing which sub-populations
you can get things for. In the end, the biggest
sub-population, you can get the average effect
for is the one where you take the instrument that gives you the highest
probability of participation. The value of the
instrument that gives you the lowest probability
of participation. Using that that binary pair as a binary instrument gives you the largest cell population. But you can use various
other weighting schemes to up or down weight
particular subpopulations. Closely related to some work that Heckman and
Vytlacil have done. Essentially what they do is, look at the limit of these local average
treatment effects. If you have a
continuous instrument, you could imagine looking
at these average effects. Looking at closer and closer
pairs of instruments. In the limit, you would get the average effects for
individuals who would change their treatment status if you
move the instrument up by an infinitesimal amount and so that's denoted here by Tau_C. You can relate that. This gets close to what
Heckman and Vytlacil called marginal
treatment effect. They set up latent index model where their treatment status, is equal to an indicator
that some function of the instrument plus an
unobserved component is greater than 0. You can define their
marginal treatment effect as the average effect
conditional on this unobserved component
that directly relates to this limit of local average
treatment effects for particular value of this
unobserved component. Now then you can characterize
any particular average, say the population
average effect in terms of this marginal
treatment effect. But again it doesn't
change what is actually identified
unless the instrument moves the probability of receiving the treatment
all the way from 0-1, there's always going to be some marginal treatment effects that you cannot identify. In fact, this is somewhat restrictive in the
sense that you don't need to be able to estimate all the
marginal treatment effect separately in order to be able to get the overall average. It is just a single
binary instrument, that moves the probability
all the way from 0-1, would give you the overall
average without giving you any of the marginal
treatment effects. But this is helpful
is that it allows you to characterize a lot of average treatment
effects in terms of this marginal
treatment effect. Let me then make a
couple of comments well, another extension
that what happens if we do go back to the
binary instrument case, but the endogenous
regressor is not binary, takes on multiple values? The main result I want to discuss here is what the
interpretation is in the case of the standard instrumental
variables estimator. Making the same type of
assumptions as before, that all the potential
outcomes are independent of the
instrument and making a monotonicity type
assumption that increasing the instrument may increase the level of the
endogenous regressor but doesn't decrease it. You can show that the
instrumental variables estimates, to this ratio of
co-variances estimates the weighted average of the unit level increases
in the treatment. Now we get two types
of averaging going on. Before we just had
the average effect of the treatment for
particular sub population. Now we have a similar
expression at all levels of the treatment. The example I'm going to use in a minute returns to
schooling example. So y_ij minus y_ij minus 1 is the effect of an
additional year of schooling for
particular individual. We get the average of that for the sub population for whom the instrument
changes you from a particular level
of schooling to below j_2 level of
schooling above day. So that's the first
type of averaging. Then we weight all these unit level average
effects by a set of weights Lambda j where
the bottom of the slide gives the representation
for this Lambda j. The key point is that
these weights add up to 1 and non-negative. Let me illustrate this using this returns to
schooling example, that Angrist-Krueger
estimated the effect of schooling using quarter
of birth as an instrument. There's a paper that motivated a lot of econometric theory the last 10 years because
it's also pretty much the basis of the weak
instrument literature. But using this as a
single instrument case, there's no concern with the
weak instrument problem. Defined a small correlation between education and
quarter of birth, they find a small but again
significant correlation between earnings and
quarter of birth. They interpret the
ratio of that as an estimate of the
returns to schooling. In this particular
case it's clear that the instruments
is relatively weak. That is only few individuals
affected by this. So maybe it's of
interest to look at what the sub population
it actually is, that is affected
by this and what particular averages that
we're getting here. Here are some figures that are intended
to illustrate that. The histogram at the top is just a distribution of
earnings in this data set, so you get a lot
of individuals at 12 years of education. There's a smaller mode
at 16 for college. If you look at the
normalized weights so that they
integrate out to one, what you see is that the
instrumental variables estimate gives you a weighted average of unit
level increases in education, where the particular year
that's being averaged over, it's actually
remarkably spread out. There's considerable weight, not just on very low
levels of education, but it goes up almost to 16. This is somewhat
surprising given the nature of the
instrument here. As this may actually indicate other concerns with using this quarter of birth
as an instrument here. But if you take these
numbers at face value, it suggests that the
compulsory schooling laws didn't just change people by moving them from 10 to
11 years of education. But it actually
moved some people from whatever level
they had before, encourage some people to
increase their levels of education beyond 15 or 16. The other point is that if you look at the unnormalized
weight function, which is just the difference in the distribution functions, you see that the size of this complier sub population
here is incredibly small. In the end, there's only a very small
sub-population that is affected by these
compulsory schooling laws and so even if the
instrument is valid, even if all the other
concerns are taken away, then this is a
sub-population that is potentially very different
from the population at large. Now, may very well be a very
interesting sub-population for policies that involve keeping people in school
longer at low levels. But it certainly is in a
very small sub-population, and concerns about extrapolating this to other sub-populations, probably more severe here
than in other applications. I think this is pretty
much right on time. So actually let me see if
there's any questions. Sorry. Yes. Speaker 1: Sorry I got a
question, [inaudible] . Guido Imbens: Sorry. Speaker 1: You assume
that the variance of 0-1? Guido Imbens: Yes. Speaker 1: How large is
that number [inaudible]? Guido Imbens: Well. Speaker 1: [inaudible] . Guido Imbens: In fact, that's not all that critical
an assumption. If without that assumption, what is being estimated by
instrumental variables is a weighted average of the effect for
compliance and defyers. The weight still
add up to one so that's all nice and well, the only problem is
that the weights for the defyers is negative
and so it could have, if there's a very large
group of defyers, it could be that both for
compliers and for the defyers, the average effect is positive. But you could be estimating a negative effect because
the weights there, are just very far from
outside zero and one. So in some sense this is
not a critical assumption. Nowhere near as critical as
the exclusion restriction. But it significantly
simplifies the interpretation. 