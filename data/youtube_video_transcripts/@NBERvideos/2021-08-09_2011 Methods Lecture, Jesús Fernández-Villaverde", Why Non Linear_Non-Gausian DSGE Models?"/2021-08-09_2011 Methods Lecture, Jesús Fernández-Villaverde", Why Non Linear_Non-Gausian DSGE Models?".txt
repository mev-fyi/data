Jesus Fernandez-Villaverde:
My idea is, in the first session, I'm going to try to motivate
a little bit why we may care about your favorite
equilibrium model, and think about
situations where they are non-linear or non-Gaussian
or both at the same time. Probably Larry told you
yesterday that in many, many occasions we can actually work with linearized version of our models and all
that is good and fine, and by the way, I want to be very open
about this is not that I don't think that linear
methods are not useful. It's just that there are
situations where you want to use non-linear methods, so don't think this in
any way as in opposition, is really a complimentary way
to think about the world. What I want to tell
you a little bit is why we are interested
in this class of models and what are
the situations in which we want to apply it. We are going to do
everything in discrete time. I think that yesterday was
also in discrete time. There is an argument to do
things in continuous time, and the reason for that is that when you have
the equivalent of the Bellman equation
in continuous time is the Hamilton Jacobi
Bellman equation. Thanks to an applications
of the Ito's lemma, one of the expectations that appear in the standard Bellman equation is
going to disappear, and you're going to have
a closed form for that. Actually, there is
a lot of things that you can do in
continuous time. However, the amount of applications that
people have done of a stochastic general models using continuous time
is very limited, so it seems a little bit crazy for me to start telling you about very advanced material when we haven't covered
the basic stuff, and I will say
that 95 percent of them literally have to do
with discrete time models. Also, I'm going to focus
on the DSGE models. But when I teach
this stuff at Penn, I always emphasize
to my students that you want to think
about these model skews to present the methods
and to think about concrete examples that as a limitation of where these
methods can be applied to. At the end of the day, what
we're really going to do is learn how to solve
functional equations, and pretty much
everything in life can be set up as a
functional equation. You may end up, it was before
yoke in a little bit about this paper of mine about
the evolution of built. This is not a DSGE model. There are no sticky prices, and they are sticky wages, but they're not the
end of the day type of methods that we use
to solve it and to estimate they're
pretty much the same that the type of methods and
going to tell you today. As I was saying before, when you actually go over your favorite real
business cycle model or your plain vanilla
new Keynesian model. These models are non-linear. Equilibrium
conditions defined by the first-order conditions
of the agents, of the firms, the households, and
the government, and the market
clearing conditions are non-linear equations. The common practice, however, is to apply these as a
good engineer will do. These things are non-linear, it doesn't matter, we
linearized them we'll go ahead. This was the tradition, I think, since the early 80s and with the combination
of having Gaussian shocks, it makes for very simple and very straightforward
computation on, in addition, since you are going to have a linear state space
representation, you can use the
Kalman filter to very efficiently evaluate the
likelihood function. All this is good because we
have plenty of evidence that your favorite basic
neo-classical growth model is narrowly linear for the benchmark
calibration or the benchmark estimation that you will do for
the United States. A few years ago, I brought one of my many crazy papers with [inaudible] who is now
at University of Maryland, and with [inaudible] who is usually the other person in
the conspiracy in my papers. What we did over
there was to solve the basic neoclassical
growth model with any method
known to humanity. The argument was that in fact, the linear approximation
worked pretty well. You want to think about your favorite new Keynesian model as really the stochastic
neo-classical growth model with a little bit of
nominal rigidities on top. Everything that you learn
in these contexts can perfectly be translated
into any Keynesian model. This type of results in my
paper in 2005 will suggest, well, why in the
world do you want to bother about non-linear methods? In fact, we started to think
very carefully about that because I have done some other work on the
estimation of nonlinear models, and I was interested to
think about examples, cases where these non-linearities
will be important. Let me give you three examples. Of course, I'm going to borrow all three from my own papers, not because I feel they are particularly deep or insightful, but because I know them well. By the way, all these
papers I'm going to cite they are on my web page, and you're more than
welcome to download them and even better to cite them. The first example is moving away from the standard expected
utility function. We go to first-year
microeconomics, and we take [inaudible] Green, and Winston and
they tell us about expected utility
is representation. But the problem is
that expected utility, that traditional expected
utility has a lot of problems. They have been documented in the data that have
been documented in experiments and decision
theorists have also always had some
problems with some of the axioms that
justify expected utility. In particular, over
the last 20 years, there has been a lot
of new alternatives, new ways to think about
utility functions, about preferences that imply particular representations
that people have found useful to rationalize some observations
and in particular, one of these very wide class of non-standard preferences is the ones that were originally
proposed by Krebs and [inaudible] and then extended by Larry Epstein,
seen unveiling. Like everything in life, Krebs portrays that they've seen [inaudible] will be an
extremely long way to refer to these preferences of people
either call this Epstein seen or they just call
them EC preferences, or they call them
recursive preferences. They are very
popular in finance, especially to do things like account for asset
pricing observations. What is the motivation between these Epstein seen preferences? We all know from
our first-year in macro that in the standard
utility function, the intertemporal elasticity
of substitution and risk aversion are just
the same parameter. One is the inverse of the other. Let's remember the
first year we got into economics before we
even we opened barrier, and so we will not know anything whatsoever
about economics. Someone explains you
what intertemporal elasticity of substitution is
and what risk aversion is. They asked you,do you think
this is the same parameter? I'm pretty sure no one or narrowly one will have said yes, it's the same parameter. Let's think about
this as an example. The elasticity of
intertemporal substitution is how happy you are about
substituting things over time. I tell you, hey guys, you are going to be for
two days at the NBER, the first day you can go to
a very bad restaurant,and the second day to
celebrate the end of the class you will go
to a greater restaurant. How much money or how much consumption in other
good I need to give you to induce you to switch
that allocation to have the good restaurant first and the bad restaurants later
or the other way around, how much money will you be
ready to pay to do that? Risk aversion is
about the following. Imagine that today at five, when we finish, we're
going to flip a coin. If it is heads, we will go to the best
restaurant in Boston. If it is tails, we will
go to White Castle. How much money are
you going to be ready to get away from
this type of lottery? You think about that
there are two very, very different things,and
in particular, the reason why they are
two very different things is because you can
have someone like me. I have extremely
high elasticity of intertemporal substitution
and I will dare to say that most of you
as well, after all, going to grow at a school is
the ability to sacrifice, especially if you lived
like me in Minnesota, five-years under the snow, I'm going through a
stock Lucas Prescott to become an
assistant professor, so you could really
spend six years at Penn, a struggle like crazy, so you could get promoted
and so on and so forth. Well, this is having
very high elasticity of intertemporal substitution. It basically means that
the payoffs in the future induce me a lot of substitution with respect
to my utility today. Now risk aversion, I'm extremely risk averse guy. I'm the type of guy who will go to the airport three
hours before if we could, because I will always freak
out about loosing the plane. I will have three spare tires in my car if the trunk
was big enough because who knows
what if you have a coordinated failure of all
the tires at the same time? Well, I cannot be explained by expected utility because if
I'm extremely risk averse, I will also be extremely unwilling to do
intertemporal substitution. I should have never say, let's go to Minnesota and
work there for five years. It cannot be explained
by expected utility. I need to be explained by
Epstein same preferences. Well, the interesting
thing is once you realize that
this is the case, you need to start thinking
about how we write models with this type of
recursive preferences, and you need to start thinking about what are the
consequences for business cycles or welfare for optimal policy
design, et cetera. I want to emphasize
before I move on, that is the Epstein seen preferences are just
a particular example of a much wider set of new preferences that people are thinking about these days. If you go to the
NBER macro annual, maybe 2007, 2006
plus minus one year. There is a very
nice survey paper by Backus, Zin and Routledge. I think I maybe forgetting
the last co-author, and is called something like exotic preferences
for macro economists, and it's very nice
because they go, they give you a very
gentle introduction. I think there is a lot
of work to be done over there and a lot of
interesting applications that need to be
done and certainly any graduate student
with a little bit of ambition will get a lot
out of that I think. An interesting thing about Epstein-Zin
preferences is that they also have a very tight
relation with robust control. I don't know how much familiar
you are with the type of work that Lars Peter
Hansen and Tom Sargent do. But they basically like to
think about situations where I don't know for sure
the model of the world. Imagine that tomorrow
I get a call, we are going to have
elections soon in Spain. There is a new party
in power and I get a call from the new
prime minister and he says, Jesus, I appoint you, I
don't know who knows, Governor of the Bank of Spain. I go there to my
office the first day and I say now I'm going to boldly implement
the new policy. How do I know I have
the right model? How do I know that my
diamond debit view of banking sector is
actually the right one? In addition to it, this is
not that I'm a Bayesian, it's not that I have three
or four models and I'm just integrating over these
different models. I have this model
is 0.3 probability, this is 0.5, this is 0.2 and
I just integrate over that. Is a much more
fundamental field. I may be missing something and I'm not quite
sure of what it is. Hansen and Sargent show
that there is a way to think about these
types of problems that came from engineers. Engineers were also facing
several problems like this. We are going to
send a satellite to the moon and who knows
exactly what is going to happen and I cannot even express it in a
particular model. It happens to be
that some version of Epstein-Zin preferences imply a representation that is the
same that you will get with robust control and that's a
really, really cool result. You can also think about Epstein-Zin preferences as
representing this situation where we have these
concerns for robustness. What I'm going to do to illustrate why we care
about nonlinear models. I know this was a very
long-winded introduction, but you will see how this makes sense in just one second. It's a paper that I have
called the term structure of interest rates in a DAC model with recursive
preferences. Where basically
what I do is I say, can we explain the
interest rates pay at the different maturities of different bonds in a simple
model with production? This is interesting because most of asset pricing is done
in endowment of economies. You are going to
assume that there is some endowment process
and then you are going to basically
price that endowment. You are going to
provide some type of financial contracts
on that endowment and you're going to price those. I think we can learn a lot
from that type of situations but it is also the case
that production economies, the fact that you can
invest in capital and the endowment
is not exogenous or endogenous implies a lot of restrictions that are very
important to understand well. In particular, they are very
important coming back to my example of the governor
of a central bank, if you are into monetary policy. You want, for example,
to understand how the interest rates at different maturities
responds to monetary policy. Because maybe what
really matters for monetary policy is
the interest rate at 10 years or at 15 years and not the interest
rate at three months. It's important to
understand those things. The standard model does a terrible job at explaining
asset pricing and explaining the yield curve both with and without
the sticky prices. This is something for which
Neo-Keynesian models do not help whatsoever
so we argue well, what happens if we put
recursive preferences? Let me show you what I mean by recursive preferences
and the reason I'm going to present this slide
is basically to introduce some
notation that will come useful in just one second. The utility function under recursive preferences has this structure
where you're going to see the utility here
and the utility tomorrow and that's why this thing is going to
be called recursive. It basically says that my
utility is the following. This is my kernel of today, it basically says
that my utility today is going to be
a function of how much I consume and how
much leisure I have. Once I have this
thing over here, I could eliminate labor if I wanted so it will be
just consumption, is going to be put in this
constant elasticity of substitution function with my
expected utility tomorrow. First of all, I'm
going to discount tomorrow, that's fine, but I'm going to risk adjust my utility tomorrow by raising
it to this 1 minus Gamma. Note that you will have
1 minus Gamma over here divided by Theta and 1
minus Gamma over here, and then 1 over Theta. But of course, because
of the expectation, I can not take this thing out. Imagine that the world
was not a stochastic. The expectation is the
trivial identity operator so you will just eliminate
this thing and then you will have 1 minus
Gamma divided by Theta, 1 minus Gamma divided by Theta, and over here, Theta
divided by 1 minus Gamma. What is this Theta?
Well, this Theta, I know this is a little bit crazy because you have the
1 minus Gamma over here, and the 1 minus
Gamma here again, but I will tell you in
a second what it is. This Theta is going
to be the ratio between the Gamma and
this parameter over here, this C. The Gamma is going to
control your risk aversion. You can see over
here that this is precisely what these risk
adjustment operator is doing and this
parameter over here is going to control
your elasticity of intertemporal substitution. By the way, notice that I'm saying parameters that control risk aversion and
parameters that control elasticity of
intertemporal substitution. I'm not saying risk aversion and intertemporal substitution. The reason is because
in models where you have both consumption and labor, you actually need to
do a little bit of work to show what
risk aversion is and what elasticity of
intertemporal substitution is. There is a very nice paper by Eric Swanson at the
San Francisco Fed, where he actually develops
all these formulas in detail. This is actually something that is perhaps sometimes
not well understood. Well, in the expected
utility case, a standard for Z micro, this Theta will be
equal to one because the parameter controlling
risk aversion and the inverse of the elasticities of intertemporal
substitution are the same. This will be a one, this will be a one, this will be a one, and we are back into the
standard utility framework. When Theta is
different from one, we are going to have the marginal case where a lot of interesting
things can happen. In particular, these preferences
have a characteristic that is not clear. I asked Larry Epstein once
about it and he told me he thinks it is a bug and some other people
think it is a feature. Which is, you are
not going to be indifferent to the timing of
revelation of uncertainty. Let me give you a very
simple example about this. Imagine that you go and you take your prelim or your
qualifying exam, you take it down
from 9:00 to 11:00. At 11:00 sharp, you hand in your exam and I
put you in a room. There is absolutely
nothing in the room. You're sit in a
corner quietly and I close the door and
I'm going to tell you that at 12 o'clock
I'm going to open the door and you can get out
and do whatever you want. Now, do you want me to tell you the result
of the prelim at 11 o'clock right when
I'm putting you in the room or at 12 o'clock
when I'm getting you out? How many of you will
like 11 o'clock? At least five. How many of
you will like 12 o'clock? How many of you will
be indifferent? You are the only two that
are expected utility guys. Expected utility you should
be indifferent because all your decisions are based on your probability
distribution, not on realized utility. That's one very important
thing we need to remember. So which means you don't care. The whole thing about
the room being empty is that you cannot really do anything from learning before. In the standard wall,
if I tell you at 11:00 and you pass
you will go to the bar and get a
drink right away so you will prefer 11:00. Or maybe if you thought you were going to fail,
you will say well, let me give you one
last hour of happiness. But the point about me
putting you in a room with absolutely nothing
to do is that you should be absolutely
indifferent to this. Well, turns out to be
the case that any of you who say you either
prefer 11:00 or 12:00, you are better described
by Epstein-Zin because in Epstein-Zin depending on whether Theta is bigger or
smaller than one, you will prefer 11
o'clock or 12 o'clock. Some of you guys
may be at Harvard. Now, I don't remember
his last name is Thomas, he is an assistant
professor who loves theory. He has an absolutely gorgeous
paper on representation of different sets of preferences
and about the relation between Epstein-Zin
and timing for resolution of uncertainty
and all that is. I saw it last year in a seminar, I was absolutely blown away. Anyway, and then the budget constraint
is going to be very, very simple, is
about consumption, is how much you can invest. You are going to buy bonds, of course, they are going
to be nominal bonds. Because the type of bonds
that you buy out there are nominal at some interest rate. The PT is the price level and you're going to have some
income from your capital, some income from your labor, and of course, the bonds that you carried into this period. The bonds are sold
at a discount. We are going to have
complete markets so we are going to have a
standard pricing kernel. Let me complete the model with a production function very, very simple, and very standard Cobb-Douglas
production function. There is some productivity
level that evolves as an autoregressive
process logs. Something I'm going to do today and you will see later how
it comes useful in many, many occasions is to bright
the innovation in this way. Epsilon is always going to come from unnormalized
Gaussian distribution. I'm going to put the
standard deviation in front so usually in most papers you
will see that this comes from some pseudo-Sigma. I prefer to put it outside and you will see later
why I prefer to do it. Of course, doesn't have any real implication it's just notation. Then I'm going to
have this Chi over here that is going to take
values either one or zero. It takes the value of zero, this is basically
saying we are dropping off the stochastic
component of this model. If Chi is equal to 1 is the standard
auto-regressive process. You will see later why it's useful to think about
the wall in this way. Of course, aggregate
constraints areas that my consumption plus
my investment needs to be equal to my production and the law of
motion for capital given some depreciation
rate of capital Delta. Yes. MALE_1: [inaudible] Jesus Fernandez-Villaverde:
Not very strong. I think about this exercise. I'm a big fan of
partial derivatives. What do I mean by that? I think that's something
that people perhaps in other fields like engineering appreciate more
is the importance of a smallest steps
of saying look, let me take the absolutely
a standard model and yes, let me change one little thing to see if that improves or not. That's why I mean by
partial derivatives. This doesn't imply that
I don't think that in the real world or in a
very satisfactory model, I will have a lot
of other features. For example, over here, I'm assuming complete markets. They are not complete
markets out there so it will be also
very interesting to have models work in
addition to abstain C and you have other
structures of the markets, or you have a much more
general production function. The problem is that if I introduce two
or three changes at the same time is very
difficult to know exactly which of the three
is driving the results. I thought well for
this exercise, let me just introduce one. See what happens and who knows maybe this tells me
something interesting, or maybe it tells me that I need to do some other chains. In that sense, sorry. MALE_2: [inaudible] Jesus Fernandez-Villaverde:
No. I understand what you mean. My only coming back
to my answer before my early reluctance to
do that will be that then it will be
difficult to tell in the exercise how much was because of EC preferences versus how much was because
of the production function. If you're telling me, look, I'm going to take this paper, I'm going to put a marginal now that we understand
what happens with EC preferences or
later on when we finished the description
of what is going on, I understand what is happening. I'm going to put another
utility production function. I'm very happy to do that. But for what I want to say
today is really going to be the EC preferences that
is bringing the results. Any other question?
Beautiful. We need to solve this model and it's going to be a
tricky model to solve. The reason for that is
because you will see, let me define this data space. I'm going to have capital, while it's going to
be in differences with respect to
the steady-state. This is just a parameterization of the state that is useful to carry around the log of
the productivity and one, note that we do in these two, I put a comma here
I don't, I put, how do you call
this a semicolon or whatever the other thing and then a one over
here and is because this one is not going
to be a pure state, is going to be a pseudo state. What is this one really? Is going to be this
guy equal to one. I'm going to solve
basically what I'm going to do later in
the second session, I'm going to show you
how to solve the model where k is equal to 0 and use that solution to approximate the case
where k is equal to 1. Now, under different
stability conditions of the value function, I know I can write the value function of the
agent in this model as. This goes back to the
brief introduction that Larry did yesterday of this Taylor approximations as the value function
in the steady-state. The first derivative
of the value function with respect to each estate
multiplied by the states. By the way, over here, I'm following the
tensor notation. That tensor notation
basically eliminates the sum operator when
you don't need it. Imagine that this
is the derivative of the value function evaluated at the
steady state and this will be the Estates, each of the states in time t and I'm summing
over all of them. Well, once you realize
what is going on, you can't really erase
the sum operator. When you have only one
sum operator is not a big deal you will not
save a lot of time, but later on in the third order, you will see I will need to have three sum operators and the whole thing will not
even feel in the slide. Physicist came up with
the idea of doing this. I was actually Albert Einstein who made the tensor notation popular and it's actually
a great way to save time. What we have over here is the first derivative
of the value function with a evaluated at the
steady state for each state, the second derivative,
the third derivatives. Then of course I'm multiplying here or the quadratic terms, here are the cubic
terms, 1.5, 1/6. Now, this is the type
of expressions y, so if you go to Penn economics
in the fourth floor, we have all the theories on one side and all the
macros on the other. How do you know that these
partial derivatives exist? I can tell them, well, I can go to a stocky Lucas Prescott and I can show you that the first partial
derivatives exist. I can go to a very nice paper by manuals Santos in
Econometrica and I can show you that the second
derivative of the value function
exist but I'm not aware of any paper
which does mean that the first derivative of
the value function exist. Here I'm just going to wave my hands and hope and pray
that that derivative exist. I need to reopen and recognize
that we are going to do things which will make
some people uncomfortable. We can also approximate in the same way the
function for any policy, for any variable that is
picked by the agents. Things like investment,
consumption, labor, I call this var, I guess that this is
about bad notation because it looks
like it's a variance no, is for variable. Again, it's going to be the variable in the steady-state, the first derivative of
the decision function, the second derivative of the sine and the
third derivative. Now, coming back to
my point before, we know that the first
partial derivatives of decision functions exist, we don't know anything
about second, unfair. We can also find the decision rules or the pricing functions
for the yields. We are, remember
I'm interested in the whole yield
curve so I want to compute the guilt of a three-month bond
of the six months, bond of a nine month over 12 months and so
on and so forth. This is actually what
is going to make the computation of this
a little bit tricky. Just one second, because I don't want to do
an approximation. Sometimes what people
do is they find an approximation using a
console or something like that, there's actually a
non-trivial amount of numerical error that you
induce with those things. Even if conceptually is not
very difficult to do this, it implies that you
are going to have a lot of equations to solve for. By the way, over there, I'm using a slightly
different notation as a state augmented and the
reason is I didn't go, since this is just
for motivation, I didn't explain you how prices are determined
in this economy. Basically, the only thing that matters that
there is going to be some inflation level
and some process, some shock to inflation. Yes. Yes, this is going to be important, and I'm going to talk about that in a lot of detail when we get into computation. I know. This is an important point
that we need to clarify, but yes, this is going to be the deterministic steady state. Of course, what is going to
happen is that I need to come up with ways to compute
all these derivatives. I need to know them. Larry, yesterday, probably described
briefly how to do it. We're going to do it a little
bit more systematically. By the way, these
very same model, I'm going to show you later
by null code to compute it. I distributed the
[inaudible] code to the NBR. I don't know if they
send it to you or not, but it will be in some place and maybe I will put it in
the web page of Larry. For sure there, I'm
more than happy to share the code
with all you guys. Now, this is where the
fun and games start. This is why you are thinking
probably why in the world I sit down in a class that was called why nonlinear methods, and so far, we haven't really mentioned anything about
it, is the following. Let me go back to
this for a second. Why did Epstein and Zin got
very excited and then people like Lars Peter Hansen and many of the other guys who
worked with Epstein-Zin, preferences got really excited? They say, what is the problem of your standard
business cycle model? It doesn't do such a bad
job with real quantities. It doesn't do a perfect job. We know that the
labor market doesn't quite work. Well, who knows? Maybe we talk with our
search theory friends, they will fix that, but
things like consumption, investment it does a fair job. Where it does a terrible
job is in asset pricing. It's impossible to
explain the risk premium. It's impossible to explain a lot of correlations out there. If we believe that more or less all the real quantities
are determined by the elasticity of
intertemporal substitution, and most of the action of asset pricing is
by risk aversion, the fact that now we have
two parameters is great. Because I can say I can put a reasonable elasticity of
intertemporal substitution, like two, so I get
real quantities. Then I say, well, let me put Gamma to 200, so I can also explain
asset pricing. I have one more parameter.
I can do better. If we're going to think
about this type of models, Gamma is a key issue, is precisely what is making this different from the standard expected
utility framework. Now, turns out to be the case
that the constant terms, so the value function in the steady state,
all the variables, all the endogenous variables, and all the yields
in the steady state do not depend on Gamma. That's not a big surprise. It's the deterministic
steady state. Why in the world, in the deterministic
the steady of state, risk aversion should
play any role? Because risk aversion is
about the stochastic staff. There is no stochastic
staff, so who cares? Fine. More interesting is two. Two says, none of the terms of the first
order approximation. So the first derivatives of the value function of
the decision rules, or the yields depend on Gamma. What you do is you
take your model with Epstein-Zin preferences
and you linearize it, is exactly the same
model that with expected utility and the same elasticity of
intertemporal substitution. Linearization and Epstein-Zin
do not go together. If you linearize, you just kill Epstein-Zin. Hence, if you were more or less intrigued
by anything I said before about the usefulness
of Epstein-Zin preferences, by definition, you need to
go far than linearization. I emphasize that this is not about having a little
bit more accuracy, because I'm a typical
guy who likes to claim that I have
better accuracy than you. No, this is if you
linearize your model, Gamma does not appear. If you're an econometrician,
is not identified because it doesn't appear
in any expression. If you are interested
in business cycles, it doesn't have any impact. If you are interested
in asset pricing, it doesn't have any impact. It is only when you go to the second order approximation
than Gamma plays a role. In particular, is going to
play a role in a constant. As we will do later, we will see later in
more detail when we talk about this type of perturbation problems in the second order approximation there is going to be a constant, and that constant is going
to be affected by Gamma. The interesting thing is, yes, Gamma shows up in the
second order approximation, but in some sense, it
appears in a very soft way. It only appears
in that constant. It's going to be very
difficult to learn about that Gamma using the
second-order identification. It is in the third order
approximation that you are going to have a bunch of terms that are going
to depend on Gamma, and hence, for example, if you evaluate the
likelihood of this model, you are going to be
able to learn about it. Again, later when we
do the diner code, I will show you how
this works in practice. But this is, I think, an extremely compelling
argument of why in a very natural framework you want to go to
second order, at least. You say look, expected utility
is unknown satisfactory, what do you think
about the wall? Well, I think I want to
search for better ways, let me go to something else. Epstein-Zin is the
smallest deviation that I know with respect to
a standard utility function, turns out to be the case that
you need to go nonlinear. Now, I haven't really worked out this type of results for other exotic preferences like the ones I told you
before there were reviewing the NEBR macro annual, but I'm pretty
sure that you will find pretty much the same. So it really tells you
that once you break away from a standard
expected utility, nonlinearities are the
key of the argument. I'll come back to that later. Let me show you another example. The first example,
what you really care about is nonlinear structures, because you want to do something different
with your preferences. Many arguments of
why you want to do something different
for your preferences. Second situation,
volatility shocks. What do I mean by that? When I brought this paper, I thought that this is going
to be about Argentina, Brazil, Ecuador,
Venezuela, poor guys. I guess that if I
brought it now, it will be about Spain
and Italy and Greece. But the paper is already
published but it's forthcoming, so I will leave that
to other people. But this is basically a way to try to organize our thoughts about what
is happening in some of these countries
along some dementia. You are on a small economy, you're Argentina, you're Brazil, and of course, you are going to borrow a lot in the
international markets. What is going to happen is that the interest
rate at which you borrow is the international
risk-free rate. Think about this as
the treasury bill of the US government. Although again, things have changed a lot
since we wrote this paper, I guess now the
three-month treasury bill is not risk-free anymore, at least as long as the lunatics are still
in power in the Congress. But, [inaudible] you need
to pay a country spread. If you are a well-run
country like Canada, that country spread
is nearly nothing. But if you are a Spain
or if you're Argentina, that country spread
is quite a bit. Let me show you the
countries spread. This is something that we got from our friends at JPMorgan. It's called something
M by NB data, is basically a
measure of the spread of sovereign bonds
of these countries. It is actually surprisingly difficult to put
together these data. Even today if you want to get very good data on
a Spanish spread, it actually takes
a little bit of time to put the whole
series together. It's one of these things where good dataset building is
actually a little bit needed. But we put this together. Let me show you who these
different countries are. These black one,
first Argentina. You can see, of course this is the corralito and this thing goes down and then
they restructured their debt and then
the spread went down. But you also have
over here, Brazil, you have Ecuador,
you have Venezuela. You can see how this is
an order of magnitude. These spreads are
much bigger than these very small line over
here which is the T-bill. By the way, we are doing
everything in real terms. You can see how the T-bill was actually negative in real terms in the middle
of the decade. This is the type of
fields that people like John Taylor complain
about monetary policy. Also, before I forget all
these sovereign bonds, that we are discussing
this issue in dollars so the only irrelevant inflation
is the US inflation. People have argued
this big spike by the ways the Russian default, and Russian default will play
a little bit of a role in my argument in just one
second so remember that. A lot of economists
have looked at what happens when you have
these socks, two aspects. Very nice papers, basically what they say you have a
sock to the spread. It's more difficult to
finance yourselves, so stuff is going to happen. What we were trying to
argue is different. Look, if you look at the
volatility of this series. Think about the
standard deviation, some type of rolling window of the standard deviation
of this series. This thing, sometimes
it's very volatile. I look at this and
sometimes there is very little volatility
over here for example. Let me again talk about
the Spanish debt, since unfortunately one of the first things I do now
in the morning when I wake up is I pick
up my iPhone and I look at how the Spanish debt
was trading that morning. This morning we were trading
and 266 basis points, yesterday we were
trading at over 300. Think about a mongoose level of volatility that, that thing is. Yesterday, there
was an auction of 10-year treasury bills of
Spanish treasury bills. We sold them at 5.85, if I remember correctly, this morning they
were trading at 5.70. I don't know how many
of you guys have a little bit of familiarity
with the bond market, as you know a different of 15 basis points on a 10-year bond decided a lot of money and we are talking here about 12 hours. However, if you go to 2003, the Spanish debt, the average change will
be one basis points, two basis points,
three basis points. Of course, there is
some correlation of level shocks and
volatility shocks. When we do the hard
work in the paper, we actually take account
of that correlation. But, what I want to
think about is what happens if you are Brazil over here and suddenly
the Russians default. Why do I want to
think about that? Because you're Brazil, you don't have anything to
do with the Russians, you have never been in Russia, you don't trade with Russia, you don't have the same type of exports or imports
that Russia have. The correlation in fundamentals between Russia and Brazil
is pretty much zero. Suddenly, Russia is not
going to pay their debt. You are going to have the risk that you have to pay or
your loans goes up a lot. That's the level shop, but also the volatility goes up a lot. Suddenly handling your debt, the Brazilian Ministry of Finance is a much
more difficult task. Why is this interesting? Because it's the perfect example of an exogenous
shock to volatility. You were minding
your own business, and suddenly something
else happens and your debt is
much more volatile. What are the consequences for your economy of this increased
volatility in your debt? In other times, you
could argue that some of these increases in
volatility are endogenous. But actually when you talk with financial people
in financial economics, they will tell you
that over 90 percent of the volatility and spreads for emerging economies can be explained by the
volatility of the SP 500. To a large extent, a lot of the volatility shocks are where there are exogenous. You are the prime
minister of Peru, you didn't have
anything to do with this situation and suddenly
your volatility goes sharp. This is like a couple of
years ago in an IMF meeting, one South American
Minister of Finance say, it is so great to be in a crisis where we were
not the guilty part. How can we think about this? There are many ways out there to think about volatility
changing over time. The one I'm going to propose, I'm not going to defend is
the better or the worse, it's just something
that works for us. What we're going to do is
think about the following, the composition of
the interest rate. The interest rate
at which Argentina borrows is some mean. Then there's going
to be a shock to the US T-Bill, Treasury bill, and there is going to be
a shock to my spread, to my country risk spread. What I'm going to assume is
that these spreads follow an autoregressive process like the normal autoregressive
process you will see in any paper, except that here
in the volatility, in the standard deviation, I'm going to put a
T. You see that? The standard model that it goes up now is not the standard
of model anymore, I put a T. How does this
standard deviation? Well, it's not really
a standard deviation, it's the exponent
of the standard deviation but you will see in a moment while I do that. It's because this is
standard deviation or this exponent or this log off is going to follow an autoregressive
process of its own. The reason I want
to put an exponent over here and a level over here, or I could put a log
over here and a level over here is because I want the standard deviation
to be positive. I'm not so good at statistics, about
that at least I know. The interesting thing
about this process is that you're going to have here a second shock which
is the volatility shock. What does this mean? What do I mean by that? This is Russia default. I think this has a
volatility shock, that implies that the
volatility of my spread goes up and that fits over
here into my level. I was saying before
that there are many ways to think about
change in volatility. For example, instead of thinking about
discontinuous process, we could think about
discontinuous, a marco forgione, you have high, low and you just switch
between high and low. The reason I'm going to
concentrate on this is because by assuming that the
standard deviation is a continuous variable, I'm going to be able
to take derivatives. I'm going to do perturbation and perturbation is
about derivatives. If I had high or low, I will not be able to do that. Even in this case
where it's continuous, I'm going to argue in
just 1 second that we're going to have an inherently
nonlinear problem. Well, if I had discrete, it will be even more non-linear. I don't know if the more
non-linear exist or not, but I hope at least you
understand the intuition. In some sense, this
particular structure, which I'm not particularly married too but I
find it useful, is the least or the
smallest deviation with respect to other
ways to think about volatility from the
basic linear framework. In this basic example, I don't allow for
correlation of shocks. In the paper, we
actually do allow for correlation of shocks and we
estimate the whole thing. What do we do? Well, what we can do is we can plug in these processes in an otherwise
absolutely the standard, a small open economy model. The standard is small economy of model is by Mendoza 1991, Correia and Neumeyer and Perri and Uribe and Yue
have another version. But basically what
is happening over here is you got a
small open economy, you have a representative agent that is borrowing from
the rest of the world. The interest rate at which you borrow follows this
stochastic process. We have some
discussion about that. We use something called the Greenwood Markowitz
half-man preferences, as you can see today it's
about finding preferences. People in international
macro love those because they don't
have wealth effects. If you want later in the break, I can tell you a little
bit more, but it's a little bit collateral
to the main discussion. Yes, believe me
for a second what I'm telling you that it's
useful to have those. But actually in the
published version of the paper, we
don't have them. I guess these are slightly
worked on before. They showed the
interesting thing is that the interest rate that you borrow follows this process. Then the household
budget constraint, this is the debt, this is the interest rate at
which you borrow. What is going to happen is
that changing the level of debt with respect
to some parameter, to some deterministic
steady-state level is actually costly. That's the reason
why you actually are able to close this model. Otherwise, that will be a random work and the model could not be solved
in a stationary way. This is just a trick proposed by Schmitt Grohe
and Martin Uribe. Then we are going to
have some law of motion. For capital, this is a
model where you need some type of investment
adjustment cost. The reason for that is
because, otherwise, in small open economies, investment will
fluctuate like crazy. Because you could
always go to the international market and borrow 10 trillion dollars
if you wanted. You need to slow down on how well you just introduce
adjustment costs. Not very pretty, but
gets the work done. Then you are going to have Cobb-Douglas
production function with again, some
stochastic process. Already here by the way, notice that we don't
put a t. I have other papers where I have put
a t also on productivity. The reason I didn't
put t over there, is because I want to focus your attention on
the risk of spread. This is a little bit the same spirit of my answer before. I could put a t over there. I prefer not to do it, not to introduce distracting
parts in the argument. Now, again, why did I
introduce this model? Well, because this is a model where nonlinearities
are again essential. What do I mean by that? I'm interested in looking at the effects of a
volatility shock. Imagine my example before, you're Brazil suddenly
Russia default and your debt is going to
be much more volatile. Or you are Belgium and
Italians and Espanas are doing funky things and suddenly your debt is
much more volatile. What are the consequences for you of that higher
level of volatility? Well, you could solve by linearization the problem of the representative
household in this economy. But you will find that the
first-order approximation, as Larry probably explained yesterday, is
certainty equivalent. By certainty equivalents, I just mean volatility, that is, the standard deviation
does not show up in the decision
rules of the agents. By definition, you cannot talk about the effects
of volatility in a linearized wall
because linearized walls are walls without
care about that. I want to think about
linearization is that it's equivalent of finding a second-order approximation of the utility function and
solving that problem. We know that with quadratic
utility functions, you are risk-averse, you don't like risk. But you are not prudent, you don't do anything about it. The reason why you changed
your behavior is because, you have at least a
third-order derivative that is different from zero. That means that you need to go higher than linearization. What happens when you
go to a second order? Well, basically that
in that situation, you are going to have
volatility shocks entering into your economy. But they will only enter in
terms and all these will be much clearer later
when I actually introduce all the perturbation
notation in detail. They will only
enter in terms that multiply the level shock. You're going to have
a term that has the volatility shocks multiplying
the volatility shock. Well, what is the
problem of that? I'm interested again in
the partial derivative, in what happens when only
the volatility changes. If only the volatility
is changing, the level of shock is zero. Zero times something is zero. Yes, I can generate a little bit of effect of volatility in real quantities in the
second-order approximation but it's only through
interaction terms. If I want to compute an
impulse response function, let's say I need to go
to the third order. Because in the third order, it is where the volatility
shocks appear on their own. You see the beauty of this? Volatility shocks are
something that we care about and they only appear in
third-order approximations. Again, this is not a case where I want to
do non-linear because I have this gigantic computer at my office and I
want to justify to my chairman and my dean why they gave me $10,000
to buy that. I say no, I need something
more complicated. No, it's because I
need to think about volatility that I need to go
to second and third order. I have actually another paper that hopefully, I
will circulate. I haven't already been
given it in a lot of seminars over the
last 3,4 months. Hopefully, I will
circulate it as an MBR working paper
in a few weeks. It's a very similar story, but about volatility
shocks to tax rates. Imagine that suddenly there is a lot of uncertainty about where the tax rates are going to be in the US economy next year, which I think is a
reasonable description of what was happening
over the last 2,3 weeks in the big
discussions between the president and congress. Over there what you're
going to have is precisely that all these types of federal effects are
going to be important. Is the volatility about
tax rates that matter, it's not about the spirits. There are really a lot
of situations where this volatility
may be important. Moreover, we show in the
paper two more things, the cubic terms are
quantitatively important. This is not, when we first presented this thing
people immediately say, you got to federal order.
That must be tiny. Yes, they are tiny if you are Canada because your volatility, your experience goes
from 1-2 basis points. They were tiny for
Spain in 2003. They are not tiny
for Spain today, and they were never
tiny for Argentina, who has always been
on a wild ride. The second point and again, I need to go over here to the whiteboard is that and that goes back to
your question before. Other point of fun and games is going to
be the following. Imagine this is capital, and I plot capital
in the steady-state. Now what I'm going to
do is I'm going to simulate from the
ergodic distribution. What do I mean by that? I solve my model
and I just generate a bunch of random numbers
and I run my model for, I don't know, 10,000
million times. Then I just do the histogram. When I do linearization, linearization is
going to be right. It's going to be like
that. It's going to be centered around the capital
in the steady-state. Why? This is just a
linear state space with Gaussian shocks. Remember I'm linearizing
so anything that has to do with non-linearities, etc, disappears, is
centered over there. When I'm doing non-linear, I'm going to have all these
second and third-order terms. We will see them more in
detail later on when we talk about the structure
of perturbations. But basically, if you have constants and
second-order terms, what is the funny thing
about our quadratic term? Doesn't matter if the shock
is positive or negative. The quadratic term is always
going to be positive. You are going to have that when you simulate
from the second or the third order this is going to be the ergodic
distribution of capital. The ergodic distribution
of capital is not going to be centered around the
capital in the steady-state. Why is this interesting? Well because if you are a
naive calibrator you're wrong. You're wrong in a
very fundamental way. You go back and read Cooley's frontiers of business cycle
research Chapter 1, Cooley and Prescott telling you the canonical description
of what our calibration is. They say, well, we
borrow some parameters from micro evidence,
whatever that means. Then we pick some parameters to match some properties
of the steady-state. For example, we pick Beta, the discount factor, such that the interest
rate, something like this. The inverse of the
discount factor is just one plus
the interest rate. Well, using the capital and this is going to
be the steady-state. This works fine when
in the linear wall, this capital in the
steady state is a good summary statistic
of this distribution. But if this is the true
ergodic distribution, because you have all
these non-linear terms, this is no longer the case. Because here, for example, you are going to have
more capital on average. It may be the case that the interest rate on
average is lower. You don't pick Beta
to match this, you need to pick Beta
such that these are ergodic distribution
generates a moment in the model that is equivalent to what we
observe in the data. Because in the US data, I don't go to the Rudolph
economic analysis. I click over there and I observe capital in
the steady state or interest in the steady-state. What they give me is a series
of the interest rate in the US economy and I can think of it as a draw
ergodic distribution. The presence of second
and third orders, is an enormously
interesting challenge for all of us who care
about ergonometrics. Because basically,
it tells us that the standard calibration
approach doesn't work and that we need to
do something more serious. We either need to go to likelihood estimation or
if you're so inclined, some type of method of moments. Let's recap why we get about nonlinearities. First example I told you guys was any type of
non-standard preferences. Second example, any
situation where there are some
volatility shocks. By the way, before
I move into this, there is a way to make
volatility shocks even bigger and more important, which is by any type
of non-convexity. There's a very nice paper
that I invite all of you to read by Nick Bloom,
2009 Econometrica. The main difference between
Nick and what we do is that he has this
adjustment cost and that generates SS rules or what he shows in a very
nice way is how these SS rules are affected
a lot by volatility shocks. Maybe some of you were in my group with
[inaudible] last week. One of my co-authors
presented a paper with me where we have a limited
participation model. What happens, however, is that
you're also going to have these SS rules and volatility shocks are
going to change them. The really funny thing
is when you have the zero lower bound in
addition kicking in, then everything
becomes a lot of fun. Now, the third example is
fortune versus virtue. Again, if you go to my webpage, you can find the whole paper
where I explained this. There is again, a
lot of evidence of time-varying volatility in
US aggregate variables. In some sense, we knew
this from a long time ago. There is a beautiful
paper by [inaudible] in econometric theory
back in the early '90s, maybe 1994, 1993, where he does a
very simple thing. He estimates an ARIMA or an
ARMA for the US economy. But except that instead of
having Gaussian shocks, he has these shocks. You have a little
bit more fat tails. He shows that that fits
the data much better. Well, a simple way to generate fat tails is having a
stochastic volatility. It's not the only way, but it's a very good
and very natural way. Now, this sounds very
econometric and very complex. But when we talk about the
great moderation that has become so famous that we can't even read about it
in the newspaper. Usually with economies we're so stupid in Bolivian integrate moderation type of sentences is just about
time-varying volatility. What is the great moderation
that for a while, we have lower volatility
in the US economy. By the way, the reports
of the death of the great moderation are
grossly exaggerated. If you actually follow
and do all the updates of the type of things
that the Stock and Watson did up to today? Yes. There is a little bit of an increase in
volatility after 2007, but not nearly as much as getting back to the
levels before 1984. I have been trying to say
that for three years, so far no one has paid
any attention whatsoever. Hopefully, in a few years, I could be able to
say I told you so. But in the meantime, suffering pain and solitude. Now, how can we explain this? How can we think about
why in the world the US economy was much more stable
between 1984 and 2007? Again, there are many
explanations out there and I'm not going to
review the whole literature. But two explanations that
are intriguing or at least that caught my attention, are what I'm going to call fortune and what I'm
going to call virtue. The main guy behind
fortune is Chris Simms and Chris Simms just says we
just got good shocks. In the '70s, a lot of bad things happen. The Yom Kippur War, the Iranian Revolution, blah, blah, blah, oil shocks bad stuff happened. The '90s were great. The Internet
revolution, stability. After all, we were
already thinking about what the
president was doing in his oval office and
not about anything serious. We were just lucky. I'm going to call that
fortune because the way we are going to model it is through a stochastic volatility. We are going to have
some shocks that hit the US economy and sometimes they are going
to have high volatility, the '70s, sometimes
they are going to have low volatility, the '90s. But there is a different story. This sad story maybe told for the first time,
well, by many people, but in a very influential
paper by Clarida, Jordi Gali and Mark Gertler, which is we started doing
monetary policy right. I don't know how
many of you know the intricacies of this
Taylor rule literature. But basically in the Taylor
rule, one basic principle, is the idea that when inflation goes up by one basis points, you need to increase the
nominal interest rate at least by a little bit more
than one basis points. The argument is, this increases
the real interest rate and that destabilizes the
economy is a result that you get in a very
general class of models. Basically what this guy's
side view is when you estimate Taylor rules
before Volcker, before 1979 or before 1984, depending on how you
split the sample, the Taylor rule seems
not to be satisfied, but it was satisfied after 1984. It was just the
great moderation was finally central bankers
work through our textbooks. They learn what to do and they know now how to fix the economy. If you think about it, the
policy implications are very different because the
stochastic volatility really has the policy
implication that beyond praying you cannot do
anything about it. The story about
parameter drift in about the policy rule of the monetary authority
changing tells you that there is something
that economists can do, that we can manage monetary
policy in a better way. The problem is that there
has been a lot of papers that argue that the
stochastic volatility is what drives these. In particular, I have
here a few of them. For example, Sims and Zha, in a very influential American Economic
Review paper in 2006, they estimate a structural
vector autoregression and they claim this is just
a stochastic volatility. I already talked about Clarida Gali and Gertler,
and my colleague, friends of [inaudible] Ben and Thomas Lubik have another
paper based on that, and they basically argue, no, this is really about
monetary policy. But we haven't really
had around are papers that try to put
both at the same time. Say look, this is going
to be a hot race. We are going to have a
stochastic volatility. We are going to have parameter
drifting and we are going to estimate this model and
let's see what happens. Let the data speak and show
what is really going on. This is interesting because it forces us to do
a bunch of things. The first one is we need to write a medium-scale DSGE model because we are going to take this to the data
thus it needs to be sufficiently reached to
be a credible exercise. It needs to have a
stochastic volatility and parameter drifting. I already told you that the stochastic volatility
was about non-linearities, but parameter drifted is also going to be about
non-linearities. It's also going to be the
case that if you want to think about Taylor rules
that change over time, this is going to be an
inherently non-linear process. Again, if you want to think about the causes of
the great moderation, I think in a very
natural way you are going to end up
with nonlinear models. In the paper, we need to
do a little bit of work. In particular, we need
to develop ways to evaluate the likelihood of such a complicated
and rich model. We want to do things like
build counterfactual history. What will have happened if. I don't want to get into
much detail over there, except just to
point out where we are going to introduce the
stochastic volatility. For example, we are going to
have a utility function for the household and
they are going to have a shock to
intertemporal preferences. This dt over here, and a shock to
their labor supply. People introduces this type of preference shocks in
New Keynesian models because they give you a lot of flexibility in capturing
consumption decisions over time and labor
supply decisions. They are going to follow autoregressive
processes in logs. Again, the contribution of this paper is to
put a T over there. Again now, this is going to be an
autoregressive process. In this particular case,
I'm writing it in logs, but it's the same as before and you can see this over here. Then let me not go in
detail over all the model. Then there is going to
be a bunch of equations. Believe me, I think
they are right, I hope. But we have a Taylor
rule and the type of variations we are going
to be interested in the Taylor rule is things,
for example, over here. This is inflation and this is the parameter that governs
your average inflation. Again, we go back
to this thing that I was showing you before. Because instead of
writing over here, when you have nonlinear terms, the average inflation that you
observe in the data is not the target of the
monetary authority as expressed by that, the monetary authority
understand they are going to be
non-linear terms. If you want to target
a two percent average, you may need to put in
your Taylor rule, 1.5. That's again, an interesting
point over there. But what we really care about
in this particular case is this parameter over here changing over time in
an autoregressive fashion. Again, I could do it
in many other ways, but autoregressive
is just simple. and this is what we are
going to call virtue, the fact that we are
going to become more aggressive in responding
to inflation. Those three examples
are my motivation today of why we care
about nonlinearities, non-standard preferences, anything that has to do with
the stochastic volatility, anything that has to do with
changing policy over time. There are many other examples which I'm not going
to get into detail, but let me at least
enumerate them. Things like asymmetries. So the standard New
Keynesian model or the standard real
business cycle model, a positive shock and a
negative shock are the same. They just have a different sign. That's when you look at the impulse response
functions in papers that you only
plot a positive shock. Because if you want to
see the negative shock, you flip your iPad and you
see it the other way around. Well, the world is asymmetric. One of the few things that Jim Tobin and Milton
Friedman ever agree on was the following idea. But when you plot GDP over time it looks a lot like this, with long and smooth expansions and sharp and short recessions. I remember when I
was an undergraduate back in Spain we had this
book by Doris Fisher, which I guess most people have studied or at
least people in my age used to study,
Introduction to Macro. We had this professor
that came the first day and he had this graph. I perfectly remember a
cycle around that trend. He said, "Throw away that page. Throw it way right now." Everyone was like, what
is this guy saying? He was a rather forceful person. He actually walked
to the first row, picked the textbook
of a poor girl who was seated in the first row, open it, tear the
page away, and said, "Now I want all of you to do the same," which was a lot of fun. The reason is because this is a much better way to think
about the world than this. It is not really
symmetries around a trend, it's more about the
sharp recessions that actually have
long run effects. This by the way has a lot of
consequences, for example, for the welfare cost of
aggregate fluctuations because if we miss one
percent of GDP forever, that's much more costly than in the standard Lucas computation. You cannot think about any type of asymmetries in
a linear world. You need to go nonlinear. Press whole effects. People claim that maybe
the economy may change, a lot of the behavioral
agents may change a lot when you get to some
threshold. I don't know. When you get to 100 basis
points of a spread, suddenly you start putting
attention to that. One problem for example, again, going back
to Spanish debt, is that every time we cross
one of these thresholds of 100 basis points with respect to the Germans or 150 basis points, a lot of big insurance and
retirement funds out there, big institutional investors, had to dump a lot of Spanish debt because of
regulatory concerns. When you get to
200 basis points, suddenly you need to reduce
your exposure just by law. That actually introduce a lot of volatility in the market. I already mentioned
precautionary behavior. Any situation where
precautionary behavior is important, big shocks. Let's think about the
great depression. We don't think that the
great depression can be explained without a small,
plain vanilla shock. How do we solve in big? Again, that will force you to some type of non-linearities. Convenience away from
this steady-state. Again, we usually think
about the US economy as fluctuating around
some steady-state or balanced growth path. But if you are thinking about most other countries
in the world, it looks much more like
they are converging. If you think about China, why is China growing at
a 10 percent per year? Well, because they are
converging to the steady-state. They are going to slow down for sure by converging trend. A linear model will not
be able to capture that. If we want to think about China, we forget for a second about the fact that they lie
about every single number. But you really want models that introduces
something like that. Here this is just craziness, except for the first one. Zero bound on the
nominal interest rate. I think it has been patently clear over the last two years that when you have zero bound on the nominal interest rates
a lot of things happen, a lot of standard results in models go in the
opposite direction. Mike Bouthfor calls this always the black hole
of deflation at the zero lower bound and how
the standard way in which market economies work to make
business cycles smaller, actually going the
opposite direction when you are at the zero bound. That's, again, a
fundamental non-linearity that is very difficult to
think about in a linear mode. There are issues about
approximation errors. I'm not going to get into those. Now, there needs to be truth in advertisement and
there also needs to be truth in advertisement
in nonlinearities. I do a lot of nonlinearities. I have fun with it, I like it, I'm not dogmatic about it. I'm not the type
of guy that if you come to my office
and you say, look, this paper I linearized
and it works fine, I'm not going to say
anything about it. Because Ian Robinson famously talk about economies as having a toolbox and we
need to think about nonlinear methods as another
tool in our toolbox, not as in that beautiful
quotation by Mark Twain, "To a man with a hammer, everything looks like a nail." I don't want you to get
out of this room and say, look, now I learned how
to do nonlinear models. No matter what, it's going
to be a non-linear model. Maybe for your particular
case it just doesn't matter. Also, the second other
problem is that some of those nonlinearities ask for a little bit more computation. Power and Taylor,
who was a physisist, one of the guys
with the hydrogen bomb on the nuclear program of the US has this great
sentence that "A state of the art computation
requires 100 hours of CPU time on the state of the art computer
independent of the decade". Another way to give it to you is every time that Intel gives me a little bit of
a better processor, my optimal response is to put more points in my grid
and not to reduce the amount of time
the paper needs to take to be computed. Yes, some of these non-linearities are
going to be a little bit more painful in terms
of computational time. What I want to mention though is that I will try to argue later, when we talk about
perturbation, that [inaudible], which I think that yesterday
you did a little bit, allows you these days to do second and third-order
approximations in a relatively efficient way. There is a few glitches
here and there. But I think that at this moment, the entry barrier into second and third-order problems is much lower than it used
to be and it's something that can be done in a
relatively efficient way. Well, this is my
general notation about how to solve this, but since Larry did that yesterday I don't
think I need that. I just want to finish
this first session. Unfortunately,
because we only have six hours I'm not going to have much time to talk about how to estimate these
nonlinear models. I'm going to concentrate
on how to compute them. However, I want to
take five minutes now to talk a little bit
about it and in addition, I distributed an additional
set of notes that are called something
like filtering unlikelihood or
something of that sort. I'm not going to talk about them unless I run out of
material to cover, which will be probably unlikely. But hopefully over there, I tried to present some
of the basic ideas. At least will be a pointer
for future research. Just let me give you the particle filter for
dummies introduction. When we linearize the model we can write the linear
state-space representation. Which just means we are
going to have a bunch of linear decision rules and we organize them in a
measurement equation and in a transition equation. There's something
absolutely beautiful called the Kalman Filter, which is a set of equations that allow you to evaluate the
likelihood of the model. Note that over there I put evaluate in Italics because this is not like when we take green introduction
to econometrics we write down the likelihood. This is just that the
computer is going to tell us that for some
parameter values, the likelihood has the value, I don't know, in this case will be the
log-likelihood, minus three. The only thing we are
doing is evaluating it. Well, what is the great thing about likelihood-based methods, either maximum likelihood
or Bayesian methods? That the only thing
that you need is to evaluate the likelihood. If you want to maximize
you say, well, if I know for every
parameter value, what is the value
of the likelihood? I just can try a bunch of them. If I'm smart enough
I may even read a book in optimization and learn how to do
this efficiently. But worst-case scenario, yes, do brute force. I try one thing to get it on different combinations
of parameters and I just keep the one where the likelihood has
the highest value. That will be my maximum
likelihood estimation. If you're a Bayesian, you can use something called
Markov chain Monte-Carlo, which is basically the
Metropolis-Hastings algorithm. Is a way to simulate
from that posterior, which remember is
just the likelihood times the prior and
then a constant. The interesting thing is
that for Markov chain Monte-Carlo you only need
to evaluate the likelihood. The Kalman Filter makes
your life very nice and that's why I will say that
over the last 10 years, there has been this big move in estimation of the AC models. People realize this
is very easy to do. You linearize the model, you build the Kalman Filter, you evaluate the likelihood, you are up and running. In fact, even [inaudible]
these days does it for you. At this moment we
know an estimation of a linear model is something that is actually a relatively
straightforward one. What happens when
we go nonlinear? Well, the Kalman Filter
doesn't work anymore. You need to do something more. By the way, it's 10:15. I'm going to do
intertemporal substitution. We claim at the beginning
of this lecture that we are people with high elasticity of intertemporal substitution, so I will go a few more minutes and then we will just
start a little bit later. Believe me, I'm not going to
default on this commitment. I was talking about how we do this non-linear estimation. People in the '70s and the
'80s tried things like the extended Kalman Filter and different variations
of the Kalman Filter. None of them actually
work quite well. You can actually show they
work not very well at all. Fortunately enough, in
the '90s people came up with the idea of using something that's
sequential Monte-Carlo. Which is basically smart
simulation method. I guess that these
days even if you go to Wikipedia you are going to find basic introduction
to sequential Monte-Carlo. I have written many papers about sequential
Monte-Carlo methods. I think they are relatively
straightforward to follow. My webpage I have a very
simple example of how to do a very simple sequential
Monte-Carlo that hopefully is easy to understand. Basically, the idea
is we are going to use a Monte-Carlo to
simulate a bunch of states from my model
and I'm going to be able to evaluate the likelihood
using all those states. Yes, in an absolute 30 seconds Kernel of what is going on. Think about, this is my period likelihood. This is the probability
of observing these data. Sup index here means
this particular period, conditional on all the
previous observations. Sup at index means it's the whole vector and
my parameter values. In general, evaluating this when the model is non-linear is
very, very complicated. But let me forget for a second about
this integral. Evaluating this, where this is the probability
conditional observation and the states is very easy. Because basically
what you do is you go to your decision rules, you plug in the states, and you just look
at what type of innovation in that period
will give you this choice. You say, well, if somehow I can come up with this
probability of these states, I can integrate and I
have my likelihood. Usually what we do in
econometrics is to try to integrate out the
stuff that we don't like. Sequential Monte-Carlo
is the other way around. We unintegrate. The way we are going to do this, this I know how to evaluate. This I'm going to simulate, that's what it's going to call
a sequential Monte-Carlo. This integral is just going to be approximated
as a sum, as an average. That's what the chapter on sequential Monte-Carlo tries to tell you a little
bit how to do. What I'm going to do
now before we break is, again, I know this was a
very short introduction. By the way, if any
of you guys want, I will be more than
happy later in some of the breaks or maybe
after lunch or when we finish to talk more about
these things is give you the basic algorithm of how you will think about this. The basic algorithm will be how to evaluate
the likelihood. The input will be
some observables, the DAC model, and
some parameters, and the output is
the likelihood. Basically, what you are going
to do is the following. Given some parameter values, you are going to solve
for the policy functions. This is what we are
going to try to do in the next three sessions. Then given those
policy functions, you are going to bright the
state space representation, ST is the state, this is the law
of motion for it. Given some states, there is
going to be some observable. This idea is going to be shocks, measurement errors,
these are going to be things like innovations. You are going to be able, doing this type of
sequential Monte-Carlo, to evaluate the likelihood. Then you can do
whatever you want, you want to maximize
it or you want to do this in a
Markov-chain Monte-Carlo. One, the fact that
we need to solve the model for each particular
set of parameter values is going to emphasize why in the next three
sessions we need to do quite a bit of work on being able to do
this in a fast way. Because look, if we just want
to do this for one set of parameter values and it takes one day, there's
not a big deal. We leave it running on Sunday, we come back on
Monday, it is there. But if we need to
run it 10,000 times, I think that most advisors
will be unhappy when Gravitas students tell
them that they will need 10,000 days to
run simulations. I had this student that would come to my
office basically to report that his value
function iteration had done one more iteration. I tried to express to
him forcefully and honestly that this was not considered
acceptable progress. I wanted a little
bit more than that. We really need to
do this for many, many combination of parameters. We want to do it fast
and that's why in the next sessions
we are going to try to do this in a little
bit efficient way. But the way I always
explain this to people is as long as you keep the basic idea of what we
are doing in your head, it's very easy to understand. You pick parameter values, you solve the model, you bright the state
space representation, you evaluate the
likelihood function. Then you put this in a
loop and do this for many, many parameter values and
you are ready to work. This will be the
case where you are doing maximum
likelihood estimation, which is you basically
try a bunch of different parameter values to see if you are getting them, if you are maximizing or not. In the case that
you are a Bayesian, where you will do it in
a Metropolis-Hastings. I'm not going to explain what a Metropolis-Hastings
is over here. But again, the idea is there's a simulation method to
do this efficiently. What I would like to emphasize here at this point is, again, many of the tools
I'm presenting here do not really depend
on the fact that this is a DAC model. For example, imagine that
the only thing you want to do is to estimate a model
with stochastic volatility. Forget about all the
behavioral relations. Just you say, look, I want to estimate a pure
statistical model. By statistical, I mean there is no behavioral framework behind, it's just a nice flexible
empirical specification. I'm going to estimate stochastic volatility
model of who knows what, a standard on SP 500. Then you will have, again, some law of motion
for the levels, some law of motion for the stochastic volatility that will be a nonlinear model. You will still need to use
a sequential Monte-Carlo. Then you will, again, plug it in into your maximum likelihood or into a Metropolis-Hastings. Or you say, look, this
is not a DAC Model. Imagine that I'm an
industrial organization guy. I do some of these models of
Markov perfect equilibria, alpacas style or
I do some type of models of replacement
or genres style. Well, if you think about it, eliminate where it
says DAC over here, put I0 and then it goes. You just have your model, it's going to be
a dynamic model. You are going to
solve that model hopefully in an efficient way. You are going to have a
state space representation. As long as there is dynamics, there is always a state
space representation. You evaluate your likelihood
and you estimate. Except, I'm a macro guy, I will estimate things
like risk aversion and discount factors
and the response of monetary policy to inflation. People in Iowa may estimate the parameters that determine the market
power of different firms. But in some sense,
these are just labels that we attach to mathematical
problems that are at a very core of tabulating. I could keep talking about not only IO,
but labor economics. I could any type of model of decision over the life cycle, any type of models in
international economics, even in international
trade these days, Mark Mellitus and
all these guys, Paul and Truss are doing all these things in
dynamic frameworks, all these type of things, It will be very
natural to set up in dynamic frameworks of the models with the computer
and estimate them. Those will be very
cool applications of all these type of things. 