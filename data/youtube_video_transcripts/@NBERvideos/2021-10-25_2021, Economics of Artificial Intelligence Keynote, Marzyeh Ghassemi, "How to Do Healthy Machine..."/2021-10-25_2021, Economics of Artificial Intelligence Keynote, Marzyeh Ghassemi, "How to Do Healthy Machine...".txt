so i'm going to talk to you uh briefly about my research in uh what i call healthy machine learning um and i say that because i think that uh we all in machine learning got very excited that we could uh sort of do things and then uh and now we're sort of taking a step back so machine learning and health 101 is we collect data on those who are sick right so we take hospital records or we look at clinical trials and then we predict when a bad event or an outcome like worsening sickness for example is going to happen and that's worked really well so we have state-of-the-art methods that perform at or above humans across the range of the human lifespan so every one of these lines is a paper where somebody did exactly that and machine learning researchers have really engaged um you know in the last decade there's been this explosion in a number of venues that are either having a separate track for submission of machine learning for health uh content and research or it's a whole conference devoted to that so much like natural language processing and computer vision goes to mainstream machine learning conferences and has their own side conference health is now a full-blown sort of application area however uh you know models are also these regulated advice givers right so the fda actually clears ai algorithms to give advice to clinicians as they're performing um care um this is really concerning to somebody like me because bad events in sick people is actually anomaly detection in anomalous data if you're sick enough that we're watching you you're an anomaly right most of the time hopefully you're not in an icu or in a hospital or having data being recorded about you and then you're asking us to detect an anomaly in this data that is already anomalous this is really hard technically this is not something we often do in machine learning and so we're working in a space where we don't have any idea what normal looks like we don't know what a diverse population looks like when they're healthy and so the question is what exactly are we learning i'm going to walk you through uh one example technically of something i've done that has convinced me this is a a big problem and then we can talk about uh the findings and so one part of my research is what models are healthy and so that's a lot of the work that i do in trying to develop models that are robust private and fair so let's focus on privacy for a minute so anonymization is not robust to a linkage attack right and because of that some people have said we really should be using um more advanced technical constructs like differential privacy if you're not aware technical uh differential privacy is the construct that was used by the us census it's used by tech companies like google and apple and it ensures that if any individual is too unique based on things that have been anonymized like maybe sumana is in data set a but based on her gender race age and zip code she's unique there's nobody else in that zip code of her gender race and age and so differential privacy ensures that some adversary who could access either the data itself the anonymized data or the outputs of a model trained on the data can't identify her but the way this is done technically and if you're in this space this will look very familiar to you it's epsilon delta privacy guarantees if you're not in this space don't worry the way that this happens is when you're training your neural network you sample a batch of data and then you learn something on it by measuring a loss so for example am i predicting mortality well when you take a gradient and see oh i'm not doing so well on predicting i'm going to move my neural network weights in this direction right so gradient is a direction what differential privacy does is it noises and clips that gradient if any point is pulling it too much because that data point might be uniquely identifiable right and so that's how through the cyclical process of training a neural network you ensure differential privacy and so we tried uh implementing differential privacy in a standard clinical prediction task which is yearly mortality prediction so i observe your past data and i predict uh whether you will die in the next year and so the issue with this is that uh differential privacy actually works really well in other areas so computer vision and natural language processing but in our task it actually destroys our performance going from something like 0.8.9 which is an acceptable level of performance in this kind of task to almost no better than chance even in these high-capacity models like gated recurrent unit neural networks that's really bad and what's even worse is that the data that loses its predictive influence on uh the test subjects when you add privacy is minority data so adding privacy changes the most helpful training group from black patients to white patients on black taste test patients and that's not because those patients are now more similar it's because who do you think is an outlier in the data set minority patients right and so machine learning techniques like differential privacy this is one example but there are many they're built on finding and enforcing similarity but the issue is if we're finding patterns in data and extending them and removing outliers like this is zero shot image retrieval if i know what an arm and a hand and a shirt look like then maybe i can figure out what a shoulder and a skirt look like even though i've never seen an example because i know what clothing looks like generally but what does it mean if a human is an outlier right like that's that's something that i think we haven't fully engaged with um in the machine learning for health space so i'm going to talk about some of the audits that we do um in my work and we do audits of machine learning models because bias is part of clinical care because doctors are humans and humans are biased right and so we're not observing perfect data so we did one experiment where we took three large chest x-ray data sets over 700 000 images and trained a convolutional neural network to predict no finding that means the patient the model thinks is healthy and then we're going to compare the false positive rates in different subpopulations and we can think about this as model under diagnosis rate because it's predicting that the patient is healthy when there's actually an underlying problem and if you deployed a model that had a higher false positive rate for healthy then uh in in one subpopulation then that model would lead to a higher rate of no treatment in patients that need treatment if it were deployed and so what we found was that state-of-the-art models that do chest x-ray-based diagnosis and this is a very common task several uh models have been cleared by the fda to do this have higher under diagnosis rates in female patients young patients black patients and medicaid insurance patients and the rate is actually much worse for intersectional identities so black or hispanic female patients are under diagnosed more than white female patients this is not the training data this is the model's under diagnosis rate in unseen test data and you might say well you know just audit it marzia you just showed me an audit and now that i know that maybe i can fix it some way with post-hoc balancing um as we did here in a paper on predicting mortality we saw that there were different rates of accuracy by ethnicity by insurance type and by uh reported gender however it can be really hard to figure out where those biases are going to lie and so this is one of my most favorite least favorite examples where we took a contextual language model that's publicly available you can download it it's called cybert and it's based on uh pubmed abstracts right so they trained on all these um these medical abstracts and then it generates um human realistic text and so we gave it a prompt blank race patient became belligerent and violent sent to blank blank and when i say caucasian or white patient the model fills in the blank with hospital but if i say african african american or black the model fills in the blank with prison and models like this are currently being used by companies to complete notes to chat with doctors to fill in recommendations these are real systems that are really being used this was not easy to find when my graduate student found it initially i thought it was a mistake surely they had trained the model on reddit not scientific abstracts health is really complex and so it generates data that is complex because it's part of this system and so we need to think about behaviors in this space more than just data or modeling or audits and so there's this beautiful study that i remember hearing about when i was an undergraduate where they found that ebay auctions uh for hand-holding ipods remember what an ipod looked like um if the hand was black it received substantially fewer offers than if the hand were white it's the same ipod and so uh we thought let's see how people feel about ai right so we did this experiment where we uh varied two things we varied whether we said advice that was being reported was from an ai or from a human and then we varied whether it was accurate or not sometimes it was false and then we presented eight cases to uh radiologists and to internal and emergency medicine doctors and we measured two things whether they thought the advice was good and whether they got the diagnosis correct and so we found that experts rate ai substantially worse it's the same advice but they rate the advice as substantially worse than the human advice this is not true for imem doctors and also their accuracies are no different in fact they're a little bit more accurate using the what they think is ai advice but what's maybe more interesting is that while experts have a diagnostic accuracy that's better half of them get seven out of eight cases correct which is really great that's not true for the non-experts they are susceptible at a similar rate to incorrect advice so every uh thing that contributes to this blue bar was when we gave correct advice and so if you see only blue bars that means that that doctor participant 60 here got no cases right when they were presented with incorrect advice so here are some doctors who got them all right and here are some doctors who got no case where they were presented with incorrect advice right and so if if we're seeing these kind of anchoring effects even when the advice is not produced by a machine learning model when it's curated device that we're evaluating it's really demonstrating that there's no simple fixes to do more ethical machine learning and health it's going to require an ongoing process of engagement with a diverse team i want to focus briefly on data collection before i close because i think it's something where as a community uh we've really started to engage in machine learning uh embodied data data from human bodies is really valuable companies buy up a lot of it but there isn't that much robust uh private fair high quality uh algorithms that can be developed because we don't have large-scale diverse data sets that are available for research use and that's a big problem because health is really lagging other machine learning subfields and reproducibility so if you look at other fields like natural language processing or computer vision or just general machine learning publications we're really underperforming on these code release data release and multiple data set facets and uh for anybody who's wondering because it's a pet peeve of mine after a comment i got uh during a talk last year we published a paper this year showing that synthetic data is not a robust solution to this problem so if you have a biased data set it can have different impacts on minority downstream classification tasks and this is true even if the real data set isn't directly used so even if you generate a synthetic data set for training that is balanced you still get the downstream effect it's also not true that you can rebalance that train a test time so uh if you for example uh are if you have access to a data set privately that's very biased and you create an embedding space and then you release that that biased embedding space and now other researchers are allowed to take that embedding space map their balanced fare data onto this biased embedding and do a downstream prediction we still find that having a biased embedding space affects your downstream performance even when you train with balanced data and that's because on this sphere that we've created an embedding space from there's a hole where you might be able to map certain patients to and so it's just not representationally rich enough to accurately classify them later for downstream tasks and this is just you know the tip of the iceberg i'm very interested in these issues uh and we're sort of working on things that we can see but there are really a much deeper and richer set of problems that the community is working to tackle this is work from students and collaborators and i think that uh while my talk maybe sometimes seems a bit cynical i do think that we have the opportunity to create actionable insights in human health i think that it requires that we engage across the entire spectrum of these research challenges thank you 