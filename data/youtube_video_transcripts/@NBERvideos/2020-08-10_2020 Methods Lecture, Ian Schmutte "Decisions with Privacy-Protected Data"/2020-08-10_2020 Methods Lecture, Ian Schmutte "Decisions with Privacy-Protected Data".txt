Ian Schmutte: In this lecture, I'm going to springboard off of Dan's introduction and
spend some time talking about three topics that I think economists may find particularly interesting and where I
think economists have some potential to make
contributions to research. There'll be complimentary
to the statisticians and computer
scientists who've been working on these problems. In particular, I'm going to talk about three different things. I'm going to think
about the problem that statistical agencies
and other data custodians face when
they're trying to trade off privacy protection
versus data accuracy. Differential privacy
as Danny got into provides some tools help clarify how we think about balancing these two objectives. I'm going to argue
that the decision about where we land
is fundamentally an economic problem. Then I'm going to turn to thinking a little bit
more systematically about how differentially
private data allow us to be more careful
when we think about learning from
privacy-protected data. As social scientists, we
know that we need to be careful to take
account of the fact that the thing that we're interested in learning
about is not always necessarily the thing that's
being measured in the data. Differential privacy gives
us a system for doing privacy-aware analysis when
we're engaged in research. Then finally, I'm going
to talk a little bit about the concept of accuracy. We'll talk a lot
about trading off privacy and data accuracy. But whereas the concept that we get from
differential privacy of a notion of privacy loss is extremely specific
and very precise. The notion that we get of
accuracy is not so clear, so I'm going to discuss
accuracy in terms of improved decision-making
towards the end of the talk. Let's get right into
the first thing. Statistical agencies face
this dual mandate where they have collected some information we're going to think of
it as being confidential, and then they're
charged with producing accurate statistical
summaries that can be made available to the public for all decision-making
purposes. But the same time, they're responsible for
maintaining the privacy, sensitive characteristics
for the individuals and businesses that have interested in them with
their information. What we get from
differential privacy is a very specific concept
of what privacy loss means. It allows us to make this
concept of a trade-off between privacy and
accuracy very clear. We know from various
theorems that each calculation that's
based on the data is going to consume some
part of the privacy budget. Danny alluded to this and
I think he's going to discuss composition
more in a later talk. But essentially say if we're going into the current
population survey, ignoring issues of
sampling for the moment. Say we want to
know the number of unemployed individuals and
we're going to publish that say using the
Laplace mechanism with parameter Epsilon 1, one and that gives us a total
privacy loss of Epsilon 1. Then we go back and
we want to calculate, say the number of labor
force participants. Well, they're going to be the same individual so now we've incurred a second privacy
loss of Epsilon 2 and so on. The total privacy
loss accumulates in this additive fashion across all the different statistics. We can either think of
this as the total cost of producing the statistics
that we're interested in. Or we could think about it
in reverse that we may have a total privacy budget and then we have to figure out
how we want to allocate, say that Epsilon total across all of the
different statistics that we're interested in. What that means then is
that it say if we want to obtain an estimate of
the employment count, that's more precise and
we're going to have to sacrifice precision
somewhere else. These are not any more
abstract decisions. The Census Bureau
is making decisions about where to send this
privacy loss budget on different products with
increasing regularity as it shifts to thinking about formal privacy as a default
mode of protection. I've just listed here a few of the different
settings of Epsilon for some products that we'll
talk about later on today. In my 2019 AR paper
with John Abag, we argued for taking an economic perspective to this question of
where to set Epsilon. We know that we can think of the data publication problem
as having two parts. There's a production function
so differential privacy essentially gives us some
production technology. That production technology
tells us how we can convert information in the database into information and statistics that have a
given level of accuracy. Returning to this
earlier example, if we think about
we want to publish, say the unemployment
count and we want to entail no privacy loss, then we would be
setting the variance, save that Laplace
noise to infinity and essentially it would
be just as good as not publishing anything. We might as well
not even bother. Conversely, if we wanted to
incur infinite privacy loss, we were willing
to tolerate that, then we're adding Laplace
noise that has no variance. Then we might as well
just be publishing the sensitive data
in the first place. This is our production
technology. This gives us a locus of points. Their points are available
in-between the describe, the different combinations of privacy loss and accuracy
that are feasible. We really have a
finite resource. It's this information in an existing database and we
can allocate it in-between, generating statistics
that we can use for calculations that have increasing accuracy or
we can reserve some of it to protect against
privacy loss. The key idea that comes
from economics is, we know this is fundamentally
a social question and what economic analysis tells us that an optimal allocation
should equate the marginal rate
of transformation. That is, how much it costs to increase statistical
accuracy or increased accuracy and
decision-making measured in units of increased privacy loss. We know that optimally we want to equate that to
the willingness to pay for increased
statistical accuracy in terms of foregone
privacy loss. From the differential
privacy side, from the computer
science side we get a good notion of what this production
technology looks like. But we do not have a
very good notion of what these social preferences
look like or what the social welfare function would look like if we
tried to account for the value of
statistical accuracy and the value of data privacy. Where more theoretical
and empirical research is needed is in coming up
with better ideas to think about how we understand
or conceptualize the value of obtaining more statistics or obtaining
more accurate statistics. For instance, how much would we be willing to pay right now to have more accurate
information about, say the prevalence of COVID-19 cases in a
local jurisdiction? Or how much would
we value having had somewhat more accurate
statistics from the current population
survey during the pandemic and lockdowns? Those are questions that
we can pose precisely, but we don't have
clear answers to them. We don't also have
clear answers to understanding the costs of incremental privacy loss
when we measure privacy loss using the concept of Epsilon that we get from
differential privacy. It's idea of bounding a base factor on
posterior inferences. What's the actual cost associated with incrementing
that privacy loss? These are questions where
research is ongoing, but I think more economists have a lot potentially
to contribute. Moving on, the second topic
I want to think about is the challenge of learning from privacy-protected data. In this context, I want to focus on what I think
is a false dichotomy. Some people that's out
there that the data that we're currently
using are not subject to tinny privacy protection. When I get into it, I want to develop a concept of a privacy aware analysis and this has come up in
some of the previous talks. I'm going to work through a very general model
just to fix ideas. The idea here is that
we can think about the statistical agency
is having data that come from some population and is in possession of a
complete data matrix. We'll ignore for the
moment questions of sampling and missing data. Statistical agency
is in possession of the complete data matrix D, and as researchers, we might think that there
are some data generating process which we
could specify in terms of the data model parameterized by
this thing we call the process parameter Theta P. We might have a
prior over that. In general, there are two
estimators of interest, one would be functions
of the data themselves. These would be finite
population estimates so that would be
something like we need to know the total number
of individuals in each state for the purposes of congressional apportionment. We might also be interested
in learning functions of the superpopulation so functions of the process parameter. The idea that I want to
develop is with analogy to the idea of
ignorable missing data. We could also have the concept of ignorable privacy protection. The data that we observe are not the data D that were collected, the data that we
observe are Data Z, the data that were published. This is the case
whether or not we're thinking about a system that has been operated using
differential privacy or indeed the types of privacy
protections that are currently being used at the Census Bureau and other statistical agencies. The data that we get out as researchers are not the same
as the data that went in. Regardless, the data
were subjected to some privacy model which
we can think of as being probabilistic
or deterministic, where the probability
of observing the published data depends
on the input data and some parameter that describes the privacy model so in the randomized
response application, for instance, Theta
M would be P, the probability that the
respondent is told to lie, or the probability
on the spinner. That parameter we
can think of is also having a prior and if we have all this machinery then in the most general case, we know what the likelihood
is for the published data, we just obtain it
by integrating out the true data from the product of the privacy model and the data generating model. In that case, we'd
be doing inference on the posterior distribution of the process parameter given the published data which again, we could obtain by
just integrating over the posterior distribution for the process parameter given the true data and the posterior
predictive distribution for the true data given
the published data. We can characterize ignorable
privacy protection here, is just saying that
privacy protection is ignorable and exactly the case where the posterior distribution for the process parameter given the published data is exactly equal to the
posterior distribution for the process parameter
given the true data when the true data
happened to be equal to the published data. Or in more simple
terms, basically, privacy protection
is ignorable if we can just plug
the published data into the true data estimator and everything will
work just fine. If that's the case,
then we'll say the privacy protection
is ignorable and so this leads us to three
questions we should be asking about any privacy
protection system. First, is it ignorable? Second, if it's not can a privacy aware analysis be conducted based on what we know and this means that
we're going to have to have some information about
the protection model throughout the privacy model. If we don't have any information about the privacy model, is there some way to
discover that information to back it out from things
that we know in the data? Bring this a little
bit down to Earth, first, privacy protection method we can think about
is top coding. Top coding in the current
population survey says that if your household
income is 200,000 or above, it's going to be suppressed
so that we don't give away too much sensitive
information about households. In the current population
survey in 2017, that threshold, there were between seven and
eight percent of households were
above the threshold. This form of statistical
disclosure limitation, researchers are used
to thinking about it. This is the intuitive thinking about privacy protection
engaged in all the time. We know that this
privacy protection is ignorable if the inferences
that we're interested in, say our quantiles less than the fall below the top
code so we know that we can estimate 90:10 earnings ratios or income ratios in the Current
Population Survey. Of course it's not ignorable
if we're interested in quantiles above
the threshold. Occasionally if we have some additional
information about the distribution of incomes
above the threshold, then we could engage in a
privacy aware analysis. But there are other
more insidious forms of privacy protection that
are currently in place. They're not formerly
private, for instance, suppress and impute
says if there's a value of a sensitive
characteristic, say it's age at 50, then I'm going to
suppress that value. I'm going to use some
imputation model and replace that value with an
imputed value, say 51. What we know about
missing data imputation in general from a paper by Chris Bollinger and Gary Hirsch in the journal of
labor economics is that it's not
ignorable in most cases. If the imputation model is less rich in a
sense that they make quite precise than the model that you're trying to estimate, then the use of the imputed values is
going to induce bias. Of course, for missing
data imputation, you can throw out the imputed values because
they're flagged in the data but when suppression and amputation is used for privacy protection, the values aren't flagged. In this case, the
privacy model is not ignorable and it's not
possible to engage in a privacy aware
analysis because we don't know the rate at which suppression is
being carried out, we don't know the model
that's actually being used to impute the values. In fact, we don't even know what the variables are that
are being imputed. It's also not possible
to discover what this is and try to
adjust inference for it. Swapping is a similar story. Swapping is another
method that also tries to protect sensitive
characteristics, essentially operating
in the following way. If there's a record that's
deemed to be high risk, say an individual who
lives in, say, Athens, Georgia who has a
particularly large family and/or high income, then you can swap that
record on the basis of some matching characteristics to a nearby jurisdiction where that individual is
not such an outlier. You're essentially
trying to move outliers into areas where
they're not outliers. This has the advantage
that it preserves counts on key
characteristics and so it allows enumerations to remain on particular characteristics,
to remain fixed. It can help with preventing the disclosure of
sensitive attributes. It's ignorable only
in the case that you only care about the
matching variables. It's not going to be ignorable. In fact, it's going to just
destroy the covariance between any of the matching
and the other variables. Once again, the parameters
of this process are secret. In fact, they're kept secret deliberately because
it's believed that publishing information
about these parameters would allow attackers to undo the privacy protection
that's built in. This is unlike formal
privacy models where the whole point is
that the methods can be made transparent, which then facilitates
privacy aware analysis. These distortions
might matter a lot, but we don't actually know because we don't know
anything about the model. We don't know too much about the error that they
introduce into the data. There's a fairly
interesting analysis in a paper by Alexander
Davern and Stevenson, where they compare
population estimates from the 2000 census, the tables that are based on the 100 percent enumeration with tables that they generate themselves from the
five percent public use micro-data
sample and they're looking at population estimates disaggregated by age and gender. The way that these
protection mechanisms work, they should be implemented
the same way on the data. They're used to generate
the 100 percent tables as on the public use
microdata sample. That means that up to
sampling error the estimates from the public use
microdata sample and the published
tabulation should agree. That's the case
for men and women whose ages fall below about 64. The two estimates line up and where do they start to
separate is around age 65, say in the five percent
microdata sample. For men, we see that there
are too many men close to age 65 in the public
use microdata sample relative to the 100
percent tabulations. Then again into few men
between ages 67 and 80. It's not entirely clear what's happened here other than
that, we know that, according to some
technical notes, this is due to some
misapplication of privacy protection mechanisms in one or both of these sources. It looks like at
least potentially, that the distortions
introduced by privacy protection methods could be ignorable if what you're
interested in is prime, working age individuals, and
perhaps not ignorable at all and in fact quite
destructive if you're interested in individuals
above age 65. But of course, if they
hadn't made this error, we would never know even that
much in the first place. This is not exactly telling
us entirely what's going on, but it suggests that these distortions could
matter quite a bit. Final form of traditional
privacy protection, I'll discuss cell suppression. Cell suppression, I think most economic researchers
will be familiar with, but the idea is that
especially in business data, we could have cells if
we're looking, say, at a very narrow
accounting industry, we might have a cell where
one large firm dominates. By looking at the
tabulated data, you might be able to
infer something like the total receipts or the
employment trade secrets of that that firm. The Census Bureau is committed to protecting those firms
against that disclosure. The method is to
just blank out cells where you'd think that might
be particularly disclosive. But because tabular data have the property that they
need to sum up to margins, then that means that you can't just
blank out the cells that you're worried about because you can just do
some subtraction, figure out what the
suppressed value was. Then you've got to blank
out some more cells to prevent these
subtraction attacks. Then you might have to blank
out some more cells to prevent some traction
attacks on those and so on, assuming putting increasing
number of holes in your data. This is a protection
method that's used in tabulations from the economic census and had been used in the accounting
business patterns. Self suppression
is not going to be ignorable unless the suppression was random with respect to
your estimated interest. But since it's
explicitly a function of how dominant firms are
relative to the data, it seems like there
are many cases where that's probably
not going to hold. That's not ignorable.
Then what can you do? Can you engage in a
privacy aware analysis? In general, we don't know exactly what the
suppression rule is, but there's another
thing you can do. You can try to hack
the protection. There's a very interesting
paper in the 2020, NBER working paper
by Eckert, Fort, Schott and Yang, where
essentially what they do is use commercial
grade linear solvers to take all of the
restrictions that are implied in the county
business patterns and use that information to try
to reverse engineer what the suppressed values in the county business
patterns were. I think this is a very
interesting approach, and I'm very curious about
exactly how effective it is. I don't actually know. But one suspects
that it could be. What's interesting
to observe about this is now we have
a situation where the researchers pursuing
their interests and getting more
accurate statistics are actively trying to undo the protections that
are being built into the data by the data custodians. This seems to me
to be a situation that is undesirable
when we could replace this with a
system where we could offer the same degree
of protection to the firms contributing
their data and allow researchers to have as much information as they
need to make adjustments for the privacy protection and to not be able to undo
the privacy protection. We know that suppression
is not ignorable. In a paper that I'll talk about a little bit more
in my second talk, Chetty and Friedman introduce a method of privacy
protection that they used to publish a dataset called The
opportunity Atlas. They were quick to
point out that if instead of using the formal privacy
model that they set up, they had instead used account suppression rule
that it would in fact destroy inferences on
certain questions that might be of interest to
social science researchers. They focus on the example of looking at the relationship
between the single parents share and attract and the teenage birth rate for black women raised in
low-income families. It turns out that the
cell suppression rule, which suppresses cells that have a small number of individuals, is correlated with the single parents share and with
the teenage birth rate in a way that alters the estimated relationship
between these two. Again, the point is with
the count suppress data, it would be very difficult to conduct a privacy
aware analysis that would allow you to draw the inference that you would get from the noise and fuse data. We know that analysis
needs to account for both the phenomenon
that we're interested in, but also the way
that that phenomenon is being measured
and reported to us. But accounting for
traditional privacy models either can't be done
or in extreme cases, actively undoes the
privacy protection that we were trying to
achieve in the first place. What that means is that
privacy aware analysis requires more transparent
formal privacy systems. This is one of the
key advantages of working with differential
privacy models, that we can know
exactly the process by which the data were processed. Then if we hold to
Danny Gorbachev's idea that if you can't deal with the distortions that come
from a known distribution, we're going to take away
your social science card. Maybe that's a little
harsh, but probably not. Last thing I want to
focus on is trying to understand what exactly it is we're discussing when we're
talking about accuracy. In other words, when
you draw this diagram, we know that the horizontal axis is this measure
epsilon that tells us about the maximum increase in the base factor
for some adversary. How much an outsider
can learn or increase their
posterior odds about, say, some sensitive
characteristic. The concept of accuracy is
not so clearly defined. It's important for us to
think about what it is so, especially if we're
interested in being able to calculate optimal
privacy laws. I'm going to think
now about accuracy, as it applies to
decision-making. In general, what we're
trying to do with data is not just publish statistics, but we're trying to make decisions on the
basis of those data. Again, if we think that D is
a population level dataset, q of D is some population
statistic that we're interested in and now a
is our published output, or accuracy is always
going to be based on some concept of a loss function. The question is,
how are we going to define that loss function? We can think about
it in terms of the different types of decisions
we might need to make. In social science research, we might be either trying to learn the
parameters of some model or engage in hypothesis testing essentially amounts
to decision problem. But the data are also
going to be used to make public policy decisions or used by businesses to make
business decisions, say about where
to locate a firm. The question is whether or
not those different use cases require different concepts of accuracy and if they do require different
concepts of accuracy, how would we balance
them off when we're trying to again make these decisions about where to set the privacy loss budget. It also applies to what
mechanisms we plan to use. This problem is hitting the Census Bureau
hard at the moment. This is the list of proposed
accuracy measures that I got from a Census Bureau
document published in March. This is the list of different accuracy measures
that they're using to determine whether or not the 2020 disclosure
avoidance system, which is using
differential privacy, is going to be fit for
various use cases. In that document, they had described six different
categories of use cases where you're
allocating some resource, but there's a zero-sum total. Or you're trying to
think about allocating some total across the
different categories. You need to know
information that's precise about a single year
of age and so on. Each of those different
use cases may be more or less well-represented by different types of
accuracy measures. We've got an increasing
kitchen sink of different concepts of
accuracy that are being used to evaluate this
disclosure, avoidance system. The mean or median
absolute error, the mean or median
numeric error, the root mean squared
error coefficient of variation, and so on. This highlights that in preparing these formal
privacy systems, there are a lot of
different stakeholders. All of these different
stakeholders have different things that they
want to use the data for. What ends up being
accuracy for one, may not be accuracy for another. The question is, is there a
more systematic way that we could think about
addressing these issues? In the interest of time, I'm going to skip past this application to thinking about with
policy decision, but I'll come back to this in a different
way towards the end. There are a few options that the Census Bureau has
or that in general, we might imagine having to
solve these sorts of problems. One will be to just sort of make Bespoke publications that are tailored for each
specific application. We know that there's a
specific application, say, publishing data for the Voting Rights Act
that might require one model that
delivers accuracy and a one concept of accuracy,
and that's different. We could return to
the data multiple times as long as we stay within the privacy budget and publish data using different mechanisms. Another possibility
is to reserve part of the privacy budget to get improved inference
on particular questions. This is the concept of special
tabulations, which again, we might use if we wanted to get improved classifications
for a particular problem. I suspect something
like this will be part of the system anyway. The third option that I want to focus on for the rest of my time is that we could use
mechanisms that are broadly optimal for a wide
range of uses. This brings in the idea of universally optimal
privacy mechanisms. The basic setting
is we have, again, I'm going to think about this in the context where we have a single counting query so
we have a data-set and we want to say count the number
of entities in the data-set that satisfy some criteria, like individuals
who are unemployed. Then we have different
data users i, who all have potentially
different preferences. Their preferences depend on some choice variable so some action that
they're going to take a_i and the true value of the counting query or the
true state of the world. Given the published output
that comes from mechanism, data users or these
individuals are going to make choices based on
their expected utility. Where their
expectations are over posterior beliefs about the
true state of the world, the true statistic Q of D, given the output that they
saw that was published, what came out of the privacy
protection mechanism M of D. In this setting, we're imagining data
users that are engaged in a privacy aware analysis. I think this is an
important point to think about and thinking about publishing data using
formal privacy. If we imagine that data users
are using the data naively treating them as though they are not being subject to distortion, then it makes the analysis
more complicated, but probably more realistic. In a 2012 paper Ghosh, Roughgarden and
Sundararajan introduced something called the
geometric mechanism, which is just like the
Laplace mechanism except for instead of adding Laplace noise to the count
of interests Q of D, we instead add geometrically
distributed noise that scaled to the
privacy loss epsilon. Geometric mechanism is provably
epsilon differentially private but even
more interestingly, it's provably
universally optimal for a particular class of
information consumers. What it means for it to be
universally optimal is that if data users have access to the
output from this mechanism, they couldn't do
better by going back to the data custodian and asking for a date of publication that was tailored
to their specific use case. This is not only
theoretically interests and the way this works is
the geometric mechanism, as it turns out, is able to publish count data in a way
that make the privacy, the constraint that
the publication be differentially private
makes that constraint tight. You're getting as
much information out of the publication as
you possibly could. Good news is that the
geometric mechanism is approximated by the Laplace distribution,
which as we've seen, is in regular use and we'll
see in a second talk, but it's also easy to
sample from indiscreet, so it works well with
these kind of count data. Bad news is that this
universal optimality result requires that the actions have
the same finite domain as the outputs and the payoffs are maximized when the action
matches the tree count and loss is symmetric when
the action matches the count. It corresponds to
things that we think of like quadratic loss
would be an example of a use case here but this universal optimality
result does not apply to other types
of decision problems, like the Voting Rights
Act application that I didn't go over
but essentially any type of classification
problem where you want to make a
classification or a decision that changes
discretely when you pass some threshold
and others like it. To explain exactly
what kind of problem, so simplified Voting
Rights Act application of policymaker has preferences
where the count that they're interested
in and say the count speaking Russian who
have limited English because we think that
we might have to compel a jurisdiction to provide
voting materials in Russian. In this simplified example, the action is just
in the space 0,1, this is the Voting Rights
Act classification. The ideal case is that the action that's
chosen is exactly equal to the action that the assignment mechanism would choose if you knew
the true data. There's more good news and some more bad news
with regards to this. In a paper that I've
been working on with my University of Georgia
colleague Nathan Yoder, we've been thinking
about the date of publication problem as a constrained information
design problem. In this setting, we think
about the data custodian as trying to design
informative signals on the basis of the
data that they have, but they're operating under a differential
privacy constraint. It turns out that
in that setting, the geometric mechanism
is optimal in any case, as long as the decision
problem is monotone. That is in any setting
where you want to choose a higher action when
your beliefs put more weight on a higher count. This would include
classification problems like the Voting Rights
Act and it means that if that was the policy problem that
you were trying to solve, then the decision-maker, again, as long as
they can conduct a privacy aware analysis, would actually not
do any better, by asking Census to provide
a classification directly, they could just do the
analysis based on the counts published with
geometric noise and obtain equally accurate
classifications. The bad news is that the geometric
mechanism doesn't work for non-monotone
decision problems. Non-monotone problems are ones where you want to take
a particular action if this count is low or if it's high and do something
different kind of in-between. An example you
might think of is, if I'm osha trying to
decide how to allocate inspectors if the number of accidents saying a
firm is very high, I might suspect negligence
and send inspectors there, if the number of
accidents is very low, I might suspect fraud and send inspectors to
investigate that, but if the number of accidents sort of matches my expectations, then I might not target
inspection activity there as much as I went
to these other two cases. You can also think about SARS,
COVID-19 antibody tests. If I'm teaching my
class in the fall, if I knew that deposit, if I had access to accurate antibody counts,
should be really cool. If the positive
count was very high, I might not be too
worried because I know there's a high
degree of immunity, so, well, science is evolving. I might not require masks. If the positive
count is very low, then I think that the spread is under control and so I
won't worry about masks. If I've got an
intermediate count, then I might maximize my
precautionary behavior. Takeaways, for counting queries, publishing the geometric
noise could be optimal for a wide range of
monotone use cases but it does require that we assume that
the users are going to be engaged in some
privacy aware analysis. If the users end up treating the data as though
they were undistorted, then we have problems. There's more bad
news in that there's no universally optimal methods for many non counting queries, there's a series of impossibility results in a paper by Brenner and Nissim in 2014, essentially show in cases
where you're either considering decisions based on multiple accounting queries
or other types of queries that it's no longer possible to develop
universally optimal methods. Just to close out, other types of decisions that economists
will be interested in, of course, statistical
decisions with differentially private
population statistics. I'm not going to
say too much here. Dan, in his next talk, I think is going to talk
about privacy aware hypothesis testing and he's got a couple of papers there. Literature seems
to be evolving and thinking about when
and how we can test hypotheses when we have access to data that have already been distorted by
differential privacy. In another setting of
interactive data analysis, which Danny Gaurav
talked about in his opening comments
is the setting where you get to choose questions to ask,
differentially private filter. Then when you see the results of your queries, go
back and ask them more. There's some good news here, and that is that
as he articulated, differential privacy seems
to be able to help in preventing overfitting
or generalization bias. What's exciting about
that is that even if multiple research teams had access to the same query system, they present
generalization bias across the ensemble of analysis
even across different teams. There's some bad news
and that is that there's a very recent paper by
Komarova and Nekipelov, showing that point
identification for certain estimators that we tend to be very interested in, like regression
discontinuity designs might be impossible. This is an area where there's active research it in
and I think growing need for more analyses
on these topics. 