I'm received Italian from Prince University Matthias and I are going to jointly talk to you about regression discontinuity designs I'm going to be doing uh the first part and Matthias is going to take over the second part and we're gonna have a 10 minutes in between um we are going to continue the causal inference on program evaluation framework and so we're going to be thinking about a framework where our goal is to learn about treatment effect so basically the effect of a treatment or a policy or an intervention and as we know if the treatment is randomly assigned then it's going to be relatively easy to estimate effects although there can be complications but in at least straightforward how to proceed but if the treatment is not randomly assigned then we're in the world of observational studies and we need to invoke some sort of assumption like selection of cerebrals instrumental variables assumptions some other assumption the the feature that all these assumptions will have in common is that they're not going to be um they're not going to be known to be true they're not going to they're not going to be true by construction they're going to be held as assumptions and we're going to hope that they're true and we're going to do more than that we're going to try to provide some evidence that is consistent with their being true but it's always a complicated but it's always you know a complicated problem and you know how credible are uh you know our estimates and our conclusions based on observational studies so the regressions continually design is one observational study is one type of observational study that shares some of the limitations that are you know inherent to observational studies um but have some advantages relative to other observational studies and the first one is that it's based on a simple assignment Rule and this rule is based on external factors and this creates an objective basis to evaluate the assumptions that we're making and this objective uh kind of grounding in in you know in reality and in some like actual uh you know rules that are that are verifiable allow us to falsify uh the design in a way that other designs uh can be falsified and so this increases the credibility of our designs they're also easy to interpret and of course they have some uh peculiar uh uh characteristics and some limitations and one in particular is that the parameter that that uh we can uh study is is is very local and we're going to see uh what that means so what is the regressions continuity design is defined by a triplet of a score a treatment and a cutoff so the units in the study receive a score and I'm going to call that score um X and the treatment is going to be assigned based on the score according to a rule and without loss of generality we're going to assume that the treatment is given to units whose score is greater than cut off and the treatment is withheld from units whose scores are below the cutoff and when I withhold the treatment I'm going to call that control um and so you know if you plot the score on the x-axis and you plot the you know probability the conditional probability of being assigned to the treatment or their control as a function of the score you're going to see that it's zero for all values of the score that are below the cutoff and it's one for all the values that are above and so there's a discontinuous jump in the probability of being assigned to treatment from zero to one in all Rd designs and so there's you know abruptly I go from having a probability zero of being assigned to the to the treatment to a probability of one and so the regression is going to be design is basically a design that is based on this type of this continuous assignment to treatment and what we're going to do is we're going to place assumptions on top of this assignment rule so that the abrupt change in this probability of treatment assignment will will use that abrupt change as the basis to learn about uh about the treatment effect so there's many aspects of the regression that's continually design that we can that we can talk about we call it the regression it's continuity taxonomy uh there are different Frameworks and what we call the Frameworks or approaches our approaches for identification and interpretation analysis so we can make continuity assumptions and extrapolate or we can do local randomization assumptions we're going to talk about this in detail we can think about uh settings where the score is continuous versus where it has repeated values there's also different kinds of settings in terms of whether compliance with the treatment is perfect or imperfect and that distinguishes between sharp and fuzzy there's also situations where we can have a multi-dimensional cut-off uh variable a multi-dimensional score variable uh we can also think about a time as a running variable or dynamic treatments and so those are going to Define different settings that I'll discuss briefly and give you an overview here but we can also think about different parameters of Interest so we are going to base most of our discussion in the average tree manifest of some sort and we will discuss you know different types of average effects but like in any other you know causal inference setting we can think about uh quantile facts or distributional effects and we can think about uh partial effects we can also think about um you know heterogeneity by subgroups and sub-publishers and we can also kind of think about generalizing the parameter of interest in talking about extrapolation which we if we will have time we'll discuss later so what I will do is I will give um a little bit of an overview about you know the general Frameworks and I'll talk a little bit about different kinds of settings to give you a flavor of what different kinds of regression this continuity setups uh you can encounter the regression is going to know it is not something that you design typically something that you come across and so to give you an overview of the different types of designs that you might come across might give you an idea of you know when can I apply these tools with different kinds of contexts uh and then uh in the final part of my uh presentation I'll talk about local randomization methods and Matthias will take it from there um so let me introduce a little bit of notation and I want to introduce a notation uh in the context of a randomized control trial or RCT uh because uh we're going to be discussing the differences between rcts and regression discontinuities in you know in the next hour or two so um similar to Alberto the notice is going to slightly uh change so Alberta had YN I am going to have y zero but we're going to adopt a potential outcome framework so we have potential outcome under control y zero we have the potential outcome under treatment and we have X in an RCT you can think of X as being any covariate in the Rd design that's going to be that you know the score I'm going to index units from by I from 1 through n and the treatment is going to be T and it's going to be a binary treatment for the rest of our presentation and in our city you know as We Know by virtue of the treatment being randomly assigned the treatment is going to be independent of the potential outcomes also of any pre-trument covariates and you know the observe that we're going to have an observed outcome so the data that we observe is going to be the observed outcome we're going to call it Y and then data on T on X for all the units and of course we have the fundamental problem of course a lot of influence that we only observe potential outcomes under control if I give you the control and we only observe potential common to treatment if I give you the treatment and that is a fundamental problem but in the case of an RCT it kind of it has an easy solution which is just go to the treatment group and you know get the observed outcome in the treatment group compare that to the Observer outcome in the in the control group at least if we're talking about average treatment effect it has an easy solution we can just readily identify the average treatment effect in the way that I know everybody um connecting today nerves so that's that's you know the setup of an RCT I'm going to keep the notation and now I'm going to change the assignment I'm going to go from an RCT to a regression this continuity and so what's going to happen is uh if you have you know the independence of between T and the potential outcome is really going to be replaced by something that is you know has nothing to do with Independence which is basically an indicator that it's going to be uh one whenever our score uh is above and known cutoff that's going to be fixed and zero otherwise the data is the same data as we have before I'm now using X is going to be the Rd score and we still have the fundamental problem of causal inference but we don't have the easy solution anymore because we don't have Independence between the treatment and the potential and the potential outcomes and so now the question is what uh what are we going to do to try to identify uh you know some kind of average streaming effects and what can we do and so we're going to talk about two different um so so the first thing to note is that um there's no identifying assumption that holds by construction so this uh this continuous rule for the for the assignment of the treatment doesn't imply but it doesn't imply uh in any way um the identification assumptions that we need and so we need to place those assumptions on top of the assignment rule uh and so it's um it's up you know we we have different choices of what assumptions we want to impose and then based on those assumptions we'll be able to you know identify a treatment and go from there and think about estimation and so on so we're going to talk about two kinds of assumptions um and those are going to be um focusing on different types of effects so we're going to Define um in what we call the continuity based assumption we're gonna assume continuity of regression functions and we and we're going to define the parameter of Interest as the average treatment effect at the cutoff so this is an average human effect of the kind of it's a little bit different from what you are used to seeing because we're not used to seeing of uh average human effects at a particular point and so Matthias will talk about um the you know the implications of that uh for for for estimation and inference um when we can also make different kinds of assumptions and instead of you know instead of making continuity assumptions and focusing on the average treatment effect at a point we could uh Define a parameter of interest that is the average treatment effect in some window or some interval around the cutoff so it's not a point but now an interval and we can think about the average human effect in that interval and that's what we're going to call the local randomization uh based um approach to organ analysis both of these analysis each one of each one of these approaches is going to make a different assumption uh that it's you know going to be placed on top of that discontinuous assignment rule is going to have a different way of exploding exploiting the uh the the the idea that you know above and below the cutoff we're going to make some comparison um and we're going to discuss both of them uh in the next uh you know hour and 50 minutes hour and 40 minutes um I'm going to be focusing on uh local randomization Matthias is going to come back and then it's going to talk about continuity base so when you think about continuity base this is the canonical plot that you're going to see to represent um this um this approach so you're going to have the score uh in the x-axis and then you're going to have the you know the potential outcomes on the y-axis and you're going to plot in particular you know what we call you know the regression function so the conditional expectation of the potential outcomes as a function of the score and um and so basically you're going to have the regression function on the treatment the regression function under control and you're going to define the treatment of Interest as that you know vertical distance between these two regression functions at the cutoff and you're going to baste you're going to base of course this is an unobservable effect and you're going to base um identification on continuity assumptions about these function so Matthias will give you more um details so in the local randomization approach we do something a little bit different and I'll come back to talk about this um uh by the end of the presentation uh we basically assume that um there's some window um so basically we Define the average treatment effect in this entire region here in this entire window and we assume that in that window basically that the regression uh the regression functions are not going to be functions of the score and that's going to allow us um to um to estimate and recover treatment effects so those are two different approaches um we're gonna cover those um in the remainder uh I'm going to cover local randomization materials will cover continuity based but before I go into the details of the local randomization approach I we wanted to give you an idea of this taxonomy that I was talking about what are the different uh aspects of regression this continuity that you might encounter along these different dimensions um and so we'll give you um we'll give you some of them here and then uh we'll talk about you know resources that we'll share with you um we have created a GitHub um site where you can you know download the materials for today and will also share references and resources so one distinction that is very important uh when you encounter regressions continuity design is whether you have perfect compliance with the treatment or imperfect compliance with the treatment so here on the left I have what I call a sharp what we call a sharp ID design you know the literature called sharper the designs this is basically a design where uh there's perfect uh compliance so everybody uh everybody with the scores above the cutoff is is assigned to the treatment that's always the case but also receives the treatment so now we are looking here at the conditional probability of receiving the treatment not being assigned to the treatment but the conditional probability of receiving the treatment as a function of the score and when you have perfect compliance in a sharp art the design the probability of receiving the treatment above the cattle is one and the probability of receiving it below the calorie zero and so there's a full jump from zero to one in the probability of receiving treatment when we have non-compliance that jump still exists so it's still the case with that when you cross the card of the probability of receiving treatment changes discontinuously but that is not necessarily A change between zero and one so in this particular plot here you're seeing a one-sided compliance so that everybody uh who's uh whose core is below the cutoff is in the control condition doesn't receive the treatment but above the cut if there's a fraction of the treated people um the treated units that don't take the treatment and that's why you see the jump in the probability uh the best smaller than than one so it jumps from zero to something that's smaller than one so this is typically what you'll see in a fasi Rd design which is a jump in the disc this continuous term the probability of receiving treatment but it's smaller than one and we can still use the ideas um of the regression is gone you know it typically you know if you are using a continuity based approach you're going to define a you know parameter that's going to you know it's going to be a little different from the sharp parameter and it's going to take this ratio uh um this ratio uh form that is going to be you know and sort of has gonna have an analogy with instrumental but like two-stage lead squares or instrumental variables type of s demands that you might be familiar with where you'll have an intention to treat effect divided by your first stage effect uh and that's going to be a canonical less demand that we can Define in both Frameworks so you're again like um like uh it happens in in you know say the instrument available framework you're going to have to bring some additional assumptions to um say exclusion restrictions or um local uh local Independence assumptions to interpret but this is one distinction that we might encounter uh in practice and fuzzy or the designs are very very common the other kind of regression related design that you might uh that you might encounter and this is common in economics in particular is uh what are called Kink designs so these are situations where you have a treatment or a policy but the policy is not uh binary it's a continuous it's a continuous policy that depends on its core VI formula and the formula introduces Kings so it could be for example a piecewise linear rule that a particular levels of income changes at the rate at which you have to pay taxes or particular levels of uh you know income changes what kind of unemployment insurance uh you can get and so this rule introduces um introduces Kinks and so when you look at outcomes so you might expect that the outcome so the the rate at with the at which um the the policies operating is changing at this Kink point and when you look at outcomes you might not you're not going to expect a jump in the outcome like in the typical Rd but you might expect a kink like a change in the slope right at the cutoff which basically is equivalent to a jump in the first derivatives of the regression functions and so you can Define parameters of interest that are kind of analogous you can Define you can Define them analogously and identify them with similar assumptions to their usual parameters but you're going to have um instead of the you know the the levels of the regression functions you're going to have the first derivatives and and when you think about King designs you can you know you can they can be sharp in with you know perfect compliance or they can be fuzzy with imperfect compliance and you can also think about them in terms of continuity based or local randomization base so a lot of this taxonomy um dimensions are cross-cutting so you can have you know local organization fuzzy or local randomization sharp you can help me and you can have a lot of intersections between these different categories all right all right so another um another design that is you know are you know type of RV design that is very common uh and that you might encounter in practice is an RD design where we have uh you know uh either a score or a Cardiff that is is not one-dimensional but is multi-dimensional so here on the left I have uh well you know we call the multi-cat of our design so we might have a regression that's continuity rule um that uses some cutoff for some population uh and then for some other population uses some other paddles and so really what you have is regression functions for one population and regressions functions for another population you have you know if you go cut off by cattle you have you know two different um two different treatment effects right so imagine that I have two colors here then I have the usual Rd parameter but now I have you know one uh Rd effect for card of one another at the effect for cut of two um so this makes it also opens the door to you know you can first of all very easily look at heterogeneity of treatment effects across cattles but then it opens the door to think about what uh you know oh what would have happened you know what would have been the effect you know let's say this population is exposed to this cutoff and this is the effect what would have been the effect for the same population uh at the at the higher cutoff and so this introduces some ideas about extrapolation because as we'll talk about um later and Matthias will also emphasize when he talks about uh continuity based a lot of these methods will be you know a lot of these regressions Community designs will be focused on very very local effects and so the idea of you know what's going to happen what happens to this effect as I go away from the cutoff is going to be an important in question that you know we're constantly looking for ways to try to answer that question once we have answered what's the effect of the cutoff or what's the effect in this very small window around the cutoff we do want to know you know what's the effect beyond that and for that we're going to need to make you know external other assumptions and multiple kind of this one way in which we can we can get at that another way in which we can have a multi-dimensional Rd is when uh we have a score that is multi-dimensional so for example when I have a language exam and a mathematics exam and then I'm gonna you know Place uh children in a particular program when they score above a cutoff in each um in in image score and so basically what that creates is um it's an entire boundary instead of you know an entire boundary of uh of coins an entire boundary where the where the with the treatment is changing uh from when when the status is changing from control to tweeted and again so basically we're going to have uh an over number of parameters that we could estimate along this boundary but you know in practice was you know typically going to happen is that you're going to choose several points along that boundary and you're going to try to estimate um the effect there um and again you know you can basically Define you know Define and generalize uh the parameters and the approaches uh for these different types of multi-dimensionality um one important uh one important case particular case of a multi-dimensional you know two two score are the design is a geographic one in particular uh that is you know it is commonly used uh in in a lot of areas of you know social science and economics in particular where you can think of the score uh you know units in space and and each unit having a latitude and longitude score so you know two scores that determines the location and then having a boundary uh that separates a 3D area from a control area so a treatment that changes this continuously at the boundary uh and then you can think about estimating different treatment effects along the boundary and so basically you can think that if you stop you know if you stop and you focus on that particular point of the boundary then at this point in the boundary uh you know you can calculate the distance of all of these units to this point say the geographic distance of all of this point uh all of these units to that point and then you can estimate a unidimensional Rd effect at that particular point you can repeat that for as many points as you want and so um basically it comes down to all kinds of you know there are different kinds of Rd designs but they're all share if you know they all share kind of the same basic structure first of all they all share the fact that obviously there's a discontinuous uh change in the probability of receiving treatment at a particular cutoff this kind of might be one might be you know by more than one maybe multi-dimensional the score might be single dimensional multi-dimensional but there's always going to be this formula that creates this jump in the probability uh of receiving treatment or at least you know a kink uh in in a more general rule um the causal effect that we're going to be able to you know identify uh with this design is going to be different in general from an RCT and I'm going uh to discuss that more um all of these are the designs regardless regardless are going to be based on this idea of exploiting this variation this like abrupt change uh near the cutoff and we're going to be introducing some assumptions that are going to basically give us some sense of comparability that if I go to units just above the cutoff and I go to units just below the kind of in some sense those units will be comparable or we're going to be able to use them to approximate and get at something that might be comparable there is no overlap right so meaning that every unit that is treated if you think if you think about a sharp writer think about perfect compliance every unit that is treated uh has a value of the score that is higher by definition that every unit that is controlled and this is a it's a lack of um there's a lack of overlap in this very important query typically are the designs are given based on covariates that are very important and I'll give you an example very soon and so this lack of common support and it's it's going to basically imply that 3D units and control units are going to be different by construction and so we're going to need you know something to uh basically we're going to localize and go close to the cutoffs to try to you know regain comparability of some sort um we're going to talk a little bit more about how you know how to analyze them graphically and and this is going to convey a lot of information about the design and and then another takeaway will be falsification and validation methods that Matthias will discuss later so let me talk a little bit about uh how to visualize the design and for this I'm going to use uh software so here is the information of where to get our um where to access our software packages so we have already packed we'll have packages for Rd estimation uh visualization and inference in Python's data on R and you can have the GitHub page over there you can get um the multiple um you can download them and install them in your preferred platform um and I'm going to use them to I'm going to use now a little bit of those packages to illustrate how to how to use how to use them to analyze a real design so the application that we are going to analyze in materials we can we also analyze later so I'll just introduce it once and then we'll come back to it at various points it's uh an application published in the QJ in 2007 by Ludwig and Miller um the question is uh to study the impact of uh giving municipalities assistance for uh for the rollout of Head Start and to look at that assistance on uh infant mortality municipalities um so the unit of analysis is municipalities in the U.S uh the treatment is whether the municipality received assistance uh to um to roll out Head Start and that the score is um a poverty index so important you know the the property index that they use is um poverty indexed before so this is happening in that in in the 70s so they use the poverty index of 19 and according to the census of 1960 to the to rank uh municipalities according to um most uh most Portuguese poor and so this is important right because it gives you an idea of uh why regression is currently doing this are in a way the opposite you know in one sense the opposite of randomized question trials because in this case we order municipalities from uh most for to lease four and then we and then basically the way they did it is because of budget considerations they could give assistance to 300 municipalities so they counted you know started with the poorest municipality until the 300th poorest municipality and that's and and where the poverty index of that number 300 was the cutoff uh and so that determined the kind of to be you know that particular number which means that basically you're now sorted you sorted all the municipalities by poverty and all the Poor's municipalities are in the treatment group and the control group so basically this lack of common support is a lack of from supporting the property index that it's going to be highly correlated with the outcome of Interest which is child mortality uh so mortality of health of causes that might be affected by Head Start in children five to nine years away page so you can see how this you know immediately creates a problem of comparability between treated and controls so I'm going to talk I'm going to visualize this uh I'm going to talk about how to visualize the design and I'm going to show you an RD plot uh I will just um just show you very quick little bit of code um we have uh shared this uh data set and the code with you uh in our GitHub page you'll find it in the MBR um in the MBR link and you can download it I either run it now with me or you can download it later and and replicate it yourself so I just have the outcome and um I have the score and then we have uh several covariates that for now I'm not going to use and I'm going to create a treatment indicator so the only command that I'm going to focus on here is I'm going to use the command Rd plot which is going to plot uh the outcome as a function of the score uh giving the particular value of the cutoff and with a few options uh that I'll talk about this is actually the default and in the default you can actually remove even that and that's going to be the default and so this is what a regression that's continuity you know either way RP plot looks like um how we have you know the outcome on the the outcome on the y-axis the score on the x-axis we have the cutoff and we can see there's there's a jump at the cutoff so the body plot is built with two ingredients one uh the lines that you see here those are Global polynomials so separately fitted to the right and to the left in particular the default is in this Commander it's a fourth order polynomial it of you know of Y on X and on on each side you can see here that and then you can you can the points here are um so the lines are the fits from Global polynomials fourth order on either side the points are are a local means so basically we split uh we split the support of the running variable into intervals and for every small interval we calculate the mean of the outcome we plot that as a point um the fit is is fitted on the raw data not on the point right and so the idea is that the global fit will give you an idea of the overall shape of the regression functions and the point will give you a better idea about the local variability around the functions and and what we're looking for is well if there was an effect of the treatment the treatment is changing you know there is no Head Start assistance to the left there's a system here to the right um what we should see is you know if there was a treatment effect on the outcome we should see a big jump now what we see here is well it's a huge jump right uh at the cutoff but we need to be a little careful here because we they're you know there seems to be quite a bit of overfitting uh going on and so this is a problem that uh the Matthias will talk about later when we used to remember that we are trying to estimate what happens right at the cutoff and so we're trying to estimate a treatment effect at the cattle point we're trying to approximate that regression function at a boundary point there are no there's no data to the left to estimate the the the this intercept here there's no data to the right here to estimate that intercept boundary points have the feature that make Global polynomials very unstable and lead to uh over feeding and so if I run the same Rd block but uh but I but I use a linear polynomial instead of a global you can see the difference this the the local means are not moving right what your brain is doing is all all that is moving is the global fit so what your brain is doing is in the first plot your brain says the effect is huge in the second plot your your brain says the effect is moderate and nothing has changed all all has all that has changed is how we fitted the um the global the global regressions so um this is why you cannot use really an RD plot to formally estimate and and to formally analyze a regression this continuity design because there's you know because there you know there's many possibilities for how you can present this block nonetheless it's a useful tool to visualize where the data points are how much variability you have uh in the trade in the control group uh and and you know it's it's also kind of I'm set setting up for Matthias to come back and and tell you why we're going to be focusing on polynomials of lower order when we when we use continuity based analysis and why we want to go local to the cutoff and how we're going to have to take care of the problem of you know of boundary bias and the fact that this is a boundary point um all right so for now um let me go back to the slides um we have more um we have more information to give you about how to particularly choose the bins for example in an RD plot and you know um just invite all of you to consult the references in about optimal selection of of bins and the number of bins if you are interested but in their interest of time I'm going to keep moving up apologies keep moving along uh and I want to talk for the last minutes that I have in my presentation I want to talk about um estimation and inference in particular so basically up until now I gave you a big overview of different kinds of issues and then uh and then how to you know how uh how to visualize an RV design and now I want to get a little bit more formal in terms of how to proceed to um you know for how to estimate and how to perform statistical inference under under different assumptions so as I've been hinting from since the beginning we're going to discuss two approaches I'm not going to I will discuss local randomization Matthias will discuss continuity based um there are um there are a lot of things that this that these approaches have in common and so basically um we are going to need localization so we are going to go close to the cutoff before we do anything else and why because you know our assumptions are going to are going to require you know unless we want to make very very strong parametric assumptions we're going to you know need to go close um and so this will mean that we either need to select a window in the case of local randomization or in the case of local continuity we need to select the bandwidth then when we talk about Point estimation we're going to have um a choice between uh you know what kind of um what kind of uh methods we're going to use um for Point estimation uh and and for inference uh and for inference we're going to you know have a focus on non-parametric and non-parametric methods and in local randomization we're going to have a choice between uh you know fine sample uh valid methods and large sample methods um there are um so basically we're going to focus on these two approaches there are many others that exist uh empirical likelihood methods derivation methods Etc um those are um those are not as widely used uh in in in the social sciences and sometimes require um uh you know kind of ad hoc choice of tuning parameters which is something that we will try to avoid and we'll talk about how to do data driven and optimal Choice as much as possible of any tuning parameters that are needed and so we are not going to you know in general methods that require users to choose these tuning parameters by hand are going to lead to some you know problems in terms of replicability and so on so uh but but those exist in Matthias and I are writing a review article for the annual review of economics and there we will uh Pro you know we provide comprehensive sites in case you are interested Okay so I'll talk in the final part about the local randomization approach to Rd design what is the key assumption so as I said the the tree the regressions continuity is defined by this discontinuous treatment assignment rule that is the definition of it but that's still not enough to be able to you know say well there's an average human effect here that's identified I need something more so in the local randomization approach we impose an additional assumption and the assumption is basically informally it says it's really the original assumption that um that um that was kind of the intuition behind the first Rd paper in 1965 which is the assumption that okay if I go to a window around the cutoff you know a little positive number W so I go C plus W C minus W to a small window then in there there exists such a window exists and in that window subjects are as if randomly assigned to uh to either side of the cutoff and therefore to the treatment and so that is the intuitive assumption so the intuitive assumption is there's there exists a window near the cutoff where this behaves as a randomized control trial um how to formalize that is um is actually requires a little bit of work and so we're going to split that assumption into two so the first is going to be that the joint probability distribution of this course for units inside the window is going to be known and that's what would happen in a randomized control trial if you assign uh the value of the score randomly you know exactly the joint probability distribution of that score so we're going to assume that it's known in practice you you know you you you don't need yeah you don't need to know it but you might you know things like assuming that everybody has one half probability or whatever it is Will imply that you know destroying distribution so we're gonna assume that we know it and then we also need to assume that inside that window uh the potential outcomes are not affected by the by the score so basically it's an exclusion restriction where the potential outcomes inside the window don't depend on the value of the of the score and so this is a stronger assumption than you're gonna see later um that the standard assumption for continuity based and um if we make these two assumptions okay so then we can say well then we have a sort of randomized control experiment in this window um why is this uh Stronger Than continuity because uh you know given these assumptions is not only true that the regression functions are continuous at the cutoff which is what you'll need for the you know the standard continuing device approach but they're also inside the window they're completely unaffected by the running variable uh and that's what allow us to treat this as you know an experiment so if you think about the typical Rd plot so you have on you know this is the Rd plot that I introduced before so this is the typical you know regression functions as a function of the score um you know if this is poverty and this is child mortality you know the potential outcomes are going to have you know a relationship that's going to be a slope but if you think about what that similar plot would be in a randomized experiment um if if you take the score to be you know the the random number that you use in your computer to assign people to treat it on control and then you plotted the regression functions so the regression functions the potential outcomes as a function of that you know pseudo uh random number you would see basically no relationship whatsoever between the regression functions and that number because that number is just an arbitrary thing that you it's an arbitrary device that is unrelated to the characteristics of the units so the idea is if we want to use the ideas of the you know of the analysis of experiments to analyze in our new design we need to kind of put put this um you know this feature of a randomized experiment in you know as an assumption into their design and so the way we do it is by basically localizing as I was saying before is of course overall we're not going to be able to assume that there is no relationship between potential outcomes on the score because by definition as we said the Rd score if it's poverty if it's you know and potential outcomes are you know outcome is mortality those two things are going to be very strongly correlated but what we're going to assume is that there is a window it could be very very small right close enough to the cutoff such that if I if I'm able to get close enough the relationship between the potential outcomes and the scores disappears and so these regression functions are flat as they would be in an experiment and in addition you know I know or I can assume possibly what is the distribution of that joint distribution of the score in in that window so um for example if I have you know if I you know if I have um a hundred units in that window and I have you know 60 and 40 interested in control and I assume a fixed margins randomization took place which you know hypothetically then I can then I can use uh that that treatment uh that distribution uh for uh for for thinking about analyzing this as an experiment so basically if you're willing to make those assumptions and we'll see um and we can validate this to some extent then then you can analyze if this assumption holds then basically you can deploy all of the tools of the analysis of experiments that you already know to analyze the regressions continuing design wait so there's two steps that we need for implementation and the first one is we're going to need to to select that window right we we need to select that window we can deploy all the analysis all the tools from the analysis of experiments once we are in that window but we need to know where that window is and so the first step is to select it and the second step is while given that window we can use you know just perform estimation and inference uh so the windows selection step can be a challenge although I will show you a data driven method to do it in a minute um you know but it is a chat you know but in general this you know the the the challenge of of using this this um this approach is that the Assumption of you know acid Randomness those two components is is really it's going to be strong and it's going to be a good approximation only if you really are able to go very near the cutoff uh and so that will that will probably lead you to uh discard a lot of observations and you're going to end up with small sample and so that can introduce some that can introduce some um challenges in some terms of statistical power and so on so how to choose the window well you can do several things you can use pre-freeming covariates which is our preferred method you could you could or you could choose uh you could find a neighborhood where the outcome and the score are independent uh but doing some Independence tests or uh least prefer Choice uh you can just uh do it uh you know just do it based on some substantive knowledge that you have in an ad hoc way the preferred way is to use some sort of data driven method that allows you to validate and defend and defend how you did it um so the idea of the convertiba so the idea of using free treatment array is to choose the window is very simple is that if you have a covariate or more than one that is related so here I'm plotting the regression function of the covariate as a function of the score and this is again the cutoff and and this is the true window where local randomization holds um so imagine that you can find the covariate that is very related to the score overall okay but you but but it's a pre-treatment covariate okay so you know there's no treatment effect because this is a pre-dreaming Corey so you know there's no jump at the cutoff and you also know the career to be very correlated with the score and so what you're going to do is as is then say well if there is a w where the local randomization holds if I start with the largest w with the largest window and I do say a difference in means that so I test the null hypothesis that the that the average of the covariate the mean of the converting the treatment group is different from the mean of the cover in the control group in the largest possible window of the entire support I'm going to reject that I'm going to reject that no and if I keep doing you know if I do it for a slightly smaller window again I'm going to reject the node that the the covariant distribution the covariate means are the same uh but there's going to be a point when I hit the window where local randomization works when I'm gonna stop rejecting that hypothesis and then I'm going to read I'm going to stop rejecting it there and I'm gonna and I'm then I'm gonna filter reject it in all the smaller Windows contained in it right so this gives you a data-driven method of choosing the window uh by basically choosing the largest window where you fail to reject uh the node that the covalent distributions are the same in that window and in all windows that are contented and I will show you in a moment um how to implement that with the data once you've found that window then you can deploy as I said all the tools from the analysis of experiments you have the three canonical choices uh for you know imminents and Ruben uh books so you can do randomization inference featuring methods so these methods are actually related to the methods Alberta was talking about permutation methods to use in the synthetic control design where you will uh for me run you know that in and regenerate different versions of the treatment um and then create uh you know randomization distributions under the null that way so these methods are going to be finite sample exact uh the advantage of this method is that if you have to do it in a very small window they're going to still have um known properties in this very final sample you can also use Neiman methods where the potential outcomes are fixed but you rely on large samples or you can uh use the standard large sample method based on random potential outcomes that are large sample boundaries all of these methods will require a window selection and then a choice of the test statistic if you're if you're if you're using fisherian or Neiman inference you're gonna need to think about um the random uh the particular assignment mechanism as well um let me go for the last bit back to the code and I wanted to uh show you if I can move this away yes um the Rd plot that I just did uh a few minutes ago was part of the Rd robust um library or command um the Rd that the command that I will use now which is the Rd window win select is part of the Rd log run so basically we have separated the Rd software by Arduino Ragnar di robots by continuity based message and local randomization methods are completely separate intentionally kind of to emphasize the fact that they're based on very very different assumptions and very different conceptual understanding of the design in some sense so what I will do here is I have in x uh is the score and I have a number of covariates that I'm going to pass to this window selector I I also have the cutoff and I'm going to have a few um parameters here so I'm going to run it um I'm gonna run it uh and while it's running I will continue talking so basically what this is going to do is going to implement the windows selector it's going to start um is going to start from the smallest um from the smallest possible window and we're going to impose a minimum of 10 observations on either side of course the one of the challenges of the windows selector is that you have to you have to ensure that the last window that you test doesn't have very very few observations because of course with one of the region on either side you're always going to fail to reject any headphones you will have no statistical power so um you know one recommendation based on some power calculations that we've done is to use you know a minimum of two observations on either side so you start with the minimum window in terms of you know 10 observations on either side and then which is this one remember the the cutoff is about 51.1 uh and and then you start increasing the windows symmetric uh symmetrically on either side you can increase it by a step of length or you can increase it by number of observations and you continue to perform balanced tests and then here we are doing balance tests for all the covariates and keeping the minimum p-value and this is the covariate associated with a minimum p-value and then basically what's going to happen is that you know in cases where you can find a window where this works you're going to find um you're going to find that the p-value starts being large and as you make the window larger the p-value is going to start to drop um and it's going to start to drop until it you know it it falls below you know conventional levels of significance so if I show you in this slide I run it for I run it for many windows which takes a little bit of time so I didn't want to do it live and what you see here is that this is a typical plot for the windows selector you have um for small windows and here's the length of the window this is the length of the window that I'm plotting and and and here's the minimum p-value for each window so for windows that are very small around the cutoff so length you know one or two the minimum P values are all very large meaning that basically you cannot reject that the distribution of physical variates is the same and then as you increase the size of the window As you move to the right the length of the window those P values start to decrease they're not not exactly monotonically sometimes they go up and down but you know but there's a trend right that you know that that it as you increase the length of the window Disco great balance gets worse and worse as you would expect because the outcome and the score are very correlated and and Discover it is very collated with the outcomes and so basically what you're going to do is uh in this case this is the last window right this length right here right which is about a little bit under five in terms of the you know the poverty rate the poverty Index this is the last window where the p-value is about 0.15 the recommendation is to use a threshold for the p-value that is much higher than the conventional significance because we are flipping the null here right we are interested in in we're interested in in the lack of effect not in that effect so so that kind of changes the way you think about testing and so you want that pul that threshold to be higher we recommend 0.15 but you know as high as possible so this is the last window where basically in this window and all the windows smaller than this one right or the p-values are all higher uh and so there's no window here or in on any sub window where you reject that null hypothesis so basically that would say that this window is the chosen one and so given that chosen window which ends up being it's around here so you drop see how you drop below um Point 15 in this window so we choose this one as as the window now you can go to that window and use I'm going to use an RP plug to give you an idea of the effect so if I go to that selected window and I plot it uh what I basically what the local randomization approach does is after I've localized I got very very close then I just do a mean I just calculate a constant polynomial I mean on either side and I can look at the effect and I can see an increase in in mortality that you can replicate with the command Rd random which will basically give you the difference in means and give you the p-values from planet sample and large sample um all right 10 54. okay so I have one minute I I'm gonna uh I'm gonna do this and I'm gonna this is where my tears will uh take over when we come back from break um I'm not gonna offer conclusions or big lessons because Matthias will be doing that at the end of uh at the end of his part um so I will kind of leave you hanging we have a 10 minute break so we are going to be back at 1105 Eastern uh in exactly 10 minutes I think yeah because I just saw the minute go to 55. uh thank you and uh we'll see you we'll see you in time 