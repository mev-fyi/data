Lawrence Christiano: What
I have here on the slide is a webpage for at least today. I think he's source
and I are going to coordinate and make it include all of
the material that he's going to be talking about. Just want to draw your
attention to this though, because you can get all of the material for what I'm doing. You can just download
it from here in case you lose the handouts
or something like that. This is on my webpage and
has all this stuff on it. Let me begin with the outline here for
what I'm going to do. What are we doing
here. Oh, here it is. I guess the name that I'm
giving to what I'm going to do today is this solution methods
for DSG models and applications
using linearization. What we're going to be
doing over these two days is Jesus and I together is we're going to talk about
linearization procedures and non-linear procedures
and applications. My emphasis here is going
to be for the most part on linear procedures
and their applications. Let me give the outline
what I want to do. I have way more here than
I could possibly do today. I'm just doing that as insurance to make sure I don't actually, at some point run
out of stuff to say. The last part we may not get to. What I'm going to do at the very beginning
is I'm going to try and put the both days into a broad contexts from a
computational point of view. I'm going to talk
about the two types of solution strategies that we're
going to be focusing on, perturbation methods
and projection methods and I'm going to do an
overview discussion of that. These methods actually go back, well, I guess what seems
like a pretty long way. Computation of DSG
models started in the '70s of
approximations to models. Actually if we go back
and now I'm saying it, I realized that in the '60s already there was
a huge amount of work done, but in the '70s there was a
resurgence of this stuff. Then in the '80s, then a big figure
appeared in the '90s. In particular, Ken Judd, who declared that economists, we're very bad at doing
computational stuff. He proposed these projection and perturbation method
at that time. These things go back to the ;90s and more recently there
has been, once again, a huge increase in interest in these perturbation
and projection methods because there's been an increase in interest in dynamic stochastic general
equilibrium models. I think that with the crisis
that started in 2008, my impression is
that interest in these things is going to
explode now is exploding. That the lesson that a lot of people will learn
from and are learning from 2008 thing is that we have to become
much more sophisticated, not less sophisticated
than we were before. That is in case, from the
point of view of macro, we need to integrate financial
factors into our models and that's going to make
things more complicated. Where you see this
in particular, if you look at central
banks around the world, they're massively
beefed up in terms of PhD economists with
frontier level skills. I'm going to start with just describing as an overview
in a simple way. Although I'm going to
describe the results I'm going to describe
are completely general. I'm going to start
out by talking about the two basic methods that are used and then I'm going
to work on applications. The first step I'm going to
do is I'm going to go to the simple New Keynesian model, and what I want to do is I want to do a couple of
things with that thing. First of all, we're
going to derive the non-linear
equilibrium conditions. We're going to need those non-linear
equilibrium conditions for several reasons. One is, we may
want to use one of the non-linear methods
I've just described above. Another one is that we may
want to do optimal policy. In fact, I don't know if
I'll get to this later, but there's a strong view that the right way to represent policy is as optimal policy. In particularly, if
you're looking at the Riggs Bank for example, you have Lars Jensen who
was a governor there, and they conduct monetary
policy in this way. They compute Ramsey
optimal policy and on the basis of
what they get from their computations that
constructor graphs and forecasts, and make
their decisions, and so on. I'm going to use a simple
New Keynesian model. One of the things
I'm going to do is derive the log-linear, I'm going to derive the non-linear equations and derive this log linear
approximation to that thing. I'm going to talk about
Ramsey optimal policy, which is going to
require that I use the nonlinear equations
we'll see that. Then I'm also going to
use this model to talk about Dynare, which is I guess the computational software for implementing much of the
stuff at the very top. The Dynare
computational software can implement
projection methods. It is not for a
perturbation methods. It is not a position
to implement projection methods for
reasons that you'll see in this discussion. I'm going to use this simple
model to also illustrate how to use Dynare and
then we'll be doing that. Does everyone have a
computer laptop with them? Does anybody? I don't see
any laptops out there. I see two. I guess there are two
people raised their hand, three. Okay, four. We'll have to share amongst
all of us four laptops. The idea is to, I've got a very simple
little dynamic code. I'll explain how to do Dynare. One of the things I hope
that you can get out of this thing is if you don't
know how to use Dynare, you'll learn how to use
Dynare in the process of I don't know, playing around with the code, to learn Dynare I'm going to convey several substantive
principles that are emerged from this modern
macro literature. One of them is the idea
of the Taylor principle. The other idea is the notion of working capital and how that can destroy the
Taylor principle. I'll talk about new shocks
and we'll talk a little bit, possibly about monetary policy using the long
term interest rate as opposed to short-term rates, which very much the
QE2 policy of now. Any event that's that stuff. Then what I'm going to
do is I'm going to get into much more
substantive stuff. Here is I'm going to
discuss the introduction of financial frictions into dynamic stochastic general
equilibrium models and the big lesson
there is going to be a new shock that comes
out of the analysis. I might take this model.
I'm going to take it to the data and then
we're going to come back and we're going to
have in our hands a shock. I want to call it a risk shock, as a major driving
force for the economy. Maybe 50 percent
of fluctuations by one estimate and
we'll discuss that. I had tried to put together a dynamic
exercise for this thing. However, I did not succeed. When you prepare exercises, they have to totally work. They have to be
totally bulletproof and if somebody drops
it on the floor, it has to still
work on the floor. This is a rather large
modeling exercise. It did not succeed in
getting into the air, and I also failed
to take it out of my outline here when I realized I wasn't going
to be able to do it. Then the final thing here would be to talk about Ramsey policy, which is to take that discussion and then really put it under a microscope and discuss
the various issues, time consistency, something called the
timeless perspective, and so on, but I doubt
very much that I'll actually get that
far, but we'll see. I could've depending
on how things go here. Let's begin with the first part. The discussion of
Perturbation and Projection Methods for
Solving DSGE Models. These methods do not originate
from economists at all. This was actually one of
the big messages that Ken Judd made in the '90s
when he came along and said, "You guys are
totally uneducated. You don't realize that
people are solving dynamic models out
in other areas." He told us about projection and perturbation methods
at that time. He was not saying
that this was stuff he was bringing to us, but rather stuff that
other people found very commonplace so that's
what I'm going to do. Let's talk about the outline of how we're going to do that. The perturbation and
projection methods, I'm going to start out by
explaining it in a toy model. Totally, it has the basic
essence of an economic model, but it's a totally
simple to allow me in the easiest way to
discuss these things. Then what we'll
do is we'll go to the neoclassical
growth model and talk about projection and
perturbation methods in the context of the
neoclassical model. That's going to be the
simplest economic model I can think of and
we're going to derive some basic properties
of what you get from perturbation methods
and we'll discuss some basic ideas with
the projection methods. Let's get started with
the simple example. Basically, the simple example
is the example that you would find if you looked up a book on the Implicit
Function Theorem. That's what the whole
damn thing is all about. Let's imagine that we have
some function there, age, which is a function
of two variables, x and y and we imagine that, that equation there implicitly defines y as a function
of x. X is some kind of an exogenous variable
that's given by some outside
mechanism and then the economic environment has that equation that
determines the value for y. What I'm going to do is
I'm going to think about the solution as
being a function, this function g here. I'm going to be looking for
this function g that solves this h of x of y is equal to 0. We can think of that problem is a problem of solving
a functional equation. The thing there at
the bottom there, r of x:g is a function of
a function and I want to find that function that solves
the functional equation. We'll call that r there
an error function. That error function is
going to be floating around throughout
this discussion in this first section here. First of all, to find
such a function, you have to do an approximation. You might say, "Why do
you have to approximate? Why don't you just go get it?" The answer is that finding that function is equivalent to solving an infinite number of equations in an infinite
number of unknowns. The unknowns are for
every x I want to have a y, so the unknown
object there, g is an infinite number of
unknowns and the number of equations is for one
equation for each x. There's a continuum of equations
and I'm trying to solve and with respect to a
continuum of unknowns, that's real hard to do. If you have three equations
in three unknowns, I find very difficult. Four equations and
four unknowns is really, really hard and five equations and five unknowns, you want to kill yourself, that's so many things. This is a big deal. The first thing you
have to give up on is the possibility of getting
an exact solution. That is to say finding
a g function which sets this functional equation to
zero for all possible x. You can write down special
cases where that's easy to do but in general and economics, those are very isolated
and special things. The projection and
perturbation methods are two alternative strategies for approximating that function g. Let me first start talking
about the projection methods. Another word that we could
use for projection methods, that you can call
them global methods as opposed to
perturbation methods, which we'll call local methods. Let's talk about
the global method, that projection method. What the projected
methods says is, let's give up on finding
the actual g function. Let's look in a space of
parametric functions. I'll call it g hat and
then the parameters of g hat are these vector Gamma, it's a finite vector
and what I want to do is I want to
choose the Gamma so that the behavior of g hat mimics the behavior of
the actual solution. What's the behavior of
the actual solution? Well, the actual solution g sets this r of x,g to
zero everywhere. Why don't we just imitate that? Choose the Gamma so as
to set our hat of x: Gamma and make that
somehow small a lot. We're not going to be
able to make a zero everywhere because for
that we would need an infinite dimensional
Gamma because there's an infinite number of x's. We're not going to make it zero everywhere so we're going
to have to somehow make it close to zero in some thing. You can see here by the way, the two major issues that come up with a
projection method. Number 1, what function
are you going to use? Number 2, how are you going
to define close to zero? In fact, this whole topic of projection methods is actually an enormous one
that we will only touch on very, very lightly and
you could write a book about the phrase close to zero and you could
have read a book about all the possible
parametric functions. I'm going to now
discuss separately the functions and the
cost to zero definition. Well, inside the
set of functions, there are two types
of functions. One of them is called
Spectral functions, and the other one is called Finite element functions
or approximations. By the way, the
language that I'm using here is taken from, I have a paper with
Jonas Fisher in 2000 which they're documents where you can find rationalization for the type of language
that I'm using it. There's always, as in any field, there's a little
bit of differences among different people for what the language is exactly and there's
no exception here. There's a reference to paper
of mine with Jonas Fisher, which summarizes surveys,
all of this stuff. You can find the
language there is, we cite various textbooks to
talk about this language. Because you know in
time series analysis, obviously, spectral things have a particular meaning and
this is not related to that. Spectral functions, that's one type of function
as opposed to finite element functions
or approximations. Spectral function is
a function that has the property that as you move the little
gamma guys around, you change the function
globally everywhere. An obvious example of
that is a polynomial. If you take a polynomial, a plus b times x plus
c times x squared, you change c you
change the behavior of the polynomial all
over the place. The way you represent
these spectral functions, rewrite it in this form here
as a linear combination of a bunch of these
things here are called basis functions and we'll do, I get a little bit confused here whether I wanted to
zero or I wanted a one, so I put in both a
zero and a one here. What I was thinking about with a polynomial you like to have a zero intercept term and that's notations
screwed up here. This should be a one
here to be consistent. It doesn't matter,
solves notation, but in any event, the spectral
representation functions can be written in this form
where this is something called a basis function
and one example of a basis function is where the x's are ordinary polynomials, where this h i of x is
actually x to the power i. Now this is a disaster. You never ever want to use
a basis function like this because the reason
is that if you use a polynomial that's of
order three or four or five, then you've got things
in this expression here that are
something to the power five and you have something
to the power one, you get huge
imbalances in numbers. It totally screws everything up. This was only here for the
purpose of illustration is the obvious thing but it's
certainly an example. For example, actually,
the first example that I know of where
this stuff was applied is in a paper
by me and Terry, and Keyhole in 1992 in the JP. Anyway, there we use
Chebyshev polynomials which is very, very
carefully crafted. What you do is the
basis function now is t sub i and t sub i is a so-called
Chebyshev polynomial. It's just a polynomial, but the i'th guy has power i in there and power
i minus 1, and so on, in very particular pattern
that's of great interest. It's rigged up also
to be always inside the unit box so
you never get into weird imbalances in magnitude. You express things as a Chebyshev polynomial
by first of all, mapping your little guy x
into the unit box so that you don't do crazy stuff with
them, and then that's how you construct this thing. I don't want to get into
the detail of this thing. I just want to show you
just by two examples here of I have something called
a spectral function. Where the parameters
are, it's these weights. The weights here are the
gammas and so what you do is you choose those
gammas so as to make the functional equation as close to zero as possible. What I mean by close to zero, we'll get to that in a moment. Right now, I'm talking
about the functions. One class of functions
is a spectral functions. Then you also have
another function is this finite element function. The finite element function in some ways is
very different from the spectral because the
finite element thing, all the parameters, they only influence things in
a region locally. They work hard in this neighborhood and they leave that neighborhood
unaffected. With polynomials, you look at the whole polynomial and you
jigger around parameters. You change the polynomial everywhere but with a
finite element function, you change some
parameters and you're only working on it
in one neighbor. Here's an example of a finite element function
is piecewise linear, and what the parameters are, if the following the
way you construct the parameters is that you create a grid
of little exercise, here's the domain here at big X. These are all the
possible values of x and then you create a grid X1, X2, X3, and so on, all
the way up to this point. Notice this is not an equally spaced grid that I've got here. You might want to do and equally spaced grid, you might not. Here's a non equally
spaced grid, but part of the definition here of this finite
element function, of this piecewise function
is these node points, what are these points here? Then the parameter gamma, say the fourth
parameter gamma four is the height of the
function at that node point, and then the function itself is just straight
lines in-between. Of course, once you see that, you realize you can do all
kinds of stuff because you can make polynomials between the two places and
the polynomials they could smoothly merge with a neighborhood polynomial
and so on and so forth. But the point is that about the finite element function
is that you can work hard in one region rather than another region and
as you can imagine, that might be helpful in
certain circumstances. For example, if you're solving a model with inequality
constraints, for example, a zero bound
on the interest rate. Now, what happens is in the neighborhood of where the zero balance starts to bind, the behavior is different. This offers you a
certain flexibility to work hard in this neighborhood
and do this stuff in this neighborhood, and do other stuff in another
neighborhood, and so on. Whereas the spectral methods is very clumsy in that respect, the spectral method it's hard to work hard in one
neighborhood with your parameters because the parameters
influence everything. Of course, you can combine
the damn things you could say it's a spectral
thing over here, and it's a finite
element over here. You'll get creative
with this anything. There are no, by the
way, good theorems here which is interesting. Now fortunately, there is a test for whether
you're doing a good job. There is a test for whether
you're doing a good job. The test is the
head damn function, get close to zero everywhere. Of course, you still
have a problem like what do you mean by zeros? Ten to the minus 40 is
10 minus 80 and so on. But economists know
how to deal with that. I'll tell you what zero is. Take the tables in your paper and produce the tables in your paper for some
degree of accuracy. Now, we increase the accuracy
and redo the tables. Of course, they're all going
to be different, but if the language you use to describe the tables
hasn't changed, I say the tables have converged. Now you have to be
very careful because I'm assuming monotone
convergence. They may not change very much
and then all sudden boom, they turn upside down. You do have to be
careful with that but that's a general principle. There is a fundamental check on whether what you're
doing is right because there's always this
error function and it doesn't
matter what you do if it kills some old person on the street to get that error
function equal to zero, you've succeeded because that's
all you're trying to do. If you're doing some goofy crazy thing in terms of space or function doesn't
matter if you get the error function
equal to zero. Now, there's a uniqueness
issue, of course, floating around in the
background and general, they won't be just
one g function here. You do have to worry
about that a little bit. This is the finite
element thing. This is the two
general classes of functions that people work with and now let me talk about the two general approaches
to defining close to zero. Probably, the most
straightforward thing is called co-location. With co-location, what you do is you are with co-location, you focus in on, I'm using little n to be the letter denotes how many
parameters are in gamma, how many parameters you put in your function, whatever it is, finite element
spectral, whatever, that's n. With co-location, what you do is you
pick n, these n, and you go and find
endpoints and then you choose the n parameters and set this thing to
zero everywhere. Of course, in general, you
can do that because you got n equations in n unknowns. It's no big deal and then you also have
a nice check available, which is that you
can look in-between. I thought, I would have something to write
on a little bit. I want to show you
how the picking of these points could be actually
quite a tricky business. I have a hidden file here. It's so hidden, actually,
you don't have it in your handout but I'm going
to show it to you anyway. This is one of the
most fun things I ever saw in numerical analysis. I want to show that to you.
I need to unhide them. No, wait, I'm hiding it now. I don't get it. It
was already hidden. This is a problem called,
I'm being retarded. Wait, don't record
that last word. Well, why isn't it doing what it's supposed to do? Wait. I like that. Oh, I know. Maybe I know. Catastrophe and I'm about
to lose my confidence and then I won't be
able to minimize. Here's the example. Now this is one of the most
shocking examples. I took this graph from this
thing with Jonas Fisher. This is a problem
called interpolation. It's not the same as the problem of solving
this functional equation, but it's so similar. The problem is the following; interpolation is that
you have a function, you don't know what
it is, but you know its value at a bunch of points and you're
trying to figure out what's going on in between. This is a famous example here. It's not due to us, this is a famous,
famous example. What I've graphed
here is this thing, the dashed line is
this function here. It looks like the nicest
function in the world. Who could ever think that any harm could come from
this little function. It's 1/1 plus k squared going
from minus five to five. The interpolation problem is to take the value of the function
at a bunch of points. Let's suppose you're
capable of doing that. You can't draw the function. Well, here of course we see it. But it's a function
that you can't draw because you can't evaluate
it too many times. You can evaluate it sometimes, and then you want to
interpolate in-between it. The obvious natural
thing to do is to select a bunch of grid points
that are equally spaced. Equally space is the
natural thing you do. Always go equal space. That's got to be the
first thing you would do. Then evaluate the
function at those points, so take this point there, and so on, and then draw a polynomial through
those points. We even think we have theorems that say, "That's the
right way to go." We don't actually. In fact, that's the wrong
way to go in fact. But you might think
of there's theorems, Bernstein convergence, no. This is actually
a stupid thing to do and in this
particular example, and you should go
home and try this. Do it at a party with friends
because they'll be amazed, they'll find you much
more interesting. If you fit the polynomial
to this thing, what happens is as you make these grid points
more and more narrow, you would think that the
approximating function, which is this one here, you would think
that those errors would squish down as
you made the grip. But no, the errors
become infinitely large. You can actually prove that the errors become
infinitely large. As you make it finer and finer, you get further and
further away on the tails of this function, so equals space stuff
ain't the right way to go. It's a terrible way to go. That sequence of approximating
polynomials does not converge to this nice little
dashed line over here. There are theorems about
convergence of polynomials, go and check those theorems. Those theorems say there exists a sequence of polynomials
that converge. This is not the sequence
that converges. But if you instead just choose the grid points in
a slightly different way, where they are slightly more
populated in the tails, there's something
called the zeros of the Chebyshev polynomials. Don't worry about what
that is right now, but it's a way of getting
more populated in the tails, and if you populate them more
in the tails at this rate, what happens is you can prove
theoretically that you get uniform convergence of the
approximating polynomial. How you choose those grid
points, it's a big deal. Now, of course, if you follow the checking strategy
that I am advocating, which is always graph
your error function, you would notice
this immediately. You don't need to know
any theorems or anything, you would notice
right away that, hey, the error functions doesn't seem to be getting
squished down. You are forcing it to be zero on the grid
points all the time. But the more grid
points you get here, the worse this error becomes. The point here, I love this
example because I mean, numerics matter how you do your calculations,
makes a difference. At some level, I guess,
that must be obvious. If you want a square
root, you want to take the square root and not the
cube root or something. One definition of close to zero is delivered by this
idea of co-location. The idea of co-location is you pick a bunch of grid points, a quantity of grid
points it's equal to the quantity of parameters in your approximating function, and then you set the
functional equation to zero at all of those points. Now, of course, there is a long
discussion about this. In fact, if you go into my
paper with Jonas Fisher, lots of discussion about exactly how to
pick those points, lots of discussion about how it's useful to have points out in the tails relatively more populated than the
tails and so on. By the way, it's all
very counter-intuitive because in economic
applications, having points in
the tails tends to mean the equivalent to having points that are
relatively low probability. Your intuition says, "No, I want to get this
function to work nicely in the region of high probability." But if you do that,
you will in the tail, you could in the tails if you don't choose those
grid points right, you could make things deteriorate out
there in the tails. Of course, the beauty of
all this thing is you don't really have to know all that much because
all you have to do, you can always graph
the error function. You always graph
the error function, you always have a way of telling whether you're on
the right track or not. Anyway, that's co-location. Now, an alternative method
is called Galerkin. Galerkin takes a very, very large number of points, much bigger than n.
Thousands of points maybe. But of course, you
can't set it to zero at thousands of points if you
only have 10 parameters. What Galerkin does is it takes 10 linear combinations of those points and tries
to set them to zero. Maybe the average, maybe the average
is in the tails, maybe the average
is in the middle, various types of
linear combinations to set those things
equal to zero. The weights in that averaging is given by this thing
here, for each i, you would have a
different weight and then there's i equals 1 through
n. This here is n equations. You have n equations
and n unknowns, and you choose the
Gamma i's to set those n equations to zero. Of course, how you choose those weights is a huge science. Lots of discussion with my
paper with Jonas Fisher, that's also a huge science. My own guess is,
you're fine with the damn co-location thing and not getting all whacked
out about this thing here. Although there are
some simple strategies for choosing those
weights in fact. That's the projection stuff. Let's go onto the perturbation. For some reason before
going on to perturbation, I had to say a couple more
things about projection. What the projection
does is it works on the global behavior
of the function. It's looking at the
functional equation over a large range, it's trying to get the
error function zero over a large range when it's looking for its
approximate solution. The problem with this procedure is the non-linear
calculations you have to do. You have to solve n equations
and n unknowns to do this and that is one heck
of a huge pain, as many of you probably know. It requires tons of
what babysitting time. You have to sit next
to your program. You have to say
special magic words so that it doesn't
do this or do that. It's a very, very difficult
and unpleasant thing to do. As a consequence, often
it is not used today. One reason for that is that
in current applications, people are very keen on doing serious econometrics and
the problem is, of course, when you do econometrics, what you're doing
is you're changing the model parameters and each time you change
the model parameter, you have to solve the model
and compare it to the data. Well, that means
solving the model many, many times and if each
time you have to solve model you have to
solve the non-linear equations and do babysitting, stay up late at night, sing to your computer program and
all this other stuff, you're never going to
get anywhere with that. This projection methods, while at some level my own intuition would
be that those have to be the best method at the same
time they're often not used. Now, on the other hand, I guess economics
is on the one hand, this and the other hand that. You might see a resurgence now on our projection methods. The reason is those are uniquely well-suited for dealing with zero bound and the
interest rate. The projection methods
are well-suited for that. The perturbation methods are silent when it comes to that. Projection methods
are obvious ways of dealing with zero bound
on the interest rate. In fact, my paper with
Jonas Fisher was all about using these methods to solve problems with occasionally
binding constraints, which is what a zero bond
in the interest rate is. It binds sometimes, it
doesn't bind at other times. My guess is we're going to see a resurgence of
projection methods, but nevertheless they
are a huge pain. It requires a capacity for suffering that is much greater than what
most people have. I don't know if anybody
here, for example, did a dissertation that
does computational stuff. That is suffering because
programs they never work. Anyway, the advantage, as I already just said, is that you can adapt
them to any kind of weird way, again, because there's so
much flexibility when you combine the spectral, especially with the finite
element method things. Now, perturbation methods
are totally local. It's totally local. They're basically operating
off Taylor's theorem, which was one of the most marvelous results in the world. What they tell me
is that apparently if they take a little piece of skin off of you and
they study the DNA, they can find out what
kind of movies you like, what are your favorite
hors d'oeuvres? What kind of people
you're interested in. You can tell all that stuff. Taylor's theorem is the
analog for a function. See if you take a function
that looks like that then you just study one place
in that function, and you can actually
learn about what the damn thing is doing
all over the place, under some assumptions
of course. Those are called
regularity conditions. As long as the function is smooth enough, you can do that, but the set of smooth functions, I don't know, I'm not going
to state this is correctly, but it fits into anything. Well, not discontinuous
functions, but certainly non-differentiable
type function. With a Taylor's theorem and the implicit
function theorem, the perturbation method does, is the basic insight that it's using is the fact
that functions that are long and they're
smooth enough can be arbitrarily well approximated by just studying one point. Now there is a problem however, as anyone who sits
down and it gets all excited about this
and then starts doing the actual calculations. It could be that if the
function is a weird function, that yes, indeed the
Taylor's theorem is true, but you might have to take 149 derivatives at that point, at which point the numerics become unstable and it's
really not very interesting. As a practical matter, when you're doing
perturbation methods, you have to hope that
one or two, or three, or four derivatives
are enough to get you interesting behavior globally, and there in, you
might be concerned. On the other hand, your
only alternative is the projection methods and those require intense suffering. Some people would prefer to drive a cab for the rest
of their lives than ever even touch
projection type methods. The perturbation methods
work on the local stuff. The extreme beauty of the
perturbation methods compared to the projection methods
is nothing iterative. It's all based on
computing eigenvalues and various things, and you go, boom, you
get the solution. If you want to do estimation, it's very straightforward to do estimation using
perturbation methods. Of course, the disadvantages
are that you may need enormously high
derivatives to get a decent global approximation. Then of course, if you have important
non-differentiabilities, then they're silent because you really have to have the
thing being differentiable. At the moment, there is an extremely important
non-differentiability that our people are
very interested in, which is the binding zero lower bound on
the interest rate. Both of these methods
have pluses and minuses, but let's get into the
perturbation thing. All right. For the most part, to do the perturbation method, and this is not actually true, but the simplest way of
thinking about it is that you need to know the
function at one point. This function g, I don't
know the function. If I had a method that required that I know
what the function was, of course, it wouldn't be
a very interesting method. Well, this method doesn't require that you
know the function, but it does require
that you know the function at a point. For some economic
examples, that kills you. But it turns out for the mass of macroeconomic examples that
are of interest today, that is not a problem. Then what you do is you just use the implicit function theorem to approximate this function g in the neighborhood
of that point. Then you use all the
regularity conditions that are in the implicit
function theorem. Well, things have to be
differentiable, Number 1, and Number 2 there are there certain
invertibility conditions that we'll see, I think, in a second in these notes
that have to be satisfied. Let's just talk about this
just for a minute here. I'm doing this in the
toy example because it's the easiest
place to do it in. Again, we have this function. When you plug in the the correct policy rule g
into this function, you get this r of x thing. With the true function
that's equal to zero for all points in the domain. Of course, if our function
is equal to zero, well, then its
derivative is zero too. The number seven, you
differentiate that, it's seven. It doesn't change.
It's always seven. If the r function is
always seven or zero, from that we can figure out that the derivative of r
with respect to x is zero, the second derivative is zero, the third, the infinite, whatever derivatives, all the derivatives
are equal to zero. It's on that basis
that we're going to get this whole damn
thing working. The way we do this is
just differentiate that. Well, let's go step by step. Let's take the first derivative
of r with respect to x, but I will need to know the value of the
function at this one point. Let's suppose we know the value of the function at
that one point, and so we're going to
evaluate this derivative of x star and I know
what g star is. I differentiate r
with respect to x and evaluate it at x star. What is that r function? It's this thing here. The x appears in two places. Here's the first place and
there's the second place, and boom, there's only one
unknown here. You see? Because I presume to know g star and so g prime is equal to the
negative of this thing. Of course, here you can
see that assumption that's required implicit
function theorem is an invertibility assumption, which is that the derivative
of h with respect to its second argument has to be invertible
or in scalar case, it has to be non-zero
or something. That's how we get the
derivative of this thing. Then we get the second
derivative by just doing the same thing
all over again. Now, take the second derivative. By the way, if anybody
here has a tendency towards carpal tunnel syndrome, this thing will give it
to you because while it's conceptionally very
easy to do perturbations, it is a nightmare of notation. You can see that
a little bit now already with the
second derivative. If you go to the
third derivative, the fourth derivative, you
just want to kill yourself. It takes a very special
accounting type of mind to put this
stuff together. Fortunately, all this
software now has been put together so you don't have to get into these kinds of details, but here you see here you
get the second derivative, and there's a very
important thing that's appearing in here. If I differentiate
this thing twice, the key thing to notice if
you look in this expression is that it only has
one unknown in it. You know what g prime is from the first calculation you
did, you know what g star is. That's what we're presuming. There was only this one
g double-prime thing, and it solves a linear equation. This is a recursiveness
property which is, you don't have to calculate all the derivatives
at the same time. You calculate the first one by doing the first derivative, take the second derivative, and you can build up your
derivatives all in that way. Of course, you better
hope that things are differentiable and so
on and invertible. For example, if this
guy here is zero, then you got a problem. I blot it out. I think you have on your slides a big circle slide which gives you an illustration of the implicit
function theorem, which I'm not going
to talk about. You can look at it. But you can get a feel
for what it means, what these invertibility
conditions involved and so on. By applying this
thing sequentially, you can get first g prime, then g double prime, and so on. Now Mr. Taylor steps
into the picture with the observation
that this g prime of x, under the right conditions is a pretty darn good approximation of whatever function you're interested in as long as it's over a reasonable interval. As long as you make n large
enough and the thing it has all the differentiability
properties and so on. That's the perturbation method. By the way, in the case of
the perturbation methods, you've got the same check. I've never seen this check
implemented, I don't think. I don't know. I'm not sure why. Because you also have this check because what this is
giving you in the end, you're getting a
parametric function. The perturbation method
applied 2, 3, 4, whatever, number of time or just one time, the linear perturbation gives
you a parametric function, g hat of x. Well, it has a nice feature that the parameters are
incredibly easy to compute. They just all pop out
of this function, which I know what
the function is. I do have to know
what g star is, but there's no decisions like, "Oh, what if my
grid going to be? What family of approximating functions I'm going to look
at?" And stuff like this. There's no real decisions here. Everything goes,
boom, boom, boom. It's just straightforward
calculations. But in the end, what you do
have here is a function, and so that function really ought to be plugged back into the original error
function to see whether in fact that thing is zero
over a reasonable range. That's the perturbation and the projection stuff explained
in the simplest way. Now, I don't have to explain the nuts and bolts
of them that I did with my toy example. I can talk about a couple of substantive results we get from the point of view while
especially perturbation method when we're talking
about simple models. Here we have this simple
neoclassical growth model. There's no hours worked in here. There is a shock in here. I want a shock because that makes this a
little bit interesting. I mean, what I've
said up until now, it's not totally obvious what you do when there's uncertainty, so we will extend this
in a very simple way. What I've just said to
the case of uncertainty. Here we have our little example. I'm also writing everything. Little k is the log
of the capital stock. This is the production
technology here. I'm expressing it in terms of the log of the capital stock, little k. Big K is the
actual capital stock. Here you can see big KT. You have to exponentiate
my little guy in order to do that. The reason for doing that is, later on we're going
to get from these methods, in particular
perturbation method, we're going to get parameters, coefficients and they're
going to be on things called k. If k is the log of
the capital stock, then it's easier to
interpret than if k is the actual
capital stock because the units are not well pin down with actual
quantities like that. Let's look at the analog of that little h function that we're trying
to set to zero. That analog is this
efficiency condition, the first order
condition associated with the investment decision. Here you see consumption today. Notice, I have to exponentiate this little k because it's just supposed to be the actual
capital stock here. I have to exponentiate that. This is going be the analog
of our error function. I'm imagining that at time t, these were sitting
at some time t. Kt and at are just given
numbers implicitly. Therefore, epsilon t is
just a given number, it's hiding inside a, of course. But there is one source
of uncertainty here, which is the epsilon t plus 1. At any point in time,
that's the situation. For purpose of the discussion, it's convenient to think of that neoclassical model as
really a sequence of models, indexed by this parameter Sigma. The variance of epsilon t
plus 1 is actually v_Epsilon. What I'm going to
do is I'm going to put in Sigmas in front of the Epsilons and drive
them potentially to zero. Sigma equal to 1 is
the actual model. Then I have other sequence of models as Sigma goes to zero. I screwed up on the notation. I just noticed that
now a little bit. But the thing is that
this Sigma here, when it's equal to 1, it's equal to the model
that I'm interested in. When Sigma goes
down towards zero, it goes to a version
of the model where there's no variance. In the subsequent slides, the notation will
be more sensible. Here, I put the Sigma
in front of here. What I'm going to look for,
is I'm going to look for a solution to the model. Now, as you all know, there's two ways
of thinking about solutions to dynamic
models in economics. One is the sequence perspective. You think of a solution
as a sequence of numbers. When there's uncertainty,
it's a sequence. If you think about
any event tree, the sequence of numbers is really the numbers that fill out a tree with lots of branches as opposed to
when it's deterministic, a sequence of numbers
is just a sequence of values of the capital stock. One way of thinking
about the solution to a dynamic model is from the point of view of
sequence space or something. We do not think of it that
way when we solve it. Sorry. In this, course, we're not going to
think about solutions in that way as sequences. We're going to think about
solutions as functions. This is this functional
analysis approach to dynamic models. There are actually weird cases where you have to think
in terms of sequences. We will not touch any of
those sequence cases. I don't believe Jesus
will touch any of those cases in this course. We will think about
a solution as a time invariant function
g as a function of kt, as a function of the variables, so to speak, kt, and I'm going to think
of Sigma as a variable, but it's not a time variable, it's just a constant variable. It indexes the problem, but I'm going to
think of g as being a function of that variable. When I have Sigma equal to 1, I'm dealing with the policy rule associated with the problem
actually interested in. When Sigma is equal to 0, I'm dealing with the policy
rule in a different problem. The one in which the Epsilons have no uncertainty on them. This is the function, I'm going to look
for a solution. By the way, notice that this function
delivers sequences. By just iterating
on this function, you can fill out an event
tree and put in the capital associated with every
possible sequence of shocks into the future. This certainly
implies a sequence. But not all sequences
can be represented as the solution to a time
invariant function like this. If you write down an
economic model where the solution is not a
time invariant function, sometimes you can't
use these methods. I don't know if
I'll get to this, but one classic example
where you seem to get time-varying functions is with a ramsey problem and even those we can trick into this anyway. We're going to
think of a solution as a time invariant function g, which sets this
guy equal to 0 for all possible values
of the state. The state is the things
that are known at the time that this decision
here is getting taken. Actually, there's
only one random variable in the
whole picture here. From the point of view of
this functional equation, there's only one
random variable. This a here ain't no random variable
because it's observed. The only randomness is in here. What I'm going to
try and do is find a function g that satisfy this. We all know there's also a transversality condition here. I'm going to look for a
function g without even thinking about the
transversality  condition. While we will think about it
a little bit, occasionally, we'll see that in a minute, we'll think about
it a little bit. In practice, the way
you check to see if the transversality condition
is satisfied as you check to see if you
get an ergodic set for the case as you know then that the
transversality condition has to be satisfied. Generally speaking,
however, the word transversality never is
used in these discussions. But I'm sure some of
you are thinking, this is not a sufficient
condition for an optimum. We also need a
transversality condition, where the heck is that? Well, it's floating
somewhere in outer space. Let's first talk about
the projection methods. For the projection methods, I'm going to look into space parametric function for
g. In that's space, I'm going to look in either
the finite element space or in the spectral space. Then I also have something
to set to zero here. Which is, I would like to get this function here to be 0
for all possible k and a. But I can't do it for
every conceivable k and a because only
the true function will do that and if
the true function is infinite dimensional. I'm only going to
attack this thing with finite
dimensional functions. In general, I'm not going to set this to zero everywhere. There are, of course,
special cases where you can, if utility is log and if
depreciation is 100 percent, then of course, you can
find that function, but that's just
an isolated case. It's actually of
zero interest in the dynamic stochastic general
equilibrium literature, which tends to be focused on monetary economics
and stuff like that, so that the 100 percent
depreciation assumption, in the special
example where you can calculate that thing exactly. The 100 percent depreciation assumption you have to makes that model totally
uninteresting from a short run analysis
point of view. Maybe 100 percent
depreciation must mean the model is like
a 30 year model or a 40 year model or
something like that. I'm not sure what.
That's my policy rule. Projection methods, there's my
approximating function, it's living, and it's a
finite dimensional function. Then what I have to do is I want to choose the parameters Gamma to make this thing as
close to zero as possible, and then I can use
Galerkin or Colocation. Now, maybe by just looking
at that and pondering it, you can realize what this is actually a humongous problem. Because let's take
the extreme case where the a is a
continuous variable. Let's start with
the easy case where a can only take n five values. One way you could think about, if a takes on five values, you can imagine looking for a different policy function associated with each
possible realization of a. If a can take on five values, then your Gamma of a would be five different sets
of parameters, one for each value of a. The k however, that's always going to be
a continuous variable. There's no way to make that
entity non-continuous. Now, with this endogenous thing, you're having to solve for
five policy functions. If you were, suppose that a has only five possible realizations, well, that's a big job. One thing I should add, this is a one capital model. Suppose we have some other
things in the model. For example, suppose
that we have entrepreneurs who
have net worth, now you have a second state variable and that
has to go in there. Now, I've doubled the size of the function that I
have to go and find. What if it's a
two-country model? Well, then I only
need at least two capital stuff, and pretty soon, it gets to be a nightmare
to the power of four to try to imagine
using projection method. But projection measures
are quite tough. It's not a conceptual problem, interestingly, in this
age of fast computers, it's a huge
computational problem even with fast computers did not fast enough to do this in any normal person's lifetime
in a serious size model. By the way, we could mention,
right off the bat here, that the projection
methods will work in this case when you have what's called occasionally
binding constraints. The obvious occasionally
binding constraint in this type of model is non-negativity constraint
on investment, that's investment over there. You can have that
constraint there, and that constraint is going
to be binding for some k, a and non-binding for others, and that's why you could call it occasionally binding
constraints. Maybe you can see
right off the bat, I haven't written this
out more carefully, but maybe you can see
right off the bat that this fits right into
the framework that we've been talking about because what you do is you
imagine writing up the RBC problem in
Lagrangian form. Add this constraint as a Lagrangian constraint to
attach a multiplier to it, and then include
that in the problem. Then you get one of the conditions are
complimentary slackness condition. You get a condition that
the multiplier times that thing has to
always equal to zero, and the multiplier and that thing has to be greater
than or equal to zero. Well, that thing equal to zero becomes part
of my error function. The multiplier is
another policy rule that I have to go and find. It fits in quite nicely into this error function perspective, even if you have occasionally
binding constraints. This paper with Fisher was completely devoted
to occasionally defining constraints and goes into many different issues
and stuff about that. But it should be maybe
obvious that even if you have inequality
constraints that bind sometimes
and not other times, you're still basically
in the environment that I laid out even with
my toy example. But let's talk about the
Perturbation approach, which I say is not feasible in the occasionally binding
constraint case. Well, first of all,
to apply this thing, at least, I apply it
naturally or normally, you need to know the value
of the function at a point. Well, that's no problem in the case of the new
classical model, and this is the
miracle of that model, you can compute the
steady-state of that model without knowing
what the policy rule is. Now, always used to
take that for granted. I never thought of
that as a miracle. But actually there
are models out there, models of strategic
interaction between agents where this feature
it's not a feature. If you have the
government, for example, is choosing monetary policy, and it's thinking
about the reaction of the private economy, well, the reaction of the
private economy is itself an equilibrium function object
that has to be computed, and the government's behavior
is going to involve, to some extent differentiating
that equilibrium object. Any concept of steady-state in such a model of
strategic interaction involves an Euler
equation that has the derivative of the damned unknown function that you want. Even in the steady-state, the function is there and
you can't get rid of it. But in the Neoclassical model, you can calculate
the steady-state without knowing what
the policy function is. In fact, all the models that I've seen being analyzed
today have that property. The models in macro where there was a strategic interaction
between the government and the private guys and
the result first comes out in NIO models where you have strategic
interaction between, you know, whatever duopolies
or something like that. Those models don't
seem to be used so much in macro these days. That limitation of this perturbation approach
isn't so obvious. Now by the way, I should
add one thing though, which is that Krusell
and Victor Rios-Rull, wrote a paper where they
showed how to adapt the perturbation approach when the strategic considers
iterations arise. However, it gave rise a
horrible fight Ken Judd, declared a fatwa on them, and they are now hiding. No, that's not true. But there's a lot of controversy over
what happens in strategic, when the strategic interaction, because when there's a
strategic interaction, there is no point where you know the value of the policy rule. In the strategic
interaction case, all the points have the
property that the value of that error function depends on the nature of
the policy rule. Anyway, the overwhelming
majority of models used in macro at this moment do have the property that you can compute a steady-state
for those models, and you don't need to know
the policy rule to do that. That's the non-stochastic
steady-state. Now, the non-stochastic
steady-state is a special case of the model I wrote down because
that's the case where Sigma is equal to zero. In the Sigma equal to zero case, that model is non-stochastic,
it's deterministic. In that case, I can
find a k star that has the property
that I know what the value of the function
is at that k star, namely as k star itself. All of you know that, but it turns out that's of a critical importance for straightforward application of the perturbation approach. As I understand Ken
Judd is working on the correct way
of doing this now. But at least as of last night, he hadn't totally
figured this out yet. What we do is to apply the perturbation approach
and the neoclassical model, we rig the model so there's
one parameter in there, this is Sigma, so that there
is a point in the state. The state is now k
a and then Sigma. There is a point
in the state where I know what the g function is. There's a k star, I can
find it, there it is, where I set the a
equal to zero and the Sigma equal to
zero, and there it is. I have a point where I can
evaluate this function. The error function
is this thing here, is the thing we
wrote down before. For the true g function, this is zero everywhere, no matter what Sigma, no matter what k,
no matter what a, as long as they're
admissible, of course. In particular, all
the derivatives, because R is zero, no matter what k a and
Sigma is the derivatives of R with respect to k a
and Sigma are zero. Now, we're assuming some
differentiability here, of course, but anyway,
they're all zero. There's going to be
differentiability assumed inside these functions. What I want to do
now is I want to show four properties using this simple neoclassical
growth model. I'll show you four results that we learn by
doing perturbations. The four results are
actually completely general. They're not of r with respect
to k a and Sigma are zero. Now we're assuming some
differentiability here, of course, but anyway,
they're all zero. There's going to be
differentiability assumed inside these functions. What I want to do
now is I want to show four properties using this simple neoclassical
growth model. I'll show you four results that we learn by
doing perturbations. The four results are
actually completely general. They're not going to get
the squiggle equals, so it's not exactly equal. Then I've got the
first term there is the linear term that the variables in
this argument are k, a, and Sigma, there's
the linear term. Then here you have
the quadratic terms and then the dot-dot-dot
means the higher-order terms. I want to say something that we know that's very easy to show actually using the
neoclassical model that I've already described and that
is true also more general. Number 1, the first
observation is that to a first-order
approximation, certainty equivalence is true. That is to say that
first-order approximation of the policy function
has the property that Sigma don't
make no difference. The coefficient and the
slope term on Sigma is zero. Trivial to show here,
true generally. Certainty equivalence
is the right way to go if you're doing a
first-order approximation, first-order perturbation,
I guess you could call it. The other interesting
property also completely general is that all
the coefficients in that Taylor series
expansion in for all except one involve
linear equations are trivial to solve for. The one exception is g_k, which involves looking
at eigenvalues, which is complicated
in some sense, we'll see in a second. It's a sense in which
you-all already know, but we'll just get it all on
the table at the same time. The one parameter in there
that's complicated to compute and it involves looking at eigenvalues
and stuff, but conditional having
solved for that, all the other ones are solvable
by linear expressions. Another result is that to a
second-order approximation, so bringing in the
second-order terms in there, we find that to a
second-order approximation all slope terms are
independent of variance, so g_k Sigma, g_a Sigma, which
are the slope terms in here, are also zero. The way uncertainty
starts to creep in as we go to these higher
approximations is through this, you could call this
intercept term. This guy here is
going to be non-zero. Again, these are trivial
to show in this example. Then of course,
if we go to third and fourth higher terms, you're not going
to get any simple zeros out of that stuff there. To a first-order approximation it's fun that you can show so easily these little results. Then the third thing
is recursive property, which I've already pointed
out in the toy example, you don't have to solve
jointly for everything, you can go recursively, you solve the linear. Then in order to
solve the quadratic, you need the linear as input. Then to get the cubic, you need the quadratic and
linear as input and so on. You can go recursively and
go as far as you want. You can, of course,
see that the notation is going to get to be a
nightmare very, very quickly. As I say, the actual
programming of these things is all about how to manage this complicated systems
that you end up getting. This by the way has all
been done now and Dynare. Dynare will do I think the plain vanilla Dynare that you just pulled
down from the web, Version 4.2.1, we'll do
third-order approximations. Then there's a super-duper
secret stealth weapon version of Dynare that only
certain people with the right password can get, we'll do any order
approximation. I'm not totally going
to show you this, but I want to get incredibly
close to showing you the key property about the first-order
approximation there is the g_Sigma equal to zero and the other one is that the g_k is a
complicated thing. That's the key thing that we can show with the first-order
approximation. The way you do that is you
take the derivative of that error function with
respect to the, well, it's only got three
arguments, and of course, at zero and then work out the derivative and evaluate it. We're doing everything around this one point; the
non-stochastic steady-state. Evaluated around that one point, and then what you get is
you get three equations. There are three
derivatives there. You get these three
equations, of course, there has to be
zero at that point because the thing
is zero everywhere, so zero everywhere
it's got to be zero at that one point too. I'm pointing the wrong
thing there first. You can see at the
very bottom here that the g_Sigma shows up multiplying the
square bracketed term. The square bracketed term in general is not going to be zero. If it were zero, then that equation would say
nothing about g_Sigma. But in general, that
square bracketed term is non-zero, so that that equation requires a g_Sigma be zero. That's a very important
result that g_Sigma is zero. To a first-order approximation, certainty equivalent holds. Then the second thing is that when you work out
these derivatives, especially if you go
to higher order k, k, k, a, all these
different things, what you find is that you
never see the number 2 showing up, except here. In the linear place, the number 2 shows up, the g_k shows up as a square and so finding
what g_k is is a problem. Of course, this applies in
the matrix case as well, so that g_k will be a matrix and something like the square in there.
Let's look more. Now, I've written out
this first term here, this first equation by dividing and multiplying
and doing a bunch of stuff. Written out this first
equation and you end up with what is probably for most of you or at least for me, if I woke up in the
middle of the night, I would look at that
equation and say, I feel so good now. That's the standard
second-order polynomial that you look at,
when you look at the Euler equation associated with the neoclassical
growth model. If you want to have
a hidden file, I think you have
the hidden file. There's this nightmare of algebra associated
with this thing. I think you have that in the
handout. Maybe you don't. In any event, one thing I want to draw
your attention to here, this is a K, K, K,
these are Ks here. These are derivatives
with respect to K. Only if this is a K will
you recognize this. If it's k is not the same thing as you've always looked at. This is of course, the curvature of the production function, this is the curvature of
the utility function. Both of these are concave
functions, so this therefore, is a positive number. The fact that this is a positive number
and that this is 1 over Beta implies that, but well, we know there are two
solutions to that thing, but it implies something
about those two solutions, which is that there's one g_k
that's bigger than 1 over Beta and one g_k
that's less than 1, that solves that equation. This is the confusing equation because I got to a choice
which one to pick. It's very straightforward to show that analytically. This little diagram
to show that. Now, in this particular example, the neoclassical growth model, you can show theory tells you that you're an idiot unless you pick g_k less than 1. The reason is that you can
show theoretically that the policy rule cuts the
45-degree line from above. You know that the actual thing you're trying to approximate is less than one and
that this thing is something else presumably
has some interpretation, I have no idea what it is. But this other thing here
is not of any interest, we know that in the case
of this particular model. Now, in practice, of course, it doesn't
always work out this way, and there's a lot of
hand-wringing associated with this equation that power 2 is a problem because it
gives rise to a decision, which is the right solution. For example, you can find interesting models
where there are 2g_k's that are less than one that satisfy that equation. Then of course, in
cases like that, you actually have
multiple solutions. Those are examples where you
have multiple equilibria. They actually might
be very interesting. They have the
classic examples of strategic complementarities. I'm going to work
hard if I know you're going to work.
We're at a factory. You're making the wheels
on the car and I'm making, I don't know what, the
bumpers on the car. But I know that if
your wheels suck, then what's the point
of making nice bumpers? I guess, you don't
have any reason to think that way of me. I make the motor,
you make the wheels. You make the wheels, you know that if I want to
make a bad motor, you have no reason to
make decent wheels. If I think you're going
to make bad wheels, that I have no reason
to make a bad motor. These are strategic
complementarities. We have an equilibrium
here where we could both make crappy stuff. On the other hand, if both of us thinks the other one
is working hard, in this example, you have
another possible equilibrium. One of the standard sources of multiplicity and
dynamic models is the strategic complementarities, whether we turned to my effort is an increasing
function about hard. I think you're
going to your work. It's not the only source, we'll talk about another one when we get to the
dynare example, but these things do happen and then you
really have to struggle. Now there are also examples were both g_k are bigger than
one in absolute value. Now, if it's bigger than one
in absolute value, well, I'll tell you this 99.99 percent probability is
that you made a mistake. There are economic models where both the Gs are bigger
than one in absolute value. If x Tolkien Lucas
have a nice example in their Chapter 6. But it's a goofy
model, crazy model. I've never seen a real
model that had them both bigger than one in
absolute value. That is a possibility, though extraordinarily
unlikely to be actually a true possibility. That is the thing
that could occur. Of course, in the case
where they're both explosive and you
haven't made a typo, all these methods
are irrelevant, these computational methods. Because the
computational methods assume you're going to stay in a neighborhood of the point where you're doing
the perturbation, and if you're going to wander away from
that neighborhood, you have no
justification for using perturbation methods because
perturbation methods are only going to be
valid in a neighborhood. If you find that there
are both bigger than one, you better hope
you made a mistake and that you find that mistake, because if your model
implies bigger than one, then you can't do perturbations. Now, in this particular example, once you've figured out what
g_k you're going to use, then g_a is simply found by a linear equation off
of this guy here. If you look inside here, you'll see that g_a appears
everywhere linearly. This e_g is showing up because e^g is there because it's the exponential of g that appears in the return function. All these results completely generalize to finite dimensions. It is always true that when
you do these perturbations, you're going to
have a g_k problem. In practice, that problem is not finding the eigenvalue
of a polynomial, it's going to be the
finding the Eigen matrix of a matrix polynomial instead, but it's still the same thing, and the analog of g_a, which is the feed back onto the exogenous variables is always a simple linear function of whatever you did with g. Finding g_k
is always a simple, involves solving a simple
linear equation once you figure out what g_k is. Let's go to a numerical
example to try to pin down stuff and see as
an illustration here. What I did was I took the
neoclassical growth model. Prescott, in 1986 wrote this paper about the real
business cycle models. In that model he has
hours worked in there. Of course, we don't
have hours worked here, but I tried to pick
parameters that looked like the ones that he picked. He actually used log utility, but I wanted to
make things tough. I used a Gamma equal to two. Gamma is the risk aversion, also 20 will do Gamma two
and then in parentheses 20. This is going to capital. This is depreciation. This is supposedly
a quarterly model, so it's two percent per quarter,
eight percent per year. The correlation parameter that he said was correct is 0.95, and then the innovation to the shock he estimated was
actually 3/4 of a percent, I got one percent in here, so I'm making it a little
bit on the big side. Then I went off and computed the second-order approximation. This g hat function, here's the g hat function. It's a function of k.
This is the logs again. This is log of k star, I don't know what
that means exactly, and then you have the
slope coefficient. The slope coefficient is 0.98
with this risk aversion, and it's much bigger
without risk aversion. This makes perfect
sense, by the way. When g_k is big, it
means that because this is now all in terms of the log of the capital stock, this is now the percent
deviation from the steady state. What g_k is, k minus
k star is a gap between the capital stock
in the steady-state, and g_k controls what happens to that gap
in the next period. If g_k is very close to one, then the gap is being
narrowed very slowly, because this period is about the same as
the gap last period. If g_k was 0, then whatever the
gap was last period, you'd had the gap closed
in the current period. What's happening
when Gamma is big is that the gap gets
closed more slowly. The g_k goes up is a general standard
result, for good reasons, which is in order to close the gap and
go to steady state, you got to not eat
so much right now. That's a painful thing, if your Gamma is really big, because there's a lot of curvature in that
utility function. The less food you have, the more unpleasant it is for
you in that case. You get slower convergence
when Gamma is equal to 20. There's no surprise there. This is the coefficient on the realized value of the
exogenous uncertainty, that's g_a, that's equal
to this number here. Here we have the g_Sigma, this is not 0.00, this is zero, completely zero here,
times this Sigma. If I made this sigma
equal to 0 or 1, it makes no difference to
the linear approximation. This is the certainty
equivalent thing. Now, we get to these
other terms here, and then what we get, these second-order terms
are really rather small. For example, here's
G Sigma Sigma is 0.000024 times this number here, but this is a number
between zero and one, then you have to square it, and then you have
to divide it by 2. That's not a big deal. This term here plays a very
minuscule role. If you look at this term here, here's another quadratic term. It plays a very small
rule for example, suppose that your one percent
below the steady-state, in normal fluctuations
in the United States, you're going to be one
percent, two percent. One percent is 0.01. This number will be 0.01. Now, you square it, is 0.00, whatever they wanted, it's
like a whole bunch of stuff. Then you multiply it by that, pretty soon you've got the
thing is squished down not doing very much. Here's the slope terms I
was describing before. These are exactly zero. Notice as you go
to second order, this is a quadratic term
in terms of the Sigma, but same with a constant. You can think of this
as actually linear. If we go back to the
linear representation, you could just think of
this as being part of that, not in the Taylor
series expansion sense, but in the real sense. What the fact that this
is zero means is that, this should be a star this
term looks like that one, so the fact that this is zero, it's saying that
uncertainty also does not affect your reaction
to this thing here. It does not affect
the constant term, but it also doesn't influence
your reactions to things. This guy here is zero. Your slope doesn't respond. It's just total certainty
equivalence here, both the level and the
reactions to things to in the linear
representation are zero. Studying just
aggregate variables like consumption and whatever, that the linear approximations probably give you decent stuff. Now, at the same time, we know we don't do terribly well on financial variables so
we may want to look at more interesting utility
functions and so on, then the second-order
approximations might make a difference even in a country
like the United States, where you have not
very volatile data. If you leave the not very volatile post-war
United States and go to the great depression then
the linear stuff is no good and you need nonlinear. If you go to Argentina, you're going to need the
nonlinear stuff and so on. If you go to Europe, fine, it just post-war, it's fine. I don't know where this Greek
thing is going to take us, but for the time
being, post-war, Europe is fine in terms of linear approximations
and not getting into any super exotic preferences
and stuff like that. The conclusion from this
example here is that for modest US size fluctuations and for aggregate quantities, it's probably reasonable to work with first-order
approximation. What is the first-order
perturbation? Take the equations that whose expectation we're
trying to set to zero, replace them by
linear equations, and then solve the
resulting system. Do the linearization around non-stochastic steady-state
so we're implicitly supposing certainty
equivalents and this is okay as a
first-order approximation. Let me give you the general description of linearization, that's the last slide on
this discussion here. The general thing is, here you have these
dynamical equations here. Think about these
other Euler equations, but also resource constraint
and all the equations that characterize the equilibrium
are all lumped into here and they involve
two types of variables. One type of variable
is this vector S, which is the exogenous
variables, either the shocks. And another type of
variables is vector Z, which is the ZT contains all
the endogenous variables whose values are
determined at time t. It is a pretty canonical, in fact it's
completely general in some sense because any system, even if it involves
equations that look really far into the future, can always be written in this
kind of first-order form. Anyway so there is a
canonical representation of a set of equilibrium
conditions, but written after expressing
it in linear form, after taking the linear Taylor series
expansion around steady-state. The system is then
completed when I write down what the
law of motion is for the exogenous tracks,
so there it is. Then the way I solve that thing is I'm just going to pause
it a linear solution, a and b and this is the
analog of g_k here, this is the g_k, this is g_a and s t here is the analog of a, a, of course, was a scalar now, s can be any vector
whatsoever of any length and the way we solve these things for a
and b are unknown, we use the method of
undetermined coefficients. Method of undetermined
coefficients is a solution method that pins things down by what
they're supposed to do. What a and b are supposed to do is they're supposed to
when you plug in z, according to this law
of motion in here, it's supposed to set this thing exactly
equal to zero for all possible values of ZT minus one and ST,
those are the state. To do that, you just totally straightforward
algebra and stick this thing in here and deduce
the implications for requirement on a and b
that this has to be zero, what you find is that
the matrix a has to satisfy the same type of polynomial equation that g_k did before except now
it's generalized because these are matrices now. Then conditional on
what you find for, a the solution to b
is completely linear. This thing having to be equal
to zero is linear in b, conditional on the matrix a. The thing on the
right is no problem. The thing on the left problem. Because that's a matrix. We know that with
scalar polynomials are multiple solutions
where you can bet your life that with
matrix polynomials there's an even more solutions. So solution algorithms that
go to work on this thing. What they do is they look at this equation and
they ask themselves, what are all the possible a's
that solve that equation? Now, it turns out
it's completely trivial to instantly
figure that out. Instantly figure out and
put them on the wall it's a finite number so
you can put them on the wall like
dynare does this. It puts them all up on
the wall like that, all the possible a's that
solve that equation. Then it calculates
the eigenvalues associated with each
one of the a's and then it looks and if it finds only one that has all the eigenvalues less than one in absolute
value then it says, "Ha" that's the a that I want. If it finds two that
have eigenvalues that are less than one
in absolute value, it crashes and send you
an insulting message, that no blonde shard Con
condition is not satisfied. I'm not sure that
should be insulting, but you feel bad when
you see the message. Then other possibility,
of course, is that when it looks
through all the a's, it finds no a that has
eigenvalues all less than one, then it crashes also
because that isn't. So it's going to
look for a unique a that solves that thing and if it does not
find a unique a, that solve that thing,
it will then crash. Obviously, if there are
two a's that solve it, then there really two
equilibrium the two equilibria, you asked it for an
equilibrium, it found two. So then it crashes because it doesn't know which
one to give you. Now there's all kinds of
interesting things that happen when there are
two a's, first of all, you have to equilibrium, but
you also have a continuum of sunspot equilibria
attached to it as well. You could imagine actually modifying dynare so they will give you a menu of possibilities when you
have multiple a's, but that's not the way
it is set up right now. If you have exactly one, has eigenvalues less
than one absolute value, that's the solution
and otherwise, you have many solutions and
conditional on the matrix a, you then solve a
simple linear system, which is that thing
up there, to find b. 