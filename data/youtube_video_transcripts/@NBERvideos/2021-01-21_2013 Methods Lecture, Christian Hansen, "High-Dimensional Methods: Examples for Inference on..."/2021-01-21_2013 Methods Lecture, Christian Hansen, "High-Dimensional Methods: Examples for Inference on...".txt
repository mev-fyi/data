Christian Hansen: What
I'm supposed to do here and what I'm going to try to do is Jesse, and Matt, and Victor have gone
through a whole bunch of different mechanics and
methods and applications. In applying a lot of this stuff, there are a bunch of nuts
and bolts and knobs that you have to turn and choices
that have to be made. What I want to do is walk
through some examples to illustrate some of this stuff
in some real data sets. How would you
actually turn some of these knobs and what choices
do you really have to make and hopefully get across some of what we'd actually be involved
in doing this. I wanted to be perfectly clear that everything
I'm going to talk about our main as
illustrative examples. At no point am I going to
present anything like novel in terms of this is a brand new research thing that you should
be interested in. Hopefully you will find examples mildly interesting anyway. Then along those lines, these are all examples
taken from other papers, all of these examples have
identifying assumptions, I'll talk a little bit about that but that's not
what I'm talking. I'm going to condition
on those guys who thought carefully about
these problems actually having done a good job doing that and those
identifying assumptions actually making sense in the
context we're looking at. I want to just talk
about what the stuff we've been doing for the
last couple of days could bring to these handful examples, and how you could have applied those things in these examples. I think importantly, before
actually getting to this, you need to remember all this high-dimensional stuff
we've been talking about, as economists should be, hopefully thought of as ways to complement your
economic intuition, complement the things
that you're doing, not replace them. All the stuff that you
always think about, how do I identify this? What things do I
have to control for? Where's the source of variation? You still have to do all that. You can't just say,
they've got these nice, beautiful model selection
procedures and they'll do everything for me and I don't
have to worry about that. Part of what I want to
illustrate is in fact, you have to do a lot of
thinking about this stuff. In those choices
you have to make to implement
high-dimensional methods, you have to choose a
variety of things, and many of those things
should hopefully be motivated by your underlying economic intuition
for the problem. With that said, let me just move on and actually start
talking about some examples. The first example I
want to talk about is taking from a
series of papers by perturbofanti and y's
on their responses to these papers by
Incangallon shoals and that was in the mid-'90s. I cited three, but there
are at least 10 papers on the basic topic of
what is the effect of 401k's on people's
wealth accumulation, their savings, something
along those lines. The baseline model,
this is roughly the model that is estimated in the material
event dy's papers, I admit I have not looked at those papers in
five or six years, I just have this dataset
that Jim was nice enough to give me
and I have applied. Assuming that these are
the controls he used, which my recollection is they are because this
is what I started doing when I did this back
with Jim once upon a time, but they may have
changed, I don't know. Anyway, the basic specification
is the following, we've got some
measure of assets, accumulated assets on
the left-hand side, that's going to be the
variable of interests. The two things, I'll look at
are net financial assets, which as its name
for getting the net, which is a little confusing, I'm not going to worry about,
is just things you hold in financial assets,
stocks, bonds, etc. The other thing I'll
look at is total wealth, which of course, is not total wealth
when you think about human capital on
a bunch of other stuff, but this is total
wealth as measured in the Survey of Income and
Program Participation, this is your house, various things, you
own your car, etc. Those are the two outcomes
I'll be looking at. The variable of interest here, I've labeled d to be consistent with the other
notation we've been using, that's whether or
not the person is eligible to receive
a formal 401k. What we really want to know is when this program came around, when people had
access to 401k's, did that increase some
measure of savings. We might be interested in
that for a variety of reasons and hopefully as economists you can think of some of those, I don't need to talk
too much about that. Now, in addition to that, I'll talk about X
in just second, let me remind you of one
important thing that came across hopefully
in Victor's talk, which is important when
thinking about these methods, at least as we know
how to use them now for answering
economic problems. The thing that we're really interested in is
small dimensional. If we were interested in some underlying very
high-dimensional object, then the things Victor
talked about where you can decompose this
cell, we can screw up, we can have more selection
mistakes and estimating this nuisance part
and those don't spill over onto the
low-dimensional part, that's not going to be true
if we actually have to do selection about that
low-dimensional part. It's important in
everything we're doing that we're
conditioning on, we're economists we know this is what we want
to learn about. We want to learn about the
effect of being eligible for 401k on how much
assets you have. That's one thing where
there's no selection over that one thing we
don't care whether we're not going to try to learn that that's identically zero, that might be so
expos thing we do, but that variable
is in the model, that's the thing we
want to learn about. Now, once we've got that
thing we want to learn about, we have these other controls. In the pattern of
Venting Wise example, the argument roughly for identification is the following, I'll tell you what the
controls are in just a second. This is data from
the 1991 survey of income and program participation I'll wait for it
doesn't really matter. The argument for why you
might be able to treat 401k eligibility as exogenous conditional on stuff
is roughly as follows. Like I said, we're going to condition on this
argument being true. There are good reasons
it might not be, but you can talk to me
about those later and I'll say those are good reasons
and then we'll move on. The argument is as follows. In 1991, this is data collected, of course around 1991, 401k's we're pretty new. Now we know that in principle, if people had
arbitrary foresight, they should be choosing
which job they have based on whether or not there will be
eligible for a 401k. Of course, if they're doing that and whether they want to
have a 401 k is correlated, something that we
don't observe, say, how much they like saving
then if we just run the regression of wealth on whether or not you're
eligible for a 401k, we're not going to be
able to figure out whether we're learning
the effect of being eligible for 401k
or whether we're learning the effect of
just a fact you like safe. That's fair but this is
when 401k's where new, we're hoping people really weren't super four-sided and they didn't
sit down and think, when I take this job in 1990, I'm going to take it because maybe a little off
from your 401k in 1991 so they're not
thinking about that. But we do know that
preferences for saving, or we assume, and it
seems reasonable, might be related to
people's incomes. Maybe for a variety of reasons when we see people
have higher incomes, they also like to save more. It's also true certainly
at this time period, that if you were in a job where income was going to be high, you are much more likely
to be offered a 401k. Their argument was roughly the big source of endogeneity of confounding here
is the fact that people have this income thing, income is related to how
much they like save, income is related to
whether they have a 401k. If we don't control
for income now what we're estimating is a
confounded effect between the effective actual 401k's and the effect of preference for savings but once we control for income,
everything's good. That's a very simplified
version of 12 papers. With that simplified
version out of the way, that leaves us with
this question of, we need to control for income. Once we've done that, we
have exogenous variation, we can estimate the effect
of 401k's and go home. What did Peter Venting Wise do? They did a very sensible thing. They went out, got some data. In their data they had
measures of income, they had measures of other
things which seem plausibly related to income
and other factors you might care about. They controlled for
I don't remember, I could count them right here, but I'll say 15, that's in the ballpark of 15
characteristics. Those characteristics are dummy variables for
income categories. You can see the income
categories they used up there. They included age
and age squared. Clearly, we can think of
stories for why preferences for saving and wealth accumulation would be related to
how old the person is. They controlled for how
big your family is, how much education you have, whether or not you're
married, two earners, so this is household level data so they control
for whether there are two earners in
the household or just one earner
in the household, whether you have access to a defined benefit pension plan, which arguably is
endogenous but again, we're conditioning on
all this being fine, whether you have an IRA,
whether you're a homeowner. There's our variables. They
have 9,915 observations, we've got 15 controls, we run OLS of some measure of wealth saying that total financial
assets or total wealth. We get some estimates
for the impact of being eligible for 401k,
on accumulated savings. Those estimates here at
the bottom of this slide. The estimated impact of, sorry, of 401k eligibility on net financial assets
is 9,216 additional dollars in accumulated assets
if you're eligible for a 401k with a standard
error of around $1,000. On total wealth, the effect is smaller, a slightly bigger
standard error. We can tell stories
for substitution between asset classes. If you're given a 401k, that's of course tax advantage financial assets so you should shift some of your saving into those tax advantage
financial assets. Maybe not as much actual
total wealth accumulation. We can tell stories
along those lines and that's part of what they do. That's the baseline. I think
that's a sensible baseline. I think the thing that
you need to keep in mind, and we're conditioning on
their argument being true, they need to control for income. If their story is right, controlling for income
is the key to getting exogenous variation
in eligibility that we can use to actually learn the effect of eligibility. A question that a person might
ask, maybe you wouldn't, I would is, seven
income categories. Is that controlling for income? That's flexible, but we could think of
other things to do. Victor talked about this
little education example, where a simple
function in education, it gets the pattern of how wages change with education but it misses some
important stuff. Are these seven dummy variables sufficient to capture
the entire pattern well enough that what we're leftover can be taken as
good as randomly assigned? That's a question we might ask. What might happen,
there might be more complicated nonlinearity, there might be interactions. Their model is everything is nice and additively separable. Your income and your
age don't interact so wealthy older people are exactly the same as
wealthy younger people. Maybe that matters.
We don't know. But that's, of course, arbitrarily disallowed
in their specification. They're smart guys,
I'm sure they thought about these
things and say, well, we can't do
everything, so here's 15 things we can try
and there they come. Now, in addition to just asking, did we do a good enough
controlling for income? There's another question that we didn't talk much about but we might be able to do
better in terms of what they did just from an
efficiency standpoint. One thing which I don t think is happening with 15 variables in nearly 10,000 observations, we might be over-controlling. Maybe we've put too much
stuff in there and we're losing efficiency because we're just absorbing
degrees of freedom. That seems like a terrible
straw in this example, but it's at least possible. I think more importantly, suppose we were to
take the baseline. The eligibility was
actually randomly assigned, which I think most of us
would agree it's not, but pretend for a
second you thought it was randomly assigned. Even then, you might
want to control for more than 15 things to absorb residual variation and get more precise estimates of the effect of the randomly
assigned variable. We've got this baseline.
It's a sensible baseline but we are wondering, or at least I'm
wondering and I'm trying to convince you
that you should wonder whether we've done
an adequate job in addressing both the
identification concern, did we do a good enough job
controlling for say income and age and asking, could we do better in
terms of efficiency? That's the questions I
want to address here. The overarching theme of the
entire two days worth of lectures has been
regularization is a good idea. What's regularization? It's variable selection,
dimension reduction, how you want us
to talk about it. We've got this really
complicated world out there. We'd like to learn about
this complicated world, but if we allow the world to
be arbitrarily complicated, we can't learn anything so we have to impose dimension
reduction somewhere. I think it's important to
note if I went back to my income control question.
Sorry, I'm wandering. If we went back to my
income control question we might come back and say, well, gee, I don't know
how to control for income. Maybe to adequately
control for income, I just need a coefficient for every single person
in the sample because every single person could have a different effect of income
under saving preferences. It's possible. If that's the world I want to
live in, that's fine. I don't think any of us want to live in that world or
we wouldn't be here. We'd be doing theory somewhere. But if that's the world
we want to live in, the answer to any
inference question based on data is you
can't learn anything. Once I allow for arbitrary
complexity in the world, I have no way to learn
about the world from data because the next
observation I see could be arbitrarily different from
anything I've seen so far. I have to at some point impose regularization
on the problem. Often we impose
regularization on the problem by intuition. That's what perturb of NT-wise did in these
examples I talked about. They have some
economic intuition. They're smart guys, they sat down, they thought about this, said here's the key driver
of omitted variables bias. It's income. Why is it income? Because we're economists,
we thought about this. That's what we think
is driving them. Once we have decided that income is the key driver of
omitted variables bias, we're going to try to
flexibly control for income. How are we going to do
that? We're going to do in flexible functional form based
on seven dummy variables. Fine. That would
correspond on this slide. See. I don't know
how this works, but the second bullet
point says intuition. Intuition is probably the
most commonly applied way to do regularization or
dimension reduction in applied work everywhere. You sit down, you think
about the problem, you come up with your list
of your favorite confounds, you put them in the
model and you go home. We're all familiar
with the tables that show up in applied papers. You then do some small tweaks to that confounds and say
how the results don't change that much when I
tweak this set of confounds by a variable or two
so everything's fine. Not a bad idea. I want to be very
clear about this. That's actually a
really good idea. The stuff we've been
talking about for the last couple of
days is hopefully a way to add a little bit additional to that already very sensible thing that
people are already doing. Another approach,
which is of course the gold standard
and we would all love to do it is
the first approach, the dimension
reduction, which is we just randomly assign stuff. If we randomly assign
the treatment, then we have already
reduced the dimension of the necessary
control set to zero. We don't have to
control for anything. We might want to control for stuff to improve efficiency, but if we really have randomly
assigned the treatment, then we've randomly
assigned the treatment. We can go out,
compare two means and go home. That would be great. Obviously, we didn't randomly
assign 401k eligibility, and unfortunately, we
don't randomly assign a lot of the stuff that
we do in economics. Even if you did do
random assignments, sometimes the
world's complicated. Maybe you do stratified
random sampling, maybe you do something else, maybe people have done
stratified random sampling and they haven't told
you how they did the stratification
in which case you still have to control for the things that
you stratified on. Even in that idealized
random assignment world, there is still scope
and principle, if we're doing variable
selection by ideas. Now, the thing that we're going to or we've been talking about for the last
couple of days, are formal regularization
procedures. I'm going to focus on formal
model selection procedures, but there are lots of
regularization procedures. That's fine. You've heard about several of those and you can
go read about a lot more. Now, a very important
thing which has been under the rug, I feel, in the last couple of
days of lectures is formal selection is a
really nice idea but it is, I think, exactly equivalent to the problem of searching
for a needle in a haystack. What we're going to do is
we're going to go out and say the world is
really complicated. There is this signal out there that we're
trying to learn about. That's our needle and then everything that could explain
the world is our haystack. Now, if you looked at Victor's rate conditions,
this shows up. There's a log P in all
those rate conditions, and Victor said log P, that's small. We don't
need to worry about it. Well, if P is E to the N, log P is really big and you
do have to worry about it. What we're doing when we do
formal variable selection, it's very important
to keep in mind you still can't do everything. You still have to use
economic intuition to guide yourself to keep that haystack to a
manageable size. Now once the haystack
is and manageable size, then we're going to hope
we can sift through that haystack and
find the needle. The usual intuition
approach to doing dimension reduction corresponds to the case where we already said the haystack
has eight pieces in it. Some of them are
needles, some of them are hay but there
are only eight pieces. We're going to look at all
eight. The formal approach to dimension reduction says it's a pretty big haystack.
Most of it's hay. There's some needles in there. Something that
again, if you look at those rate conditions
really carefully, what you'll see is if the
haystack is really big, those needles had also better be really big or you won't
be able to find them. There's some trade-offs here. You have to think carefully
about this and that's part of what I want to talk
about. What are we going do? We're going to use our economic
intuition to help us to keep that haystack to
a manageable size. When that haystack is
in a manageable size, we're going to apply
these methods, say lasso, to try to
find the needles. But again, I think it's
important to keep in mind, you can't make the haystack
arbitrarily large. That gets you back
to the world where you can't learn anything. That's what we're going to do. Here's a slightly more formal
or more laid out approach to the intuitive dimension reduction that we talked about. There are a bunch of
choices that PVW made. They chose the data.
I'm not going to talk too much about that,
but there it is. They made some a
priory selections about the day they're
going to look at. In particular, the SIPP
is actually a panel. They ignored the panel
aspect. That's fine. We can talk about that later. They also impose some
criteria on things like age to keep things to
a sensible population. Of course, that's also
controlling for age in a very flexible manner along
the dimensions you cut. But we're not going to
worry about that either. Now, anytime you
approach real data, you're all aware of this, there is a data selection step that is roughly unavoidable. The SIPP Wave 4, I looked it up before, a couple of weeks ago when I was putting these
slides together, has 655 raw variables in it. Now, if we started off
with 655 variables, in principle we could do that. The variable selection
methods say, P is 665 and 9,915. Maybe we can even try some functional form
tricks in there, which I'll do in just a minute, but that's a pretty
big set of variables. In a priority, we know most
of those things aren't going to matter or we hope they
aren't going to matter. If you look at those
665 variables, a whole bunch of them are
purely administrative, technical kinds
of variables that obviously don't work.
We throw those away. Perturb of NT and Y's
have told us a story, which we're leaving right now, that really we need
to control for income and things that look like they might proxy for
income or savings preferences. We sift through that set of 655 variables and
we come up with nine variables that
look like they might proxy for either preference
savings or income. Why did we choose
those nine variables? Well, I chose them
because that's what Perturb of NTY's
chose and I'm too lazy to go back to the SIP and actually draw my own data. But I presume they chose
those because they actually carefully thought
about things that of these 655 variables, here are nine of them that look like they're
important confounds. Now, once we've got
those nine variables, we choose a functional form. Their functional form, like
I said, was those dummies, a quadratic and age, some dummies for education. Now, that's all fine. We gave you those results. What does formal dimension
reduction look like? First of all, we need a model. Victor talked about this model. Here is the partially
linear model. We have one parameter
of interest. Parameter of interest
is the effect of 401k eligibility on
either total wealth or net financial assets. We then have this
confounding effect, g of x. What is that? That's
omitted variables, whatever it is, that's the
control we need for income. We have an unobservable. We're going to do what
we did in Victor's talk. We're going to approximate
that function g of x with a linear combination
of other stuff. This is the approximately
sparse model, where my Z_i prime Beta g is the linear approximation
to g of x. Then we should probably call that Zeta tilde,
but I'm not going to. We've thrown the
approximation error into the unobserved stuff. We're going to do
the same thing for the treatment based on the intuition and the model that Victor already
talked about. We know that we need to
model both the propensity, which is, whether or not
you have eligibility as a function of important
confounds and we also need to model the outcome. There's our model.
We're assuming that conditional on this set
of characteristics, in particular income, that those error terms
have a mean of zero. Given this, we write down
the two reduced forms. One reduced form for financial
assets or total wealth, another reduced form for whether or not you're
eligible for a 401k. Second big picture
in terms of stuff Victor talked about using
these high-dimensional methods and putting them into
a treatment framework or structural estimation
framework more generally, the statistical
methods for doing high-dimensional modeling are all designed for prediction. At least the ones I know. Now, why do we go back
to reduced forms? What are reduced forms? Those are predictive
relationships. Once you've turned
your structural model into a set of predictive
relationships, it's plug and play with your favorite method
to do prediction. I'm going to use lasso because Victor and I have some
nice results on LASSO, it's easy, there are some
nice features about LASSO. If you don't like
LASSO, if you like SCAD or your other favorite way to approach variable selection, I'm going to argue
it doesn't matter that much. Victor
might disagree. He might say we haven't
done the theory, but I'm going to argue it probably doesn't
matter that much. What's important is you go back to prediction problems and you use methods that are good for doing the
prediction problems. You then figure out how you do a good job predicting things, which is what the data can
actually tell you about. You use your economic model coupled with what the data
can tell you about to learn about the effects of
the economic parameter of interest. That's the game. Then this is just a quick recap, I guess, of what
Victor talked about. Why do we want these
two equations? Why not just focus on
the first equation? Well, we get some
nice robustness by looking at two equations, the reduced form
for the treatment is hopefully fairly clear
what robustness we get there. We're finding variables that are strongly predictive
of the treatment. Variables that strongly predict whether you've received
the treatment are pretty good candidates
for things that might cause omitted
variables bias. We want to make sure
we control for those. What do we get from the
reduced form for the outcome? Well, of course
we find variables that are strongly
related to the outcome. That gives us two things. Number one, if those variables are strongly
related to the outcome, but they don't have
much predictive power for the treatment, they'll help us
improve efficiency by absorbing residual variation. The other thing we get is,
as Victor talked about, a problem with all this
variable selection stuff is variable selection
is in real life, or at least under
assumptions I think represent real-life,
never perfect. Even though we're
going to find a lot of important confounds
in that first step, there's possibility we miss some because the coefficients in that first step are
small but non-zero, and they're big enough
that if you ignore them, you would still have big
omitted variables bias. What are we doing? Well, we're looking at
the outcome equation. Anything in the outcome
equation that has a big coefficient could
be associated with a small coefficient in the treatment equation
and failing to control for that would lead
to omitted variables bias. What we're missing
when we look at these two steps is things with small predictive ability
for the outcome and small predictive ability
for the treatment. If you combine small
predictability at both those stages, it washes out at
least in theory. You could all ask, does
it matter in practice, and that's a good question, but the theory says it's fine. That's what we're
going to try to do. Now, given that we've
laid out our framework, our model, we have
choices to make again. Number one, dataset, same thing as PVW because
that's the date I have. Number two, baseline variables, same as PVW because again,
that's what I have. They gave us, hopefully
in their papers, a compelling argument that us as the economists would
be compelled to make in our own papers
for why this is a good set of characteristics
to control for, to hopefully get around that
omitted variables bias. These are really
the features that are driving the endogeneity. If we miss features that are driving the endogeneity
at this stage, doing high-dimensional methods
aren't going to help us. We need to have done a good job thinking about we
have a clear idea of where the endogeneity is coming from and we have a clear idea of
how to control for that. We get rid of income and
things that look like income. Now, we could at least in principle
think about adding additional variables. That might be a good
idea, might not. I'm not going to do it. The thing to remember, anytime you add an additional variable, you're making that
haystack bigger and that's going to reduce your flexibility in
other dimensions. If I add another variable, one more variable
probably doesn't matter, but if I start thinking
about interactions and polynomials and what sorts of transformations should
I really be taking, one more variable quickly makes the space of things I'm
considering much bigger. I hopefully have a good idea of what those variables might be doing before I start
just going out and adding additional
variables at this stage. Now, what else do I need to do? I need to choose
functional form. I need to choose functional
form to control for income. I want to be very
flexible on that because that's at least my straw man. That's the thing I think is
driving the endogeneity. I want to really do a good
job controlling for income. I'll give you some more
details on the way I chose to do that
in just a second. Before I do that, we've still got our method for selecting the models
we're going to use. My choice is going to be LASSO. There are other
choices you could do. I'm going to use LASSO,
and I want to just remind you really quickly what we have to do to
actually use LASSO. First of all, I think most of us who are
trained in economics, at least since I went to graduate school
probably earlier, believe the world is
fundamentally heteroskedastic. We're going to use the
heteroskedastic version of LASSO that Victor
talked about. If you remember,
roughly has this form. We have to choose a
penalty parameter that I've called Lambda hat. We have to choose these
loadings on the Betas, which I've called Gamma hat. Theoretically, I'll give you the expressions
in a seconds. Those are choice things
or things that have to be estimated from the data to
implement this procedure. Now, once I give you Lambda hat and Gamma hat,
things are trivial. You go out, you use your favorite software
package, Stata or MATLAB. I assume this is
in other things. You do LASSO and
you get answers. How do we choose
these two things? First of all, Gamma hat. Gamma hat turns out to be the
annoying one in practice, but Gamma hat, at least in principle,
we know what we want. We know theoretically that
Gamma hat should look like the variance of the score, the variance of X_i Epsilon_i. If we knew Epsilon_i,
this would be trivial. We would just go out and plug in Epsilon_i and that would
give us our Gamma hats. We don't know Epsilon_i and that's where the pain comes in. What do we do? For a fixed value of Lambda, we go out and we guess a value, an initial value for Beta. The easiest one, the one
Victor mentioned is zero. We just plug zero in
for all the Betas, that gives us an estimate
of the residuals. We use that estimated residual to form these penalty weights. We then go out, we estimate the LASSO coefficients or
the post-LASSO coefficients. We get new estimates
of the residuals. We plug those in, form
new penalty weights, and we go through that thing
until we get convergence. The stuff I've done,
I've either iterated these two convergence or
stopped after 100 iterations. In every one that I checked, the convergence was achieved in fewer than 20 iterations, but I have to admit I did enough of these
things in preparing these slides that I didn't check the convergence of all of them. It's possible some of these
went to the full 100. If you care, you can go get my code and check
on your own later. That's the basic idea for
a fixed value of Lambda. That's easy. How do
we choose Lambda? We've talked about two methods during the last couple of days. One is cross validation, one is using a
theoretical value. Theoretical value
is super simple. You put a number in, you stop, and you let the thing run. There are a couple of choices
I use in the examples. They are both bounds on the theoretical value that
shows up in the paper. Why I chose to use two different bounds
instead of just one is hysteresis on the
way code is written that it originally
started off as one, then morphed into
another and you get roughly the same answers
and that's what it is. These bounds do depend
on a choice parameter. We know what p is. Phi inverse is of course the
inverse of a normal CDF. We know what n is. I think there might
be an n missing in the simple bound one, but can look up the
papers for the details. Q is a choice parameter. Theoretically, we
need q to go to zero. In most of the things I've done, I've set Q equal to 0.1
divided by the mass of P or N, which is the number
of Victor and I and Alex have used and seem we like. That 0.1 if you remember what Matt Tutty
talked about yesterday, he talked about
the rule of thumb for false discovery
rates is 0.1, that people will use. Q here is at some level
relates false discovery rates. Q is related to the
size of a test for the null hypothesis under the null that all the
coefficients are equal to zero. Q is a value drawn from the distribution
for the maximum of your estimate scores
but related to the estimators of Beta hat under the null that
everything is equal to zero. You can think about
if Q is really small, what you're doing
is you're saying, I'm going to choose this
penalty so that things enter the model only when I'm pretty sure the
coefficients are non-zero. If Q is really big,
you're saying, I'm going to let everything enter the model and you
can see how that works. If I set Q equal to one, I forget which
direction it goes. Q at some, at one
of the cutoffs, either one or zero gives
me a penalty of zero, which says everything
goes into the model. Looking at the normal CDF
when that would be Q of zero, because phi inverse of
one is Q of whatever. I'll stop here because
I am getting myself confused. You can figure it out. The two things we've
used are 0.05 or 0.1 divided by Max PN. No application that
we've looked at. Has that made a difference? In principle, it
could, if you choose Q to be a very tiny number, that would matter if
you chose Q to be a very big number
that would matter. We're choosing tell
probabilities. Cross-validation is at some
level fully automated. There's no cue that
goes into that. As Victor mentioned, there
is an issue that it may not be theoretically justified in the settings
we're looking at. It's probably valid, but
the theory isn't there yet. The practical issue is that when we're doing
cross-validation, I'm trying to do it right, which means we're cross-validating
the entire procedure. Which means we're
iteratively estimating the penalty loadings inside each of the
cross-validation steps. It takes quite a while, as opposed to the simple
homoscedastic case Matt Tutty talked
about yesterday, where cross-validation is
actually really simple. The iterative estimation of the penalty loading slows
the thing down by maybe a factor of 10.
It's not trivial. I'm going to report
the results with both. What you're going to
see is you can't tell the difference or I shouldn't say, you can
tell the difference. It's not clear they're
economically meaningful. The other thing that I think is important
to mention now, the cross-validation I'm using, Matt mentioned two rules for choosing the
cross-validation parameter. One is the obvious, just choose the minimum value of
the cross-validation. The other is choose
a value which is one standard error away from the minimum, the
cross-validation. Because what we're actually
doing here is cross validating post lasso that in and of itself is
less stable than the generic very simple plug-in
homoscedastic version. Going off by one standard
error actually really helps with the stability of the
cross-validation here. That's F range rule of thumb. Matt mentioned that
that's the rule of thumb I'm going to use. It does seem to look
better in terms of the stability of the
things you're choosing. With all of that out of the way the other really
important choice, which is unavoidable is we still have to specify a functional
form of search over. We might not want to,
but we've got to do it. I would like to
be very flexible. Using a lasso gives us the
ability to be very flexible. First of all, I'm going to use five dummy variables for the obvious
categorical variables. I did all of this in Stata. The code will be available
sometime if it's not already. Because I'm lazy, I've made fifth-degree polynomial
and family and schooling and
third-degree polynomial in family size using
status worth poly command, I actually don't know
which orthogonal polynomial state he uses because it's not in the documentation
that I had available, but some orthogonal polynomial. That's what it is. One is a fifth-degree
orthogonal polynomial, one is a third-degree
orthogonal polynomial. I thought the effect of
age might be important in non-linear for reasons
that we talked about, so I put it in a cubic spline with 10 equally
spaced not points. That's 30 total terms and age. I put it in a cubic spline with 15 equally spaced not points. That's 45 terms to
control for income. Now, since I was worried that there might
be interactions, I took those five
dummy variables and interacted them
with everything. That gives me 190
interaction terms with schooling,
family size, and age. I then took those and interact with them because remember, income was the variable that we were really
worried about. I took those 190
interaction terms and interact with them fully with my 45
terms in income. That gave me a total of 10,485 terms to
control for income. Now, you might argue that 10,485 terms is insufficient
to control for income. Maybe it is. I'll never
know, neither will you. But it seems pretty flexible. That's what we're going to do. I have to admit after
doing this and looking at the distributions of some of these things which I
should have done ex-ante. But again, I'm lazy and
the econometrician mainly, so I don't think
about this stuff, but I shouldn't have used
equally spaced not points. I should have used 15
unequally spaced not points, but whatever, same idea. When you look at
this, that gives me a total of 10,763 variables. Now, of course, you all know that that's insane given that I have
9,900 observations. The goal here is
to convince myself that whenever functional
form I need to control for, for income, I can get it. If I can approximate
it with this, it's true that I'm in trouble but they seems pretty flexible. It's important to note this is a choice you have to make. Maybe I should have put
in 20,000 variables. I don't know. Neither
does anyone else. As we increase the
number of variables, the ability to pick out the
true signal gets harder. We want to try to
think about things. This is sensible. This is giving us an
overarching picture, or do I really think I
need all these terms? Probably not. That
goes back to sparsity. The final assumption we
need is we need to believe that among these
10,763 variables, there is a sparse representation that captures the
effect of income. What does that mean? That means note that
the loadings on five of these
variables are non-zero and the rest of them
are exactly zero. We're allowing for
approximate sparsity. It means that the loadings on a bunch of these things are big enough. We can pick them out. Balloons on the rest
of them approach zero in a way that ignoring that
part of the approximation, just as with traditional
non-parametric, isn't going to be a huge bias on the resulting estimates. Again, this is subject to
debate as is everything. Is a 15s not spline, cubic spline, and
income interacted with all this self-sufficient
control for income? I don't know, but I am
personally willing to believe that whatever
effect I need to control for income is
captured pretty well by some low-dimensional
representation within this space of things. That's a fundamental
belief. I have to believe that to use the
sparsity-based methods. Maybe there are other
methods that would work. Sparsity-based
high-dimensional methods. I have to believe that. That's my belief.
Now, given this, we of course know
if within my space of 10,763 possible functions, I actually thought I
need to control for every single one of
them, I would be done. I could not possibly learn
the effect of a 401(k) eligibility from
9,900 observations controlling for 11,000 things. It's impossible. By the way, if someone from Stata ever
watches these videos, you need to increase the
math size beyond 11,000. The reason that this
is cut at 10,763, I originally tried
a higher-order, not a fifth-order polynomial, but a sixth-order polynomial, and that put me over 11,000 variables and
stated didn't like that. Anyway, that's an aside,
but that would be nice. We've got our 11,000
variables which is generated by the fact that I'm using
Stata instead of MATLAB, or R, or something
else at this point, that's what we're
going to search over. Now, we've got that, we're going to do the selection, we'll get some results out, this doesn't matter but it
might help make some of these coefficients
or the variables I selected make more sense. I normalized all of the
income and age to be on the 0,1 interval just
because it made sense to me. When you say c
income minus 0.33, that's not income minus 0.33, that's normalized
income subtract 1/3 of the way across the scale. When I look at what
variables do I select in the propensity
score equation, their equation to predict
401(k) eligibility, given my set of controls, I found this interesting. I get exactly the same
set of variables whether I use cross-validation
or a plug-in rule. Those give me identical
sets of variables, which is coincidental I admit, but I was happy about that. I only have to present
one set of results. The set of variables
you can see there, this might be surprising, it's not that surprising. Simplest functional forms, Teams seemed to do a pretty
good job. What do we have? We have the linear
term and income, so we've just got a
linear effect with a little kink for high income. We then have some stuff
interacted with income, we have a cubic and age, which if you look
at that is flat and then there's
something funny. If you actually
plot these things they're not ridiculous, but those are the
variables you choose. Not surprisingly, given that we were
worried about income, the variables you
choose are related either to income, or to age, or things like whether
you own a home which is obviously a proxy
for something about income, whether you have
an IRA, which is a proxy for something
about income, so we're choosing variables that correspond to our intuition. Now, what are we going to
do? We're not done yet. This is just the first step
of our double selection, so we're going to take these
variables, set them aside, and then we're going to estimate predictive relationships
for either total wealth or net financial assets. That's on this slide. Again, interestingly enough, in this example cross-validation
and the plug-in penalty give you identical coefficients, I shouldn't say
identical coefficients, they give you identical
selected variables for either net financial
assets or total wealth. The variables I
won't go through, but they're still
parsimonious functional forms mainly in income and age, and once you condition
for all this, now we've got a different
set of variables, the size of this set of
variables is remarkably close to the original size of
the set of variables chosen by the totals
of NTYs, they chose, like I said, 15 or 16 variables, we ended up with between
15 or 17 variables, so we're in the same ballpark. I have a tremendous amount of respect for those guys and I'm not surprised that
they basically controlled for what you
needed to control for. If you look at the effects, the effects are somewhat attenuated relative to
the original results. Why? Well, maybe these
things are actually soaking out more of the income effect
that was in the residuals, because this functional form wasn't capturing
some of these kinks, and especially the interactions. Then you could go
back and if you're [inaudible] you could
argue that that means really what's going
on is there's omitted preferences for saving and we just showed that you're
picking some of that up, so maybe you haven't
picked all of it up yet, and maybe there's a
problem and we should go back and think more
carefully about this. I'm conditioning on that
not being a concern here, but that's a sensible
question you might ask. As you can see, like I said, the effects are
somewhat attenuated relative to the PVW effects, the standard errors
are slightly smaller, there appears to be
a slight increase in efficiency, we
must be so keen. Some additional
residual variation out that they weren't, results are roughly in
line with the PVW results. In terms of big picture, this is something that
I want to get across. Again, I don't think any of us are arguing
one should use high-dimensional results to replace what
you're already doing, I think there's
something reassuring about being able
to go out and say, look, I tried this heuristic, intuitive way to
control for income, I think it's good enough, here are the results, but just in case I missed
something important, I've tried this really
broad set of controls, I've carefully
searched through them, and look, I get roughly
the same answers. I think that's nice. Other people may disagree, that's fine, but I think that's a nice thing that
can go on here. We'll see another example, maybe you'll pick up something
that you didn't initially think. That's these
these results. One other thing we can
do in this application, the treatment effect or
the treatment variable here is 0,1, so we can talk about
what Victor did. The previous model was a nice additively separable model, it's this partially on your
framework, very simple, instead of doing that we
could actually go out and allow for a fully
heterogeneous model, if you want to you
can embed this in the usual potential
outcome framework and we could talk about that, you can estimate the
average treatment effect allowing for fully heterogeneous
treatment effects. Importantly, despite the fact we are allowing for fully heterogeneous
treatment effects, we are still estimating
one parameter. There is no selection
over that parameter, we have the thing we
want to learn about, we're going to go out and try to control for the other stuff, those are nuisance functions, but there is no selection over this average
treatment effect. We have our target, our
target is low-dimensional. The examples I'm
going to give today the target is one-dimensional, it doesn't have to be
one, it could be two, or three, or five, but it's some fixed low-dimensional thing that you want to learn about. Now, our framework then we have our parentheses
score equation which is exactly the first stage
from before where we regress eligibility on xs. Our outcome equation
allowing for fully heterogeneous effects can be represented as Victor did, where we take the
treatment variable and interact it with two
different functions of x, we allow those functions
to be different. That's our g1 of x
and our go of x. Now what we're going to
do is we're going to take Han's moment condition, use that to estimate the
average treatment effect. At this point the inputs
are roughly the same, so I can go through this
without all the setup, as they were before, we want to estimate
the function m, which we've already done, we want to estimate the
functions g0 and g1, the way we're going to do that, we're going to take
all of our data, we're going to run
variable selection, we're going to get
the things that look like they predict g1, we're going to get the things that look like they predict g0, we're going to get
the things that look like they predict m, we're going to put
all those together, form some estimates of averages, plug them into my little
formula for Alpha hat, and it's pretty straightforward. Again, there's still a code for this that will
eventually be posted, but this is just a fairly
straightforward exercise. We already did estimates of m, to do our estimates for the two different g functions we need penalty parameters, rather than redo the
entire CV exercise I cheated and took CV from the full exercise
and scaled it, so that corresponds to
the right number of observations for each
of the g0 and g1 parts. Nothing says I
couldn't have done full cross-validation again or I couldn't have used
the plug-in here, but that was a nice
simple thing to do. I'm not going to talk about
all these variables again, but you select some variables in each of those equations, they look very similar to the variables you
selected before, it's not that surprising. The point estimates are very similar to what
they were before. I probably shouldn't say
this on camera but I will, partially estimates from
partially linear models versus full
heterogeneous estimates of average treatment effects, they tend to look pretty
similar, and they do here. They're pretty similar, the
story is exactly the same, standard errors
are very similar. That's Example 1, this is a straight
application of what Victor talked about with the
double selection, there's nothing more
fancy than that here. To reiterate, there are a
bunch of choices that go in, hopefully I'm
conveying the thought that while you can be flexible, you can't be
arbitrarily flexible. But you should still
use economics to guide your intuition about where
these functions are, what are the sources
of endogeneity, how am I going to construct controls to get out those
sources of endogeneity, and really account for that. If I tried to do everything, I would learn that I
couldn't do anything. I don't even know how
long I have to talk, but I saw a question. I
know I am not supposed to take questions. FEMALE_1: [BACKGROUND]. Christian Hansen: Yeah. MALE_1: You are to
repeat the question. Christian Hansen: Yeah,
thank you. The question was, when you do these, and this is absolutely true and you do
these fully automate things, they are fully
automated and variables are going to pop out
like homeowner times, age, without homeowner
in the model or age in the model or homeowner times h squared without
homeowner by itself, age by itself, h
squared by itself, that can absolutely happen. If you look at Victor's
picture from before the break, maybe you didn't see that, that corresponds to there being a really sharp discontinuity but the fact that this function, it's zero across here and then there's something
weirdly out here. Do we want that? If I saw these, so maybe the question could also be rephrased
if I saw this and I really wanted to put in
homeowner and age as well, would that mess anything up? As long as homeowner and age, and age squared are
only three variables, the answer is no. Go ahead throw those in, if the results changed
dramatically because you through those three variables
and then something broke here because
that shouldn't happen, we chose the things that we're going to make things change a lot but if it makes you feel better to have
those in the model, as long as the number of them is small then that's really
not going to affect things. Now the problem would be if
we chose night in this case, but we chose homeowner times h^100 and we didn't have any
of the previous variables. Well, there's question whether we should have been
considering that, and that goes back to how many things you really
want to consider, but when you throw
99 more variables in that might be
enough variables that you would
actually break things. But if you're throwing
just a couple of additional variables
then that's not going to matter or the theory says
it shouldn't matter, in practice, if it does matter,
then something's wrong. Either you broke
something when you did the estimation or one
of these assumption we're making is just terribly violated and I didn't do that, but it would actually be worth considering, I don t
know what would happen, but my strong belief given that we've chosen the
variables which are meant to drive changes is that
things wouldn't be altered by adding those a couple of additional things,
that's a great question. With that question done, let's look at example 2. Here's an example, I am fairly sure most people
are familiar with. This is trying to understand the effect of abortion
rates on crime rates. The idea is the title suggests is that's the
causal question of interest. Is there a causal link between abortion rates and
future crime rates? I'm not sure I define this
carefully in the paper, but this is the effective
abortion rate related to the crime rate and
that means this is a weighted average of
abortion rates like 18, 19, 20, 21, 22, whatever years in the past. The timing here is lined
up, so it's not ridiculous. I'm not going to make that explicit at any
point beyond this, but the timing here
actually makes sense. We wanted to estimate, is there an effect of
abortion on crime? Well, we all at least are going to believe that maybe abortion rates are
not randomly assigned. What we'd like to
do is compost story where we think that the variation in
abortion conditional on stuff can be taken as exogenous. Now, I said I'm going to
just adopt the formulation from the Levitt
and Donahue paper. I believe because they did
that if I were able to condition on the right set of state-level characteristics, abortion rates could be
taken as randomly assigned relative to the
observables driving crime. Now, again, I think when you're thinking about these
high-dimensional methods, it's important to keep in
mind what the sources of indoctrinating might be because you can't control
for everything. What are the key things that might be driving
indoctrinating here? The big ones states
are just different for lots of reasons and most of these reasons
we don't observe. It may well be the case that crime rates in
different states evolve differently than
crime rates in other states, abortion rates in
different states evolve differently than an abortion rates
and other states. Those two evolution patterns are related to each other
but they're related to each other only
because there is some other unobserved state
specific characteristic that's driving
those two patterns. If Matt Tutty were here
he would throw out one example which
she gave to me, and which is a cell phone usage. If you actually go out
and plot these things, cell phone usage
patterns look a lot like abortion rates, in that they start
off near zero for a long time and then
cell phones get invented but they stuck for a while
so they tick up a little bit and then they get
good and they jump up and then they level off,
everybody has a cell phone. If you actually go and
plot abortion rates, that's where the abortion
rates look like. There's one unobserved factor that seems like it might be
related to abortion rates and could be related
to crime rates in that maybe as people get more
access to cell phones, they call the police more and that relates to lower crime, I don t know, that's the story. What are we worried about then, we're worried about things
that are different across states that may be related to the evolution of abortion rates and the evolution
of crime rates. Those are the key sources of confounds and
I'm worried about. If you sit down and think about your favorite stories for
confounds in this example, most of them will fit
in the category of unobserved state
specific stuff that might be related to
crime evolutions and abortion rate evolutions, there's our world, that's
what we're worried about. The Donahue Levitt
baseline model is the baseline model I think everyone would estimate
in this scenario, it's a difference
in differences are fixed effects style
specification. We have the crime rate
on the left-hand side, we have the abortion rate
on the right-hand side, that's the variable of interest. One variable again, we want to learn the effect
of abortion on crime, we're worried that we need
to control for stuff. We control for a set of time-varying,
state-level variables, those are mentioned
in the slides below, I'll talk briefly about
those in a second. Importantly, as we all know, we control for
aggregate time effects, we control for aggregate
state effects, and then there's some
leftover stuff that we're hoping is unrelated
to abortion rates. Obviously the state effects
and time effects are meant to capture these aggregate
stories I talked about, cell phone usage
or whatever it is. Whether they do that and we'll maybe talk about
that in a second, that's what they're meant to do. These other variables are hopefully going to soak up
additional variation which is either important for getting a more precise estimate or is related to this confound story
that we're worried about. If you look at this set
of characteristics, most of these look
like things that are more useful for soaking up residual variation and actually addressing the
endogeneity concern. In my opinion, in that most
of them are things like the number of
policemen last year, as opposed to the number of
policemen 18 years ago or something but we can argue about that
doesn't really matter. Those are the variables that Donahue and Levitt used,
you can see them up there. Here are some baseline
results, the first column, the one that says D L Table 4 is directly copied from
the Donahue Levitt paper. There's no estimation of mine at all going
on at that stage, just so you know what that is, that is not exactly
fixed effects, that is a JLS version
of fixed effects. They use [inaudible]
JLS standard error is when the report
those standard errors, and we could talk about that some other time
but that's what those are. The second column are
estimates obtained by me, those are just running
first differences of the specification
on the previous slide. First difference is to
remove the fixed effects and because I happen to prefer first differences
to fixed effects, but that's again a
topic for another day. The thing I want you
to get from this is you would probably reject a specification test that these coefficients
are the same, economically, you're
getting the same story. I'm going to work with the
first differences from now on because
that's what I like. Interpreting these
things is causal in either event relies
on the belief that any omitted variable that is related to both the evolution of crime rates and the evolution of abortion rates is
perfectly captured, at least to the extent that correlates to these
two things by an average difference between states and an
aggregate time trend. We're not allowing
for the fact that maybe there's something
that's specific to California that means that trend in California relates
some unobservable in California and that relates
to differential trends in California as opposed to Arizona for reasons
that aren't observe, those of you who do diff-in-diff or this style analysis
would probably say, oh, well let's go
out and throwing state-specific
linear time trends. Shouldn't that soak
up that maybe there's an additional state
specific trends we need control form. Donahue love it do that, that adds an additional
50 variables. I don't report the results here, again, I don't for a
variety of reasons. Importantly, when you control for states, for time trends, you can't learn anything in
this data which might be an indication that you should worry but more importantly, relative to what they say is that's a lot of
degrees of freedom, that's very flexible, maybe that's overkill, maybe we don't actually want to control for state-specific
time trends, maybe we want to be a little more parsimonious than that. Again, for better or worse
that's the starting point. Now, what do I want to do? Again, I'm worried about
unobserved factors that are related to evolution of crime rates and evolution
of abortion rates. I can model that as an
arbitrarily flexible trend, which is what I've done up here, you clearly hopefully
you recognize you cannot estimate the, "model on the board here." What that model says is
that the outcome at time t is a function of all the possible values
of all the regressors, state and time, and that that
function can differ arbitrarily across every state
and in every time period. The only way to estimate
this thing would be to put in a state cross
time fixed effects, which we all know will absorb all the degrees of freedom, we couldn't learn about this. My model then says I
have the same structure, maybe it's a different
function but have the same structure
for abortion rates. I'm worried that there's
a correlation between this unobserved latent trend, which I've called g, and this
unobserved latent trend, which I've called it
l. Now, as I said, if I'm worried
about these things being arbitrarily flexible, I can't learn anything, I might believe I could learn
something from this data. Rather than be
arbitrarily flexible, let's impose some parsimony, we're going to do that
via dimension reduction. What is our baseline? Our baseline says
that functional form g is the linear function plus fixed effects for state and time that
I just mentioned, that's a nice baseline. Let's see, what else
do I have on here? The other thing which just to go back to the theme in terms of
intuitive dimension reduction. This is the baseline, the intuitive
dimension reduction Levitt and Donahue used. One is a functional
form assumption. The other intuitive dimension
reduction they used is, they said of all the possible
state level macro series that you could see, eight of them are the
ones we care about. Those eight happen
to be the ones that showed up in their model. Why those eight show up? Again, I assume
there's a really good, actually, I should say, if
you look at those eight, they actually make
a lot of sense. That's where they come from. But maybe there are
other ones out there. Again, because I'm too lazy
to collect my own data, because I'm largely
an econometrician. I'm going to pretend
that those are the right eight variables. In principle, you could think about doing variable selection, allowing for more state-level macro series to
actually be part of the model. I'm not
going to do that. Now, what I care a lot about are these trends
because in my mind, and maybe not in your
mind, but in my mind, the possibility of
correlated trends is the thing that I think is most likely to be driving an endogeneity story here that's not just absorbed
by a fixed effect. I'm going to go out and
I'm going to try to do a good job modeling these trends.
Here's my "model. " I've already done
the approximation of those trends by
linear functions. If you're wondering
where M and G went. M and G are now wit
prime Pi 1 plus Gamma t and wit prime Pi 2 plus
Kappa T. Those are my trends. If my wit's were the differences in the
original excitation, this is exactly the first different specification
I already estimated. Now, importantly, I think time effects
are actually important. I think there might
be in macro trends, so I'm not going to do any
variable selection over that. I'm going to put the macro
trend in every model. This goes back to the
question I was asked, do I to do selection over everything or can
I add additional stuff? The answer is as long
as the additional stuff is not huge dimensional, sure. In this case, the
additional stuff is like 10 additional
dummy variables. That's not huge
dimensional. I'm just going to put the 10 dummies. The other thing I'm doing,
I'm taking first differences. I'm saying, I know is state specific
differences are important, so I'm just allowing all those fixed effects
to be in the model. They're all in there because
I differenced them out. Now what I want to
ask is there are other features of this trend, maybe nonlinear features of the trend that I didn't account for with
this specification. As an aside and part
of my soapbox here, given that I do have a
soapbox for a second, linear trends for lots of things might be reasonable
approximations, but they're silly. What's a linear trend
in crime, for example, saying over a period of 13
years that same crime rates are increasing over 13 years or decreasing over 13 years. I actually think
that first of all, that can't possibly be
a global approximation. Crime rates do not
linearly blow up, and it's questionable whether that's even a good
local approximation. What I want to do is
allow for something more general than
a linear trend. I want to allow for maybe the fact that crime
rates might go up and taper off and they might
do something with me. At the same time, I don't
want a fully flexible, non-linear trend
because that absorbs everything. What
am I going to do? I'm going to specify a
flexible non-linear trend and use variable
selection to tell me, are there important pieces to this flexible
non-linear trend that I missed when I just
had level differences and a single macro trend. What are the variables
I'm searching on? I have my eight original
controls in differences. I have initial conditions in all of the controls
in the abortion rate. I actually don't have the initial condition
in the crime rate, I should cross that
one off the slide. I didn't put the initial
condition in crime. I just have the
initial conditions of stuff that shows up on the right-hand side
of that equation. I could put it in the
additional condition in crime, but I didn't, that's a typo. I also have within state
averages of all the controls, within state average
of the abortion rate. I have t and t squared. That's allowing for
some non-linear trend. Then the important part is I'm interacting all of those
variables in one through three. Importantly, the
initial conditions with t and t squared. What is this model doing if
you went back to levels? This is a model with a
flexible cubic trend. If you integrate this
back from differences to levels to model with
flexible cubic trend, where the way the trend
evolves can depend on say, the initial abortion rate. Maybe states that had initial low abortion
rates might have had trends that were different for a variety of reasons
that we don't observe, and maybe that's
actually important. That's my justification for where this set of
variables came from. Again, it's being driven by me as the researcher and
you can disagree with me, but by me as the researcher
sitting down saying, where do I think
omitted variables bias is going to be coming from. How can I construct
functions that will capture those important
features of variables bias. My idea here is I want a nice non-linear trend to
capture the fact that crime rates might
not blow up and abortion rates probably
don't blow up either, to approximate this relationship over a period of 13 years. When I put all
these variables in, I have 284 variables, 576 observations. Now, if I want to, I can actually run
the regression outcomes on everything here. I've got enough degrees
of freedom I can do that. Here are the results. These results shouldn't
surprise anyone. If I have 576
variable observations and I think I need 300 controls, it's going to be really
hard to learn anything. There are my estimated effects. The estimated standard
error is roughly say, the effect of
abortion is between absurdly large and
absurdly small. That's probably true. The effect of abortion
probably is somewhere between so big it's implausible and so negative it's implausible. But we might think we
could do better than that, so probably what's going
on is we've got a lot of these 284 variables that
are actually important. What assumption says that? That's the approximate
sparsity assumption. I don't actually need a really flexible
third-order trend that varies arbitrarily across all these individual
characteristics to capture the important
features of the state. I'm going to do
variable selection. I'm going to choose
variables again using LASSO, using either 10-Fold
Cross-Validation or the plug-in value. I'm going to use
penalty loadings estimated via the
iterative procedure. Here in the abortion equation; There are three at
crime outcomes that you saw in the previous slides.
I'm going to break that out. Violent crime, murder and property crime when
I show the results. The results are the same
when you look at it. In the abortion equation, this is the variable to predict the right-hand side variable of interests given the
plausible controls. We choose the same
number of variables, 11 with the same identities whether we use cross-validation
or the plug-in method. Those variables are up here. Some of the variables
like lag prisoners, lag police, lag unemployment. Those are the things that
showed up in Levitt's model. Others of these variables, I think are more interesting. There's a non-linear trend that seems to actually be really important here. What do we have? We have income times time, we have bear
consumption times time, we have income times time. We have the initial
abortion rate. If you integrate all that up, that's a quadratic trend. That depends on some of these initial state
level characteristics, and importantly, the
initial abortion rate. For whatever reason, these things seem like they do a good job predicting abortion. We actually do get
slight differences for cross-validation
versus plug-in when we look at the crime
equation itself. When you use cross-validation, you select no variables, in the crime equation we
select two variables. Those two variables, again correspond to a
non-linear trends. There are some quadratic
trend going on there. I'm going to fly, I'm not going to talk
too much about this. You can do the same thing
for property crime. The same sets of
variables are chosen. In this case, this is the biggest difference between
cross validation and the plug-in estimates is
in this equation. We'll see what that does to the results in just a second. The sets of variables though, corresponds again
to what appears to be a non-linear trend that interacts within baseline state level
characteristics. All right. Then murder rate we get the same things. Here's the results. This is just the full table
of everything. I want to focus on the last
two rows at this point. Post-DS CV is the double
selection method, selecting over this
flexible trend using the cross-validated
penalty level. Post-DS plug-in is the
double selection method using the plug-in penalty level. In no case do we select exactly the same variables
in both equations. In most cases they're similar. The point estimates are, except for violent crime,
very very similar. For violent crime, the
stories are the same, although again, you might reject that the coefficients
are the same here. The standard errors are big. Some comments about
this. First of all, the results we're getting
are, I would argue, economically different
from the results using just your intuition
to choose the variables, saying, everything is captured by the level
on the aggregate trend. The point estimates are similar, the standard errors are way bigger. What's that telling us? It's telling us that if we allowed for the fact
that there might be a non-linear trend that depends on initial
characteristics, maybe California trends
differently from Arizona for unobserved reasons that happened to be
related to whatever was happening in California in 1987, then we can't tell whether the abortion rate is what's
causing crime to change, or whether it's this
other unobserved thing that's out there that's
just these trends. We can't tell. Pointers for the same standard errors
are big enough that we would probably draw
different conclusions. I would draw different
conclusions. I would draw the
conclusion that I can't tell what's
going on here. There is no strong
evidence that if I think states might
be different based on unobserved trends that I can distinguish the
effect of abortion from those latent trends. In terms of, I think
sanity checks for myself, Foote and Goetz in
a couple of papers, but I've cited 2008 one, raised the identical
concern based on economic arguments
and noting that if you control for the crime rate before abortion could
have had any impact, so if you just put
in the initial level of crime in late 1970, that the abortion coefficient becomes indistinguishable
from zero. If you look at what we're
doing, that's actually the same conclusion
we're drawing. We can't tell what the
abortion coefficient is, whether it's zero or not, once we allow for trends that change depending
on initial conditions. For me, I like the fact that an actual economist
who thought carefully about the problem raised
the same concern. We get the same, gee, maybe you should worry about
unobserved state trends concern through a data
mining technique, which maybe should,
maybe shouldn't, but that should complement
the way we think about these economic
analysis, I think. The other thing to note in
terms of what's going on, all of the action is in
the abortion equation. If you look back say
at the murder rate, we're choosing these variables
in the abortion equation. We're not choosing anything
in the crime equation. That's almost true in
the property equation. That's almost true in the
violent crime equation. If I really wanted
to believe say the first different
scroll, maybe I could. What would I need
to believe, I would need to believe that the coefficients on
all these variables that we're selecting
in the abortion rate equation are
identically zero. Maybe they are, I don't know. Maybe those coefficients in the crime equation
are identically equal to zero so there's no omitted variables bias if I just drop them from the model. In that case, the fact that
I chose some variables that forecast abortion doesn't matter in terms of drawing
my conclusion. We can argue about whether
that's true or not. What we're doing with this double selection
procedure is guarding against the fact
that maybe abortion is well predicted by a trend times the
initial condition in abortion and maybe
that actually has some small amount of predictive
power for crime rates. If that's true, we
need to control for the reasons Victor talked
about, for that variable. Once we control for
that variable, again, we can't tell whether this estimated coefficient
is being driven by the Donahue story or whether it's some
other unobserved state specific trend factor. I will stop elaborating that point and move on
to the next example. How long do they have? Sorry. MALE_1: They have 30 minutes. Christian Hansen: They
have 30 minutes left. Next two examples I think
are a little bit shorter. Example 3 is the
Acemoglu, Johnson, and Robinson paper that looks at the effect of
institutions on growth. The baseline equation we
think we have GDP per capita depends on some measure of
institutions which we've chosen to measure as protection from
expropriation risks, and then maybe some other stuff. That's my XI Beta. There's a clear story for
endogeneity here so we might think that
better institutions lead to higher incomes, that seems to make sense, but it also seems
to make sense that places that have
higher incomes may actually cause having them
have better institutions. You've got more money, you've got more
time to devote to developing good legal systems, whatever, and so there's a
reverse causality question. What we want to do
is do something to account for that
possible simultaneity. We're going to use the
same IV strategy as AJR. One thing that you'll
see in just a second, but the reason I
picked this example, this is a nice example.
It's very simple. It doesn't identically
fit into either of the frameworks Victor talked
about but it's super close. In particular, it's going to
be exactly that framework, but we're going to
use two equations. With that set up, what
are we going to do? We need an instrument. The instrument we're
going to use is the instrument AJR chose, which is European
settler mortality, the earliest visual available
estimate of that number. That's just drawn exactly
from their paper. Why should this work? Their argument is
in the first stage. Settlers are going to set up better institutions in places that they might want to live, i.e., where they
aren't going to die. As such, if settler mortality was high someplace they should have bad institutions because the Europeans didn't
want to stay there. Then there's the argument that institutions are
really persistent. Once something gets set up, it's hard to get rid of it. That's their baseline argument. We're going to buy
that. Second bit is why this is excludable? The exclusion restriction
comes from the fact that GDP, it's
pretty persistent. But it's less persistent
in institutions. In particular, GDP right
now is probably far less influenced by
things that determine, say, the development
of institutions hundreds of years ago than by other just
idiosyncratic stuff. Whatever happened
in 1500s should be unrelated to GDP right now, except through the channel of the institutions that
were established in 1500. That's the argument. I think
it's a clever argument. We're going to live with that. MALE_2: [BACKGROUND]. Christian Hansen: Sorry.
I'm killing our cameraman. I lost my trail there. The problem with this story, and AJR recognized this, there may be other problems, but the one they worry about
is of course there are other persistent
factors that might be related to both GDP
and institutions. The story they're
really worried about is on geographic determinism, which is this notion
that it's geography. GDP is just a function
of geography. Geography is of course,
highly persistent. It rarely changes,
and if geography is what's driving GDP growth, it's probably related both to the institutions and to GDP and we need to
control for that. We want to control for geography and then use variation in this mortality variable that is not related to geography. That's going to be the identification that
we're going to rely on to try to separate
these stories out. The baseline AJR results
control linearly for latitude. That's a sensible idea. Just how far you are
from the equator. I should be clear;
latitude here is normalized to be between zero and one and its distance from the equator and
it's always positive. It's between zero and one. They're going to control
linearly for latitude. They also do a bunch
of specification stuff where they include interactions, they include continent
dummies, and other things. Again as the econometrician, when I look at some of
their robustness checks, I get a little worried because the first stage has
started to get really weak when you control for some of these other
functions of geography. Maybe it's hard to learn about the effect of institutions when you try to fix geography, maybe it is, maybe it
isn't, but one might worry. Rather than try to
control for everything, let's hope that there might be a parsimonious function of
geography that we can use. Again, to fix ideas to
give us some numbers to talk about or for
me to talk about; their baseline estimates which control linearly for latitude, you have a pretty
decent first stage. The coefficient is
around minus 0.5, standard error is about 0.15. Your t-stat is whatever, three-ish, and your f-
stat is around nine. It's borderline weak instruments using the usual rules of thumb, but let's say that's good. Then the estimated
effect of institutions on GDP is a big number. If you look at the units and these are all logged anyway, 0.96 with a pretty
big standard error. That's a big number.
It looks like institutions are
pretty important. I want to do the same thing. I want to allow ourselves to control flexibly for geography, but I don't want to
limit myself and say, I know the way to control for geography is by
latitude linearly. Maybe it's something
more complicated. It's actually not going to be. That's maybe a thing
that comes out to about. Let's at least worry about that and think about
what's going on. We have three equations now. First equation is the structural
equation of interest, where we have GDP as a function of institutions and geography. X here is geography
d is institutions. We have a first-stage equation. First stage says,
institutions are a function of mortality
and geography. We have a third equation, what's that third equation? This is the reason we have
to control for geography, third equation says mortality is a function of geography. If mortality were not a
function of geography, then we wouldn't
need to control for geography would just run IV using mortality and go home. We now have three equations, they're very simple equations, but that's the
framework we're in. Our belief is that
if we were able to put enough stuff about
geography in that vector x, the instrument would
be a valid instrument. What do we do? We take the
system of three equations and turn them into reduced forms.
Why are we doing that? We can use variable
selection methods to learn about reduced
form relationships, we then have our structure, we can figure out what that
means about the structure. Our three reduced forms, we have the regression
of GDP on geography, we have the regression of
institutions on geography, we have the regression of
mortality on geography. How are we going to work this? We're going to do three
variable selection steps, we're going to take the union of the variables chosen in each
of those selection steps, use those as controls when we
run our IV, pretty simple. Here we go, my choice again, I didn't state this because it's hopefully implicit by now. I'm using Lasso, I'm doing two versions
of penalty selection, and I'm using iterative
methods to allow for heteroscedasticity in
those penalty loadings. I need a flexible function over geography because that's
what I'm worried about, I could try something
more flexible, but given that I only have 60 some odd observations here, my flexible functional geography
is pretty simple still. I have continent dummies, I have a cubic
spline in latitude, so there's my set of things that might be
driving the results. Again, the number of variables, if you count them here is
smaller than the number of observations, I can run this, just throwing everything
in and if I do that, I end up with no first-stage, so my first-stage point
estimate is minus 0.21, my first stage standard
error is positive 0.22. Any sensible rule of
thumb would say that probably isn't going
to work and we can, of course construct
the IV estimator. Even if you took the IV
estimator at face value, you would probably conclude you can't learn anything here, again if you take
that standard error 0.7 and multiply that by two, that says the effect
of institutions on GDP is between minus 600 percent and positive 900
percent or something like that which it probably
is again, but. MALE_3: Does it actually
say that the effect that was directly. Christian Hansen: Yes, which
is what we're trying to do. Remember, that's what
our goal here is, we want to know
what is the causal effect of institutions, sorry, thank you
restate the question. The question was,
isn't this telling us what we can learn about the effect of institutions not
mediated by geography? The answer is exactly, that is exactly what
this is telling us and that's the causal
question of interest to us, we would like to be able
to actually say, no, this is due to institutions. We don't want to be able say,
well here's an effect and this effect could
be institutions or it could just be geography, we want to actually
be able to say no, good institutions
actually are good. We can't do that unless we mediate or get rid of
the effect of geography. Now it might be impossible
to learn about the effect of institutions getting rid of
the effect of geography, because maybe institutions are so highly correlate
with geography, this is just an impossible
thing to decompose. That might be the
case, that's one way to interpret these results. But the goal is to
try to disentangle those two effects and if
that's not your goal, then of course you would
do something else. That's a fair question, but yes, that is the goal stated in their paper
anyway and as such, that's the goal of
my two example here. Anyway, that looks like either we're over
controlling for things or we can't learn
about the effect of institutions once we
control for geography, how might we try to
get a handle on this? We might try to ask, well, do we actually need
to control for this pretty flexible
function of geography? Again, same deal, we do lasso, so we choose variables in these three equations,
once again, you get identical
variables regardless of which method of choosing the lasso penalty
parameter you use. Only one variable pops
out in all three of the equations is the
same variable which is nice in that it means we
only have to control for one thing and all of you who look at this
will probably say, well, of course that's
the main variable. The variable that
pops out is Africa. Once you control for Africa, it looks like the
rest of the stuff going on in geography, you can't tell
whether there's any predictive power leftover, so whatever is driving
geography is Africa, so let's go out and
control for Africa. Here are our results, the
three sets of results again. As you can see, the first
stage coefficient is similar, it's a little bit attenuated. The standard error is a
little bit bigger than when you control
just for latitude, but they're in the
same ballpark. The coefficient estimate is attenuated and the
standard error is a little bit smaller. I say things on camera that
I probably shouldn't say. Given that the initial
estimated effect was implausibly large, it's nice that this estimated effect is
slightly smaller, it's probably still
implausibly large. But it looks like if we're
willing to buy the sparsity, which we might, may or may
not be willing to buy, but if we're willing
to buy the sparsity in that there may be a complicated
relationship between GDP and all these other
variables and geography. But that complicated
effect is well approximated by
some small number of the things I've
written down up here. We're going to tell almost exactly the same story as the original Acemoglu,
Johnson and Robinson paper. We're not going to change our fundamental economic
story very much. Institutions appear to
matter even after you take out the main
geographic factor. Then the final example
that I want to talk about is a straight-up many
instrument IV example. As Victor said, this is probably the simplest application of
variable selection things, the things economists
might want to do. In this case, what we want
to do is try to estimate the consequences
of eminent domain on some economically
interesting outcome, eminent domain is, if you don't know,
it's just the jargon for when the government
sees is private property, so the government says we
want your property to build a highway or whatever and
because we're the government, we can take it and they do, and then maybe they compensate you, that's something else. We want to see, is
the threat or is the exercise of eminent domain related to some
economic outcome. In our case, we're
going to measure that by judicial decisions
related to eminent domain. We're going to, well, I think I don't have this written down
here, but I'll talk about. In terms of the outcome, there are variety of things
you could look at, this is in the IV
paper with Victor, Daniel Chen and Alex
baloney the main example, we're just going to look
at one little piece from that where we're
going to look at the Case Shiller price index as the economic
outcome of interest, maybe there are other
things, but let's look at the effect on
say, house prices. The way we are going to
measure eminent domain is we're going to measure
the number of times that a circuit court decides
in favor of individuals. In other words, where
the circuit court comes along and
says, you know what? When the government seize your property that was illegal, they shouldn't have
seize your property. We're going to either
give it back or compensate you in some way. That's going to be the way
we measure the exercise of eminent domain can argue about that, but that's
our measurement. Whether or not circuit courts decide in favor of individual property owners
is not randomly assigned, or may not be randomly assigned. Maybe they're looking
at housing markets or economic conditions or
other things when they're deciding how to decide these cases about whether government seizures
were lawful or not. We're worried a little about endogeneity and the way we're
going to get around that in this example is to note that judges are assigned
in the circuit court, which is why we're using
the circuit court, may not be the most direct thing you would care to measure, but it gives us a nice
identification strategy. Judges in the circuit court
are assigned randomly. Wherever judges hear your case, when you go to a circuit court, that's a random decision made
by hopefully a computer, but some random mechanism decides which judges are
going to sit in on the case. Roughly what that means is
that the demographics or any other characteristics of the panel of judges
that sit on your case, isn't instrument for the way
the judges decide the case. Since the judges were randomly
assigned to the case, their characteristics are also randomly assigned to the
case since the judge was randomly scientists
characteristics randomly assigned, and then those characteristics can be used as instruments to guess whether the verdict
was for you or against you. That's the basics
of the argument. The econometric problem
is that we have three judge panels deciding
circuit court cases. We observe all characteristics of the judges on
three judge panels, and so that actually gives us a huge set of
potential instruments. Any combination of the
demographic characteristics of the set of three
judges that sit on your case is a potential
instrument to be used to predict whether the case went for
or against you. As you might, maybe
you believe some of these characteristics may be
more useful than others for actually figuring out the
way judges decide cases. Maybe we don't want to just use all possible combinations of characteristics of three
judge panels, in fact, that's impossible given
the sample sizes, but we might think about
choosing variables that are really useful for predicting the way judges decide cases. There are a few ways
we could do this. One approach you could take
is again to use intuition, so this is a straw man, but we all like straw-man. Something that comes out of the judicial
literature is judges political affiliation
can often be used to figure out the way
they decide cases. One might hypothesize
that Democrats, for example, they are
more pro government. At some sense, they might
be less likely to overturn a taking decision and say no, that taking was illegal, and maybe Republicans
like individual property rights more and so
they're more likely to overturn governments seizures
of private property. Maybe that's an intuitive story and we're going to use
that as the straw man. If you go out and you just say, I'm going to use my intuition. I've got actually thousands of potential characteristics
I could use. I'm going to choose one of them which for in other cases looks like it predicts
judges decisions, I'm going to just use how many Democrats
were on the panel. If you look at that,
there's no first stage. It turns out for this
particular setting that knowing whether
or not there were Democrats on the panel gives you very little
explanatory power and decisions regarding
eminent domain cases. Maybe not surprising,
maybe it isn't, but that is what's in
the data in this case, so that's not going to work. Rather than do that,
let's just treat this as a big variable selection
problem where we're going to try to choose characteristics
of judges that forecast the way they decide
eminent domain cases. There's the many
instrument model up there and I'm not going to worry too much
about all that stuff. We control for some stuff and I'm not going to
worry about that. Important leaf from
my standpoint, we have 42 baseline variables about judge characteristics, and that's already
dimension reduced. We had potentially more than 42 baseline characteristics, but we said here
42 things we think might be useful for
predicting judge decisions. For each of these
characteristics, we know the number of
panels that had 1, 2 or 3 members with
these characteristics. We have a bunch of counts
that say 0,1, 2 or 3. Then because that's actually probably plenty and I
could have stopped there, but because of the goal
of this exercise, say, well maybe linear stuff in those three variables
isn't the right way to do it. Since all these things
are counts of 1, 2 or 3, we can just use
cubics and throw in all cubics and all
these variables and throwing a bunch
of interactions. Why not? We can, in principle, maybe
there's something that pops out, maybe not. What that gives us
is a big set of variables bigger than what
I'm going to talk about. One thing that I think both Jesse and Matt mentioned
as well as Matt Tutty. In many of these cases,
you're going to want to do some preprocessing, so if you look at
that big set of variables I just talked
about constructing, lots of them look like simply based on the
characteristics, the instruments themselves. It's unlikely they're
going to do anything, so we throw away
any variable that has an average of
less than 0.05. If all these things were binary, not saying anything that happens less than five
percent of the time, we're just getting rid of that. We don't think the
things that happen that rarely are going to have
much predictive power. Maybe they do, but we're ex-ante eliminating
those things. We're also eliminating
any variable that has a standard deviation of less than 10 to the
negative sixth. Which as you all might guess, is roughly saying
we're eliminating anything that is perfectly
predicted by a constant, so those are
unlikely to actually have explanatory power. We might as well just before we do any variable selection, get rid of all
that stuff because it's just going to slow computation down and it's
not going to help anyway. We also go through and manually remove some
multicollinearity, so any variables that have pairwise correlation
bigger than 0.99, I just arbitrarily
drop one of them and there is no rhyme or reason
to which one I leave in. I just go through
and the first one that shows up in the data set is the one that gets thrown out. Once you've done all
of those selections, you get 147 instruments. In terms of the IV problem, I think it's important to point out any method of choosing variables that doesn't look
at why the outcome or D, the endogenous variable is fine. If I wanted to randomly
choose instruments, I could do that if I want
to throw instruments away because I don't like how
much variability they have, I can do that because I'm not cheating by looking
at the data first. All these ex-ante
eliminations are perfectly kosher
and they're mainly done for it to ease computation. There's no reason
to carry around a bunch of stuff which
is not going to help. With that, we've got
our 147 instruments, we have 183 observations. We do the same deal we've
been doing all along. When you don't do the selection, you choose just one
variable of those 147 is perhaps not the
most intuitive variable, but we might add the main effect and then it
would be more intuitive. The one variable you choose is having one or more members on the panel with their law degree from a
public university squared. If you actually go
back and don't do all the interactions
and stuff you choose just the main
effect of this variable, it turns out that the
quadratic term actually has more explanatory power
than the main effect. That's whatever it is. The intuition for that variable
ex-post you can justify people who got their law
degrees at public universities, maybe they're poor or maybe they like private property more
and that's why it loads up. You can expose,
justify anything. The argument is, of course, based on the notion that all of these instruments were valid
ex-ante because they were all randomly assigned
and we just want to find some sediments students with
a lot of explanatory power. Relative to the baseline, we have a really
strong first-stage, so our T-statistic
here is about nine, so that's an
F-statistic around 81, as opposed to the
baseline straw-man where the T-statistic is one and the F-statistic is
therefore also one, and we get a fairly precise
positive but small effect of additional pro
plaintiff decisions on the Case-Shiller price index. The magnitude is maybe
a little hard to interpret because I haven't given you any units of any kind, but it is a small effect. Again, if you think
about this variable, it's the number of decisions
in favor of the plaintiff, which is a decision overturning the government seizure
or in other words, that's the decision in favor
of private property rights. It seems like
additional decisions in favor of private
property rights are associated with
higher home prices in the area where
this is going on. If we believe the random
assignment story, we can take this as
a causal effects, so reinforcing private
property rights seems to be associated to appreciation in prices
of private property, and that differs strongly
from the straw man baseline. I tried to push down, but that's obviously my last slide
because it stopped. That means I'm done. I got time for questions, so if there are any questions, if not, I would happily shut up. 