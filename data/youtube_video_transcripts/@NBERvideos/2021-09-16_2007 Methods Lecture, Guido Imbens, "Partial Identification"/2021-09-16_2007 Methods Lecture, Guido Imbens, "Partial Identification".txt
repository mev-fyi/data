Guido Imbens: In this
9th lecture I want to talk about partial
identification. I said this is an area I was personally very skeptical
about it for a long time. I remember first seeing
some of this stuff in, I think 1990 when Manski was beginning to work on this and I didn't really think
much of it at the time. I think in 1998 I had dinner at some point
with Adlima and Manski, and Adlima was
questioning whether this bound stuff as people called it at the time
was really going anywhere. Both of us didn't
seem to think of us and Manski couldn't have convinced us at the time but I think actually
we were probably wrong and I think
now this is actually a very important area. There's been a lot of
interesting work done there. Instead of a lot it starting with work done
by Manski but also very interesting applications
by Tamer and co-author. Considering in
contrast to some of the earlier lectures
where I felt much more confident about what specifically people should
know about these areas. Here I think this is much
more an area that's still very much in flux and so what I want to do instead is just
discuss at some length there some examples
that show how the, so at least in my view, this is a very promising area. Then only towards
the end talk about some specific methods and
issues and you can read the statistical
issues that have come up some of which I think we have some idea how to deal
with but some of it I'm sure we'll see
further developments. I'm assigning points really, traditionally I think people, when they're writing
down statistical models that people looked at
several points identified. Meaning that if we had
a large enough dataset, we would be able to figure
out without any uncertainty what the values are of the
objects of interests there. Another way of saying
that is that you would write down a complete model. Says that in the end, that's a one-to-one link
between the joint distribution of all the observed variables and the parameters
you're interested in. We've just seen that if
you had a model where you couldn't figure out what the values of the parameters, even in infinitely
large samples, there wouldn't be much hope of learning much in finite samples. Finally, what these
recent applications have shown in yellow, this approach has shown
is that even in cases where we can learn exactly what the value of the object of interest
is in large samples, we may still be able to learn a fair amount even
in finite samples. What I want to do in
this lecture is give some examples that
make that point and argue that suggests that we can learn
interesting things. That a lot of case it is
useful to think about relaxing some of the
assumptions that actually make the model point identified. Where these last
few assumptions are often very dubious months don't have much
credibility rather than make those assumptions
to hold your nose, try see what you can
do without making those last couple of assumptions and see
if there's still something we can learn. In a lot of cases, turns out, you can
learn very much, but there are certainly
a lot of cases where you can learn something of interest. Then in the last
part of the lecture, I want to talk a little
bit about inference. Now, they're at a
couple of issues that have come up in the
theoretical literature. First one is, given that
you're not point identified, meaning that in large samples, infinitely large samples,
the best we can hope to find is a set of values that is consistent with the model and
consistent with the data. How should we think
about confidence sets? There's the two views there. One is that even though we
can identify the parameters, we should still look for confidence and confidence sets for the parameters themselves. In the end, even if he can learn what the value
of the parameters, there is some
parameter out there that we're trying to get at. We're trying to construct a confidence set
for that parameter. Just the same way we do an
in point identified models. Second view is that we
should get a confidence set for the set itself. Confidence set that
includes the entire identified set with
some fixed probability. I'm doing quite
understand why that, why that second thing
is of interest, but there's other people
would disagree with that. I'll try and illustrate what the differences
between the two. Why I think we should be interested in
confidence sets for the parameters rather
than for the set itself. Second is, in a lot
of these cases, there's concern about
uniformity of inferences, but what I mean by
that is they're trying to maybe be
interested in constructing a confidence set or
confidence interval. We want to be sure that that
confidence interval does well uniformly over in some parts of the parameter space. In particular,
instead of some of the proposals for confidence
intervals that are out there do badly when the model is in fact
point identified. I'll give some examples
to make it clear, but it turns out they're
actually narrower than the confidence intervals would be in the point
identified case. That clearly doesn't
make any sense, so that suggests that we
should be concerned about uniform coverage of these
confidence intervals. Instead, that's led to
some modifications and some outstanding questions for doing inference in
these settings. The first thing I want to do is just discuss a bunch
of these examples. First on statistical one and sensors and not of
particular substantive interest, but it illustrates a lot
of the issues very well. I'll come back to this example. We will look at it when we're
talking about inference. This is just a
missing data problem. We're interested in
the population mean of some random variable Y. We only observe Y if D is 1, we don't observe Y if D is 0. We always observe the missing
data indicator D. I'm setting p be the
probability of observing Y, so the expected value
of D. mu 1 and mu 0 be the conditional expectation of Y given D is 1 and D is 0. You can write the parameter of interest as theta is p times mu1 plus one minus
p times mu naught. Now, what part is the data informative about
in large samples? We can estimate p consistently, we can estimate
mu1 consistently. There's nothing we can learn about mu naught from the data. If you don't have any
information about mu naught, if you don't have
any information about the distribution of Y, given D is 0, there's nothing we can say about the range of
values for theta. If even if p is very close to 1, if even if there's only a very small amount
of missing data, if Y could be extremely
large and extremely small, then there's no
limit on the values that theta could take. In this case, the whole real
values for theta would be consistent with the data and calling this more or less a little bit
of an exaggeration, but all values of theta would be consistent with the data. Now, suppose in fact, we do have a little
more information. Suppose that Y is binary, then clearly mu naught has
to be between 0 and 1. This actually gives us potentially informative
bounds on theta. Now there's no possible way that theta could be less
than p times mu 1, which corresponds to
all the missing values being equal to 0. It's no possible way it
could be greater than p times mu 1 plus 1 minus p, which corresponds to all
values of mu being equal to 1. Now, this is pretty much the simplest case
of all these problems. But somebody a couple
more comments on this. Here in this case,
these bound are sharp. There's no way we can get better bounds on theta without
additional information. Another way of
saying that is for any value of theta
in this interval, we can find a joint
distribution of Y and W that is consistent with the joint distribution
of the observed data, and consistent with that
particular value of theta. Here, without any
additional information, this is the best we can do. If p is in fact very close to 1, this will be a very
narrow interval. We would learn a lot. But obviously if
p is very small, we wouldn't learn very
much in this case. Now, there's another
way of getting informative bounds
instead of looking at the case where the random
variable itself is bound. Suppose we're not interested
in the mean of Y, but we are interested
in the median of Y, then defining Q_tau to be the tau quantile of the conditional distribution
of Y given D is1, we can find bounds on the median of the marginal
distribution of Y. Upper bound it's the
1 over 2p quantile of the distribution
of Y given D is 1 lower bound is 2p minus
1 over 2p quantile of Y. You get a range of
values consistent with the median by
essentially imputing very large or very small
values for the missing Ys and calculating the median in each of these extreme scenarios. But again, we end up with an interval that depending
on what the data look like, it be more or less informative and especially
if p is very close to 1, this could be a very
narrow interval. You can do that. I'm just
setting this up because I want to build on that in one
of the later examples. Here if p is less than a half, you wouldn't be able to learn
anything about the median without having some bound
on the values of Y. But in general, if p is
greater than a half, you would get a bounded interval irrespective of the
marginal distribution of Y. You can extend that to look at any other quantile and you get the identified sets that is
given on the slide there. You get a potentially
informative, let's say generally formative lower bound and upper bound on the theta quantile of
the distribution of Y. Now, let me move on to a
more substantive example. This is from a paper by
by Manski and Pepper. Manski and Pepper
are interested in estimating the
returns to schooling, so they start with
an individual. They want to allow for
a lot of heterogeneity. It starts with the individual level response function, y sub i of w, where for each value
of W that measures the log earnings that a
particular individual would have received had their level of education being equal to W. What they're going to
be interested in is the average return of moving, randomly selected
individuals are moving the entire population from some schooling level S
to some schooling level T. If we just look at this for the values of S and T
that differ by one. If we assume that the
effect of schooling is constant for every year
and for every individual, this would just be the
regular return to schooling, but here they only allow for heterogeneity in the
level of schooling, as well as heterogeneity
in the returns to additional year of schooling. The object of interest
is these differences, delta S and T. What we observe is actual
years of schooling at Wi and the realized earnings
Yi and the evaluated of Wi. Now, if you're willing to make, in exaggerated generity, your [inaudible] found in
this type of assumption that conditional some
set of covariates, W is independent
of Yi of w so that the level of schooling
that someone gets is unrelated to the unobserved components of their earnings. Then we would just
be able to estimate delta of S and T for all values of S and T. At least if
there's enough support, but I clear this very
strong assumption. This is been the subject
of a huge literature, instead of early
work really tests, couple of years ago it was
a paper by David Card, surveying that the literature, there'll be lots of interesting identification strategies for trying to get
at these questions. About the approach months
can paper take here, is to try to relax this assumption and
just make without relying on additional information that will get you to point
identification, see what we can learn
about delta for S and T. They're arguably much
weaker assumptions. Two of the assumptions
they consider, and the ones that I
want to discuss here. First, they assume that
increasing the level of education does not decrease their earnings at the
individual level. For a particular individual, I, if we increase their
level of education, their earnings may stay the
same or they may go up, but they cannot go down. That on average that holds
for each individual. Second assumption is what they call monotone
treatment selection. If we compare individuals, we choose particular
level of education, we compare them
with individuals, we choose a lower
level of education. If we compare
average earnings at a fixed level of education, it's always going to be
higher for the first, or at least as high in the first group as it
is in the second group. Think about this in terms
of ability bias problems. If you think that
individuals with higher ability choose
higher levels of education, this essentially amounts
to assuming that expected earnings increase in ability. But it's not imposing functional form or
linearity type assumptions. We're assuming that this is dominance relation
between the two groups, that individuals would choose a higher level of education, on average have higher levels of earnings at each
value of education. In both assumptions
you can argue about, suddenly assumption one is
very strong in the way it imposes this at the
individual level. At some level, this is still
not a credible assumption. It's saying that no
individual would be harmed by having more higher
level of education. Clearly, if this takes away from the labor market
experience that is, which is to form
this assumption, is making a claim
that is certainly not credible for all
parts of the population. But at the same time it clearly captures a lot of the
features of why we think simply comparing earnings for individuals with
different levels of education may not give credible estimates
of those returns. What [inaudible] in
paper then do is take these two
assumptions combined with the joint distribution of
earnings and education and see what they can learn about
the returns to education. It's fairly, let's say somewhat messy and not particularly interesting
calculations, showed that in the end, you can get a lower
and upper bound on the expected value of earnings given a particular
level of education. Which is given here on
this on this slide. Then itself is still obviously not particularly informative, but if you apply this to the returns and going from
12 to 16 years of education, they find an upper
bound of 0.397. Now, suited to put
that in perspective, if you divide it by
four and look at the implied average
yearly returns this gives us about 0.1. This actually gives us a somewhat arguably
informative number upper bound on the
returns to education, if you're willing to believe
these two assumptions. Now, I may actually not sure whether
this is in the paper or not. They don't and certainly
don't recall what the lower bound is under
this set of assumptions. If you actually look
at these assumptions, they're very much geared towards being helpful
for upper bounds. I think actually what we call, I've seen these
numbers at some point, the lower bound for this number is unbelievably large negative. You certainly don't get it particularly
tight interval here, but you get a arguably
informative upper bound. The reason for
making a comment is, you do have to keep in
mind here that once, I'm only giving you
the upper bound here, but once you have an interval, you cannot interpret it at all. It's a standard, confidence
interval where you think the midpoint is at some level
the most plausible value. Here, if you take
the midpoint of the interval for delta 12,16, you still get a very
large negative number, which is extremely implausible. Sorry, actually, but
you get zero here. It's under a different set of assumptions that you
get a negative number. Here, it's by assumption. The returns are non-negative. You always get zero. But if the interval itself does not have
the interpretation at the mid point
is plausible here. But the main point of this
example is that under, clearly a very different
set of assumptions, then standard exogeneity of instrumental variables
type of assumptions, we actually get a remarkably
informative upper bound on the returns
to education. Same time, the data here, I'm not informative
about the lower bound, the lower bound just
comes directly from the assumptions that the
earnings go up with education. We get a lower bound of zero. Third example. Here is one of the tamer paper
said this on joint, the ball on array. But looking at here is a problem that was
viewed as somewhat intractable prior to this partial
identification literature. Looking at a problem
instead of at a non-linear panel data model with a binary response variable where the latent index is
linear indirect outcome and as an individual in
a specific effect Alpha and idiosyncratic
error epsilon IT. The interests here is in Gamma. Given this part
of Tamer given we first write down
the equation that characterizes by IT given the past and given
the covariance. Suppose we are also willing to postulate the parametric model for the random effects. It could depend on the axis. It could be independent
of the axis. Then that's still not
enough to identify Gamma the focus here. The only thing
that is missing is the model for the
conditional distribution of the initial condition given all the covariates
and given Alpha I. That's an awkward thing
to specify as it is large literature the discussion
of this in Jeff's book. It's awkward distribution to specify because you'd like to specify it in a
way that coherent, irrespective of the number
of time periods you have, you would like to specify a model that would be the same, irrespective of whether you have three or four time periods. To do that coherent, this is a challenge. People have done a
bunch of things. But here they've
established that in fact, with three time periods, Gamma is not identified here. We can't get a
consistent estimator for Gamma in this case. People have moved to use slightly different
models for example, in some cases you can do fix effect logit type
things with panel data. But here with normality even if you have a parametric
distribution for Alpha, we cannot identify Gamma, which seems very awkward in
a sense because it seems so close to models that
we can identify. Partly what people
did to change things slightly to get back
to identification. But what Haile and Tamer had that let's
try to figure out. But even if we can't
identify Gamma completely, but if we can get ready, we can still learn
something about Gamma. What they do in their paper, they see if they take a particular fairly
flexible distribution. For alpha make a discrete
distribution with a finite known set of support points but
unknown probabilities. What they do then is try
to find the range of values of Gamma consistent with the data-generating
process. They actually just do. They only look at
identified such itself. They don't focus on inference. But what they find is that, say in this case
with t equals three, but it was known that
Gamma wasn't identified. Define that depending on
what the value for Gamma is the width of the identified
set can in fact be zero. Even if for values of
Gamma different from zero, the width of that identified said it's typically very small. This is a figure taken from
the an array tamer paper. The x-axis is true
values of the Gamma. Then for each value of Gamma, it shows you what the set of
values of Gamma is that is consistent with the
data-generating process. If Gamma is equal to zero, we can in fact identify. But even if Gamma is minus
one or one, it's very small. In this case this is really the main point I want to
make with this example. Even though identification here, it's very hard to
come by with using this partial identification
analysis we can actually get potentially fairly
precise inferences. The fact that this
set isn't a point, but it's actually an interval. Shouldn't really matter. Once you tried to estimate this sample data there's going to be a lot of
uncertainty there. That probably swamps the
uncertainty coming from the width of the identified set. Fourth example, this is from a different paper by Tamer, this one joins with Phil Hill. They are looking
at auction datum, so they're looking
at English auctions. The idea there is
in the auction, bidders keep increasing
their bids until there's only one bidder left who gets the object that's
being auctioned off. Haile and Tamer focus on
model this as a symmetric, independent private
values auction. What they're interested in, they're assuming
that they have data from a large number of auctions, each with a number of bidders. For what I'm doing here, I'm just going to assume that
the number of bidders in each auction is the same. Each bidder has a
value drawn from some distribution here denoted
by f of nu and f of nu. This value distribution
is the same in each auction and in fact here on Tamer having general
and more credible modal this covariance in this as well. But the idea is that somehow
there is a component of this value distribution that is the same across
all these auctions. You're trying to
estimate by using data from multiple auctions. In the theoretical
literature people thought about having this
type of oxygen, but you actually see
for each bidder whether they're still in the auction or whether they've dropped out. That point, would be able to infer for each bidder,
what their value was, each bidder would actually
drop out at the point where the bid exceeded their value. If you observe
that for each bit, you would just directly
get this observations from the value distribution
that you can estimate the value distribution. Now, that's never what
you actually observe in these auctions when they actually do these
things in practice. What you see is
someone makes a bid then someone else
makes a bid that goes up by some discrete amount that the current bid doesn't
go up continuously. You don't really see
for a particular bidder when they're still
in the auction or whether they've dropped out. If they subsequently make a bid, their value must have
exceeded the number. But if they don't
have a bid, again, you don't at what point the bidding exceeded their value. Haile and Tamer
are interested in, is trying to figure out
what you can learn about the value distribution
in that setting. What you see is the sequence, this finite number of bits, and the identity of the Betas. But you don't see
when people actually drop out of the auction. The key assumptions that they make in order to get
some sense of what the value distribution is both seem very reasonable
and in this setting, first of all, is the
no bidder ever bids more than their evaluation. If they do, then I
guess they would, then pretty much anything goes. The second one, there that may be slightly
more restrictive, but it still seems
very reasonable. No bidder will let
someone else win the auction if the winning bid is actually lower than
their evaluation. If in the end person i wins
the auction then it must be there is no bid
of price of 80. It must be that there's
no other bidder with a value higher than 80. Given those two assumptions, they show how you
can construct both upper and lower bounds on
the value distribution. Here these are
conceptually very simple. But suppose that the highest bid for participant i in
the auction is b sub i. Then since nobody bids more
than that value for nobody, the value of their bid is
higher than the actual value, and it gives you an upper
bound on the distribution. On the value distribution. Then the second path is that they get
a lower bound on the value distribution, instead of using the
second assumption. Given that the fact that no one walks away if the winning bidder is
less than the evaluation, must be the case to the second highest of the values among the end participants in the auction must be less than or equal to
the winning bid. We observe the winning bid, so we can get the distribution of winning bids over all these auctions, bound on the
distribution function of the second highest
order statistic. Having a bound on the
second highest order statistic gives a bound on the distribution function that the order statistics came from. That gives us the second bound on the
distribution function, and so together they
give us upper and lower bound on the
distribution function. This is going be the
fifth and last example, and then I'm going to
make some comments on the theoretical part, that's on the kinematic
part of this. There's a bunch of
papers looking at partial antifictions
in setting minus the paper by [inaudible]
this also work by Andrews Barry and
share this paper by [inaudible] I'm just thinking it's essentially a
very simple version, that this is a special case, pretty much of all these papers. Here, suppose you have two firms that contest
the set of markets, could be different
pairs of firms contesting sets of markets, in market M, the profits for the two firms
are Alpha A plus Delta A times an indicator whether
from B participates in that market plus some
idiosyncratic component. Same for profits
for firm B and in market M and decision whether to participate in the
market depends on whether their profits
are non-negative. Now, that seems all fine, we are assuming here there's
complete information, every firm knows everything
about the other firms. The complication is that
this doesn't really specify the decisions
completely, in the terminology
of this literature, this is an incomplete
model for some pairs of values of Epsilon
A and Epsilon B. These two solutions that satisfy the Nash equilibrium
conditions set as two solutions that satisfy these conditions
on participation, namely, for intermediate values of Epsilon A and Epsilon B. The solution is that one of the firms should
enter, but not both, but it could actually be both would be maximizing profit given behavior of the others, if they do the opposite of
what the other firm is doing. But so far, I have not specified on what conditions today
that would be 0-1 versus 1-0 in this case. What is argued in
this literature, is that it's very
difficult to specify in this multiple
equilibrium settings what each of these equilibria you would actually observe, which equilibrium
would actually rise. Here, these partially
identification strategies being used to remove the one assumption they
don't want to make, namely the selection
of the equilibrium. Just to illustrate this
figure essentially from the [inaudible] for intermediate values of
Epsilon A and Epsilon B, you get this indeterminacy
where you could have one of these two equilibria where
for all the other areas, you will get unique equilibrium. Another way of
thinking about this is that once we have
distributional assumptions on. Epsilon A and Epsilon B, even given the fact
the parameters. Alpha. A, Delta A, Alpha B and Delta B, we can write down the probability of a
particular outcome because it still depends on this unspecified equilibrium
selection mechanism. It still depends
on how we select, in these cases where
profit maximization is consistently with either one of the two firms participating as long as the
other one doesn't, we haven't specified
how they get selected. Which means we get a range of probabilities of the outcomes 0-1 that are consistent with the model and with the data. If you write it down for
all four situations, upper and lower bounds
on the probabilities. Actually for the first
and the last one, the upper and lower bounds
would actually agree, but it's useful to think
about this in general as this type of model
generating knots. The moment conditions of the type where we have
expected value of parameters that is
equal to expected value of some function of a random variable and parameters
that is equal to zero. But we have an inequality
where probability. It's something
that we can write, this expectation is bounded by a function of the parameters. A general way of setting up these problems is by thinking about problems that can be written in
this following form, what I call the generalized
inequality restriction form, where we have a known function of a bunch of random variables set and the parameter
that is non-negative. The traditional
method of moments setup refers to cases where this expectation
would be equal to zero. This fits this Silla, Batter, and Tamer example. It fit's a whole
bunch of examples. A number of papers have
studied estimation and inference in this
particular setting, which seems a very
useful general framework for thinking about
these problems. In particular, for estimation is this work by Chernozhukov, Hong, and Tamer who try
to develop methods for estimation and
inference in this setting. You need a little bit
of extra notation here. For a particular factor, X, the factor in brackets, X, plus or minus referred
to the component via positive and negative
parts of that so that you can write this
x in brackets minus, plus x and brackets plus. Godanna suggest look at this population
objective function, the negative part of the expected value of this
moment function, psi, and a product of that with some weight matrix W times again the negative part of this expected value of
this function, psi. This is a useful way
of characterizing the problem because now for all values of Theta and in your identified set we have
q of theta equal to 0. For Theta now that the
identified set we have q of theta positive. The natural thing analogous to standard generalized method of moments will be then, say, to take that function and
look for the values of Theta that set that equal to 0
or that minimize that. That doesn't quite work because even if
there is a set of values for which the
population objective function is equal to zero, there may be no values
of Theta for which the sample equivalent to
that is equal to zero. In fact, there may be very few values for
which is minimized. Just minimizing this is
not going to give you consistent estimates
of the identified set. What Chernozhukov, Hong, and Tamer suggest
then is estimating the set by looking for
all values of Theta such that the sample
objective function is below some sequence of values, Alpha of n, where Alpha of n is going to zero at
the appropriate rate. This is partly where this literature is still
very much in flux. This certainly seems a
very promising way to go, but it hasn't been tried
out a lot in practice. The last part I want to do in this lecture is
look at inference. Here there's actually
been a lot of work in the last five, six years, a lot
of it by Manski, by students of Manski's. The first question is
whether we're interested in the set that includes each element of the
identified set with fixed probability or the entire identified set with
that probability. The first condition would be that for all values of Theta, you would need to be
in the confidence set with some probability. Second one requires that the whole set is in
the confidence set. Second one is much
stronger than the first. Generally you would end up
with a bigger confidence set. I said before, I think there's no
particular reason why you would need to have a
confidence set that includes all possible values in it
with this fixed probability, rather than each
separate possible value with fixed probability, which is much more analogous to standard
confidence intervals. But there's been some discussion about that and there's not necessarily complete
agreement on that. Now let me look at two issues there in estimating
these confidence sets. I'm just going to focus on the case where we're interested in confidence sets for the
parameters themselves. Going back first to the
missing data example, the identified set
is p times mu 1, is the lower-bound P times mu 1 plus 1 minus p is
the upper bound. The only thing we
don't know here, assuming the probability
of missing data, P is known, the only thing
we don't know is mu 1. It's obviously very
easy to construct a standard confidence
interval for mu 1 so that they can be used
in three ways to construct a 95 percent
confidence interval for Theta. The first one is just to take the lower confidence
bound for mu 1, plug that in into the lower bound on the identified
set and take the upper confidence
bound for mu and plug it in into the upper
bound in the identified sets. That's the first
suggestion there. That's going to be conservative. It's going to be conservative
in the following way, for every value of
Theta in the interior, the coverage rate is going to be one for either of the
two bounds as long as the probability of
the data missing is greater than zero or the probability of observing the data is less than one. The coverage rate is
not 0.95,but 0.975. In some sense, if the interval is actually as nonzero width, you only have to
worry about errors of one type because errors on the other type are
still going to be in the identified set. You can fix that problem by just changing the contents of the normal distribution
that you're using there and replace the 1.96 by 1.645. That gives you point-wise 95 percent confidence interval. But now the problem is, if in fact p is equal to one, if in fact it gives the
point identified and we observe the y with
probability one, then this confidence interval doesn't give you the
standard thing back. It gives you a standard 90
percent confidence interval rather than a 95 percent
confidence interval. In cases where we're very
close to point identification, this is not going
to work very well. What has been suggested
in work I did with with Manski is to modify this a little bit depending on an estimated version of the width of the
confidence interval, adjust the critical values
a little bit in a way that if the width of the
confidence intervals of the identified
set is big enough, you're going to get to
this tighter interval based on the 1.645
critical values. But if the identified
set is very narrow, you would get back to
the standard interval in a smooth way that you get coverage that is uniform over all values for p. The last problem and this is much more famous area where
things are still in flux. I'm going to just look
at a very simple example of this setup with
inequality restrictions. Suppose we just have
two random variables, we know that both of them have expectation greater than Theta. If we know that Theta
is non-negative, the identified set here would be the interval
from zero to the minimum of Mu
of x and Mu of y. The difference with the
previous example here is that by having the upper bound with the minimum
of two unknown parameters. Obviously, we're
going to estimate these two means by
sample averages. But we're going to have to get into the problem
that the minimum of these two things is now going
to have a distribution at this very well approximated
by a normal distribution, if the two population
means are very close. Let me make that point
slightly differently. A naive 95 percent
confidence interval would be to take the minimum of, suppose these two random
variables actually have the same variance Sigma, and we have sample size
in both case of n, a naive 95 percent
confidence interval would have an upper bound
equal to the minimum of x-bar and y-bar plus
1.645 times Sigma over the square root of
n rather than over n. Essentially
what it's doing is ignoring the moment condition
that is not binding. We have these two
averages, x-bar and y-bar. Essentially here, this
confidence interval ignores the average that is bigger
than the other one, and constructs an interval
that would be the natural and the appropriate one if
all we observed was, x-bar and suppose that
was the smaller one. That's fine. That's going to give you 95 percent coverage
asymptotically pointwise. The two conditions, one is that the minimum of these two
things is actually positive. Otherwise, you get this problem that was addressed on
the previous slides. But you also need to have these two means and not
too close together. Essentially, what I
mean by that is if you need to be sure which of these two-moment conditions
is actually binding. Where this gets into trouble is in particular in cases
where you have a lot of inequality restrictions and you can't tell from the data which ones are actually binding. Here, I just tried to
illustrate this problem with the case where there's just
two inequality conditions. But the type of
problems [inaudible], talk about the concern about cases where they
have a large number of inequality restrictions. They can't really
tell from the data which ones are binding
and which ones are not. You need to take into account
the uncertainty coming from not knowing which of the moment
conditions are binding. That's going to give you trouble with this type of
confidence interval. The formal version of that is confidence intervals
are going to have the right coverage pointwise asymptotically,
but not uniformly. In practice that means
they're not going to work very well. Now there's a couple
of ways around that. There's some ways
of constructing conservative confidence
intervals by essentially taking into account
more moment conditions. Then do seem to be some ways of constructing
confidence intervals. They do ignore the irrelevant inequalities
asymptotically, but without getting into trouble with this
uniformity problem. The only thing I think that's been shown
to work so far as is subsampling variation
on bootstrapping. Where standard
bootstrapping doesn't work by sub-sampling
where you only take sub-samples of the original
dataset does seem to work. But the main point
of this stage is that this setup in terms of inequalities seems very natural one instead
of very general one. But there's still
some outstanding questions that the inference, but it does seem that the
setting is actually fairly relevant in presenting
a lot of cases. That's it for this one. Any questions? Yes, David. David: [inaudible]. Think of male, female labor supply
or is there any word on whether you're actually observing an incoherent model. Or if there's like a family
utility function that's generating the entry
example would be to live there distinctly as anybody can figure out where
you can really have. Guido Imbens: But I
think my reading now of that literature on the
coherency problem was that people were just very concerned with the fact
that what this was, it was essentially
an incomplete model. That even knowing the values
of apps that we typically think of the unobserved
components that tells you what the
outcomes are going to be. That somehow was a
necessary condition for a model to make sense and the label
that was given to that concern with these
models were incoherent. I think now that is
just a misnomer. These models were not incoherent,
they were incomplete. People didn't specify what the household
bargaining model was, and maybe they were right that it was difficult to specify. But the way those models
were dismissed in the econometric literature
as just not being coherent, not making sense, I
think was wrong exposed. David: [inaudible]. Guido Imbens: I think you
certainly in some cases, it's going to be possible that these identified sets
are inconsistent with other models and so that you can get testable implications there. I know completely sure whether these tests are always
going to be informative. But I think, that's an
extremely interesting question. I've only really seen some
of these incomplete models. These binary response
models in Iowa, but certainly the original
literature that wasn't labor, and I think it would
be very useful to revisit some of that. 