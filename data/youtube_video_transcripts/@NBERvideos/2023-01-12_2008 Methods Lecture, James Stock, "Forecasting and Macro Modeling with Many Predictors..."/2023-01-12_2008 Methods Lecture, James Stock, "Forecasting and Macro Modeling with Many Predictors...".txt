um so one of the areas that has seen substantial work in the last 10 or 15 years is empirical analysis of macro models where you're using a large number of predictors um so uh there's a lot of different ways to think about this issue um certainly when I was getting we're preparing for these talks forcing yourself to go through all of this literature I guess gets you to make some connections that you certainly haven't been consciously made maybe subliminally I don't know but it it seems as though one important theme running through many of the issues that we've talked about with GMM and dsges and and HACC estimation is limitations in the data that we actually work with the questions that we want to answer are ones that are perhaps a little bit ambitious compared to what the data are able to support there there seems there's basically two answers to that one is given the data and the questions try to come up with better software that is better econometric algorithms better distributions that can help you answer it and the other approach is to see if you can get more data so in the more data category uh in svars that was what I was referring to the other day as thinking about natural experiments or bringing in something from the outside to help with identification and this clearly falls in the more data or more information category and for a number of the problems that we face in a in you know we meaning the profession facing a very practical way such as forecasting um the tools certainly the time series tools that were available say 10 15 or certainly 20 years ago were ones that focused tremendously on very small models and those models prove to be reasonably successful but they still had some trouble against the judgmental forecasts and green book forecasts and things along those lines and I think the big one of the big challenges is is there a way to take into account all of that information that um that that is used in the heads of experts in a systematic way and so so the big so then the big challenge is look in real time around every month there's you know a thousand two thousand data release data series that are released and we look at six and those six are kind of interesting but that leaves you know uh 1994 left and is there anything you can do about it well um so as it says here the idea that you would want to look at all of those theories is in some historical sense in econometrics somewhere between radical and stupid and the reason for that is anyone who's brought up on Standard Time series analysis learns uh the principle of parsimony which is uh essentially work with small models and few parameters and keep it simple so as Arnold zelner would say keep it simple stupid his principles kiss um so you know and the thing is quite obvious even if you're running a VAR with just a few lags and six variables you start to really have a parameter proliferation problem so so um so the question then is how do you uh go ahead and try to tackle the large amount of data that's out there and make progress I'm going to talk about four areas that have seen work in this uh the one that's seen the most work is in the area of economic monitoring and forecasting and so I'll talk about that uh in the most detail but then at the end I'm going to talk about extensions of that using uh for using large data sets for structural Vector Auto regressions the buzzword there is favor um and then IV estimation and dsge estimation using um using large data sets okay so uh just to look ahead a little bit um it turns out that a very useful tool for a lot of this is dynamic Factor models Dynamic Factor models were proposed by John gaverty in this dissertation and uh in their extension of factor models that were say prevalent at the time in psychometrics Sergeant Sims have an early paper using this as well um there's I I should emphasize that although a lot of what I'm going to talk about is dynamic Factor models there's no particular a priority reason that Dynamic Factor models would have to be the best way to handle a lot of these questions and indeed in 10 years we might decide or we might have decided that they're not the best way to handle these questions it just turns out that when you look at a lot of different procedures Dynamic Factor models happen to be a very robust tractable and uh and and senses will make precise uh you know bit of reliable uh set of a family of tools that that that work well in this problem and seem to work uh at least as well and in many cases better than some of the other tools you might think of using so that's what there's going to be a lot of focus on that but I want to emphasize that the focus on dfms is not in any sense dogmatic or religious it's more empirical based on you know this was one of several ideas that you could pursue and it turns out to be one that worked pretty well okay um I should say also that there's a broader Trend which I hinted at uh yesterday and it will come up again uh in many of the Sciences in the Life Sciences but in some of the other observational physical sciences like astrophysics towards moving to massively larger data sets so uh apparently my understanding is that the newest chips for uh the newest sequencing chips have uh an individual site on this chip is called a probe so you smear a bunch of DNA on it and then it goes to these different probes and these new chips have 30 million sites so one observation on a sample of DNA so if I take some DNA and I smear it on this or I don't know that they do something like that it comes back with a vector that's 30 million observations long okay so then you do it for a bunch of different samples and they like effect gets pretty big pretty quickly um we are not at 30 million observations so the order of magnitudes here compared to astrophysics where there's quite a few stars that they're studying uh and uh and and genomics is really not not anywhere of the same of the same order but it is the case that there's some pretty interesting tools that have been developed for these extremely large problems that are potentially of some interest at least at a theoretical level there are some interest and I'll talk about them I think as it turns out I'm going to come back saying that you know the tools developer genomics don't work as dynamic as well as Dynamic Factor models and maybe there's some reason to think about that but we'll be able to make that precise okay so why do you want hundreds of series and so let me talk about this dimensionality from curse to blessing because certainly for those of you who went to graduate school a while ago uh and if you did take a Time series course boy those were small models and principles of parsimony and you really don't want to have large uh large regressions so let me give you an example this is going to be a in some sense a toy example but it's going to be a pretty good analogy for a lot of stuff and that we'll be doing so a toy example is where you would have a progression of why on P okay why am I using p P for predictors and also for principal components and you'll see why that is but the key thing is that I want these to be orthonormal because that's going to make my calculations easy just for this example okay so we're going to have an orthonormal set of regressors so we don't have to worry about any covariances and an error term and these are going to be strictly exogenous regressors and the errors are going to be IID normal and that's going to make my calculations for this example simple all right so this is not a Time series problem even though I've got some sub T's but it's going to be simple um it it might be that if you have normal noise that what this is a model of is these might be wavelet Basics or something these might be wavelets bases that you're trying to do some analysis of uh of uh of signals on so this would be a model that comes up in Signal extraction um okay so if you have a we're gonna have a quarter quadratic loss function simple as could be and what we want to do is we want to make a forecast we want to make a forecast using data through cap T of the value of y tomorrow so it's a single series and we're going to think about this as being a large number of predictors and you've got normal errors and you have squared error loss all right so it's a standard problem with the exception that we're thinking of p as being large now one thing you learned in graduate school is that if you've got a ton of regressors don't just throw them in OLS because that's a bad idea okay um it is a bad idea okay so we're going to go through why it's a bad idea you probably didn't think you needed algebra to know that's a bad idea but we will do it anyway all right one way to make precise that is a bad idea is to say that in a particular you know here's the loss you make if this is your forecast one realization of your forecast and this is the true value that's the forecast error the mistake you made you square it and that's how bad you feel okay this is how bad you can expect to feel on average when you make forecasts using this procedure right that's what is called the forecast risk the so-called frequent is forecast risk so it's how bad you expect to feel on average all right now I'm going to do a little bit of calculations how bad you feel on average well there's two reasons you can feel bad one is because the future just you know stuff happens and you didn't know about it and it's just in the future well there's not much you can do about that the other reason you can feel bad is because you've used a lousy forecast during your forecast excuse me use lousy a lousy foreign estimator of the coefficients all right so if you use a lousy estimator of the coefficients well that's going to make you feel bad because you're going to have a quadratic loss that's associated with your forecast based on this bad set of four coefficients well you can break these out because what happens tomorrow is unrelated to your the data that what you used in the past to fit the to estimate the coefficients the IID data right so what happens tomorrow is unrelated to the past so these are two separate things you can break them out and you have two parts to your forecast risk you have a part that you can do nothing about that's just the way it is and you have a part that you can do something about which is uh the risk associated with this particular estimator the risk associated with this particular estimator is the inner product of the expectation of its inner product of this vector or sometimes referred to as the trace MSE risk because it's the expected value of the trace of this um it's basically the MSE matrix it's the trace of the MSE Matrix so it's the difference between your estimator and the True Value this isn't a variance right it's not the difference between the estimator and its expected value of that estimator it's the expected value the difference between the true value and the estimator itself so this is an MSE it has variance and bias built in yesterday I suggested that this risk function is a risk function that is not an appropriate one for Point estimation in dsge models that we're actually not interested in some bias variance trade-off and dsgs models we want to know what you know the calvo parameter is we want to know it here we actually couldn't care less about the coefficient on the ith predictor we're not interested in that at all we are interested in one thing which is not feeling bad about our forecast at least on average right so that says that um that this is a reasonable pretty reasonable loss function for us to think about since we can't do anything about tomorrow at least we can do something about the user the procedures we're going to use to estimate Delta okay all right so that's our forecast and I should mention that if you're following along on the slides you've probably already noticed about 20 discrepancies between what's on the screen and what's on your slides because of a large number of typos but since nobody seems to have been paying attention to the handwritten stuff or the stuff on paper that's perfectly fine the revised version will be on the web uh in due course okay so okay so so let's just make this thing concrete uh sort of obviously if you knew the true value of delta somehow uh then of course you would use it and this whole term would be zero your ex your frequentest risk your forecast risk would be excuse me your estimation risk would be zero because you haven't made any mistakes so in that case you would get what we can call A first order efficient forecast and it would be as good as if you knew the true ones and you actually would know the true ones if the coefficients were consistent then again the estimation risk goes to zero so you would have a forecast risk that doesn't go to zero close to Sigma squared Epsilon and the forecast would be first order efficient but of course now now that's actually a reasonable thing to think about if you only have a few a handful of coefficients so if the sample size is large and you only have a handful of coefficients thinking about these coefficients being consistent for the true value is probably not unreasonable but the situation we're going to be interested in is trying to exploit a large data set where there's a lot of information potentially um and in that case um OLS is not going to be first order efficient and uh in fact because um by the exogenity of of these uh of the regressors And the fact I assume normality we have an exact distribution but for what Delta tilde our estimator is the OLS estimator is exactly this distribution it's the textbook X Prime X inverse Sigma squared uh distribution there's a there is a there's not supposed to be a t here it's p Prime p and there's not supposed to be a t here and that's going to result since this is orthonormal that's going to result in a t there so this equation is correct all right um okay so that looks pretty good I mean it looks like you would have a variance getting smaller but the trouble is there's a lot of these things so if I compute the trace MSE um it's centered around the right thing so in this case there's no bias and then I just compute the trace MSE well that's going to be the trace of this Matrix here because there's no bias term well the trace of this Matrix here is going to be single squared Epsilon over T that pulls out but then it's the trace of this n by n identity Matrix so I have to add up n terms and I get n over T times Sigma squared Epsilon so if I put that together the forecast risk is n over T times Sigma squared Epsilon plus the sigma squared Epsilon which gives me one plus this ratio n over T times Sigma squared Epsilon okay so that's actually an instructor formula what it says is that if um if you use a lot of of variables uh predictors then in fact you're going to get a forecast that can be potentially quite a bit worse than this first order efficient forecast right so um Okay so a couple of other comments as I mentioned again I think the other day in passing there's a couple of other reasons why you might want to think about OLS as not being particularly good and one of them is this really amazing result by Stein in 1955 which said that OLS isn't admissible under this Trace MSE risk which is to say that this risk function okay this this risk function here if you look at this Trace MSE risk function it turns out that there exists other estimators that have a risk function that is uniformly no higher than so it's below in some points and it's perhaps the same and other points as OLS and in fact then so they Stein proved that that existence existed in 1955 no he proved that there must be such in 1955 and then he actually constructed one in 1960 and that's this James Stein estimator which is a shrinkage estimator and it shrinks towards zero so in in the area of zero since Delta is being shrunk to zero the James Stein estimator does better than OLS and the interesting part of the calculation is that everywhere else it still does a little bit better at least does no worse so the James Stein estimator has a risk function that strictly lies below the OLS risk function so even if you only have three observations three three regressors OLS is probably not a great idea and we're going to be talking about having 100 series and so this is another reason to think that you can do you can't not only is OLS not not a good estimator but there exist estimators they're going to be better Okay so um okay so so that's so if you will that's the cursive dimensionality part okay so why am I doing this right you guys are probably all expecting me to like write down a dynamic Factor model and to start talking about estimation of dynamic Factor models and you know how to use binding to estimate the number of factors and all that stuff and here I am running rambling along about they about this risk business and the reason it's twofold one is you might not know this it's supposed to be a class the other the other reason the other reason is I want to impress upon you because there's actually some theory behind all of this stuff and it's not just people running stupid regressions and seeing what happens when they do simulate it out of sample forecast exercises okay so I'm trying this literature can get a bad rap if you don't know that there's actually something going on behind it all right so so here's what's here's part of what's going on behind it blessing of dimensionality so so I just gave the curse of dimensionality problem but the cursor dimensionality wasn't a curse of dimensionality it was just said we didn't use the right tools all right so let's use the right tools and so here's a way to think about it the right tools are so I'm going to adopt a local nesting in which um in which the parameters are local to zero and the only reason I'm doing that in this setup is to prevent the r squared going to one if if you have a large number of predictors and each one of them is important you're going to get the total sum of squares being totally eat just equal to the explained sum of squares r squared is going to go to one in some any of the asymptotics so this is just a device to keep the asymptotics from being unreasonable and so so these D's now are equivalent to the Deltas these D's are what we don't know if we knew them we'd be in business these are the coefficients scaled but what I am going to assume this isn't an assumption this is just a definition just take those D's so take these D's which are these two population coefficients that we don't know divided by times square root of T and just order them from smallest to largest and then compute an empirical CDF all right so you guys know how to compute empirical cdfs uh mark this wasn't this an empirical media where you had empirical cdfs that you were he was inverting okay so that's an empirical CDF except it would be kind of squiggly because to a step function all right so you construct the empirical CDF of those things and you call it g sub n and that's just going to be a name all right and now I'm actually going to talk about estimators that have a particular uh particular characteristic which is that they're invariant to the order in which you put the regressors so when you are running stator or rats and you have to type in a list of regressors you get the same r squared whether you type uh you know consumption first and income second or whether you type income first in consumption second as the records I see I see puzzles you're probably wondering does there exist any estimators that don't have that property and they they there are you could you could write an estimator down that doesn't have that property but I'm not going to consider them okay that doesn't seem unreasonable to this audience okay um all right so so okay so now I'm going to do some a heuristic calculation the heuristic calculation is formally justified in some references that I'll give here's the frequentest risk okay the frequentest risk is the trace MSE loss and so I've now written it out with a summation sign okay and now I've done nothing of Interest here except I've divided I've got a I've I'm dividing by n because there's an N here and then remember the Deltas I wrote these these Deltas are equal to d i over the square root of T so I'm just doing a local nesting so Delta I is equal to d i over the square root of T I'm doing the local nesting here so I can change the Deltas to the D's and I can pull out a t all right so I haven't done anything I've just done the substitution and now here okay here I actually looks like I've done something that I haven't done anything this is a summation over the D's the summation over the D's identically can be written as a as an integral with respect to this step function CDF so this is again an identity okay so this is just this is writing this in okay but now this is amazing let's look at this for a second it's an integral of the expected loss with respect to this empirical CDF we defined this yesterday to be the base risk okay the base risk is the expected value over some CDF over some prior this is the so this is a base risk it's a base risk it's very funny base risk because it's a Bayes risk with respect to not a prior but this empirical CDF of the D's so that's just a definition we have defined the base risk to be an integral with respect to some density or some in this D CDF okay this actually this so I can say that this is a deep link and and I can get away with saying that because it certainly isn't something that we thought of it's this is an old deep link due to Herbert Robbins between Bayes and frequency inference so so let's so let's think about this I want to find the estimator that's going to be the best possible and I'm a frequentist so all I care about is I want to minimize my frequency risk I've learned that unless it's a bad idea because there exists things like James Stein when I've got more than three regressors that are going to be better and I don't have three I have a hundred so there's things that are going to be tons better than OLS so I'd like to find that function of the data Delta tilde that gives me the lowest frequentest risk this is just a series of equalities and what it says is that my problem my my previous problem is actually the same as dysphasian's problem now this is a very strange Bayesian there's a Bayesian like none we've ever met before because your prior is actually the empirical CDF of the true D's but suppose you were that busy and I suppose this were your prior if you could solve your problem for the optimal base estimator it would actually be my optimal estimator these are the same problems that's the idea so this we've solved the frequentest optimal estimation problem and therefore the optional forecasting problem under Trace MSE loss and the way to do it is to use the base estimator with respect to the CDF of these parameters now there's a lot of special things about this like the it turns out that the orthonormal regressors is really important to this argument but I think this so this is all but it's all correct under what I've said here this is um so then you could say then you could say what do you mean so now now we're gonna okay this is the underlying idea of empirical bed and so what empirical base says is all right I want to solve a base problem but it's a very strange based problem it's a base problem where I'm actually going to be solving it with respect to some empirical PDF of the underlying print true parameters and if you can solve that problem then that's going to be uh the optimal thing to do from this forecasting perspective okay so one way to think about this is that we're going to have a whole bunch of bayesians come into the room and all of these bayesians because they're subjective as patients they all have their different opinions and they're all going to produce an estimator and we're going to choose the Bayesian with the best prior that's a stupid thing to think about how can you choose the Bayesian the best prior and that's not a sensible thing to say if you're a subjective expedient but it's actually incredibly sensible if you were at the Fed and you have a committee meeting and people come in with their models from different priors and they keep coming in and again and this guy like is always too pessimistic you eventually you'd say that's a bad prior your your prior doesn't forecast very well this person's always too optimistic their prior is just not very good they're always too optimistic they don't forecast very well one of these people in this meeting one of the people in this room will produce the best forecast over time it's logical to think of that person as having the best prior so that's what Imperial base does is it finds the prior that is going to produce the best forecast that is to minimize the trace MSE loss that's not how it does it but that's the idea so it's quite well posed to say that one person's prior is better than another's it's not well posed from the perspective of personal subjective Bayesian decision Theory but from the perspective of making a forecast it is there's an empirical reality which is the future and you're trying to forecast it okay um so there's a bunch of really cool stuff about empirical based estimators um there's there are certain senses in which they're asymptotically optimal that was shown by Robbins it turns out so everyone Morrison a really important paper in jazza showed that the James Stein estimator is actually empirical Bays so this this thing that I call this shrinking this James Stein shrinkage estimator can be justified as empirical base so this gives you it's going to give you an algorithm for constructing uh good estimators there's a couple of papers in the annals the 2005 paper and the Animals provide some strong properties about empirical based estimators in a problem that's extremely close to the one that I just post actually the problem I just post is a subset of the problems they consider um there's a variety of methods to estimate G I'm not going to go into those methods I'm only going to talk about it in a very limited parametric way let's think about the following exercise I I had the committee meeting at the FED where all the people came into the room with different priors I'm going to constrain that committee meeting a little bit I'm going to insist that everybody uses the same family of priors that family of priors is going to be finitely parameterized the parameters of that family of priors are called hyper parameters different people come into the room with different hyper parameters for their priors one of them is going to win okay those are that's going to be the empirical days estimator of the hyper parameters or the prior so that's a parametric empirical base these wonderful theorems are all about non-parametric empirical base but as we all know you know you can get a long ways by doing things parametrically and actually you can do worse by doing them non-parametrically in practice okay um all right so those are some important lessons one of the big lessons is that these shrinkage and base methods can be very important in this framework okay so that's point one blessing of dimensionality 0.2 to blessing of dimensionality so instead of cursive dimensionality blessing of dimensionality and so now we are going to shift gears we're going to talk about Dynamic Factor models which might seem totally unrelated but if you're still here and not catching a plane towards the end you will see that these are not unrelated okay so I'm going to talk about this in more detail later so just live with this for the time being okay so here's the idea we have a ton of series I don't know 100 or 200 and they are related to something you can't observe f it's only a few F's some people say there's one some people say there's nine but there's a few num a few of them and then there's some other idiosyncratic noise sergeant had a model like this in this 1989 paper Larry said that you observe some things and there's an underlying state that was evolving and then there's measurement error because the statistical agencies just added error to data that was his model um and then this underlying F evolves over time and I'm going to ignore that for the argument here this model is a dynamic Factor model and this is what was first proposed by Gava key and in his thesis and there are a number of papers that looked at this empirically so England Watson figured out a way to estimate this in the time domain using the common filter and then nobody did anything with it for a while and then uh then there's a series of papers that use that algorithm uh in the late 80s and and estimated this and those are all on really small systems so I guess Mark and I what we had four variables how many did you have with the angle five okay and and and I think uh I think uh Tom sergeant had six or something these are small systems like one factor and you know a couple of lags here and maybe modeling some Dynamics here and so you've got a pretty small number of variables and you know if this took a long time to do the MLS I mean the MLS were slow back in the 19 in 1990 and it was tempting to try to increase the number of parameters I mean you'd really like to go to a higher system because it would be interesting to see what would happen but it just wasn't realistic okay it wasn't a realistic exercise so so that looks like a cursive dimensionality okay well so in a paper in a discussion of Quant Sergeant John gabecky suggested that actually you could really do a lot better if you had a lot of series and he didn't really propose any computational methods it was all just kind of intuition and his suggestion was if you had a ton of series you actually you might be able to estimate F well see here's look at this and so Mark did the common filter and remember there's the state estimation equation if this is the state Vector F in this setup so this is the Observer equation and then this is the state and the state has you know a dimension of one or two or something like that and you only have a few series you can use the common filter to estimate F but if you only got a couple series you're going to be estimating it with air so it'd be 12 to no F it'd be really interesting but you're going to be estimating with air using the common filter with a small number of series intuition was that if you have a ton of series that you'd actually be able to estimate at arbitrarily well and I was there at that conference and I thought he was crazy um I don't know maybe you thought you we edited the volume I mean we left the comment in but we I don't know here's an example that makes to shows that what he was saying makes complete sense and this is following a paper by Foreign about 10 years ago so now I'm going to make a really simple assumption suppose you have one factor F so this is a simple version of that Dynamic Factor model and I'm going to say do something totally idiotic so we're not going to estimate something using the common filter and state space form by maximum likelihood we're going to compute the average of the X's right so what could be simpler well compute the average of the X's then it's just the average of this bit here so just substituting things in and now this ft doesn't depend on I so I can pull that out of this summation and then diff you know pull the ease out of the summation and what do I have well I've got a term right here which is an average of these e both these E's are idiosyncratic in the sense that they're not really from one series to the next they're pretty much uncorrelated you're just going to have a weak law of large numbers telling you that that thing's going to go away and now if these lambdas let's say these lambders are positive on average that's an identification assumption so we might want to think about that but let's just suppose for the sake of this argument that these are positive on average well then this thing is just going to converge to some number and there you have it okay so it turns out that if you have a ton of x's all you got to do is just average these suckers productive X's one factor factor loading being positive on average just average these guys and there you are you're done so you don't need the common filter and state space and ml or any of that stuff just got it so this is a blessing of dimensionality right you've got another parametric estimator that gives you this really interesting object which is the factors now this is not the most sophisticated estimator of the factors that you might ever find it's just averaging these but but this is all the this is gonna this is the intuition for the you know the other estimators that are used okay so that's so that's two two interpretations of blessings of dimensionality blessing of dimensionality number one being you can not just not just is this is not not just is the frequentness problem equal to the base problem but if the number of observations the number of predictors is large you're actually going to be able to solve the base problem that is we're going to be able to estimate G sub n and therefore use that estimated value of G sub n to come up with a good a good a good forecast so that's the that's that's the blessing of dimensionality in the forecasting problem and here it is in the dfm problem all right so now I'm going to talk about dfms so now this is probably where you thought I would start foreign and he said we're going to make a we're going to write down a little model here and the model says that I've got a bunch of x's and they depend on two things something that's common so a common factor and then something that's not common in these are idiosyncratic terms and a model that he wrote down and he added gaussian errors and all that stuff but in the model that he wrote down he said that these idiosyncratic terms were not correlated from One X to the next X so between X's there's no correlation between these what that means is that the entire coal movements at all leads and lags of the X's arises from this common component in particular from this F now this F because things evolve over time it has some time series structure and so we can just write that down as a VAR right so it's got some time to restructure and it might appear in with lags so you know some series might so in the jargon load on the X with the lag others might load contemporaneously others might load with a distributed side and so that's the exact dynamic Factor model and that's actually what was that's what's programmed up in the common filter in those early 1980s papers um so a couple of quick comments comment number one is um just I just be clear we'll come back to this kind of talk about svars or favor uh the factors themselves are only identified up to some uh up to some rotation so I could insert a matrix HH inverse in fact if you wanted I could even make these a lag polynomials but I'm not I'm not I could even make these lag polynomials in this setting um these uh I could insert a matrix H inverse um I could make these lag polynomials if I don't care about one-sidedness if I want to make this two-sided um so I can insert this HH inverse in here and I'm going to get the same uh the same uh the same product Lambda L times ft so um so there's a lack of identification that's involved in this setup uh that actually is not important for any of the applications we're going to be doing so for it for the first bit so if we're talking about forecasting we don't really care whether we have in some sense F or H times F we only want the space Band by F in a regression so we can project it on it we don't really care so this is not going to be of any importance at all it's only going to be important if we start talking about like what's a structural shock and what do these apps actually mean and we'll talk about that at the very end when I talk about save R and bovandianone second comment um is that there's a spectral factorization and that spectral factorization is this which is this the spectral density Matrix of the X's at frequency Omega and because these are the F's and the E's are uncorrelated that means that all article variances between the two and all these and legs are on zero so that means that the variance the article variance function of the X's is equal to the auto covariance function of this guy plus the article variance of this guy so the Fourier transform of those is equal to the sum of those two Fourier transforms that is to say the spectrum is equal to the sum of these two bits and this is just using the the usual spectral algebra it says here that there's a common component in the Spectrum that's going to have reduced rank so the dimensionality of f the number of dynamic factors we're going to call that q and so the rank of this bit of the spectrum is q and then this is a diagonal matrix all right a diagonal matrix diagonal spectral density Matrix uh we're not going to exploit that in anything that we do but that's just for those of you whose intuition is best Guided by relationships in the frequency domain I think there's one in the audience [Laughter] forecasting so this is so this is pretty interesting pretty important and maybe obvious but it's worth going through these steps if you want to forecast one element of x given past values of x um and F right so this is the magic bit if you're given past values of X and F Well given this structure and in the fact that it breaks down into this part that depends on the uh F's and the part that's the idiosyncratic terms that this forecast is just going to end up being a function of the F's and then your own uh X so I'm sorry I said that mathematically this is really the whole this is don't look at the math look at this equation look at this equation if you want to forecast X well you're going to have some serial correlation in the air term but this error term is unrelated to everything else so that serial correlation in the air term can be obtained it can be taken care of by throwing X as a regressor as a lags of X in this equation and then you've got the F's so you can forecast x given the F's and given the X F's and it's and your past right from this equation and then you could ask a question would I do better if I had any of the other X's like I've got if I I have 199 other X's would it be useful for me to throw them in this regression and the answer is no the reason the answer is no is that once you've got the F's condition on the F's all you've got is these ease and these ease are uncorrelated across all of the leads and lags with all of the other series so there's no point throwing on the feather axes all of the information the dynamic Factor model that you need for forecasting is embodied in this app so if you could get that F you're in business all right so now that's too strong um there's that that's making assumptions that there's no relationship at all in the asynchronic parts uh even if you just take it literally at the measurement air story you could imagine that there might be some series uh there are series that are measured with the same survey and if it's a service done in a certain way there's going to be correlation and Survey area across the different series um and and if and if the measurement error it has more substantive content than there might be correlation the approximate Dynamic Factor model takes that into account I'm going to give some particular assumptions that our statistical implementations uh or work those statistical implementations of the dynamic Factor model but basically the approximate dfm says that the that the um that most of the variation in The Matrix of X Prime X is coming from uh the uh is coming from the the factors and um there's a little bit that might be coming from the uh the anti-syncratic terms but asymptotically that Matrix the the variation in that Matrix is dominated by the factors another way to say that is in the X Prime X Matrix if there are Q fact Q uh there if there are Q factors this is it's going to be dominated the q i there's going to be Q really big eigenvalues and the rest of the eigenvalues are going to be small okay so um we're going to translate the static Factor model into a dynamic Factor model into What's called the static form which is actually going to be quite convenient for estimation so to do that um I'm going to take the dynamic Factor model and and okay well here so so here's what I'm going to do which is I'm going to take um I'm going to take this all this F and these lags and I'm going to assume that Lambda L is finite order finite degree lag polynomial so if it's finite degree say p sub f then I can write all of these lag polynomials lag polynomials out like this they're the coefficients of the lag polynomials in the first equation and then I can take all these F's and I can write them in a vector form like this and then of course that means the first equation is just x one t plus the distributed lag of these F's plus E1 T all right so I've just written this in first order form and that's assuming that this is got a finite degree lag polynomial so under that assumption this is I here's what I've got which I've just got x t is this Matrix now which is n by R and then this uh this excuse me this Vector here which is the capital F which is R by one now the way I've written it out R is going to be equal to the number of little F's which is Q times the number of lags PF now there might be some redundancy or it might not be that much but that's the most that it that it could be so this is now going to be what we'll refer to as the dynamic the static form of the dynamic Factor model and it's uh it's just written so it looks like it's a regular cross-sectional Factor model but it does have Dynamics living aside of it in terms of the the lag structure here um in addition I can take the um I can take this VAR for the little F's and then in stacked form that's just going to imply a VAR for the big F's um and uh and so I've just written that down and um okay so there's a dimension issue which is that these F's are R by one and these Ada's are Q by one so that means I've got to have some Matrix which is going to be R by Q which is going to take this reduced number of Adas of the dynamic shocks and turn the dynamic Factor Innovations excuse me and turn them into Innovations for the static factors right so that's uh that's the static Factor model and um just to make it uh simple I'm going to just make this a first order first uh a var1 just so that it's easy to work with and and most of the rest of the talk that's just a var1 just to to make life simple so there it is there's the dfm in static form and this dfm in static form is now just looks like a standard states-based common filtering exercise so uh so so that's that's uh that's what it is you can put the whole thing in the in state space form uh if you want to model the Dynamics of the uh of the adiosyncratic terms which will be useful you could imagine that those follow Auto regressions so I can then stack the the E's and their lags as much as needed so that that auto regression can be written in first order form and so that's the first order form of that auto regression H is now going to be a matrix that takes these um n uh idiosyncratic Innovations uh Zeta and then transform and basically has a bunch of zeros in it so that this is the first order form of the uh of the uh of the auto aggressive representation for the ease should emphasize that because the E's are supposed to be uncorrelated across each other in the exact Vector model that this Matrix D is diagonal all right so these are not it's not a VAR for the idiosyncratic terms it's just a whole bunch of ARS and so so then this puts the whole thing just for completeness this puts the whole thing into a state equation here and uh an observer equation right here and you're ready to apply the common filter and so that's how that's how these common filter algorithms were originally done or versions of this is one way to set it up okay I said this just is what I said okay so so so just to step back let me summarize we've written down the dynamic Factor model uh we did just a few things it's a linear model and it did just a few things to put it in exactly the state space form that you saw for the common filter when Mark presented that a couple of days ago and since it's now in that form that means we can run the common filter and among other things we can compute the likelihood if we have the likelihood we can then maximize it and so this can all be estimated these parameters can be estimated by maximum likelihood and uh and that's that's what was done um that got uh sort of frustrating Quant Sergeant uh actually we're able to estimate a system with 60 variables but they did it in a very very special structure that um that allowed them to do some tricks that were not able to be we're not readily generalized so although it looks like it's a big structure computationally this was I mean not it was certainly non-trivial but it wasn't really moving all the way towards a full uh system okay so but of course that was a while ago and computers were better so I'm going to talk a little bit about comp mles right now um computers are better um one of the one of the interesting developments since there was there was in this intermediate period a fair amount of work on alternative non-parametric estimators of the factors and those estimators are actually pretty good what that means is that you can now get using those preliminary alternative estimates of the factors you can actually use that to get pretty good starting values so if you you don't know these factors here but if you actually but if you could you know you had some reasonable estimate of the factors then you could get a pretty reasonable estimate of Phi and you could get a reasonable estimate of these of the of maybe of G and then you could get a reasonable estimate of uh in your regression let me put them in regression form here you could get a reasonable estimate of the factor loadings by regressing um f on Lambda and then you can take the residuals and you can fit an ar2 to those so all of those parameters can be obtained using very simple OLS type algorithms once you have uh once you have estimates of these factors so you can so because we now have procedures that I'll talk about in a minute that give good estimates of the factors um you can get quite good starting values for the um in fact you get you can get consistent starting values for the uh for the estimate for the common filter so um so uh and and then I guess a final point is there's some recent work here by Young Baker and coupon that provides an additional speed up it's actually a clever projection device that um that simply that's turned some of the common filter up being equations into just regressions so um so there's additional additional advantages that you can get instead of having to work with the otherwise large matrices that would be involved um there's some Theory that's been developed about ml estimation of the of um of these systems when you have large of dynamic Factor models uh when you have large um numbers of uh of series um and the relevant papers this one by dos giannoni and reichlen um and uh they show that you could that these mles are going to be consistent or most always be more precise than that what they show is that you will get consistent estimates of the factors by estimating those parameters by mle and then by by using the common filter to estimate the factors um so I'm going to talk about there's some some empirical evidence but I'm going to talk about that once we go through some of the other estimation methods another comment about the mle and the states-based formulation of this um the the state space formulation is has a couple of advantages one of them is a couple of nice features I should say and one of these is that is it's possible to accommodate a large number of data irregularities so if you have data that occurs uh some monthly date and some weekly data for example or some some daily data you can actually set up but then you know some days you might observe uh you might observe some macro variables and those macro variables at those days The Observer this being The Observer equation at those days The Observer equation is going to be related to the factors but only in a way that links it to the variables that we're actually observing today so the Observer equation is going to have uh this a a a version of Lambda that's going to be varying over time depending upon whether we see it or not and that allows you to go ahead and compute a likelihood and run the common filter based on the data that you have available today and in the past even though the dimensionality of that data is going to be changing depending upon the day that you have it and moreover you can do things like say I suppose you have temporally aggregated data so you want to model consumption as a flow and what You observe is consumption over the month you could say I'm going to average uh the factors over the last month and I'm going to observe once every 30 days I'm going to observe the average of uh the of the average of the F's every 30 days so you can set up a lot of things like that in the common filter that that's not new it that you can do that is is described in Harvey's 1993 books book um certainly older than that but Harvey has a ton of papers on this but it was all it was implemented in this context using a large system by these three authors uh using a regular Mix Daily data and the nice thing about that is that if you once you have the parameters of the DSG excuse me once you have the parameters of the dfm uh you um every day when new data is are released you just like can put those in and then you run the common filter for updated for that one day and you have an new estimates of the state based on that day's information and those feed into new estimates of your 200 series so like you could you have in real time updates of everything you'd want to know based not just by some clue G method like chaolin or something like that but by in a completely internally consistent mechanism that's going to give you an updated piece of information on all of the series that you're interested in and um so I I Dominico you guys did you were involved with the one doing this sort of thing at the FED isn't that right with Lucretia so I don't know if that still exists is that actually running or was that a hypothetical trial experimental system Okay so I mean once these get running in real time then they know they disappear from the literature but uh but but but but but these you know this this is the sort of thing that can be very uh valuable because it provides a complete and internally consistent update on a large number of series okay all right so okay if that's estimation mle all right all right so here's the chronology right the chronology was the original stuff that gave Ricky did was was spectral and then um and then England Watson figured out how to do this in the time domain by the common filter and then everybody gave up on the common filter because it was too hard using computers that we had and so then some other procedures were shown to work and those were principal components families or procedures but now the computers have gotten fast enough that we're kind of back to doing mle and these really huge systems and but that's very recent just the last few years and I think that this coupon jungbacher thing is going to help a lot in terms of making it implementation implementable so now I'm going to talk about the inner that intermediate set of procedures which is principal components related procedures so here's principal components foreign so let's show the time being think of this as a regression model or some model I don't know it's some model I'm going to say we're going to treat f as a we're going to treat Lambda as a parameter and F is a parameter which sounds idiotic but let's just do that and see what happens okay so I'm going to say I want to fit this by non-linearly squares because I can't do OLS if I knew Lambda I could figure out F if I knew F I could figure out Lambda I don't know either of these but I'm going to do say some nonlinearly squares thing to try to figure out Lambda f looks like a regression problem so why don't I minimize the sum of squared residuals all right so I'm going to minimize the sum of squared residuals and Lambda and F are parameters whatever that means I'm estimating them as parameters so they're parameters they're Theta well this problem actually has a really nice structure and I'm going to concentrate out F so if I knew let's imagine that I knew Lambda then Loosely speaking assuming that I can do my matrices with the right transposes and all of that in my head f-hat is going to be Lambda Prime Lambda inverse Lambda Prime X so the residual from that is going to be x minus Lambda Prime inverse Lambda Prime Lambda inverse Lambda Prime X so the sum of squared residuals from that is going to be X Prime the projection on the Lambda Matrix X well I've gotten rid of f which is fine if I knew Lambda I don't know Lambda but now I can solve this problem I can say minimize this projection sum of squared residuals over all possible values of Lambda and you look at that you say that doesn't look very fun that would be a big grid search but if you do a little bit of algebra it doesn't have to be a big grid search so minimizing this projection on The orthogonal Matrix to Lambda is the same as maximizing the explained sum of squares and now I remember my Matrix tricks from graduate school where I can take the trace and do transposes and pull things to the side and it's the same answer and the advantage of that is I got all of these summation things in the middle and so it's I want to find the Lambda that minimizes this inner product well this Lambda Prime Lambda to the minus one half Prime Lambda is normalized so that this thing together is itself Prime itself is equal to the identity Matrix so I can replace this or at least throw away this and impose that side constraint and I say I want to maximize Lambda Prime Times Sigma x x Lambda subject to this restriction and that solved Us by being the first our eigenvectors of this Matrix and that gives then F hat as Lambda hat Prime XT is the first principal components first our principal component so so so so the reason I did that principal components is not new and nonlinearly squares is not new but in some sense principal component sounds like this stupid algorithm that you never really wanted to use in SAS but it actually has a very sensible origination okay so the consistency of principal components in the exact static Factor model with t fixed and intending to Infinity was shown by Conor and karachik and um then uh there was a there's a pretty substantial literature now over the last 10 years that's been looking at results for estimation in the approximate Dynamic Factor model in static form here are some typical conditions one of them is that you assume the F's are well behaved another is that you assume the lambdas are well behaved so there's obviously some identification conditions lying around in there but um but it the the it's basically these are definitions that you have the right number of factors so if you have R factors that make sense that these things would be true this was the approximate dfm that says that the E's are going to be weakly dependent over time and across Series so there might be some correlation across series but it's not going to be so much that it's driving the covariance of all of the series it's just maybe a block of series like you know maybe there's some consumption series that are related or something in the same survey that's related and then the F's and the E's are uncorrelated and then um the asymptotics here are ones that have nnt tending two in Infinity and this is going back this asymptotics works remember the the the um foreign Riceland take the average of the X's that worked because n was going to Infinity okay this stuff works because n is going to Infinity this is this blessing if you will of dimensionality okay so there's some results so uh for the dfm uh they're consistent uh you can use f as a regressor um the original proofs were clueji and they got fixed up and that's not that's not fair the original proofs were good but it turns out you can get better rates and so these guys got better rates for using things and there's also now distribution theory for things like confidence bands for predicted values let me make a comment about that conference bands for predicted values are different than confidence than than prediction regions Mark showed you this fan chart right that's not a that's not a confidence band for predicted value that's the entire range of uncertainty that includes not just your predicted value and its uncertainty But it includes all of the other bad things that could happen in the future right so conference means for predicted value of course of some interest but in forecasting we want to have the distribution the conditional distribution of why uh in the in the future which is not just the estimation part part of it but it's the bad things that can happen okay it is possible in principal components to handle data irregularities and there's some uh I believe this has actually been used once or twice there's a some details about how you can do that in an appendix of one of our papers although arguably the data irregularities are handled probably more systematically certainly more elegantly in in the common filter set up and they can be used in the common filter setup in this real-time way okay so that's principal components now principal components there's some other methods that have been proposed so let me just go through those other methods that are in this family these other methods are so this is this is easy to this is really easy to understand so I motivated principal components not motivated but derived principal components as the solution to a least squares problem okay those E's what if those E's were heteroskedastic heteroskedasticity then you might not want to do least squares you might want to do weightedly squares or if there's a little bit of correlation in the ease across with each other you might want to do generalize these squares and so that's what it does all right so that's so instead of the identity Matrix in there why don't we put in a different weight Matrix and so that's just GLS and it turns out this you just go through the algebra this is also an eigenvalue problem it's an eigenvalue it was a generalized eigenvalue problem and the only difference is that now we're looking at um the first our eigenvectors not of the X's but of this GLS transformation transformation type thing and so these are the uh what's called generalized principle components with respect to this weighting Matrix um that what I just gave you was infeasible because that assumed that you knew this um the tricky part here is thinking about the best way to estimate these and the reason I say you normally when you you say you know you go through feasible GLS and you say now you just estimate it and you move on it's not totally obvious in this circumstance because this is an N by n Matrix this is one big Matrix all right so an nbine Matrix has order N squared elements in it and if you've got a 100 series you're talking about a serious number of elements in this Matrix so there's a couple of ways that you can kind of get at this and they're discussed in the literature probably one of the simplest ideas is impose the exact dfm structure so in the exact EFM structure these ease are uncorrelated with each other and um and that is that this is a diagonal matrix and you've changed it you've since you zeroed out all of those off-diagonal terms you only have N Things to estimate and so that's easy and bovan and ning suggested doing that there's some other methods uh that have been proposed uh um there's a particular procedure using Dynamic principle components um that these guys proposed in 2005 uh to come up with a weighting Matrix um approach we looked at uh so if you think about this as a GLS problem we're only doing G where's there's actually there's actually three dimensions in which you want you might want to think about GLS and so one dimension you might think about it is the heteroscedasticity another dimension is the cross correlation across the ease but there's actually a third dimension which is the time series Dimension which is that in the in the exact EFM the E's are going to have some Dynamic structure so you so use if you really wanted to be serious about this GLS you'd somehow be doing heteroskedasticity and a little bit of cross-correlation and serial correlation and that's a lot and so we don't do all of that um but we do this one focuses on the serial correlation aspect so you can do a Cochrane Orcutt type transformation to the data and then use that as a way to estimate estimate the factor so there's a variety of things that are floating around out there um there's uh so that's the uh let's see um this is this is our out of order I mean I'll come back to this let me go through the slides in this in the in the right or in the order that the slides are are written and then I'm going to come back to this so a logical question a logical question is I've presented a whole bunch of different things I presented mle and principal components and generalized principal components and there's at least three flavors of generalized principal components and what is a poor practitioner to do and that slide should be next but it's actually in about seven slides so just deal with it okay all right so so let me once you've got these factors once you've got these factors um what can you do with them and going back to the original motivation is that in some sense these factors are sufficient statistics for all of the other x's and the exact dynamic Factor model so if you knew the if you trying to make a forecast if you're trying to run a VAR with 200 series but somebody said that you actually among you actually have some of those of these factors you say this is great I can throw away all but the factors and maybe you'd want to use your own the own lags but you don't need any of the other Series so you can just put them in as regressors um one of the theoretical results I mentioned uh both in the like in the stock watch in 2002 paper and The Binding 2006 paper uh is provides certain conditions under which these factors are useful as predictors um one thing that I I guess I didn't emphasize but I would like to emphasize is that is that you it's really you really have to this is really hard you really have to this is so different than almost all of other econometrics in the sense that the ends that we're talking about here are really big okay there's a large number of series and the asymptotics and the econometric theory reflects that remember when we were talking about many weak instruments all right so or if you're talking about optimal GMM estimation through sequences of things or so think about all of those all of those econometric seminars that you left in the middle of the moment you left was when they wrote up these technical conditions where we're going to look at spline estimators where there's K splines and we're going to assume that K tends to Infinity but it doesn't intend 10 to Infinity any faster than excuse me any faster than the cube root of n okay or you were at a spectral density estimation thing where they had the bandwidth tending to Infinity but the bandwidth divided by the sample size what is what's supposed to be cubed again cubed tends to zero or it's you know some kernel estimation seminar where the you know where the bandwidth of the kernel well this is going to be a little bit different but it's the bandwidth of the kernel has to tend to zero but it has to it's this is in the reverse and B and uh cubed or something like that tends to Infinity I I've probably got that one backwards okay so uh no I I gave to that I will the bandwidth one is too hard that's upside down okay so but it's the same idea or the optimal instruments we're getting back to I to GMM the optimal instruments they say we're going to come up with something that's equivalent to GMM and we're going to do it by approximating the score using some sequence of instruments that's going to give us projections and this is so cool and the sequence is going to have the number of instruments tending to Infinity but the number of instruments over uh the sample size uh cubed tending to zero and in many weak instruments the many weak instruments thing remember how I did the sequential asymptotics where you had I took T to infinity and I got my weak instrument Lambda plus z v Prime z u over Lambda plus e v Prime Lambda plus CV and then I went down here to go to Infinity taking K to infinity and I let K tend to Infinity but for that to be justified I had to have K Over t to the fourth go to zero okay so all of these things are fake Infinities okay so you're supposed to have it's like what Mark was saying this morning you know you're supposed to have bandwidths that have sort of infinite lags but they have to be infinitely small by a big ways to be able to estimate them precisely these are all fake Infinities this couldn't be more different the results for buying are N squared over T tends to Infinity in over uh N squared over T tends to infinity have I got this right let me get the result let me get the rate properly okay here we okay now N squared over T10 to Infinity okay so that's a by name rate think about that an enormous number of n so you know if you have a hundred observations you should be looking at a lot of ends now this is just how the asymptotic works and it it doesn't really tell you what the scale is supposed to be so the only way to really figure that out is to do Monte Carlo simulations and when you do Monte Carlo simulations on spectral densities and you look at sizes when you use optimal Andrews things you get rejection rates of 40 percent that's what we had this morning but when you do Monte Carlo's here about whether or not it really works to estimate the factors and whether it really works to run regressions with factors as predictors and you have a couple hundred observations and 400 or you have 50 observations and 100 series it works great it works great and that's that's just a statement about Monte Carlos it's not a statement about empirical reality so it's a completely different world completely different world okay now where are we that was a digression oh yeah multiple Horizon forecasts this gets to this really interesting topic of direct versus iterated forecasts I could tell I'm sitting in the back I could tell that everybody really perked up during that part of the conversation foreign approached us with this Pro with this project we just thought this was so stupid it was such a boring project but it turned out to be pretty interesting I have to say there's two ways to do this so there's two ways to do your direct versus iterated forecasts and actually I don't think that we have a good sense we meaning the people who are involved with this empirically in the context of dfms as to which of these actually works better and one way is you could take your factors that you've estimated Say by principal components or dynamic principle components or maximum likelihood excuse me by maximum likelihood or generalized principal components or principal components and you could use them as regressors in an H Step Ahead regression Nothing Stops you from doing that you can include lags of your own variable this is what most of the literature does another thing you could do is you could just do one step ahead and then you could iterate and this kind of has a VAR type flavor it seems to be restricted because there's no feedback from the X's to the F's there's not supposed to be by the way right because if you know the F's only you don't need the rest of the that's the point right so you don't need all those x's in there but you could iterate this and I don't think we have good empirical evidence one way or the other as to whether one or the other of these is better this is almost this is the majority vast majority of the empirical work uh one if you're doing this thing in the context of the I'll tell you one disadvantage of the direct forecast that I don't think Mark mentioned is that every Horizon is a different is a different forecasting regression and that means you have this it's not completely internally consistent it's not all generated from the same model it's there's all these different models and one nice thing about this is that this all from one model and if you're doing this in the common filter it all just naturally is going to come right out so I think that's something that somebody wants to write a really uh fascinating uh paper uh can investigate whether the director iterated forecasts are actually working better empirically okay and you can evaluate these using the data sample methods all right let me talk briefly about something called Dynamic principle components so one of the things that was developed okay so here's the remember the chronology uh gave Ricky and his thesis did this all by Spectral method Sergeant Sims did it by Spectral methods problem with the spectral methods is you could do things like try to figure out the number of factors but you couldn't actually estimate the factors you couldn't use them for forecasting all you could say is at the bottom at the end of the paper you say so the rank of the spectral density Matrix is two which is not you know it's not the most interesting thing to say it's all right but there's more you'd want to say so then you go to the time domain things and the mles the mles kind of hit a wall computationally you know people are thinking about other stuff some of them were principal components but one of the other things that was thought of by the um for this Forney Holland Lippy Riceland group the Euro collaborators was using a device that was developed back in the 60s by David brillinger and and that's called Dynamic principle components a dynamic principle components is actually principal components analysis done in the frequency domain and so you estimate the the spectral density Matrix now as a function of Omega of the entire x's and then you do principal components frequency by frequency because at each frequency we saw that there's this spectral decomposition that says there's a all the common the cross things have this reduced rank Matrix plus then this diagonal piece and so you can that's the structure of principal components in the time domain in the classic psychometric setup and so instead you just flip it around you do it in the time domain in the frequency domain and that's going to give you these so-called Dynamic principle components and then you can do some inversion and actually get back stuff in the timed in the in the time domain by doing um since you've essentially so one way to think about that is that this is not the way it's actually done but this is a way to think about it Mark showed you that you can take those matrices that he called H and you can factorize the entire covariance Matrix of the data so by and those H matrices were the Discrete Fourier transform so you go to the frequency domain so you could go H inverse and going from the frequency domain to the time domain so once you've parameterized the um the entire article variance function in this reduced rank way in the frequency domain you can H inverse everything you have the time domain uh full covariance Matrix of the entire data and since you if you then make assumptions of normality once you've got all of the data and the full covariance Matrix you can come up with any conditional expectation that you want and you can think about expectations of the fact now that's not actually how it's done that's just how I think about it but that's not that's not how it's done in practice the problem with this method and I think the reason of the method isn't used Much Anymore uh is that um is that uh It produced two-sided estimates of the F so if you want to think about any application where you'd want to have a one-sided series like forecasting or uh uh any you know any any application like that uh it's you're not going to get the you're not going to you're not going to actually get this one-sided um you're not going to get the dating right it's going to include future information and that's problematic for certain for most applications um it is the case that these um that this factorization can then be used to uh produce them uh to provide potentially improved uh generalized principal components estimates and that really whether that helps or not uh is uh is essentially a Monte Carlo uh and uh empirical question which we'll look at okay in fact now here's the slide that I was looking for which is what should you use so what's the poor practitioner to do we've got a whole ton of things we've got mles principal components generalized principle components of which I've listed three flavors and there's actually more flavors than that and so what's the poor practitioner to do and that's a a good question there's a little bit of theory okay so here's piece of theory number one piece of theory number one is hypothetically let's just suppose that you knew to suppose that you knew the parameters of the dynamic Factor model okay just suppose you did then the optimal thing to do is actually quite straightforward at least in the exact dynamic Factor model you should estimate the minimum mean squared estimate of the factors and that's obtained by running the common filter right so it's clear that the common filtering approach is the best way to estimate the factors in the exact dynamic Factor model that's assuming that you know the parameters there isn't any theorem that I'm aware of that says that the best way to fit the best estimates of the F's are obtained by applying the common filter using the mles it sure stands to reason that that's a sensible thing to do but there's no theorem that says that the only piece of evidence that we have at a theoretical level is a comparison by Choi uh where he computes the Compares the asymptotic variants of principal components to the asymptotic variances of generalized principle components under um under the exact dynamic Factor model structure so you assume exact dynamic Factor model and then on the one hand you do principle components on the other hand you use generalized principle components and he does it for the infeasible principle components and he finds that you get a smaller covariance Matrix I don't think that you you get you get more precise estimates using the the generalized principle components I don't think that that's a very surprising result because if you knew remember the generalized principle components uses the covariance Matrix of the errors inverse if you knew that then sort of it seems kind of obvious that that's what you'd want to do to estimate the parameters and so it's not a big surprising result that you would get those improvements I think it's much less obvious because that's such a huge matrix it's less obvious that if you estimated it you would necessarily get those improvements I think it it's I suspect that this work this this is not impossible I suspect somebody could do this that's a bit somebody could work out the algebra and I think that would be a great project for a diligent graduate student okay it's a good question there is some simulation evidence a lot easier than the algebra is the simulations and Troy produces uh some and although his theory only does for infeasible GLS he actually produces some results for feasible GLS also and uh what he finds is that um that for feasible TLS it's one of these things on the one hand and on the other hand and depending upon the design depending how big n is sometimes it's better to do principal components and sometimes it's better to use uh feasible um uh feasible GLS and there's really no clear bottom line from those simulations there's a nice simulation study in this dose genonian Riceland paper and they look at principal component and principal components um oh I didn't say this this is a really cool idea that that makes just so much sense in in that that that's developed at some of the in some one of the Euro peoples I have a reference in the slides one of the Euro groups papers which is just doing a what they call a two-step all right the two-step is you estimate the factors by principal components you then regress the x's on the factors to get the lambdas you take the residuals you get the E's you with the E's you can buy OLS you can estimate the auto aggressive coefficients in the F if you've got the F's you can regress the F's on the past and you can get the factor State Dynamics you take all those equations you plug them in those coefficients you plug them in and now do a single pass of the common filter don't do any parameter estimation but just do a single pass of the common filter to get an updated estimate of the F's right so that's really easy because you're not doing any non-linear optimization for the parameters and the one pass or the common filter is no big deal and so that's something they look at also and then they look at the full mL of everything using an em algorithm and what they look at as this measure is a measure that we've looked we looked at before in some earlier work and a lot of folks have worked looked at which is basically if you have a if you have a bunch of F's uh what you'd like to do is you'd like to you'd like to have the pre the estimated F's equal the true F's and you can think about projecting the estimate the true F's on the estimated F's and then sort of this is sort of like a Trace r squared for a regression of the of the of the true F's on the estimated F's and you'd like this r squared to be one and um okay so so what do we have here and now we're looking at ratios of Trace R squareds so this is a table from these these these guys Dosh giannoni and Richland good and so this is the ratio of the trace r squared for maximum likelihood to the trace of squared for principal components for one of the models that they're looking at and the really interesting result for this model is that the principal components and maximum likelihood are giving just about the same Trace r squared okay this was a ratio of Max from likelihood to two-step and you see you so for n equals 10 . so this this if this makes so much sense look at T equals 100 and n equals 10. all right so T equals 100 and n equals 10 is kind of not quite you know for the old literature it was like n equals five but this is kind of in the flavor of the old literature and maximum likelihood is doing better than principal components and in fact maximum likelihood is doing better than two-step but as you start going to infinity or getting bigger principal components is doing just fine and there's they have other simulation results and they have this sort of the same flavor in their other simulation results so um I take that as evidence that um I mean the ml is clearly giving some improvements there's no question about that for the small ones it certainly is never hurting anywhere on the other hand the the actual gains when you have a large number of series seem to be modest in these simulations and that you know this is just a simulation so it could be more substantial in other simulations okay so both van and ning have uh have um also some simulation evidence um comparing uh printable components to generalized principle components and so I have a table of results and so the columns to prepare to compare is this I don't know what I don't this column here so I looked in their paper and this is the column first for principal components with an unrestricted regression divided by an auto regression as a benchmark and this is generalized principle components uh using this is the particular covariance Matrix scheme they're doing is just the weightedly squares version the head of statisticity version and if you compare these columns so and one at One Step at let's look at four steps ahead uh in this simulation that's a calibrated one off of real data uh 0.64 means that you're doing much better relative to an auto regression by using this particular procedure you're getting improvements for about 40 35 percent and if you use generalized principle components it's 0.67 if you use regular principle components it's 0.64 for income personal income it's 0.69 versus 0.69 for msmtq which must mean something uh it's 0.69 to 0.7 I should know what that is manufacturing and trade sales um LP nag is non-agricultural employment and you get just a tiny Improvement by using principal components you know if you look at this oh here's a case where using regular principal components at the six-month Horizon for personal income isn't as good as using the dynamic generalized principle components basically there's no difference okay so in their simulation you know you can split hairs and maybe look at second decimals or third decimals but basically there's no difference okay um so that suggests that principal components and generalized principle components make really pretty modest difference I I'd say that in our work we we would basically agree with that um there's some empirical evidence um the broad summary of the empirical evidence is that these things really have fairly similar performance the different flavors of principal components I guess one when one of these when these so this is more an impression on my part rather than this is an impressionistic which is it seems as though in those the versions that are estimating a ton of parameters for the feasible GLS that those can be prone to outliers in both directions sometimes particularly good but sometimes particularly bad now when I say particularly it's it's it's it's it's not we're not talking about huge outliers we're talking about you know two standard deviation blips in in you know 0.89 compared to a 0.94 or something like that but but there seems to be more variability in the generalized principle components approach um here's a here's one nice picture and so this one now is comparing uh principal components versus the filter this is one realization from a paper by recent Watson which is comparing one uh one realization of a filtered ml estimate of a sing in a single Dynamic Factor model with a bunch of price series to a principal components and the one reason this is kind of interesting methodologically is that these as you know price series are incredibly noisy there's and so the r squared the fraction of the variance that's explained by the principal components and any one of these regressions is reasonably small so you can think about this as not being a particularly dominant factor and so what what you see here is supposed to be in color does that appear in color not really well there's a smooth line and there's a bumpy line okay so if you're still awake do you think the smooth line is from the common filter or there's a bumpy line from the common filter the smooth line is from the common filter all right the thing that the common filter does is it does the cross-sectional averaging but actually does a little bit of temporal averaging as well and that's going to give a smoother estimate of the common factor and just sort of looking at this a common factor that has that smoother estimate kind of is is sensible whether that translates into forecasting improvements I think is a question that you know we'll just need the the we broadly Define need a little bit more experience on but it but it certainly in this example looks quite promising uh to be giving improvements okay so this is a good place to break so we'll come back and talk about selecting the number of factors or estimating the number of factors the smallest and they look like this and then you figure out what's big and what's little and just by looking at it and then you decide that there's seven big factors okay so that's one way to do it and it's not useless it has no it until very recently had no distribution Theory associated with it um uh so I'm not going to go into any of that let me talk a little bit about some methods for estimating factors so there's a the main procedure that's used right now is something called The Binding information Criterion and looking around this room I think for the benefit of being expeditious since I'm left with mainly the hardcore econometricians um I'm gonna the next oh [Laughter] I have I have a long series of slides that's for general purpose edification which is what's an information Criterion and I was going to relate that to Bayes and akiyuki information criteria in terms of estimating the lag length in an ARP and and so I will give you the very brief version of that and information Criterion so suppose you don't know you don't know P you want to run an auto regression and so here's the game we're playing so the game we're playing is that you know it's an auto aggression you know it has finite order the order is no more than a certain number seven and you just don't know what it is it's less than maybe it's seven maybe it's six and you want to figure that out and if you do sequential testing like if you use five percent significance testing if you go from the beginning to the end that's a bad idea because like you could have an auto regression where the first coefficient is zero and look you know you never get beyond that and you could uh you go from the other direction if you're just doing hypothesis testing you've got a type one a type 2 error that's going to end up not going away so information criteria get around that and provide a way to provide a consistent estimator of p and the way it does it is it says I'm going to ask you to trade off uh if I look at the lag length P I'm going to provide you with a with an objective function and the objective function is going to do two things I'm going to say that as you increase P You're necessarily going to improve the fit the r squared has to go up the sum of squared residuals has to go down so you're necessarily going to be improving the fit but I'm going to penalize you and the way I'm going to penalize you is add in a penalty factor that is increasing actually I wrote it drew it as a curve but it increases linearly in these as you add as you add um as you add p as you add more lags and if you then add these two together there'll be a minimum and that minimum is going to be your estimate of uh is your is going to be your estimate of the uh of the uh of the lag length and so the whole trick is figuring out what the slope of this penalty Factor should be and the theory of uh there's the the the the theory basically says that if this penalty Factor goes to zero but it goes to zero slowly then you're going to get a consistent estimator and so the way that the proof works is that you look at two cases the case that you under predict and you show that that can't happen well why couldn't you under predict well okay so we'll to under predict what you'd have to be doing is giving yourself a really big penalty because by under predicting you you're you're necessarily losing some gain from the sum of squared residuals uh being decreased so you've got to be using a really big penalty factor to underpredict so if your penalty Factor goes to zero you're not going to underpredict uh to overpredict to over predict is kind of more subtle because you know you'd love to over predict just by maximizing the r squared and minimizing the sum of squared residuals to over predict for that not to happen well you have to be using a penalty Factor it can't be too small so the penalty Factor has to be Well it can't it can't be too small and and and if you choose it just right then um then you're not going to over predict her so there's a proof in an in an introductory econometrics textbook for undergraduates that does that okay um so The Binding Criterion is exactly the same thing which is which as it says I'm going to look at the log of the sum of squared residuals divided by the sample size with a penalty Factor log of sum of squared residuals is a little more complicated because it's not just a regression we've got this big Matrix X and we're fitting everything to the Matrix so the log sum of squared residuals is going to be well you could do it as a Trace Way Or you could do it as a sum over t and a sum over I of fitted values of uh of given given residuals and so but that's basically the sum of squared residuals from uh from doing uh this fit so it's not by the way the generalized least Square sum squared residuals it's the regular OLS sum of squared residuals and now you got to add a penalty Factor well this is a bizarre penalty Factor the Bic penalty Factor was log T over t which goes to zero but it doesn't go to zero too quickly this is very strange this is n plus T divided by n times T times the minimum of N and T and you say well where does that come from and it actually turns out to be quite clever and what it's supposed to do is is it handles n and T getting big at lots of different rates if so if you but n and T both have to get big for this thing to work suppose that n is fixed then this is going to then this is going to cancel and this is going to get big at rate T and it's not going to have a penalty factor that goes to zero if you want to plug in a case where n equals T then you work it out uh oh I'm sorry oh wow I have an important typo right here this is an important typo ready I'm I the whole purpose of this was to give you the formula for what seems to be in practice of the various information criteria that have been performed produced the one that seems to work the best in Monte Carlo simulations and that's by and ning's icp-2 and I typed it wrong if that's bad okay it's got a log in there this is it okay it's right on the next page here's the intuition for why this actually is pretty sensible Bic works pretty well I mean that's the recommended procedure for an auto aggression if you don't want to overestimate AIC it'll give you some overestimation it actually AIC overestimates with finite probability asymptotically suppose that n equals T then we have 2T here and we have t squared down here so that's going to be 2 times T over t squared which is 2 over t n equals T then the minimum of N and T is T so it's going to be 2 log T over T 2 log T over T that's b i c that's the Bic Criterion okay so this actually is pretty sensible it it's very much like Bic Bic works well and this seems to be the one that works the best in practice now that said in practice different information criteria can yield really quite different answers so and there's some delicacy and you have to use judgment so like any any empirical work you have to you know think about what you're doing not just not just mindlessly apply a procedure and then when it says that there's 29 factors you just sort of say okay and and estimate 29 factors I've never seen it say 29 factors actually I haven't seen it say nine okay which might be sensible or not okay um and let me make a brief aside about estimating the number of dynamic factors that was estimating the number of so-called static factors which is the number because this is this uh P principal component's objective function the number of static factors is not equal to the number of dynamic factors in fact if you remember the number of the way we derive the static factor form was by stacking the dynamic factors so in principle there's quite a few potentially quite a few fewer Dynamic factors and static factors maybe maybe not but it depends on the Dynamics so for some purposes you might want to estimate the number of dynamic factors um basically so here's the basic idea of how you can go about doing this this is the static factors suppose that you actually knew the static factors the F's if you ran a regression of the F's on their lags you're going to get some residuals from that VAR that VAR of F's on its lags is going to have some residuals the rank of those residuals is actually going to be the rank of the number of shocks or the dynamic factors so the intuition here is take the principal components run a regression on their lags and then look at the rank of the residual Matrix there's a number of different ways that can be implemented on Amazon Watson have one approach to doing it by a ning have another approach to doing it there's a paper that came out in jazza that has a completely different uh it's the same fundamental idea which is a ranked efficiency idea but it's implemented in the frequency domain um I'm just going to refer you to those we did in some work that we've done we've done a bunch of Monte Carlo simulations on these uh and we didn't think that any of these methods particularly worked very well um but uh I don't know Mark liked Eminem Watson all right so all right um testing let me say just a couple of words about testing this is actually it's this is an interesting digression ah interesting to some econometric theorists this is actually an ancient and a very difficult problem so um one question that you might ask I just gave an information Criterion approach a different approach go back to the auto regression problem one approach is estimate the lag order another approach is simply can you test the hypothesis of three lags against the alternative of four lags or zero lags against the alternative One lag that's a well-defined problem we know how to do that using T statistics and F statistics and all that here you can ask that you could ask what's the can we test the null hypothesis of no factors against the alternative of one or can we test the null hypothesis of one factor against the alternative of two and it turns out that's actually a really difficult problem if the data are exactly normally if if the data or IID normal so if the X's are IID normal then the X Prime X Matrix is going to be a wishart of degrees of freedom of of wishart that has the dimensionality of the number of series N by n and of degrees of freedom that's uh T so that's going to be a that's going to be a wishart if I'm sorry if the X's are high ID standard normal then that will be a wishart matrix there's well-developed theory in Anderson 1984 summarizes all of that multivariate analysis on the distribution of eigenvalues of wishart matrices and so that's well understood the trouble with that is that the assumptions that I just made IID standard normal is not at all of interest in the Practical applications that were interested that were that we're looking at and and those distributions that exist are actually very delicate to that normality assumption there's no asymptotics until very recently and recently it turns out that something that these theoretical physicists some some theoretical thermodynamics did in 1994 Tracy and whitham applies and so some guys at Stanford Johnston and then a student El karui were able to generalize the Tracy whidham distribution what's the Tracy Witham distribution the Tracy Whitman distribution is an asymptotic distribution for the maximum eigenvalue of that wishart and it turns out it goes to some really bizarre limiting law called the tracing Tracy whitham law and and they were able to generalize that recently onotsky has tried to apply these to the problems Alexia natsky at Columbia has tried to apply these to problems of dynamic Factor modeling where we now have a much more complicated setup than just this IID gaussian thing where there's going to be a variety of potential correlations and uh and so forth and he's made a couple of important steps and one of the steps in a paper in 2007 which is very cool which he solved a classic problem which is in in the cross-sectional Factor model what's the distribution of the screen plot right and so he's he solved that what's the distribution of the of a group of K largest eigenvalues of uh of of uh in the classic static Factor model that still isn't quite good enough for the dynamic Factor model applications there's a paper of his that has a test it is I think this test is not ready for prime time but it's an interesting line of research and I think it's promising it's and we'll just have to see how this plays out but it's potential additional tool not really ready for practitioners at this point but it could could prove to be a useful tool in addition to these um in the in addition to these uh um information criteria of binding okay foreign talked very briefly in the interest of time I'm going to skip some of this material because my guess is that this group is more interested in you know what I'm going to do I'm going to actually go to the stuff I'm going to reorganize this talk because I I didn't sort of think about this in terms of people trying to catch a plane I went through all of this stuff at the beginning about empirical bays and that was actually going somewhere and this is where it's going so I I said at the very beginning that Dynamic Factor models um are just one of a possible set of ways you could approach this that there's a whole family of different procedures and that the whole reason of going through that empirical Bays was there's very sound theoretical reasons to think about alternative procedures not just dfms so before one just says dfms are great and you go on your way it really is incumbent upon everybody to look carefully at all of these Alternatives that are theoretically suggested and so this whole set of slides has to do with forecast based on Bays and empirical Bays methods and um I will come back to that because I think this is quite important and in the bottom lines it ends up making Dynamic Factor models looking uh very uh very promising so but I'm going to come back to that empirical evidence on Dynamic Factor models and their forecasting performance and their fit uh which is a substantial amount of work but I'm going to do that at the end because I have a feeling that there's probably a fair amount of interest in talking about applications of dfms first of all as uh in the context of uh structural Vector Auto regressions second in the context of IVs and third in the context of dsge estimation so I'm going to talk about those three topics and then I'm going to return at the end to topic five is not exactly The Logical order a logical order here is before just jumping on board the dfm bandwagon here we really should test it against a lot of other things it turns out it does really well okay so we'll come back and we'll give that evidence okay favar so so far what I've been talking about is using dfms uh solely in the context of forecasting we've been interested in the space span by the F's and if we had the space Band by the F's we can use that for forecasts and and and and and I mean that's a very interesting application if you think about it we can have internally consistent forecasts for a large range of different series we can do it on a daily basis or a weekly basis you can have a completely consistent updating system when review data revisions come out and it sort of incorporates all of this stuff in a completely coherent way so that's very appealing um and it's I think that's a really big Triumph of extensions of Time series technology in in without imposing any structure at all um it is that said many of the questions that one is interested in a structural questions such as the svar impulse response question about the impulse response with the Spectrum monetary shock okay so you remember all of those critiques from yesterday about svars and favar is our attempt an attempt to get at some of those critiques so um you know how Rudy Bush and others pointed out that the FED uses more information than is in a standard VAR uh and that's just true it's completely true there's this question about the inverte ability problem whether or not the space of the structural shocks are spanned by the space of the Innovations and then um and then there's the question even if only you want to use a VAR for forecasting you might want to be able to use a extension of a VAR with lots of variables but not necessarily in the dynamic Factor structure and so in an attempt to address these different concerns uh has has been to try to take some of the technology from Dynamic Factor models and pull it over into the svar literature so that there's a bunch of papers that have done that they all differ and there's other ones I'm sure that I haven't listed they all seem to differ in the details of the implementation I don't think there's any any when you sit down to the level of the specific steps in the algorithm or the computer program everything is a little bit different I don't think this is a convergence on a single method what I'm going to describe here is something that's very similar to the original the key paper in this literature which is Bernanke uh bovon and giannoni uh and um oh and excuse me uh Bernanke bravon and Elias BB e uh and um and uh not every detail is the same in the way I'm describing it as in BBE but this is the general idea all right so okay so here's what we want to do is we want to identify a structural shock and we want to use this uh to produce impulse responses for a large range of variables so we got to deal with the identification problem so we somehow need to formulate this in a way that we can attack the identification problem you can't just like throw in the F's as regressors the F's are not identified right the F's that we've been using so far are unidentified they're just there's an arbitrary normalization in in the way they've been constructed by principal components that it's not a kolesky factorization they're orthonormal by construction but it's because of this eigenvalue thing I mean it's totally arbitrary right it's a totally arbitrary normalization we've got the space Band by the F's but we sure don't have any structural shocks so we're going to want to if we want to think about those apps in a meaningful way we're going to have to somehow do some identification the SVA the favar approach let me just step through this let me step through it all right so here's and I'm looking at a dfm with first order Dynamics and so I it's just easier without lag polynomials all right so first order Dynamics so the the static factors obey A first order Vector Auto regression these are the these are the structure these are the shocks uh let me use the better word these are the Innovations in the dynamic Factor model the dynamic Factor model Innovations there's only R of the there's only Q of those but there are static factors so G is a r by Q Matrix which takes the reduced number of dynamic Factor Innovations and turns them into Innovations to the static factors the static factors then enter into the X equation and I'm going to assume first order Dynamics for an ar1 for each of the E's so it could be higher order the the fact that these are ar1s and VAR ones makes no difference at all but it just makes the the exposition simpler okay so what I'm going to do the first step is I'm going to turn this system here which is a state-based representation into a VAR something X here here's the X depends on current F I want to get rid of that and I want to just have it depend on shocks and lag stuff all right so I'm just going to do a little bit of algebra the way I'm going to do that is I'm going to first quasi-difference the x's and then I'm going to substitute this uh the F equation in and then I'm going to rearrange everything and then I'm going to stack it with the F equation and here it is so it says that the F's depend on lag F's with this F this is this first equation is just the state equation for the F's the second equation is it's X depends on lag depths and lagged X's and then some Innovation type thing so this is the Innovation this is the The Innovation the disturbance type things this is the innovation in the factor equation and this is the an error term or a measurement error if you will in the in the X equation all right um so so okay so now how does this relate to usual vars um the uh residual or the innovation in a conventional VAR which is the projection of uh of uh of Exxon itself well throughout all of this like I did at the beginning I'm going to assume that the factors are observed you know why am I observing this factors are observed because we can actually estimate them consistently so for the sake of this argument let's just imagine the factors are observed so the projection of Exon itself is equivalent to the projection of all of X on the factors and itself and so um if that's just going to be these these bits here is going to be Lambda times uh G ETA t plus uh plus Zeta T where the Zetas are the disturbances to the idiosyncratic process okay so with that setup we're now ready to ask this identification question and the identification question is what's the relationship between the Innovations in the dynamic Factor model and the underlying structural disturbances to the dynamic Factor model this is the exact same notation this is now an ADA instead of a u but this is the same business as yesterday when we were talking about svars what are right so that's the identification question and at this point you can go start talking about identification in a variety of different ways but the way that um so I'm sorry let me postpone that just for a second if I once I know that rotation Matrix r i could then compute the impulse responses so I substitute everything in and I just uh and I and I just write things out in moving average representations and this is going to be the impulse response uh lag polynomial or this is just going to be the impulse response Matrix uh that that's what I want which is going to be the effect on the X's of an Impulse response to the epsilons okay so um lag so I just wrote on the formula in case there's lags there and so the main issue is what do we do um in terms of identification and at this point actually I think the easiest thing of all to see is this idea of Heralds which is I've got this system now and I can actually I I could say that these are right this is this has a covariance matrix as the identity Matrix I can identify the space spanned by the Adas so I can come up with R that's going to be a kolesky factorization of the covariance Matrix of the etas and I can just go through and I say I only want those R's that satisfy certain sign restrictions okay so the the easiest way to think about a notification here without having to do any complicated projections or anything it's just to say I'm going to go sample R after R after R that's going to be a rotation of a clusky factorization and I'm going to keep the ones that have impulse responses that look good that's probably not a fair way to say it but that satisfy a priori theoretical restrictions but actually you can you can work through the algebra and you can do header Schedulicity here too if you wanted to now what BBE do I actually haven't worked through the algebra and long run restrictions I bet you could do that but I I haven't done that algebra what BB and E do is they do a timing scheme a slow path a slow fast timing scheme just like we set out in in just like set out in the handbook chapter by Cristiano rack and Bowman Evans let me talk about that just briefly okay so here's the here's the idea not not the this is not actually how they implement it this is how I understand how I would implement it which is you categorize the variables into three groups into slow variables a policy variable and fast variables and um and then the factors are completely unidentified but what you would posit is that one of the factors you posit that one of the factors uh is actually a monetary policy Factor that monetary policy Factor isn't necessarily observed there's some measurement error on it so what you'd like to know is you'd like to know the ETA T of the monetary policy factor with this fast slow identification scheme some of the factors are going to be loading contemporaneously on some of the factors I'm sorry the the Restriction is that the monetary policy Factor does not load on the slow-moving variables so here's the idea you take the VAR residuals for the slow the implied residuals from the structure for the slow-moving variables you do a reduced rank regression of those onto the space spanned by the Adas you then take uh you then now you've got the space Band by the 80s and uh space Band by the eight is and also the space Band by the slow moving variables you can then use the exclusion restriction that interest rates do not the interest rate shock does not enter the space span by the slow moving variable Adas to identify the interest rate shock in a completely parallel fashion as to how you would do it in a regular timing the AAR and therefore you have the Ada TR and so you've got Ada TR and then you zap it through so that's in words that's not exactly how they do it but that that's that's described in more detail in a stock watching 2005 paper okay and the thing is like you get oodles and oodles of impulse responses out and also what's kind of interesting I think this is a good question and the whole what part of the point is that you're using a lot more information so you should get more efficiency and maybe that means that the fact that they have tight conference bands here is actually correct uh I mean it it it it you it's really it's plausible that you're going to get more efficient estimation of all of these things by using a much bigger system and you could just go on for pages and pages in terms of impulse responses uh because you've got so many variables in the system that you can actually tell a story about what's happening to every variable that you could you could want okay one other important Point that's I think quite nice about this structure is that we're used to thinking about uh structural Vector Auto regressions that are exactly identified this is way over identified there are a ton of exclusion restrictions because there's a ton of slow-moving variables and we're saying that those interest rate shocks don't enter any of those moving variables so you can go through and you can see well do you does that look credible or not I mean you can estimate you could put those in the regressions you could see what the coefficients are you can do J tests but you can also just see is this a big deal or not do we believe these exclusion restrictions so that's kind of a nice feature that you could do svars that aren't exactly identified so there's a lot of nice things about this actually very intriguing okay factors as instruments let me just make one quick other comment about fave ours which is as this list indicates I think there's a lot I haven't seen a single I haven't seen any paper that's done any of these three items so I think there's a lot of interesting things to pursue here oh there is one with sign restrictions who's okay is this an uncirculated paper so so it's all right that I didn't know about it okay okay there will be a paper on sign restrictions in five hours good have a good forecaster okay factors as instruments foreign in some sense this is all going to be just an application of theory that's already been presented remember in the stock watch in 2002 and then refined in The Binding 2006 paper theorems were proven that indicated that you could use the factors as regressors in forecasting equations without having to worry about a generated regressor problem basically or estimating them well enough that you could just throw them in as data and well a forecasting equation is sounds a lot like a first stage regression and so if you can do it in a forecasting equation you ought to be able to do it in a first stage regression and so that's that's what this literature is about this is not uh actually all that new an idea so I understand like I understand that this was done like 30 or 40 years ago by this simultaneous equation folks who would have lots of instruments and they didn't know quite how to use them all so they would compute a principal component and they would just use that as an instrument so there's no Theory or anything like that but it's not it's actually not new to to knowledge the certainly the proof about how this actually works um is is new to knowledge but it's um but it's it's not exactly a new idea um one of the interesting uh one I mean so the whole point of this I mean going back to weak instruments the whole point of this is you know we're not so in in some applications some applications instruments are strong and GMM works great and we're happy but in some instruments some applications we just don't have strong instruments and in those circumstances you can there's some technology that's available for improving the quality of inference with weak instruments but it sure is a nice idea to try to get better instruments and so that's what this is about maybe this will produce better instruments uh Stronger instruments um the uh the main theorem uh which is in both of these papers there's there's two independently written papers that were written apparently within a week of each other uh by capitanus and Marcelino and Bayan ning and the main theorem that they both prove is that you get first order asymptotically equivalent results under the usual conditions with many uh regress many instruments uh that these are going to be uh it doesn't matter whether you use the true factors or the estimated factors and you know before we were talking about the many weak instruments problem and we're talking about many many instruments like case 10 to infinity and K Over t to the fourth tends to zero we're talking about tons and tons and tons of instruments here but the reason this isn't a many weak instruments problem in that technology and that constraint doesn't abide is that of course the linear combination that you're using isn't being estimated in a first stage regression the linear combination is being estimated through principal components and it's it's the all all of the action is coming through the the dynamic Factor structure and then once you've got a small number of instruments two or three or four then it then then it's just standard uh it's no longer many instrument problem um okay a couple of interesting notes on this um this is I this so this is I think I can't think of an application off the top of my head for which this is a useful comment but it's an interesting comment so the interesting comment is that under this Dynamic Factor structure any individual X might not be a valid instrument but the principal component formed by the X's might be of common a valid instrument how can that be and the reason is if the idiosyncratic terms are correlated with the error in the equation of interest you essentially average out the idiosyncratic terms and you're only left with the factor so if the factors are valid instruments and the idiosyncratic term and the individual ones or individual X's are not because of the idiosyncratic terms you can get rid of that endogeneity using principal components or by this Factor method so that's an interesting point that helps you understand what's going on I can't think of any application in which that's a useful observation but maybe there is one um this doesn't saw this this the Hope here of course is that this would solve the weak instrument problem or at least it would solve it in the sense that you wouldn't have to worry about it that may or may not be true that might be a false hope uh if it's a false hope then you know if these are if these factors are weak instruments then you're still back in the first uh of the first of my talks dsges and Factor models any any questions on that okay well you know when we talked about Factor models yesterday the steps were solve it for the Euler equations log linearize solve for the expectations put it in state space form and that state space forum for uh the so so that's for example that's what Sergeant did he didn't have to log linearizes a quadratic I believe his original model if I recall correctly that states-based form can be thought of as as this well well this is actually a reduced form remember I already said these F's you could like put in a HH inverse in here so this you can think about this is reduced form I haven't imposed any structure at all on this there's some crazy normalization like you know the principal component's normalization or something like that well if you post some structure on this and you make this a structural model and maybe impose enough normalization to identify it let me call let me put tilde's on top of everything so now I'm going to call this a structural model well what does a structural model mean a structural model means that now these fives if I want to think about this as a structural model or more particularly more precisely a state space the state space equation the states the state equation for a dsge these fives are going to be functions of some underlying dsge parameters say Theta so are the covariance matrices here these X's are going to be the things that you're going to use to estimate the dsge I've thrown on some measurement error sergeant had some measurement air in his paper he said that the statistical agencies added on measurement error so you could do that or you could say I don't like that idea and you could zero out this term aside from notation this is the model that I wrote down yesterday right okay here's what bovan and giannoni did which is really cool okay football van giannoni said is everybody who's estimated dsge so far has a taken them far too literally there's this thing that they call X in a dsge there's one X in the economy and that 1X is I don't know different people sometimes somebody might think it's labor share or somebody else might think it's outward Gap or somebody else might think it's consumption growth but there's one X in the economy and there's one R and there's one pi and that's stupid so here's what they say they say there's a latent Factor X I don't know I have income in consumption I probably shouldn't have done that I should have to keep this consistent with the lecture yesterday I should have had X and pi and and okay so this is let's call this x there's one thing it's called X it's X it's X in those equations well what's X is related to maybe some income growth series or some output gaps or some consumption series or something like that there's no 1X that's the right X there's a number of things that are very closely related to it but they're just measurements that are related to it but then there's got to be some measurement error too because they're not the same thing and there's one pie in the dsge but we know that there's a ton of different pies in the real world and they're all kind of related to that pie we don't observe the dsge pi it's a well-defined thing the well it's well defined it's just that it's latent and you you know there's one R in the dsge and that's related to a whole bunch of things in in the real world so there's these latent variables that have multiple indicators or multiple variables that are related to them and they're related to them in a very precise way the way you get identification is by imposing zero restrictions in the in the factor loading Matrix this x only loads on certain variables and this r or Pi only loads on certain variables this is actually if you think about it how many identifying restrictions do you need it's going to go back to the same r or Q times Q minus 1 over 2 and this has got you know millions of zeros in it so this is heavily over identified that's a good thing right so you can check it out so that's that's what they do so they say we've got the way I would throw I don't know this is good terminology or not I call these variables that are related to I've called these variables that are related to the unobserved factors as measurements of them things that are like sensors so why do I say that it's like you've got some latent you know you're measuring a wing in an airplane and you've got a whole bunch of centers on it that are going to give different readings of what's going on in the wing of the airplane and then you've got those this one has to do with the wing of the airplane and this one has to do with the engine and I don't know okay so there's sensors and then we've got other things that are just plain old information series well what is an information series an information series is something that might respond to it might actually be based on expected future values of the F's well we can think of objects that are going to be based on expected future values of the F's like the stock stock returns so if the stock market moves it's in X it's responding to expectations at least one imagines that's expounding to expectations of things that might be happening in the future if the only things in the world that could happen are F's that means it's responding to expectations of future F's if it's a future f that is responding to the projection of the future f on today's f is going to not involve a single F but all of these things because it's a VAR so the projection of Ft onto ft minus 1 is going to be Phi F T minus 1. so we're going to in general have all of the F's loading onto the stock market so we have some sensor variables and we have some information variables and they are identified by zeros in the center variable Factor loading equation and Phi the state transition equation is a function of the deep parameters of the DSG and I think this is just such a wonderful idea I think this is such a great paper they estimated by uh you know these Bayesian methods you don't have to I mean I actually really really be interested in seeing this estimated just by ml because you know you're using now a great deal of information to try to identify this and if there's deep problems in terms of model identification here then you know it's gonna it's hard to see how you're gonna you're gonna be able to make substantial empirical improvements so in any event they have three different cases so case a is like the vanilla case where you might it's a medium-sized model it's not a small model uh so it's like they have seven equations and they just make some in the standard Pi is like GDP deflator or something like that uh and then they ex move on to 14 variables and then they move on to 91 variables a lot of which are these information variables the information variables aren't going to help with the identification of the F's but you know what they're going to do is they're going to help with the parameter estimation and they're going to help really being able to figure out what these shocks are and so what we have here is we have some point estimates for a number of different uh a number of different cases and then um and then posterior ranges of posterior distributions and uh and uh I don't know that's just what it is um I guess one of the you know it's these particular results are somewhat disappointing in that one would have hoped to have seen standard errors reduced substantially but I don't think that the fact that the standard errors haven't come down so substantially uh here um these are first of all they're not standard errors they're posterior spreads um and second of all I mean this is just the first paper trying to implement this methodology and I think there's you know a lot of a lot of promise a lot of promise I mean if the standard errors if the posterior spreads aren't coming down at all it's got to be some indication that these models are essentially um unidentified in some sense okay um so let me now go back to uh go back to this empirical excuse me some of the so this the two sections that I skipped which is section four and section five okay so so far everything we've been talking about uh in terms of concrete empirical stuff has been Dynamic Factor models and um as I emphasize I think it's it's a question as to whether or not the dynamic Factor models are actually an empirical question ultimately as to whether Dynamic factors models are the best window or the best model or the best framework to use to tackle some of the questions that we've been talking about so um what that means it's incumbent to compare Dynamic Factor models to other things there's a number of ways to ask this you can ask the question so that you can ask in terms of empirical assessment you can talk about forecasting you can talk about in Sample measures of fit uh you can talk about other Diagnostics right now I'm just going to focus on alternative forecasting methods and then we'll come back more generally to measures of fit okay so the forecasting problem that we're going to talk about is the one where we have a large number of predictors and we're going to look at the projection of why on these uh orthonormal predictors these orthonormal predictors that we're going to focus on are going to be the Principal components in the empirical application and this is the problem that we studied in terms of the bays and empirical Bayes estimation problem so the frequentest problem if you recall was to find the estimator that minimizes the frequentest risk through that series of calculations the frequentest risk is equal to the Bayes risk with the um with uh with some G sub n with the actual empirical CDF of the D's um the empirical base estimator uh one way to think about the empirical base estimator is that you try G after G after G prior after prior after prior and then eventually you find one that does a good job forecasting that's actually not how it's implemented of course but that's the the general idea um and so uh the theorem is that you do that again and again and again and eventually the G that you settle on is going to be G sub n and by empirical Bay is equal to solve the frequentest problem um okay so we're going to look at empirical base estimators and I'm also going to introduce as a general General matter a number of other um estimators that have been developed in the um in the statistical literature and again so so I think the reason I went through all of this you know if if you saw the macro seminar and it said the title was Bayesian model averaging bagging lasso and hard thresholding forecasts of macroeconomic activity most of you would skip the seminar okay the reason that this is important is that these are attempts different Frameworks that are actually at a fairly deep level closely related their attempts to implement what can be thought of as empirical Bayes estimates that would provide an alternative to Dynamic Factor models so let's think about let's think about this problem I have all of the principal components here 135 so some data sets have 135. the dfm literature has you do the following algorithm run by ning icp2 without the typo choose R according to binding run the regression of Y on just those principal components and then if there's some own lags that you need you can put those in I've ignored that in this problem that says that the dfm literature says we're going to impose the Restriction the Delta 5 through Delta 135 equals zero that might be a good restriction I mean you know so gay Ricky thought it was a good restriction and sergeant and likes that restriction but it might not be a good restriction and it's an empirical question and if it's not a good restriction and if we're supposed to use all of the data that actually is going to tell us a great deal so if it turns out the dfms don't forecast as well as these other methods that would use all of this information that actually tells us something about the limitations of dfms as a way to think about the world okay so it's a relevant question to compare dfm forecasts to all of these other forecasts that would be using the remaining 131 principal components the optimal base thing to do if you're a plain old-fashioned subjectivist Bayesian is simply to use your own personal prior G and that's a nice framework for thinking about it it's definitely going to be admissible that's the that's the the decision theoretic result that the complete class theorem if you are a lucky Bayesian and you happen to use the right uh prior you happen to have the right prior you'll also be the one that solves the frequentest risk problem a hard thresholding is a great buzzword for something that you already know hard thresholding is you compute the T statistic on a testing Delta equals zero when you compare it to a number okay so now you have another way to AIC and Bic in this context reduced a hard thresholding um let me tell you what false Discovery rate methods are very very briefly so this goes back to the 10 million probes on a chip and um so I'm just going to put State the problem the the problem for false Discovery rates is it's related to What's called the multiple uh testing problem or the multiple comparison problem if you um so when you're doing this genomic DNA stuff and you have these this is 10 million but I understand that it's actually supposed to be 30 million um probes on a chip um what you'd like to do is uh apparently these probes measure some sort of intensities and you want to find the values that are big and not the values that are small and these values that are big apparently have to do with potential well actually I know what they exactly have to do with which is when you measure the the numbers of the the numbers on the probe are related to um uh uh genes make make they normally come in pairs and if there's a problem uh if there's a cancerous problem there might be a single one or there might be more than one pair they might make multiple ones and so when you shine a light through them on this probe there's different levels of densities and intense light intensities so you can actually pick up uh whether you have one copy or two copies or three copies so what you're trying to estimate is the copy number of gene on each probe and if it's tumorous uh or if if it's defective then you will instead of just coming in pairs you're going to have multiple copies and so what you want to do is you want to identify those sites that have multiple copies and then see if you can relate that to the cancer that you're studying the um if you do it as a multiple hypothesis testing problem suppose that you look at these intensities and you construct some sort of T statistic if you will at each site and you compare each site to 1.96 and there's really no tumors then you are going to have out of those 30 million sites you're going to have an awful large number of faults negatives it's that's not that's not a helpful way to proceed so instead what they try to do is instead of controlling the uh instead of controlling the false negative rate you can control the false positive rate which is the false Discovery rate and so there's there's particular methods that are done using that the simplest of these two methods is to imagine that you have two do you have two uh you have you have either the right number of Gene copies or you have like four three Gene copies and then you're going to try to distinguish between those two and it's going to be some mixture distribution and not surprisingly that mixture distribution the decision about whether you would decide this is going to be a bad one that is with three Gene copies is a Bayes rule type calculation and how do you calibrate that you calibrate it by empirical Bay so it turns out that this false Discovery rate is an empirical Bayes procedure and the mathematics of that has gone through in an article that I haven't cited oh there's a paper there's some sites in an analyst paper by Ephraim in 1983 that that's supposed to be right here um okay so that's another procedure that you could use it's actually not at all relevant to us so I'm not going to present any results for that but it doesn't make sense in our context because our problem is just not that problem okay um there's another thing called bootstrap aggregation which has to do with I'm not going to describe it there's a paper by anui and kenilian in the jbes this year that describes that um and there's a really nice theoretical treatment of it by buellerman and you and the animals um and that is not an empirical Bayes method but it could if as I'll show you it can turn into an empirical Bayes method by a suitable calibration of a tuning parameter here's BMA Bayesian model averaging so this is an explicit Bayesian procedure um and um so so uh Bayesian model averaging was actually invented by uh Ed lemur I guess that's that's the credit that I usually see and the other reference that's often given is Bates and Granger because it turns out it's very closely related to model combination or forecast combination um this is it was sort of a Bayesian model averaging uh is here's the it was basically a Backwater for many years uh where nothing happened and then these guys invented these mcmc methods and they could suddenly solve really hard problems so it's been an area that's received a great deal of attention in the last decade or so there's a few applications to it in economics I'm not really convinced that these are very interesting the Fernandez land steel applications but but the technology is uh the technology is good the coupon Potter application is a forecasting application along the lines of what we're talking about now um so the basic idea is that there's a lot of you're trying to do a regression and uh this so here's the setup you're trying to do a regression and you've got 135 regressors and you conjecture that maybe there's a true model and the true model has regressor 2 17 and 100. but you don't know what it is okay so how are you going to come up with a procedure that's going to take into the possibility that there's a number of different models that you could be using uh and some of these models are going to be better than others well better than others means that they probably will have some higher posterior probability so what a Bayesian model averaging procedure does is in principle it estimates all possible models which is 135 choose one plus 135 choose 2 plus 135 choose three and so forth so it's quite a few models so in principle that estimates all possible models and then for each one it evaluates uh posterior uh posterior likelihood that it's true and then you take the forecast from each one and you wait those forecasts by the posterior probability and or and then that's going to give you your forecast so that's the basic idea um you can see why you can see why when it's 135 choose one plus 135 choose 2 plus 135 choose three why it was a Backwater for many years and then when these computational guys got into it they they thought this was really fun all right so so the computation about this is how do you sample models because you can't possibly go to Every model but can you come up with clever ways to sample all these different models um and they have something called mcmc cubed or MC cubed or something like that okay um so here's the BMA idea which is that you would take the um this the thing this thing called the predictive density is the density of Y given the past data so in this jargon it's y t plus one given everything that you know up to today and you're going to um you're going to average that over all the different models so you look at the predictive density for model K and then you say what's my posterior probability that model K is correct given that um what's the posterior probability that I assigned to model K given the past data and so then you the next step is you've got to figure out what this posterior probability of model K given the past data is and the answer is going to be that's going to be figured out by base uh by Bayes rule and given a prior probability of the model so given that the model is true you then need to figure out what the probability of the data is given the model is true and for that we're going to have to do another Bayesian step so the basic idea is four given model we have a prior on the parameters and then you have probabilities over models and so that's the that's the setup it's uh it's um uh uh it's something that is computationally quite tedious uh and interesting to those who find that interesting I guess that's a tautology uh it turns out that in the orthogonal regression problem it's completely simplifies because uh whether see if all the regressors are correlated then whether variables one two and three are in the model that actually make it makes a difference whether you would add four that's going to change the correlation structure but if everybody's uncorrelated then all of that breaks up so you can just look at one variable at a time so it turns out that you just get this huge simplification if you have orthogonal regressors and those formulas are given in some papers in the statistics literature which are cited in the literature survey and then each variable then the model is quite simple each variable is either in the model or not out of the model with a coin flip and then if it's in the model you have a posterior probability excuse you have a prior probability on the coefficients so it's a it's a and then what happened is that you get an explicit expression for this weighted average and you get an explicit expression for the forecast now one reason that if you read these surveys uh in um BMA they like you know the survey always has to start with some history thing and they often go back to Bates and Granger because Bates and Granger this is Bates and Granger if you think about it it says I've got a forecast I've you know I've Mark talked about this where I'm doing forecast pooling and I've got a forecast from my guy at Citibank and I've got a forecast from you know the person at Bear Stearns I said a conference I said a conference um last in in June where the guy was uh the the guy was Chief Economist for Bear Stearns which I thought was terrific it was actually he has there was a name tag which was printed up before bear Stearns croaked um so uh anyway you you get these guys forecasts and then you average them together and you average them together based on how good you think they are and how good you think they are using the optimal well using the Bates Granger weights is going to be based on their sum of squared forecasting performance and if you work through all of the BMA algebra uh in the case of orthogonal regressors that's exactly the explicit expression that you get which is that you get it's not it's not the Bates Granger expression but it's very close to the Bates Granger expression where you penalize people who have bad some squared forecast errors and give more weight to people who have good forecast errors so it's very closely related to that it actually goes back as sort of it's yet another way to think about this optimal forecast you know this forecast the straight mean pooling is a puzzle uh but but it it kind of makes you think that if historically the forecasters haven't been very different that you might want to give them weights then the in the end they might end up with weights that are pretty similar okay so this Bayesian model averaging depends on parameters of priors but you will not be surprised for me to hear me say that those parameters then could be estimated so those hyper parameters can be estimated and this Bayes procedure can be turned into empirical Bays I just mentioned is a digression that all of these procedures have shrinkage representations remember we started talking about the failure of OLS as being a consequence of going at in first principles the failure of all s being the inadmissibility of OLS and the fact that one could construct a shrinkage estimator a James Stein estimator that dominated OLS and um and all of these uh that I've just talked about have interpretations as shrinkage estimators to be precise we can write the forecast of Y as the product of the I predictor times uh the OLS coefficient times a function that's a function solely of the T statistic and this function is a function that goes between 0 and 1 at least in all the cases we've looked at in all of the cases I listed they go between 0 and 1 and so what that means is you're basically shrinking this coefficient to zero so they're all shrinkage estimators where you're shrinking the coefficient to zero but that that that that that makes that makes um that makes sense okay um so so uh and it also makes sense from a be it from Bayesian perspective remember you've got these these that are small so you're going to be doing some Bayesian shrinkage all right um I just give some formulas for what these shrinkage Things Are all right so in the finally on the list there is one large dimensional method which I'm uh which doesn't seem doesn't fit in this and that's because it's a multiple equation method uh using large vars and so there's one paper that looks at using really large vars with strong priors and my understanding is that this is uh that the hyper I mean my reading of the paper the results were presented to a variety of hyper parameters for the priors and the hyper parameters that seem to work the best were the ones that were used and that sounds to me like one interpretation of empirical uh Bays is that is that a fair assessment I don't think you use the term empirical bays in there but that's essentially what you did yeah okay all right so how do these perform so this is really the test and there's at this point there's an enormous literature with empirical evidence on Dynamic Factor models and I'm not going to pretend to give a comprehensive survey of every paper in that literature and so what I'm going to say is a distillation of a lot of different results there have been papers that have as a first cut looked at how well the dynamic Factor models describe the data in terms of you know R squares and stuff like that a second cut is testing some of the over-identifying restrictions implied by a dynamic Factor model I think if I I think if I remember one calculation if you have 135 variables and you think about a VAR with 135 variables there's something like 240 000 over identifying restrictions implied by a dfm so that gives you a fair number of things you could take a look at um the uh there's also most of the literature is focused on forecasting performance and assessment through that and I think that that's a natural thing to do and it's important there's a survey uh by Ike Meyer and Ziegler which is now a couple years old but it has a pretty good bibliography in it there's a meta-analysis in it too um I do not think this is a very useful meta-analysis so I'm just going to make one small digression on the ike Myers eagler meta-analysis they use as the unit of observation or relative mean squared error which from tables of results in these papers relative mean squared error is the performance of a particular forecasting method a principal components method for example divided by some Benchmark the problem is that different papers use different benchmarks and the um the uh the meta-analysis takes those as data and then tries to understand uh understand the the variation in these relative mean squared errors as a function of a variety of different features but since the denominator and these relative mean squared errors isn't the same from paper to paper I think that that's a really problematic a problematic exercise so it's a valiant thing to do but I'm not but but I have some questions about that particular I think it's a very hard very I'm not sure that I think meta-analyzes are Valiant efforts they're fairly controversial for the sorts of reasons that is difficult to combine studies in a formal way um okay so let me make a couple of comments I'm going to go through a list of things first of all with data selection as a practical matter there are a couple of most there isn't a lot of talk about data selection in the papers but of course in practice that's an important thing to do I think that when you're actually formulating and estimating a dfm you have to bear in mind that the factors that you get out are going to depend upon the variables that you put in if you put in entirely output series you're going to get out output factors you put in entirely price areas you're going to get out price factors it depends what your application is but you have to if you want to have a broad swath of economic variable factors if you want your factors to reflect a lot of different economic features you have to have a large number of varied different things I think one of the issues one of the one of the critiques of these papers that find a very small number of dynamic factors is that potentially or arguably the data that they're using are heavily dominated by just real activity variables um so I've talked about comparison among the different Factor estimation methods I don't think that there's a great deal of evidence suggesting that one is better than the other if I had to recommend one right now uh in terms of a combination of convenience and efficiency I guess it would probably be principal components followed by the two-step ml so that still requires you to write down the state space representation and pass it through the filter but that does give you a lot of flexibility in terms of doing things in real time and working with Messy data there's been a number of papers that have been can compare that have compared many forecast many predictor forecasting methods I've listed a few of them here and there's some other ones right um My overall impression from those uh is that by and large uh these Dynamic Factor models come out looking extremely good there's there's little evidence that any other alternative prediction method uh among these many predictor methods works better than the dynamic Factor models and most of them tend to work worse in some by a fair margin so I'm going to show you uh one just one set of results which are results from a paper of Mark and myself but I think they're representative and typical and give you a flavor of what's going on and those results um are for this is using these different shrinkage factors and this is a mechanism for comparing a large number of different alternative procedures remember how I said that those things like bagging and BMA and all those things that you wouldn't go to the seminar for all turn out to have these shrinkage representations and that means that the performance of these different procedures can all be compared just by looking at that shrinkage function and these are just different shrinkage functions functions for different procedures bic is a hard thresholding so for the data set we have lnt over T happens to be two point whatever this is 2.4 so if it's if the T statistic is less than 2.4 you don't use it and if it's greater than 2.4 you do use it so that's a hard threshold L sort of s uh an L hard thresholding function these other things more have more s shapes this thing here is for bagging which I'd never described so it's hard to understand what in the world this is except it turns out that bagging has this PSI function the shrinkage function so that if the T statistic is one that coefficient does enter the regression but the coefficient is shrunk to zero with a weight of about 0.4 here's a green one which is BMA and actually this green one is an important one this green one is empirical Bayes so there are two parameters to this family of priors uh it's a a probability P that a model that a variable is in the model and then there's something called a g prior tuning parameter and so those two priors are estimated by empirical bays and the empirical Bayes estimator says that if your T statistic is two you basically are getting very little weight and to get a lot of weight you've got to have a t statistic that's out in the four or so range so it's actually being pretty pretty tight in terms of How It's discriminating about what variables that it puts in in the model all right so um let me show you some forecasting some results and so what these results are here are their weights on the uh different principal components and the principal components are ordered in terms of the size of their eigenvalues the first being the one that explains most so like the real Factor right so it's the the first is the one that explains most of the variation and then number 121 doesn't explain much of the variation of the X's the uh the forecasting methods here are really not paying any attention remember how I said that the that the estimators are equivalent of permutation equivalent what that means is that these forecasting methods every one of these forecasting methods does not look at the index I all right it does not depend on I there's no forecasting method that says oh it's the second principle component I'm going to give it a break right it it bagging one works really terribly and it's you know it works really bad and the BMA one works really bad this green one is empirical bays and the empirical base is supposed to work well and it does work really well the green one is empirical base it works really well what is empirical base put weight on it puts weight on the first four factors and zero weight on everything else so it doesn't know they're the first four factors it just says I'm going to do the best empirical based job forecasting that I can and it turns out that this forecast is the same as a dynamic Factor model forecast that's for the unemployment rate same thing for the inflation rate it turns out the same thing is not true for the t-bond and actually the t-bond rate was very difficult to forecast in this exercise and none of the forecasts really did particularly well compared to low dimensional systems so it doesn't work universally but the evidence on sort of these real and nominal variables is that in fact it worked in fact what's being pulled out is in fact just the uh first four principal components so this is one take this is not looking at any forecast relative mean squared forecast errors like Mark was talking about this morning this is looking at something else but what it essentially says is if you ask what there's this you know really profound Theory they're saying will produce the optimal estimators they're going to solve this frequentest risk problem it turns out that it's his dfm okay um so there let me uh let me just this is one other interesting thing so this is a this is from a comment of Marx on a paper by giannoni reichling and Raceland and Salah in the macro annual uh four years ago and this is looking at the fraction of variance explained by one and two Factor models for a variety of different series and one of the things that he noticed was that this Sergeant Sims paper a long time ago uh used many of the same series or things closely related to the series that were used by giannoni Reichel and Saul of course these guys used tons of series and these this is all that these guys used but one of the things they're able to do is they were able to use their they they actually couldn't extract a factor because they're using these frequency domain methods but they can compute fraction of variance explained r squared type measures and what they found is that the r squared explained by one factor was very high for some of these things so uh you know for the for industrial production it was 94 for layoffs it was 83 percent for new orders it was 67 percent um if they increased it to two factors the amount explained really didn't go up by very much in many of these cases the remarkable feature that uh seems to be the case is that many of these results are actually still true when you look 20 years later 30 years later and look at a much much larger data sets with any more in you still see fractions of variance explained that are very amazingly close quantitatively to these smaller dfms in some cases like the the the the the the the Euro guys macro annual paper does a better job with prices they had more prices in their system than the sergeant Sims one did 20 years ago 30 years ago which didn't have prices only at One price uh little the marginal gain from going from two from one factor to two factors and this model was also a fairly modest and so they end up concluding that the number of factors number of macrofactors was very very small that uh potentially is a function of what variables were in the system you know if there were more variables measuring other things like interest rates or prices maybe you would get different answers and there are other other data sets that are given somewhat different answers to just one factor versus two-factor I would not want to emphasize the one versus two number I'm a little bit more agnostic on that but but the the indication that it's a small number is pretty uh I think pretty compelling all right so I think what I've done is intentionally gone fast to try to get everybody who wanted to go to the airport out so I'm done all right okay thank you 