Guido Imbens: The first part of this afternoon I'm
going to talk about regression
discontinuity designs. This is a very recent
thing in economics really. Although the basic
methods go back to I think 1960 paper by
Thistle Waid and Campbell. I certainly haven't seen any applications
in economics for a long time before 1997. But since then there have
been a lot of applications through empirical studies using these methods as well as
some theoretical work. What I'm going to do in
this lecture is talk a little about the
basic itself then talk a lot about what I think
are useful practices for implementing these
designs instead of what things can be done. Well, and in the end, my view is that this is actually extremely important
method in practice, one that has great credibility
when it's applicable, and one that is in fact very widely applicable in economics. Talking about the
basics, I want to talk a little bit about
graphical methods for analyzing these designs. I want to talk about the specific methods for
getting estimates, namely local linear regression there that is based
essentially on throwing out observations far away from the
discontinuity point. I'll talk about
specifically choosing that point there
somewhat optimally, and I'll talk some about variance estimation
about methods for checking the
validity of the design. When there's this
rise in practice, the idea here is somehow that the participation in a
particular program that we're interested in evaluating is based on criteria with a very clear cut off points to
particular program may only be open to people over 25, as well as be mandatory
to people over 25. In that case that the
idea would be that we just compare people
very close to that age limit on either side, and use that to get
a sense of what the effect of the program is. That's all that's known as a sharper regression
discontinuity design, where all individuals on one
side of the cut off point are participating
and individuals on the other side are not
participating in the program. Matter of fact, probably
more common for acinus, the fuzzy regression
discontinuity design where would you see at a particular value of this covariate that
the proportion of individuals participating
in a program goes up for
discontinuously jumps up, and the more
obviously, the better. In all those cases said, the idea is to
compare individuals just on either side
of the cut-off point, and when these cut-off point is clearly exogenous and not
affected by individuals, then they can give you
very credible way of estimating the causal effect. The key thing is that this may work very well in terms of getting good credible estimates of the causal factor for a very particular subpopulation, but in principle,
it doesn't really have much external validity. It's only valid at that
particular cutoff point, and the fuzzy regression
discontinuity design, it is only valid for sub-population within
that sub-population. But the good thing is that
for that subpopulation, it may be extremely credible. In addition, the reason why we've seen a lot
of applications, I think in the last decade
is there's a lot of cases where this
cut-off values exist. It's often very attractive
for administrators to have very clear rules rather than discretion about who participates in these programs. Some of the examples
in the last decade, and I think a very
clear examples of this incredible examples, is some of the work by
David Lee who looks at the effects of elections, where he's interested
in what would happen if the election results
had been different. The idea there is to look at elections that
are very close, whether the winning
candidate won by just a very small margin but the election could
have gone either way. It looks like that, for example, to look at the effect
of incumbency. There's some work
by Angus and Levy, where they're interested in the effect of school
size and exploit the rule that class size has to be less than or equal to 40, and so they compare cohort sizes close to the
cut-off point, even though that
paper originally published first note
that paper cells sort of interprets it that more as an instrumental
variable design. I think that that's
really more usefully viewed as a regression
discontinuity design. I'll come back to the
implications of that in general. There'll be more papers
where people viewed the design as instrumental
variables methods, rather than them
being explicit about the regression
discontinuity aspect of it. Another example is a
paper by Sandy Black, who looks at housing values and how they relate
to school quality. But the idea is to
exploit the fact that schools have
catchment areas, and so if you
compare individuals, if you compare housing
values on either side of the boundary of
the catchment area, you can end up with
comparisons of houses that actually
very close physically, but that correspond to different schools and you get are going to be
very credible effects, estimates of the effect of school quality on
the housing values. Last example, and I'll come back to that again as well as some one by the Cloud, was interested in the
effect of financial aid on the college admission
acceptances. Where he exploits the fact that the particular
school he looks at puts applicants in a couple of discrete categories based
on some numerical score, but the score is based
on a whole bunch of characteristics
of the applicant. But once you are in a
particular category, your chances for
financial aid are very different from individuals
in another category, even though your actual
score and so in a sense, your actual application
may be very similar. The last example is useful
because it highlighted, it's extremely important
for those cases to look very carefully at exactly what determines the assignment to the program or the
assignment to the treatment. Here it's the fact that the schools don't just look
at the overall application, but they have this sequential
procedure where they first put individuals in
different categories, and so you end up with lights
differences in average, financial aid offers
for individuals who are very close in terms
of characteristics. Let me set up the
basic framework and that's very similar to what
we looked at this morning. We look at the effect
of some binary program, then there's two potential
outcomes by 0 and by 1, the causal effect
we're interested in is the difference Y_1 minus Y_0. This a binary indicator W_i, which is equal to one of the individual participates
in the program and zero, otherwise, we observe is the
triple y_i, w_i, and x_1. The key thing here is that at a particular
value of the covariates, the incentives to
participate change. They may change dramatically
to the extent that all the individuals within value of x less than this
cut-off point C, don't participate
in the program and all the individuals the covariate value
greater than or equal to c do participate
or it may just be that they change
somewhat more modestly. What you see is a jump in the probability
of participation, discontinuity in the
propensity score at that particular
value of the covariate. Typically that's just
a single covariate that drives this in principle, could be a function
of covariates. But typically, I think, oh, application, I've seen it for the case where there's
an observed covariate. We know what the rule is, we know what the
discontinuity point is. Although we may not
know exactly what the jump is in the probability. First case, referred to as a sharp regression
discontinuity design and the second one has the fuzzy regression
discontinuity design. As an example, show some graphs actually from
this paper by David Lee, who is interested
in the effect of incumbency on the
election outcomes. The idea there is
the use of elections where to compare subsequent
elections in cases where the current election had an outcome that was
very close to 50 where essentially one was more or less random and then going
to be to compare subsequent election outcomes
compared to ones where Democrats won the first
one with the ones where Republicans
won the first one. The key assumption that underlies this is that the
conditional expectation of the two potential outcomes as a function of the covariates
is continuous in x. In some sense, it makes the assumption slightly
more functional form dependent and so really, what is more general first, stronger version of
this assumption, but one that's typically equally credible is that the
conditional distribution of y_0 as a function of
x is continuous in x and the same for the conditional distribution
of y_1 given x. If either version of
that assumption is true, then what we can do is look at the conditional expectation of y given x and take
the limit of x going from the right to
the cut-off point and subtract from the limit
of the testing of the regression coming
from the left and then look at a difference as the average effect
of the treatment. That's the average value
of y bar minus Mu 0 at that particular value of x. This is the first part where the external validity is fundamentally limited no matter how much data you've got, if there's only
one cut-off point, you're not going to
be able to say very much about the effect of the program for other values of the covariates other than at this particular
cut-off point. Now, you may be able
to do a little more. You may be able to condition on additional co-variance and get a sense of how
much variation is in the treatment effect. But in the end, all
the data is going to be informative
about effects of the treatment for individuals
close to x is equal to C. Second point is that this is fundamentally always go
to rely on extrapolation. We want to compare the two limits of the
regression function. We're going to need to use
observations somewhat further away from this
cut-off points and one of the key issues and implementing this
is trying to figure out how many observations
to use on either side and how exactly to do
the extrapolation. Ideally, obviously, we just have individuals
extremely close to x, so close that whatever the derivative of the
regression function is, it's not going to affect
things and you could just compare simple averages
on either side. But in practice,
there's often a lot of concern that you may need to use too many observations
and you may want to use additional adjustments. We'll talk a bit about that. The second example is fuzzy regression
discontinuity case. They're probably used to
fund the Cloud, for example, a couple of times to
illustrate some of the issues. He's interested in the effect of a financial aid offer by college on whether they're admitted. Individuals admitted
actually accept their offer. I'll leave it there. The inside he had
bringing into this study is that the financial aid, the procedure has a lot
of structure on it. Instead of just looking at
the overall application and then deciding on the
financial aid offer, it's streamlined by first putting the applicants in
a couple of categories. First, they assign numerical
values to a whole bunch of the characteristics
of the applicants. Based on that, they put them
in a couple of categories, and then they decide
financial aid first within these categories and the
result of that procedure is that the financial aid
offer is very highly correlated with the
category even though the individuals in neighboring
categories need not be particularly
different in terms of these scores
and so the idea is going to be to compare
the individuals were very close to the
cut-off point for a particular category
and compare them with individuals in
the other category, but more or less the same score. However, even though this changes the probability of financial aid considerably, it doesn't change it from 0-1. Individuals in
both categories or an older categories that found the Cloud looks at to get, they have some chance of
getting financial aid. We can't just simply compare the average outcomes
in these two groups, but we need to adjust
for the fact that the probability didn't
change from 0-1. In that case, instead of just looking at the
difference in the outcome, we divide that by the difference in the two limits of the regression function for the treatment indicator and this is where it gets very close to instrumental variables. We'll make that connection very explicit in a little bit. But essentially, this
is going to look like regressing the outcome on
the treatment indicator using the indicator for
x being greater than c, as an instrument, will explore
that connection for us, and then exploit it more on
some of the implementation. The first thing I'm going to do is think a little bit about the interpretation of that and relate it to the instrumental
variables literature. This is one of the contributions of the
econometric literature to this. As I said before, there's a very old
literature in statistics. One way to think about this, think for each individual about, let's do this in terms of
the financial aid example. The treatment indicator is a binary indicator
for whether you get financial aid or not. The covariate here is
this numerical score that is assigned to the
individual's application. I think here about what particular individual
would have called them financial aid if the
cut-off point for a different category would
have been set equal to x. To be able to think
about it that really requires
you to think about the possibility of changing
the cut-off point. So suppose the scores
on a scale of 0-100, maybe the current
cut-off point is 60. W_i of x requires you to think about what
would have happened with particular individual had the cut-off point being
59 or 58, or maybe 62. Assumption that we
make here is that wi of x is non-increasing and acts at the particular
cutoff point that is actually in place. It means that if we
lower the cutoff point, every individual would
have been more likely to get financial aid. If you look at it,
individual supposed to cut off point at
the moment is 60, if we move it to 59, some of those already getting financial aid still gets it, but if someone we didn't get financial aid before
because they had a score of 59, may now get it as a result of the cutoff point
being lowered to 59. In that case, you
can think about the individuals who are actually affected by
the cutoff point. I'm going to come back
to this notion that later this afternoon in
instrumental variables lecture. But the idea is that
some individuals who actually affected
by the cutoff point. Remember, in the fuzzy regression
discontinuity design here, some people will
get financial aid even with a very low score, even if they're below
the cutoff point. That there's more individuals of a higher proportion
of individuals gets financial aid if their score is above 60 than if
the score is below 60, but in both groups, some individuals
getting financial aid. The group we're most
interested in here is a group that is actually affected by this cutoff point, who gets it only because
they just ended up being on the right side of
the cutoff point, as we'll refer to here
as the compliers. These are individuals
will only get financial aid if they're lucky enough to get into
high category, but not if they were
in the low category. Let me just make
this more specific, this is a very important point for the interpretation
of these methods. Suppose that the
individuals with very low scores will
get financial aid. Get this because
they're good at sports, they get this a
separate category of Atlantic fellowships and the individuals were
below the cutoff point of 60, then will get financial aid. Do so because they are
particularly skilled in some other dimension that wasn't part of the
overall score. Then in the end, this design is not going to be informative about
financial aid for them, it's going to be
informative about the individuals for whom
the cutoff point matters. Will only get financial aid if the scores just high enough
to be in the category. This makes sense,
but in the end, depending on how
big the dump is, there may be a relatively
small part of the population. First of all, it's
essentially only individuals who score 60 or very close to 60 on
there on this admission score, but even then it's
only the individuals for whom small changes can make a big change in
getting financial aid. This is one of the main
themes of these methods, is that it may be extremely credible in getting
good estimates for a particular sub population, but it may also be a very small, potentially not representative
sub population, but in some sense, it's
all you're going to be able to get in all these cases. Let's say this is
just repeating that. Let me make a second point
and it's relating this to the first lecture
this morning. In the fuzzy regression
discontinuity design, if we look at individuals reasonably close to
the cutoff point, we do actually have
overlap in the covariance. If the design is really
fuzzy rather than sharp, there's individuals
getting financial aid in both the low scoring group as well as in their
high-scoring group. In principle, we could have done an alternative analysis along the lines of the first lecture, then assuming
unconfined in this. Assuming that conditional
on the test score, who gets the financial aid
is more or less random. There's nothing particularly
in the data that tells you that that would
be the wrong thing to do. It's going to give you
a very different set of estimates potentially. It relies on a very different
set of assumptions. I think in cases where you see where you have reason to believe. Let me back up there. In the thundercloud example, if the probability of getting financial aid at being a
smooth function of your score, I think there would have
been a natural thing to do. Comparing individuals
who get the same score, some of whom get financial aid, and some who don't. At a point where you
see that there is a discontinuity in
the probability of getting financial aid as
a function of the score, just comparing individuals with the same value of the score, but some of whom we
get financial aid, some of whom don't, it seems very
unattractive thing to do. In the end, what the regression
discontinuity design suggests that the individuals would get financial
aid despite having a relatively low score, a very different from the
individuals who don't get financial aid and
getting a low score. Now, for this
particular example, you could disagree with that, but my main point here
is that in this setting, you do actually have a choice what methods you use
and in most cases, it's very likely
that these methods will give you very
different results. The statistics is not
going to tell you what the right set of
assumptions is here. In most of these examples, if there is a discontinuity
in the propensity score, I would strongly suggest
looking at this type of estimates rather than trying to condition on these covariates. Unless you have a convincing
story why there may be a discontinuity in
the propensity score even though these
individuals are comparable. But again, the point is there's
a decision to make here, you have to think about
the substance of the case in order to make the decision. Here, despite, in principle, the data being perfectly compatible with using
unconfined in this, here, I would argue there's
a specific reason why not to use those methods. Then the next set of things I want to do is talk a little bit about doing
graphical analysis. Sometimes this is not a
very deep point and this is something that pretty
much everybody does using these methods. What you want to do is look at the regression
function and see whether there's actually
any evidence of a jump at the cutoff point. Here's a paper that I borrowed
from David Lease paper, where he looks at the
probability of someone winning the next election given the margin of the
previous election. What you see is that the margin is the difference between
a winner and the loser. It was just a little
more than zero. If the person won the
previous election, the probability of them winning the next election are far greater than if they just
lost the previous election. Without doing any
further analysis, this picture pretty
much sums it up, that is a very big effect of having just gotten
those couple of extra votes on outcomes in
the subsequent election. Clearly, one of the things
that you always want to do is look at the regression function there as a function of
the forcing variable, the variable that
determines the assignment and see what is actually
happening at a cutoff point. Now, to be slightly more
specific in the advice, I would be inclined to just use a very simple method
for doing that rather than smooth things
using kernel methods. I would be inclined to
just calculate averages in discrete bins making sure that these bins don't
cross the cutoff point. Why is that it's proudly to not be misled into
thinking that there's a lot of smoothness away from the cutoff point that may
not actually be there. If you use a kernel
estimator to the left of the cutoff point
and a kernel estimate it to the right of
the cut off point, you're going to
impose smoothness on both sides without imposing
it at the cutoff point, and so it's always going
to look as if there's a much bigger jump at the cutoff point relative
to anywhere else. Instead, use something that
is in fact much simpler, this group today discreetly
and see whether at the cutoff point there is a big jump relative to what
you see everywhere else. If you can't see it
with the naked eye, it may be possible that it's
still there in the data, but it's clearly
going to be much less credible than if you get evidence as clearly as
that in the lead example. Here is a case where I would fairly strongly
encourage you not to use more
sophisticated methods than just simple averages. The second set of plots that
I think is very helpful and I'll later give an example of that also from the leaf paper. Look at other covariates. Here in these designs, the covariance don't actually
play that bigger role. In the end, almost
always the story is that the covariance would have
the same distribution on either side close to
the cut-off point. There should not be
much difference in the covariate distributions
by treatment. The covariance are
not going to be necessary in Ghana for
consistency, for removing bias. But when you sit where they are extremely useful for
stacking whether the design is credible. What you should plot there
is essentially the same graphic just described for the outcomes but using
the other covariates. If you find that there
is a jump there at the cut-off point comparable to the bigger than jumps in any other value of
the covariance, then there's a problem that
needs to be explained. Well, actually, let
me take that back. I've not seen any
examples where there was a convincing story
for other covariates to change that at cut-off
point and so it's pretty much if you see the substantial
dump in any other covariates. It suggests something's
wrong with the design. You can do the same way
just using these bins, there's no particular reason
to impose smoothness Sam, and in fact, it may hurt. Last thing, last plot. This is really based on work by Justin McCreary is to look at the density
of the forcing variable, the variable that
determines the assignment. The idea there is and I'm
going to come back to that. Actually, me talk a little
bit more about it here. But the idea there is that you would be concerned if there is a discontinuity in the density of this variable
at the cut-off point. The concern would be that
individuals or someone in the whole process may have been successful in
manipulating this variable. In the college example, suppose that this
college actually announced that their
financial aid categories were based on whatever SAT
scores and that in fact, the cutoff point was
1200 on their SATs. Suppose it was
possible that suppose individuals could
retake those things. I'm revealing my ignorance here of American high school system. I like to know about
the relevant skills for this course but you
can retake this. But you could imagine that if individuals know what the
cutoff point is and if there's some way they could affect
their cutoff point by retaking an exam or manipulating part of
their application, what you might find is that as more individuals just to the
right of the cut-off point, were happy with their score. That individuals just to the
left of that cutoff point, they find some way of
moving up by trying again. Very interesting
example that the sexy attributes to how
fast some of these methods travel these days
given that I hadn't seen any applications
in economics before 97 but this year
there were actually two potential graduate students that I talk to during
the recruiting process. We had very interesting
papers using regression discontinuity
designs and there was one, Joe Shapiro who had a paper
looking at the returns to education in using compulsory schooling
identification strategy. This is data for,
I think Argentina. The cutoff date
there was July 1st. If you're born before July 1st, you would go to school earlier than that than if
you were born just after that similar to the Anders Krueger
identification strategy. What they found in that paper was that there
were a lot more birth just before July 1st than they
were just after July 1st. But I said it before
looking at it, you might have thought
that the date of birth would have been very
hard to manipulate. I'm not sure I get this
completely right but I think the story but
it's partly that there was a substantial number of induced cesarian
section birth that moved the birth up and
that hospitals essentially encouraged people to be aware
of the cut-off date there. The key thing is that even though you don't actually need the density to be smooth around the cutoff point for
this design to be valid, there's nothing we're
assuming about the density. If you find that there
is a discontinuity in the density at that
particular point, it's just very hard to understand where
that could come from without being concerned about
manipulating individuals, manipulating the
covariate value. A third plot that is extremely useful in
order to build credibility for a particular set of
estimates is to plot the density of x and then I look at
a discontinuity there. If there is one there, you should have a
story to explain that. Otherwise, it's hard to
believe the results. Now let me talk specifically about methods for
implementing this. Let me first talk about the sharp regression
discontinuity design. Essentially all
these methods are nonparametric
regression methods but I'm going to simplify them considerably by only using uniform kernels and
rectangular kernels which basically means I'm just going to throw out observations
that are more than some amount h away from
the cut-off point. In principle, you
can do slightly better by using more
complicated kernels. You can try to make
that very complicated. But the keyword there
really slightly, there's the gain, in the end, I think from trying to be very sophisticated on the kernel is very minor here and you lose a lot of
the transparency. That's always the case but here the key thing
is that there's just a single covariate and there's a very specific
object we're interested in, namely the difference in these two regression function on the boundary of the support. Just a simple kernel regression where with the rectangular
kernel would amount to just taking the average of the observations
within some amount h from the cut-off point. That doesn't work very well. The reason it doesn't work
very well with trying to get the regression function at a value c using
only observations to one side of c. This
would work very well if you could use observations on both sides of the cutoff point for estimating the
regression function. But on one side, the observations are
in a different group. The way they show
this up is you get very slow convergence rates
for estimators of this type. What works much better is to use local linear regression. Instead of the way you can think of the first estimator as
locally fitting a constant, what you can do is take
these observations there between c minus
h and c and fit a linear regression function to those observations and
then predict what would happen at the cut-off point if you set it up the way
I did here where you specified a regression
function is Alpha plus Beta times x minus c. The prediction at the
cutoff point is just Alpha. You just use the regression part to remove some of the bias. Now, you can simplify that further and just put it all in a single
regression function where you put in a dummy for their treatment as
well as an interaction between the X_i minus c between the covariate centered at the cut-off point and
the treatment indicator. Then the least squares
estimate for Tau gives you the local linear regression
estimate based on this particular set
of observations. Once you set it up this way, it's clear that you could
very easily include other covariates as well
and that should work fine. The key point I'm going to
come back to in a second is how to choose the bandwidth, the H here which is just how far you extend the set of observations you're using here. Before doing that, I want to just set things up for the Fuzzy Regression
Discontinuity case as well. The idea would be to use local linear regression for
both the treatment indicator and the outcome and estimate
the two things separately, take the ratio of them and it gives you this thing
at the bottom. One advantage of using the
rectangular kernel here is that previously was that we can just interpret
it as doing or less. Here there are fantasies
that in the fuzzy case, we can interpret it as doing
two-stage least squares, where we try to estimate the regression function given there and the two SLS equation
using SDN instruments, the Phi_i and the indicator for the X_i being greater than Z. That's going to help us estimate the variance as well using
this interpretation. Again, adding extra
covariates here is going to be very straightforward in this two-stage
least square setup. The next thing, I want to
spend a little bit of time on is choosing the bandwidth. Partly this follows a working paper by Ludwig and Miller. There's very large
literature in general on choosing bandwidth in
nonparametric regression. But sort of these two things
here that are different. One is we're interested in
the regression function at the boundary of the support. Almost all the results
have for the interior. Second thing is they
were interested in the regression
function at a point rather than at fitting a
regression function everywhere. Again, the standard, the cross-validation methods and optimal plugin methods are based on the criteria that
minimize things like integrated square error. Here, they may still give you perfectly
reasonable result but it's very specifically not
what we're interested in. We're only interested
in this difference in these two regression functions
at a particular point. Here, talk about
cross-validation type method that is geared specifically
to those circumstances. To set it up, it's maybe a little
more notation here. Now for all values
of x in the support, I want to think about estimating a linear regression
function from the left and from the right. By estimating it from
the left, I mean, using all the
observations within a bandwidth H to the left of that cutoff to that point 8x. By estimating it from the right, I'm going to use
observations to the right of that particular value
of x but within a distance h. At a
particular point x, we then estimate the
regression function by the intercept from
that local linear regression if we're to the left of the cutoff point
with the intercept from the Alpha R if we're to the
right of the cut off point. Now, we can think of looking for a
cross-validation criterion that minimizes the sum of the squared differences between the actual outcomes and the estimated
regression function. But the regression function's always based either
on observations to the left or it's based
on observations to the right of the cutoff point. So that deals with one issue, namely the fact that
we're interested in estimating something
at the boundary. Now, when we're doing
the cross-validation, every time we only use
observations on one side of the particular observation. It still doesn't deal with the fact that we're
just interested in the regression function
at a particular point. If you average this criterion
over all observations, we would still be
letting things be determined potentially
by the tails of the distribution
where in general, you're going to not be able to estimate the regression
function very well. If you use cross-validation
there in wave, I'll suggest using a
lot of observations because there's few
observations in that area. Instead of doing that, the suggestion
here is to average that squared difference between the outcomes and the estimated
regression function. But only do that somewhat
close to the cut-off point. Maybe on both sides
of the cut-off point, throw out 50 percent of the observations
in order to estimate the optimal bandwidth but somehow make sure
that the bandwidth, the choice doesn't get driven by the tails of the distribution, which would tend to lead to much bigger
bandwidth choices, than would be optimal in the
center of the distribution. Here, the second part deals
with the second concern, namely that we're
interested in only at what happens to the regression
function at Z rather than all over the support. Now that was essentially for the sharp regression
discontinuity design for the first one. In principle, you could
do this separately for both regression functions. You could even choose a
different bandwidth for both. In practice, that makes
things very awkward, and so I would be inclined to just choose the smallest
of the bandwidth which in a sense focuses mostly on bias
rather than their parents. In fact, I would be inclined to just calculate the
optimal bandwidth only for the outcome and ignore the one for the
treatment indicator. The one for the
treatment indicator, the optimal bandwidth
there would tend to be much bigger anyway. Typically, the jump
there as much bigger, and the regression
function would be flatter away from the
discontinuity point. In fact, if you close to a sharp regression
discontinuity design, the regression function
would be completely flat in both cases. In practice, I would recommend using the same
bandwidth for both and just estimating optimal bandwidth on the outcome regression function. Variances. Everyone wants the easy way. The hard way is somehow
useful for actually thinking about what the
variance looks like. This is essentially taken from the hand to fund
the Cloud paper, where I've simplified things by using a rectangular kernel. What you can do is
just look separately at the variance for the difference in the regression functions
for the outcome and for the treatment as
well as the covariance. I'm talking here about
estimating the variance. You can decompose the variance
using the variances for the outcomes and the
treatment indicator and put it all together. One way of actually
estimating the variance in the end is you just estimate all these different
components here. The last name means covariances
and these variances. This is also based on using a bandwidth that goes to zero, that essentially implies
we can ignore the bias, which we cannot estimate anyway. That's what you have
to do in practice. The second way of estimating the parents that
really rely very heavily on using the rectangular kernel
is using the fact that here the estimator can just be written as a two-stage
least squares estimator, conditional on bandwidth and conditional on using
a rectangular kernel. We're just using this
smaller sample with observations where X is between
c minus h and c plus h, using the indicator for X being greater than c as
the instrument, the treatment as the
endogenous regressor, and this interaction is the difference between X and c and the interaction of that with the treatment indicator as
the exogenous covariate. If you use the robust variance for two-stage least
squares you will get the same in the end as what is described on
the previous slide. Now let me talk a little bit
about specification checks. There's basically
two concerns in the application of these methods with the sharp or fuzzy design. The first possibility is
that this other changes at the same cutoff
value of the covariate. If in fact this
particular age limit makes you eligible
for one program, it may also make you eligible
for another program. There may be other changes in the individual's environment and attributing
all the changes to the one treatment you're
looking at may not be correct. Now, the second concern
is that somehow individuals are able to choose the value of the
variable that determines which side of the cut-off you're own in a way that invalidates
the whole design. What can we do? Some of these things I've
talked about already. The first is to investigate
whether there's discontinuities in some
of the covariates. Almost I've never seen a case where there was
a convincing story for having a discontinuity in any of the other covariates, so if you find one, should either be able
to explain it or be concerned about the
credibility of the estimates. The second one is the McCrary, the concern about the
discontinuity and the distribution of
the forcing variable. Third, you may also
want to see if there's discontinuity in the average
outcomes at other values. The graphical analysis that we talked about before
is very helpful there. But if the graphical
analysis isn't conclusive, you may just want to test
at different values and see how anything comparable to what you find at the
cutoff point is there. You may want to check how sensitive things
are to the bandwidth. If you have discrete data, you may want to think a little bit more
about the variance. Let me talk about
it in a second. This one is pretty much covered. What you want to
do is the same way we did in the analysis
on [inaudible] you want to estimate the
effect that you know is zero in order to see whether
the methods are working. Here's an example from the
leaf paper where he looks at election outcomes prior to the one that was very close. There, what you see
is that even though this regression function isn't of any substantive interest, what is of interest
is defining that around the cutoff point this is actually a
smooth function. If there was a big jump here
you would be concerned that somehow these districts they were on one side of
the cutoff point, they were substantively
different from the ones that were on the other side
of the cutoff point. It would be very
hard to come up with a story, but it
would be the case. But in other settings, there may be a
considerable concern with that type of misspecification. The second one, we talked about a couple
of times as well, is looking at the distribution of the density of
the covariates. You may want to test whether there's discontinuities
in average outcomes and other values of the
covariates to get a sense of whether
the statistical methods are working well. Again, here you
would end up with estimating something
that you know is zero. Finding something that is close to zero statistically and substantively makes the overall
estimates more credible. Even when you choose the
bandwidth optimally, you still always check how
sensitive the results are for this and then you put in half twice the
original bandwidth. The last point is in practice, there's a lot of cases where
the covariates are actually discrete and sometimes they
may be close to continuous, but in some cases they're
clearly very discrete. Even though you may try to compare outcomes very close
to the cut-off point, you may end up comparing
average outcomes or values of the covariates that
are somewhat far away from the cut-off point. One way of trying to see whether that is a concern is the
idea from Lee and Cart. We put a more parametric
specification on the regression function and
then use the difference between that and the
average outcomes as specification
error that allows you to adjust the
standard errors and get more credible
confidence intervals. I think I'm running a
couple of minutes over, but it seems a good time to stop here and see
if there are any questions. Otherwise you're in
for 15 minutes. Yes. FEMALE_1: [inaudible] . Guido Imbens: Yeah.
Let me just go back to the first Lee graph. Here to say, obviously, a
case where you actually know exactly what
the cutoff point is. Now, suppose you legitimately don't know
what the cutoff point is, but there's strong evidence that you have strong prior knowledge
that there should be one, then you can actually
estimate that there's methods for estimating
structural breaks that would allow you to do this. I'm somewhat skeptical about the empirical relevance
of that setting. I've not seen a good
example where you really legitimately
know that there is a cutoff point that
is impossible to actually buy prior to
just looking at the data, but by just investigating the process where it's impossible to figure out what the cut-off point should be. You could imagine that it was in recorded value and then
somehow that it got lost. I think in most cases
you would see it pretty clearly in the data and you could estimate it and pretty much ignore the uncertainty coming from that because it would be of
lower order anyway. The jumps should be high
enough to be big enough for the uncertainty not to
matter much anyway. If you really relied on searching for
the cutoff point and it was not completely
clear where it is, I think it's very
hard to imagine the eventual estimates
having any precision. If there's a lot
of uncertainty by the cutoff point these estimates are never going to work. No other questions. Well, let's start again at 3:15. 