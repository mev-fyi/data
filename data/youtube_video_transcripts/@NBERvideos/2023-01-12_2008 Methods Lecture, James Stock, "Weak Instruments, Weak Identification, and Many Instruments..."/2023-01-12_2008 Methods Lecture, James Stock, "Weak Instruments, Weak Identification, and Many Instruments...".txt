so this is uh this is a great opportunity uh to be able to talk to all of you about some of the recent developments in Time series and in particular this afternoon what I'm going to talk about is recent developments in the general area of weak instruments or weak identification this is a topic or an area that's seeing a great deal of work over the last 10 or 12 years and a a real um a lot of new procedures have been generated and I think a lot of new insights empirical insights have been gained and so I'd like to take this these three hours this lecture and the next one to go over some of those new developments uh some of the motivation for it some of the theory underlying what's going on and then emphasize some of the tools that are available I don't know if Mark made mention this or not but we were joking beforehand how this basically has the feeling of a nine hour oral exam uh where uh where I I'm we're talking about papers that were written in the last two years by a large number of members of the audience and um so if you want to swap places you're welcome to oh uh but um you know so so give us a break here in case we make a minor slip we think we understand this material reasonably well uh but but I'm sure there's going to be lapses all right so what I'm going to talk about this is an outline of what I'm going to talk about I'm going to spend a little bit of time going over motivation for why we might be interested in Weak instruments I think there's a lot of topics where econometric theorists get all exercised and interested and work hard for many years and then are ignored and that's often of the right thing to do and uh in this case it's not because I think this has pretty important empirical characteristics or applications I'm going to spend actually even though this is time series I'm going to spend time talk about time series I'm going to actually spend about half of the time the next two lectures from all of this lecture talking about the classical cross-section IV regression now we don't actually have many of those opportunities where it's cross-section classical regression uh in practice but I'm going to go over that for a couple of reasons and one reason is that that's actually the area that we know the most about it's the easiest model to work with so the most progress has been made but the second reason is that many of the lessons of the classical IV regression cross-section regression model carry over very directly to the time series setting with modifications okay then I'll talk about this in GMM and it will all be in the context of General non-linear GMM although all of that will specialize to the linear case at the end I'm going to talk a little bit about many instruments um the all of these first topics topics one through nine so the majority of what I'll talk about you really want to think about a case where you won't you have a handful of instruments um maybe a typical application might be if you're using time series data and you have a couple hundred observations you might have instruments that would be maybe five or ten or uh 15 or something like that a moderate number of instruments there are other applications where you might have many many instruments I'll talk about those briefly I have some questions or skepticism about some of that I think it's a difficult area but uh but so that's why I'm basically postponing that to the end okay so I'd like to start with four examples in the first example the example that's got this whole literature going was a paper by angrist by angrist and Krueger where they were trying to figure out what the returns to education is and the instrument that they uh were so the regression that what they wanted to regress uh was a regression of log earnings on years of education and then the coefficient on years of education supposed to be the returns to education and so there's a number of reasons why either ability bias or measurement error or whatever why that might not be such a great regression just by OLS so you want to use IV methods and their instrument was a quarter of birth and so here's the story which is that some states allowed you to uh some states this is and they used data from a long time ago from the Census so the deal was that some states allowed you to drop out of school at a certain age so let's say you're allowed to drop out of high school at age 16 if it's May and you turn 16 you might just hang out for another few weeks and graduate but if you turn 16 in September then maybe you might as well just drop out so uh so that means that there might be some marginal effect maybe a small effect but some effect of when your birthday actually is on how many years of school you complete all right so that would be and and it you know that that would presumably be independent of measurement air or some other things like that when you're born would be independent of measurement error and ability and so forth that's the story that would suggest it would be an exogenous instrument the thing that makes this result really interesting from an econometrics perspective is that they used 329 000 observations so that's really great because that is as close to Infinity as we're going to get uh in in in practice so you know when you see that you think asymptotics okay so that's fine so they got a great result that which is the return to education was eight percent plus or minus 0.01 so you'd expect reasonably good Precision with 329 thousand observations and then in a paper which I understand was first presented in NBR summer Institute uh about somebody can correct me on that one if I'm wrong about 12 15 years ago or something like that was it a summer Institute okay uh um John bowne and a couple of uh his co-authors did a simulation and what they did is they took the actual earnings they did a simulation one Monte Carlo draw and they took the actual earnings data and the actual years of Education data so this is the real data set but then they gave people fake birthdays okay fake birthdays and then they ran IV with the fake birthdays and the real data and they got an estimate of 0.081 with a standard of 0.011 okay with 329 000 observations okay so that's a wake-up call um and the problem was and this has been studied to death because it's so interesting is that basically uh the regression they ran it wasn't just years of education but what they actually ran was a whole bunch of interactions with the state and uh and the and the year and so they got 178 uh instruments by the time you got 178 instruments the average information per instrument was so small that despite there being 329 000 observations essentially the instrument was very nearly uncorrelated so if we think about instrumental variables as having two criteria having relevance and exogeneity let's put exogeneity aside it wasn't particularly relevant another way to say it is it was weekly correlated with x okay so here's a second example a little closer to home which is uh looking at um looking at regressions that are trying attempting to elicit the uh the inter-temporal elasticity of substitution and so basically if you work with the Euler equation and you linearize it or it's just a this is very standard stuff out of Campbell's 2003 Handbook of economics and finance linearized Euler equation uh gives you this orthogonality condition that's written down right there and that suggests that if you have some instrument anything that's predetermined if this is rational expectations orthogonality condition then anything that's predetermined should allow you to run GMM order straight IV since there's essentially there's not supposed to be any serial correlation in this under right so there's not supposed to be any serial correlation and since there's not in practice much heteroskedasticity this is really just an IV problem you don't really need to worry about GMM or any of the hack or any of that any of that stuff okay so you can run this and what Campbell reports in his handbook chapter is quite stunning results and there's really two ways you could run this you could estimate this using a regression of consumption on the interest rate or you could or the asset return or you could regress do it backwards right because this is a first order condition it doesn't say one of these things is supposed to be on the left hand side one is on the right it's just an orthogonality condition so it's just an equilibrium condition so you could put it wherever you wanted you could run at Delta C on R and estimate PSI or estimate r on Delta C and estimate one over PSI and of course you know one over PSI Hat by the Delta method is going to be consistent for one over PSI and then they're going to have these things should be the same they should be the same should get the same answer and you get extremely different answers so if you run the regression the first regression regression a up there of consumption on the interest rate you get estimates or PSI between negative 0.14 and 0.28 and the other one gives estimates of one oversi from negative 0.73 to 2.14 . so this is completely internally inconsistent it makes no sense okay um there's an easy explanation here too think about this regression you're regressing an asset return on consumption using consumption growth using instrument what's that instrument have to do it has to forecast consumption growth okay so you can probably forecast consumption growth but you can't forecast it very well Paul told us that still true um okay so uh here's another one which is the new Keynesian Phillips curve another example and uh so there's a this is a big literature that I've we've never worked in um so I don't have any personal uh ax to grind here one way or the other there's a very nice survey paper uh by kleiberg and maveritis and that's available on their websites um so basically there's the new Canadian Phillips curve and I've tacked on some sort of unobserved cost shock at the end a to T that doesn't need need to be there uh the point is that there's an X an orthogonality condition because of the expectations that's generated there uh z uh excuse me X is maybe an output Gap or in the way golly gertler um uh do it it would be like a labor share but it's it's some some some uh some activity measure and um the point here is that the instruments then uh that are generated by this rational expectations orthogonality condition are going to be um certainly lagged values of things that appear there but maybe some lag values of other of other stuff uh and um the hard part of this is if you think about what you're doing is these instruments have to be effective at instrumenting for pi t plus one all right so what you need to do is you need to have an instrument that is going to maybe I can just point with this instead you need to have an instrument that is going to be exogenous well that's the dating convention under rational expectations but it also needs to be correlated with pi t plus 1 given Pi T minus 1. all right so being correlated with pi t plus 1 given Pi T minus 1 is another way to say it is that what it needs to do is it needs to be good at forecasting the change of inflation okay so those of you who are in the inflation forecasting game for a living forecasting know that forecasting the change of inflation is very very difficult in fact there's this Atkinson Ohanian paper uh which uh which says that the best at least since 1985 in the U.S the best you could do for forecasting inflation is just modeled as a four quarter random walk so that's really problematic again a circumstance in which you're trying to do a structural estimation but you have an instrument that's weakly correlated with included endogenous regressor in the final example which I won't go into now but I'll give you some empirical results at the very end is just the non-linear version of the second example uh and I'm I'm focusing on the very simple cases I'm not trying to this is a model that nobody believes at this point as far as I can tell which is fine with me but it's perfect but it also it makes it just easy since no one believes that we can all agree that we'll look at it from an econometric perspective and also it's a lot easier than the more complicated models so I won't go through the more complicated models all right so this is a GMM setup all right but again it's fundamentally the same problem which is that you're going to have an orthogonality condition and your instrument has to be correlated with consumption growth and you know if I wrote down one of the more complicated models it might be a more trackable water pleasing one from some economic perspective it's going to have the same econometric problem which is that the instruments in an orthogonality condition are have to be correlated with consumption growth and future asset returns okay all right so that's the uh that's those are some motivating examples and we'll come back and we'll look at empirical applications in all of those except not the Angus Krueger because that's really not germane to this audience okay um so let me talk a little bit just to set the stage so this is this is the this is the pack to graduate school part of everything go through the definitions of uh identification um so and observational equivalents so uh Theta is going to be the parameter Vector Capital Theta is the parameter space two values of the parameter Vector Theta one and Theta two are observationally equivalent if they give the same distribution of the data well that makes a lot of sense if they give the same distribution of the data then there's you're not going to be able to shift it around or distinguish between distinguish between the two different points um it's a Theta is locally identified or that is to say it's identified at the point Theta naught if there's no other value of theta such that they're observationally equivalent so that just says at least there's one value of theta Theta naught that's okay and Theta be then you should globally identified if it's if it's identified for all values of theta naught okay um in GMM we're not actually working with likelihoods so but it's useful to think about a version of this that would be valid for some alternative objective function like a GMM or IV regression objective function and one way we can think about that is that um is that if the is that these objective functions would have the same value for the same values of theta 1 and Theta 2 at least in expectation and um and and uh and then we can go through saying it's identified at a point and globally identified in in the same way right so um those are sort of those are traditional or classic definitions of identification and observational equivalents the I think one of the important points here conceptually is that um actually there's two two points I want to make conceptually and the first point is that I is this one here the identification doesn't necessarily imply consistency I think um we've gotten so used to uh thinking of identification as as so if you think about say the standard GMM definition you go through some assumptions and one of them is an identification assumption and then you simply take into infinity or t to infinity and boom you've got consistent estimation and an asymptotically normal estimator so it's conventional or standard for us to think about identification as being directly associated with consistency but that of course isn't true so let me just give you this here's a really simple example where I'm going to give you an example where a parameter is identified but it's not consistently estimable and imagine you just have a process that's kind of going along for a while and then there's a break in the mean well if that break in the mean occurs after let's say 10 observations and then it goes on for Infinity in the new regime I only have 10 observations at the original value now that's identified that is to say the Val the distribution will change as I change the value of that first period mean but there's no way I'm going to be able to estimate it consistently because even though my process goes on to Infinity I've only got 10 Ops all right so identification doesn't mean consistency the second point is that um and I think it's related to that is that we often think of identification as binary all right identification I mean it is the way it's been defined identification being binary or not I mean either identified or it's not but but that doesn't mean that there's this binary implication for inference and it's useful to think about something being well identified and something being poorly identified it's in a very meaningful sense sensible to say this is not very this mean in the first regime isn't very well identified because we only had 10 observations on it if we only had five observations it'd be even less well identified but if we had 50 it'd be better identified that's a sensible thing to say and actually it it's a meaningful thing to say too I mean in a formal sense okay so um the framework in which we're going to talk about weak identification is one that has a very practical orientation which is that I'll say that something's going to the a parameter will be weakly identified if it's um distribution is well not well an IV distribution or a GMM parameter estimated by TMM is not well uh let me start over I will say that a parameter Theta is um is weakly identified if it's estimator or test statistics computed by GMM or IV have a have a distribution that is not well approximated by a normal all right so that is to say we're looking at circumstances in which the usual first order asymptotic Theory doesn't really provide a good approximation to the sampling distribution all right I would argue so go back to Angus and Krueger 329 000 observations first order asymptotic Theory says that that should be a consistent estimate it also it would have no explanation at all for why the bound completely artificial birth dates would have produced the same point estimates and the same standard errors all right so um we're going to focus on large sample inference again that's one of the nice things about the bound application its sample size is large but realistically in um in what we do in macro we have hundreds 100 or 200 or sometimes more observations that's quite a few observations from the perspective of say Central limit theorem if you look at Central limit Theory the right way or from consistency so we're going to use large sample approximations and this literature almost entirely relies on large sample approximations in one way or another um but that's I don't think that that's a big big problem what it doesn't require is uh identification um being strong that is it doesn't require identification in some sense going to Infinity at the same time as a sample size all right and so we're going to assume uh instrument exogeneity throughout that's an interesting and of course important question whether something's exogenous in practice but that simply isn't what we're talking about it isn't what we're talking about here okay so um there's a couple of ways to think about weak identification one of them which is what I'll start out with and that's what I'll talk about in this first lecture uh is thinking about it from Just A first order condition or an estimating equation perspective um and uh in that in that one one way to think about that is well we'll see basically uh if the z's are weakly correlated with the included endogenous regressor then deviations of theta from its true value are not going to be something that the instrument's going to pick up at all and because it's not going to pick it up we're not going to be able to distinguish values of theta other than its true value from the true from the True Value um and then there's also an objective function interpretation which I will return to when we do GMM and then finally a couple of other preliminaries um a special case of all of this is going to be that Theta is unidentified in other special cases that some elements of theta are strongly identified and some are weakly identified another case is that it's a partially identified okay what I mean by partially identified I mean that some elements are identified and some are unidentified we won't come back to that again that's just it's it's just it's a it it's you can find all of that in the literature that I'll be citing but it's not worth conceptually going through that and I do want to distinguish between something that really sounds similar but it's not at all the same which is the notion of set identification so set identification which we will actually I'll be talking about set identification tomorrow in the context of uh svars but said identification is something that's received a lot of attention in the econometrics literature mainly in the cross-section panel literature it's a very interesting area that's really something quite different what that says is that it's identified on part of the parameter space but there's part of the parameter space in which the likelihood is flat that is to say there's no identification at all on another part and then the task is to estimate the set that's a different thing if we're not talking about estimating a conference yet we're talking about estimating a set on which it's um not a on which is not identified so um we'll talk about that later um but that's not at all what we're talking about today okay um so um I think I've said I think I've said all of this stuff yeah there's some literature reviews so these are three literature reviews that were written at the same time that are more or less non-overlapping so it's kind of amusing to read them they're all out of date uh but but they're but it's amusing um this is this is a literature review that is incredibly technical uh and that's only for the strong-willed I think um okay okay so let me get to work here all right so uh I'm going to work through this whole talk to the first bit um until three o'clock it's going to be entirely on the classical IV regression model um I have uh I have put subscript tees on there just because that's what I've done but really this is all subscript I stuff okay uh I figured if I had eyes and T's eyes oh I'm we use eyes to to denote variable x sub i t so there's no we're not using eyes for observation we're using t's for observation but this is cross-sectional stuff all right so um a little Y is the left-hand variable Big Y is the right hand variable possibly a vector although most of what I'll talk about is just a scalar um the uh there's a bunch of instruments we'll call them K instrument z uh there's an additional equation y equals Pi Prime Z plus uh the error term let me just make a comment on this um this this equation right here and so I'm sorry and then I'm sorry in one second the the data are all going to be IID so that's the cross-section assumption so everything is IID it's a very simple world and for some of this I might even want to make them normal but I'm not going to do that for much because mainly we do asymptotics and so then here's the equations in Matrix forms so this should remind you of your undergraduate econometric class or or something like that or somewhere in a in an early problem set in your graduate class and so there's the there's the IV regression model in Matrix form let me talk about this equation for a second so depending upon when you took your graduate econometrics this might have been presented in some in some meaningful way as a meaningful equation that is to say one could ask the question whether or not this equation was mispecified let me suggest that that's not a valuable way to think about it you shouldn't be thinking about the possibility that this equation could be mispecified this equation is actually a definition of the projection of Y on Z so it doesn't make sense to think about what is the remitted variable bias in pi or anything like that that's not what Pi is Pi is just a bunch of things are used to to make the linear combination that's going to predict y all right so there's no room for mispecification in that equation the very that's just a subsidiary equation that's sometimes useful to define v as the residual from this population projection all right so um okay so here's the two stage Lee squares estimator of which in the case that we only have one instrument is just the IV estimator and so um two CG squares the is that the projection uh the projection part drops out and and that's uh just the IV uh just the IV estimator and then if I carry substitute in the first line uh y beta plus U for y and then I simplify I just get this final expression um this final expression right here okay so what I'm going to do on this one slide this one slide is going to take you through the intuition for what's going on in that bound angular angris Kruger example and this intuition actually is the right intuition to keep throughout thinking about weak instruments okay here we go if Z is irrelevant if Z is irrelevant then that says that in this projection of Y on Z in population at least Z doesn't enter okay because it's irrelevant that's got to be true for those fake quarters of birth in population those fake quarters of birth get a coefficient of exactly zero all right well now I can plug this into this expression upstairs all right so y isn't just V in this fake quarters of birth example well what's this Z Prime U is just this summation of z u and Z Prime Z is this summation of z v and I'm going to scale it and I'm going to scale it and this is where the magic comes I'm going to scale the denominator by 1 over the square root of T so you've probably never seen in regression scaling the denominator by one of the square root of T you all squared up by 1 over t this guy obeys a central limit theorem it's just his EU right so U is just an error term mean zero Z is uncorrelated with a valid instrument so Z isn't correlated with u this obeys a central limit theorem converges to a normal random variable but zv also obeys a central limit theorem Z is this random number that was generated in the computer so the limiting distribution the large sample distribution when you have 329 000 observations is that the two safely squares estimated subtracted from the true value is the ratio of normals now if these normals were independent that would be a Koshi they're not independent but that's that's the distribution so this is this is everything this is I can have 329 000 observations and fake data and I'm going to get a well-defined asymptotic distribution Theory out but it sure isn't what I want okay okay now let's do a little bit more with this expression right so it's the ratio of two normals that have mean Zero by the way when you divide by a normal random variable that has mean zero some bad things happen okay all right so this is a little bit dense but I wanted to get it all in one slide just to show you could do it all right so now what we're going to do is we're actually going to show that in the bay in the Jager and we're actually going to show that in the bound simulation remember how they got the same answer as two stage V squares I'm going to show that in that simulation actually what they're going to get is the same answer also as OLS so that in fact the two safety squares estimators in case of an irrelevant instrument is centered around OLS so the way I'm going to do that is I'm going to write z u H remember they're jointly normal so I can always project one of the Z's onto the other one and then I have an orthogonal error term which is also normal so I've just done a kolesky factorization for the Hollywood macroeconomists all right so then I'm going to substitute in this kolesky factorization projecting the z u on zv into that expression for the ratios and then I get Delta plus ETA over zv well now ETA over zv remember ETA by construction is a random variable that's independent of zv so it has mean zero and conditional on zv it's it's it has this is this very has this strange variance where there's actually a random variable in the in the denominator it's going to be a mixture of normals where it's a an inverse chi-squared mixture of normals um but so if I plug so if I think about that what I can well so that says that this is as I say here it's going to be a mixture of normals random variable and that's in the case of this irrelevant instrument but moreover if I look at this expression for Delta plus something that now has mean zero well if it has a mean if the mean exists it has median zero because of a normal mixture of normal is around zero if I plug that in here and I remember I remember if I plug that if I plug that in oh I don't know if I wrote that out so let me just okay so what's beta 2 stage G squares remember beta 2 says these squares so beta hat two stage Lee squares now is equal to Beta naught which is beta which is this part here plus okay plus Delta plus ETA over zv okay so that ETA over zv is from this term right here um and yep it's from from that term uh right there so that um so that this thing now has median zero it actually doesn't have any moments but has median zero and the median is around beta0 plus Delta all right so Delta just happens to be the projection in that funny definition well it turns out that if you work through what OLS does OLS has that same projection in fact OLS is consistent for beta0 plus Delta so in fact in this circumstance this actually is equal to the plim of beta hat OLS so what's that say it says that in irrelevant instruments two Sage these squares is centered around the p-lim of OLS plus something that has no moments but median zero so you've taken OLS and just made it much much worse thank you all right that actually makes complete sense right if you're doing IV with fake instruments you can't be getting any more information that was already there in OLS and you're making it much much worse okay and you know what's worse about it the worst part about it is not that you're centered here and I'm not going to go through this algebra because it's more tedious the worst part about that bound result was not that he got the same point estimate but he got the same standard error that's the scary part so it's clearly completely misleading so if you've got a standard error of an Infinity that'd be okay all right all right so it turns out that there's this um thing called the concentration parameter which once you start to move Beyond these this simple example the example that I gave was irrelevant instruments irrelevant instruments is obviously an extreme case it's useful to frame your intuition around irrelevant instruments but it's not something we would want to develop a theory about for taking to the instrument taking to the data it's not like it's irrelevant or fine there's this Continuum okay so that Continuum is described by something called the concentration parameter the concentration parameter um is this thing mu squared so stare at this for a little while and this will come alive okay so how will it come alive it will um it will okay one way to think about this is that this is like the explained sum of squares of the regression of in the in the first stage regression and the regression of cap y on Z right so it's like if there were a hat there would be exactly to explain some squares so this is to explain some squares if you happen to know the true value of pi another way to think about it and this is going to be quite important is that if we think about the first stage regression F statistics so that is the F statistic in which you look at whether Pi hat has statistically significantly different from zero this is close to the expected value it turns out this is the non-centrality parameter of that s statistic so that statistic will be big if this is big right so this is called the concentration parameter turns out that this is the key object in this very simple model that describes the strength of the instruments and it does so not just in a casual way but in a very precise mathematical way and I put this up this calculation due to rothenburg up on the slide because it's not something you'd see I mean this is a really old handbook chapter it's quite brilliant but it's very old and most people don't really read it and I just thought I'd put this up here in case you want to work through this the very interesting thing about this calculation is that you go you start here and then you do a couple pages of algebra or something depending on how good you are until you get here okay and so from here to here is you don't you can't see that in your head although you can see it's probably not unrelated because there's things like Pi Prime Z and stuff like that that must be in here somewhere okay but you cleverly rearrange things because you know where you want to go and what you get is this expression which is very interesting and the thing that's interesting about this expression is that every one of these random variables in this expression have a distribution that does not depend on the sample size so nothing the only one way that the sample size enters through here and the sample size enters through the concentration parameter and if you look at this the sample size does enter through the concentration parameter because there's the Z Prime Z the more observations you have the bigger Z Prime Z right so the sample size enters through the concentration parameter but nowhere else but what this also says is that mu if I normalize it this way and if mu is large this term goes away this term goes away and this term goes away and I'm left with a leading term that leading term is actually the leading term that's left over in standard asymptotics and it gives the normal distribution to the two-stage Lee squares estimator two stagely squares estimators are asymptotically normal as something tends to Infinity asymptotically with respect to something but it's not with respect to the sample size it's with respect to the concentration parameter the concentration parameter is the sample size it's the effect of sample size in the two stages squares asymptotics if the concentration parameter is large even if the sample size is small it's going to have an approximately normal distribution if the concentration parameter is small even though the sample size is large it will not okay so I just said this this is important so here's a simulation from Nelson and starts and this is looking at the T statistic for a variety of different values of the concentration parameter um I believe okay so this it starts out with a dark line is irrelevant instruments so they did another one of these bound Jager type simulations uh this was in a different completely different design so it's not exactly the distributions are different but it but conceptually it's it's very similar um this dark line is the uh is is when it's irrelevant instruments and what happens as the concentration parameter increases is you see that this distribution of the T statistic gets more and more peaked until then eventually it's starting to look like a normal distribution and it there really is a Continuum here this this is for these calculations uh were done for a single sample size and the only thing that's changing is the uh is the concentration parameter okay so um let me talk a little bit about how to get a handle on this um what's the big picture Okay so the big picture is we've identified what the problem is uh the problem or a potential problem or a problem is it when is instruments being weak we actually haven't come up with at this point any tools at all to deal with that so what's the logical progression the first step is coming up with some analytical framework that's going to allow us to characterize this distribution theory in a way that's more relevant than just saying lar you know asymptotic normal uh in in the usual way so we kind of come up with a better distribution Theory second step is once you've done that you can start to talk about tools and tools that are going to work that are going to be valid for inference when you have weak instruments let me say an important point about this research program and about the tools that are available look the best thing of all would be to have strong instruments and not have to worry about this okay so one answer and this this happens yeah you're in econometric seminars I've been an economy metric seminars where somebody's talking about weak instruments and they and they're asking well what's your advice and they say get stronger instruments and that's great advice for an econometric theorist to give but sometimes it's not the most practical advice in the world the other thing is use tools that are going to be robust that are actually going to work if you have weak instruments okay so that's actually this research program which is coming up with tools that do the best that you possibly can in in an honest way when you have weak instruments okay so the first bit is what's the analytical framework okay so um there's uh four different things first three which we're not going to use um is finite sample Theory so this this idea of weak instruments is really not that new uh it was a you know these like Ted Anderson knew all this stuff in the 1970s and Rothenberg knew this stuff in the 70s and they were working on it really vigorously but unfortunately the tools that they were using were just tools that led to a dead end and that's finite sample Theory the finite sample Theory just has too many things going on and the distributions are incredibly complicated and they're not tractable and you can't even compute them to give you an example a problem that was sort of the terminal problem that they couldn't solve and that literature was what's the distribution of the two Sage Lee squares T statistic um so uh you know writing it out so that just it just didn't work okay second idea is Edgeworth expansions Edgeworth expansion to provide approximations for improvements upon the normal distribution so we're trying to improve upon the normal so let's do Edgeworth expansion it's not a bad idea the problem so here's the problem with Edgeworth expansions Edgeworth expansions I'm going to say this in a loose loose way Edgeworth expansions are you start from a normal and then you take sort of a perturbation or an expansion around the normal and that perturbation or expansion around the normal is an expansion that disappears as the sample size gets large and is an approximation that's only good when the sample size is is moderately large that's well done in terms of the sample sizes but since it's the concentration parameter that matters in this circumstance the Edgeworth expansion is in orders of like one upon the concentration parameter and that's still not going to work when the concentration parameter is really small all right so that's going to be a better approximation than the asymptotic normal their first order normal but it's not going to explain the bound Jagger problem it can never deliver that ratio of normals the Edgeworth expansion will not be able to do that right so another way to say that is that Edgeworth expansioned they were known at the time not to work and so he's quite clear about that but another way to say that is they're not uniformly valid over all values the concentration parameter and we'd like to have inference that's uniformly valid we'd like to be able to go from the bound example all the way to you know everything's working great bootstrap and sub sampling I'm going to come back to this so you know a very common response oh my distributions are not normal I'm going to bootstrap either of these work here they don't they neither of them work all right so we'll go over why that is so the procedure we're going to use is something called weak instrument asymptotics which has developed it's not a general procedure at all there's nothing General about it at all it's a specific tool to solve this specific problem all right so to say it's not General at all it's not out of the blue either essentially this is just using a technology which is quite common this it's a statistics literature which is making local approximations using something called a Pittman drift this is something that you've seen in local community asymptotics which I would imagine most of you are familiar with although Mark and I will not be talking about these during these lectures because they're supposed to be recent advances and there's no recent advances in local Unity asymptotics um so uh so um so what we're going to do is we're going to say that this concentration parameter is we're going to consider asymptotics in which the sample size gets large but the concentration parameter doesn't well I guess go back it's actually really obvious what you have to do if you want the concentration parameter to be constant as the sample size gets large well Z Prime Z is getting big you're not going to change that those are your instruments the only way to deal with it is if Pi gets small all right so how what's the rate at which s gets small well Z Prime Z is getting big at T So Pi better get small at square root of T so that's it that's all there is to it turns out if you plug that in and then I won't take you through any more all right so I suppose as opposed to that you know that he's supposed to take people through the easy one and then they believe you for the hard ones all right so here we go so this is a this is now going to be two stage D squares with um one uh with with your relevant instruments all right so I've actually got the general two safety squares where we have a number of regressors so we might have several included endogenous regressors all right so um this is the two stage Lee squares oh I'm sorry I only have one I divided by it so I only have one included endogenous regressor this is the two-stage lead squares estimator centered around uh beta0 and so I'm just going to look at the denominator and then I'm going to look at the numerator so let's look at the denominator first so the denominator I'm going to uh okay so the denominator the denominator is um which is y Prime p z y which is I skipped a step so y Prime z p z y is uh writing that out y Prime z z Prime Z inverse Z Prime Y and then if I substitute in what that capital Y expression is for that I get that first expression up there and then I've done something again which is going to be this little magic I'm not going to divide by T here I'm going to divide by square root of T so I've got a couple of square root of T's there and a t in The covariance Matrix bit so the covariance Matrix is going to be well behaved that's just going to go to the it's its population value and what's interesting I'm saying if I break this up this is going to be well behaved Z Prime V is very well right that's just a that's just this um if we're in an IID setting here so that's just something that's going to obey a central limit theorem and then the one piece that's different is this pi times Z Prime Z over square root of T well that's set up in exactly the right way so that I can take advantage of this pi times square root of T being C that puts another T in the denominator and then this Z Prime Z is going to be well behaved so this whole thing is going to converge it's going to converge in what it's not going to converge in probability to anything it's going to converge in distribution to this object here this ratio where now Lambda has got this C Prime Q Z I've got a z Prime Z to the minus one half built in here and Q Z is the covariance Matrix of disease and then I've got my same old z u and zv that we had before all right so um so it's it's a z u and zv from this irrelevant instruments from this relevant instruments problem and so the only difference this actually is if you recall the same calculation we did for irrelevant instrument it's just that this part wasn't there right so this part wasn't there but that was that square root of T magic in the denominator and now I've kept that square root of T magic in the denominator but I've done so in a way that um that that deals with this um project this projected part the explained part all right well I can do the same thing in the numerator and I'm going to get a similar limit and so what I get then in by putting these two together is this expression here um and this expression says that essentially the two states e squares estimator subtracting out beta naught is going to be converging not Con in probability so it's not going to be consistent for anything but it's going to be converging distribution to this object if Okay so let's think about the simplification that you've seen if Lambda is zero Lambda comes from um Lambda comes from this bit here so if Pi is 0 then C is zero so if we're irrelevant instruments Lambda is zero and in the case of a scalar then these zvs would cancel and we're left with z u over zv which is the calculation from before and so all this does is it allows a non-zero uh a small amount of of info coming from the instruments and then it allows for multiple instruments uh through that through that inner product yes oh that's a great question I'm so glad you asked that well there's the glib answer which is I can do whatever I want but there's the more serious answer which is what's asymptotics for asymptotics is not for like proving theorems to get them published in the anodes of stats or something asymptotics is for providing good approximations that we can use to analyze situations of empirical interest all right that's why you want to do asymmetology fine if we could all just do finite sample we would do it we wouldn't have any asymptotics because we want to know the sampling distribution turns out finite sample isn't possibly complicated so we can't do finite samples too hard so what in standards we want to have some device a tool tools that provide good approximation to sampling distributions of estimators and test statistics in many cases it's our textbook asymptotics in this case the textbook asymptotics lets us down so we got to come up with a different tool this is a tool it's just a tool this now that said I'm not doing this to prove a theorem I'm doing this to provide a good approximation if it turns out this provides a bad approximation then we just throw it away the reason we didn't use Edgeworth of Edgeworth expansions provide an approximation it turns out they don't provide a good approximation in this problem right the bootstrap provides an approximation it turns out it doesn't provide a good approximation in this problem you can prove it doesn't turns out this provides a good approximation in this problem so it's a good tool for this problem same thing with local Unity I mean you know if you think about the local Unity approximations those are good approximations for that problem and there's a lot of mileage that one can make about inference using local Unity approximations okay so so I've shown you sort of we've we've gone through the basic two safety squares calculation for weak instruments I'm not going to go through any of the others let me just go through some of the implications here so under this under this asymptotics there are another number of implications for estimators and test statistics that we normally use in 2C in in IV inference it's important to emphasize that the reason we're using this asymptotics is because it provides a good approximation okay it's not that so so these implications are implications that provide good approximations to what happens in empirical practice when you have weak instruments Okay so first of all just as a technical aside it turns out that this um these distributions are the same as in the exact normal model ex with fixed instruments but known covariance matrices which is nice because you can draw on a rich literature um and there's a lot that can be done with that in this context IV estimators aren't consistent in general they're biased we saw that with two sagey squares the two stages squares estimator has a mixture of normal distribution where it's not centered at the right place um the uh test statistics typically don't have the right distribution so jstats for example don't have the right the right distribution they don't have chi-squared distributions confidence intervals don't have the correct coverage we already saw that from the bound Jager thing right you know he got .081 plus or minus 0.011 so obviously that wasn't giving that can't be giving the right coverage if you have irrelevant instruments um if you have irrelevant instruments if you have irrelevant instruments what's what's the only valid suppose that suppose now you know that those instruments are irrelevant what's the only valid what's what's a valid confidence interval if you have irrelevant instruments so it's going to be the real line you do a coin flip and 95 probability you say the real line and five percent of the time you can say whatever you want plus or minus 0.011 or something like that okay so that but that's correct because that's going to cover the true value 95 of the time because the True Value can be anything because we don't have any information on it it's unsettling to get confidence intervals that are the real line but it's the right answer okay there's actually a paper but I do for an econometrica in 1997 that makes that point um okay there's a problem with this distribution Theory which is that it involves mu squared mu squared is this concentration parameter it's a nuisance parameter that describes the quality uh the the strength of the instruments so unfortunately these distributions can't be used directly to obtain corrected dis to create create to correct the distributions but instead they can be used to develop theories about how to perform inference better okay so so what I'm going to talk about now is a little bit of detection of weak instruments and then I'm going to talk about hypothesis tests and um and confidence intervals okay okay so I'm doing I'm actually doing okay detection of weak instrument should actually be pretty in in some sense this is like really easy in the IV regression model if every data set came with mu squared so it was like at the top of the Excel spreadsheet it had mu squared equals then you just know like how big mu squared is and then it would just be a matter of doing some simulations with that value of mu squared and then you could decide is this instrument weak or strong but unfortunately the data sets don't come with mu squared written at the top remember mu squared involved the true population Pi it didn't resolve the sample of Pi and those aren't going to be the same thing under this nesting or in when pi is really close to zero Pi hat and Pi aren't going to be the same thing in important ways so um so back to uh angrist and uh Krueger um in in their situation okay so if we go through and we do some calculations uh there's and I'll I can explain how how this this works so basically oh I okay basically here's the deal all right so mu squared is the non-centrality parameter of the first stage F statistic testing the hypothesis that the coefficients Pi are zero up to a factor of K so that says that the expected value of the first HF statistic is equal to Mu squared over K plus 1. now the F statistic has a distribution so that's not like going to be the greatest way to estimate it but this but basically what that says is the first stage F statistic testing the hypothesis that Pi is zero in the first stage regression is intimately related to the concentration parameter divided by the number of instruments plus one so big values of the concentration parameter means that the first HF statistic is going to tend to be big now the F statistics are random variable so it's not going to be equal to this exactly but it kind of gives you a hands on what's going on right so for reasons I'll explain concentration parameters of like 10 are pretty big concentration parameters are two or small 30 is is no problem at all in terms of the normal distribution Theory you know you can see it all from this Rothenberg thing right see look what if I divide these things this is just a random variable it's like this is like a chi-squared and this is like a chi-squared and this is normal what if I divide this by 30 and I divide this by 900 and this by 30 well all I'm left with is just the normal so it's going to be normal what if I divide it by 2 well I've got a normal plus a Chi Squared divided by 2 and a normal well 2 cancels one plus a normal plus a Chi Squared divided by four that thing is going to be really nasty two is a bad number and 30 is a good number and 10 is like a cut off that some people use it all seems so easy in hindsight okay so um so it depends on how many interesting instruments these guys use so when Ingress and Krueger only used um only use three instruments so they just use the quarter of birth but no interactions the rest statistic was 30. they actually didn't like that idea they wanted to get rid of some primary effects because they thought they thought that you know maybe if you were born in January you'd be smarter than if you're born in May or something like there's actually that's there's another reason but whatever having to do with your application they want to get rid of the primary effect and only have interaction effects and um and so uh 30 was a big number but they didn't really want to use that so instead they wanted to use all these interactions getting rid of the Prime effects and in that case the F statistic was 1.8 and so the estimate of mu squared over K is 0.8 or 0.877 and that's a very small number and that's a very small number and that's basically what the problem was okay now uh there's a rule of thumb in this literature which is to use the first stage of statistic of 10 as a cut off and a first HF statistic is bigger than 10 you can just use two CH squares and if it's less than 10 uh you need to worry let me let me make a comment about that um from an econometrics so I was talking the other day to a leading researcher in this area who is in this room and he said I never look at the first stage of statistic in my empirical work and but he said that for the right reason so there's a good reason and there's a bad reason to say that the bad reason is that you just want to ignore all of this and hope it doesn't hurt you hopefully nothing you know hope it goes away I hope it goes away and that that wasn't he actually is deeply concerned about these issues so instead the reason it was a good reason is he just says I'm going to move Beyond here and I'm going to use procedures for which the inference is completely robust whether or not I have a big mu squared or or not all right so if you are using robust procedures that work all of the time no matter what mu is then you actually don't need to look at the first digestive statistic and worry about it many people in practice do however like to like if you can run stata and look at two stagey squares and then in IV reg 2 it says first stage that statistic is a certain number and if that number's 20 you can say great I don't have to do anything more and if it's four then you have to start to worry so some people like to have that cut off as a practical matter and I can certainly see why um there's a number of ways to refine that cut off so that cut off is just a rule of thumb and so here are there's a whole bunch of methods that have been proposed in this area uh and I think this is the most recent one the 2005 um paper uh is the most recent one that I'm aware of of methods in this area uh for this sort of pre-test for weak instruments um so I'll describe this method briefly because that's essentially a justification or it turns out that that yields this cutoff of 10 as a rule of thumb and then there's reasons that you wouldn't want to use these other methods so I'll talk about those briefly so essentially okay so so there's a so saying that I'm going to decide whether so the question now at hand is do we have weak instruments Would we not have weak instruments so that's a decision we can you can pose that as a decision problem um and you want to have some database rule that is going to give you a yes no answer to that um comparing F to the number 10 is one way to do it uh more generally we could compare F to a critical value some critical value and the critical value might be 10 or it might be some other number let me give you let me give you let me give you uh let me give you one way to do this see here's the problem this is this is I really don't like this slide because it involves saying so I'm making some approximations and sort of you know purists don't like making approximations here's here's the problem which is look there's a Continuum of mu squared equals zero to Mu squared equals 100 and um and as a practical matter you might want to have different approaches to your data depending upon what mu squared happens to be so you need to somehow draw the line any way that you draw the line is going to have an arbitrary nature to it so here's one arbitrary way to draw the line one arbitrary way to draw the line is I want to make sure that my value of mu squared is such that the bias of my two stage Lee squares estimator relative to the bias of OLS will be no more than 10 percent so I'm only 10 as bad as OLS okay turns out that's tractable that's that's one thing you can say for it so you can write down this formula here and uh that's looking at the ratio of the two stages squares estimator or the IV estimator in a normalized normalized uh to the OLS estimator and the nice thing about that is that's a ratio of quadratic forms and it turns out that you can solve that ratio of quadratic forms in a way that turns out to be an eigenvalue problem and you can say what's going to Mac what's what's the correlation between what's the degree of endogeneity that's going to in a vector sense that's going to give me the greatest bias uh in of two seji squares relative to OLS as a function of the concentration parameter and it turns out that that's an eigenvalue problem that you can solve and so that gives you a number okay and this is the number as a function of instruments so K2 is the number of instruments and if you have a small number of instruments uh for bias is equal to 10 the cutoff number is maybe four or five or six or seven or eight and then it sort of bias a point of ten percent then it sort of asymptotes out here 2 to 10. you can actually prove that this asymptotes to about 10. okay so then then you can say okay so this is the kind this is the concentration parameter that I'd like to have and now remember the F statistic is related to the concentration parameter because it's the non-centuality parameter of the F statistic so you can then say I want to have a test based on the F statistic related to a critical value well that critical value now is going to have to be based on the non-central F distribution with this concentration parameter so um so so that's the that's the procedure and um and so that's that's what the that's what the test is uh and so um the general version of this is something called the Crackdown statistic and so you compute the minimum eigenvalue of essentially a matrix F statistic so it's a matrix version of the first HF if you have multiple included endogenous regressors then your first stage regression is is multiple regressions right and so then there's going to be a matrix F statistic and then the minimum eigenvalue of that is something called the Crackdown statistic and we want to test Craig Donald introduce this to test for non-identification we are totally uninterested by the way in non-identification right if it's I don't care if it's unidentified that's not interesting we're not it's so senior colleague of mine many years ago when I was just first thinking about this said well this is really not an issue because all you have to do is whether is just test whether that Pi is zero or not but you see the whole point is that that's not good enough because Pi it can be small you're not interested in rejecting whether it's zero or not you're interested in it being big enough and mu squared being big enough that the contr that the distributions are well approximating and normal and having it reject zero isn't going to ensure that those ultimate distributions are going to be good approximations to normals so you need to be bigger than that and so we're not going to be testing the null of non-identification we're going to be testing the null that this minimum eigenvalue of that ratio up here is big is is uh is at least a certain value right so um anyway this is all implemented in software and then there's some critical values and you know what these critical values if you want to have this 10 ratio this critical values are like 10. well they're 11. or 10. so so here's these critical values but they're like 10. so I just say you just use 10. although people do use these critical values okay so there's other tests to use there's other tests that are available um we propose some other ones that are that are not as desirable for a variety of reasons I think one one important reason is that is that what well what we're really interested in is we're interested ultimately in having as much inference as possible to be fully robust to this concentration parameter that is to work uniformly in mu squared as long as the sample size is large and um and pre-testing doesn't really fit into that framework very well uh it certainly doesn't fit in that framework in terms of tests because we actually have hypothesis tests and contradance intervals we meaning the literature has hypothesis tests and confidence intervals that are in fact uniformly uh uniformly valid and so thinking about this as a testing problem is not I think very helpful I think thinking about an estimation problem estimation is much harder as I'll come back to later so there is some validity to it in an estimation sense I guess there's a couple of other procedures uh Han and Hausman have a test in econometrica and the intuition of their test is pretty cool it's remember that uh in into temporal elasticity a substitution example in Campbell's handbook chapter where you estimate it one way and you got you know one number and you estimate it backwards and you get another number and those numbers are supposed to be the inverse of each other and but they weren't at all so that suggests intuitively that what you could do is you could take those two and then you could do some sort of Delta method type calculation you could see whether one is different than another in a statistically significant sense and that's what Han and hausmen do so there's a nice idea and um so that's actually now going to be essentially if you think about the asymptotics they're supposed to be close to each other so you're testing the null that they're close against the alternative that they're not which means you're testing the null of strong identification against the alternative of weak identification which is the opposite of that F statistic rule the extrateric rule was testing the null of weak identification against strong identification unfortunately this test actually has no power to speak of so it you it actually doesn't reject against the null of non-identification you could put in non-identified data and the trouble is the numerators will be really different but the denominator will be really huge so it turns out that this test statistic doesn't uh doesn't actually provide any any power so this is it turns out um and and houseman's a co-author on this paper so it turns out that this is not a great idea um okay there's actually a whole bunch of if in stata as far as I can tell at least as far as I can tell the waste data is programmed is is that every time there's a new test proposed it just they just add one on but nothing's ever deleted so it seems as though there's like 27 test statistics or something for weak identification and stata most of them are not in fact any good um okay hypothesis has some confidence intervals all right so as I mentioned before uh it's useful distinct to make a distinction between fully robust methods and partially robust partially robust methods a fully robust method is one that at least when the sample size is large is going to work well uniformly in mu squared mu squared to be zero or two or ten or 500 and it's going to work equally well the coverage is going to be good for a for a confidence set the rejection rates under the null are going to be five percent for a test and so that's in some sense what you'd like to do is you'd like to have a fully robust method and then if you can't do that you could have partially robust robust meth Dawes and those those would be methods that are going to be less sensitive to weak instruments than two stage three squares well it turns out that for tests for tests it turns out that um that uh that these um procedures are really well developed and there are quite a few procedures now at least in the IV regression case for fully robust inference and I'm going to go through some of those so this literature is in a state of evolution it's not a finished product and the researchers have done very sensible things which they started at the easiest and they're working their way towards the hardest and the easiest thing of all is IV regression with one included endogenous regressor and the testing problem in that is I think solved in terms of first order asymptotic Theory so that's really great um but then the next step is two regressors and it's not solved there's some ideas floating around and I'll talk about them and uh and and then another problem that's much much harder is estimation and and there's not I'll give you some thoughts on that but there's not a lot of good ideas you know this is part of our bad habits and graduate teaching graduate students econometrics you know out of green it's just like you know root T beta hat minus beta goes to normal zero Sigma squared and so then you go beta hat plus or minus 1.96 standard errors and you think about those as being the same thing and they're not the same thing at all okay so testing and hypothesis testing on the one hand any estimation on the other are just completely different objects and it turns out that in the case we there actually exists in effect a uniformly most powerful procedure uniformly in mu squared for hypothesis tests in the single included endogenous regressor problem and no one has a clue about estimation it's a little bit strong but it's approximately true okay all right so so let's just think about two stages squares and fully robust testing um so the two CH squares T statistics remember I showed you those plots those simulations by Nelson and starts back in 1990 that has a really nasty distribution uh that we might be able to even see it here okay so one approach one approach is you could take a distribution like this and then you could look over all possibly say I'm not gonna I don't know mu squared I don't know mu squared but I could look over a range or some large range of mu squared and I could take a worst case critical value here and a worst case critical value here and then I could use that for my inference but as you can see from the two stages T statistic that's going to give you really idiotic critical values uh you know you'll reject for minus 1.96 or something but the other one up here is going to be you know plus 10 or something like that so that's going to be that would be one approach you could use for testing but it's a bad idea so using adjusted critical values based on distributions that are highly sensitive to instrument strength is is not going to be a promising approach um so there are other approaches that are more promising one of them is to use a distribution this approach number two uh to use a test statistic whose distribution doesn't depend on mu squared and you might say well gosh why have we been spending all of this time worrying about um worrying about uh all of these perverse problems when there's distributions or statistics that don't depend on mu squared and in some sense that's a great question so we could just do that and then there's a third approach um these are good but it turns out they're not ideal and then there's a third approach which really was pioneered by Marcelo Moreira which is using statistics whose distribution depend on mu squared but then evaluate those statistics conditional on a sufficient statistic from U squared so that you can get rid of the dependence on mu squared and you can do conditional inference so I'll talk about approaches two and three approach one um some of the very first papers in this literature tried approach one and then you do some simulations and calculations and it turns out is a bad idea so I'm not going to pursue it approach number two so this is in the nothing's new Under the Sun category uh which is that there is a statistic which dates back now um 60 years you know it's amazing I was at a conference I guess two years ago and I you know I was just I was just giving some talk and I was looking around the audience and there was Ted Anderson in the audience and so Ted Anderson is this guy who in 1949 with Reuben figured out this test statistic and he's still an active econometrician it is a very uh it's a thing to think about actually it's very impressive um anyway uh what the Anderson Rubin statistic does is it's incredibly uh clever and simple idea which is it says the following thing I know how to test the hypothesis beta equals beta naught in a way that doesn't depend on the strength of the instruments and here's how I'm going to do it I'm going to hypothesize my value well since it's hypothesized I can compute y minus y beta naught I can just compute the residual under my hypothesized value if that hypothesized value is the truth under the hypothesis that it's correct then that says that the error term is supposed to be uncorrelated with z right so the residual here under the hypothesis is just you which is uncorrelated with Z because e is exogenous so I can regress this on Z and check whether Z has a non-zero coefficient so I can compute the F statistic in the regression of Y minus y beta naught on Z and that's what it is and in the case that you have normal errors and exogenous X's this has an exact F distribution all right so that's the Anderson Rubin statistic so with this really clever so you haven't needed this you didn't need strength in the office and the reason you didn't need anything about the strength of the instruments is there's no first stage regression right so you're hypothesizing this and then you just project it on Z under the null this thing has an F2 testic now under the alternative if you get it wrong if if your value of beta naught is not the right number if it turns out you're wrong then that says that you've got some of the cap Y in here so you're going to reject but it actually depends on the strength of the instrument so if the instruments are strong so if the instruments are strongly correlated with cap y if you're off by a little bit it's going to pick that up on the other hand if the instruments are weakly correlated with cap y then you could probably be off by quite a bit and it's not going to pick that up so the power of this statistic against an alternative is going to depend upon the strength of the instrument from the concentration parameter but the validity under the null does not all right so that's method number one that's Anderson Rubin statistic okay so that's I just said all this you don't need it you don't need normality so I said exact F but of course this just asymptotically Chi Squared K Over K under usual Central limit theorem assumptions and this is this is uniformly valid in the strength of the instrument so that's procedure number one that works all the time okay [Music] okay uh uh so here's some advantages and disadvantages it's easy to use I think that's important to think about this for one second um that there's something very bizarre about the Anderson Rubin statistic in practice so let's just look at this for a second suppose uh where's my uh okay so here we are in our regression of Y minus y beta naught on Z suppose the beta is at the True Value so you're right you know beta naught but the instruments are not exogenous okay then this is going to reject so there's a very difficult aspect of interpretation of the Anderson Reuben statistic which is that under the null of exogenous instruments under the joint and all of exogenous instruments and radicals being a nod it has an asymptotic Chi Squared K Over K distribution but under it can reject for two reasons it can reject for non-exogenous instruments Anakin reject has beta is not equal to Beta naught now the way that anderson-rubin statistics are used in practice and I'll go through this is by inverting the Anderson movement statistic to construct a confidence interval so you find the set of values of beta naught that are not rejected by the Anderson Ruben statistic you can have a small a very small confidence set for two reasons a reason that you like because you have strong instruments and good inference or a reason you don't like which is that your instruments are kind of exotic not exogenous so you can get a lot of precision falsely because of non-exogenous interests it requires a fair amount of this is a subtle thing to interpret I mean you just you just have to unders this is just what it is okay there's another test uh kleibergens LM test this is the other statistic I'm aware of that doesn't depend on mu squared um it is uh has advantages and disadvantages it's advantages that is pretty easy to implement and it's available on some software disadvantage is that it has some very strange power properties so I personally don't recommend this there's another test that dominates both of these called the conditional likelihood ratio test okay so let's go back to uh go back to go back to your first course in statistics in graduate school and I'm sure you will all remember the definition of sufficiency that if uh that if you have a statistic s with the distribution that depends on Theta and T is a sufficient statistic for Theta then the distribution of s given T doesn't depend on Theta and so what that says in our problem at hand is that we can think of s as being a test statistic that's testing beta equals beta naught and then T is some statistic that's sufficient for what's Theta Theta in our case is Mu squared so that these test statistics every statistic we've looked at except for just these two LM and AR depend on mu squared and their distributions but if we can have some statistic that's sufficient from U squared at least under the null hypothesis then that suggests that we will then we will in fact be able to compute a distribute another will be able to look at the statistic of s but then conditional on T it doesn't depend on Theta what does that mean what that means is in operationally what you would do is you compute two statistics you compute s and t and then to reject s you have to look at a table of critical values that table of critical values has one one column one row of it is the value of T so you look down that Row for the value of T you look down that Row for the value of T and then you read over to the percentile so so so that's the sense in which you'd be doing conditional inference you're you're choosing your critical value based on the value of this other test statistic are these the other statistic t all right so that's actually what marrera pursued in his thesis and a couple of papers and there's this 2003 econometrical paper uh where he proposes a particular approach which is using a likelihood ratio statistic and so s in that case is going to be a likelihood ratio statistic and then there's some other object and I I think I I think I have a slide giving it for completeness but some nasty looking creature uh that is the sufficient statistics so this the sufficient statistic is this object right here QT which is this element in this thing everything here is internally defined so you could actually compute everything based on everything here I checked that that doesn't mean you have any idea what's going on okay and then that's so but basically basically she's got a statistic that's sufficient from U squared it turns out I I when I first saw marrera's paper the first question I asked why you are just conditioning on the F statistic and it turns out that's not the right statistic to be sufficient from U squared under the null hypothesis so that's actually sort of the deep a deep contribution of his papers figuring out what the right conditioning statistic is there's you can condition on the statistic but it actually isn't going to deliver inference as as powerful as this okay so that's a nice this is a nice paper so anyway that's um so how do you implement it uh the the tedious way to implement this is you could have a big table you compute the likelihood ratio statistic you compute QT you look it up in a table uh it turns out that this has all been programmed up and and thanks to Anna makushova uh you just type something in stata and it all works really well um okay so it turns out that this is uh so why is this this is actually I think the recommended procedure so I when I said I think the problem has been essentially solved in the um in the case where there's a single included endogenous regressor uh what I mean to say precisely is that the problem of hypothesis testing of beta equals beta naught and the problem of construction of confidence intervals has been essentially solved in uh to first order asymptotics at least in the in the problem of uh a single a single included endogenous regressor and the solution to that is actually using the conditional likelihood ratio statistic um uh this is which is uh this marrera marrera statistic um the big only downside is it's kind of complicated but you know that's just the way it is on the software that does it now I hope this let's see if this uh all right so if I have this going let me see if I have this going I want to show you some power curves all right so what we're looking at here is I'm actually what I've actually the quality of the graphics is not so terrific ugh so what we're looking at here is power curves and um what this is from our paper in a kind of Metro with Barrera and myself and um Don Andrews and so what this does is it evaluates what's called a power envelope which is the envelope of most powerful tests uh that you could have and so that power envelope of most powerful test there's this mu squared problem and then there's the alternative that you're testing against and so in principle you have a very rich family of optimal tests that's now got parameterized in two Dimensions mu squared and the alternative you can derive optimal tests in these circumstances and what we're looking at here is cycling through a variety of values of of parameters and the the the best line here the farthest up line is actually theoretically the best that you can possibly do with normal errors um in the normal in the usual T statistic a problem that you know when we go plus or minus 1.96 standard errors we do that so routinely that we don't really think about it it's quite remarkable that statistic is in in the classical setting uniformly most powerful for testing bait equals beta naught or at least asymptotically has that interpretation that doesn't happen very often in the rest of Statistics that's a very special problem it's an important problem but a special one so instead there's usually a family of best tests and Mark touched upon this in the um in the problem for break testing and this is just like the problem for break testing something where there's a family of most powerful tests and in general no one dominates the other the remarkable thing about this series of plots is that if you look at this heavy dashed line which is called CLR this heavy not this one down here this heavy dashed line is essentially always on the power envelope and this there's a this you almost can't even see it because it's so much on the this one here that's not moving is the Anderson Ruben statistic so there's the number of instruments increases and as the cover and as the correlation increases the Reuben statistic can have pretty weak power this one here is this clay broken LM statistic so the Cleburn LM statistic has really strange power of properties in the sense that you can be more likely to reject a close value than a distant value there's this non-monotonicity in the power function for the LM statistic when you invert it that results in confidence intervals that will that has have strange the and the clyber and LM statistic when you invert it for confidence interval if I remember this correctly it contains the limited information maximum likelihood estimator but it also contains something you could Define to be the limited information minimum likelihood estimator which is the wrong eigenvalue of the of the limit of the limo problem and that corresponds to these points over here so anyway if what this is all supposed to do is impress upon you the idea that this conditional likelihood ratio statistic of marrera is on the power envelope essentially all of the time that is to say it's effectively uniformly most powerful of all of this family of valid tests so this it turns out this uniformly most powerful result is something that holds um this uniformly most powerful result is something that holds among all tests both conditional and not conditional for beta equals beta naught in this problem of a single included endogenous regressor so basically a fair amount of computer power and hard work has gone into this but it seems as though the conditional likelihood ratio statistic is really the by far clearly the best test to use so let me just talk very briefly before we break about confidence intervals um there's testing and confidence intervals are the same problem estimation and confidence intervals are not the same problem standard errors are not involved with this all right so go back to confidence go back to hypothesis testing basics uh and hypothesis testing Basics and confidence interval Basics so um a 95 confidence interval is a set is a set valued function of the data that contains the True Value in 95 of all samples equivalently or alternatively it's a construction method you can think about constructing a confidence set as the set of values that cannot be rejected as true by a test with a five percent significant level so those are just complete those are equivalent statements so hyper confidence sets come from inversion of test statistics and normally we think about this as being beta hat minus beta naught over a standard error plus or minus 1.96 okay so um if this is the hypothesis test so right now I've set this up in a hypothesis testing framework this is the T statistic testing the hypothesis beta hat equals beta naught and I will reject um if it's less than 1.96 or greater than 1.96 but in flipping this around pulling standard errors over to the other side I get the usual algebra that delivers a plus or minus 1.96 standard error confidence set but that's very special to the fact that inversion of the T statistic literally entails no more than just multiplying both sides by the standard error um when it's a more complicated function of the data you can't do something as simple as that but you can nevertheless find the set of values that are not rejected when taken one at a time as the null hypothesis and that constructs a 95 confidence set as I mentioned this to four paper uh which says that these confidence sets if the if the situation is unidentified in the case that it's truly unidentified if 100 John bounds walked into the room with their fake data if their procedures were correctly working 95 of them should have had the real line as their confidence interval and that's what this Dufour paper says so the Anderson Rubin statistic has that property and the CLR has the property and so in terms of construction of them it's actually rather straightforward just for the Anderson Rubin you just just fiddle around with the quadratics when you have more than one you can do it there's here's some references when you have more than one I won't go through that um and then there's some other papers that are working on uh the case of of more than one coefficient okay um there's a variety of possibilities that can arise when you have a a Anderson Ruben confidence interval it can be disjoint uh it can be unbounded it can be empty and so I think the same thing is true for a con for the CLR statistic uh the CLR statistic thanks to Anna makushova's uh paper and um Brian poye's software this is all something that can be implemented the Anderson Rubin statist the the um the CLR statistic will always contain the limo Point estimator which is one nice feature of it okay I am going to what I'm going to do is I'm going to we're going to break now and I'll come back looking at an example of these hypothesis tests off which is by going through an empirical example which is a paper by Moto Yogo in uh restat a few years ago uh where he looks at this linearized uh consumption cap M trying to estimate the uh um this uh inter-temporal elasticity of substitution PSI and I'm not going to really focus on the results uh as much as just on the the methods that he uses and sort of typical things that he finds so I don't know if this is going to I hope this is visible on your uh print out these on the on the handout of the slides but this is a a table of some first stage F statistics that he found in this data and remember the puzzle was that you could run basically two stage three squares either direction and you got answers that just were not internally consistent and um and what he finds just for starters here's for the two-stage squares just for the instruments that happen to be used in this example which I'm not even going to go into because I don't really care about the example particularly it's just an illustration um the instruments that he used uh really did a fairly mediocre job of predicting consumption growth which is no surprise consumption growth is really tough to predict um and uh and then this is uh I think this is return on equities and not surprisingly he added half time predicting the stock market too so uh so the first HF statistic is quite low and um and uh and that's uh and and that was actually true for other countries uh as well you really couldn't predict consumption growth in the stock market in many of these countries um something let me digress just for one second something that came up with the break somebody asked me a question about first HF versus the r squared from that regression um sometimes people look at the r squared from the first stage regression let me give you two reasons why that's not the statistic to look at reason number one is that and the most important reason is that you can have an r squared of say 15 percent and r squared of 15 percent with 20 Observer patients indicates not a lot of of movement in the data that's going to be useful for identification but an r squared of 15 percent with 2000 observations is a lot of movement that's useful for identification so the non-centrality parameter um for an r squared of 15 with 2 000 observations it's going to be quite large the non-centrality parameter for an r squared of 15 with uh with uh 20 observations are going to be quite small it's the non-centrality parameter that matters not the not the r squared the second reason is another problem with the r squared and you see this this is really irritating sometimes people report r squared to First Stage regressions but that includes not just the instruments But it includes the other included exogenous regressors as well and that has no relevance at all what matters is the marginal predictive content of the instruments above and beyond the included in exogenous regressors so reporting the r squared for the whole kit and caboodle is a completely meaningless statistic it's the F statistic testing the hypothesis for the coefficients are zero on the instruments so if you have additional exogenous regressors in there it's only the instruments okay um then uh then he um then this this then he just finds that uh there's some uh this is just this table of results where you get um mutually inconsistent values for in two size you squares for PSI and one over PSI let me point out just one Intuit one thing he produces a couple of other uh estimates something called Fuller K which I'll talk about very briefly and limel limited information maximum likelihood let me point out something about limel which is it appears as though 34 is the inverse of 0.3 and in fact it is the inverse of 0.3 so limel is the one estimator here that actually is normalization invariant that's the way it works so the limel estimator does not by construction have this inverse problem that doesn't mean it has a normal distribution it just happens to be the case that it doesn't have this inverse problem all right so um here's some confidence intervals and so what he finds so this is this is this is this is really hard Okay so these are weird confidence intervals these are confidence intervals for PSI constructed in three different ways so he finds for the U.S the Anderson Rubin confidence interval is empty now that doesn't mean that you have a precise estimate what that means that there's no value of beta naught of the inter-templo elasticity of PSI there's no value of PSI that is consistent with the orthogonality conditions this is based on using the two equations both the stock market and the bond returns so there's an equity premium thing so that's what's going on okay so that actually makes sense there's no value of PSI that is consistent with both the equity and bond returns not violating the orthogonality restrictions so that's okay this null confidence interval is okay now in other okay so he finds one that's okay the LM uh we're not going to look at because I don't like the LM statistic because of this quirky properties conditional likelihood ratio statistic it actually looks like it gives you well it's centered around zero which is a little bit odd but it looks like it gives you reasonably precise inference taking these two together would say that your exogeneity cons your rational expectations conditions your your predictability conditions are being rejected so it doesn't make a lot of sense to use this as a confidence interval right it's not as though you would say I've answered the question the inter-temporal elasticity substitution lies in this range and ignore the fact that that's inconsistent with your orthogonality instead the model is rejected right so that's how I would read this line now there's also intervals that give the real line that's okay if there's weak instruments that's not necessarily A Bad Thing all right so here's some other ones that give the real line all right so that's just an example of the sorts of things that you can get but getting the real line is also is is actually in some circumstances the right answer when you have very little information you want a procedure that tells you that it's not the most thrilling thing if you're trying to publish a paper but it's correct all right all right um so extensions to more than one included endogenous regressor uh there's work by clybergen a 2005 paper that's not listed here um 2007 paper and there's other work that I'll talk about uh later okay let's talk about estimation and this is going to be I have a fair amount of slides but I think I'm not going to spend much time on them here's the so I made a statement that um might have seemed a little bizarre which is I said that testing and hypothesis hypothesis testing and confidence intervals for the single included endogenous regression regressor problem in IV are solved problems and we have essentially something that's uniformly optimal which is really unbelievable but estimation is really hard and we don't know how to do it so how can that be well so let me pose the question to you suppose that you have again go to the bound example of an irrelevant instrument can you construct an unbiased estimator and the answer is no you can't you can't construct an unbiased estimator because the estimator can't because it's totally unidentified the distribution of the estimator can't depend on the True Value so if the distribution of the estimator right because it's totally unidentified that's what we mean if I totally unidentified the distribution doesn't depend on through value the distribution data to spend on the True Value distribution of the estimator can't depend on the True Value so I've got an estimator that's got some distribution I can pick My True Value whoever I want right so it's not going to be unbiased it's not going to be median unbiased so to come up with there's no it's not really going to be a good research program to say I want a uniformly optimal estimator uniformly and mu squared because we know that there's not going to be anything with any you know optimality for Point estimation there's not going to be optimality problems think about interval estimation is that we know in the unidentified case the right answer is minus infinity to plus infinity and but in the unidentified case for Point estimation there isn't I mean I suppose you could randomize right you could go to some estimator that devolves to be 51 you flip a coin half the time you say minus infinity and half the time say plus infinity and then that's going to be in some sense unbiased if you can average the infinities but that's not the world's best estimator okay so the whole challenge here is to come up with estimators to do better than two states C squares it turns out two THC squares is just about the worst thing you can do for estimation and for hypothesis testing um so there's a whole family of things that um these guys back in the 60s and 50s invented called k-class estimators limo is one of those which is it's the smallest route of the certain certain determinantial equation um there's something called Fuller K which you saw in the um in the uh Yogo table which is to take limo limo doesn't have any moments so if you do a little modification to it you subtract off a small number then that makes it have moments and that's some guys proved that a long time ago and uh so there's been some simulation studies because of these problems of weak instruments there's been more interest in this problem there's been a fair amount of simulation work there's a very nice paper by Han Houseman and Kirsten that does a big Monte Carlo study back when these guys were doing it you know Anderson and kunatomo and More Moon and rothenburgen this group that was these people that were doing it back in the 60s and 70s they essentially just had to use algebra and their Monte Carlo studies were extremely limited because of the computational resources at the time so Han Houseman Kirsten did a really nice study they seem to think that this limo K estimator works pretty well compared to some of the other things let me give you my opinion on this and I'm going to emphasize that this is just my opinion my opinion is that the point estimation I'm not I'm just not a big fan of Point estimation in this problem um and I'm much more interested I think that the interval estimation is a more useful device basically the point estimators can have quite strange properties a great degree of bias really large Tails whereas these um whereas these confidence sets have um really quite well understood properties that and and nice Theory associated with them uh if you want to report a point estimate limel has some nice features which is that limo is going to always be in the Anderson Ruben confidence interval and it's always going to be in the CLR confidence interval I should mention something um which is that limo turns out I guess they didn't write this down limel has another nice I did the mill has another nice feature very interesting feature this is a non-obvious feature which it turns out that limo minimizes the Anderson Ruben statistic so if you did the exercise where you're you're doing this hypothesis test you regret you take y minus cap y beta naught and regress it on Z and you do that again and again and again and again and you get all these F statistics and then you say you find the minimum of those F statistics and you take the value that minimizes those F statistics that's the limo estimator so it's necessarily going to be in the Anderson Ruben statistic if the Anderson Rubin statistic is non-empty and it's also going to be in the CLR cclr confidence set okay bootstrapping it's been just a short time on this bootstrapping and sub sampling so just to be concrete here's a bootstrapping algorithm so you estimate beta by two stages squares and you estimate Pi just by the first stage OLS and then you take the residuals u and v the residuals you had in V hat and then now I'm going to now the way the bootstrap works is what you do is you resample from these residuals to compute fake data or artificial data resample data right so you're going to take the U hat and the V hat and the estimated Pi hat and the estimated beta hat and you sample at random with replacement from the residuals and you create some new artificial data why tilde and cap y tilde and then with your new artificial data you're going to do two cage these squares and then you do that again and again and again and then you build up a distribution of two stages squares estimators NT statistics and if you build up a distribution of two stages squares T statistics you could take the critical value of the bootstrap distribution and you could apply that to your actual data for example through the T statistic Computing in the real data so um the problem is that this doesn't work okay so um in fact you can prove that this doesn't work so it under strong instruments it does and you can prove that and under weak instruments it doesn't and you can prove that and the reason is that uh I guess one way to say the reason it doesn't work uh is that for this bootstrap to work if you think about what's going on you would need to have Pi hat being consistently estimated but under weak instrument asymptotics Pi hat isn't consistently estimated and moreover the distribution um essentially what you'll be simulating here's one way to say it you get the pi hat and the data Pi hat and the data is going to imply a non-centrality parameter mu hat based on the estimated Pi hat you're going to be bootstrapping a sampling distribution that has the that's based on the empirical non-centrality parameter based on the empirical Pi hats the problem is that empirical Pi hat because this F statistic doesn't equal the non-centrality parameter it's a random variable around the non-centrality parameter that those Pi hats don't equal the true pies and the delicacy of these distributions is such that you'll get the correct bootstrap distribution around the wrong mu hat around the wrong mu it'll be around mu hat rather than around the correct mu now you say oh that can't make a big difference well that's just a quantitative question either it makes a big difference or it doesn't and so you do a simulation it turns out it does and the reason it does especially for two safety squares is because you saw with that two stages of squares t-statistic distribution look like it was insane so it turns out that it depends delicately enough on mu that if you simulate it around the wrong Point you're going to get the wrong critical values and it can work quite poorly turns out there's another thing called resampling so there's a really nice paper that goes through all of this a couple of them by marrera Porter and Suarez you can read all that um they have a way to make the bootstrap work to give second order improvements um and the um in for example the ancient Reuben statistic but that's I'm not going to deal with that there's another thing called resampling um resampling is some modification where you don't actually estimate the pies and instead what you do is you take a little sub sample of the data so if your first data set is like 400 observations you might take the first 30 and then you a window of about 30 and then you slide through if it's in Time series and for each of these you would compute the two safety squares estimator and now you've got some scaling problem because you started with 400 observations and you're only using 30 but if everything is going to be approximately normal the scaling factor is going to be square root of 30 divided by 400 and then you can get an approximating distribution and sometimes that works in fact in many cases that works and it doesn't work here and the reason it doesn't work here is that the non-centrality parameter that you're going to be looking at is a non-centrality parameter for 30 but your actual estimate is based on 400 and so you've got the wrong non-centrality parameters the same problem the same reason the bootstrap doesn't work you're going to be getting a good estimate at the wrong place right so and it turns out this doesn't matter or not it's numerical and it does and so there's a paper a couple of papers by guggenberger Andrews and guggenberger that go through all of that stuff bootstraps don't always work okay so let me um shift gears here and talk a little bit about GMM so everything so far has been talking about IV regression so for a group of Time series um economists uh none of that is probably directly relevant a little bit of it maybe but actually it turns out all of it has analogs here and it's just easier to talk about it there okay let's do some GMM basics okay so GMM is uh you've got some some first order condition type thing here uh so in the linear case that's just going to be y minus capital Y times beta there's a two parameter value you've got some instrument Z um the orthogonality condition is that uh is that the expectation of the error term uh and in this case it's a g dimensional equation times the instrument is zero so you form this big vect thing that's GK by one and those are the moments that you're going to take that you're going to evaluate and you're going to use them to estimate this unknown parameter Theta and the way you do it is by constructing this big vector and you and you have some weighting Matrix and then you minimize this GMM objective function to get the GMM estimator the GMM estimator there's a lot of flavors depending upon how you construct the objective function given whatever moments you happen to have in the linear case this is just a y minus Theta Prime YT and this is just going to be if the for the right estimator here this is just going to deliver two stage least squares the right weighting Matrix it'll just deliver two stage three squares so that's GMM right so that should be completely familiar okay um so I'm going to go through the large sample distribution theory of GMM I'm going to do so in a way that is not completely standard uh because it's going to lead into how I'm going to talk about weak identification in GMM so what I'd let me introduce a few things here which is first of all I'd like to introduce the this idea of the centered sample moments um okay what in the world is this so Phi T remember Phi T from the previous page is the errors times the instruments so the centered sample moment then this thing is going to have mean zero at the True Value Theta naught that's the identification condition for in standard GMM setup for some wacky reason I'm actually subtracting the value here this is this expectation is zero at the True Value Theta naught but I want to carry this along for other values of theta 2. usually we don't have to do that usually we don't have to do that because all GMM asymptotics and the standard case is local so all that matters is the true value but in the IV case remember toothache squares wasn't consistent so I'm not going to be able to restrict myself to local asymptotics in the GMM case so I'm going to drag along this mean and look at this deviation right here all right anyway if you want to think about it as being at the mean that's fine and there's going to have some covariance Matrix and the version of the slides says typos in there which is GMM efficient GMM I think in your slides is upside down or efficient GMM is inefficient GMM I think in your slide so I think there's no inverse in quite a few of your slides but not all is that correct is the version of the handout yes okay they're sort of sort of a test I guess a test of myself I guess um so okay so the efficient TMM is you take this covariance Matrix and you use that as the weighting Matrix that's 2 pi times the spectral density of of the moment condition at frequency zero which was introduced this morning I'm not going to go through estimation of the spectral density Matrix that's Mark's going to talk about that on Wednesday morning in the first lecture on Wednesday morning okay so but it is you know the usual thing it's just like Nui West if you want to do it that way although we'll suggest some reasons why that might not be the best way to do it um so this is Nui West and uh and so that's going to be efficiency mm efficient efficient feasible GMM is you could do it in a number of different ways um you could you could for example do a two-step and you just started an identity Matrix or some fixed weighting Matrix you then use the parameter values estimated in the first step to compute the weight Matrix that you would then use in the Second Step you could iterate that so you could keep on going all right so that's those are these are two efficient emm methods another efficient GMM method is to use what's called The Continuous updating estimator for chance and Heaton and roan introduced in 1996 which is kind of interesting because what you do it's much more tedious than this this is pretty easy you get a Theta hat you evaluate the weight Matrix and then you find another Theta hat and evaluate the range here you have to evaluate the Matrix weight Matrix at every value of theta and it's it sort of seems like more of a moving Target now in terms of first order asymptotics it's not going to make any difference all three of those are first order efficient and um and they're all going to be consistent so to establish consistency just to remind you of that what you want to do is you want to argue that the objective function is going to be such that as the sample size gets larger it's going to be crunching down into a steeper and steeper objective function so the probability of being outside some range is going to be increasingly small so that in fact within some Epsilon window the probability of being inside that of the peak of being inside of the estimator being inside that goes to one so that's the consistency argument once you've got consistency perhaps at a suitable rate you can then take a Taylor series expansion and then because you're only local you know you're going to be in this neighborhood the Taylor series expansion can be made can be made such that you're just evaluating the derivatives in the second order the second derivatives at that local point and then you get this approximation to the the objective function so the approximation to the objective function works by the objective function getting tight and being approximately quadratic if it's approximately quadratic and if it's tight centered around the True Value then we're going to have an approximately normal asymptotic distribution and that's the essence of the standard GMM derivation and so you just work out what those terms are and you get the usual thing this should be completely familiar although this derivation might not be exactly the derivation you're used to usually the derivation works from the first order conditions but I don't want to work from the first order conditions I want your intuition to be developed not from the first order conditions but from the objective function because what's really happening what has to be happening is that objective function has to be going like this and getting quadratic up there and I'm going to suggest that's not a good approximation for IV or GMM with weak instruments and that's the essence of the problem why this is going to provide a poor approximation you know I say this will provide a poor approximation we already know it is a special case of GMM is uh is the linear IV regression model in which case efficient GMM is two CH squares so uh so so uh um so that's that's uh if that the two-step I should say two-step efficient two-step GMM is two safely squares and we already know in the 260 squares problem this doesn't work so it can't it stands to reason that once you start throwing in a bunch of non-linear stuff it's not going to work any better somehow the non-linear stuff isn't going to compensate for the weak instruments okay um okay minor terminology weak identification instead of weak instruments I think weak identification is a little bit more General a phrase weak instruments really thinks about instruments correlation between x and z and you don't really have a z in the general GMM setting in fact whatever the Z is is going to change depending upon where Theta is so weak identification is maybe a better way to think about it okay um so I think I've already said this if there's weak instruments um there's a couple of problems and one uh the curvature well one of the problems is they're not well approximated by a normal uh it turns out that this derivative here isn't going to be well approximate by the normal well approximated by a normal with mean zero and you can see that in two stages squares here's the two stages squares objective function and so this is exactly quadratic so certainly the quadratic approximation is a good one in this instance so it can't be that the real problem is failure of the quadratic approximation it's got to be a little bit more than that it's so this is an exact quadratic and now I've just expanded this out and so this corresponds this expansion corresponds exactly to that General Taylor series expansion except there's no Pro no remainder term here because it's it's exactly quadratic Okay so what's going on is twofold first of all the curvature this thing here is small well what is this this is Cap y Prime p z capital I well that should ring a bell it turns out the curvature in the two stages squares objective function in this gmms framework is proportional to the first stage of statistic so that's that first stage of Statistics showing up again let me see if I have I don't have the expression so the first stage F statistic the curvature is proportional to the first stage s statistic times the number of instruments and then just uh uh the variance of the of the error so the bigger the first HF statistic the more curvature there is the first the curvature Matrix this what that says also remember if mu squared is small what's going on here if in this usual asymptotics this thing is converging in probability to something to this converging in probability to this but this is the first stage F statistic and if mu squared is small it's not going to be well approximated by something that's non-random all right so this thing is actually in the weak instruments case going to be random well how about this is this going to be normal well no it's not that's that I mean it's not going to be unconditionally normal it's going to be it's going to be this object right here and we looked at that in fact in the weak instrument calculation we did at the very beginning this was this is the numerator term and this is the denominator term and remember how that numerator term had that Lambda plus z u prime zv or is Lambda for z v Prime z u expression so that actually is some fairly complicated fairly complicated object and conditional on this it's actually got a mixture of normals distribution or this actually has a mixture of normal distribution it's normal conditional on this so um so everything is messed up here where this approximation says that we're going to approximate this as being normal with a non-random variance Matrix and we're going to approximate this as being a constant well in Weak instruments this is the F statistic basically and this is some messy thing that's got some funny mixture of normals distribution that's not even centered properly so the problems with the two stages squares so the the whole problem with the GMM is going from here to here and going from here to here and so we need to have a different framework for thinking about GMM okay um so what are the alternative approaches uh by the way there's the Hanson heat in your own paper did a really great job of documenting the problems of uh some of the some of the strange features of the two of the GMM estimator and how so what did they find they found that in a problem in a Monte Carlo study that was calibrated to some a couple of different empirical problems uh that two stages excuse me that the two-step GMM converge to a different was it the the iterated GMM and the cue and two-step GMM all gave different answers that were quite different the distributions were centered in funny places T statistics didn't reject five percent of the time conference intervals didn't have the right coverage there was all these very strange very strange uh features that were quite quite troubling and that makes sense in the sense that they're all um when you look at it from this sort of weak instruments perspective again we have the same possibility of doing a variety of different approaches you could try finite samples but you know since there's no way you could try Edgeworth but you know that wasn't the right approach in the simpler problem so it's not going to be the right approach in the more complicated problem bootstraps and resampling they're not going to work here either so you're left with some dealing with something else and so this is here's another nesting that works and this gives some some results so this nesting is just an extension of the weak instrument asymptotics nesting that gives some uh to give some tractability uh and gives a framework for thinking about asymptotic robustness of different procedures and essentially what um the the device is going to be is exactly like this this nesting of the of pi over the square root of T but instead what we're going to do is we're going to think about the expected value of the GMM moment function not exploding so the one way to think about one way to think about what makes GMM work in the usual setting is that if you move a little bit away from the true parameter value the expected value of that moment function gets so out of control that your um that you're you're going to be driven to be near the true uh near the true objective function once you're in that local neighborhood then then everything and have a steep a steep objective function you're going to be you're going to be in good shape so we we need to break that link because that's that's the link that's not working and so the way to break that link is the same way as to break that link with the pi over square root of T pi over the square root of T approximation or um or C over the square root of T approximation and that's to allow uh this thing uh root T times this GMM uh objective function in expectation get um uh converge to a converge to a a constant so that's just the analog to the uh that's just the analog to the pi over root t or Pi or pi times square root of T approximation okay so where does that uh take us if I now look at this um these moments t to the minus one half times the moments that we're working with uh in the two stages squares case um I can just this is it's nice to work with that example because everything can just be done explicitly I just rearrange so I'm going to now write YT minus Theta not Y is capital is just U and then I'm left with Theta minus Theta naught times Y and so then I can break this up as to be U times ZT and this is going to be very well behaved that's what's going to obey a central limit theorem U times ZT plus something else and that's something else is going to be Theta minus Theta naught Prime y times ZT um and so this is explicitly what this first term is the society which is going to be Phi T minus its mean in the um in the in the uh in the in this expansion for the for the um two safety squares estimator plus then this non-random bit and this non-random bit is something that involves the strength of the relationship between the instruments and the estimators and the the instruments and and the included endogenous regressors um this thing is this this part here is going to be super well behaved and it's going to obey a central limit theorem and that's going to be a good approximation even with a reasonably small sample size um the trick is that this mean function here is going to be non-random and it's not going to be increasing uh quickly that is we're going to look at it in local local neighborhood and in fact it delivers exactly the right local neighborhood if you have in this linear case setting a p pi to be C over the square root of t so putting all of these pieces together um in general what we'll do be able to what we'll be doing is we'll write the moment function this the uh this sample average of the moment function with the square root of t normalization uh in these two pit bits the part that has mean zero now for all values of theta and this is why I've included this additional thing that I was mentioning at the beginning of subtracting out all values of theta plus then uh the mean term and so these are these two terms a mean zero piece and then the mean term okay what does that give us in terms of asymptotics well this this mean term here uh we're just going to posit that this thing converges to some limit uniformly in Theta and that's going to be the modeling device that will deliver this other non-standard asymptotics This this term here this PSI T of theta well in the two-stage D squares case this was just going to converge to a normal more generally this PSI term here which corresponds to this term right here is something called an empirical process it's an empirical process an empirical process is some function of the data like this it has mean zero that's indexed by something and here it's indexed by a parameter and this is so this is looks like something that should obey a central limit theorem and it does it's going to be a central limit theorem at any particular value of theta this thing's going to obey a central limit theorem we're actually going to model this or assume that it does so at every value of theta and in fact if you put together all those values of theta it's going to obey it's going to obey a functional Central limit theorem so this thing is going to converge to a gaussian stochastic process on the parameter space with mean zero and then a covariance function that's given by oh this should be subscripted Theta 1 and Theta 2. so that's a typo this should be Theta 1 and not Theta t this is the this is the covariance a gaussian stochastic process a gaussian a gaussian process a gaussian random variable a normal random variable is characterized by its mean its covariance Matrix a gaussian random process is characterized by its mean which in this case is zero and its covariance function which gives the covariance between the process at one point and another point those points are supposed to be in the slide Theta 1 and Theta 2. they are Theta 2 and Theta t and subsequently nothing at all okay okay let me talk a little bit about this so this is this goes in the category of complete digressions that are good for you so what's a gaussian stochastic process on Theta you've so you've seen you saw the functional Central limit theorem this morning right I you saw the functional Central limit theorem this morning and the functional Central limit theorem said that um if you if I consider some function PSI sub T of Tau which is going to be a partial sum of these epsilons from one to Capital Tau this is actually a function on the unit interval all right so Tau runs between 0 and 1. so this is this is in this terminology an empirical process it's a mean zero function uh that has that's indexed by this parameter Tau uh and so it's a mapping from in this case from Tau to the space of well if I may disconnect the dots continuous functions this obeys a functional Central limit theorem a functional Central limit theorem is um is what we need here and the only difference between that empirical process and this empirical process is that this is is well there's a couple of differences first of all this empirical process is in is indexed by a scalar whereas that one is just some general higher dimensional Vector but that shouldn't stress us out and the second Pro aspect which is a little bit interesting is that this process this empirical process has a very specific covariance Matrix so the covariance Matrix for this empirical process if I pick two values of this empirical process it's i t at Tau 1 and PSI T at tau2 this has a very specific covariance function and that's going to be this going to basically be the minimum of Tau 1 and Tau two right so that's the covariance function for Brownian motion process and so uh so that's the covariance function here there's a covariance function over here which is this thing that I did with the wrong subscripts at Theta 1 and Theta 2 that's going to be its covariance function it's not going to have this particular covariant structure it's going to have some other covariance structure but otherwise it's completely parallel at any particular Point Tau this thing is going to be normal at two points Tau Tau 1 and Tau two it's going to be jointly normal and the limit and any K points this will have a joint normal limiting distribution at any K points same thing here I could slice this at any K points any K thetas and it's going to have a joint normal limiting distribution and this is going to be as covariance function and so we need so what are the two things we need to um what are the two things we need to prove that this works the first thing that we need is we need something called and I think Mark mentioned this this morning which is conversion to finite dimensional distributions the convergence of finite dimensional distribution says that if you take this process and you slice it at any K points you cut it take any K values of it that that's going to converge jointly to a normal distribution right that clearly is going to be the case here I could slice it at two points in time it's converge to a joint normal same thing here uh same thing here the second thing we need is we need something called tightness so what I if if we can characterize it at any finite number of K points that means we can come very close to characterizing the distribution of the process over the whole thing but not quite we need something that's going to help us connect the dots so I can get two points that are very close to each other but how am I going to bridge that in between well I'm going to need some sort of continuity or what's called stochastic equi continuity or something like that that's going to allow me to make sure that I can carry that distribution from here to the distribution to here and so that's this this condition called tightness or stochastic continuity that entails going from a distribution on very many but finitely many points to the Distribution on the function and that's really actually this is the hard part of functional Central limit Theory and so um there's books on that okay so what that means is that this object here converges in a functional Central limit theorem sense to a stochastic process defined on the parameter space plus a mean process on the parameter on the parameter space and that makes sense it makes sense that this thing is going to have a normal distribution that doesn't blow up if we're blowing up it means that the instruments are strong all right so if we put this all together and we assume that the the weight Matrix has some limit then this is that the objective function is going to well what is going to converge to a chi-squared type process on on the parameter space so it's going to be a random process on the parameter space now this looks a bit much but in fact this is just a natural I think extension of what the two safety squares uh calculation did and in fact all of this if you follow through all of this and if you take derivatives of this objective function for example you get the limiting distribution of the two stages greatest estimator that we did back at the beginning so um so so this just generalizes this two-stage Square stuff all of the findings uh in the usual weak instruments asymptotics uh apply here in the sense that you don't get consistency you have non-standard limiting distributions J statistics don't have the right chi-squared standard errors aren't meaningful but all of this is exactly what Hanson Heaton your own found okay so so that gives us the tools that we need to start analyzing the problem all right so now this is completely parallel How would you detect weak identification this is actually unfortunate so this is a good research project okay this is a good research area um there's not unfortunately there aren't good tools for this I'm going to give you a couple of ideas a couple of I'm going to give you some basically kind of lame ideas okay so I lame idea number one is you can always just look at the first HS statistic that the trouble is it doesn't really mean very much and here the reason the first HF statistic was useful is that its mean was the concentration parameter unfortunately if I look at a robustified first stage statistic the it's non-centrality parameter is no longer the concentration parameter that's relevant for the distribution of the even in linear GMM of the of the uh of the IV estimator the GMM estimator right so the link between the first stage F the tight tight parametric link between the first stage F and the distribution is no longer there and so there's work to be done so I don't have an answer to that that's a good project all right um You can look at it I mean if it's 70 I guess that's probably a big number I have no idea what 10 means if it says 10 I don't know what that means another thing you could do is you could use a rights test so what right does is he essentially uses a version of the Crackdown statistic and this works in the non-linear case so this is not recommended uh the problem with this test the problem with this test is twofold first of all this is looking at the this is looking at the local curvature of the of the of the um of the GMM objective function there's two problems one is one problem and this is an important one which is in the GMM case in the general GMM non-linear case the objective function doesn't need to be single doesn't need to be single peaked so the objective function could be like this right in the general nonlinear GMM case in the IV case was nice and quadratic but that's not necessarily two in the general nonlinear gmf so you could any analysis that you do about the local curvature around the GMM estimate is only telling you a little whatever tells you it's only going to tell you something about what's going on right there in Weak identification it's a global problem and in fact I don't know if I'll get to this but there's an example and there's there's an example in one of these consumption cap M things where you construct an Anderson Ruben confidence interval for um this Interline for the coefficient of risk aversion and the council last in the crra utility function problem and you get two values you get two conference sets and one is centered around uh one is centered around two and the other centered around 40. and this literature says right you could you can reconcile the um you can reconcile the uh one way to reconcile some of these uh Equity premium puzzles is by having incredibly risk-averse people and you actually see that in terms of a double dip in the GMM objective function so so that's a a potential uh a potential problem of the right procedure which only looks locally around the GMM estimator the other problem is that actually this is a test for non-identification the issue is not non-identification but weak identification now the thing is the test uses some bounding procedures so it actually uses a really conservative critical value so it has very little power against small departures from non-identification so in practice it seems to work okay but I'll tell you the theory is not really what one would like so for those of you for the graduate students here are trying to find a good theoretical econometrics problem this is a good problem to work on um here's some symptoms so if you run your if in your analysis you do GMM and you do two-step and you do iterated and cue and you get three really different answers that's a really bad sign okay that's a bad sign so that's an indication that something is going on if you can plot a concentrated objective function and you you know that's another sign things are not working well okay unfortunately there's not a lot of tools there let me talk about hypothesis tests and confidence intervals because there are some procedures here that are useful all of that setup of that gaussian stochastic process stuff has a payoff here and the payoff or part of the payoff is in noticing that at the True Value uh this should this would typo this should be Theta subnaut at the true value of ah let me back up I'm gonna back up all of that work setting up the gaussian stochastic process has a payoff and here's where the payoff is and I want you to look at this particular objective function which is something called The Cue objective function so this is the Hanson heat in your own continuous updating estimator objective function the thing that's interesting about this objective function is that instead of being iterated or two-stage we're evaluating the weight Matrix at the same point as we're evaluating these moments all right so remember this weight Matrix is the hack estimator and it's the hack estimator that's based on the basically the outer product of these things all right so the weight Matrix is evaluated using the optimal GMM weight ratex at the same points the numerator and the denominator are evaluate the same points that's what we saw in the Anderson Ruben statistic the numerator and the denominator were evaluated at the same points cue objective function is the GMM analog of the Anderson Rubin statistic and in fact suppose that I'm evaluating it at the troop and suppose that this is evaluated at the True Value the cue objective function evaluated at the True Value is actually going to be chi-squared well why is that at the True Value this gaussian stochastic process is normal but there's no mean part of it because it's the True Value so the moment condition holds so this is a normal this is a normal what's this well is that the True Value so there's nothing fancy about this this is just a hack covariance Matrix estimator at the True Value it's just going to be what it is well what it is is the covariance Matrix of this guy so it's chi-squared and so at the True Value the cue objective function it has a chi-squared k limiting distribution and that's robust weak instruments there's no I didn't say anything about strength of instruments anywhere that's the True Value so that's exactly the same story as Anderson Rubin so we can actually think of the cue objective function as the non-linear extension of the ancient Reuben statistic so actually that's what I call it I call it the non-linear Anderson Ruben statistic or the GMM Anderson Ruben statistic this Insight there's this really great like paragraph and table and a paper by Katra Lakota in 1990 that had this Insight that was just totally missed it was lost to the world as far as I can tell but this was figured out quite a while ago okay um so what that says is you can test you can come up with a you can do you can do a test that's valid asymptotically no matter how strong the instruments are that is it'll have the right size by just evaluating the cue objective function and comparing it to the chi-squared K critical value you can also you can also construct confidence sets and so the way you would construct confidence sets is by considering the set of values of theta for which you fail to reject using this GMM Anderson Rubin statistic and that's going to be a valid 95 confidence set for the true parameter values and that's true no matter how strong or weak the instruments are I should mention that this has the same interpretation issues that the Anderson Ruben statistic does so it could be in some sense at the True Value but if your instruments are invalid then you're going to be rejecting so it has this interpretational uh has this interpretational problem and it's a subtlety I guess problem isn't the right word it's just something you have to be aware of okay um and this just goes through the algebra saying that this is uh this is in fact in the uh in the GMM excuse me in the uh linear IV problem this is just the um uh this is just the uh uh Anderson Rubin statistic and that comes from the fact that the optimal weight Matrix is just going to be z z Prime Z inverse Z Prime um with uh and I've just put in some estimate of the standard error the second stage regression and then and then you get this Anderson Ruben statistic out okay um there are a couple of other men approaches and I'm going to mention these I'm going to mention these but I'm not actually going to recommend their use and the reason is not so much that I think that they're necessarily bad ideas although I'm not so sure I would endorse the LM version of it because the LM Works has such strange properties in the linear model it's more that these really haven't been particularly explored so there's been very very little uh study of in Monte Carlo situations or empirical empirical experience working with these methods there's an LM version of the GMM statistic that kleinbergen developed there's this GM there's a there's a GMM excuse me there's a GMM version of the LM statistic that clydeburger can developed there's a GMM version of the CLR statistic a couple of those one that we proposed one that's in a paper bike Library there's some other methods that have been proposed looking at so-called generalized empirical likelihood objective functions these these so so the generalized empirical likelihood objective function stuff one of the main points of guggenberger Smith is when they use this weak identification asymptotics they actually show that under weak identification all these gel likelihood GE yell tests are asymptotically equivalent to the cue so it's it's really not clear why you'd want to use any of these others maybe there's better performance in certain regards but they're going to essentially have the same properties as the cue objective function based test which is the non-linear or GMM version of Anderson Rubin so I'm I'm putting these slides up there this is an active area of work as you can see these papers are all quite recent um I don't think that there's any I it's there's I don't think there's any uh uh Clarity in terms of advice coming out of this at this point Beyond what's on the previous slides here which is using this GMM Anderson Reuben okay um and I'll go through an example of that in a little bit okay Mark did warn me that keeping you guys awake at 4 30 would be a challenge so um uh it's been a lot of work here okay talk a little bit about estimation so I I guess I told you my opinion about estimation in the linear model which is the estimation as a theoretical matter is hard and as a practical matter you get some goofy things in Weak instruments we've seen that in the GMM simulations in Hanson heat and your own so all of those opinions carry over to the uh to the situation a situation in GMM I guess there is something to be said if you want to do estimation so so so the two-step is just two step is the GMM version of two safety squares we know for sure that two set G squares is one of the worst things you can possibly do in the linear case so that does suggest that two-step is probably not the greatest in GMM uh limel will at least be inside this cue or GMM Anderson Rubin confidence set so Computing the limo estimator seems to be a plausible thing to do you have to be prepared for some enormous outliers however um so I don't I don't think I have too much more to say about it uh there's um there's some interesting theoretical work in this paper here about um uh and and Monte Carlo uh stuff too but interesting theoretical work this is a very nice paper by uh Houseman by the by the MIT crowd um on uh on uh on uh on cue um although it's also been found to exhibit wide dispersion okay as I mentioned um as I as I mentioned the uh the so-called generalized empirical likelihood estimators are asymptotically equivalent to cue under weak instruments so I don't at least to first order don't see any particular reason why one needs to go down that road um uh so those are just some thoughts this Fuller k type thing is promising but this is in you know some unpublished paper that we you know just don't know too much about it yet okay so let me um let me try to get back to reality here talking about some empirical application okay so I'm going to look at the new Keynesian Phillips curve and the uh so this is great okay so look it says golly and gertler maverittis misspelled Nathan and Smith okay so three of those people are in this room well let me talk about their work okay um so here's a new Kingsman Phillips curve uh with a lag in it and then I've thrown on an error term which does or doesn't need doesn't need to be there for these purposes and uh and so this is the moment uh the the rational expectations moment condition um and uh and then here's the instrument set which is with these additional lags the T minus one dated uh the T minus one dated uh data there's two included endogenous regressors there's the expectational term here with your intra instrumenting for and then there's um uh the uh the activity variable um whether that's a you know an output Gap or um or uh or or or whatever um so uh so there's two included endogenous regressors in this case there's one so-called included exogenous regressor which is part of the instrument set which is the lagged value of uh inflation and um and so in the terminology we have here there's two included endogenous regressors m equals two we will want to incorporate the possibility of uh of say heteroskedasticity so this is not going to be a straight IV regression problem um and and there's an interesting issue about potentially incorporating serial correlation as well and so I'm not going to take a stand on whether you should or you shouldn't that really depends on what you consider to be part of the model's Primitives but I will at least just talk about what those results uh look like there's a very nice review paper which is um by Clive Bergen and mavrides which goes over this and has references to most of the literature so all I'm going to do is I'm going to show some exercises that a couple of people have done constructing Anderson Rubin confidence sets in this model and one of them is uh this paper by Dufour calleth in kitchen and so the way they do this is they constructed well they constructed this Anderson Rubin conference Set uh let me say they constructed this kind of Anderson movement conference that I Believe by a grid search which is why you see these um these bumpy regions in here and it's the non-rejection region of the Anderson Rubin statistic and so then this is plotted this is the thetas and omegas so for the derivation of the new Keynesian Phillips curve their status and omegas and then these are these Gammas here are the functions of the thetas and omegas and they're the they're the things that actually appear so I'll just focus on those there's the gamma and the future value and the Gamba on the back uh the lagged value and they're empirically what they find is that it looks like um you can't distinguish between what gamma and the two forward and lagged ones are but they seem to add up to one so that's the and so that's the that's the empirical finding from looking at the Anderson Rubin test I think one thing that's worth noting is that these things are most certainly not ellipses so if you were doing standard GMM and you want to construct a confidence set you would take the point estimate and then you would draw that little ellipse around it and so uh we even have Mark and I have a textbook that has that picture but this is not that picture okay so this is the point estimate and then this is certainly not the ellipse so that's that's actually the main this looks a little bit more ellipsoidal but it's probably two numerical accuracy kind of hard to tell um there's this this my understanding is that what these guys did is they they did not hack this that is to say they did not take into account the possibility the the did not um allow for um any serial correlation in the uh in this orthogonality in this in this excuse me an error term and so uh in the air term is is H times uh Z T minus one so that should have conditional mean zero given the past so that means it should be serially uncorrelated um but uh the instruments are doubly lagged so there could be an ma1 error that's introduced by the doubly lag instruments and if I understand it correctly that's what clyberg and maverittis did I should actually look at if I got this right good that's what they took into account and so that methodology and this was also done um it was this was this was done by resistance no this would have been done using a grid search also right on a finer grid and so these are their confidence sets they call these s sets the the terminology uh this is just the non-linear Anderson movement confidence set and so what you can see again this is in well now this is in Lambda and gamma f space so it's unfortunately not the same space gamma B and Gamma F it's Lambda and Gamma f space um where this is the forward value and this is the the activity variable and so the um uh confidence set is clearly not ellipsoidal uh in in either of these cases um how do you get if you have a confident set well you can get confidence intervals um by projecting that set down to one Axis or projecting it down to another axis and and that's what that's what you can see in those uh in these uh calculations so that's the um that's the uh that's the Clive we're going to have radius example and again um you know it's not like this is really bizarre a bizarre set it's kind of ellipsoidal but uh but it's not exactly ellipse oil but I mean my take on it is that in the end you know once these guys have figured out how to do this this isn't that hard to do and look either you're going to get standard inference in which case that's fine it was just a robustness check and you can put it in appendix or a footnote or you're going to get a different answer and if you get a different answer this is the answer to believe not the GMM printout okay so here's an example where identification is really weak and uh and this is this uh this is this um this is just the non-linear version of the of the Yogo problem so again a model maybe nobody really much believes but it's an interesting one to look at and certainly has played an important role in the literature historically um so we looked at uh we looked at some uh this should be two thousand we looked at some annual data here uh with stock returns and bond returns and then we did uh double lagging because of temporal aggregation concerns using consumption growth and stock returns and bond returns uh to as instruments and so here's the sets that we got so the so the the gray region is the Anderson Rubin confident set and so this is gamma and Delta okay so gamma is the this exponent coefficient and Delta is the discount factor and so um so the gray thing is the Anderson Rubin nonlinear Anderson Ruben confidence set this is the confidence ellipse that's produced produced by the output from a two-step optimal GMM so um so there is no overlap right there's no overlap between the GMM confidence ellipse and the Anderson Rubin confidence set now you can't report this right this this can't be this can't be what you would report I mean if you have these information you can't dismiss this I mean this is these it's possible as a matter of theory that these in fact if it's strong identification this basically this ellipse would be shaded Gray these two would overlap they're completely unrelated to each other but that's completely consistent with this weak instrument Theory that the two CG squares just go somewhere it goes somewhere useless and it produces standard errors that are just about as good as that .011 from bound and Jager okay let me talk a little bit about many instruments I don't know I let me see maybe I'm not clear enough two-step GMM is a really bad idea in this problem here is an example where fully robust weak Insurance instrument inference gives sensible and useful answers unfortunately you know this gamma F doesn't seem to be very well determined but that's because you've got weak instruments it's correctly telling you the right foot puts in the data okay so anyway I just wanted to be clear all right let me talk about many instruments well optimal GMM requires using the optimal instrument usually optimal instruments are really reasonably complicated functions of the data so in our settings in time 30 settings those optimal instruments probably include projections on the entire history the optimal projection on some entire history of the data where you can't Implement that you can only run a finite order Auto regression or something like that or in the first stage but we can play a game and the game we're going to play is that we're going to let the number of regressors that you include grow with the sample size so you might say so a typical rate that you'll find and we'll come back to this in in a hack a typical rate you might find would be something like say a k cubed or k is the P cubed which is the number of lags over T tends to zero or something like that or maybe it depends I mean you might get a square root or you might get a p to the fourth tenths upon how you do the calculations so the game then is more and more and more lags are always going to be adding just a little bit more to give you more efficiency more lags is better you're going to get a smaller Omega you're going to get more efficient instruments higher bigger values of P and that is incredibly misleading right that whole literature that I think that just really takes you down the wrong road think about what I've been talking about with weak instruments it's really hard to forecast consumption growth okay how is it going to be better to say let's so if I'm using this year's consumption to forecast consumption growth in this year's interest rates and this year's stock prices how can it possibly be better for me to start adding last years and the years before that and the year before that I mean that's not going to make that's not that in fact that's only going to make the weak instrument problem worse it's going to get you back into this bound Jager angris Kruger problem where it was all right if they only used three instruments but by the time using 178 they're cooked all right so so if you're working say in a hypothetical example estimating a new Keynesian Phillips curve using four variables and six lags those those like lags two through six are probably hurting you in terms of your in terms of your inference they might be adding a little bit in some vague sense to non-centrality parameters but they're making the distribution Theory worse let me give you let me give you the example all right so I'm going to go back to this example we did this at the beginning this is beta hat two safety squares remember how I did the weak instruments asymptotics converging to this object here I mean you I don't know if you're anyway I did this we did this you saw this okay there's enough information in the slide that you could reproduce this you can step by step okay and then we did then we did this projection remember I did the koleski decomposition of z u and zv I so I projected z u on zv and I got this orthogonal residual these are all asymptotically normal so this is these are independent okay so now I'm going to play a game I'm going to take this expression here and the game I'm going to play is I'm going to imagine that the number of instruments is getting bigger and bigger and bigger and so now Lambda has the dimension of the number of instruments so it is z sub V all right so it's the dimension of the number of instruments so how what's going to happen so I'm going to say Lambda is getting bigger and bigger but on average the amount of information per instrument is tending to a constant number okay so I don't know so what's Lambda Prime Lambda by the way you know what Lambda Prime Lambda is Lambda Prime Lambda as equal to Mu squared in this setup all right so Lambda I should have said that right a long time ago the denominator is a non-central chi-squared with non-centrality parameter Lambda Prime Lambda Lambda times Lambda is Mu squared so I'm going to say mu squared on average the average concentration parameter the average information per instrument is converging to a number it's not getting big it's not going to zero so I'm adding instruments and each one I'm adding a weak instrument I'm not adding irrelevant instruments because that would drive this to zero all right I'm not adding strong instruments because that would drive it to Infinity I'm adding weak instruments so I've got a list of weak instruments and I'm going to play the game add more and more and more of what happens with the distribution all right so more and more weak instruments and I add it to the distribution well Lambda Prime zv Now by a central limit theorem this is a just a non-random thing Prime a normal divided by K by the central limit theorem this is going to converge and probably to zero do you see this uh uh it's so late how can you do central limit theorem convert okay so Lambda Prime zv over K all right so what is that that's 1 over K times the sum I running from 1 to K of z i v v yes okay so Z times Lambda i z v sub I so z v is a k by 1 vector and it's a and it's a normal and so what am I doing I'm taking these normals am I adding them up and I'm dividing by K and so what is this thing going to do well it's going to converge in probability to zero because I'm dividing by K all right so that term so actually I'm going to get some nice simplifications weak instrument asymptotics when I take it to a limit many instrument asymptotics or more precisely many weak instrument asymptotics get rid of this term here that's nice and it gets rid of gets rid of this outer term here and it's going to get rid of this term you're going to get rid of some other terms and they're going to keep this term actually it's going to get rid of this is evz U term put it all together and what do we have well it turns out that if I let K 10 to infinity and I work everything out I'm going to get this expression right here Delta plus 1 over Lambda sum Infinity but wait we've seen that before saw that right here so what did it so it says that as K tends to Infinity beta hat two stage Lee squares minus converges in probability to Beta naught plus Delta over 1 plus the average amount of information per instrument well if what's what's what is it if the average amount of information per instrument is small how about zero zero is easy number to plug in it's the Plymouth OS is that amazing so you can actually have a you can have a lot of weak instruments actually if you have a lot of I'm sorry a lot of irrelevant instruments it's going to converge in probability to the P lamb of os wait that's exactly but right that's exactly so why did why did why did when they why did why did bound why did bound get exactly the number 0.081 remember I didn't really fully answer that I just said koshy ratio messy centered in the right place but in fact this conversion probability zero is K tends to infinity and the 878 instruments so that so they actually got exactly the same thing well use a lot of instruments and you don't have much information you're basically going to be OLS so my takeaway from this is just stay away from a lot of instruments other people have a different Takeaway on this and and and I think there's some validity to I so this is this is I'll call this more of the MIT research program and I I understand see there's some beauty to this look at all these things that go to zero when you have lots of instruments you get this additional level of averaging right so instead of having just averaging in the sample size we now have got an additional array of averaging in the number in in as the number of instruments gets large and that's going to kill a lot of these problematic terms so it turns out that if we imagine approximating a large instrument situation by K tending to Infinity that some of the real complications of the many ins of the weak instrument problem actually disappear and Things become quite nice you get some limiting normal distributions there's bias but sometimes you can even correct the bias so that's what I call the MIT research program which is essentially turning many weak instruments from a problem into a blessing okay so now that here's the rub so it's very it's a very interesting and intriguing research program let me talk a little bit about some things we know and some things we don't know one thing we know for sure so for those of you who are technical or sort of paying attention trying to catch out the professor this calculation was wrong what did I do wrong in this calculation I mean technically as a technical matter I made a technical sleight of hand I did sequential asymptotics so I took a limit as n tends T tends to Infinity to give me this fixing K and then somehow I assumed that that was uniformly good and I could let Cato go to infinity and that's not right it's not necessarily uniformly good in fact the condition that I need there's there's a there's a condition that I need for that uniformity it's pathetic it's cake to the fourth over T tends to zero that's not much worse than most of the conditions in this literature and the reason is that there's a lot of really big nuisance parameters like that Z Prime Z Matrix Z Prime Z has to be controlled but to estimate all of those terms in Z Prime Z right away you've got K squared terms so all of those are going to have to be well behaved so at a minimum right off the bat you can see you're going to need something like K squared over T going to zero it turns out that things get worse than that and you actually find a typical term to be like K cubed over T kending to Zero weakens from it asymptotics is K tending to Infinity but K cubed over T tending to zero so that's a very slow infinities all right so what do we know we do know some things we know that under these very strong rate conditions K is big but not too big arlm and CLR still work CLR it turns out you can prove some unbelievable results you can actually prove that in this limit that the CLR is uniformly optimal so that's not a proof that's often available uh it's uniformly optimal among all tests in that limit so it's really quite an astounding an astounding result it's not necessarily so surprising because we know that it works extremely well along the sequence but actually to be able to prove it in the limit is is interesting um it's a theoretical matter uh in terms of estimation you can exploit some of this you can come up with bias adjustments and standard error adjustments and that's this um and that's the series of papers that are provided provided there um let me give you my uh take on on on this literature I think this is a really intriguing idea and I think there's a I think it's very intriguing and there's potentially a lot to be gained from looking at these additional levels of smoothing or averaging that you get out of looking at many instruments I worry a lot about this rate condition and this rate condition does not seem to be something that's just there as an approximation it really seems to play an important role when you look at Monte Carlo's Monte Carlo performance when K gets very big really tends to deteriorate as a practical matter so if we look at 200 observations and six instruments K cubed over T is one so I don't know if that's big or small or going to zero if you look at the angris Kruger K cubed over T is 17 these these look like moderately I mean I don't know we don't have a metric for for saying that's big or small but these look big enough to me to make me worry about really trusting some of the asymptotics that would be coming out of this many uh this mini instrument literature that Sid I think that this you know it's very promising and there's some potential stuff that might be coming out of it but I would think that this is not really at this moment ready for prime time in terms of trying to exploit the many instruments so where that leaves me is in terms of practical advice so there's two things what do I give as advice to graduate students the last this whole the last hour has had about three or four PhD theses in it for sure to be written um the uh graduate students in econometrics what do I give to graduate students in empirical macro I'd say stay away from lots of instruments for the time being at least until we understand it better because what we do understand is they can really mess you up all right you can have seemingly precise inference in around OLS okay so how do you reduce the number of instruments well there is this procedure by Donald and newey which is an information Criterion method and I think there's some applications in which that's probably not a bad idea so they if you can rank the order of the instruments they have you do something I'll talk about information criteria more I guess on Wednesday afternoon but they have used an information criteria method that penalizes you for adding more instruments and looks at some variance amount of variance explained what they can prove is that if you have some strong instruments and some irrelevant ones you can prove that you're going to get only the strong ones and that's a nice result and that does suggest that if you have some strong ones or some weak ones it's going to sort of drive you in the right direction if you have all weak instruments I don't think that this this none of their Theory applies and I don't think any of their Theory would carry through to all weak instruments it's not at all clear what the procedure would do in the entire weak instrument case and I'm not sure that this would be the right way to approach it but it might be useful you know if you're saying I've got three I've got my first set of lags and I kind of think they're okay should I add second and third that's maybe a useful tool um the other thing I guess I would just urge is uh you know humility don't add lots of lags and um and and try to stay with a relatively small number of instruments okay so um I guess some I guess a final comment here and I probably said this about a hundred times is not to be fooled just because you run GMM two-step GMM and get small standard errors don't let that fool you and there are an awful lot of techniques that are available and becoming available in the in the sites that are given here that I think are reasonably useful at least as robustness checks or diagnostic checks so that's it thanks very much 