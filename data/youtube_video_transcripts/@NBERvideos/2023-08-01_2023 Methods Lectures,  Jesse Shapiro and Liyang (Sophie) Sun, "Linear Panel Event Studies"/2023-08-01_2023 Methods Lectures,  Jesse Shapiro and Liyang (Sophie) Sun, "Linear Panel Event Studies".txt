thank you all very much for being here thank you so much Jim for giving us the chance to talk about this I am Jesse Shapiro this is Leon you will hear me call her Sophie and we are going to be talking to you today about linear uh panel event studies so let me first set the stage and say what I mean um and of course it will sharpen up as we go so we're going to be imagining a situation today we are we are interested in some units I so maybe U.S states that we observe over some time periods T say years and we want to study some outcome why say employment and there is some policy variable Z say the level of the minimum wage and we are interested in learning the effect of the policy variable on the outcome and we're going to be imagining that the we the researcher are willing to specify some kind of linear panel model for the outcome and that is something we will make more precise as we go these kinds of methods are extremely widely used in Empirical research this is something that we produce Based on data from a recent paper over a quarter of NBR working papers use this kind of method in one way or another which is just a huge fraction and as you can see it's been growing very quickly something else that I think is a really positive development is that this growth in empirical practice has brought with it a growth in methodological interest and so this is just one manifestation of that this is counts of related papers in the Journal of econometrics and you can see that is also exploding although with something of a lag so this is an area where the econometric theory literature has been coming in to try to fill in gaps in understanding that are being exposed by advances in empirical practice which I regard as great um this is also an area where plotting is not an incidental part of the methodological toolkit but really an essential part of the toolkit and this is a picture of the kind of plot that people this is I think using made up data but this is the kind of plot people will be very familiar with seeing in many research papers or producing in their own papers and a quick count that we did suggest that the large majority of articles that are published that use these types of methods include something like this type of plot so that is a theme that you will see us come back to repeatedly throughout the day um there are lots of ways things can go wrong one of them is making plots that aren't very informative or potentially misleading another is in some ways specifying one's model or one's identifying assumptions or misunderstanding maybe the goal of the exercise but on the bright side there are lots of resources for addressing these uh uh challenges such as new resources for data visualization and new ways to assess sensitivity to various kinds of modeling specification and so we are going to spend some time talking through those things today and in particular this will be the outline for today I will give an overview which I am almost done doing and then Sophie will talk about issues around identification and estimation I will come back to talk about making plots maybe we'll have a little break then we will talk about some ways that things can go wrong and what we can do about those things Sophie will talk about confounding and testing for pre-trends I will talk about the popular subject of what happens when treatment effects are heterogeneous genius and then Sophie will wrap us up let me just give you a couple caveats so one caveat and this applies to me not Sophie is that I'm not objective um so I'm going to be giving you my take on you know what you should do and other people might see it differently and that's fine but I'm going to do my best to try to distill what I think I have learned from studying and working with these tools and then this one unfortunately will apply to both of us these lectures are not going to be fully comprehensive we're not going to cover every facet um and for sure there will be people in the audience who have questions or things that they're interested in that we will not develop today um the good news is that there have been a lot of really good survey articles that folks have written and we have links to a number of them in the slides and the slides are on the NBR website as we speak or at least they were when we started talking so folks can go and click through and read those and I'm not going to go through and read the titles but this is all available for people going forward so with that we have completed the overview and I will turn things over to Sophie to talk about identification and estimation thank you thanks so much Jesse for the introduction I'm in Yang San you can also call me Sophie it's a great honor for me to be back in Cambridge and to present part of this methods lecture um so this module is titled identification and an estimation I will start with the simplest model of linear panel event studies which is the classical difference in difference estimator for two periods before and after and two groups treatment and a control group for completeness I will review The Familiar assumptions that the did estimator needs to estimate a causal effect namely no anticipation and the power of trans assumption after that I will extend the classical did estimator to the settings firstly with multiple periods and then to multiple treatment groups that receive the treatment at different types thank you um as you just mentioned we are building on our material of several survey papers and for this module in particular I'm also going to build on classical references such as mostly harmless econometrics to motivate the classical difference in different estimator so this is roughly adapted from the 1994 paper by David card and Alan Krueger um the context is that the state of New Jersey introduced an increase in minimum wage in 1992 and the authors were interested in estimating the impact on on unemployment to do that the authors collected employment information for a representative sample of fast food restaurants not only in New Jersey but also in the neighboring state of Pennsylvania um so to motivate their did estimation strategy the author wrote in the introduction and I quote since seasonal patterns of employment are similar in New Jersey and Eastern Pennsylvania their difference in different estimation strategy effectively differences out any seasonal employment patterns so whatever is left is the causal effect of minimum wage increase unemployment the author very clearly Illustrated how this did estimator is constructed in their table 3 which I copy here and to help me describe this table I'm going to introduce a couple more notations so I'll use the binary treatment indicator di to differentiate between treated units and control units so since in linear panel event studies treatment usually happen at a group level here on all the restaurants in New Jersey are treated because they face the increase in minimum wage and on all the restaurants in Pennsylvania belong to the control group or untreated I'm going to use -1 to refer to the pre-treatment period and zero to refer to the period when the treatment is actually implemented so with this I can describe the four ingredients used for the GID estimator which are the four sample average of the outcome in this case employment before and after in Pennsylvania versus New Jersey as shown here in this two by two subtypo now we can construct the did estimate which is shown here in the bottom right corner of their table or formally it's the difference in the sample trends of of the outcome in the treatment group and the control group so New Jersey and Pennsylvania okay and um to discuss how this did estimator estimated the causal effect I'm going to introduce the follow the convention and introduce the concept of potential outcomes here denoted to be a yit of D where D belongs to either untreated or control and on treatment so here the potential outcome says the employment that would have been if the minimum wage increased for this fast food restaurant and did not increase for this fast food restaurant Now using this concept of potential outcomes we can see what the observed outcome in the data set would map to so all the classical restaurants in Pennsylvania are not treated so their observed outcome would map to the potential untreated outcome for New Jersey on all the fast restaurants are treated and face the increase in minimum wage so their absurd outcome or observed employment would map to the potential treated outcome now in this example we are interested in the average impact for New Jersey after the minimum wage increase or formally this is the average treatment effect on the treated as shown here written using the potential outcome notation on the slide so let me unpack this expression a little bit in the square bracket we have the unit level treatment effect so this is the difference between the potential treated outcome and potential untreated outcome for the same unit the same restaurant so this is telling us the change in the employment for this restaurant when the minimum wage increased for the restaurants in New Jersey we already observed their potential outcome with the treatment however the potential untreated outcome at the same time for the same restaurant is actually not observed so that's why this is usually referred to as the counterfactual outcome because that's never observed in the data now if we take the average over the treatment effects within the treaty group within the New Jersey then we would arrive at this average treatment effect on the treated and to estimate this quantity seems as I um explained the counterfactual outcome for New Jersey is never observed data alone is not going to get us anywhere to estimate this ATT and we would have to introduce some structure or assumption for us to make progress so there are two usual assumptions invoked here for the did estimator to estimate the average treatment effect on the treated the first one is called No anticipation so this assumption says the outcome is not affected by the treatment prior to its implementation um in the example of minimum wage study this means the employment is not affected by whether this restaurant would eventually face the increase in minimum wage and assuming no anticipation let me walk you through how this assumption can help us make progress so as I mentioned the observed outcome for units in Pennsylvania which is the control group would all map to the potential untreated outcome for both periods without any assumptions The observed outcome for the restaurants in New Jersey would map to the potential treated outcome however with this no anticipation assumption since the outcome is not affected by the treatment prior to to the implementation we can swap this potential treated outcome for the potential alternative outcome so this top right corner is what the no anticipation assumption buys us this is an assumption we made about the data so of course we might wonder about potential violations and in this example a potential violation would be that the fast food restaurants started laying off minimum wage workers in anticipation of this increase in in their wage other examples out in the uh in the real world would be for example and there could be consumption smoothing behavior in anticipation for a future job loss the other assumption that's required for the did estimator to estimate causal effect is the familiar uh parallel trans assumption and this assumption says that the potential untreated outcome for the treatment group or the counterfactual trend in New Jersey would evolve in parallel with a potential entreated outcome for the control group or Pennsylvania so this is in this example if minimum wage never increase for New Jersey the average Trends in the employment would coincide between New Jersey and Pennsylvania again we need to think about what potential violations of this powertrans assumption could be so for the example of minimum wage increase that would be that there is a confounder which is the local labor market demand and if the local labor market was improving for New Jersey around the time of minimum wage then we would see then we would probably guess that on the counter Factor trend for New Jersey would have already been an upward Trend compared to Pennsylvania other examples in the literature would include for example an action felters dip which describes the phenomenon that participation in job training programs would already be on a downward Trend in wage income and that is the reason they select into job training program so this parallel Trends assumption is an assumption that tries to connect the potential outcomes between the Twitter group and the control group however this is still very different from the unconfunded assumption that is usually invoked in an rcts which says there's random assignment such that the treatment assignment is independent of the potential outcomes so firstly if we are looking at a situation with random assignments this is rooting out any selection bias and in that regard a powertrans assumption is different as it allows for potentially non-zero selection bias when we measure the selection bias in terms of the level different average level difference in the potential untreated outcome between the treatment group and the control group and in particular um if we just manipulate the power trans assumption a little bit moving terms from the left of the equation to the right of the equation we would arrive at a stability of selection bias type of assumption which says that we can have non-zero selection bias but it has to stay the same in the pre-period as the post treatment period so in that regard parallel trans assumption is weaker but it's stronger in different aspect and the important one is that the random assignment assumption or uncomfortable assumption is invariant to scale but on in contrast the power trans assumption is very dependent on the scale we're measuring the outcome at so in the example of minimum wage study if parallel Trends assumption within code for levels of employment and buying our hold for a log of employment and vice versa in fact recent work by John Roth and Pedro Santana showed that if we change the scale that we're measuring the outcome at then for the power trans assumption to always hold we actually have to be in a setting that's very close to our CTS thank you now that I have introduced the two assumptions um let me just quickly verify that the did estimator is indeed a non-biased estimator for the average treatment effect on the treated so on the first line of this derivation I just have the um the equality that this did estimator is unbiased for for the average expected sorry expected difference in Trends between treated and control group without any assumption now in the second line I'm going to use the no anticipation assumption after I have mapped The observed outcome to their potential outcomes and the no anticipation assumption allowed me to swap the potential treated outcome for a potential entry outcome here in the last line of this derivation I use the subtract and etric to introduce the potential treated outcome for the treated units in the um in the post-treatment period so that we arrive at this average treatment effect on the treated and conveniently the leftover terms here is assumed Away by the parallel trans assumption so this is how we see the GID estimator is indeed an unbiased estimator for the ATT under the two assumptions we have laid out so far I've been talking about how to do did By Hand by taking the sample averages but in practice we are usually implementing the did estimator using regression and to do that let's define a vit as Jesse mentioned but just more concretely this is a treatment status indicator that is equal to one if the unit I is treated and is also in the post treatment period and zero otherwise then if we regress the outcome variable on the group fix effect and the time fix effects as well as this treatment status indicator the regression coefficient beta would actually be numerically equivalent to the did estimator that we have done by hand so far and to see that this is a two periods two group situation so this regression is saturated and based on the saturated specification we can easily conclude that the regression coefficient estimator would be numerically equivalent to the difference in the sample average sample average trends this regression representation is also useful when we don't have panel data sets for example if we have a repeated cross-section data we can still implement the did estimator and under the aforementioned assumptions arrive at an unbiased estimator for the ATT and so is the corresponding regression representation we can also collapse the data set to group level and run a group level regression subject to correct weighting by group sizes we can again recover the did estimator if we have panel data how this is usually implemented in practice is to run what's known as commonly known as the two-way fix effects regression so what's different from before is that I just swapped the group fix effects for the unit fix effects and everything else stay the same this two-way fix effects regression has been fairly popular in the large subsequent literature minimum wage and have been adapted to allow for a continuous treatment covariates multiple time periods Etc uh we will return to some of these extensions later but for now let me focus on the extension to multiple time periods first I'm going to extend to the setting um with multiple periods but still just one treatment group and a control group this might be helpful if we are interested in the effects of the treatment on a longer Horizon for example in this Seattle minimum wage Inc study in 2022 in this case let me use a G Star to denote when the treatment is implemented for the Trader group and we keep the same definition for the treatments status indicator CIT sp4 that's equal to one for a unit that's treated and also in the post-treatment period now if we take the first difference of this treatment status indicator Delta Zig and manipulate the time subscript a bit then we can create a vector of relative time indicators so to see that let's take k equals to zero then this would be an indicator for contemporaneous treatment status change and if we take k equal to K being positive then this would be an indicator for the start of treatment K periods that goes so it would be in the lacked period of the treatment and similarly if we take here K to be negative then this would be an indicator for the treatment is going to start K periods in the future so we are in the period leading up to the treatment um the set of relative time indicators are very useful because we can use them to construct what's commonly known as a dynamic specification so compared to the two ethics effect specification we saw just now here I'm going to swap the treatment status indicator CIT for this relative time indicators and of course if we put in all the possible relative time indicators we will run into multicolinearity so normalization is necessary and Jesse will talk more about the choice of normalization but the convention is to normalize relative to the minus one period and we can interpret the regression coefficient Delta case associated with the other relative time indicators as the normalized differences now with some algebra we can still show the regression coefficient as estimator for this Delta case can still be thought of as a did estimator that tries to capture the average treatment effect K periods after the implementation of the treatment using the -1 period as the Baseline period so this provides a very nice bridge to the to the two by two classical case we have seen just now now with multiple time periods we would also need to generalize the no anticipation and power trans assumption appropriately to argue this regression coefficient estimator is estimating the causal effect so the generalization for the no anticipation assumption is quite natural it still says the treatment has no causal effect prior to its implementation and for the power trans assumption it just says that the counterfactual outcome for the treatment group has to evolve in parallel with the control group for all the time periods now under no anticipation and parallel trans assumption we can interpret the regression coefficient associated with the lagged relative time indicators as estimating these cumulative average treatment effect on the treated so this would be the impact of the minimum wage increase on employment K periods after it's been introduced there are also other implications by the uh no anticipation and parallel trans assumptions in the setting of multiple time periods which is that the regression coefficient associated with the lead indicators should all be zero and this is the basis for the pre-trend testing that we will discuss later next I'm going to um oops sorry extend to the setting with multiple periods but also multiple treatment groups and this is also a common situation because policy interventions might be rolled out over time and minimum wage increase was introduced gradually across States so for Simplicity suppose we want to estimate the impact of having experienced any increase in minimum wage then the setting would map to what is now commonly referred to as staggered adoption in a literature so mathematically what stagger adoption really means is that recall we have this treatment status indicator zit under the stacker adoption setting the zig has to be now decreasing in in the calendar time T So once a unit is treated it has to stay treated for the rest of the panel this simplification to stacker adoption is useful because it allows us to categorize units uniquely into treatment groups where the treatment timing is referred to as a GFI here where G stands for group and it's defined to be the earliest period at which the unit I has received the treatment so each treatment timing group or adoption group collects the units that started the treatment around at the same time so here I just want to point out the convention in literature is to use G of I equals to Infinity to refer to the um to the group of individual or units that never receive the treatment so this could be a little confusing but that's just a convention in the literature um now let me also generalize the no anticipation assumption and the parallel Trends assumption again no anticipation assumption generalizes uh rather naturally um but here the power trans assumption can have variations in terms of the generalization to stagger adoption so the strong version would say that the contrafactual outcome or the potential untreated outcome for all the adoption groups would have you would have to evolve in parallel to the never treated or control group so this is a strong assumption because it imposes power trends for all groups including a never treated group and for all time periods we can also think of a weaker version of the generalization which is to only impose the parallel Trends between the among the adoption groups so this might be useful in situations where we don't have a never treated group or the never treated group is actually very different from the groups that ever received the treatment so um the never treated group had they existed may have evolved not in parallel to the adoption groups but that's irrelevant because we only require parallel Trends assumption among the adoption groups now suppose we um continue to estimate the dynamic specification for the Stagger adoption with the normalization relative to the minus one period then in addition to no anticipation and the power trans assumption for stagger adoption settings if we want to interpret these Delta case as estimate for the cumulative atts here the specification also has a homogeneity restrictions placed on the treatment effects and to see that note here the regression coefficient coefficient Delta case only depend on the relative time K but not on the treatment timing Group G so this has the dynamic effects can only vary over time but not across adoption groups we will return later to cases where the homogeneity is violated but for now let's proceed assuming homogeneity on the treatment effects then under no anticipation and parallel trans assumption these relative time coefficient coefficients Delta k for the lagged indicators are still estimating the cumulative atts and we can take an average of this Delta case to summarize the overall average treatment effect on the treated so another option that's common in practice to estimate this overall effect is to estimate what's usually known as a static model so this is basically the same specification we saw at first in this module where we only have the treatment status indicator as the regressor instead of the relative time indicators the regression coefficient associated with the treatment status indicator beta Post in this stagger adoption setting is the correct summary for the overall effect if the specification is correct namely the treatment effects are truly static and other Delta case um stays constant for K greater and equal to zero however if the treatment effects are not static and then this model is misplacified and a recent work has shown that in settings without a never treated group so only adoption groups there are cases where this static specification can have severe mispecification under severe mispecification this coefficient beta post might not correspond to any proper averages of the dynamic effects Delta case and by that what I mean is that the beta post can be negative even though all the dynamic effects are positive and vice versa the 2022 paper by Claremont de chasma town and Xavier and um paper by Andrew Goodman Bacon have more details and proposed diagnostics for misplacification in this static model as a concrete example Claremont and Xavier had a example of two adoption groups and three time periods where we indeed have a positive and growing Dynamic treatment effects for both groups if we estimated the dynamic specification we can correctly summarize the overall effect by taking a proper average of the Delta 0 and Delta ones here however if we only rely on the static model we can sometimes be severely misled because in this example when these two groups are equal sized beta posts would actually be equal to minus one-half so this provides a very strong argument for not relying only on the static model which restricts the Dynamics of a treatment effect to summarize the overall effect of the treatment if we report estimates from both static and dynamic models and they are quite similar then we can combine these estimates into the single one while staying agnostic about the degree of misplacification in a static model and this is based on a framework developed by an ongoing project that I am working on with Tim Armstrong and paccline now I have covered um the common strategies under the no anticipation and power Trends assumptions in practice we might always might not always have strong reasons to believe these assumptions are plausible or applicable then there are many proposals for alternative identifying assumptions proposed in the in the literature and um previous editions of methods lecture including the inaugural one has covered several of these including changing changes semi-parametric difference in differences and phonetic controls and for this module I'm just going to mention a couple recent variations and won't go into the details so the first one is when we actually observe a cross-sectional data but units can be categorized by their birth cohorts sometimes we can still leverage cross cohort comparison for example the late birth cohorts in the treatment group are more exposed to the treatment than individuals in the early birth cohorts intuitively The Birth Cohort play the same role of calendar time so we might expect many similarities with the settings linear panel events at these but still there are many differences as discussed in the 2017 paper by Carmen Xavier they propose alternative assumptions as well as estimators foreign the second variation I want to quickly mention is when the treatment is indeed randomized or the treatment timing is randomized so for example in rcts treatment is randomized but we might observe past outcomes from Baseline surveys and another example is the treatment timing is randomized even though other units eventually receive the treatment so this is an example where the author studied in a setting where the schedule of tax rebate is randomized we can now rely on the random assignment assumption which is invariant to scale and recent papers by for example David McKenzie and John Ross and Pedro Santana proposed estimators that will be more efficient than the did estimators with that I'm going to conclude this module these are the list of the papers that are the basis of this module and I will pause for questions and then turn to Jesse for the next module great thank you very much and we are now going to talk about making plots so I'm going to start by just going back over some of the same material that Sophie covered but in pictures so graphically the uh and I should say I should say this I said this at the beginning I'll say again because I think this is important to emphasize for this class of methods plotting is not in my opinion an optional nice to have it's an essential part of the methodological toolkit and a plot like this should accompany every paper that is using this type of methodology I'm not aware of a reason for there to be an exception and increasingly there are a few exceptions in the literature um so let's talk a little bit about what these plots show and then some suggest I will make some suggestions for how to make them as informative as possible for the people using the plot which is usually the readers of your paper so this is a plot that uh represents uh the two group two period setting that Sophie started with so we have a before period which we're denoting minus one we have an after period which we're denoting zero and we have an average outcome for the control group and an average outcome for the treatment group and the first uh complication that we might introduce this picture is we might have data that cover many periods so Sophie talked about that and so we get to a plot that looks like this and this is a great kind of plot if you're in this situation this plot is starting to get a little bit busy um and because as Sophie said the idea the underlying identifying assumption here or the underlying set of identifying assumptions imply constant selection a common way to re-represent the data that makes a less busy plot that still captures in some sense a lot of the fundamental information would be the difference these two points so take the vertical difference between the average outcome for the treatment group and the average outcome for the control group and then I would get a plot looks like this where now instead of the average outcome on the y-axis I've got the difference in the average outcomes now again the the underlying notion here is relying on Parallel Trends parallel Trends is in some sense always about relative changes so we can't say anything about the absolute effect of the policy at a point in time without reference to a base period That's the essence of the identifying assumption that Sophie described so we one way to kind of visualize that or remind ourselves of that on a plot like this is to normalize the plot relative to a base period and a common choice of normalization although it doesn't have to be the choice is to normalize relative to the period immediately before the adoption of the policy or the treatment and so if we do that then we are uh basically just putting a horizontal line that hits the y-axis at zero and goes through this point uh uh at minus one and now we can read off immediately I haven't put numbers here because this is a made up cartoon example but if we had numbers we could read off immediately be implied yeah difference in differences estimate of the uh instantaneous effect of the policy on the outcome just by looking at the y-axis okay so what have we got here what's the information that's captured in this plot so we have one period where we're learning nothing that's our normalization right we're basically saying we can't say anything about what's going on in that period because we had to throw out we had to sacrifice that to go from unconfoundedness to parallel trends that was the point that Sophie made so we had to give up on that so that's our normalized period we have our pre-event periods that we might use for pre-trend testing and here it doesn't look so great Sophie will talk more about that and then we have our Dynamic tree manufacturer Dynamic policy effects so these are the post event effects and you can see here as I visualize them they're not all the same so this is a situation where that static model wouldn't be a good description of the evolution of the difference in outcomes following the adoption of the policy now one way to implement this plot which is a common way is to do it via regression so again I'll just to make this self-contained go back over the notation just so we're on the same page we're letting Z be an indicator that's one if I is in the treated group is in a treated group and T is after the treatment date and zero otherwise and then as Sophie said we can make this picture by running a regression of the outcome y on a unit fixed effect Alpha I time fixed effect gamma t and there we're going to allow for an error term Epsilon I won't talk very much about that and then we have this big infinite sum of first differences because Z is an indicator uh you can think of these in a situation where there's only one treatment that turns on at only one time you can think of these as just indicators for how for is there a change in the treatment status at K periods relative to period t right so that is as Sophie said before so if z i t minus K is one then that means that treatment started for this unit K periods ago right um and you can immediately see that this this regression is oversaturated so we won't be able to include the full set of these differences alongside these fixed effects so we're going to have that's a regression interpretation of the fact that under parallel Trends we're going to need some form of normalization and so again a common way to do that is to normalize this coefficient at the period minus one right before treatment so everything is in differences normalized relative to that period we can estimate this regression we can plot its coefficients that is a plot if you remember like your middle school representation of your Cartesian plotting that's a plot of this object here so we're going to plot the series that consists of these K's and the associated Delta hat K's right and what picture are we going to get out if we make that plot we're going to get the same picture that we had before because in the situation where we have many periods but only two groups this is an equivalent way to accomplish the same task as before now not all situations are as simple as this situation so you might find that you're in a situation like the one Sophie talked about where you have different units treated at different dates like maybe the minimum wage increases in New Jersey at one time and then later it increases in Pennsylvania you might have a policy that's not binary like the level of the minimum wage maybe you think that you know maybe minimum wages take many different values over your sample period not just one or two not just high and low and so you may have a non-binary multi-value treatment or you may want to think of it as a continuously valued treatment or you might be in the unfortunate situation where you can't estimate an infinite sum because like a lot of the data sets that I've been stuck with your data set doesn't have an infinite number of periods in it which is a super big bummer so you might need to do something a little bit different and one way to approach these situations though it's not perfect and we will come back to why is to just adapt that same regression model that I showed you before to this setting okay we can do that with very few changes actually um because what I showed was just a regression it didn't running that regression didn't require anything about the structure of Z the interpretation of the regression depends on the structure of Z but the computation of the regression pretty easy no matter what Z looks like so just to go back over the objects that you're looking at here what I am doing is I've got the unit fixed effect and the time fixed effect and then I've made this sum have finite endpoints so implicitly I'm only including going to be including in my plot B periods before the policy kicks in and a periods after and how am I going to do that I'm going to include a variable that tells me the status of the treatment a periods before period T and the status of the tree period the treatment b period B minus one periods after period T why are these the exact right ones to include we can talk about that in the Q and A if you want and we have the algebra I've cited a couple papers here that go through the algebra if you're interested what this will uh uh allow me to do is have a plot that goes from B periods before the policy kicks into a periods after the policy kicks in where the periods at the end are now in a sense grouped effects because I'm making the Dynamics finite so this is basically a simple version of what you might think of as The Modern event study plot um and uh uh that name was adapted from pictures and finance that looked a little bit like this so what we have here is uh we have our two pre-event periods except that now I put a plus to remind us that we've actually got a group of periods here and we have our post event periods where again I've put a plus to remind us that we've got uh uh more than one period represented here and we have our normalization um we have made a number of substantive decisions in going from the simple setting with the very general form of the plot to the more general setting with a somewhat more restrictive form of the plot one important restriction that is a substantive restriction and is not innocent is that I'm assuming that the Dynamics of the effect of the treatment on the outcome stabilize at a certain point so that I don't need to worry about what's going on more than b periods after the treatment effect in my view I mean obviously that kind of assumption should be justified by economics but I hope that something like that is reasonable because if it's possible for treat for things like the minimum wage to have arbitrary effects with massive and unknown lags it's going to be very difficult to learn what we need to know to decide what good policies look like so that's that's an assumption that I hope holds for the sake of all of us um likewise we're making a similar assumption in the other direction that whatever pre-event Dynamics there might be due to violations or parallel Trends or violations of no anticipation those Dynamics somehow are going to settle down at some point so that maybe people are not anticipating um uh the effect of the minimum wage 300 years in advance or there's not differential selection based on what was going on 300 years ago and then as I said before but just to emphasize this uh we are always going to be interpreting the Dynamics here relative to some fixed normalization we shouldn't forget that we can't say anything about absolute effects in some sense because we're using parallel trends for identification I will also now warn you and I will come back and flesh out why I'm warning you that this is only one possible regression generalization of difference in differences um I found it useful in applied research and it has the virtue of being very flexible because it is something you can essentially always do you need to make some decisions about those Dynamics you need to pick a normalization period but subject to doing those things you can always do this even if your Z is a continuous variable or multi-valued and no matter what your y looks like and so I think it's a helpful starting point for making a plot which in my experience is a helpful starting point for doing analysis and modeling and thinking about the economics of your setting but it may not be the ending point and you may want to do other things depending a little bit on the situation you find yourself in and that is something we will come back to in some detail because plots are a very important part of the toolkit I will also make some suggestions for trying to make these plots convey as much information to the reader user of the plot as possible so let me go through a few recommendations that I will would like to communicate so here is our most basic plot and I made it a little bit longer so there's a little more going on um so we have all the same elements as before we have our you know seven periods and more ago indicator we have our other pre-event periods we have our post event periods and we have our Dynamics forced to settle down between uh six periods and seven periods or after and we're plotting our coefficients and we've normalized the coefficient at uh uh one period ago one period before event time to uh zero as a normalization as I mentioned before so the first thing we'd like to do to make this plot more helpful is to incorporate some kind of inferential information so right now if I told you the effect of this treatment in the first year was one you might like to know is that statistically significant one way to make it easy to answer that question is to put pointwise confidence intervals on this plot and because of the choice of normalization these pointwise confidence intervals immediately allow us to test Point wise pre-specified hypotheses about particular event time effects so if I want to know is the effect at Time Zero statistically distinguishable from the base period I can answer that question by asking whether zero is contained in this point-wise confidence interval and as long as I knew in the usual frequent descent that that was the hypothesis I wanted to test before I looked at the plot everything will be totally fine sometimes I might like to test more complex hypotheses though I might like to be able to test the hypothesis like can I pass a line through the coefficients a flat line or is this represented by a quadratic or is there exponential decay or linear decay and if I want to test hypotheses like that and I haven't pre-specified them then I need to worry about the possibility of multiple hypothesis testing because I'm simultaneously testing hypotheses about many parameters at once and not just one of them this fortunately is a problem that has been studied a lot and it is possible to adjust the visualization in a simple way to make it possible to test such hypotheses by adding what are called uniform confidence bands so in the same way that this confidence interval is designed to contain the true value of this coefficient 95 percent of the time these confidence bands are designed to contain the true entire path of the event time coefficients the full path of the Delta K's 95 of the time and they're quite easy to calculate you just need to do a little calculation to get a critical value and they're obviously very easy to visualize because all you need to do is add these little end caps here or you know whatever you put little arrows or whatever you want to allow the user of the plot to test any hypothesis they want about the path of the coefficients without having to do any further adjustment for multiple hypothesis testing so that I think is an improvement to the informativeness or usability of the plot I think it's also useful and we'll talk a bunch more about this or Sophie well to make it possible for people to know whether the assumptions we made in building the plot are rejected and some of the more important ones that I mentioned are the assumptions about the way that the Dynamics stabilize at the ends so um one thing that I would suggest is to include a test of the equality of this coefficient and this coefficient that's the hypo so the test of the hypothesis that this parameter is equal to that parameter that's what I've called leveling off and I've got a p-value for that here and then another hypothesis that is suggested by the assumptions of no anticipation in parallel Trends is that all of the parameters all of the coefficients prior to event time are zero so the hypothesis that all of these here are zero that is the hypothesis of no pre-trends and that hypothesis I'm here I'm showing the p-value right on the plot so that seems like it may be a useful thing to label somewhere on the plot although there are caveats with interpreting that that Sophie will um come back to in the next bit of the series um the last thing that I think lots of folks use these plots for and part of why I think they've become so essential is because this plot helps us to ground our beliefs or intuitions about whether or to what extent we find confounding of one kind or another a plausible explanation for the patterns in the data so do we believe that there's really a causal effect of the policy or do we believe that there's something else going on a third factor that is related to the policy or the timing of the policy and is related to the outcome and so if I showed you a plot like this and said this is my estimate of the effect of the minimum wage on employment you might be reassured that you know for the most part we can't rule out that there's a pre-trend here through at least through the uniform bands and you might conclude that the minimum wage is great for employment and that could be right but you might also be thinking that it's possible that there is another Factor like the state of the economy that is you know moving along and maybe legislators at State levels don't increase the minimum wage when the economy is in recessions they tend to increase it when times are good and so you might believe that there is a slow moving compound that is uh instead explaining this pattern one way that you can use the plot to ask questions about that is to ask whether for example a line is statistically consistent with these estimates there's one way to do that without any help from any software is just to take your finger and ask whether you can fit a straight line through the plot I can't put my finger on Zoom so for those of you at home you can imagine that I'm doing that um and that's fine that will work another way to do it is to ask whether a line is inside the walled region that is can an f-test reject that all the coefficients lie on a line and that's also very easy to do helpful to have a computer for that one you need a little more than a finger but it's not that big of a deal so you could ask does a line fit inside the walled region and if you thought that one did and you were in an economic situation which you know you may or may not be where you thought that this is what the Dynamics of confounding might look like then that might make you worry on the other hand if that's the sort of situation you're in if you think that confounds look sort of smooth and you see something like this you might say well I don't think I can fit a line through this plot and indeed this is an example we cooked up where the line is rejected by the F test it's not in the walled region so maybe you would find it more plausible that there's really a causal effect although again that's something you would have to evaluate context by context depending on what kinds of compounds you think are reasonable so one suggestion that some co-authors and I have made is to automate this visual process by asking what is the least Wiggly path that you can fit through the walled region so what is the least Wiggly path that is not rejected by an f-test and we spent a great deal of time in ink on formalizing what wiggliness means in this setting and I will not uh unless folks ask about it in the Q and I will not belabor to that now um but uh uh I will say that that is a suggestion that we have made and to make that suggestion and other suggestions that I have made to you today easy for you to adopt right out of the box in your Empirical research we have made some software available a package called XD event and stata and another package called event study R in r that Implement all of these plotting suggestions automatically so the idea would be that to make your plots more informative you can let the software do a bunch of the work and get some of these additional pieces of information onto your plots so that your readers can make more informed evaluations of your estimates thank you very much great thank you so much for um staying for the second half and to all of you in person also on Zoom um so I will present this module titled confounds and pre-trend testing oh this works okay great thank you um so as we have discussed uh the difference in difference estimators and relative methods would rely on a no anticipation assumption and a parallel Trends assumption to identify causal effects but in practice we are often not sure if these assumptions hold so this module I'm going to discuss the common practice of using the pre-trend tests to test these assumptions these pre-trend tests is usually interpreted as a test for the powertrans Assumption so I will discuss the role of the no anticipation assumption in these tests and also cover several challenges in implementing these pre-trend tests that has been investigated in recent literature and I'll also discuss several solutions to the uh to these challenges but first before diving into the details let me first cover the basis of the pre-trend test so in the classical example where we only have two periods before and after and two group one treatment and control group the model is just identified and by that I mean under the no anticipation and power trans assumptions there's only one way to identify the average treatment effect on a treated based on the data and in fact this is the basis of the GID estimator after we do this there is no additional restrictions left from these assumptions that we can use data to test Things become different after we have multiple periods even with just two period uh two groups so in this case we can estimate this Dynamic specification with appropriate normalization and under the no anticipation and power Trends assumption we can interpret the regression coefficient associated with the lag indicators as the estimator for the cumulative average treatment effect on the treated after that there are still additional restrictions left from these assumptions in the sense that the regression coefficient associated with the lead indicators should all be zero so now we actually have additional restrictions that we can use to use data to test and this is the basis for what's usually known as the pre-trend test that tests whether we have a zero pre-trend that all the pre-treatment regression coefficients are jointly equal to zero so this test is usually interpreted as a test for the parallel Trends assumption and next I'm just going to quickly discuss what what is the role of the no anticipation assumption in these tests and to do that I'm going to go over a series of graphical illustration so on this graph we observe two groups a treatment and a control group and we are plotting their average outcomes over time where the treatment receive the treatment treatment group receive the treatment at Time Zero here we do see a diverging Trends between their outcomes leading up to the treatment between the two groups and if we conduct a pre-trend test very likely we're going to reject it and we might interpret this rejection as some failure of the Palo Trends assumption but when we make this conclusion what we're really thinking is that the observed pre-trend in the treatment group matches with what the counterfactual outcome would have looked like for this treatment group as shown here in the purple dashed line so if this pre-trend um this confounding Trend continues onward to the post-treatment period this would have implied a confounding effect that would bias our post treatment effect estimate however the same data is actually consistent with a totally different story and in this story powertrans assumption actually holds because it could well very well be that the counterfactual outcome for the treatment group is involving in parallel as shown here in the purple dashed line between the treatment and the control group the Divergence we are seeing leading up to the treatment between the treatment and control group is entirely due to the anticipatory effect and this is very likely to happen in several empirical settings depending on your application but conceptually I want to emphasize that violations of the no anticipation and the power of trans assumptions are distinct so anticipatory effect happens because the treatment has a causal effect prior to its its implementation and this anticipatory effect might be very interesting to learn on its own um on on the other hand the non-parel trends happen uh probably because there is a confounder around the time of the implementation of the treatment such that this confounder is causing a confounding Trend so that we do see a Divergent Trend between treatment and the control group leading up to the treatment and we might believe this confounding Trend would continue onward and bias our post-treatment effect estimate the challenge here is that observationally violations of these two assumptions are not distinct as I have shown you here using graphical illustration that both scenarios are possible interpretation for this rejection so this means whenever we have a right rejection of the pre-trend test we would want to have a careful interpretation for this rejection based on the setting you're working with whether this is because of anticipatory effect and parallel trans assumption code or it's because there is no anticipation and all we are observing is a violation to the Palo Trends assumption so now I'm going to delve into details of several challenges that recent literature have investigated in implementing the pre-trend test so as a reminder to implement the pre-trained tests we first estimated this Dynamic specification collect the pre-treatment coefficients into a long Vector Now call it Delta pre and test whether these pre-trend coefficients are jointly equal to zero so recent work has pointed out that when we interpret these pre-trained tests as a test for the power of trans assumption so assuming away anticipation for now these tests actually can fail to detect violations of parallel trans assumptions so again I'm going to use a series of graphical illustrations based on the 2022 paper by John Roth so here is a pop quiz to see if we follow Jesse's module closely I have a event study plot here with the regression coefficients associated with the relative time indicators attaching confidence interval or uniform capudance bands and the question is that uh can we reject the power Trends assumption assuming no anticipation for now based on the pre-trend test in this event study if we followed Jesse's module the answer is at least from the statistical sense the answer is not sure so it's true that we cannot reject the null hypothesis of zero pre-trend so that is to say if we test the pre-trend coefficients are jointed equal to zero as highlighted here on the green triangles we can now reject this now hypothesis however this these coefficient estimates carry very wide confidence interval and um it's a very noisy zero pre-trend so if we just look at a neighboring null hypothesis as highlighted here in the red squares we also cannot reject this null hypothesis so the reason I'm picking these particular set of hypothesis the value for the for the pre-trend here is that if we draw a line through these rare squares um they can be and assume a linear extrapolation this uh pre-trend in the pre-treatment period can extend beyond the post-treatment period and imply a large confining effect that would explain a large share of the estimated treatment effects and I would be evidence for bias in our treatment effect estimate so um in this case this is the issue of a low power test even though we cannot reject a zero pre-trend which is nice but we also cannot reject a pre-trend that under the linear extrapolation which is very reasonable would produce substantial bias in the post-treatment period so as always it's a dangerous to rely on a low powered test to draw conclusions unfortunately this issue of low power isn't pathological that's been cooked up by econometricians in the simulation calibrated to papers published in recent AAA journals John Ross in this paper found that many tests have limited power against reasonable alternatives for example the linear confounding Trends I described just now the good news is that we can evaluate the power for the pre-trend test in any given application using software packages currently provided by the author and if the power for reasonable Alternatives is too low then we might feel skeptical skeptical whether the power trans assumption actually holds even though we cannot reject zero pre-trend because this might just be a noisy zero pre-trend so the Second Challenge is related to this first challenge about the low power of these pre-trend tests and actually comes from a very reasonable question which is that if we report estimates only if we pass the pre-trend test can this improve the post-treatment effect estimator so at first we might think this is always a good idea to do because effectively we are using the pre-trend test as a screening device hoping to screen out the situations with a confounding trend however when the power of these pre-trend tests is low there can be unintended consequences because the estimates for the pre-trained coefficients are usually correlated with the estimates for the post-treatment effects so that means when there is indeed a confounding Trend so all the pre-trend coefficients are not zero if we condition on passing the pre-trend test which would happen quite often if we have a low power pre-trend um then effectively we are looking at the sub samples where the estimated free Trend are small enough and close to zero and um due to the correlation between the pre-trend estimates and the treatment effect estimates this would affect the original asymptotic normal approximation for the Post treatment effect estimates and distort the inference so to illustrate this I'm going to show you a very quick simulation study so on the left hand side this is a situation where we have an upward confounding Trend and zero treatment effect so the estimates for the pre-trend and the estimates for the treatment effect are going to be centered around non-zero values across some posts now suppose I'm going to screen based on whether I pass the pre-trend test this is a low power test so 50 of the time I'm going to get lucky and actually pass the pre-trend tests so if we only focus on the subsample where I pass the pre-trend test we're going to be in the situation on the right hand side where the positive correlation between the pre-trend estimates and the post treatment effect estimates would mean the distribution of the pulse treatment effect estimates also got shifted up so on average if we look at the post treatment effect estimates in the sub in the selected subsample we are going to on average have a larger bias than before without screening and this is a general phenomenon known in econometrics as a pre-test bias which says if we screen based on a preliminary test and then decide whether we're going to report the estimates for the treatment effects usually this can exacerbate the bias again this is not a pathological case the same simulations category simulation suggests that screening in many empirical relevant settings can induce large bias that can be similar in magnitude to the estimated effect so here the solution is actually relatively simple when the power of the pre-trend test is low it's not a good idea to draw conclusions based on the pre-trend tests we would only want to emphasize these pre-trend tests when they have a good power for reasonable alternatives uh the final challenge I'm going to talk about for these pre-trained tests is quite subtle and only happens when we only have ever treated groups so no one ever treated the groups and I'm going to go through a series of graphical illustration first and then give you a bit more theoretical details for why this happened so this is the issue that we cannot detect a linear violations to the identifying assumptions namely no anticipation and power Trends with only ever treated groups and on this figure I have a early adoption group that is treated at time 0 and in late adoption group that's treated at time one so this data as plotted is consistent with no violations to both assumptions because the counterfactual outcomes for both groups may have well-being evolved in parallel as shown in the purple dashed line and there is also no anticipation um however as pointed out in the paper by Kurio bruziak Xavier jehabel and John spies as well as in the earlier version of their paper the same data set could be consistent with the linear violations especially to the no anticipation assumption because it's also plausible that the counterfactual outcomes for both the early adoption group and the late adoption group would evolved as shown here in the purple dash line and I was seeing the upward trend for both groups is in fact due to anticipation effect how this issue is manifested in practice is that as pointed out by the authors if we estimate a dynamic specification and without a control group and include all the possible relative time indicators subject to the normalization to for example the minus one period there is still a multicolinarity between the relative time indicators and the calendar time indicators due to the linear relationship between the calendar time and the initial treatment timing so this multicolinarity would mean that it might well be the situation where uh in the speaker it's only positive calendar time fix effects and no treatment effect or no anticipation so um non-zero time effects but Delta K is equal to zero but it could also be the situation where there is no calendar time effect and only uh non-zero treatment effect and anticipation effect but the data alone cannot tell these two situations apart and to make progress to detect these violations we would have to introduce some restrictions about the data first and then test the remaining restrictions implied by the assumptions and the reason this point is worth emphasizing is that if we estimate the dynamic classifications in common software packages the collinear regressors tend to be directed directly omitted without being reported so it'll be good to check whether the ones omitted are compatible with restrictions we want to impose on the data because we don't want the software to implicitly determine what are the identifying assumptions should be for us the solution to this issue is actually to just make a conscious decision about the additional restriction on the data which would take in form of additional normalization in addition to the for example the minus one period and in terms of what are the reasonable normalizations this would of course depend on your applications but for example we can normalize at least another distant lead so this would mean we are assuming no anticipation and parallel Trends assumptions to hold at least between two periods for each group in the protein session that Jesse presented we also made the suggestion of imposing Dynamics are stable for more than b periods before the event and on a periods after so these are the challenges that have been investigated in recent literature and next I'm going to talk about some solutions under potential violations to parallel Trends which would be handy for example we have a low powered pre-trend test so um the first solution is based on sensitivity analysis and um the motivations that a non-zero pre-trend can be informative about the violations to the parallel trans assumption in the post-treatment period since we usually think the confounding Trend would evolve in a smooth manner so the estimates for the pre-trend would provide information on the amount of bias in the post-treatment effect estimate how much of it is actually due to confounding not due to the treatment effect this has already been done in practice sometimes not called sensitivity analysis explicitly so for example empirical papers often informally extrapolate the pre-trends by assuming the um the confounding trend has to evolve linearly so this would be controlling for a linear in relative time added to the dynamic specification but sometimes we might not want to impose the exact linear extrapolation from the pre-trans to the combining Trend in the post periods and to relax the exact extrapolation recent papers by Chuck mensky John pepper schlemershan and John Ross they have proposed sensitivity analysis and to illustrate this approach I'm going to borrow a figure from Ash and jump so here if we impose a exact linear extrapolation of what the confounding Trend would look like in the post period that would be based on this dashed Blue Line and if we subtract this confining trend of the estimated post treatment effect with debias the treatment effect estimate however this linear exact linear extrapolation might be two uh two stringent so ashes and Zhang consider bounding on how far the confounding Trend in the post period can deviate from a exact linear extrapolation so the confounding Trend in the post period can fall into this blue region then if we subtract off all the possible values of the confounding effect in the post periods then we get a set of values that are the biased values of the treatment effects and the paper has more details and construct the confidence sets for the treatment effect with the correct coverage under the assumed Bound for example as shown here on the figure and also takes into account of the estimation error and they're all implemented in statical software packages provided by the authors the second solution that I'm going to talk about is able to produce a point estimate and it's based on the proxy instrumental variable approach and um this approach would need additional data but I'm going to argue that the additional data is often available in practice so the premise for this approach is that sometimes we actually know the cost of the confounding Trend in the example of minimum wage study the local labor market is the confounder and that could if it's improving it could imply that the confounding trend is upward sloping when we are trying to estimate the impact of minimum wage increase on youth employment the challenge of course is that we don't observe the confounder otherwise we can directly control for it in our Dynamic classification but oftentimes in in applications we do observe a noisy measure for the confounder so in the example of estimating the minimum wage increase on youth employment a candidate for this noisy measure is the employment in the prime age group which is also affected by the local labor markets but not so much affected by the increase in minimum wage recognizing that we often have access to these noisy measures for the confounders in this 2019 paper by Simon freiadohoven Christian Hansen and Jesse Shapiro they argue that under some conditions the leads of the instruments can be the least of the treatment sorry can be used as instruments for the noisy proxy to uh when when the noisy proxy are included as control variables in the dynamic classifications so using the treatment as instruments can correctly remove the bias due to the confounding effect in the post-treatment periods for the main outcome of Interest and this is also implemented in software packages so I'm going to go through a series of figures to illustrate how this method would work but before that I'm just going to make a note and this is also to a question from the first half which is that if we only include these noisy proxies as a control variable in the dynamic specification but don't do the instrumental variable correction this is not enough to remove the bias due to confounding because these are a noisy proxy for the confounder notch the confounder itself so how this proxy instrumental variable method works is that let's start with the events that you plot for the main outcome of Interest say youth employment in response to minimum wage increase here we do see a significant post-treatment effect estimates but the bad news is that we also see a significant upward pre-trend so we might worry that the post treatment of our estimates might be due to some confounding effect not due to the treatment effect of minimum wage increase if we apply to the event study for the noisy proxy as well so for example the employment in prime age group we also see a similar pre-trend Dynamics and that's because this noisy proxy is affected by the same co-founder as the main outcome however this is a noisy measure for the confounder so we don't want to only control for the covariates in the in the dynamic specifications what we should do is to use the instrumental variable which are the leads of of the treatment to rescale the Dynamics of the noisy proxy here to match with the Dynamics in the out in the main outcome of Interest as shown here in the orange triangles once we do this rescaling and since the proxy noisy proxy is not affected by the treatment the post-treatment periods would carry the Dynamics that are reflective of what the confounding effect would have looked like for the main outcome of Interest which are shown here in the orange triangles so now subtracting of these rescaled noisy proxy of the original treatment effect estimates for the main outcome of Interest as shown here in the bottom right figure we would get estimates for the treatment effect that are adjusted for the confounding effect so with that let me conclude and again this is the list of other papers that are the basis of our module in case you are interested in more details and I'll pause here for questions before Jesse takes over so now get out of my own way and continue the uh the with the next module um I'm gonna be talking about situations where there are heterogeneous effects of the policy on the outcome so for the purpose of this module I'm going to consider a situation where we're now I'm going to set aside the things that Sophie was just talking about that is we we may be worried that there's a confounder so the parallel Trends does not hold or that there's anticipatory effects I'm going to set those aside and suppose that we really believe we're in a situation with no anticipation and parallel Trends but I'm gonna allow that the policy we're interested in might affect different units differently so for example it might be that you know if you're a less productive firm then the minimum wage is a bigger deal then if you're more productive firm you have made less room to you know for employment because maybe you have less room to to increase wages so uh lots of economic situations are going to exhibit that type of heterogeneity and of course that's been a huge theme of econometric and met over going back many many decades I'm going to talk today about what some of the implications that that type of heterogeneity has in the sorts of settings that we are interested in and in particular I'll talk about what it means for identification what can we learn hope to learn from the data and then also what does it mean for estimation what happens if use an estimator that is not correctly specified or is based on an incorrectly specified model but in fact um that that assumes there's no heterogeneity but in fact there is and then I will talk about some very practical easy to adopt solutions that apply in certain situations and say a little bit about situations where there are fewer off the shelf Solutions so let's go back to the beginning and imagine again that we have one event okay and we have a treatment group and a control group so we have an affected group and an effective group just like the very beginning where Sophie started what problems does it cause if under the hood of this average some of the treatment units are more effective than others so in our minimum wage example what kinds of issues does does arise if some restaurants have a larger employment effect of the minimum wage than other restaurants and the answer is it doesn't cause any problem at all because nothing that Sophie assumed ruled out the possibility that the difference between the factual and counterfactual employment for restaurants in New Jersey was the same across all restaurants that was that was not an assumption that Sophie made and it she didn't make that assumption because we don't need that assumption so if we have and I'm representing this just very Loosely by these dashed lines here we envision that there's some underlying heterogeneity around this average so this is not a confidence interval now this is just a way to visualize where there's heterogeneity we're not going to run into any problems everything that Sophie said would be true and we know that because Sophie did not articulate any assumptions that preclude this possibility so that's great so if we have no anticipation and we have parallel Trends we can have all kinds of heterogeneity and how the treatment affects the treated units and we don't have to worry about that at all if we're interested in averaging facts great okay so with one event and a control group heterogeneous effects no problem nothing that we've said so far nothing that Sophie said rules that out or is complicated in any way by that now I'm going to start thinking about richer settings and in particular let's go to the static adoption setting and I'll go to the simplest possible static adoption setting or sorry staggered adoption setting where we have an early adopter and a late adapter okay so we have no pure control no unaffected group just an early adopting group and the late adopting group okay what happens if we try to learn the effect of the treatment on the early adopter in the first period of adoption can we do that yes that is exactly the same as the situation before because under no anticipation the fact that there's going to be an adoption later for the late adopting group is irrelevant under parallel Trends we can use the late adopting group as a counter factual for the early adopting group so everything that we said before that Sophie said in the first part of the series talking about the classic difference in differences setting applies immediately without alteration to learning the effect of adoption on the early adopting cohort rarely adopting group in the first period of adoption okay so far so good what if we are further prepared to maintain so here we haven't said anything about Dynamics because there's only one period what if we are prepared to maintain that the effects are static in the sense that whatever is going to happen as a result of treatment to the early adopting group is done after one period so if the minimum wage affects employment that effect kicks in after one year and if we look at subsequent years we're not going to see any further Dynamic effects of the policy it just it's one and done right the policy has an immediate effect and that is the full uh Dynamic effect of the policy well in that case we can just repeat that same logic to learn the effect of the policy on The Late adopting group why because the early adopting group goes back to a trend that reflects only the underlying effects of calendar time there's no longer any Dynamic effect of the policy on that group and so this group allows us to form a counterfactual for what would have happened in this group had there not been an adoption and so now we've learned the effect of adoption on the early adopters and we've learned the effective adoption on The Late adopters each in the first period following adoption but that's all there is to learn because there is no Dynamic effect so there's no effect of the on the early adopters in the second period And if we want to average them we can average them or if we want to wait at average them we can wait at average them and we can decide how we want to aggregate those into a single number okay and the reason is that the early adopters here are providing are counterfactual for the later doctors so to summarize in this situation for the early adopters the lead adopters under our maintained assumptions are a valid control for the effect in the first period after adoption if the trends between the early adopters and the late adopters diverge it has to be because of the effect of adoption on the early adopters that's what our assumptions tell us for the late adopters the early adopters are also a valid control for the effect in the first period after adoption because if the trends diverge since the policy is no longer having any effect on the trend and the early adopters it has to be because of the effect of the policy on The Late adopters under parallel trends and so everything is still very simple we can estimate anything we want we can take averages if we like to take averages and uh there is no uh super big problem okay now I'm just going to change I think just one word in the title of the slide I'm going to go from heterogeneous static effects to heterogeneous Dynamic effects so in this situation we're still fine in the first period because we can use the late adopters as a control for what would have happened to the early adopters had they not adopted but unfortunately in the second period we are not in such a simple situation because we can't use the early adopters as a control anymore for the late adopters because the early adopters are on a path that may be affected by treatment and we can't use the late adopters as a control for the early adopters because the late adopters may be on a path that can be affected by treatment and so in this case there is no way to learn without further restrictions anything about the dynamic effect of this policy okay so to review for the early adopters the late adopters are a valid control for the effect in the first period after adoption just like before I haven't changed the text at all I think if the trends diverge it's because of the effect of adoption on the early adopters but for the late adopters the early adopters are not a valid control for the effect in the first period after adoption because the early adopters are now being dynamically affected by the policy in a way that we do not know and ideally would like to learn from the data but can't and so if the trends diverge here it could be because of the static effects of the policy on the late adopters or could be because of the dynamic effects of the policy on the early adopters or some combination of the two and we do not have a way to know from the data alone notice that um if we knew that the effects were homogeneous but Dynamic so sort of semi-homogeneous in the sense they can vary with time from treatment but not across cohort then we would be in a much better position why because in that situation we would know that the effect of treatment on the of the policy on The Late adopters has to be on average the same as the effect it had on the early adopters and the effect on the early adopters can be learned from this this period here so we could take the period the effect for the early adopters which seems to be positive and we could impute it for the late adopters and say well the late adopters their outcome declines here but it would have declined even more if it hadn't been for the positive effect of this policy on the outcome and therefore we know that the path that the early adopters would have taken would have looked like this okay as I've drawn it here okay so if we knew that the effect was the same in both of these cohorts both of these groups then we could still find a valid counterfactual for the early adopters and learn the dynamic effects of the policy in the second period following adoption okay so the common it's the the uh issue that we were having before is not just about Dynamics and it's not just about heterogeneity it's about the combination of the two because it's a kind of rich heterogeneity in the effects that means that we may not be able to learn everything that we want without further restrictions Okay so what does this tell us about the use of chiral Trends and no anticipation assumptions for identification in these settings if we're not prepared to make any restriction on the Dynamics of the effects so we don't for example we aren't prepared to say that they're static and we're not prepared to make any restriction on the heterogeneity of the effects for example we're not prepared to assume that the effects will be the same or very similar across adoption groups then for every average effect we want to learn just following the original difference in differences logic from card and Kruger that Sophie summarized earlier we're going to need a group that is either unaffected by treatment or not yet affected by treatment and that is measured simultaneously with the treated group that we are like looking to learn about and that's it we're going to need those things in order to apply the identifying assumptions that uh Sophie described so I'm now going to talk about some approaches we can take if we want to allow for this type of heterogeneity but importantly no approach to estimation can solve problems with identification solving problems with identification requires either assumptions or the appropriate data or both and so if we are not in these situations there's not an estimator you can use that fixes that problem and no software you can download that will get you something that has the interpretation you're looking for so just want to make that clear as we transition from talking about identification to talking about estimation that using the right estimator only solves the problem if you are in a situation where the effects you're interested in are identified so I'm going to talk now about how you can address these issues in a situation of staggered adoption and that is a situation that has been most heavily studied and it's the one on which the recent methodological literature has made the most progress and then I'll talk a little bit about what can be said in other situations so I'm going to go back to the regression representation to give you some intuition for ways to solve these problems and I'm going to remind you that we have an outcome y a unit fix effect Alpha time fixed effect gamma and an indicator for whether this unit I is post-treatment that's given by Z and then we take first differences of Z we're calculating indicators that tell us whether treatment occurred treatment began for this unit I at a given time T minus K so relative time compared to time T just like before and we're interested in learning these cumulative Dynamic treatment effects these Deltas I'm going to complicate this by imagining that the correct model has an i subscript on all the Deltas so every unit is allowed to have its own unit-specific treatment path so every restaurant is allowed to have its own Dynamic effect of the minimum wage on employment at that restaurant if we really want to make statements about individual uh uh effects that is going to be really hard um but there are case special cases in which we can say very useful things about average effects and one of the cases where that is possible in a fair amount of generality if we're in this situation is the case of staggered adoption in the case of staggered adoption the uh uh full path of the treatment this is very important and this is in some sense the essence of the underlying issues the full path of Z can be described by one number which is in what period does this unit adopt the policy right because under staggered adoption policy starts off off and then goes to on and stays on forever so if I tell you when it turned on you can draw the entire path of Z right so under staggered adoption the entire Dynamic history of Z can be summarized by this single thing G so we can group units according to their group G and every unit that shares the same G as another unit must have that other units treatment path policy path that is very important because it means and I won't go through the details of why that um uh it is sufficient in this situation to consider this restrictive model sort of if we're interested in averages it is without loss to consider this restrictive model because and intuitively and again I won't go through the details the reason is that all of the relationship between which unit this is and what treatment path it has comes through this object G so in this restriction on the preceding model I've replaced the unit specific if Dynamic treatment effects with group specific Dynamic treatment effects and the reason that this representation is very helpful and the intuition for why once we can get to this representation we know that we're going to be able to apply some very practical approaches to estimation is that this is an interactive regression model so how would I estimate this model I would just interact these Delta Z's with indicators for whether this is group one group two group three group four and so on so to estimate this model I only need to take the model I was estimating before when I was not worrying about heterogeneity or where I had only a single group Because treatment turned on at the same time for every treated unit and just take that model and interact the these uh Delta Z's these uh treatment variables with indicators for is this a unit that adopts in Period one is this a unit that adopts in Period two is this a unit that adopts in Period three and so on and then just like I said in those earlier plots once I've estimated those interacted that interactive model I have a group specific Dynamic treatment path maybe I'd like to summarize that in some way to so that I can make a picture of it or draw a numerical conclusion well I can average those so I can take an average of the treatment paths across the different cohorts and make my picture that way and that is uh totally fine it will get you a weighted average treatment effect under the no anticipation and parallel Trends assumptions conveniently that approach is implemented in a readily available software that I've listed here and other related approaches that have been developed in other papers that have appeared in this literature in recent years are also implemented in very convenient software that in my experience Works quite well um and this is a subset of all the software that's available there's a GitHub page that Sophie found at some point that lists all of the software options in this space and it goes to multiple Pages uh most multiple screen pages so we decided not to list all of them these are the ones that are listed in this recent uh set of review articles but uh these packages implement the interaction regression approach that I just discussed these packages what they do is use only the pre-treatment periods to estimate the time effects so basically use only untreated units to estimate the gamma T's and then do and then subtract off the time effects to get the effects on the other units these packages are basically taking individual difference in difference estimators and averaging them in a particular way all of these packages are designed to estimate weighted average effects on Instagram in cases of staggered adoption under situations like the one we've been describing where there's unrestricted heterogeneity in the dynamic effect of the policy on the outcome so all these are pretty uh straightforward to use and this is a great type of sensitivity analysis that you can do right off the shelf if you're in a staggered adoption situation and you're interested in allowing for heterogeneity of the kind that I have mentioned and these packages are supported by a number of scholarly articles which I am uh listing here people can these slides are clickable so folks can click on these and read about the details and underlying econometric guarantees that come with these different estimators um the so a a takeaway lesson that I would have is either make a case for restricting the heterogeneity or the Dynamics in your setting based on economics or some other kind of a priority logic which is always great you can use economic theory or knowledge of the context to impose some structure that you think is defensible I think that's a great thing to do or adopt some of these other approaches that I've mentioned or both you know argue for your assumption and then as sensitivity analysis show what happens to your estimates if you adopt some of these other um off-the-shelf estimators I want to spend just a little bit of time on what happens if you don't do that and so you don't have a situation where it makes sense to restrict heterogeneity and you don't investigate what happens when you use estimators that are designed to allow for such heterogeneity I'm not going to spend too long on this because I don't think there's really a good reason to be in this situation given that there are off-the-shelf tools that will work while in situations with heterogeneity but if you were to find yourself in this situation you might like to know where you may have gone wrong and so I'll briefly discuss so suppose that you were to estimate this restrictive model that imposes that the effective treatment is identical for all units I when in fact the correct model is one that say says that the dynamic effect of treatment the dynamic effect of the policy May differ by group what will happen well one way to see where you're likely to start going wrong is to go back to the picture that we saw before in this model that we specified here it is totally fine and correct to use the behavior of the early adopt the behavior of the lead adopters the outcome the evolution of the outcome for the late adopters in this period here to construct a counterfactual for what would have happened to the early adopters had it not been for the dynamic effect of treatment and if that is a correct assumption then the estimator will give you a correct answer but if that is not a correct assumption then the estimator may not give you a correct answer you might ask how different might the answer be and you can see immediately from this plot that the answer is arbitrarily different because there is no information in the data under this model there is no information in the data about the parameter we want to know here and so your estimator can be arbitrarily far away from the truth it is very easy to show that because you're trying to estimate something that is not identified so uh if you're estimating something that's totally unidentified I can find a very bad data generating process under which your estimator can be very very far from the thing you are trying to estimate so all of this is to say that estimating this more restrictive model is fine if the restrictive model is a good approximation to the economic situation and not fine if it's not and in fact similar to and you can see immediately why this is true similar to the situation that Sophie mentioned earlier with the static model the path that you estimate may not even be a weighted average of the underlying effects and you can see why you're trying to estimate something that you cannot so uh uh you may be very far from the truth you might estimate something that is larger or smaller than all of the true effects that seems undesirable so just to say again if you're in a situation with staggered adoption I recommend to make a case that you can restrict the Dynamics or the heterogeneity of the effect of the policy based on economics or knowledge of the context or use an estimator that leverages an untreated control of the kind that we have discussed or both make the case based on economics and then maybe show some sensitivity analysis to relaxing that assumption I think that would be a very reasonable approach I want to spend a few minutes on what happens when we're not in a situation of staggered adoption I'm not going to go into this into as much detail about this because there are fewer off-the-shelf Solutions in this case and what I'll try to do is give you a little bit of a flavor of why that ends um so uh this is an extra challenging setting um because uh once we go outside staggered adoption to the wide world of possible kinds of treatment well policy Dynamics we enter situations like continuous treatment and multiple treatment where it can be very difficult to define a pure control group so for example if the the treatment that we're interested in is the level of the minimum wage and we think that the level of the minimum wage can have arbitrary Dynamic effects well the minimum wage was adopted in the United States decades ago every place is treated by it to some extent and uh so if we have a continuous treatment and a continuously evolving Dynamic effect of that treatment it's going to be very difficult to find totally untreated units from which we can make inferences we're going to need to adopt restrictions and the more complex the behavior of the policy is the more restrictive we're likely to need to be to be able to make useful statements based on the data let me give you one example that um that Sophie and I have looked at in a paper that we wrote that I think is helpful for understanding why things can become complicated where we when we have for example continuous treatment and no untreated units which is often the case when we have continuous treatment so this is an example that's like a stylized example based on a paper by Amy Finkelstein about the effect of Medicare I'm going to strip away a lot of the the real economic detail of that paper and take you down to a cartoon version of the paper so the cartoon version of the paper is Medicare is adopted in the United States it's a national policy so for the elderly for old people Medicare immediately introduces Universal Insurance and so it increases the share of older Americans who are insured everywhere in the United States because wherever the state a given uh whatever wherever you are if the share of uh older people that was insured in your state was 60 before Medicare it goes to a hundred percent immediately after Medicare so every place in the country is affected by Medicare but some places are more effective than others because if you're in a state that has a high insurance rate for older people prior to adoption of Medicare then maybe you had 80 coverage before Medicare and Medicare is taking you from 80 to 100 whereas if you're in a state with low insurance penetration among older people before the adoption of Medicare then maybe you had a insurance penetration of 40 and Medicare is taking you from 40 all the way to 100 and that seems like a much bigger change the question is what can we say about the effect of Medicare in a situation like this if we're not prepared to restrict the way and to the restrict the extent to which the effect of Medicare May differ say across U.S states because it might be that the effect of Medicare on something like Health Care utilization or health is different in different places okay so just to give you a stylized version of this situation we are in a situation where there's a group there's no control group no untreated group there's a group that's subjected to a small treatment and a group that's subjected to a large treatment so this might be states where Insurance penetration was low and states where Insurance penetration was high and we're interested in the effect on some outcome like healthcare utilization or the level of health or well-being of older people and the our we have a pre-period and then we observe some Divergence between these two groups During the period in which the policy is adopted so say as we adopt Medicare in 1965 if we're not prepared to restrict the effect of Medicare on the outcome variable in uh these different uh groups what can we say about the effect of Medicare on the outcome I'm going to run a little poll in the room I won't run this on Zoom so I'm just going to I'll report the results through the microphone so how many people think that the treatment had a positive effect on the outcome how many people think that the treatment had a negative effect on the outcome okay so there were a few votes for positive no votes for negative how many people think we can't say okay similar number of votes for we can't say to for positive and indeed we can't say um why because it could be and this is I think that maybe a natural inference and might be very reasonable under further assumptions that what would have happened absent the adoption of this policy would look like this so maybe the the counterfactual for the uh small treatment group would look like this maybe there would have been a small Decline and so what we're seeing is that treatment increases the outcome variable and it increases in more for more traded units right so this is what you'd get for example if you thought that the effective treatment was fairly proportional to the size of the treatment so that the effective Medicare on health is larger in places where Medicare has a larger effect on insurance penetration and smaller in places where Medicare has a smaller effect on insurance penetration and that may by the way be a very reasonable economic assumption but if you're not prepared to make that assumption and you don't know and you're totally agnostic about the way that Medicare may affect different states and so it might be that Medicare has you know much different effects on some groups of States than others for all kinds of reasons having not to do with their insurance penetration then you can't refute this possibility which is that absent Medicare health would have improved like this and so what we're seeing is a small effect on the largely treated States and a large effect on the slightly treated States because maybe these states are just different and they're less affected by Insurance because of the way the health of the underlying population or some other difference between these states and these states and so in fact in a situation like this unless if you if you even if you have parallel Trends and no anticipation or the analogues of those for this type of setting unless you're prepared to make some restriction on the way the treatment effect differs across units you cannot estimate any average effect in fact Sophie and I in our paper in an appendix we showed that there for for there is no estimator that is guaranteed to be contained between the largest and smallest of the effects for the different units so any estimator you pick I can find a possible distribution of underlying effects that lies completely above or completely below your estimate and the proof in the appendix of the paper goes through exactly how one does that construction so we're going to need some additional information either additional data or additional assumptions or both so in a situation if you find yourself in this situation and here I have fewer software packages to recommend because this situation this type of situation is more delicate and more accustomed to your economic setting I would again recommend using economics to restrict the Dynamics of treatment effects restrict their heterogeneity or maybe restrict their functional form so for example proportionality here would buy you a lot or to use an estimator that leverages an untreated control and in this situation if we had you know another a state that was exempt for Medicare or maybe we have Canada or you know some other control unit that is not affected by the treatment then we would be able to disambiguate the two situations assuming we believe parallel Trends and no anticipation holds because we would know that both types of both of these types of units had benefited or have seen their outcome improve as their increase as a result of Medicare with a larger increase for the more treated unit than for the less treated unit so having an untreated control would be very useful in situations like this and the literature has dealt with that type of situation where you are in possession of an untreated control and are therefore able to construct uh valid estimates of weighted average uh treatment effects and with that I will I guess pause for questions before turning it back over to Sophie One Last Time afternoon um this is the last module and I'm just going to provide a quick summary for take a takeaways based on what we have discussed so far so in the identification module we started with the basic uh difference in differences estimator for two periods and two groups and reviewed the usual assumptions that are used to justify the did estimator it estimates the causal effect and to extend to settings Beyond um two periods to groups to multiple treatment multiple periods and multiple groups we also need to generalize these assumptions appropriately we emphasize that the power of trans assumption is not invariant to scale and in particular in the generalization to multiple periods and multiple groups the commonly used two-way fix effect specifications sometimes are not flexible enough so would need additional restrictions in addition to these assumptions for us to estimate the treatment effect for example they may impose restrictions such as static or homogeneous treatment effect and in practice we don't know if these restrictions are correct so we might not want to rely only on the restricted specifications to draw conclusions so for example only estimate the aesthetic specification to estimate the overall effect when there could be Dynamics in a plotting module we discussed how we can easily make event study plots after we estimate the dynamic classification and we argue that there should be an essential part of our analysis because protein makes it very explicit to what normalization we are imposing there are other things we can add to the plot to make them more informative so for example since we're depicting the whole path of the treatment effect we might want to also see what other paths are consistent with the data so therefore we can add the uniform confidence plans to do that we can also add p-values for key tests in event studies for example the zero Trend tests and the tests for whether the dynamic effects have lever have leveled off um in the module of confounding and pre-trained testing we cover the basis of pre-trend tests and discussed when we have rejection of the zero pre-trained hypothesis this is a situation that usually we need additional interpretation because we cannot separate violations of no anticipation from violations of powertrans assumption only based on the data now suppose we're going to interpret the pre-trained test at the test for a power Trend assumption there are recent literature that have discovered some issues with this practice for example these pre-trained tests can have low power against Alternatives under reasonable extrapolation would imply confirming bias for our post-treatment effect estimates and if we further screen based on these low power tests there is a high chance we can pass them even when there is indeed confounding and this could exacerbate the bias which is referred to as pre-test bias in econometrics so the bottom line here is that we might not want to emphasize pre-trend tests when the power is low instead we can consider some alternative solutions that would allow for violations of parallel trends in the last module we discussed the impact of heterogeneous effects across multiple treatment groups and how we might do estimation so um this is in sometimes implicitly as similar Way by two effects specifications that are not flexible enough and we discussed in order to even identify the average effects uh usually we wouldn't need one or both of restrictions on heterogeneity or dynamics of effects or use a new and flexible methods that leverage that leverages untreated or not yet treated groups and there are many good solutions for stagger adoption case but fewer generic Solutions outside of stagger adoption so thank you so much for participating for those both on zoom and um in person thank you for the organizers for proposing the theme and to other researchers and practitioners that have worked on and use these methods that give us the opportunity to put together the material thanks 