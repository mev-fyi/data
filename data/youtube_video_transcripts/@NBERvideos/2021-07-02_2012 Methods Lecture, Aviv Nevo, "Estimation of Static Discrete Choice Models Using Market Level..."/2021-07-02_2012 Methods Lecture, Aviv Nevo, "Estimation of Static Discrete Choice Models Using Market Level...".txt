Aviv Nevo: We're going to
talk about estimation. Let me first talk about
the data structures that we typically use. They really come in one of two main sources in a
combination of them. The first is market level data. What we mean by that is
either cross section or a time series or panel
of different markets. For now, think of a different
geographical market or a national market over time, and we'll talk a little bit more about what I mean by
that in the next slide. The other type of data
is consumer level data. We're typically
what we'll have is a cross-section or a
sample of consumers. Ideally, if we can, having a panel is going
to be extremely useful. A panel with the idea
that we actually get to see people making
repeated choices in different economic
environment that's really going to give us a lot of
identifying power. Sometimes, well, this is rare, although I think I was going
to talk about an example of this tomorrow, we will have what's called
second choice data. We'll see what product
people actually chose and then we might have a
survey of them saying, had I not chosen this car, what other car would
I have chosen? In some sense we get to see
not just their first choice, but also their second choice. That's again, you can see we'll have a lot of identifying power. Sometimes we will
have combinations, and again, that's going
to be very useful. In the combinations could be, you could either have
a sample of consumers, know exactly what
those consumers chose, but then have some
aggregate market shares. Or we could have things like we could know the quantity
share by demographic groups. We could know the, let's say the share
of people that buy a Mercedes by their
income group. That's going to be very useful. Or we can have the opposite of having the average demographics of people that let's
say buy a Ford. All that information
is going to be some combination that's
going to be very useful. For today's lecture, I'm just going to focus on the first, we're assuming we only
have market level data. In the following lectures, either later this
afternoon or tomorrow, we'll talk about how to bring in this additional information that we have here in these
other datasets. Let me just go a little
bit in more detail what I mean by the
market level data. What we get to see is
that the product level, we get to see either the
quantity or we're going to convert that to market
share by market. I put market here in quotations because sometimes
not quite obvious, what do we mean by market? We'll see what the model means. It's going to be well-defined
within the model. Sometimes you have
to figure out how to translate that into the data. The data is going to include some aggregate quantity
at this market level, it's going to have price
and characteristics, advertising or other things
that we think are important. Again for each of the product
at this market level. What we're going to need
is we're going to need a definition of the
market size and this goes actually to a
question that Dennis asked at the end
of last lecture. We're going to need
the definition of the market size
so we can actually go from quantity
to market shares. Now, what's tough here is that, this is going to include
purchases of the outside good. It's not unlike the inside
good, you could say, well, we just sum over the
quantity and divide by that. What we need here
is we need to know how many people did not decide not to buy cars.
How do we know that? If we have a sample,
we could see, let's take a 100 households and see how many of
them bought a new car this year and those that
didn't chose the outside good. But if we look at the aggregate, we need to somehow define
what the market share was and what the market size was and we'll see
ways of doing that. The other thing that's
going to be very useful though it's optional, is to have some idea of the distribution of the
consumer demographics. It's going to be
aggregated data, I don't have individuals
specific demographics. All I need to know
is the distribution. If I think of this room is
a market and the next room, the productivity guys, that's
a separate market and then the IT guys is another market. I'd like to know what
the distribution of the demographics within
each of these markets, even if I don't
know individuals. We could get that
either by having a sample of actual consumers, things like the consumer the CPS or the consumer
expenditure survey. We can literally have
consumers then do some non-parametric distribution
for this demographics. Or we could actually use some data to try to estimate
a parametric distribution. For example, if it's income, we might say, we have
some general income, we're going to income fit some log normal curve to it and just use that data to fit that and again,
we'll see how to do that. The advantages of
market level data, this is compared to
consumer level data, I'll talk about
in the next slide is that it's much easier to get. Because you might ask, well, why do we even bother with
this market level data? Why don't we just go and do everything was
consumer level data? Well, in many cases
you don't have it. It's sometimes getting
the market level data, especially quantities
can be difficult. Prices, you could just
sometimes you want to know what are the prices
of our product right now, just go look on the shelves. Quantities is a
little bit harder, but still much easier to get an actual sample
of consumers. This is true not just
for us as academics, but also a lot of
times for companies. For example, Nielsen, that
collects a lot of data, they have a consumer
level panel, and over the years they keep
thinking of shutting it down just because it's too expensive to
actually collect it. They still have it in place, but they keep thinking about. The other advantage that
this has is it does really give you in some sense the whole population in
the market you want. When we talk about
consumer level data, I'll talk about sample
selection issues and it's going to be
less of an issue here. The disadvantage, of course, is that the estimation,
as we'll see, it's going to be harder and identification is going to
be a lot more work here. That's obviously a disadvantage. Consumer level data, the main thing that
gives us is we actually have a match between consumers
and what they purchase. If I have aggregate data, this would be a market
and I know what share, how many of you drink water and how many of you iced tea? That would be the market
shares of my two products. If I had consumer level data I would know each one of you, specifically what you chose. Obviously I have the
consumer choices, including here I'd have
the outside goods. I see how many of you decided
not to drink anything. That would be the outside good. A key challenge, by the way, even if you have it, is
getting the price and characteristics of
all the options. This is especially problematic when different consumers
pay different prices. Think for example,
I have a sample of consumers and know what
autos they bought. Now, I know what price they paid for
the option they chose, I might know the MSRP for the options they didn't choose. But we know that most
people do not pay the MSRP. Now I have to figure out what would they
have paid if they'd gone to another option. Now you could say
the same problem also exists in the
aggregate data, but here's sometimes actually collecting both the prices and the characteristics of
the available options actually quite problematic
and quite difficult to do. The other thing that
I'd like to have here is actual consumer demographics. If I don't have
actual demographics, there is a sense in which the
individual data is really just the same as
market level data, that there isn't really
any much more information or useful information there. What are the advantages of this? Obviously, if I want to see
the impact of demographics, this is a much more
straightforward way of doing this, identification
estimation is easier. When we start talking
about dynamic, which we'll do at the
end of the day tomorrow, consumer level data is going
to be much more useful. Especially if I get to see consumers making
repeated choices, there is a sense in
which at that point your identification
of aggregate data, although we'll actually
see a paper that does that is very questionable. The disadvantages,
of course, harder, more costly to get, but there's also issues
of both sample selection. If it involves who's willing
to actually participate or as opposed to having aggregate data that is
more administrative, could actually come from
stores or from firms here, if we actually have
to follow consumers who is willing to actually
participate in the sample, and to the extent
that we need to have people record what they do, there might be some
reporting errors and things that we
might worry about. With that in mind, let me just quickly review the
model, the notation, because there were, most
of this was NREL slides, but maybe slightly smaller font. Let me just go through
it quickly and make sure we're on the same
page in terms of notation. We're going to work in
characteristics space, and we're going to as the
starting point is going to be this indirect utility
from the J inside goods. The utility is going to
be a function of xjt. These are the observed
characteristics. This side jt, which we'll get back to this plays an
important role here, that the unobserved to us
product characteristics, everyone in the market is going to assume to observe that. There's going to be two types
of demographics and here my notation is slightly
different than ours. There's going to be observed
consumer demographic. Now I put observed
in quotation marks because we're working
with aggregated data. We never actually observe, but these are the type of things that you could get a sample for. Think of this like income. I might be able to estimate the distribution of
income in the market, but it's not that I observe it for any particular
individual. That's what I mean
by these DITs, the VITs or the news, what I'll call them. These are the unobserved
consumer demographics. These are things that
you think, even if I had individual level data, it's unlikely they
would actually observe. Example I like to
give is if you think of a purchase of cars. Let's say the number
of dogs you have might matter for the size
of the car you'd like. In most consumer panels, we
don't actually see them. Although now more and
more, you can see it. But that's going to
be the notation. Now, side jt, this
unobserved characteristic is going to pay an
important role. Firstly, you could think it's realistic because it's
going to capture the idea that there's some
characteristics of the product that
we do not observe. It doesn't matter how
many x's we put in there, there's still something unique about the product that
we really can capture. That's going to be
one, it will act as a residual or
as an econometric. It's an econometric residual or econometric error term when you go to the estimation and it's also important
because without it, especially when we're
working with aggregate data, we're faced with this problem, often called the overfitting problem and that's
the problem that we don't know why we're not perfectly fitting
the market shares. We need an error
term to tell us, this is why our
models perfectly fit the data and that's what
this side is going to be. It's going to act as a buffer. However, it will
potentially imply endogeneity of some of
the characteristics. The one we usually
care about is price. Now a model that's used a lot and the one
that I'm going to use for this lecture and
actually for most of our lectures is
something that's often called the random
coefficients logit model or mixed logit model. That's a particular
specification. What we have is the utility for individualized from brand j at market t is xjt times Beta_i. Different individuals value
these characteristics differently plus
Alpha_i times pjt. Again, Alpha_i is the margin
utility of income times the price plus this
Psi_jt plus Epsilon ijt. That's going to be
the logit error term. Now we're going to assume that these tests parameters Alpha i and Beta i are distributed, they have mean Alpha and Beta
that's common to everyone and heterogeneity
around that mean. The random coefficients or the mixed logic
part basically says if we don't have
these two components, if Pi and Sigma are zero, then we basically collapsed to a logit model if we make the appropriate assumptions
on the Epsilon. That's the way in
which we're going to generalize the logit model. Going back to the issue of
the definition of the market, you can see here in the model, the market is defined
in something clearly. It basically says the
market is the level at which these things don't vary. If you look at the x, the Ps, and the Psi they're all indexed by t. They
vary across markets, but not within a market. Within the context of the model, that's how the market
is well-defined. We can basically say, alright, let's take an area in a time period in which
these things are constant. Now of course, when
you go to the data, we're going to have to make
some assumptions about this. Because when we go, for example, if we're going to define
the market as let say nationally annual market in the case when we're
looking at automobiles, we know that prices are
going to vary both across geographical regions and
across individuals but here, they are assumed to
actually be constant. There are extensions.
You could ask, well, what happens if
these prices vary? There are extensions to
that but for the purpose of what I'm talking about now, we're going to ignore that. Now it's going to be very
convenient for what follows to separate this utility into
three different parts. The first part is the mean
utility. It's defined here. It's basically saying let's just take everything that
doesn't vary with i. It's xjt times Beta plus
Alpha times pjt plus Psi_jt. That's the mean utility. It's the mean
across individuals. The second component
is this Mu ijt. It's basically the interaction
of this heterogeneity and the parameters with
the price and xs. Finally is the Epsilon term. Now going back, Dennis,
to the question that you asked earlier about, are there other ways of
generating correlation? Remember the problem that we have with a simple logit model. What does the simple
logit model do? It said this part is
zero because this is going to be zero and
assumes this is IID. Also extreme value but the
key thing is that it's IID. What that means is if you
actually get a shock, let's say price of a
particular product increases a little bit, well, for people on the margin, that's no longer going to
be their first choice. Then the question is, where
are they going to go? What's their second choice? In the logit model, there's no correlation because this is common to everyone. This is zero and this is IID. I'm basically saying
my second choice is going to be the
average of the market. That's why substitution patterns are proportional
to market shares. What we want to do is
you want to create a correlation structure
between the utility. If you have a high
utility from Mercedes, you're probably
also going to have a relatively high
utility from a BMW, relative to the
average consumer. There's two ways of
creating this correlation, and it could come through
either of these components. I've already gone through, this is the second point here
and we can actually create this correlation in 3D of these. We can continue to
assume that this is IID and create the correlation
through this term here. What this means is now that products that have similar xs, similar characteristics will be correlated and consumers that have similar demographics are going to have similar tastes. That's how we can
create the correlation. Or we could say, you know what, forget this, forget the
random coefficients part. Why don't we start
putting distributions on here that are not IID. That's what things
like the nested logit does or for those of
you are familiar, there's a more general
class of called the GEV, the general extreme value. That's what it tries
to do, it is say let's put the correlation
structure there. Now, just doing an unrestricted
correlation structure, we're back to the too
many parameters problem because remember
there was j of these. The correlation structure
is basically j squared or j minus one divided by
two. All the correlation. There are ways that
we could create this correlation but
we're either going to do it through this
part or through here. Now there's potentially hybrids. There are some papers that
we're not going to be talking about that we'll go back to the GEV framework and say we want to create
this correlation, but maybe we're going
to create it through some characteristic structure on this correlation structure
rather than through here. I don't have time to
fully talk about them, but there are ways in
which to incorporate. That's the interplay. Let me just stress this Delta.
I'll refer to it a lot. It's going to pay a key role and the key role is the
fact that this Delta, basically, let me go
back to the definition, just one slide on the Delta, you see the Psi is residual is going to appear only in it. It doesn't appear in the other parts of the
model and that's going to be very important for
the purpose of estimation. The other thing here
is, if you see here, I actually split before I
had all the parameters just Theta and I swim to
Theta 1 and Theta 2. The reason I do
that, Theta 1 are the parameters that enter
into this linear part. You'll see later why
I call it linear, it's going to basically enter our objective function
in linear way. These are going to be the
nonlinear parameters. The Theta 1 here, that's just going to
be the mean Alpha and the mean Beta, Alpha and Beta. Here, that's going to be this
deviations from the mean. These what I called
Pi and Sigma. Those are going to be the
heterogeneity parameters. I'm going to refer to
them as either linear or non-linear parameters
interchangeably. We talked about the
definition of the market. That's basically the level
at which these things vary. The key challenge
for the estimation, there's really two
key challenges here. One is the how to recover
the non-linear parameters. These are the Theta 2 is the one that govern the distribution of heterogeneity without
observing consumer level data. The extent there's Rabbit
coming out of the hat here, that's usually where
people struggle to well, all you see is
market level data. How can you actually get the distribution of
heterogeneity within the market? We'll see we'll rely a lot, although we don't
have to exclusive. You can actually even do
this with a single market but I think it's much more
intuitive if you think of the fact that we
see different markets with different distribution of, let's say demographics
and that's going to let us see the effect of these. Think of, I basically have market shares for
this market here, and then I have another
market next door that has some different distribution
of demographics. I'm going to see how
the aggregate choices with the aggregate
market shares vary and that I'm going to
try and identify them. The other key thing here is really this unobserved
characteristics. I know we keep going back to it, but let me just
mention a few things. First is this unobserved
characteristics one of the main differences from the earlier discrete
choice models. By the earlier, I
mean the McFadden and others work and have
in the 70s and early 80s, these earlier models often had a option specific constant, which is like this with consumer level
data and we'll see you in a couple of slides. We'll see exactly how
that fits in here. But whenever they went
to aggregate data, they didn't have that. Having this in here creates a potential for correlation with price
or the other axis, we're really going to be worried about correlation with price. Again, that's going to make
potential another challenge for the estimation. The other thing, and this is not something I'm going to
be talking about today, but will be an issue later on is that whenever we're
going to construct a counterfactual, we're going to have to decide
what happens with Psi_jt. That factor could be
whether we have a new good. We have to decide, well, what's the Psi_jt
for this new good. You could say, okay,
the xs I know, those are the ones that
want to experiment, but what do I do with
these counterfactuals? It's going to be very
important when we do welfare. Even when you talk
about dynamics, we'll see how this fits in. In some sense, it's
going to be very realistic for estimation but for a lot of what we want
to use the model for is actually going to
be problematic. If we have consumer level data, you could say the first issue, the issue of how do we
know the distribution of non-linear parameters I think or at least intuition is
less of a problem. Well, of course you get to see different consumers and
how they make choices. That's how you're
going to get some of these nonlinear parameters. The endogeneity problem
often people will say, well I have consumer level data, therefore, I don't have to
worry about endogeneity. This is a point I'll
return to a few times. Just on the face of it, that argument is wrong. That's the point
that's often missed. If there's an unobserved
characteristics, there's an unobserved
characteristics and if it's correlated
with price, it's correlated with price. The level of data that
you have just means that the way you can deal
with it would be different. But it doesn't mean that
just because you have individual level data,
it's not important. After lunch I'll actually show you some
applications and we will see that you can
actually see that in real data when people
have ignored it. You can see it in the results. I mean, that's exactly
what we would expect. Let me just step back before talking about aggregate data. Let me talk and say just for intuition sake what would we do if we had micro-level data? The way I'm going to talk about here is just
for intuition. This is not the way that we propose to estimate the model. Ariel will talk about that and one of the future lectures. Here just for intuition, just to fix ideas
because I think once we understand what we
can do with micro level data, with consumer level data, we can understand what we're doing with the aggregate data. Suppose we had micro-level data, one way to estimate
this model with this Psi in it is in two steps. In the first step, we're going to write what's the probability that individual i in market t chooses a particular brand as a
function of everything. In function of all
the characteristics, and basically here we're
going to exploit the fact the size where Delta appears. What we would do
is we could say, suppose I actually shut
off the Sigma part, so I have basically a logit but with heterogeneity
across demographics, this is what I would have. I would say it's this
exponential form. Well, what I have here is I have the mean utility and the heterogeneity across
the demographics. The mean utility,
you can think of it, it's just going to
be a fixed effect. It's going to be a market time fixed effect
and that's going to absorb both the mean effect but also this unobserved
characteristic. I can actually estimate this
using maximum likelihood, and what I get the estimates of the nonlinear
parameters here. These are the effect
of demographics and the mean market
effect. That's step 1. The next operand,
what I do is I take this thing that was estimated in the first step and
I'm going to project it onto the xs and ps. Now, if I think these xs
and ps are exogenous, exogenous in that they're
uncorrelated with this Psi, then I can just do this
using least squares. If I think there's
correlation here, then I'm going to
use instruments. But you can think of this as
a linear regression where the dependent variable are
the fixed effects from the first step estimation
and I'm just projecting them onto the xs and the ps that vary across
product and across time. There's many ways maybe why you wouldn't want to do this, but just in terms of this is a feasible and will yield
a consistent estimate. The key thing here is to realize this Psi_jt is going to be the residual in this regression. In summary, it's going to
be the econometric residual here in the second step. What's the intuition
that we get from here? I want to emphasize this because I'm going to try to
carry the same intuition over when we go to estimate the aggregate or using
aggregate data. It's two steps; first we recover
these mean utility, which you can think of it
just as fixed effects, and the Theta 2, the parameters that
govern the heterogeneity, and then we're going
to recover the Theta 1, the mean effects. It's important to realize
that different variation is identifying these
different parameters. The non-linear parameters are identified from variation and demographics holding
this Delta constant. We're controlling for
basically the variation if you want across products and
market in this Psi_jt, the unobserved, by holding this Delta with
this fixed effect. Then we're just looking across
different demographics, how they're making
different choices. That's what's giving us the distribution
of heterogeneity. The Theta 1 is actually now identified from cross-market or cross-products variation. I could actually also
look across product, but you think probably more credible if you
actually have it across market in the
second step regression. With market-level data
we're in some sense going to try to follow
a very similar logic. We're going to try and follow
these two steps of saying, let's first recover what
the mean utility is, what was the fixed effects here, and then let's project that
onto the xs and the ps. The only difference is
we're not going to have the within-market variation in the choice probabilities
because all we get to observe is a single
market share. I think of it as the
average choice probability. We're not going to have that
variation within a market. We're going to have to in some
sense for identification, we're going to
have to rely also, even for this first part, for this first step on
cross-market and variation. Really the key issue is
the fact that Psi_jt, we don't get to see multiple observations
where it's held constant. That's one of the
main advantages of the individual level
data is that we get to see the
choice probabilities of different demographic
groups holding Psi_j constant because with
different markets what we do is at least for
identification purposes, we can think that we have an
infinite number of markets with an infinite combination
of demographics, but what's changing is
the distribution of demographics and this Psi_j's. With individual level data, we get to see within a market
holding Psi_j constant, what are the different
choice probabilities? That's essentially the
difference between the two. With that in mind, let me
go to the more specifics of exactly how we're
going to estimate this with market-level data. One thing to step back and say, well, we know that really
how do we estimate thing? We say, well, we
have observed data. Here, these are the
observed chairs or observed quantities. Why don't we just choose the
parameter that minimizes the distance between
the observed chairs and those predicted
by the model? Why do you have to go and do all this complicated
stuff you're about to do? Why don't we just look at this non-linear problem and
just minimize that distance? There's a couple of
reasons not to do it. One, maybe not the most important but obvious is
the fact that there's a computational problem because all the parameters are going to enter the share
equations non-linearly, and if we have a lot
of parameters that could actually be
quite complicated, but we could handle that. More importantly is
the fact that if we want to allow for Psi's, if we want to allow
for what we think of these as structural error terms, they're going to be
buried inside here. For one thing, it's not
even clear how we compute this share equation because, well, how do you
compute the share if you don't know what Psi is? But even if you know how to compute it
or you're going to do it, let's start with a guess, and the thing is that Psi is actually is going to be correlated with price and
they're both buried in here. So standard linear IV methods
are not going to work. If there's a correlation
between Psi and p, they're both buried inside that non-linear function and
we don't know how to do it. If we want to have the
structural error term, so by structure here I
mean the fact that we have an error term that we have
in some sense a name for it, it's an unobserved
characteristic, then it should enter in here. If on the other
hand we say, well, all we really want
to say as well, there's some reason
we don't know that our model doesn't
quite fit the data, we could just look
at this difference. For the estimation,
what we're going to follow here is we're going to follow the
basic idea that it was proposed by Steve Berry
in a '94 ran paper and Barry Levinson and Pakes in the '95 econometric of paper, so this is the famous BLP with Nancy already alluded
to in the introduction. The key insight in this is the fact that it's
the following steps, is the fact that when
we have this Psi_jt's, so once we introduce it, the market shares
predicted by the model. This is basically the same
equation that I had before, basically saying we're
going to take for each individual what
their optimal choices, then we're going to sum
over those individuals that have j as their optimal choice where we're integrating over the
distribution of heterogeneity. With that, you can play
around with this residual so that the predicted
market share is going to equal the
observed market share. We're going to play
around with that, it ends up that under
very weak conditions on the demand structure, you need some conditions that
are quite weak conditions, you can actually invert
this relationship. You can write that
this mean utility is this Sigma inverse of
shares x's and p's. Here what's important is that Psi only interested
in the mean utility, enters in a linear way. The idea is after we
invert this relationship here that equates
the predicted chairs and observed chairs, we can actually think
of this just like in the individual level
data that we took the fixed effect
and regress them, so we're going to do a
very similar idea here, is we're going to
take these inverted chairs and we're going to use them as a "Dependent variable" in a
linear regression. That's going to be the key idea, is that once you can
do this inversion, the error term just
appears in here. Before going to the details, let me just emphasize one rule, and once we do this estimation, by the way we're
going to estimate the parameters via GMM, we're going to use
instruments for it, and the instruments you're
actually play two roles, and this is the point
that's often missed, they are going to generate
moment conditions to identify the
non-linear parameters. Unlike the individual
level data where we can estimate the
non-linear parameters from the maximum
likelihood stage, here we need to generate some moments to
estimate these as well, and then potentially
they're going to deal with the correlation of the prices and the error. The basic idea is
even in situations, suppose you actually
say you know what, I don't think in my data
price is endogenous. Why? Because it will
set by some experiment. I've actually heard
people say that, oh, so I don't have to worry
about instruments. Well, it's true you
don't need to worry for insulin for the second stage, but you still need to generate some moments in order to
estimate these Theta 2s. I've seen students that would occasionally
come and say, well, I'm not worried about this,
I've actually estimated without moments and actually get numbers because it's
a non-linear problem. But in some sense really,
they're not really identified. This last point I think, is often missed that when we
think about instruments, we always think about
instruments for price, but even if price is exogenous, we still need this additional
moments basically to solve to have at least as many equations as
we do parameters. The reason this is
really different from the consumer level data because with consumer
level data we don't have this
problem and that is because with aggregate data, although really
knowing the market is the mean choice probability, that's what the market share is. The average choice probability within the market with
consumer level data, we actually have more moments of the choice probability
is because they vary with the demographics, so we can use this
additional information to actually estimate
the Theta 2s, which we don't have here. But otherwise we're actually
going to try to follow the same logic as
individual level data. What I'm going to do now is
I'm actually going to go over the steps of
the estimation. I'm going to assume that
we have valid instruments. Yes, but when we're estimating the model here with the individual of [inaudible]
in some sense we've controlled for
these things here. There is no endogeneity here. Now, when we go to
the aggregate data, maybe in a way, what
you're saying is exactly a different way
of saying what I've said. There is the fact that
we need moments to identify the
heterogeneity parameters. You're right in the case that
if we actually go to what, a pure logit model with the
Arno non-linear parameters, then we're not going to need
these additional moments. But I think it's also the
way we think about them, it's a little bit different. When we think about price
and instruments for price, we should just follow
our usual intuition from demand and supply model. When we're thinking about
these nonlinear parameters, it's a little bit different. Whether you want to
literally think about them as there's a
correlation there or if you want a rain condition, then we want to have at
least as many equations as we have parameters, then you can think
of it that way. Where was I? What
I'm going to go, is I'm going to go through
the steps of the estimation. For now, I'm going to assume
that we have valid IVs. I'm just going to call them
Z's and they're there. After lunch, we'll talk about specific examples of how
people have used them. But for now, let's
just assume that we have instruments that satisfy the usual condition that
we want and we'll see specifically what they are. I'm going to follow the
original BLP algorithm. Those of you who
have been following the literature the
last few years, there has been some alternatives that have been proposed, and I'll talk about
them at the very end. Some people might
have preferred that I talk only about
these alternatives, but I think just in
terms of building up and understanding this model, it's actually easier in
this original algorithm. I'll talk about the alternatives later and also, if we have time, I'll talk about
the performance of the various algorithms later on. The algorithm takes on
the following steps. There's a little bit
of preliminary of data work, but I'm not
going to get into that. The first step, is
you need to compute the predicted shares
by the model. What you do is, you're going to guess a value or
what you need is, if you think of it,
you're going to guess a value of the mean utility. You're going to guess a value of the nonlinear parameters and then you need
a function that's going to tell you that's the share that I get if
those were the cases. That's going to be step
Number 1 and we're going to do this step over
and over again. That's going to be nested within what I call the inversion. We're going to take the
nonlinear parameters as given, and we're going to search
for a mean utility that equates this share that was computed in one to
the observed shares. For any given value
of Delta and Theta 2, I can compute what
the shares are; and I'm going to just search
for that value that sets the observed shares and the predicted shares
exactly equal. That's going to be
this second step and this rule is
often referred to as the nested fixed point
algorithm because basically, there's going to
be a search here, that's nested within a search
for the parameter Theta 2. But basically, for
any candidate value of the nonlinear parameters, I'm going to search for this value that equates
the two things. Then once I found it, I'm going to use this computed
value to compute Psi_jt. What do I mean, compute Psi_jt? It's just that basically
say this Delta is equal to x Beta plus Psi. I can actually compute
what that is and then I'm going to fit this
into a GMM objective. I'm going to go through
all the details of this. It's just the overview and then I'm basically
going to search for the value of the parameters that minimize the
objective function, the GMM objective function. That's going to be a search for the parameters and it's going to nest in it within
each candidate value. I'm going to do this inversion
of the market shares. Let me go through these
steps one by one. But before doing
that, let me give you an example of how this
works in the logit model. Now the logit model, a lot of these things can actually
be done analytically. It translates into a very
simple linear equation. To compute the market shares, well, we know that in the logit model these are
the market shares. Remember, all the D_i's
and v_i's are all set to 0 in the logit,
there's no heterogeneity. These are the shares. Well, how do I do the inversion? Once again here I can
do this analytically. This is actually well
known from the '50s, sometimes referred to
as the logit inversion and you basically
take the log of the log of S_jt minus the log of the share of the outside good and it just basically equal
to difference in these. This is [inaudible]
talked about last time. We're going to
normalize this to 0. What we have is that
the mean utility, which is just equal to the log of this ratio
of these two shares, is just equal to x_jt Beta
plus Alpha P_jt plus Psi. This is a linear regression. A linear regression where
the dependent variable is just the log of the ratio of the shares and now if you think that
prices are exogenous, just do this using OLS. Estimate using OLS. Speaker 1: [inaudible]. Aviv Nevo: Yes,
normally what we do here is we normalize this to 0. Ariel talked about a little bit last time we need to have
some normalizations. Going back actually to a
question that Dennis asked, one of the things we could do is if we have enough J's in here, we could actually put
a market fixed effect in it, to basically guide. It would be a constant in here that will
basically pick this up. You could do it or if
you think of this as a time series where t
is the time series, you could actually
put either a trend or literally a time dummy
for each time period. We could do that because
we have multiple j's for each time period. In principle, we could
still estimate it. So that's one way
to get, at least in the logit model around
this normalization. But you see basically here, this amounts of very
simple linear equation. Now, with very few exceptions, one of them, another one
would be the nested logit, which we'll see later on. We're going to follow exactly
this logic except that each of these steps has
to be done numerically. But that's basically the
idea of what we want to do. Now let me go a little bit
through each of these steps. This is the place where
I'm going to get, well, for some people a little bit too nitty-gritty and for some
people maybe not enough, but maybe on average I'll
get it right, hopefully. We need to compute the market
shares given a guess for the mean utility and given the non-linear parameters
and of course, the data. That's what it is theoretically, we want to sum over all the individuals that would check. Now of course, for some models, it can be done analytically, generally, it can't. The question is, how do
we compute this integral? There's various
ways of doing that. I think the most intuitive is to think of doing this
via simulation. Not necessarily the
most efficient, there's more efficient
ways of doing that, talk about them
somewhere down here. But let's just intuitively
now see how we do that and the basic idea is the
following: what we're going to do is we're
going to go and draw, ns here as the number
of simulation draws. Let's assume it's 100
so you take 100 draws from the distribution
of D_i and v_i? Let's assume v_i's
are just normal. You take a 100 draws from
the normal distribution. You go take draws
of a 100 people, let's say from the CPS for
that particular market, and look at their demographic. Let's say, income and education, whatever you need, so
that's your sample now. That by the way,
this draw you do once at the very beginning, you don't change it with every iteration
because otherwise, you'll have
convergence problems. But you do that once and
now what you say is for each value of this Delta and each value of these
nonlinear parameters, you can compute what the shares are or the choice probabilities
for each individual, and you're just going
to sum over them. Effectively, what we're
doing is we're going to analytically integrate
over Epsilon. That's the logit error term that's implicit in the
fact that I'm using a logit-like equation here
and then I'm going to numerically integrate over
the D_i's and the v_i's. That's how we're
going to simulate or compute this market share. I said that the Epsilons are
integrated analytically. As I said, there's other
simulations of how to actually draw this so the
original BOP article had some important sampling. So don't just take
a random draw, you maybe want to put more weight on particular
parts of the distribution. There's things called
Holton sequences. More recently, Ken
Judd and various co-authors have
proposed approximating this not by stimulation, but actually by quadrature, which actually ends up
working quite well. It works especially well if you don't have the demographics. If you're just
trying to integrate over this normal draws, you can actually do
it with and leaving very little computational error in it by just using
quadrature rules. But it's basically the same
idea of saying we need a way to compute or to
approximate this market share. The second step, what we
want to do is we want to invert the relationship. We want to basically say, "Well, these are the predicted
market shares." We want the set them equal to the observed
market shares. This has to be done
market by market. It's basically saying
here's the share, the vector of market shares
of all the products and it has to equate the observed
vector of market shares. The question is,
how do we do this? I say, some models that
can be done analytically, but generally it has to
be done numerically. You can just search randomly or a more efficient way of doing it is using this contraction
mapping, which say, start with a guess of Delta, use that to compute
the market share from that first
step and then you use that and the
actual market shares to update your guests and you can do this until
it converges so basically until the distance
between the initial get the guests h and h
plus one is very small, below a certain tolerance level. Choosing a high tolerance level here is actually
quite important, it's something I
think we didn't fully appreciate early on
and we talk about some of the recent problems
people have found out a lot is related to this. You need to compute this
actually quite precisely, at least 10 to the minus
12, maybe even more. This is probably one of the
most difficult parts of the algorithm because
you have to do this for every single candidate Theta 2. What happens is
for some Theta 2s, that are really bad parameters, they are very far
away from the truth, this might actually
take a very long time. For the truth usually, this does it fairly quickly. Fifty, 60 iterations I found usually converges
pretty quickly for good values of Theta 2 but bad values takes
a very long time. Once we have that, we can define our error term as this thing that was inverted, the results from
that second step minus these things here. Now we can write
the error term if you want as a function
of the parameters. That's the error term is a
function of the parameters, and the parameters are the
non-linear parameters. You can see now the name nonlinear parameters
enter inside this function and the
linear parameters Alpha and Beta enter linearly. Now we're basically
done, we form, here's the GMM
objective function, so we take this as
the error term, we're going to interact
it with instruments, so we're going to take
instrument that remain independent of this error term and reform this GMM objective
function where w is some a weight matrix
that we're going to use. Now what we're doing
is we're going to search over these parameters. Because it's linear, we're going to have to do a
non-linear search. There's a couple of ways to simplify these you'd ends up, you can actually one
called concentrate out the linear parameters and just focus the search for Theta two. You can use the implicit function theorem to
actually compute analytic gradients of it and that really speed
things up but again, that's in the
nitty-gritty detail. The key thing is that even
after all this stuff, this is still a highly
non-linear search problem and we don't really know
the properties of it. All that we know is
when we ever played around with data, as it seems to be that there's quite a few local optimum. That's going to be
quite important so it's going to be quite important
when you do this, is to start from a lot of different search
values and also try to different optimizers
and I'll talk about this in a second
and some other follow-up. This is basically the BOP
estimation algorithm. Let me briefly talk
about identification. The truth is I could probably
spend a whole lecture just going through the formal
details of identification, but it gets pretty messy pretty quickly so I'll actually
refer you at the end for proof and a lot of very useful discussion through recent series of papers
by Phil Hill and Steve Berry where they
actually talk about non-parametric identification
of these models with aggregate data and with
individual level data. But let me here try to talk through things a
little bit and informally. Ideally, how would we want
to identify these parameter, what's the ideal experiment? The ideal experiment that we want is we'd want to randomly vary the prices and
the characteristics, and maybe even the
availability of product so think of it as saying
an infinite price. We want to see then how
do consumers switch. When I take away
your favorite car, which car do you go to? That's how I'm going to get
the substitution patterns, seeing what's your
second choice. Maybe I don't see this
for each individual, but I want to see in the market and that's what's going
to tell us substitution. If we take away the
Mercedes and we see that nothing changes
except the share of BMW, that's going to
tell us something about the substitution pattern. On the other hand, if we
take away Mercedes and the only car that increase
in sales as a Honda Civic, that's going to tell
us something else about substitution patterns, unlikely but could be. What we're going to try to do is in practice so that's
the ideal experiment. Now, note that to run
this ideal experience, actually almost impossible
because we're going to have to change one by one all the products and
all the characteristics of all the J product and see
where all the others go. It's not just let's
take Mercedes and see what else
happens, it's like, let's take one-by-one the
100 cars we have and see exactly where the
shares go so it's a lot of these series
of experiments, but that's what
we're trying to do. In practice, what we're going to try to do is we're
going to try to get instruments that can
mimic the same idea. It's to say we're going to try to have instruments
that are going to impact in ways that
we believe are exogenous the choice set, or at least the
characteristics of the choice and after lunch we'll
see examples of this. The key issue is this, do we have enough variation
to identify substitution? It ends up that even
though in theory, these models could even be identified maybe with a
single cross-section, especially with
parametric assumptions, in practice we need to
have more variation. In practice to actually
get any estimates, especially if the
non-linear parameters, we really need to add
one of three things, or at least that's
what the literature is done, maybe there's others. One is to add supply
information and that's part of what the
original BOP paper did and Ariel will talk
about that later. The other is to
add many markets. They have not just a
single cross-section, but they actually have a
lot of variation across markets and in the demographics, that's what I did in
my work on cereal. Then a third piece
that's actually been very popular and very useful is to add
some form of micro, what's called micro-moments
or some micro inflammation and that's
what Emile patron did in his minivan paper that came out in the JPE or that's what BOP did in their so-called micro BOP all of which
we'll talk later on. All of these are actually
very useful ways and helping with the identification. That's just a very quick and
informal and probably not doing justice to exactly
what's going on, but we'll see more
as we move along. I talked about estimation, let me just say
just a quick word about inference and the
limit distribution. In principle, we can get the standard errors the way we always do out
of a GMM estimator. With a single cross-section, it's basically some
form of this equation. We basically have the derivative of the expectation of the moment respect to the parameters and the variance covariance
of the moment evaluated. This variance-covariance
matrix really has two or three
components to it, and that's what makes it
a little bit different. Want to just randomness
generated by random draws of the error term. That's the usual variance
that we have in any GMM. What's here is we also have potential variant generated
by simulation draws because we have to compute the market shares by simulation. Then there's a third part
that we usually assume away, and that is there
could be some noise in our estimates of the data of the market share because they're based on some sample
of consumers. A lot of times we'll assume it away because we're
assuming it's coming from administrative data where you literally have the population, you know exactly how many cars GM sold in any given year, and there's enough
consumers to do that. But if it's actually
based on samples especially small
samples especially if the market shares are small, there could be noise
in here as well. It'd be the three sources. Basically, as Berry Linton and Peak is showing and
released that paper, these last two components
could actually be very large if market
shares are small, but otherwise we
basically can use the following equation up there. A separate issue that I'm not
going to get into it all, is in what sense we want to do the asymptotic and
what did they do. We want to look at
the limit. Do you want to look at in J the
way we have it there? Basically we're looking
at a single cross-section and the number of products is going to infinity or maybe what we want to look at
is the number of markets. There's a lot of
subtle issues there that I'm not going to get into, and it really depends on
the structure of the data. In many datasets
we have a lot of markets and not
that many products. You might think that
what we want to do is the asymptotic are in the number of markets and then things
change a little bit, but for the most part, the
basic ideas we're going to use the basic formulas
and change them. That's it. Let me now go into some recent work
that talk both about some challenges with
this algorithm and then some alternative
computational method. Just survey them quickly, and then I'll finish by
actually showing you some relative performance of
these different algorithms. A recent paper by Chris
Knittel and ex student of his, found that different
optimizers gave very different results and were very sensitive to
the starting values. What they did was they took something like 10 different
optimizers, don't quote me, I might get the exact number wrong but on that
order of magnitude, I think starting each of them with 50 different
starting points. Then what they gave
is they said here is the distribution of the numbers that the algorithm
spit out at the end, and it was a very
wide distribution. You've got a whole range
of different numbers, and the different optimizers. Now they're a little bit silent, and it's exactly
what we should take away from this exercise. Some people have actually
basically said, well, this basically tells us
this whole algorithm or this whole line of
research doesn't work. I definitely have a
different take on it, but that's what some
people have taken. Let me just point out a few
things is the fact that what they did is not exact, doesn't really mimic what
any reasonable researcher will do in the
following two sense. First is, it's unlikely
we'll take just one starting values that,
I found an optimum. That's it, I'm done.
You probably going to start from at least
several more. I mean, ideally a lot more, but you'll start
from several more, and then you'll come to
realize, wait a second, I'm actually getting
different ones, which is the one
that actually has a lower value of the
objective function. That's the one that I want to
look at. That's one thing. The other thing is that
they actually took a few of these search algorithms
that are actually known to have very
bad properties, and it's not like the ones
that people use in practice. It ends up that
one value and say up let just find what it is. There is a very
clear caution here. These are still
highly non-linear problems and everyone's say, oh, we're hoping, you
know that one day [inaudible] will actually
be able to do this for us. I think that when that happens it's going to be a
big problem because I doubt. I mean, unless data will
actually built in and force you to specify 100 different
starting values, otherwise, we're not
going to give you a parameter estimates. But otherwise, one should be very cautious in doing that. There's a very clear warning here about the challenges
of non-linear estimation. I think that's at the end what these guys are trying to say. However, it has led a
bunch of people who say, well maybe we should explore alternative
computational algorithms and indeed Judd and Su and then in a
separate paper Dube, Fox and Su advocate the use of a slightly
different algorithm, it's called MPEC,
stand source for mathematical programming with
equilibrium constraints. The basic ideas is
actually quite simple. What they're going to do is
you're going to maximize the same GMM objective
function as we did here, except that it's going to be a constrained
optimization problem. The constraints are going to be the equilibrium constraints. What they want is they
want at the optimum, they want these inversion
conditions to hold, I'll put up equation
in a second. Maybe this will be clear and what this does is
it says it basically avoids the need to
perform the inversion in each and every candidate
value of the parameter. That's why it's very costly here to do
because some of them, you start by looking
and say, okay, this is a really crappy
parameter guess. It's going to take forever
to find the Delta that actually equates
because it's just that it's a weird parameter. But the inversion
has to do that. It's going to spend thousands of iterations trying
to actually get the contraction mapping to converge and this avoids that. It ends up that even though
you'll see in a second what they actually
increase the number of parameters by a lot, and I'll make it
clear in a second. It can actually solve
with modern optimizers, for example, quite common
is this thing called nitro. If you use nitro, you can actually solve problems
that are quite large. You can actually solve it quite efficiently and indeed
the actual report, significance speed
improvements and better, much more stable properties
using this alternative. Let me tell you what they
like because really all I need is basically
this one equation. Formally what they're
going to do is remember before what we had was the
same objective function, except the Xi's were
a function of Theta, and we minimize just over Theta. Now what we're going
to do is we're going to make these Xi. They're going to be
parameters themselves. What we're searching is over Theta and Xi's and remember
these Xi's are actually a very high-dimensional
because there's one Xi for each
observation basically. Each product in each market. We're going to search over
this high dimensional space, but it's the same
objective function except that now it's a function of the Xi directly subject to this
equilibrium constraint. This inversion that we had, we're now going to say the
predicted market shares have to equal the
observed market shares. At the optimum, this
is going to satisfy exactly the same conditions as the BOP objective function. In terms of statistical
properties is going to have exactly the same
statistical properties. This is just a way of
finding that single point. As I said, the key
thing is that you actually avoid the need to do the inversion for each and
every candidate value. Sometimes you're only requiring it that it holds at
the equilibrium. Of course, if we
actually subject to actually finding the
optimum correctly, this should actually get
the exact same solution as the nested fixed point. Now of course, there's
actually many bells and whistles on how
to make this faster. I skipped all of them. But
that's the basic idea. Let me talk about
another alternative that's recently been proposed, and I actually put this up with some hesitation because this
is actually a working paper that hasn't actually
gone through all the chats and stuff, so there's actually a
non-zero probability that this is all wrong. We haven't found
out, but it actually seems to work reasonably well. With that disclaimer, I
hope we have it on video. When I get angry emails
from various people, hopefully, that disclaimer
will guard me against it. The basic idea here is
actually potentially a very simple idea and it
build on ideas that we've seen in dynamic choice problem. It's the following saying, what we want is we want
this share equation. If you want this switch
notation here a little bit, this is basically the inversion, the thing that we're inverting. We say let's approximate it by first-order
Taylor approximation. It's basically this thing evaluated at some
candidate size zero, so let's take a guess for
it plus this derivative. Now let's set this approximation equal to the actual shares. What we're trying to
do in the inversion. Well, this now gives us, if you want an approximation, I just rearranging things and
approximation for Psi_jt. I'm just using this
first-order approximation. Just this term here. The
basic idea is saying, well, suppose we did this, we can now put this in to
the GMM objective function. Give me a guess of Psi
naught, this guess. I can use that to
approximate Psi_jt, and then that's what I'm
going to plug in, in here. How am I going to turn this into an estimation algorithm? It's basically the following. There's two steps that are
iterated until convergence. Basically, I start from a
guess of this Psi naught. I use this guess to
compute a parameter. I put it in that pseudo
GMM objective function. I get this guess and now I use this guest to update
my guess of Psi. Then I iterate
back-and-forth until they converge and converge your both the
parameters and the Psi. Very much like what we're doing in these other algorithms, we want all these conditions to converge, but in some sense, what we're doing here
is we're changing the order of the
nested fixed point. The nested fixed
point, we're searching over parameter in
the outer loop, and in the inner loop we were searching for the big size. Right here what
we're saying is say, let's switch the order. Let's start with a guess
of the Psi and then converge your Theta and then
update the Psi, do that. Very much like MPEC and envoys, the inversion and
each, and every step, but has a low dimensional
search unlike MPEC, because here the search,
remember when we're searching here for the GMM objective function
is non-linear. It's only over Theta, which is a fairly low
dimensional space as opposed to having
all these Psi. Potentially has the
benefits of both. Once again, this disclaimer, there are some proofs in
the paper that show things, those proofs still
have to be vetted through the usual process. Let me just finish
by actually showing you some results of actually a student of ours that graduated a few months ago is making a lot of money
now at a hedge fund. But before he went, he needed a quick thesis, a third chapter for his thesis. We were trying to think of what would be a good thing to do. He's a very good programmer, probably one of the
best programmers our raise that I've had. I thought, you know what?
I've always been curious about the how these works. I know all the authors
always present results, but we all know what it's like. If you propose a new
methods somehow, it always works better
than what was before. The incentives aren't aligned. You want someone
that's not tied to any one of these
to try to do that. What I had him do
was run bunch of a Monte Carlos and I
don't have the details of exactly what he ran, but a bunch of Monte Carlos that try these three
different alternatives. The tree that retried were MPEC, what he called
nested fixed point, which is the original DLP
inversion and this ABLP. There's a lot of details of
exactly how we did this, which I'm going to
skip and maybe I could detail a little
bit as we go along. What I have he reported is, he had a different number
of markets varying from 50-2500 and different number of alternatives or products
going from 10-50. For each one of these, this is the conditional
and convergence. Conditionally they
actually got a number. What's the average time in
seconds until convergence? All of these cases, it
actually converges. It's a Monte Carlo so he knows what the two
parameters are. This is convergence
to the truth. He actually I think did not have in his Monte Carlos a lot of problems of getting
to local optimum, at least I believe
that's the case. Before I actually
go compare this, I think the one thing
I want to point out, even though some
of these problems are actually fairly large, so 2,500 market with 50
products, that's a lot. You rarely will
have so much data. The original BLP paper that
we'll talk about at lunch, they had 20 markets with
about a 100 products, so about 2,000
observations overall. It's somewhere in this range, it's not exactly on here. The key thing to know, these
are seconds to convergence. I think the first thing
to note from this, it doesn't take that long. Maybe this whole idea,
is it this? Is it that? Or whatever maybe that's not the first-order thing that
we should worry about, because even the
highest number here, this is about a 1,000 seconds, it's about 15 minutes. I'm starting to feel
my age when I say, I remember when we first
started doing this, we waited a week to get
a single convergence. It's pretty simple. Now
in terms of performance, generally what you see here, and this is actually
starting from what he called Theta c, so it's a relatively
good starting value. He knows the answer, so you
start relatively close. Generally what you
see is you see, there's some variation in this. He didn't get MPEC or
the nested fixed point. Sometimes one is faster, sometimes one is not. The nested fixed
points tend to do much better when you get
to larger problem. Because here, now the dimensions really start kicking in, the fact that it's a high
dimensional problem. But the thing that actually
seems to be do quite well, is the ABLP. Similarly, when you
actually go now, the same idea, but looking from
a further point, here we actually had
a problem of even getting any
convergence with MPEC. It's actually known
that some starting values MPEC doesn't start, you have to restart it from
a different starting value. Just because it really
can find anything that satisfy the constraint
just the way that it does a search. Overall, he was
actually getting, I'd say best mixed
results with MPEC and very good results with
the approximate BLP. Just a word of caution here, I'm sure that the MPEC results here could probably be
significantly improved. If you actually took his code
and gave it to Fox or Su, I'm sure they could actually
do it significantly faster. The way that I see it this
is not actually testing which algorithm is faster,
but in some sense, testing an RA, good RA and say, "I have an RA, I
gave him doing this. What could I expect?" What I've learned from this and I've actually heard
it from others, is that the MPEC, once you get it going and get all the bells and whistles, is probably going to be faster, but it actually takes a bit
of an effort to do that. Just a word of caution in terms of the computational time. But as I said these problems
with real data are actually highly non-linear, and the problem is not to get a long time to get a
single parameter, but you really want to beat
at it as much as you can, start with his wacky
starting values, you can and throw
different optimizers and different algorithms in it. The idea is, even if one of
these is slower or faster, I would recommend trying as
many of these as you can to guarantee that you get a
reasonable answer. Yes. Speaker 2: Are you
coding this in MATLAB? Number 2, is there any
CanCode out there? Aviv Nevo: Yes. Good
question. Thank you. In terms of CanCodes
for a while, the nested fixed point was
the code that I put online. It's still available at places. There's a couple of ways in which it can
improve or a couple of ways which is
actually it's wrong. I haven't actually
bothered updating that. Su has in his webpage
actually very detailed code. I believe that either
Su or Fox might also have updated code for
the nested fixed point. There are codes
that are out there. Both of these, I
believe are in MATLAB. The MPEC, basically
what it does, what you'd want is you want to have this nitro optimizer. Because if you
don't actually have a good constrained
optimization program, it's going to take a long time. Because dimensions are highest, you need a very efficient one. People seem to be
happy with Knitro. I know Ken Judd recently has
used a different optimizer, I forget the name, but Knitro actually comes. There is now a direct interface with MATLAB, but
it's a separate one. My student, by the way, I
didn't report that here. This is with MATLAB. He did find that action
programming these and in C, you do get significant
improvements, especially for that market
share equation because that's called over and over thousands of times
throughout the algorithm. Writing that in C really helps. Again for me, I'd rather have it take another
20 minutes rather than spending two weeks or more trying to program
it, but that's fashion. The other thing in which
you can speed this up is by parallel processing. MATLAB today, if you actually
have the right toolbox, what you can do is
write a loop for this for each one of the
market separately and that's just a different command at the P4 instead of a four. Basically then it
takes automatically how many processes
you allocate to it? Anyway, I'm happy to
talk more about it and I suspect that's a little
bit too nitty and too gritty for at least
several of the folks here. Any other questions? 