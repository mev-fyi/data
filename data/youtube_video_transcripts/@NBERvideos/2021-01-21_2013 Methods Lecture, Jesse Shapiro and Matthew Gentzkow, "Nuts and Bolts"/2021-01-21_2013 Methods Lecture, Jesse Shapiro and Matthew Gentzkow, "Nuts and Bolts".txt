Jesse Shapiro: This
is bonus material from one standpoint or
maybe a further tax from another standpoint.
I don't know. Matt and I are going to
talk a bit about some of the more practical issues that come up when you
work with big data. Most of the time,
yesterday and today, we've been talking mainly about issues when the data are wide. That is, you have a
lot of variables or a lot of dimensions of the data. Another very important,
and I think increasingly important issue that people are facing is when data
is long, that is, you have a lot of units or
observations in the data and the combination of those also I think is becoming
increasingly important. Some of the datasets
we've already talked about are really large in terms of number of
units and also huge in terms of number of
dimensions of the data, especially when
you start thinking about tax and the like. That just raises a lot of practical issues
that people doing empirical research
at the frontier of data size and data richness
are having to face, whether we like it or not. Matt and I want to spend a little
time talking about just nuts and bolts, practical issues about working with data and organizing code, especially geared
towards worrying about situations where
the data are large, the code is complicated, and so keeping track of things and worrying
about things like parallelization and
data storage and efficient computing are really important higher-return
activities. We are going to try to talk about questions that
people are going to face in their daily research
lives like here is your two terabyte dataset that you got from
your data provider. Here's the external
hard drive that arrived on. Where do
you want to put it? What are you going
to do with this now? You can't just load it into
Stata in a working memory, you have to put it somewhere in order to be able
to work with it. How do you store it? Do you put it in a bunch of flat files? Do you put it in
one big, flat file? Do you try to just buy the really advanced
version of Stata that handles two terabytes of
data, or what do you do? Something that's
probably worth saying is neither Matt or I is
a computer scientist. That's probably not worth saying because it's going
to be so obvious. We'll do our best to
prove that to you. We're also not
software engineers, and we're just mainly trying to not so much tell
you how to do things, but more point you in
the direction of who are the relevant experts in
these different areas. So where do you go to learn
more and what kinds of directions should you have often when you confront
these problems? Mainly, we want to be a portal
to other resources rather than standing up as the
ultimate experts ourselves. I think the good news, there was a lot of what
we've talked about, a lot of the computing
like Tati was doing yesterday that you can just
fire up on your laptop, load your data into
working memory open R, as he showed you, run one
line of code and boom, you've got your beautiful
coefficient plots and your penalty pads and
all that great stuff. But increasingly, people are confronting data that don't have that property. The congressional, the text of the US congressional record, that Matt and I have
worked with a lot, that's about 50 gigabytes. If you throw the PDFs in there, so you think about the
images and the formatting, that's half a terabyte. The Nielsen scanner data, which is data on transaction
records for 35,000 stores over a period of five or six years
that we have at Booth, that's about five terabytes. All of the text on
Wikipedia is about six, the 20 percent sample of
the Medicare claims data, that's about 10 terabytes. Facebook is 100,000 terabytes, Matt put this together, I don't know where
that came from. Then all the data in the
world which I think actually, this can't be right,
because all the data in the world is already
on Facebook, but somebody thinks
there's data that isn't on Facebook
and that amounts to about 2.7 billion terabytes. In principle, there's a lot of data out there that
you might like to apply statistical
analysis to. At some point, you're
not going to be able to just take
it in your laptop, put it in working memory
and rock and roll. What we're going to do is, I'm going to spend a little
time talking about more nuts and bolts issues
of software engineering. How do you organize
code and data, especially when you're
collaborating with other people and then
Matt will come up and talk about issues that really
more relate to data size, like databases and
cluster computing. Try to walk through some
different scenarios of size of data and
type of analysis you want to perform and maybe give some guidance as to how to
organize that kind of work. This bid on what we're calling Software Engineering
for Economists, Matt and I have some written
notes on this subject which are linked right now at the bottom of the page
for this workshop. If you go there and
you want to read more, you can read more. We got interested in this
topic because we realized that even though our
original passion that got us interested in
economics was finding interesting datasets and
good research questions and sensible methodologies, doing all the things that the econometricians
earlier called heuristics. What we actually ended
up spending a lot of our time doing was just
writing and debugging code, working with code and
wrestling with it and unwinding other
people's code, trying to figure out what it was
doing and then unwinding our code later when we forgot what we were
thinking when we wrote it. We started finding
ourselves with lots of frustrating
situations that may resonate. We would go back to
an old directory where we had done some analysis and try to run it
and lo and behold, a key input file, the
one that's at the top of the do file was not
there so the code broke. Whoops. Or we try to use a
dataset that somebody put together and we'd find that actually we have populations
for null states. What is the state that has
population 653,177? Who knows. Now this calls the whole
dataset into question and we can't use it because it doesn't make any logical sense. We can't really
interpret anything from these data or we'd
want to change the set of control variables in our
regressions and because we duplicated the same
information dozens of times, we'd just be copying and
pasting all day or we'd go to a directory where we had tried to be careful and name
files in a useful way, but we'd realize that there are different
confounding versions like this one says use this one, but then why do
we have this one? Which one am I supposed to use? Who am I supposed to believe? The name of this file or
the name of that file? We can't get back our
original results. That's pretty frustrating. Replication is a basic
aspect of doing science and this situation makes that
really hard or even worse, you might find
yourself with tons and tons of different versions
of the same file where you and your coauthor have conscientiously tagged them all with dates and your initials. Good luck trying
to figure out if you want to get to
the current results, how you're supposed to use
these files to get there. As I said, we are not
software engineers, nobody hires us
to write software for them and there's a
good reason for that. We're not computer scientists. But what we have
learned is that most, if not all, of the
common problems like this that we're
all facing that are frustrating us when we're dealing with the computing and trying to get the computing out of the way so we
can do the analysis we really find exciting have analogs in software design or software
development contexts. Taking advantage of the methods people have developed
elsewhere and the concepts people have developed elsewhere can improve your productivity a lot at very little
cost to you, if any. Today, I'm not going
to explain in detail all the principles involved and the notes that we wrote up have some
more information. They also point to some
really useful references by experts so we'll
point you to that. Rather what I'm going to
try to do is highlight very briefly how you can try to
solve some of these problems. How you can use well-known
methods to address them and I'm going to focus
on incremental changes, things that are one step away from how you're
doing things now. Not huge infrastructural
changes to how you work, but just single
incremental changes in one direction or
another that are going to greatly improve your
life and remove some of the headaches that I showed you on
those screenshots. Let me hit a few points
that I think are really important and then
I'll turn it over to Matt, talk about issues with data of large size or computational
problems of large scale. Let me first talk
about automation. What is important to
automate in doing research? To motivate things today, I'm going to talk about
a hypothetical example of a Matt and Jesse paper, which is about the effect of
introducing television in different counties in the US on the consumption of potato chips. If you don't know our
research, that's fine. You won't get the joke. But in any case, that's what
we'll talk about. We got a dataset from a
data provider that tells us how many bags of potato
chips were bought in every county in the US in every year from some period
through some period of time. The data also include when television was
introduced in that county. We have to decide what
to do with these data. One approach, which most of you probably have
already learned by now, and most of us learn in graduate school or
maybe before that, is not a good approach is
something like the following. Open the spreadsheet,
hit Save as, output both of the
worksheets to text files, open Stata, load the datasets, merge the files together, compute the log of chips sales. Run the regression of
log of chips sales on a dummy for whether the
place has television. Do a select all, copy all the output to
Microsoft Word, save that, close everything, delete all the files that
you made along the way, except for maybe
the original data and go write your paper. That's like the fully
hands-on manual approach. No steps are automated except
the inversion of a matrix. There are actually, I do know a few people who use
this approach today. I'm not going to name names
because I'm on camera, but they're out there. There are two main
reasons not to do this. There's a scientific reason, which is it's important for
you and for others to be able to get back to every number you ended up with
from the data source. That is, if somebody wants
to follow your work, this approach gives
them no way to do that. How are they supposed
to find the source of every number if
you yourself have no pipeline from the data
down to the final source? The second issue is one of efficiency and that's the one that maybe is the
more immediate concern. If we want to change something
about our specification, maybe we want to use
the square root of potato chips sales or just the level of
potato chips sales, we have to go back and repeat all of these nuisance steps and ideally remember
how we did them before, so we don't change anything
inadvertently along the way. We really don't want
to have to do that. A lot of people
having faced that frustration probably
switch to what I'd call the semi-automated
approach. Semi-automated approach is, you script at least a
bunch of the steps. Now we've got a
script that cleans the data and another script that merges the files together, and another script that
runs the regressions. I don't know what
regressions alt does. We'll talk about that later and we've got something
called this extract. Maybe this is the
original data and here's our data on potato
chips and so on. Now maybe it's a
little bit easier if we want to go and
change something, we just have to change
one of these scripts. The problem is, how do we
know which script to change? and if you want to rerun
the whole pipeline, like let's say we
get a revision of the dataset from the source
they send back and say, oops, we actually messed up a few of the potato
chip numbers. We've gone to retype them
and they've changed. How do we know what order
to execute these steps? A really simple solution
that takes you from semi-automation to
full automation is to write a shell script
in your favorite language. This is a DOS version, you could write in Bash
if you like Linux, if you want to be slick
and OS independent, you could write it in
Python. That's what we do. That's going to automate all
of these steps from start to finish and handle the calling of these
different scripts for you. Why is this nice? It's two reasons. One,
it's very convenient. Okay, now when you want to
run all the steps from export the files from Excel down
to compile your text file, you can just double-click
or run your script. You have a single
command to execute to your operating system. The other thing that's
great about this is even if you are never actually
going to run this script, the fact that it's
there, it means you've intrinsically documented
the order of operations. Now you know exactly in what order things
need to be executed. Much of that is obvious. You know, probably
you should compile the paper at the end, not at the beginning
before you run the code, but a lot of it is not obvious
like this clean data take data from merge files
that do or the reverse. Who knows the names of
the files don't say. This avoids you having to
go sift through the files, try to figure out what
order to run things in. If you want to be really cool, we've always been
tempted to do this, but we've never done it. You could write a little
script at the end here that submits your
paper to a journal. We haven't gotten that
far and this way, using a shell
script like this or some other glue language
like Python is great because it's really
natural to call other all different things like your call stack transfer, you can call Stata, you can
call your latex compiler. You could perform OS operations like copying or
moving files around. Now you have a natural
interface in which to do that. By the way, this is really just a poor man's one
directory version of a much bigger
framework which is used in software
development called Make. It's much more
elaborate than this. Which is basically
what's used when software that you
buy is compiled. It's compiled using
something called Make and what Make
does is basically establishes a framework to take source code and
produce a target, say, executable file and what Make
does is it keeps track of dependencies so that if I change one resource and I leave
the other ones intact, Make knows which things
need to be recompiled and updated and which
things don't because it understands the dependencies
among all the pieces. This is a really a poor
man's really low-level version of something
that is much more sophisticated in the actual world of software engineering. Okay. Another tool I
want to talk about from the world of
software engineering is called version control. Let's go back to the example
I started with earlier of the directory filled with different versions of the
same file. Here we have our directory and we
have the February 21, 2013 version of clean
data and the February 21, 2013 [inaudible] are my initials version
of the file and then another version from
the same day with a at the end and then other versions from other dates and so we're using this
as a common convention, I see a lot when I look
at other people's code, we're using dates to
demarcate versions, and initials to
demarcate authors and I understand why there's
an incentive to do this. It's clear what
the motivation is. People want to be
able to compare. That is you want to know, well, why did the results
change between February 21st and February 27th? and this gives you
a way to do that and also it facilitates undo. Like if I make a mistake, Matt can just delete
the underscore j, MS versions of the
files and go back to the happy world before I got
involved in the project. You can really easily undo things and I understand
those motivations. But this approach is an enormous pain and
why is it a pain? It's a pain because you have to remember every time you
make a change that you want to consider
a new version to tag the file with your
initials and the date. That also means that if you
aren't incredibly diligent, this approach is confusing. For example, if you look
closely and you know your Stata, you might notice
that there's a do file, that February 27th regressions file that has no
corresponding log file. Now, which file is it
that's output by that one? Does it overwrite the
February 24th file or does it overwrite this file? If I want to know what
results I got from the February 27th version of this regression
file, where do I go? Okay. The burden on you to be diligent is incredibly high
because if you're not, this is actually worse
than no system at all. Now your system is
completely broken down, you've lost complete
track of where everything is and another thing, since we're economists that
I'll point out to you is, you don't own a single piece
of commercial software that you've paid for that is
developed using this method. If Google was developing
code by just putting little dates at
the end of files, you would not get any search
results back, I promise. What do software firms do? They use various types of
version control software. Version control just sits
on top of your file system. It's basically another layer
of your file system and it understands what
it means for there to be two versions
of the same file. You no longer have to
manually keep track of different dates or who
edited what because now you have a version control
system sitting on top of your file structure that understands that and it's
going to manage conflicts. For example, if Matt and
I both try to change the same regression specification
in a different way, it's going to throw up
a flag and say, hey, you guys need to reconcile your changes. You tried to do things that are in
conflict with each other. You need to fix that. Why
is that nice and why is that essential to
collaborative code writing? Why does every
single software firm that ever sold you
a piece of software you use that system? Because this way there's always one authoritative
version of everything, the current version and
you can edit without fear. Now you have an undo command for your entire file system. If I want to go back
to the state of my files on February 24th, I just click a
button to do that. What does your life look like
after you implement this? These are now my files. I took away all those
annoying dates and initials. Now have these little
green check marks which in the version
control system we use tell us these are
unmodified versions. These are the same as
the current version of the authoritative version and if I want to go and look at, say, what has changed
in this file over time, I just right-click on Windows and I'll get a little
log that tells me all the changes that have
ever been made and who made them and what they were
thinking when they made them. Okay. If I want to go back
and understand something, I can do that and
if I want to know how two versions of
the file differ or how my current version that's
sitting on my computer differs from the authoritative
version that lives, say, on a server someplace, I just click a
button and now I got a highlighted difference engine that tells me the difference
between the code. Here's what's been added and if something had been subtracted, there'll be a little red line showing me what was taken out. Now I can get an
immediate comparison between the version
I'm working on in the current version or between any other two versions if
I want to see, say, why Matt added some specification
or what changed between two versions of
the data and if I want to go back and
undo Matt's changes, I just click a little button
called revert and erase Matt from my code and go back to life say February 14th before the February 27th
modifications and by the way, there's a little
bit more detail on this in the notes we'll post. If you're really diligent
in the way you use this method, use
version control, you can guarantee replicability of your codes or your code
or your directories really, really easily by basically
just making sure you execute all the steps in
your run directory script every time before you tell the version
control system that you've got a new
authoritative version. That's a little aside and that brings me to what
goes into a directory. How do you organize your files? To make them most useful, a lot of people use a single directory,
single project model. I have a directory that does
everything in my project, everything from soup to nuts, takes the original
data, extracts it, cleans it up, merges
things, runs regressions. That's a reasonably
attractive approach for fairly small contained
projects. It's simple. A problem with that
approach is it makes it hard to figure out the
dependencies among the file. For example, if I want to
know if something changes, chips that CSV, do, I need to rerun
clean data or not. Or is that going to impact
regressions dot do or does regressions dot do only
depend on TV data.dta? This directory structure
doesn't tell me that. A more natural style, we would argue, is to split up the directories into
functional groups. For example, in this case, you might have a directory
that builds the data and a directory that
analyzes the data. Why is this nice? Well, now I can see immediately that
chips.CSV and TV.CSV. These are just temporary
files that are used in building TV data.dta. That's really the only
thing that's used as an input into my
regression analysis. Another nice thing about this is that I can use the
same resource, TV data.dta for
multiple projects. If I want to go
and write another analysis directory for
a different project, say not about potato chips, but about tortilla chips. Because we're really
branching out our research interests
then I can do that and still call the same resource from
that's built over here. I can use this code for
lots of different projects. Whereas if I use
this approach where everything is
aggregated together, that's going to be
much harder to do. Matt is going to talk in
detail about database design. Let me pause on a couple important things to note about how
to organize data. We used to face the problem
of getting datasets back like this from our
research assistants, some of whom are in the room. Where, for example, we'd be tearing our hair
out because for example, we've got Virginia in two
different regions. The state of Virginia is in two
different census regions. That doesn't seem like that can make a
whole lot of sense. Or why do we have no
state for this county, but it has a state
population that looks like the same population as other counties in New York.
That doesn't look right. Why do we have all these incoherent
features of the data? Or why do we have a
missing county in a county dataset? What's
the point of that? What does that trying
to communicate to me, the user of the data other than somebody made a
mistake somewhere, and I don't know what it is. This dataset, it's useless
because it's illogical. I know that there are mistakes,
the dataset's illogical. I don't know what
mistakes were made. But I can guarantee that
I can't just go and use these data in a
straightforward way without trying to
reverse engineer what the actual
intended logic was. If your bank stored
data like this, then what would happen
is when you would go to withdraw money from the ATM, either nothing would
happen or you would take somebody else's
money out of the ATM, or you'd withdraw the
state population of New York from the ATM or
something like that. You have to know basically just
logic about how the world works tells you this cannot be best practice
for storing data. In fact, it is nothing
close to that. Data are stored when
they're used in, say for in transaction systems like at a grocery
store or a bank. Ultimately, they're stored in something called
the relational database or a
transactional database. Where basically the data are required to be structured
in a logical way. The physical shape of the
data encodes this logic. Every variable in the data
is an attribute of an element, like an observation
and every table, which is a group of
elements, has a key. Like in this table it would be the county code or in
this table it would be the state postal
code that has to be unique and non-missing for
every element in the table. Then if I want to relate, that's why it's called
a relational database. If I wanted to relate
items between tables, if I want to know what's
the state population that corresponds to this county, then I do a join
between the tables and I use what's
called a foreign key. That is, here's the key
for the state table living inside the county table
to execute that join. These are very simple encoded in this logic
is a very simple set of rules for storing data in what's called
normalized form. Matt will talk again
about that later. If you store data
in normalized form and impose these
rules on yourself, you're going to save yourself from a lot of hassles like this. The way we would recommend now, a problem with storing data in normalized form is you can't run regressions on normalized data. You need matrices. You might need state
population alongside county population.
What do you do? Basically, again, there's more detail on
the notes we've post. What we advocate is something
like the following. Store your data normalized using the logical structure of
a relational database. Matt will talk about actual
relational database systems. For small data, you
can get away with flat files like CSV files are Stata data files
if you like that. For medium-sized
eight or larger data, or depending on
your comfort level, you could go to an actual
relational database system. It's not that hard to do. Store your data in
this normalized form. Construct a second set of
data files where you've made your key transformations
like you've taken the log of population or whatever you're going to need for your analysis. Then do the join
between the tables at the last possible stage as close to your
analysis as possible. If you follow those rules, you're going to avoid a lot of hassles and
you're going to find your data makes sense to
you six months later, after you haven't
looked at it in awhile in a way that it might not if you're doing joins really high up in your data
analysis pipeline. Matt will talk about what to do when your data are really big. Next principle that's probably worth mentioning is
called abstraction. That's a principle we all know. Go back to our example of
rampant duplication here. I've got the same controls
and like ten different regressions. We all
know that's annoying because it means that if
I want to make a change, I have to copy and paste
that change line after line. That introduces a lot of
possibility of mistakes. I want to cut that down. It creates unnecessary work and lots of potential for error. The way most people handle that is by doing
things like this, like in Stata of maybe I create a little local
store the names of the variables I want
to use as my controls. If I want to change
my control set. I just change my local once. I don't have to go
change it every time and everybody probably
already is doing that. What may be less obvious, but is common
practice in writing software is to do the same
thing with operations. Here I'm doing it with
like lists of things. There I was doing
a list of things. I can also do it for functional
operations. For example, I might want to calculate
the leave out mean of per capita potato
chip consumption. I might want to know the average potato
chip consumption of all the counties in the
state, except this county. For some empirical
Bayes analysis or as a control variable or
whatever, doesn't matter why. I might go and write
code to do that. But I maybe I will
also want to leave out mean within the metro area. The average of all the counties not this county within
the metro area, or the average of
all the counties not this county within
the metro area. But now I want household
level consumption instead of per capita consumption,
per household consumption. The way I've
accomplished that in this little example is I
went copied and pasted the first set of code
and then I modify it as needed in the second
set and the third set. The problem is the usual problem with copying and pasting
and modifying like this, I made a lot of mistakes. For example, I left the state identifier in
over here by accident. Over here I use per capita
potato consumption where I should have been using per
household consumption. Now I've made one
leave out mean and two pieces of garbage that if
I use them in my analysis, they're going to be totally
wrong and misleading. What does abstraction
do in this case? Basically, it creates the abstract version of what I'm trying to
do in this case, compute the mean of
something within a group, leaving out a
particular instance. The leave out mean and replaces all of this jumble of code with calls to an abstract function
called leave out mean, which is really easy
to do in Stata. It's even easier to do in other languages that
are more designed around writing function or
object oriented program. Now, I've eliminated the
need to copy and paste. What I want to change from state to metro area I change it once in the logical place,
the group variable. I don't have to change it
over and over again and I'm letting the code do the work of remembering what a
leave out mean is. A great advantage of
this is if I need it for another project or
if I want to do it again I have it already written, I don't have to write
it again from scratch. One of the things
you might think given how obsessive we are about all of these other things is that we love writing notes. Maybe we like to write
directories that look like this with tons of
notes, notes_data, notes_data_demo and
notes_identification and various readmes and memos about readmes and other documents
that tell you which documents you need to look at. That approach is tempting when the code is not clear
because when the code is not clear you're tempted to fix that by trying to explain it to yourself or other people by
writing lots of documents. But writing lots of documents is like keeping
track of versions by changing dates and using your initials is a
dangerous game. Because once you start playing
that game you have to keep peddling to keep up with
the changes to your code. For example, if you do this, if you write
directories like this, I pretty much guarantee you
one day you will wake up, open up your directory and
find something like this where your
documentation which is supposed to tell you the list of variables in your dataset has a different list of variables than is actually
in your dataset. What was the point of
this documentation actually that you spent
all this time writing? Was it to confuse
your future self or your collaborators
or was it to remind you that actually this should
have per capita income not median income
in it? Who knows? Now, you have a conflict between your notes to yourself and your data and there's no real way to resolve
this conflict, or you might have a memo about your results that includes
a copy of the Stata output, but now your memo has a slightly different coefficient than your Stata logs do, which one's the right one? Were you intending to
send your future self a message be aware of 0.1347 or is there some reason why actually the code's right and the
document's wrong, who knows? Do you have to go back
and change the document? Will you really do
the work to keep up your documentation every
time you touch your code? I bet you will not. As a result, you'll
end up with a lot of confusing useless
documentation that is actually worse than
no documentation. A better use of your time, another example here
which I see a lot is when I download other
people's code is here's and you'll probably find
that in some of mine if you look carefully here's a header declaring what this DO file does
only it's wrong. This DO file is not run_regressions.do
running regressions, it's append files.do
appending files. What was the point of typing this out, what was
I accomplishing? Nothing, just confusing myself. First step is delete
all that stuff. Now, problem with deleting it is maybe it's impossible to interpret what's going
on in your code. Here's my DO file
unclear code.do. I calculate EL, the ratio of 0.4-0.2 and then
I complf, and I'm done. That might confuse me. Maybe
I should start writing memos again to myself
explaining what the complf procedure is. Stop
yourself from doing that and ask yourself is
there a way I could change the code so that next time I look at it I will
know what's going on? For example, maybe instead
of calling this EL, I could call it elasticity
and instead of just having a floating magic
number here 0.4 that came from nowhere I can write percent change in quantity, and for 0.2 I could write
percent change in price. Now, it's obvious why the ratio of those has some meaning that's called an elasticity and why is that an input into
complf, well, because that's actually called
compute welfare loss. If you know your Harberger's
triangles you know that elasticities are going
to be useful for that. I've turned useless code that I couldn't use for
anything without writing documentation that
risks becoming out of date into useful code
that is self-documenting. I'm using the structure of the language in this case Stata, but could be anything
to help remind myself when I look back at this
later or my coauthors, what is going on in this code, what is it trying to do? It takes almost no
work to do this. It will probably save
you, you'll break even within hours of work if you adopt this
approach if not faster, and it means that you can delete all of those
memos and other things explaining the
complf procedure and other and what EL probably
stands for and so on. Now, these techniques are great when you're
working alone, they're incredibly
important when you're working
with other people. If you just look at trends in co-authorship rates you can see very clearly that rates of co-authorship have been
exploding in economics, there's a lot more
collaborative research especially in applied work, but not limited there. Keeping track of
other people's work is really important and
that elevates the value of all of this stuff because if
you don't know what complf means surely your collaborators are not going to
know what it means. Another piece of collaborating
together is being smart about keeping track
of who's doing what. This is how we used to do that. We used to do it over
email and we might end up having an email exchange that would be something like this, hey Matt do you have that robustness check
where we control for the amount of ranch dip sold in each county
because I'm writing this section on dipping sauces and I wanted to mention it. Matt may write back, sorry, I thought you were doing
that because it's similar to that other thing you
were doing where you are controlling for salsa sales, but let me know anyway
if you want me to do it or if you want to do it. Then I would write back
cc'ing our third co-author, Mike Sinkinson who's written some stuff with us
and I might say no, I thought Matt was
doing ranch dip and Mike was doing
the salsa dip. I wasn't doing either of those because I like to
sit back and relax. Mike might write back and say, no I did the salsa robustness check already two weeks ago, see my email from
August 14th at 9:36 AM, which is a great help and really easy way
to go find stuff. Then I might say actually in that email you are controlling for the log of
salsa consumption, but actually I wrote you back on August 14th and I said we wanted the level of
salsa consumption not the log of
salsa consumption. I'm on it. Now, at the
end of this email thread, which has taken an
incredibly long time, we still don't know who is doing the thing where we have
to control for ranch dip. We've gotten nowhere on
the original purpose of the email and we've
only barely managed to unwind the error we made on
August 14th where we had this miscommunication
about how we were going to do this salsa regression. Once again, if this is how
Microsoft kept track of who, if Microsoft's process
was going to be like add inline spell
checking in Microsoft Word, let's just send an email
to a guy and say, hey, Fred, can you add inline
spellcheck into Microsoft Word? Yeah, totally I'm on that. There would be no inline
spell checking in Microsoft Word, for sure. They don't use a
system like this, they use a much more
complex system, probably a proprietary one. To Microsoft, you
don't need to be Microsoft to fix this problem, there are free
systems like Asana, which is a free online
task management system that allows you to
have a group of users working on the same
project and to make unambiguous the assignment
of tasks to users. If I want to have a salsa
robustness check task, Jesse can assign that to Mike Sinkinson you
can't see this maybe, but I've assigned it to Michael S and I can make
explicit what I want him to do, and then if we need to have
task relevant communication that can be threaded and
connected to the original task. If I want to look
back and see what's going on with the task
at any point in time, I just click a link, open the browser and look
right back at the task. Now, if Mike wants to ask me a question like
do you want to control for the log or the
level of salsa consumption, we can keep track
of that and we want to know later when
Matt wants to now say, why did we do the level of salsa consumption not the log, whose idea was that? He can go back and
find out that it's my fault that's what
I asked Mike to do. Then when Mike thinks he's done, he can just click
a little button and say this is complete, get a little green check mark
on there and then there's no ambiguity that Mike believes at least that
the task is done. Whether I believe that or
not is another matter, I could reopen the
task or I'll get a notification via email
that the task is complete. All of this is completely free, you can be up and running on
this in like two seconds, I can show it to you on my phone afterwards if you
want to see it. Actually, the version control
software I showed you before by the way is
also free, open-source, so nothing we're talking
about today requires spending any money and almost none of it requires spending
any time actually. This would take you like
two seconds to learn, this is like iPhone software, very cleanly designed and
beautiful, easy to use. MALE_1: Can you mention the name of the version control software? Jesse Shapiro: Yes.
The question was, what's the name of the
version control software? It is called Subversion. MALE_1: The Subversion. Jesse Shapiro: Yeah, but there are a lot
of them out there. You can pick the
one that you like. but that's the one we use. MALE_2: The name of
the task management? Jesse Shapiro: The name of the task management system is Asana, A-S- A-N-A. If you're Googling
that you'll probably get some yoga links. Just skip past those, pretty soon this
software is very good, so I suspect it
will soon surpass the yoga links and Google
status, but I don't know. You can download the
iPhone app or whatever, you can use it
through the browser. If you're following
along at home, I think it's just asana.com, but if you Google Asana, A-S-A-N-A, you'll see it. Subversion, if you
Google Subversion, you will actually
get the software and not the concept of subverting. Again, we are not the
pros in any of this, we are trying to be a
portal to point you towards professional solutions to
common software problems, but I think can be useful in
doing is just pointing out. If you're doing applied
work, empirical work, you're going to be dealing
with larger data over time, more complex data
and very likely with lots of collaboration and you're going to find yourself spending
a lot of your time not thinking deep thoughts
or wondering about what would be some interesting
problems, but just wrestling with your code and other
people's code and your data and other people's
data and you're going to find that adopting the practices used by people who
do that thing for a living and who live
and die by doing it well is going to save you a
lot of time and frustration. In a very short amount of time adopting these methods
you're going to find you'll have broken even within
hours or maybe a day and you'll be gaining productivity every day
from doing this stuff. If you dig deeper and push further in the direction
of best practices, you're going to do
even better than that. Try to learn from professionals. Again, we have some
resources online and make your life easier, there's no reason not to. I'll take questions maybe
before I turn it to Matt. Questions on this stuff, yeah. MALE_3: When you work on code with people like
other institutions, are there any Subversion I imagine [inaudible] in
that matter, how do- Jesse Shapiro: The
question was, how do you collaborate with people
at other institutions, say if you're using a
version control system? The answer is, that's
pretty straightforward. So for example,
there are actually some free online sites that
will host the stuff for you. A lot of people use GitHub, I think there's one called
Bitbucket, Cloud Forage, which distributes
subversion, maintains ones. I'm not an expert in those
but there are a lot of them. Most of them have
a freemium model where you can get it free, something free right now
you could go on and have your own repository
in five minutes. Then if you wanted, a lot
of users or some other richness then they
would charge you some monthly fee or
whatever for that. You can also set up your
own version of this host it on your PC
or hosted like even better for more stability on an Apache server
and then you could have other people
access it remotely. They're going to be the
usual data security issues that you'd have with
any file system, just your regular file system, you have to worry about who's authorized to access
what and whatever. You have the same access
control system issues you'd face with any file system, but this allows you to at least contain the conflicts and the workflow and stuff like that into a collaborative
file system. Are we okay on time for
questions? Question back there. MALE_4: Can you use a
version control for binary files where only like word processing files
are ascii file? Jesse Shapiro: This
is a little bit of in the weeds question
about version control, but a question I am
happy to answer. The question is, can you
use version control for binary files or only like word processing
files are ascii file? The answer is yes, you can. We routinely do that. Version control systems are really designed to
integrate best with code. So they're designed to work
great with ascii files, they're incredibly
efficient at storing ascii files because say, I have a two-gigabyte file
and I change one number, the version control system, the increments of the storage on my computer from making
that change is trivial because the version
control system just stores the instructions to go from the earlier version
to the new version. The storage and cost is really, really small and you have a diff engine if you want to find where did the
change happen, it's trivial to do because
it's built to do that. With binaries, that process
is harder because there's no way like a binary
would be like an executable or
Stata data file, something that isn't
stored in ascii, like or in a character sac. There's no way for a
general purpose diff engine to go inside and see the
differences between files, but you can still store
them in there and we do that because it's
convenient to be able to find old
versions of things. That increases the
storage costs so you have to manage
that a little bit. But yes, technically
it's no problem, you use some of
the functionality, but you retain the ability
to see revision histories, track who did what, and importantly, I think
for replicability, you retain the
ability to go back to particular versions of datasets, like if you're using Stata,
binaries or whatever, and that's really, really nice. Depending on your purposes, if you look at around
it what firms do, you'd find some that
version there data, others that don't, it depends in part on how important is it to be able to recover the state of the database
at a point in time. So for firms that don't care about that, they don't do that, if they just care about
today's inventory, they might not worry
about that just have a backup of the
current snapshot. Other firms that might
need to know why did we execute this transaction
on this date this way, they will retain a version
file system for their data. Steve Sikala, question. Steve: Once upon
a time there were people who preferred to have all of their files, all their Stata working on a single master and here
now you're creating, I don't know if [inaudible] , another program that's
getting called. How do you manage to control in the [inaudible]
or the directory or just like running
around [inaudible]? Jesse Shapiro: The
question was basically, once you get away from, say, one directory with
a giant file in it, Steve's asking you about a model where say I'm going
to work in Stata, I have one do file
that does everything. The advantage of that is I
have only one place to look, once I go to this approach, I might have a little
function files running around all
over the place, I might have a lot
of files to tackle. How do I keep track of
all of those, whatever? The answer is, basically, that's exactly the thing that these file systems like
version control are built to do.  For example,
if you're in a software firm and
you write a function to perform some operation like execute a transaction
or something like that, or even a more basic operation
like modify a database, you don't write
that function for every single purpose every time, you keep it in a library, and then you call access to the library every
time you need it. There are various
ways to do that, you could do that without
using version control, you can do that inside
version control. There are some notes on how
to do that in the link that we've got up on the
site but basically, the answer is, you just need a teach your directories to grab resources from other
directories and that's actually not
that hard to do. If you do it in a smart way, you can actually set
things up so that you have only relative file
paths in your code, you never have to have that
big thing at the top of your code that many of you might have
right now we are here, it says global data directory, and then it says c slash, blah, global code
directory with libraries, c slash blah, blah, you can get rid of
all of that and let the directory structure
handle a lot of that for you. Follow-up question from Steve, I'll take your question later. Let's have Matt come up. Matthew Gentzkow: Basically,
all of the stuff that Jesse just talked about is common sense best practice
for computing whether your data is big or your data is small or your data
is high dimensional, your data is low dimensional. I now want to talk just
very briefly about a few computing issues
that get towards stuff specific to
working with big data. Again, it's going
to be very brief. The goal here is not to
actually teach you anything about these topics really in
part because I'm no expert, but just to give you a flavor of the things you ought to
go learn about if you're working with these problems and some practical advice given
a particular scenario, what things might
you want to do. I'm going to talk
about databases, I'm going to talk about
collaborative or distributed computing and then I'm going to talk briefly
about some scenarios. In terms of databases, the first thing to say is there are two things
that are pretty different, one is database theory, just a body of theory and computer science on how
to efficiently store, organize, and retrieve data. Second is database software, things like SQL and Oracle, that are software that
implements those kinds of principles to actually manage
that stuff on a computer. Jesse had that little section on keys where he was talking about why you might want to store your data in separate tables that have
non-missing keys and so forth. In one of the points,
I think, it emphasizes just understanding and applying the principles of
database theory is a really good idea even if you're not using
database software. These are principles
that apply very broadly, anybody working with data should understand and apply
those principles. Database software is something that you ought to know about, and perhaps use in certain circumstances where you end up using really
big datasets. A couple of just quick thoughts on what are the principles, the principles that
underlie database theory. The first is something
called normalization. According to Wikipedia, database normalization
is the process of organizing the fields and tables of a database to minimize redundancy and dependency. Usually, this involves
taking some big table like Jesse's county-level
dataset that included state
population, and region, and all these other things,
and dividing it into smaller and less redundant
tables and defining relationships between them
with things like foreign keys.  Practically speaking, the key most important thing that normalization involves is, A, store your data in
such a way that A, every file, every table, has a unique non-missing
key for every observation, so there are some variable like county code and a county table that uniquely identifies
every observation, and two, every variable in every table is a logical property of that
entity that it's keyed on. If it's a county table, it only has
county-level variables, if it's state table, it only
has state-level variables. That's the core and most important element
of normalization. There's a whole list. There's something called
first normal form, and second normal form, and third normal form, and
forth normal form, and a whole bunch of
different increasing levels of normalization that imply eliminating more and
more redundancy. For example, higher levels
of normalization involve, make sure that
there's no variable in your table that could be derived from other variables
in your table, etc. Jesse was saying, we
in our research, have come to believe basically
anytime wherever possible, even if you're just
working with text files or Stata files, you should always
store your data in normalized tables
until the last minute when you have to put
it together into a single matrix to run X prime X inverse or
whatever you're doing. Why did normalization evolve to be a key part of
database theory? Originally, this was
primarily about efficiency of storage and efficiency of transactions, modifications
in databases. If you have a county table that involves a bunch of
state-level variables, you are representing
the same information many, many, many times. The state population
of New York is repeated in a bunch of rows. Redundancy involves storing the same
information many times. If you're running the
database for Citibank and if you store all of the state-level characteristics and every individual's
observations, you end up with a
really big dataset. Originally, this is a lot
about storage is scarce. You're trying to make that efficient, so
normalization helps. It also helps with the computational
efficiency of databases because if your data is not
normalized and you want to change something like the
state population of New York, you have to go in and change
it many, many, many times. That also takes time. Those are the
original motivations. There are also two other
benefits of normalization that explain why we think we've come to believe
at least for ourselves, that this is a good
way to store data. One, it guarantees the logical
coherence of the data. You can't have something like state population for
an observation with a missing state indicator. Perhaps, least obvious but most important or
among the most important, it makes really
transparent to the user the logical structure
of the data. When you get some new data
set that you don't know what it is and you
look at it, it's much, much clearer when the
structure of the data itself documents which variables are defined at which
levels of aggregation. What are the relationships among all of the variables
in the data? The first two relate to why this is a good
idea for big datasets. The second two relate to why it's a good idea
for all datasets. A second key principle that
underlies the development of databases and why
there's such a thing as database software is indexing. As an example, Jesse mentioned
at the beginning, that NBR on our servers has about 10 terabytes
of claims data from Medicare over a little
more than 10 years. The way those files are
currently stored on NBR servers is in a bunch of individual zipped sas files. There are lots of directories across these different years
that hold thousands and thousands and thousands
of files that together comprise
this claims data. This is not a crazy, it's a very common way to store data, and in some circumstances, this might be the optimal
way to store data. Notice, however, a property
of storing data that way. Which is suppose I wanted to
do something like extract all claims for heart disease
patients aged 55-65. What would the computer have
to do in order to do that? It has to open every single
one of those thousand files and read every line of every single one of
those thousand files. Many RAs working in health economics over
the years have written scripts that loop through
all of those files, opening each one, extracting the relevant observations,
and storing them. Again, there's nothing
inherently wrong with that, but an observation is that slow. If you imagine now again, United Airlines wants to look up your airline reservation
when you get to the counter. Imagine if United Airlines
server now has to go open up every file containing
every record for every passenger who has ever flown on United Airlines, searching through until
it finds the one for you, Airline reservation systems
wouldn't work very well. What might be a
solution to this? There's something
called indexing, which is not a
particularly new concept. We already know about it. The obvious solution that we have long
understood for books, libraries, etc, is
to build an index. Store all the files
in flat files on a server approach
is the equivalent of the Library of Congress puts all the books on the
shelves and says you want to find an economics textbook, start on the first floor,
on the first shelf. Start reading through
all the books. Keep going until you find
the one you're looking for. It doesn't work very well. Clearly, the
solution is to build an index so you can
look up that book, know where it is, and that's exactly what database
software does. It allows you to specify particular fields of the
data that are going to be relevant for looking up observations for
choosing subsets, etc., for doing joins and build indexes
of those variables. When you say I want
patients age 65, if the variable age is indexed, it tells you the locations
on disk of all records. It does this in the
background, but implicitly, it tells you the location
on disk of all records for 65-year-olds and now you only have to look
at those records. We could index age, gender
or type of treatment, etc., and that would allow much faster extraction in the context of
database software. The obvious benefits of
indexing, and therefore, by extension, the
obvious benefits of working with actual
database software or one, you can look things up fast. Two, this is crucial the
way that software works. Indexing allows it
to be feasible in real-time to impose
constraints on the data like a key
has to be unique. Because if I add a
new observation to a dataset where the key
is supposed to be unique, the database software has to look at all other
observations to make sure the value of that key that I included for this observation
does not already exist. That's an operation.
It has to do a million times every day, and so that becomes efficient. If there's an index, it just
had to look at the index, it doesn't have to start
opening up the actual data files. Now, there are important
costs of indexes too. One, you have to store them
and they can be really big. Similar order to the data itself if the values are
close to unique. It takes time because every time we change
something in the data, we have to update the index, going through a large
five-terabyte database, and building an index
the first time for some variable takes a long time. This is where,
among other places, the real art and science and skill of people who know
how to use databases, unlike us who don't, really comes into play
because optimizing a database involves
thinking about how do we design the
structure and how do we design the indexing to make the operations that we actually need to do efficient? There's no right answer to that. It depends on what
operations you want to do. Database optimization
is the art of tuning a database to the
particular needs of a particular application. That's what a lot
of the real work of people who know a lot about
database design involved. Now, just because this
term gets thrown around, another thing that you'll
hear people talk about, our data warehouses, which you might think is a
synonym for databases. It's not quite a synonym. It's something slightly different
which actually is quite relevant to the kind
of things we do. Traditional databases, SQL,
Oracle, etc., were designed, built and used for
operational environments, things like the airline
reservation system, transactions at your bank, etc. If you think about
those environments, they share a number of
particular characteristics. First, the transactions with the database involve a lot of small reads and writes
in equal proportions. You change your airline
reservation or you check in. It has to go say
you've checked in, you show up at the
counter, it needs to retrieve your reservation. There's lots of
back-and-forth, lots of small transactions, including both reads and writes. It's crucial that a database
like that be robust to many users accessing
the data simultaneously. Because United is hitting that database from all kinds of places all over the
world at the same time. It can't be the
case that it breaks if I tried to look at Jesse's reservation
at the same time that somebody else tries to look at Jesse's reservation. Latency, which means how
long does it take from the time that I
put in a request to do something before that
gets up and running. That delay time is
really crucial because if there's a 20-second delay at the beginning of
every transaction, the thing's not going to work. Typically, United Airlines
doesn't care too much about what its database
looked like a year ago. It just cares a lot about what the database
looks like today. In analytic or
research environments. This includes what
we do, but also where a lot of this
stuff is developed, it includes business
environments where people want
to use their data now for analytics to figure out which customers to
market to or something. The requirements
are very different. Instead of lots of
small reads and writes, typically, the data
are pretty static. The Medicare claims data
hasn't changed much since we first downloaded
it; once a year it changes. They are typically changes. Do we add a bunch more? Unless we go in and maybe restructure things or
clean some things up. Main part of the data's
not going to change. Writes are infrequent, reads are frequent and reads
tend to be large. It's common for people
to do things like, I would like to read
a terabyte of data. Simultaneous access
is not a big deal. Latency is not such a big deal because the transactions
are often big. The startup, a fixed cost
doesn't matter too much if the variable cost is huge. We often care about history. Typically in these
kinds of environments, unlike the operational
environments, there's the raw data and then
there's a whole pipeline of extracts of that data and extracts of those extracts
that get produced, that are the things that
people actually want to use. When computation is slow, it's not efficient to, every time somebody wants to ask a new question about say, what was aggregate
medical spending in California in 2002? To go back and re-compute that
from the raw claims data, it's nice to produce once an
extract that has state by year level total
spending and store that and everybody
can work with that. You'd need to sort of manage
these custom extracts. I won't say anything
more about this, but the books you want to
read, if you're interested in, how do you design
things efficiently for this kind of situation, search for Data
Warehouse on Amazon, that's what data warehouses are, database systems tuned
to these requirements. That typically means
in accompany that typically means we have
our operational database. Then we have alongside that a data warehouse
which periodically pulls the data from the database and stores it in
a different form that will be efficient for
doing this kind of analysis. The second topic I want to talk about is distributed computing. The first is about storage. This is about actually computing all of these great high-dimensional
econometric methods as well as other things. Distributed computing,
obviously, as everybody knows, means computation shared
among many independent CPUs. There's lots of kind of confusing terminology
flying around, none of which has
precise definitions. People will talk about
distributed computing and also talk about
parallel computing. Those don't have precise, there's no precisely defined
difference between them. But typically parallel
refers to systems that have, say, shared memory, and a bunch of processors, like your PC might
have a bunch of different processors where it's distributed refers to more across machines that
have their own memory. Talk about cluster computing, and grid computing; again, there's no fine definition, but cluster computing
typically refers to some more like in a room
there are a hundred PCs on a rack that
form a cluster, and they're all the same and
they're all in one place. Whereas grids often refer
to all over the world, we have some network of lots of different computers that have
different specifications, but they're all
connected and we can farm drops out to all of them. One important thing to note about distributed
computing is just your computer already does this, your phone does this. Your operating system knows how to work with
multiple processors, each of which has
multiple cores. Your video card on your
computer probably has hundreds or perhaps
thousands of cores. Standard statistical
software knows how to exploit these things without
you doing too much work. Stata, all you need to do to do distributed computing
is by something called Stata MP and run it on a computer that
actually has multiple cores, but there are no computers
anymore that don't. You're in good
shape. In our MATLAB or even lower level languages obviously those things
give you more control. So you have more direct
control over this. Basically there's
some add-in package. Typically there's
several in ARB, but the one that people use
a lot is called Parallel. There's a parallel computing
toolbox in MATLAB. You just need to install
those things and then you control in your code which things are going
to be parallelized. For example, in MATLAB, there's something
called a par for loop. If you wrote code that
had a for-loop in it, which says, do this
operation, a hundred, do some sequence of
steps a hundred times, but using different values
for this indexing variable, that's a naturally
parallelizable operation assuming it satisfies
certain conditions. If instead of writing
for you right par for, then MATLAB will send the
step for I equals 1 to one processor and the step for I equals 2 to another process and the step for I equals 3 to another processor
and distribute that. You control it in MATLAB. If we move away from
your local PC to think about clusters and grids, larger networks have computers, you don't want eight processors, you want eighty processors. All of you presumably have access to lots
of such resources. All of our universities have computing clusters
of various kinds. There are non-commercial
scientific computing grids that are collaborations among lots of institutions distributed
all over the world. Those are things
that you can apply. Basically, you put
in an application, say I'm a graduate student at
such and such institution, or I'm a faculty
member and get access. Then you can run code on those very large
computing grids. There are commercial grids like Amazon has something
called the EC2. Lots of people use, those have gotten
to be very good. Anybody who wants to do higher-level
distributed computing, there are lots of resources. How does it work? For those of you who haven't
done this kind of thing, what does it mean
in practice to say, run some code on
the EC2 cluster? It just means you write whatever code you
would have written. You have to think about
writing it in a way that you know how it's
going to be distributed. So which operations are
going to be sent where. Then the actual distribution
of that across the nodes is controlled on the backend by something called
a batch scheduler. You write, just like
Jesse was talking about, you write a bash script
or a shell script that is a master script controlling
everything that includes instructions to the
batch scheduler for where to send things. If you've broken your code, think about code
that you've written, like the for-loop, where basically I want to
execute this code, but I want to replace
the file name. I want to loop over a hundred files. That
would be a natural thing. Then you just write a
bash script that says, Here's the variable I want
to basically loop over. The batch scheduler will know, I'm going to take each of those executions with
the given filename and send them off
to different nodes, get back the results. That you can do for basically any code with relatively
little overhead, relatively little
additional work. For applications where
the distributed part of the computing
becomes very important, one of the things that's
useful to know about is a framework called MapReduce or what's
also called Hadoop. Open source implementation
is called Hadoop. What is MapReduce? It is software, but really
basically you want to think of it as a conceptual
model for programming. This is very much
like what Jesse said about abstraction in code. You're doing a leave out mean, you find yourself writing code
to do that over and over. Wouldn't it be nice to
instead write that code once, and now that always will
be handled automatically and your code is really
legible and clear and all you have to see is the important stuff
that you're changing. This is the same thing. This originated at Google, and basically the
guys at Google felt like we're doing all of
this distributed computing, and every time we do anything, we have to write code to figure out how to cut things up into
different pieces, and which processors
to send them to, and how to handle
the output when it comes back and how
to handle errors, and 80 percent of
our code is stuff which isn't specific to
what we're doing right now. It's stuff that's about
the distributed computing. Wouldn't it be nice to
abstract that and only have the code encode the pieces of it which are new and
specific to this application. This is a model for programming
that facilitates that. The insight is almost
all algorithms for distributed
data processing can be represented in two steps. The name MapReduce
comes from calling those two steps, map and reduce. Think about you have some data like Google's cache
of the entire Internet. It's broken up into a
bunch of little chunks, the map step says,
for each chunk, apply some algorithm which maps the raw chunk of data into
some much smaller summary, and I'll talk about
examples in a second. Reduce step says, now
retrieve those summaries that you've produced for
all these different chunks and combine them in some way. At a very abstract level, that is a representation
which applies to almost anything you can think of doing in distributed
computing or a large share. The point of MapReduce
was if you can represent your code in that way, MapReduce will take care of all of the distributed
computing part because it knows
now how to do that in a generic way for
whatever you're doing. MapReduce software
will then handle partitioning the data,
scheduling, execution, managing communication
between machines, handling errors, handling failures of machines,
and so forth. Examples of the operations you could put into
this framework, suppose I have a large
collection of say, news articles and
I want to produce counts of all of the words
in those news articles. Well, the map step
would be to take each individual
document and map it to a vector of word count pairs, and the reduce step
would be to take all those vectors from all
those different documents and collapse them into a
single sum of those counts. If I wanted to do this
thing of extracting medical claims for
65-year-old males, the map step would
be to take a set of records and map it to the subset of those that
are 65-year-old males, and the reduce step
would be to just combine those things together. Often you'll see
that the reduce step can often be trivial,
just combine everything. You might recall from
Jesse's presentation of our slant paper, this marginal regression
that we were using to generate the slant
index involved basically separately regress y on counts of each
phrase individually. It's like for every
phrase in the dataset, you run one regression. That's naturally
parallelizable because those regressions can
all be done separately, so the map step there would be run the regression
basically for phrase j and the
reduce step would be combine those
coefficients together in the way that we need. This is a schematic of
the way this actually works in the original
Google implementation. We're not going to spend
time on the details of this, just one thing to observe. Part of the reason that
the reduce step is in this process is because
in large operations, you want to be able to do
the mapping and the reducing in parallel and have those things be going
on at the same time. The way this structure
works your program sends instructions
to a master that controls scheduling that assigns to a bunch of workers
to do the map step. The output from those
individual map steps are stored somewhere and the
master also provides information so that a bunch of other nodes that are going to do the reduce step can start reading from those in parallel while this
is still going on. If you google MapReduce, you'll see the original
journal article published by Google on this, which is super clear and easy to read and really interesting because it gives you
a sense of all of the stuff that this
does in the back end. There are lots of
implementations of this. Nobody uses MapReduce because that's the proprietary
thing that Google has, but there's an open-source
implementation called Hadoop, and Amazon has a hosted
implementation called Amazon EMR, which is
really easy to use. If you have some problem
you want to do this way, you can pay a little to have an account on
Amazon and run it there. Literally all you
have to do is write your code so there is
some script which you can call the map script
and some script which you can call the reduce
script and you're done. That's distributed computing. A distinct but closely
related concept is distributed file systems. You can do distributed computing with files that
are all stored in one place and then
you basically have to move data out to each of the nodes that are
going to do computing. A key thing to understand is anytime you're doing
distributed computing, the moving around of data is typically the crucial bottleneck that slows everything down. When the data is really big, if you said I'm going to
farm this out to 200 nodes, and I need to move
this five terabytes of Medicare data over to
each one of those nodes, that's not going
to work very well. In those cases, what works much more efficiently is to break
the data up into chunks. Those each live on
separate nodes and there is a file system on
top that keeps track of where they are and knows how to allocate jobs in a
smart way so that an individual operation is done somewhere that's physically close to the relevant data. Ideally, the best thing from the scheduler's
point of view is I can allocate each
job so the data to do that job is
already on that machine. Next best is the data
somewhere physically very close to that machine so
the transfer happens fast. This is closely related to MapReduce on
the software side. Basically for any
implementation of MapReduce, there's an associated
file system and those things are designed
to work closely together. Hadoop has something called the Hadoop Distributed
File System, Amazon has along with
the Hadoop hosting, they also have their
distributed storage system, which they call S3. Basically if you sign up for an account on
Amazon and do this, you get distributed
storage for free. You pay for it, but
you don't have to do any work to Hadoop. You get it not for
free, but not too hard. Here's the schematic from the Google File System paper
for the way this works. Again, there's a
master scheduler that keeps track of
where all of the files are so when you send
a request like, I would like to
read a given file, you send that request
to the scheduler, the scheduler knows where all of the individual pieces are. Again, you can look at
this, it's all interesting. A key issue here is the thing would work very badly
if the model was, you tell the master
which pieces you want, the master goes out and
collects all of those pieces, and then the master
hands them back to you, because then the master becomes a huge bottleneck in the system. Everything has to flow through there and everything
slows way down. You don't see it, but it tells your software where the
pieces are you want. You then send requests
for those things, and then the individual chunk servers where
these things are stored, send you the data directly
so there's no bottleneck. The last thing I wanted to say, that's just to give
you a flavor of some of the things that are out there that you might want
to learn about. Now concretely, when
you're doing research, you know what you'd be doing. Should you all run out and
rewrite all of your code to work on Amazon's Hadoop
cluster, maybe, maybe not. I want to give you
basically three scenarios. Scenario 1 is the easy one, which is my data are big, but maybe not that big; on the order that you could
fit into working memory, 100 gigabytes or less or 50 gigabytes or less,
something like that. In these cases basically, you don't need
database software, you don't need to do distributed
storage of your data. Distributed storage of your data would probably slow your
way down. Basically, you can store the data
the way you're used to. You should, as we've been
stressing, I would say, organize it into
normalized tables for robustness, for clarity. 