to make sure everyone can see the slides and hear me uh also thanks for including us on this great program been a lot of great papers uh i'm excited to share this joint work with with buzz brock and lars hansen so let me just jump right in so there's been a lot of calls for policy implementation to address climate change and these are based on our confidence and our knowledge and confidence in the climate economy dynamics however our view is that the knowledge base remains incomplete to support the sort of standard type of quantitative modeling in the realm of climate change and elsewhere to do this we believe that to truly engage in evidence-based policy requires that we're clear about a few things one the quality of the evidence being used in policy making decisions as well as the sensitivity of the modeling inputs based on these evidence that we have and so our aim is to explore ways to incorporate this uncertainty that exists in climate change models into quantitative policy analysis guided by the tools of dynamic decision theory now this paper is going to take inventory of the consequences of three alternative sources of uncertainty and we're going to provide a novel way to assess them so these three sources come in the form of carbon dynamics which map emissions into carbon in the atmosphere temperature dynamics which then translate that carbon in the atmosphere into temperature changes and then economic damage functions that depict the fat fraction of productive capacity that's reduced by temperature changes now rather than using highly complex climate models directly which would in some sense be quite intractable we're going to rely on outcomes of pulse experiments applied to these models which give us some nice approximations to help us pin down climate dynamics we can work with and then for the damage functions we're going to follow sort of the ad hoc static damage functions that have been used uh you know in much of the previous literature but importantly we're going to explore consequences related to the changing of their curvature which is going to open the door again to another form of uncertainty that we're going to be thinking about now we're going to use a pretty standard policy measure called the social cost of carbon which is a dollar valuation of an initial ton of carbon in the atmosphere as our barometer for thinking about the consequences of uncertain climate policy now in this setting with uncertainty we're going to depict this as an asset price where we can use asset pricing theory and tool to think about these components the social cash flow that we're going to be interested in is an impulse response from a marginal increase in emissions to a marginal impact on damages that will be induced by future climate change and this cash flow is going to be discounted stochastically in ways that account for a broader perspective and uncertainty again following sort of the tools and frameworks that we have in asset pricing now a common discussion that comes up in environmental economics can often be what rate should we be using uh to discount these future social costs we actually find that this isn't necessarily the right question to think about in our framework because we're going to instead represent broadly based uncertainty adjustments as a change in probability over future outcomes for the macro economy now before i get into the model uh i want to just note that we're going to be using a particularly sort of simple and intentionally simple for pedagogical purposes model that's going to allow us to highlight some of the key components and features about the uncertainty analysis now that's obviously going to shut down the door to some of the key policy elements that might exist economically but at the end i'll touch on some of those so let me just get through this very simple model and some of the key outcomes we have here and then i'll touch a little bit more about how we can expand this and how our framework is well built to explore that going forward now as i mentioned we're going to use uh some pulse experiments from the climate sciences literature to produce our climate dynamics right and and and beyond just building a single model we're going to build sets of models for our uncertainty analysis so two different papers here are our papers by joes and co-authors and joffrey and co-authors who provide carbon dynamics and temperature dynamics variation and uncertainty on the carbon side that's done through responses of atmospheric carbon concentration to emissions pulses going through different earth system models and on the temperature side that's being done using approximations of the dynamics relating the log of atmospheric carbon to future temperatures which builds on some of this uh really nice work by our hennias that's helped us understand climate change for quite some time what this is going to do is going to give us essentially nine different atmospheric carbon responses as well as 16 different temperature dynamic approximations and and so within that we have this combination of about 144 climate models that we can look at to think about uncertainty to help you see what those are going to look like here we have the outcomes of these pulse experiments where we're taking the pulses through the carbon dynamics and putting these through temperature equations now the top figure is going to show the spread and the temperature responses across 100 years for all 144 models where on the y-axis we're showing the degrees celsius temperature change from this 100 gigaton pulse the bottom left is going to just focus on the carbon component so we're going to take the nine carbon components run them through the 16 temperature models and then average across temperature models to just focus on the carbon component and the right side is the counterpart for temperature there's a couple of interesting and important things i want to note here one we actually get this fairly quick and flat trajectory that pans out over 100 years and that's actually going to help inform us about the type of climate dynamics we'll use in our model also importantly is that we have this pretty decent spread in terms of the implications of a common pulse so the red shaded areas on each of these figures is the envelope of all outcomes the red line the blue line and green line are the percentiles and you can see that across all the models the impact from a given pulse can range between one degree and almost three degrees celsius and on the carbon side and temperature side we see importantly related to the types of uncertainty we're thinking about variation where the temperature impact appears to have a larger effect than the carbon model variation would so that's going to motivate us to use a very simple uh approximation an affine approximation for our climate model where we look at changes in temperature or the temperature anomaly above the pre-industrial level as this approximation where we take a climate sensitivity parameter based on these pulse outcomes and it's going to scale cumulative carbon emissions now here looking at those pulse experiments we're going to take an exponentially weighted average of those and we produce a climate sensitivity parameter for each of the 144 models this plot shows the histogram of those outcomes which again highlights the uncertainty and spread that's there the way that we're going to deal with this in our analysis is use this type of a model but we're going to have in this sense 144 models or 144 climate sensitivity parameters and we're going to put prior probability weights across these models and more than that we're going to extend it to thinking about uncertainty in terms of structured uncertainty and misspecification that could exist here now the second key component of our model and the third uncertainty component that we're going to be thinking about relates to these economic damage functions that i mentioned previously here we're showing the three damage functions that we're going to use in our analysis we aren't necessarily tied to these as being the true damage functions that exist but they're going to be very useful for illustrating our purposes of the impact of uncertainty so the y-axis shows a proportional reduction in economic welfare from damages uh the x-axis is the temperature anomaly or the temperature in degrees celsius above the pre-industrial level we have three functions here that are going to be basically equivalent uh up until we reach this solid black vertical line which we're calling a carbon budget that a lot of climate scientists and economists are concerned things might uh become more severe after this point at two degrees above the pre-industrial temperature level now from there we have three climate damage functions we have a low damages which after about five degrees celsius increase leads to about a five percent reduction in an output a high damage function which leads to about a twenty percent reduction in output after a five degree and uh five degree increase in temperature and then we have this extreme damage which just highlights in some sense a smoother carbon budget that potentially there could be very negative outcomes after uh we reach a two degree temperature threshold now i want to talk a little bit about the details of this figure and this will be one of the few equations i actually show you because it's going to introduce one of the important components of our analysis so we're going to use this piecewise log quadratic damage function where temperature anomaly y this function gamma of the temperature anomaly is a log damages given by this expression okay the gamma one and gamma two are common across those three damage functions i showed you previously but there's going to be uncertainty coming in the form of a jump process with m absorbing states and those absorbing states are going to correspond to different values of gamma 3m so after this threshold gamma b or sorry y bar this additional quadratic piece turns on determining the curvature of the damage function now prior to this jump taking place the the decision maker the planner doesn't know which is going to be the the actual damage function and so we're going to place much like we had in our histogram we're going to have a prior probability specified on each of these gamma 3ms and then we're going to have this jump process that's going to occur very close to the threshold and we'll specify an intensity function that's going to be localized right around that y bar that two degrees threshold that we mentioned now i want to highlight this key point the information dynamics here we think are particularly interesting and in contrast to much of the previous research being done including our own where the analysis is either static or the uncertainty is never actually revealed there's this really neat structure in terms of when this is going to be revealed and how the planner responds to that and the uncertainty that's involved with that that's going to be an important component of our of our analysis so again uh i'm going to jump right in now to the uncertainty analysis just as a reminder i'm going to keep this at a high level and and essentially you can think about the damage function and those climate dynamics as as the key components we're thinking about in the model the remainder of the model which is going to be a single capital stock and other important components are going to be largely separable again that's sort of imposed by us to keep this pedagogically simple and informative but we're going to be able to use this same uncertainty structure and analysis on these nice simple social planner problems where we get a simple policy structure to think about the impacts and the quantitative adjustments the same structure can be applied into more rigorous settings as well so in terms of the uncertainty adjustments and the uncertainty analysis we're interested in we're going to try and open the door here to a comprehensive assessment of uncertainty based on broader notions of uncertainty we're going to use recent formalisms from decision theory under uncertainty to think about how this matters and we're going to include multiple types of uncertainty you could think about ambiguity which is going to be uncertainty across models this would be sort of a very structured way of putting probability weights on those climate dynamic models or the damage functions and and thinking about how those might be sort of weighted in different ways if we're concerned about uncertainty as well as potential misspecification either about the damage functions or the climate dynamics and so we're going to use decision theory uh as a way to form on uncertainty quantifications thinking about how those lead to adjustments and the social cost of carbon set by the social planner and that's going to inform us of how much this might matter so let me give you a little bit of the details of how we're going to think about this uncertainty we're going to think about how we're going to examine altered probabilities that are going to be implied by three different cases and we're going to combine some of these misspecification about the local climate dynamics misspecification about the damage functions and then ambiguity over structured uncertainty across those climate dynamics as well now the way we're going to do this is following the previous literature and and opening up some new ways to think about it where we're going to have a minimization problem where the planar actually internalizes this sensitivity analysis makes optimal endogenous choices about how much this uncertainty might matter and that's going to introduce these parameters cb which will be related to the misspecifying of local dynamics xcp which is going to be related to the misspecifying of the damage function next ca which is going to relate to the ambiguity those are going to guide our sensitivity analysis now those are going to be from the decision theoretic perspective preference parameters that govern aversion to uncertainty broadly conceived in conjunction with this endogenous minimization problem that the planner is using and the current states of the world that they're in to determine how much this matters now that's a key part is that the altered probabilities that we're going to be looking at are not intended to be the beliefs of the planner rather they're reflecting how large their concerns about uncertainty are and then helping them adjust their policy uh in conjunction with that let me visually talk you through this uncertainty to show how this might matter so here we have a figure of the value function and this is going to have a temperature anomaly on the y axis we have at the 2 degree threshold we have this dashed black line so on the left hand side of that we have a solid blue line which is the value function of the welfare the continuation value for our social planner here on the right hand side of that line we have a dashed line which is the certainty equivalent for the value function essentially what the planner expects to happen after this jump where the curvature is revealed and then the three colored lines are the damage function conditional on the specific or sorry the value functions condition on a specific damage function now you can see that as temperature anomaly increases the value function decreases the externality impact and damages become larger that makes sense and what the role of uncertainty is going to do is the following the ambiguity or misspecification about climate dynamics is essentially going to shift where these solid lines are showing up it's going to tilt and distort them in certain ways and then this concern about uncertainty related to the damage function is going to alter and distort where the solid blue line and the certainty equivalent after the jump are meeting up so here in the specific case uncertainty for this specific example where we put equal weights on the different damage functions is leading the planner to have this certain equivalent expectation that's going to be below the high damage function results not yet at the extreme damage function result which is again is highlighting these are endogenous determinations that aren't simply pushing all the weight towards worst case outcomes but sort of optimally taking into account what we think matters so given that i'm going to jump into a couple of the key results looking at first the emissions trajectory and then the social cost of carbons under different uh uncertainty penalization configurations okay so these are going to be simulation results we'll go for about 90 years and these are in gigatons of carbon r emissions pathways that top line is what we'll call the baseline setting where there's no concerns about uncertainty here we start with emissions that are on the order of 99 gigatons of carbon and those are decreasing over time because of the increasing impact of climate damages now as we start to turn on components of uncertainty moving from the red line to the yellow line the yellow to the green and the green to the blue as uncertainty becomes more severe we see interesting dynamic effects first an initial decrease in the emissions that can be quite substantial when we get to these larger uncertainty components that can change over time in some cases these difference in emissions will persist and sometimes they will close together and this is just one side of the coin so let me move to that barometer that i mentioned previously which is the social cost of carbon now we saw here i'm showing you the the social cost of carbon trajectories from the simulations and i'm doing them in logs we're scaling these all by the initial sort of marginal utility of consumption so that we take out any of the sort of growth effects and we focus mostly on climate and uncertainty effects and again where we saw the highest emissions that's being reflected by the lowest social cost of carbons that are imposed by the planners policy choices and again as we increase uncertainty we see an amplified social cost of carbon that can be on the order of increasing that social cost of carbon by 30 percent or more now the question is we've introduced some free parameters is this really makes sense are we sort of going beyond the mark are we just throwing in uncertainty to sort of get to the most extreme cases and so the way that we're going to look at that is by looking at how these distorted probabilities are shaping up in these different cases i'm going to focus on distorted probabilities related to this yellow case okay the first set of distorted probabilities i'm going to show are the uncertainty adjusted damage function probabilities now again remember in this case we put equal weights on the extreme high and low damage functions and the planner endogenously distorts these probabilities as a function of temperature so that even at the no initial temperature anomaly we see that they've increased the weight on the extreme damage function to be up to almost 40 percent now and then as temperature increases we see a shift away from probability on the high and low damage and as we draw near to that threshold where the curvature of the damage function is going to be revealed we see an increase up to about almost 60 percent now we think this is informative one because it shows that there are important dynamic effects but two that these probabilities we don't view these as being overly ambitious or we're not loading full probabilities on the very worst case outcomes if you look at the counterpart for the the uncertainty adjusted climate model probabilities again this is that original red histogram i showed you on the climate sensitivities that's the red histogram here and the blue shows the distorted histogram based on adjusted probabilities for uncertainty again we see a shift from the lower outcomes and the left tail to the more severe outcomes in the right tail but again where we saw these reasonably significant impacts on the social cost of carbon we don't think that the uncertainty adjustments here are too large we think these are fairly reasonable and informative adjustments to think about our analysis so i've got about one or two more minutes i think so let me uh let me discuss briefly two key components the first one is going to be uh what we think is a pretty novel and interesting contribution uh is an uncertainty decomposition so basically because we're using uh in some cases misspecification in some cases the smooth ambiguity approach that's more structured we're going to exploit that which allows us to open the hood on this uncertainty we're going to do that by doing a decomposition of the three uncertainty components in our analysis the carbon dynamics temperature dynamics and economic damage functions the way that we do that is by asking how an ambiguity averse planner would adjust their policy when they're constrained to just look at specific components of the uncertainty and so we're going to look at first each individual component then we'll look at pairwise components of uncertainty and for comparability we're going to hold fix the emissions trajectory and they're going to be held fixed at the the social planners optimal emissions when they're considering all the uncertainty components which is important it could be very different for different trajectories michael one minute morning okay cool so looking at these individual components you can see that there are very different impacts across the different components so carbon and temperature have both and here i'm showing you for the simulation trajectories the log differences so this is essentially the contribution to the social cost of carbon and a percentage coming from uncertainty okay so for the total the red line where all the components are included this is anywhere from 20 to 40 percent of the social cost of carbon now carbon and temperature are flat and lower and that's because of the emissions trajectory here and we see that damage uh leads to a large impact that's dynamically increasing when we move to the pairwise contributions in this uncertainty decomposition we see importantly that it's the interactions of damage with temperature damage with carbon that really matter and we have these nice multiplicative effects that we can identify because we can decompose this uncertainty decomposition so in the last 30 seconds i have i just want to highlight again this point that we are working and have computations and projects progress on some richer economic settings these again our setting is focused on this sort of nice pedagogical setting but our our framework is very nicely set up to explore additional policy settings more complex economic settings and in particular we're looking at two capital models with some key coupled dynamics one of them is a coal versus oil production technology where we have two fossil fuels that are both attractive for production but one of them produces more emissions which allows us to then open the hood to thinking about uncertainty and policy pertaining to differential energy inputs and how to deal with those another is looking at clean versus dirty where we have a dirty technology that's more productive but a planner can invest in r d so we can think about uncertainty about how that r d to improve the green sector might matter how uncertainty about the policy and dynamics would interact with that and then finally looking at the case of climate vulnerable versus climate resilient technologies where again climate vulnerable faces additional damages which opens additional uncertainty and we can think about policy for monitoring and limiting this i'll just give here's my conclusions all in there all right good thank you very very much um for a very clear presentation our first discussant is mara gwandt are you on mute mark no yeah it was all fired up already going so uh yeah so thank you so much for the opportunity so as you may know i'm not a microeconomist i'm an industrial organization economist working on energy and environmental topics i usually work modeling very specifically given industries so the electricity sector the cement industry or summary industry recently i've been working on a commission commissioned by olivier blenchard and gentile and together with christian laurier thinking about the challenges of climate change advising uh president macron and that has given me a bit of a more micro perspective and honestly it's quite it's quite scary and frightening as you go into the micro perspective for someone who has been focused mostly on how to make cap and treat work in the electricity sector or in manufacturing so in today's presentation i will honestly be quite critical of the work uh i'm a pretty reasonable person so i don't want you to take this as confrontational for the sake of it it i will be quite critical with the work because i deeply care about such a grave matter and i think you might not be getting quite the right answer so i'll offer my comments and i hope that will help revise the revised model so i think michael was very clear in presenting the paper this paper is about how to do predictions from a macroeconomic model that take into account any source of uncertainty i'm not an expert in continuous time macroeconomic modelling but it seems to me that the approach has many interesting features that get combined such as model music specifications brown exams and all these different ingredients that michael was explaining so an insight that i thought it was an interesting again not being familiar with the literature is that they present these representations of uncertainty and probabilities that take into account the ambiguity version and how other missed specifications factor into the model but in some ways for someone who's not familiar with how the under the hood works i can actually look at these probabilities and make an assessment of what they are thinking about so as you can see here let me let me put um here on the left i have the climate sensitivity and they are telling me the red distribution is the true distribution someone who's more ambiguity verse will uh respond more as if the blue distribution were the true one understanding that that it's not similarly here we have these probabilities they should be weighted one third each but someone who's more unbelievable so someone who has uh who takes into account all these different process that they put into the continuous time model should actually end up placing more weight on the extreme uh damages so the bottom line and this is where i will be most critical i think honestly the mathematical model seems interesting but when we get to the quantitative quantitative implications of the model uh the paper will claim that adding these effects makes the planner more conservative but i guess what i will argue in my discussion is that the planner is not being conservative at all in in their in their implications so a brief summary of the model i will i will go over different details that michael emphasized so the model is very simple i i know michael called it pedagogical but again if we care about the quantitative answer we cannot just stop at the technological model it's an ak model with emissions as cop douglas that incorporates economic damages in an interesting way actually economic damages are introduced via n so think about consumption and investment equal to alpha k and then the utility uh being this cordage function between effective consumption and emissions and here the key is that effective consumption is deteriorated as emissions accumulate into the atmosphere be at the increase in temperatures so the more emissions are accumulated into the atmosphere nt keeps growing and effective consumption holding the the the uncorrected consumption constant effective consumption uh keeps the cleaning so the meat in the model is how to model the uncertainty surrounding these relationships that put the economic damages into play so how emissions will turn into temperature increases and how then this temperature increases will hinder our ability to thrive and to inform those models and or these objects these stochastic objects they will use a scientific evidence as well as some of the shelf economic scenarios as michael was explaining so um these are the two key ingredients one is the climate sensitivity one that i already showed you um and the other one is these three cases the north house case and intermediate case and extreme case so i plotted here over a bit of a longer range so that you can see what these scenarios are telling us the extreme case is telling us that at six degrees celsius we will all die the intermediate case is kind of a bit in between but it significantly reduces welfare holding everything else constant this is the m i guess or the effective one over n kind of reduction and um and the north house one you can see that you know we can heat the planet by 10 degrees celsius and only suffer 20 percent uh in in in in welfare so this is honestly does not look very realistic if you are now on top gonna at r d so this north house case is extremely optimistic and it can only be explained with already r d already making it so i would be quite cautious in how you think about r d given that these are the scenarios that you're considering because these are scenarios in some sense are already heavily relying on significant r d i would say so this is just a small caveat so let me now substantiate my claimed criticisms that so far i haven't made explicit so our first criticism is just on the pure calibration of the model so the model starts concentrations at 390 ppm this is something you probably have seen on the news actually this week we've seen it on the news a lot because we have discovered or confirmed that we are now already at 415. so for those of you who do not study climate change um these numbers might not mean much but going over 400 is already something that had been frightening scientists for a really long time and 415 it's already a lot of the cake that's built into the model that we are dealing with here i want to show you the exponential perspective now that we are used with exponential trends so this model starts here and it's not too far off it's 2010 but in 10 years you can see how parts per million have have kept increasing at an extremely rapid rate so even just these 10 years are assuming basically we are giving ourselves 10 years that we just do not have and this is easy to fix so that's at least an easy an easy one that you can handle so the second criticism on how the scientific facts are incorporated in the model and to be fair this might be my lack of understanding of how the stochasticity is modeled in the paper so it's very unclear to me from the writing that i found a bit dense how the noise in the scientific evidence is incorporated into the brownian motion so if i understand correctly the thought experiment behind this scientific evidence so this is the scientific evidence um as as humans we are drawing one drop from these scientific evidence there are 144 models one of them is right we just don't know which one it is and this is the noise that we get from these different 144 models so my worry is that this is not what the offers are modelling my worry is that the authors are modeling something that looks more like we are drawing from this distribution repeatedly if we are not drawing from this distribution repeatedly we need to incorporate learning into the behavior of the planner but at least from my reading it seemed like the probabilities of these different 144 models were not indexed by t so if the planner is not learning we are ignoring optimal behavior in tail events so we are ignoring the fact that we might be here repeatedly um indeed i feel like from the mall again and this might be my lack of understanding it's almost as if the planner we're behaving as as basically responding to the mean drift of this distribution so in spite of having this distribution of uncertainty because either we assume that we keep drawing from it or the social planner behaves as if we keep drawing from it i feel it's only responding again almost or kind of to the the mean drift and ignoring distal behavior that in the literature has been so readily emphasized so let me now turn into the economic specification of the model so i understand this is a simple model but it just cannot be a model for us to make recommendations on how to deal with climate change so in this model emissions are appear in the cop douglas with an index of this is eight in their paper of 3.2 percent so it implies we can cut our emissions by half instantly by just reducing our utility by 2.2 percent it also implies that we can cut our emissions by 99 by just reducing our utility instantly by 13 and it implies that we can cut our emissions by 99.9 we can virtually decarbonize our economy immediately by only spending 20 percent of our utility honestly this sounds great and i wish we did live in that world but we do not live in this world so i am a bit concerned about the predictions and the model because in this model it's almost as if there is no climate change problem so we can see the issue of the dynamics here we can see that at zero depending on the ambiguity of version of the planner we can decide whether to be at eight or we can decide whether to be at five again the presence of sunk capital in the fossil fuel industry both from a practical perspective but definitely from a political economy perspective creates huge challenges to making the needed progress and this some capital will not go away i am encouraged that you are working on extensions but honestly i wouldn't i would suggest that you don't spend any time on your first extension we don't live in a world where we can afford to move from gold to go to the oil industry it's not the world we live in so finally on the policy uncertainty in the model this is not covered but i also think it can lead to misleading recommendations so in this model a social planner can choose how we can reduce emissions in the planet and again i wish this were true but it's not just what we are facing there's a huge tragedy of the commons every year we say next year we should reduce our emissions and as you could see from the graph i showed you we are on an exponential growth in emissions so this seems extremely relevant even within the pathological framework to incorporate into the model you should have some latent emissions that the planner cannot control and that can go really wrong in the mall here i wanted to put a bit of a kind of side note that got that got me thinking lately so as economists we tend to dismiss um these kind of cliff ideas you know 2 30 celsius something we really need to avoid zero carbon by year 2050 no injection combustion engines buyer whatever and yes as an economist i agree that having this kind of very fixed target might seem wrong but we have to recognize that it's one of the few things that has gotten some international representation reciprocity whether countries pretend to stick to these commitments it's very unclear but at least it's guarding them to speak up and promise things that probably won't be delivered so in spite of all this optimism that built into the model this is a doomsday model of climate change so in this model there is no room to fly climate change other than substitution and growth so these are all about how we took crumbles of what's left where we are overstating quite a bit the crumbles that are left so the discount factor in some sentence matters a lot because it's a pure cake eating problem without the emissions in the model welfare goes to zero as temperature keeps rising until we all die so this is an avoidable fade in the model is only disguised by the fact that there are some unrealistic economic assumptions and the last point i wanted to make is that there are also possibly impossible carbon budgets this is related to my first point that i told you that the moderator starting in 2010 but we have already burned 10 of those years it could also be related to the model of uncertainty that i mentioned on the climate sensitivities bottom line however the model claims that we cannot stick below two degrees celsius which agrees with what scientists are recommending left and right by only cutting our emissions in half so these are some simulations that are in a grade website on hassle for deportation our working data and they show what different climate models are telling us our carbon budget is left so the carbon budget is the area below our emissions and what these models are telling us the scientific models from which we supposedly derived these emissions but it's really this area under here so i am worried again that there is a mismatch between the scientific evidence that that's used and and the given state of the art of the scientific evidence so um again maybe the four the four or fifteen parts per million will fix some of these recommendations but in the model they will still look whatever alternative you drive from the model it will look extremely cheap it seems to me so i wanted to finish uh by just saying that climate change is the biggest threat that humanity has faced and if you look at the numbers long enough and that might be two minutes you will convince yourself that that's the case so our duty is if we want to inform the debate is to take the scientific and engineering facts accurately so i like this paper because it does try to put the scientific facts carefully in the model but i feel it's really not accurate enough when it comes to the quantitative conclusion it also means that the state of the art in as of now our inability to evade carbon although maybe some extensions with r d will help in this criticism so my suggestions for an improved paper is to really try to reconcile the results with the current scientific evidence and to try to consider the role of emissions intensive capital which it sounds like we are already doing it was not in the version of the paper that i reviewed so as a final kind of a slide i have how can economists help and i know number one will go very badly in a room full of of minor economists but in my opinion i think we as microeconomist as economists we should really stop quibbling about what the optimal path of emission production should be i complained about your path of having emissions in um having emissions like reducing emissions by half that said i would be glad if we could achieve a reduction of admission didn't have i really would i feel no matter what recommendations we make we will fall quite short so i think we should in some ways try to stop arguing about what the optimal path will be because we might well be at a corner solution no matter what optimal path we drive we will probably fall short unfortunately so i think we need to spend a lot of time trying to think okay how do how do we get closer to even the most optimistic optimal path that we can derive by thinking creatively on how to decarbonize our economies and how to phase out fossil fuels again natural gas or oil is really not the solution here finally and unfortunately i think we need to creatively think of how to preserve lives in the presence of already happening a very extreme adverse events and only bound to to increase so thank you so much uh for giving me the opportunity uh to discuss on the paper and i look forward to the discussion thank you very much mark for a thought provoking discussion our next discussion will be pair kucel you see this okay yeah just great uh i mean you could put full view on it yeah i'm going to yep i just had to unmute so it took a second yeah thanks uh um it's been uh very interesting to read and think about this work and uh kind of maybe luckily i have uh i have a bunch of comments uh but they don't overlap so much with mars discussion which i found super interesting um i want to think about more so so you know you get a different set of comments here for me so i guess the the issues are these big ones uh significant climate change large uncertainty extreme damages thresholds very scary stuff and i i agree with mars assessment uh and clearly a reason for precaution in policy making so that's kind of what motivates this approach um it's like a structured way to do to do policy uh under this uncertainty and but the focus is methodological somehow uh i i viewed this as maybe proof of concept or and this is also reflected in mars comments but i'm not gonna develop them so much but basically the authors use developments in dynamic this decision theory to analyze ambiguity and model misspecification um and then show how these features guide social planner decisions and let me just emphasize this kind of they look at really planning problems uh with uncertainty it's not decentralized models where you can identify households and firms it it's really a planner running the show and um but taking into account benevolent planner taking into account uh that we don't know uh a lot of things so basically they say in the in the abstract uh their approach reduces the many facets of uncertainty into low dimensional characterization that depends on the uncertainty a version of a decision maker of fictitious social planning and and i think the focus is more on the stochastics in each model and across models than on the economics and so in that sense kind of this is in line with what mars said but um so uncertainty in climate science now i've reproduced the graphs here and you have to remember what's on the axis but so the first paper uh was by jose al 2013 which is the carbon cycle um set of experiments and you see years here up to 100 years and the temperature change from zero to you know two and a half and then back down a little bit the idea here is in these papers they uh in this paper they they emit all at once in the beginning of time 100 gigatonne of carbon which is like 10 times what we emit in a year roughly so it's like a decade boom out there and then you let it sit and you see that the response of temperature is this it kind of interestingly within within 20 years or within 10 years the temperature is kind of stabilized in in this ensemble of path nine nine models okay so temperature dynamics are pretty fast i mean climate change is a long run thing uh but you see temperature dynamics are are relatively uh quick the climate system taken by itself so the the running various in this case 16 models by joffrey and al did that and you see bigger ensemble so so this is what climate scientists and carbon cyclists do they they they have a bunch of models mars said one is the right one this is what economists tend to think one is the right one um it's kind of the way we think uh they don't exactly think that way there's just a bunch of models out there even the same model they simulate for different initial seeds and you get different outcomes why because they're chaotic systems and so on climate climate models are very complex but anyway they they give you a bunch of output and if you put them all together 144 models um i did some factorization here because i can't do 9 times 16 but i'm convinced that it is 144. now so i i did check the reference i talked to my local um go to natural scientist here um and he knew these papers very well he said this is very solid work so i mean here um i'm not the expert but the expert i talked to said that this this is this is very kind of updated um and good stuff so we can take this to be but of course remember what is kind of interesting is this is just one pulse of this amount so for example do we see tipping points in the climate system do we see tipping points in the carbon cycle well in a way we can't really tell from this set of grass why because well there's always this one pulse i mean suppose you did twice the pulse would would the system then kind of just explode uh it's not clear so i think these experiments don't speak to the non-linearities that are potentially in the the tipping points that might be in the carbon cycle and the climate system um what i should add to that is that what they then do actually is not use these models they they summarize the results in these models and map them into that temperature graph that you saw or our climate sensitivity graph the bar chart which is really a recent result in climate science which is that you can approximate the temperature um outcome just by saying it's proportional to the cumulative past emissions like a remarkable uh stylized simplified model the temperature increase over pre-industrial uh is proportional to um the cumulative past emissions and you don't have to think about the carbon cycle just how much you emit it uh this is a relatively recent result by matthews um now of course we don't stop there because that proportionality uh coefficient it's there in all the models it's there in all the simulations but then the models vary so we don't know what the so there's a variety of possible values for that proportionality constant but actually the mathy results somehow directly says that the summary of the carbon cycle and climate system stuff is that they they don't see big tipping points at least not kind of visibly in in that line of work it's really remarkable that it's so linear and and this was a bit of a shock to me but i don't know that's the models they're on um then there's the the other element that michael described well um the damages and i this source of these as you said it's kind of like a working example i mean i would say it's essentially made up i mean it that's not meant to be criticism it's just would be nice to refer to studies but here is where empirically oriented economists must kind of gather more and more evidence to help us see what's going on uh i mean issues are you know what are the sources of the non-linearity in damages if you think about it damages what's the non-linearity a non-linearity may be that you flood bangladesh uh so that that i mean but it's when you start thinking about non-linearities in damages it's not obvious to me i mean it is that when you pass a certain threshold in temperature humans can't survive and so on there are a few of those so i just think it would be nice to relate to them when you model this um and people will ask what about adaptation so why can't people move from the areas that are so warm and so on i mean you just bound to get these questions then it would be nice if there would be some discussion because i do think like mars said you need to go more quantitative especially in the economic sport um and this is where i mean this is the economics so in a way i think the climate science stuff here is very solid in a way but the economics is a bit too made up um but it works well for um for illustration of course uh so here's my way of describing the model is a bit different so i'm going to describe a static aesthetic model which i think this can be viewed as innovative in the word houses model kind of looks like this think of preferences as log c okay you consume and then production comes from a cop douglas production function in capital labor and e is energy let's say it's fossil energy and then you have to subtract some [Music] resource costs here because it costs something to produce the e then the orange thing is an externality of using e a high e max d lower why through the carbon cycle and the climate system here's the externality and here's how you can just do a simple static model and derive the optimal to go tax i mean we can define a competitive equilibrium and without taxes with taxes and work with this so that's kind of um the first model this paper although as mark said it's a it's an aka style dynamic continuous time model with brown emotions and so on um think of it in the static version i think it looks like this because it looks the same so it's true that e shows up in utility in their framework but i can you know take log c and uh you know you could think that this e here is just in utility instead um so i mean it benefits you you in utility so i would say this is again kind of a case of pigou but it's a little bit i i maybe this is just a question to the authors i didn't understand why there was no gov so if like if you do let's say fair no tax wouldn't eb infinity i mean i didn't see the exact carbon budget that mar talked about i mean carbon budget can mean different things is there like a maximum what's the cost of extracting more e i mean maybe it was there and i just didn't see it the gov here so of course their planner will never want to uh use an infinite amount of e that's what would happen in equilibrium if there is no cost of using it but anyway so i would say this is kind of you this model is it's macro style and you can think of reasonable values for all the parameters new here mar was talking about 0.03 being low new here is in the microstyle model is the it's the share of energy in costs in gross output and that's like five to ten percent depending on so it's not three but it's not super high either now and this is a static model but i think of it as a model for like 50 year period or 20-year period period at least but even 100 and then douglas is not crazy in the very short run i think our economist would economies would stop pretty much if you just reduce e by uh ninety percent i mean it wouldn't share the cop douglas feature that you see in front of you uh in the very short run i think you think leontyaf instead so the lastest substitution it really vary with time and you can substitute but only if you have you're given uh enough time but anyway for static mode i think this is reasonable okay so i think this is nice because you can think of adding an example stage and make a policy choice example and then add your uncertainties in the various places uh and it's an entirely i mean much simpler model i think just would be nice to think about it that way uh and it would allow you to connect more with the quantitative studies um the only thing that's missing here is discounting which of course is important but you can capture everything in a simple static model okay but the economics of the static model uh this is something that we looked at recently in i mean we looked at it in in a paper that the authors kindly cite which is related as you'll see in a second but we revisited the issue very recently uh just to try to understand the results a bit better so we really took the simple nordos model that you saw here um and then we asked the following question we we thought about modern miss specification too so we thought of two errors you could make error one what if we choose a carbon tax based on thinking that d or v involves uh huge externalities it's really really bad and then we quantify this by precisely looking at those curves that the authors look at by taking the highest curve the most warming you could ever get plus taking the most uh drastic empirical estimates of damages so suppose you base your policy on that um but actually and you choose that in the example period but actually uh the externality is close to zero and climate change is no big deal okay so we computed that error i mean as percent of consumption and then we did the reverse error two what if we choose not to tax carbon um because we think e barely affects d but in fact it does in this extreme way again with reference to um to the extreme damage functions and so on and then we found that the first error is actually not a big deal it's not a big deal to tax carbon at the relatively high rate the rate you need based on thinking that climate change is really dangerous um you will of course it will be a drag but it will not be a huge drag on welfare in this kind of long run perspective whereas the second error of not taxing when really you should is actually huge so there's a built-in asymmetry and in our you know current work we we are trying to understand this and basically if you go from a zero tax to a very small tax when you should tax then like every unit of tax is super helpful in combating climate change in in reducing the externality because externality is big so it really pays off to raise the tax from zero to whatever small number whereas the opposite going from a zero tax to positive tax um isn't very costly if climate change is if climate change isn't a threat then zero tax is the right thing to do but a small departure from that is just like the the objective function is pretty flat around zero so using a positive tax even a relatively big one isn't very costly for utility so you can actually understand these results based on these asymmetries based on the simple model and and this is how it plays out kind of if we do the dynamic model over time where the black is the consumption loss of making the error of thinking climate change is not a big deal whereas the red one is the error of making of thinking acting as if climate change is a big deal but it turns out not to be so for us this type of study indicates that you know i if i don't know which error i'm going to end up making and i'll be i'll do precautionary and do the red one um and and so on so if you i com to now my perspective compare this to the present paper um so what i'm not sure in the paper is to what extent the argument the authors have for precautionary behavior which is kind of coming through ambiguity inversion and distorting the probability somehow to act as if you think it's really bad um whether the results they get or what part of the results come from this built-in asym symmetry that we have documented and what comes so what just comes from that and what comes from the ambiguity aversion that they assume um so i think it would just be good if they explain that because it would help me think about these models with ambiguity version more more generally kind of i would i would like to anchor things a bit and so i'll be done momentarily um so i would say as a way of describing what macro people again i am a macro person unlike my working in this area so here's like what i think micro people working this area have done they have thought of ways to make the optimal carbon tax become a big number i mean that's just i think a description of what they've done and i think they've done that because i mean they want they're concerned and some calculations like nord houses gives it optimal tax on carbon that's not very big and it's like well is that wrong and what what's wrong with the model so we know now that if we use a discount factor as very high low discount rate you can get the numbers to go up if if you assume risk and you assume high risk aversion or big tail uh fat tails you can get the number to go up you assume a high curvature on damages you get the number to go up you assume that higher temperature lowers the growth rate instead of the level effect you get the number to go up if you assume a major tipping point in the climate system or in damages um you get the number to go up but so for me here i mean all of these hypotheses is what they are they need to be evaluated kind of quantitatively and you like to relate to something so on this list is also ambiguity version or there is uncertainty and we are kind of we're worried about it because we don't quite know what what the right probabilities are what the right model is um so i would like just like i would like like to know some of these issues are just ethical like discounting may just be an ethical issue but some of the other ones are just we should just be able to learn more and find out so what should i base my calibration of ambiguity version on is my question okay wait one more warning yes yes um so kind of what's the guidance on that would be useful final thoughts is that i fully agree i mean reflected in my own work that this this uncertainty super important should lead us to be very cautious cautious should drastically lower emissions um and my my worry a little bit is that we as economists can contribute productively in this area more with like economic analysis explaining custom benefits figuring out how to do things in a smart way explaining custom benefits of different policy actions using our understanding of markets because that's kind of that's what we are good at i'm less sure that kind of formalization of uncertainty is the way forward but that's maybe more like on the policy side i fully agree that it makes sense to think about it the way the authors think about it but i'm kind of worried that people won't listen to us because we came up with this great way of thinking about uncertainty rather if we can explain this is how we should use markets to to move forward then they might listen or not but okay thanks sorry 