Victor Chernozhukov: Let
me begin with Part 2. I'll first talk about a
fairly simple problem which is going to
be about estimation and inference on the
structural parameters in an instrumental variable
model with many instruments. We will use Lasso to
select the instruments. Then we're going to
perform inference on the main coefficient
of interest. In my presentation, I will
focus on a simple IV model, where we have y. This is our outcome variable. G_i, this is the endogenous
variable of interest. It's going to be a scalar. The alpha is the
parameter of interest that we care about. This is the disturbance. Endogenous variable d is
related to instruments, g_i, via unspecified function g. This is the disturbance
in the first stage. Well, with this assumption that the v_i conditional on the
instrument has mean zero. This g is a regression function. It's a regression function. It's the conditional
expectation of g_i given z_i. This regression function
in this context could be called the
optimal instrument. So we have the endogeneity
in this model, so if we stack these
two errors together, they have mean zero
conditional on instruments, and they are correlated, which is the cause
for endogeneity. We could also have
additional controls, w_i, entering both equations. But to simplify
the presentation, I just suppress those controls, and I assume that we've
partialed those out because I want to focus
on this simple structure. So we just assume those controls have been partialed out. Also, we could have multiple
endogenous variables, but I will just
present the results for single endogenous variables, and I will refer
you to the papers for details and generalizations. So the main targets
parameter is Alpha, and here g is the unspecified
regression function, which is actually a
nuisance function. We will use high
dimensional sparse modeling to approximate this g function and use Lasso to estimate
this function g. So we are in a setup where we have either
many instruments, so I'll call this excise the ultimate instruments
that we will use. Z_i could be equal to
the original instruments that we have available. Or the final instruments could also be generated
as transformations of the original
instruments by taking polynomial transformations
and interactions and various other operations. Obviously, we could have a
mix of the two cases as well. So the number of instruments, x, here is going to be
large and again, it's going to be possibly much larger than the sample size. We will use Lasso for that. We will assume
approximate sparsity, namely this regression
function g, or the optimal
instrument could be decomposed in two parts. One part is going to be
sparse approximation and the other part is
approximation error. The sparse
approximation will have s non-zero coefficients in
it whereas it's going to be much smaller than
the sample size. I will define what this
means in a moment. The approximation error
will be a small conjecture to the size of the
estimation error for this regression function. So what this assumption
is saying is that, well, the optimal
instrument is approximated by s unknown instruments. S itself is unknown, but s is small compared to n. So we can call these s instruments the
effective instruments and what we're going to do, we're going to search
for those instruments using Lasso and we will form the estimate of the optimal
instrument via Post-Lasso. We run Lasso, find
the instruments, and we run least squares to find the optimal instrument
approximately. So this gives us a
single instrument. Then what we do, we go ahead and compute the two-stage
least squares estimates of Alpha using this single
constructed instrument. To motivate this setup, let me present the
following familiar example. This is an
Angrist-Krueger example. In this example, y is wage, d is in education, it's treated as endogenous. The target parameter of interest is returns to schooling. Z_i, the basic instruments include the quarter
of birth dummies. There are three of them,
as well as controls. There are 50 state
of birth dummies, and there are also
seven-year-old birth dummies. The final set of technical
instruments is going to be a set of transformations, so these basic instruments. It's going to include z_i and also all possible
interactions. It's actually not
very well-known, but if you take all the
possible interactions, you end up with
1,530 instruments. Most of the time, people don't use all
those instruments, but you do have
1,530 instruments. It's a very large
set of instruments. Here we have two basic options. I mean, obviously,
we have many options but there are at least
two basic options. One is to just use
a few instruments, like three-quarter of
birth instruments, the obvious ones. Or we could use all
the instruments. That would be another option. But the result that either
one of those options gives big standard errors when you properly
account for the fact that you're using
many instruments. So it seems a good idea to do instrument selection to
see if we can improve. Here I show the table
where we have three rows. This is two-stage least squares with three basic instruments, quarter of birth dummies. The schooling coefficient
estimate is 10 percent. The robust standard error
is 0.02 or two percent. The next basic option
is the IV estimator where we use all
the instruments. Here I use actually
Fuller's form of two-stage least squares because it's robust
under many instruments. Christian, Whitney,
and Jerry Hausman, they have a nice paper about the Fuller's form of
two-stage least squares. It turns out that this
estimator is the right form of two-stage least
squares if you want to work with many instruments. I'm using a state-of-the-art
estimator here and the estimate is
10 percent as well, but the standard error
is big, four percent. This is a robust standard error. It accounts for the fact that there are many instruments. The next line is going to be the two-stage least
squares estimator with Lasso selected instruments. So here we end up
selecting 12 instruments, including the basic instruments, the three-quarter
of birth dummies, and some technical instruments
including interactions of those quarter
of birth dummies for some of the controls. The point estimate is the same. In the third digit, they differ actually, but the
first two digit coincides. The standard error is small compared to
either one of those. I would say this is a
nice set of results. We get very precise estimates of the schooling coefficients. What I will do next, I will justify this
estimator here. I will say that under the approximate
sparsity assumptions, this is a good way
to do IV estimation. It's good to use Lasso to
select the instruments, and I will justify robust
standard errors here. These results are going
to be fully theoretically justified in what follows. To give a formal statement, I will define the
two-stage least squares estimator formally. We have two steps in the
estimation procedure. In Step 1, we estimate the
optimal instrument by using the
Post-Lasso estimator. We run Lasso, select
the instruments, then we predict the
optimal instrument using the least
squares estimator. That gives us g hat of z_i
equals to x_i Beta hat, the estimate of the
optimal instrument. Then Step 2, we'll compute the two-stage
least squares using the estimated optimal
instrument as the IV. What we have here is the standard expression
for the IV estimator. It was a single
endogenous variable and a single optimum instrument. This is cross product
between g and y. This is inverse of the
cross-product between g and g, the optimal instrument, which is the
standard expression. Here's the result. Again, under practical
regularity conditions that include the conditions
that I've listed before as well as the
following conditions, we have this result. Let me describe the
conditions that we had here. The optimal instrument needs
to be sufficiently sparse. Here we're making the assumption that s squared is
small compared to n. Before, we were
making the assumption that s was small compared to n and here we'll make a
stronger assumption that s squared is
small compared to n. The second assumption we make is that the optimal
instrument is strong. Namely, the correlation between
the endogenous variable and the optimal instrument
is bounded away from zero. That's our second assumption. Under these assumptions, we have that the IV
estimator, Alpha hat, minus the estimate times the standard scaling factor approaches a normal
distribution with mean, zero, and variance, one. Here, the scaling
factor involves the standard White's
robust formula for the two-stage least
squares estimator, so there's nothing
special there. Moreover, the this estimator is semi-parametrically efficient
under homoscedasticity. It means under the
same assumptions, you cannot find a
better estimator such that this estimator is asymptotically
normal like this and has asymptotically a
smaller standard error. You cannot do systematically
better than this estimator under this approximate
sparsity assumptions. This result was
established in the paper by Belloni, Daniel
Chen, Chris Hansen, and myself in the
econometric paper. There you can find the
general statement, detailed listing of
regularity conditions, handling of immediate controls, and things like this. Also, if you don't like some of these assumptions
that I stated here, so for example, the strong
instrument assumption, we do have a weak
instrument robust procedure in the paper available, the so-called subscore test. Also, this assumption could
be realized by jackknifing or by using sample
splitting techniques. This assumption could
also be relaxed, but this assumption
is what we need for this basic method to work. I guess the key
point here is that the selection
mistakes that we make in selecting the
instruments here, they are asymptotically
negligible. Somehow, they don't show up
in the asymptotics here, first-order asymptotics. They become more and more negligible as the sample
size becomes bigger. This occurs because the
estimating equations that the two-stage least squares uses has so-called
low bias property. I will discuss this
property later, but right now I just want to say the intuition for this
result is as follows. What Lasso does here, it finds some obviously
good instruments with large coefficients and it's going to be making
mistakes on instruments that have small predictive power for the treatment variable, for the endogenous variable. Whether or not we drop
those instruments, it actually does not affect the identification of
Alpha, the mean parameter. Intuitively, dropping
those instruments that have small predictive power for the treatment
variable is okay. It doesn't harm the
first-order asymptotics. This is actually a
nice example where the post-election inference
works, doesn't break down. This is actually very special. It will not be true
more generally. If you're doing
post-election inference, be very careful. Here, it's fine and it occurs because this problem
is somewhat special and I will explain later towards the end of my talk
what's so special about it and along with other explanations
that I will provide. Let's see if this theoretical result
works in Monte Carlo. It's a good sanity check. We've done an extensive set of computational experiments
and I will give you just two typical examples that are calibrated somewhat
to the empirical example that we analyzed in this
paper that I mentioned here. In that example,
everything is Gaussian and the endogenous
variable is related to the 100 instruments
via this equation. These are instruments, these are coefficients
on those instruments, and each coefficient is Mu^j, where Mu is less than
one in absolute value. The coefficients here
decay very quickly. This is an example of an
approximately sparse model where most of the information is contained in just
a few instruments with largest coefficients. Let's first look at the case where p is equal to a 100
and n is equal to 250. We regulate this parameter Mu so that the typical value for the first stage f
statistic is about 40. This is done to calibrate this experiment to
the empirical example that we are analyzing
in the paper. We have two estimators here. Our estimator, the
two-stage least squares, was Lasso selected instruments as well as the Fuller's form
of two-stage least squares that uses all the instruments. Here, the Fuller's form of
two-stage least squares is used because this is the form
of two-stage least squares estimator that is consistent
under many instruments and it's a defacto state
of the art method. We want a tough competitor
for our method. We don't want some
easy to beat strawman. We want a tough competitor and so this is a good benchmark. We'll look at two
metrics for performance, one is root-mean-squared
error and another one is five percent or
rejection frequency for the nominal
five percent test of testing the true
null hypothesis. A nominal five percent
test should reject, ideally, the true hypothesis only with the nominal
frequency, like five percent. Ideally, we should see
five percent here. Let's first look at the
root-mean-squared error. The Fuller gives us a
root-mean-squared error of 0.13. Our new estimator gives the root-mean-squared
error of 0.08, so it does visibly better
than the Fuller's estimator. In terms of rejection frequency, Fuller does slightly better. It's only half percent off and our estimator
is one percent off, but still, it does
a very good job at controlling the
rejection probability. All in all, it seems
like a good performance. Next, we change the
set up a little bit. We drop the number
of observations, we decrease the number of
observations from 250 to 100 and this causes the Fuller
estimator to break down. Essentially, it starts to have a very large root-mean-squared
error, which is 505. Our estimator, which is a two-stage
least squares with the Lasso selected IVs still does very well,
0.13 root-mean-squared error. This is just as
well as Fuller was doing with 250 observations, so 0.13 here, 0.13 here. Here, the advantage
now becomes very big. In terms of rejection
probability, our estimator still delivers
a rejection probability that is very close to the
ideal level of five percent. We have six percent
instead of five percent, but it's pretty close. We find the performance of this new method to
be quite reassuring. Hopefully, it gives a
nice illustration of how modern high-dimensional
methods could be used in relevant empirical
problems like this. This is that. Next, I'm going to
change gears here and I will talk about
a different problem. It's going to be a
problem of performing estimation and inference on treatment effects in a
partially linear framework. Equivalently, this is a problem where we perform
inference in a regression where we're interested in a particular
regression coefficient and we don't care
maybe necessarily about the entire
regression function. We will be interested in
the particular coefficients such as Alpha here that you see and we will not
necessarily care about all the other regression
coefficients. This is an equivalent
regression language, but I will mostly use the treatment effect
language in what I do. Let me start out by
an empirical example and then I will present some
Monte Carlo experiments as well as theoretical
results afterwards. In this example, we will look at the cross-country
growth regressions. There, we're interested
in analyzing the relation between the realized
growth rates and the initial level of per
capita GDP across countries. GDP stands for generalized
domestic product. We would like to estimate this relationship
conditional covariates that describe access to various market institutions
and technological factors. Specifically, we're
interested in estimating this regression function
here, or this relation. Here we have realized
growth rate. This is our y. This is the intercept. This is the main regressor
of interest, log of GDP. This is log of the
initial level of per capita GDP across countries. Alpha here is the main regression
coefficient of interest or using the treatment
effect terminology, it will be called the
average treatment effect. These are the controls
that appear here in an additive fashion and
this is the disturbance. I here stands for the
observational unit, so which is country here. The assumption on this
disturbance is that the conditional expectation
of the disturbance given the main regressor and other regressors
is equal to 0. We are interested
in this coefficient here for a number of reasons. One of them is testing the conditional
convergence hypothesis, namely that Alpha
is less than zero. This hypothesis says
that poor countries tend to grow faster than
the rich countries, and therefore they
tend to catch up with richer countries over time, conditional on access to similar institutions
and technology. This is a classical
prediction derived from the Solow Growth Model, but it's also an intrinsically
interesting hypothesis. We will follow the
Barro-Lee study that analyzed regressions like this using the data set they
constructed themselves. That data set is interesting because the number
of covariates there or the number of
controls is comparable to the sample size. In econometric
terminology, we could say, well, the p is large here. It's not larger than n, but it is pretty large. If we just include all the
controls in this regression, all the results will be
statistically insignificant and we cannot really test statistically this
convergence hypothesis here and so it's a good idea
to do selection to see if we could improve the
precision of the estimates. Another question is how
do we do selection here? There are two basic options. One option is the naive
textbook selection option. Typically, in the corridor, if you run into a
fellow econometrician and ask the question, so how do I do selection? I have many variables. How do I do selection
and inference there? The typical answer would be, well, you use Lasso or whatever, like other model
selection devices. You could do t-tests. You drop the variables and then you estimate the
model and you're fine. Under some assumptions, you might define, but the
assumptions are very fragile. I'm actually going to go on to argue against this technique. I say don't do it here. This naive technique
proceeds as follows. As I said, in the first step, we drop all the
controls that have small coefficients using
model selection devices, the modern techniques
such as Lasso or classical techniques
such as t-tests and so on. Then in Step 2, we run the squares of y_i on the main regressor
of interest, d_i, and the
selected regressors, so this is a classical
naive approach. I also call it textbooks
approach here in the slides. It does not work because
it typically fails to control for the
omitted variable bias. Even though Step number
1 is very intuitive. Intuitively, it should control for the omitted variable bias, but actually it doesn't
control and I'll explain why it's not enough to control
the omitted variable bias. There is also
theoretical results that Lib and Poacher
have established that says that this
method breaks down, it's fragile, and
it fails to deliver the promised performance
theoretically. I'll explain their result
in more detail later. Instead what we propose, we propose the
following procedure, the double selection procedure. We will just have an
additional selection step compared to the previous
approach, the naive approach. The first step will be
essentially the same. We will just select
the controls that predict the outcome variable. The second step, will
select the controls that predict the
treatment variable, d_i, or the main regressor
of interest. The final step, we
run the squares of the outcome variable on the main regressor of interest
or treatment variable, and on the union of controls
selected in Steps 1 and 2. The claim is that this additional
selection step controls the omitted variable bias
under reasonable conditions. When we apply this double
selection procedure to this empirical problem
that I've described, we find that the point
estimate of Alpha is negative and the confidence
interval excludes zero. Let me show you these results. These are borrowing the results. They did heuristic
selection of controls. They thought hard about which controls to
include or exclude. There was a heuristic specification
search that they did. This is their point estimate. This is the standard error. The confidence intervals
also exclude zero so supporting the conditional
convergence hypothesis. This is our estimation result
pause double selection. Procedure gives us the
similar point estimate, 0.03 and standard error, which is bigger 0.01. But the confidence intervals
robustly excludes zero. If we take this three percent plus minus two percent would get a confidence interval that
is well away from zero. What's interesting here is that, well, I actually find this
to be interesting results. There's no surprise here maybe, but I do find it's interesting that hard thoughts
economic approach for selecting variables gives us the same results as our
double selection approach, which is hard for
econometric approach to doing selection here. They are very much in line and so I find it
very reassuring. It's probably some common
sense underneath all of this. This is one. We do end up supporting
their conclusions. The butter and the conclusions. In terms of particulars
of selection our double selection find
actually eight controls. The first step, does not
find very many controls. But the second step
that I had here finds a lot of controls. I mean, not a lot,
but eight controls, which includes trade openness and several education variables. You can find the details in the reference
that I provided. Let me show you other results that we see here in the table. This row here, it
says all controls. This is what happens when we
include all the controls. If we use all 60
controls that we have, we get the same
point estimate 0.02, but it's very imprecise. This is not a very
useful estimate. If we use the naive selection or textbook selection
that you describe, we get this point estimate
0.01 was this standard error. This is actually the smallest
standard error of all. The new selection gives you
the smallest standard error. Then the point estimate is this. If you look at this estimate and compare this estimate
to our estimates, there's actually a difference which is both
economically significant and statistically significant
because economically it implies very different
speed of catching up. Also if you run the Kosmin Test on comparing these
two estimators, you're going to reject
the null hypothesis that the limits for those two
estimators is the same. Although if you look
at these two numbers, you might say, well,
they work similarly, no, they don't. Actually, there is both economic and statistical difference here. This empirical
example illustrates the point that this
naive selection does not control the
omitted variable bias. I will explain to you
theoretically why it doesn't. Let me set up things formally. In formal notation, the previous example could
be written as follows. We have outcome variable, this is treatment, this
is treatment effect. This function g is everything
else in the regression, z are controls, g is the unspecified
regression function. This is disturbance zeta. The conditional expectation
of the disturbance given the controls and the
main regressor is zero. That's the setup. What's also important
for our setup is going to be the
second equation. Usually we don't write
the second equation when we deal with
simple regressions. Typically you only see
those second equations when you deal with
instrumental variable models. This is not an instrumental
variable model. This is just an
exogenous straight like very plain regression. But we do need a
second equation. What the second equation
does it relates the treatment variable
to the controls. We have this unspecified
function m plus a disturbance. Expected value of
the disturbance given controls is equal to 0. This function m here is
called confounding effect and it's responsible for
the omitted variable bias. If you wanted to estimate
this Alpha by just running a simple bivariate
regression of y on d_i, you wouldn't be able to do so unless this m function is zero. In general it will
not be equal to 0. It's responsible for
omitted variable bias. We'll need to work
with both equations in order to provide a good
way to estimate this Alpha and good way to perform
inference on this Alpha. We will use the approximately
sparser framework to model both g and m. We write g is equal to
a sparse approximation plus an approximation error. M is also equal to
sparse approximation plus an approximation error. Here excise will denote the technical
controls that we use. They could be either original controls of transformations
of the original controls or the mix of the two
situations like before. Unlike before, we
make the assumptions that the sparsity indices in
this parameter vector are small compared to
the sample size. In short, I just write g and
m are approximately sparse. I've stated this
assumption many times, so now I don't
need to repeat it. Then the approximation
error is going to be small compared to the size of the conjecture of
estimation error. I emphasize that we need to work with both equations. What happens if we just work
with one equation only? If we work with one equation, we run into a problem. This is what the naive or
textbook inference does. I will explain now what's
wrong with this procedure. Suppose we have this
single equation, we don't look at this
second equation. This is our outcome, this is the main
regressor, this is x_i, this is approximation
error, this is disturbance. The naive approach would select the control terms
here by running loci or various types of loci
of outcome variable on the treatment variable
and the controls and then re-estimate Alpha by least squares
of why the outcome on treatment and
select the controls and just apply
standard inference, and under some assumptions
that works actually. If somehow perfect model
selection happens here, this approach is fine, and the perfect model
selection will happen if a separation condition holds. You need to have a
lot of coefficients, well bounded away from zero and then the rest
should be zero. There should be a gap between the non-zero
coefficients and the zero coefficients. If this happens, then
this procedure works. Unfortunately this assumption, the good separation assumptions
is not very realistic. If you write down
any simple examples, like the ones that I've
given you that typically involve the coefficients
decaying to zero. You shouldn't expect the
separation assumption and once this separation
assumption breaks down and the procedural
also breaks down an very badly actually. Theoretically this
has been established by Lebon Porcher, the
breakdown and practically I'll just show you a couple
of examples to convince you hopefully that it just doesn't
work practically as well. Let me show you the following
Monte Carlo example, where the number of controls
here is going to be 200, but I could have constructed
the same example was the number of
controls equal to 1. I don't need 200 controls, but I do need it for later when we talk about our proposal. The number of controls is 200. The number of data points
is 100, Alpha_naught. The target parameter is 0.5. We have two equations. Here we have the controls
appearing in both equations and the controls have
coefficients of this form cy; c is a scalar here and
Theta_naught is a vector that has components that
nicely decay to zero. The coefficients that
nicely decay to zero according to this law,
1 over j squared. This coefficients are
multiplied by the scalar to induce different
R-squares in this equation. If you set cy to zero, the controls not matter
for the outcomes. If you set cy to be
like very high then the controls play a
very important role in determining the outcomes. Likewise, here we
have a similar setup. We have our coefficients here, Theta_naught following a
nice decaying pattern. These coefficients are
multiplied by the scalar and we vary the scaler to induce different R-squared
in this equation. Regressors are going
to be drawn from a multivariate
Gaussian distribution allowing for some non
trivial correlations. First, I show you the case
where the R-squared in these two equations
has been set to 0.5 and 0.5, so 50/50. This blue histogram is the
actual distribution of the naive post-election
estimator. This normal 01 distribution is what we would like
to have ideally. If we had perfect selection here the ideal distribution
will be normal 01 and under perfect selection, the theorems, the
positive results about the naive
estimator say that this distribution should be close to this normal
01 distribution. But in reality it isn't. By the way, there's nothing
crazy about the setup. There is nothing special, which is coefficients nicely
the decaying from zero. I didn't do anything special
to create this picture. What does this mean
in practical terms? Well in practical terms, it means that the naive
post-election estimator is badly biased. You see that the finite sample distribution
has two modes. This mode and this mode and either of the modes is
not centered at zero, so there is a huge bias here. Also you end up with misleading
confidence intervals. If you use this normal 01 a lot to build
confidence intervals, you end up with very
misleading statements. Your confidence intervals
will undercover. Equivalently, your
hypothesis test will reject the null hypothesis. This is a practical evidence, and also there is
theoretical evidence against this naive
post-election provided by Lebon Porcher 2009. Again, it's related to the impossibility of
making perfect selection. Once you don't have
perfect selection, you end up with very big
omitted variable biases as I will explain later. So far I'm just saying that it will have a large
immediate variable bias. You see this bias
actually showing up here. I would say this mode is largely created by the
omitted variable bias. Let's look at another
side of the same coin. Here we will look at
rejection probabilities of testing a true hypothesis. A testing procedure that has
nominal size five percent. Ideally, it should have
five percent rejection rate of a true hypothesis. The ideal picture that we should see is
something like this. This is the ideal picture. Let me explain here what
this picture means. On this axis we have the
R-squared in this equation, actually, for this
part of the equation. This is the idea
of rejection rate and each point here corresponds to different data
generating process. The scalars induces different
data generating process. For example, if we
set the scalar CD to be zero in this equation, then controls play no
role in predicting the treatment variable
and this places us here, actually anywhere
here on this line. This is the case
where the R-squared in this equation is equal to 0, and then as we increase
this value CD here, we could increase the
first-stage R-squared and look at the resulting
rejection probability. Ideally, we would like to
see a picture like this, for any testing procedure, this should be the
ideal picture. But what we get in
reality is this. This is what we
would like to see, what we get instead is this. These are rejection
probabilities of the naive post-selection method. Rejection probabilities
deviate a lot from the target five percent for most of
data-generating processes. In fact, here the
picture is truncated, so here the rejection
probabilities, they approach one. It's really horrible there and the only cases
where it works is if the R-squares in the
first equation to be zero, so basically you set this
coefficient to be zero. Then the controls play no role in predicting
treatment variables, so they don't predict
the treatment variable. In that case, it
actually doesn't matter whether you drop them or not and then the
procedure actually works. You see there's a thin
line along which it works and then there's this small
area as well where it works. It actually very interesting
that in this area, the signal in the outcome
equation becomes so strong that you're actually
able to find the right controls enabled to control the omitted variable. But this occurs only
in the very small part of the parameter space, so it only occurs for small set of data generating processes. But otherwise overall
it just doesn't work. Again, this is consistent with
results of Lebon Porcher. They wrote a bunch
of theorems saying that this should happen and we see that
this does happen. Let's look at our procedure. The formal description of our
procedures is as follows, so to define the method, I introduce the
following reduced form. Basically what I do here, in the first equation, I just substitute
out the treatment. I just write outcomes as
a function of controls and treatment as a
function of controls. It gives me two approximately
sparse regression models and what we do here, we apply Lasso to select
controls here and there. In step one, it's going
to be direct step. We apply Lasso to this equation and then we select the
controls that predict y. Then in the second step, we apply Lasso and
select the controls that predicts d, the
treatment variable. Then in the final step we
are on the least squares that the outcome on
the treatment variable and the union of
selected controls. Mathematically, we're solving
a least squares problem where the final estimator
is going to be Beta check. Minimize the least squares
criterion function where we zero out
the coefficients in front of controls that have not been selected
by these two steps. We have two model
selection steps. We take the union of controls, we're on the least squares of the outcome on the
main regressor of interests and the controls. This is the post-double
selection estimator. This estimator has
been introduced in two papers by Alex Belloni, Chris Hansen, and myself. One paper studied
the Gaussian case and another paper studied a more general non-Gaussian case as well as more general
versions of this procedure, there's nothing
special about Lasso. You could use other
model selectors and also you could have
additional selection steps where you'd say well, for example, in
practice you could say, Lasso found this set of controls and this set of controls, but I would also like to include that other control
that I really like and I think it's important for controlling omitted
variable bias. Our theory actually
allows for that, you can have
additional controls. In this presentation, I'm not
showing this possibility, but it's there so you can
find it in this paper. Here's how it works in the same Monte Carlo
experiment as before. Exactly the same Monte Carlo
experiment with R-squared and two equations
equal to 50 percent. This is the the blue
histogram, the exact findings, sample distribution
of our estimator, the standardized form of our
estimators to be precise. This is a normal 01 curve and our main theoretical result will be that the
enlarged samples, our estimator will
converge in distribution to the normal 01 distribution. This Monte Carlo experiment is fully consistent with
our theoretical results and in practical terms, what we see here, we
will have a low bias, and accurate
confidence intervals based on this normal
zero distribution. Basically it works. But then maybe it works
only for this special case R-squared, 0.5, 0.5. Let's see if it works
more generally. We're going to go back
to our rejection plots and see if this rejection
probabilities would vary with the underlying
data generating process. This is the ideal
rejection surface that we would like to see and this is the actual
rejection surface for the double selection
methods that we get. It's not equal to the ideal. You see that there's
some bending here. There is the deviation away from five percent here and here. But all in all, it seems to be doing a pretty
good job at delivering rejection rates
that are close to the promised rejection rates. This left plot is the
rejection frequency for the t-test based on the
post-double selection estimator. There is a theoretical
result in our paper that says that these
rejection probabilities will uniformly converge to this
rejection probabilities as n goes to infinity. This is actually a
practical verification of theoretical
result in our paper. Let's do some intuitions. We see that double
selection method is robust to model
selection mistake somehow. I said Lasso is bound to make model selection mistakes and then in the
computational experiments we have a data
generating processes where coefficients
decay is mostly to zero so therefore we have
some coefficients that we cannot tell
apart from zero, so we must be making some model selection
mistakes there. Lasso makes those model
selection mistakes, but somehow in the
end it still works. Why is this? The critical step here
is the second step. The introduction of
this selection step precisely controls the
omitted variable bias. It says we do need to include the controls
that are related to the main regressor of interest and this creates this
robustness against moderate model
selection mistakes. Another piece of
intuition could be heard by recalling what
the Frisch-Waugh procedure does in the usual
regression setting. You remember
Frisch-Waugh procedure is a way to perform
estimation in regression. You'll say if you want to get an estimate of
Alpha what you want? You want to take out
the effects of x on both the treatment
variable and the outcome, and then regress one
residual on the other. Essentially, our procedure is a selection version of
Frisch-Waugh procedure. Obviously, you couldn't do a pure Frisch-Waugh
procedure here because well, if p
is bigger than n, then you end up
with zero residuals and nonsense results at the end. But our procedures is a
simple selection version of Frisch-Waugh procedure for estimating a
linear regression. This hopefully provide
some piece of intuition. There is even more
intuition coming, and you will appreciate
is a intuition if you like econometrics. Let's think about
omitted variable bias in the case where we just
have a single control. We have an equation
with a single control. This control is related
to the treatment variable and what we want
from our technique, we promised that it works
with a lot of controls, but it better work
with one control. Let's see what happens. If we somehow end up including this control in regression,
well, nothing bad happens. We just run long regression and we're fine, we know that. The bad things could only happen if we drop this extra control. If we drop the control, then we run the
short regression of the outcome on a
treatment variable, and then we could express
the resulting estimator minus the estimator times root n as equal to some good term, that will be
asymptotically normal. The omitted variable bias term, which includes root n, product of two nicely
behaved numbers. This is bounded. Then we have product of
coefficients Gamma and Beta. Gamma is the coefficient here and Beta is the
coefficient here. Gamma measures how important controls are for
predicting the treatment. Beta measures how important controls are for
predicting the outcome. Then this is the
omitted variable bias, it's a product of
those two terms. What naive selection does, it drops this control. Naive selection works only
with the first equation and it drops this control only
if Beta is close to zero. Specifically, for the choices of penalty levels
that are described, Lasso will drop that
Beta if Beta is of order square root
log n divided by n. But this doesn't guarantee
that this term goes to zero. Ideally, we don't want
this term to go to zero. Specifically, we want root
n Gamma Beta to go to zero. But if Gamma is not equal to 0, then root n Gamma
times square root log n divided by root n, well, actually goes
off to infinity at a speed of square root log n. This post-naive estimator, despite having a good term here, actually diverges off to infinity after
normalizing by root n. Remember, we're
scaling this omitted variable bias by
this root n term. This number drifts
off to infinity, and so this post-naive selection
estimator breaks down. The double selection procedure is actually a lot
more conservative. It actually drops x_i. Therefore, we end up
in a case like this only if both Beta
and Gamma are small. Specifically, we drop x_i, the control, only if Beta is afforded square root
log n divided by n, and Gamma is afforded square
root log n divided by n. Therefore, we have root n
Gamma Beta is going to be at most of order log n
divided by square root n, and this term goes to zero. The omitted variable bias term multiplied by root n vanishes, and a good term
wins, and therefore, we have good asymptotics for the post-double
selection estimator. Here's a theoretical
result finally. We show in our paper that uniformly within a
rich class of models, in which g and m admits an approximately sparse
approximation with sparsity index being
this condition, so sparsity index squared has to be small compared
to the sample size. Other practical
conditions holding, such as the one that
I listed before, we have that Alpha
check estimator, the post-double selection
estimator minus the estimate times root n times the scaling factor converges
in distribution to the normal 01 distribution. Here the scaling factor involves the standard
expression is Robinson's formula
for the variance of least squared estimator in the partially linear model. In a natural, if you
use our procedure and then you just use the
standard errors that startup provides as the output
to the final step, those standard errors are valid. I could have also
referred to White here, but I'm referring to Robinson
because he's the one who worked a lot on
partially in linear models and established some
good results about partially linear models
like our result is a generalization of
Robinson's results. Also under homoscedasticity, disposable selection
estimator is going to be somewhat
parametrically efficient. What does this mean? It means that under
the same assumptions, you cannot find an
estimator that has this normal 01
asymptotics uniformly in the same class of models that has asymptotic variants
smaller than our estimator. You cannot do better than what this estimator does under
the same assumptions. In some sense, well, I said our procedure is
more conservative, but this is just about the right amount
of conservativity. This conservativity buys
us this nice result and we cannot do better, at least under
homoscedasticity than that. The key point here is that
model selection mistakes are asymptotically negligible due to the use of
double selection. This is the key point. This is a theoretical result. What I will do next, I will present several
generalizations, and this is going to
be bonus for you. It's stag materials. This is advanced material
and it's going to be tough, it's not going to be easy. Enjoy it if you can. The question is, can we generalize
this procedure? This procedure is somehow
exploiting linearity. In IV problem, we
use the linearity of the problem quite a bit and we relied on a
special structure there. Here we're exploiting
also linearity as well. This double selection
somehow exploits the structure of the
problem very heavily to overcome the model selection
mistakes in inference. Can we generalize this? Can we construct procedures for non-linear
problems that have similar robustness
properties with procedures that don't break down under moderate model
selection mistakes? In order to proceed with
these generalizations, we can try to understand how this double selection works in terms of moment conditions because once we write down
the moment conditions, we could see the past
to the generalization. The claim is that the double selection
procedure implicitly identifies the target parameter
of this moment condition. This is a method of moments estimator based on
this moment condition. This moment condition
has this function M_i, which is a function of
Alpha target parameter, G nuisance parameter, and M
is the nuisance parameter. Then we have this residual here, d Alpha minus g. Then we have a quasi
instrument here, d_i minus mg_i,
where we took out. Here, by subtracting off M, we're taking out the
correlation of this term was anything that
is a function of g. For example, this part
would be uncorrelated with this part when this m and g are evaluated at
their true values. Somehow we are using
the structure here. Then we're estimating
the true values of g and m using
post-selection estimators. Implicitly, we're
using the controls that we selected
to estimate them in the post-selection procedure. The reason why double
selection works actually, despite the fact that g and m are "crudely estimated via
post-selection methods", the rate for estimating the g and m is slower
than root n rates. This double selection
works somehow. It works because actually this moment function
here is somehow immunized against
perturbations in g_naught and m_naught
specifically. The following condition holds the directional derivative of this moment with respect to g. If I drop g around
the true value, so you compute
directional derivatives, and all of those
derivatives are equal to 0. The direction of derivatives
with respect to m when you perturb m around the
true value also equal to 0. Somehow small
mistakes in g and m, they don't affect the
identification of the parameter. This is the same intuition
as we had with instruments, like emission of
instruments that are not very good at predicting the, the treatment variable does not impact the
identification somehow. This is the intuition and precisely if you
have this property, then you can expect that the
moderate selection errors are not going to have impact on the asymptotics of
the estimator because this model selection errors translate into moderate
estimation errors, and in asymptotics,
they wash out because they are multiplied
by the terms like this. When you're using empirical
analog with this equation, you start doing a Taylor
expansion with respect to g, this is going to be the leading
coefficient and it's zero, so intuitively, you don't
need the root and rates, you just find the slow
rates of convergence there. This is the immunization
or orthogonality property that plays a key role here. Can this be generalized? Yes, you can generalize
this to any moment problems as long as you
can do the following. You need to be able to identify the target parameter
Alpha_naught via this equation. So you do need strong identification
for routine asymptotics. But also for robust inference, you need the
immunization property. We have nuisance
parameter, h here, and what we need is that this moment is immunized against perturbations
in h_naught, so the directional
derivative with respect to h has
to be equal to 0. In the case of double
selection approach our h, the nuisance parameter was consistent in all
these two functions, the regression function
and the m function, the auxiliary
regression function. As long as you can set
up estimation problem with this two
properties, you're fine. Then you will have that the
non-regular estimation of h, the nuisance parameter
via post-election or even other
regularization methods will not impact the asymptotics
of the estimator for the mean parameter,
Alpha_naught. You could have
slower than root n, convergent estimates for h. In the case of selection,
this property here allows for model selection
mistakes in estimation. Also in the absence of this
immunization property, the post-selection
inference breaks down. You could see this by examining the single selection procedure. The single selection procedure, if you write down the appropriate
moment condition that corresponds to that
estimation method, it actually does not
have this property. You can verify that it does
not have this property, and therefore it breaks down. You do need to get
this property. Like one we looked at
the previous example from this framework,
we see that, well, in the IV
model we had success because we worked
with M_i function that has immunization property. Specifically we had the
residual multiplied by g. What happens here? If we perturb this g to
some other value g tilde, if we replace g of g_i,
the optimal instrument by something close to
the optimal instrument. The equation still holds and the identification will
still work, so we're fine. The model we can use post-selection
approach to estimate g and we will be fine and there will be no
impact on the asymptotics of Alpha_naught hat. For partially linear model, our double selection
procedures implicitly set up this moment function which has this
immunization property, and there we had the validity of the post-selection inference, but precisely because
of this property. There's a number of
generalizations you could do, but we carried out some
of these generalizations to the case of logistic
and quantile regression. If you are wondering about
nonlinear problems, yes, there are generalizations to the nonlinear problems such as the logistic and
quantile regressions. They're much more involved because you have to set
up these equations, you have to figure out
what the dysfunctions are. The functions that both
identify your parameter and have this immunity
from prosecution property. As a final example, I want to show you the
following results. They deal with estimation of average treatment effect
parameters in the case where we have heterogeneous
treatment effects. Again, this is
advanced material, but it's just going
to be two slides. It's not going to last long. Here what we do, we'd drop the partially
linear structure completely. Previously, I talked
about treatment effect, but I talked about it in the
partially linear framework, and you may not have liked this partially linear framework. Here we will drop this
partially linear framework. However, we will assume that
the treatment is binary, but we will assume that the treatment is
fully interacted with other control variables. Specifically we write
outcome is equal to the regression function which
has this general from g, g_i, g_i, and this g_i could be
decomposed into two pieces. We switched on this part when g_i the treatment
is equal to 1 and then we switch on this part when the treatment
is equal to 0. Just two parts of the
regression function, same disturbance as before, same auxiliary
equation as before. But now we could give a name
to this auxiliary equation, it's a propensity
score equation. It says that treatment is related to the control
C of propensity score. This is our regression function. A target parameter of interest is going to be the average
treatment effect parameter, which is the expected value
of g_i minus g_0, g_i. This is the average treatment
in fact for the case where the g_i is full
interacted with controls. Then the g's here and m's
are going to be unspecified, so they're going to be
functions of many controls. But we do assume that they are approximately sparse
structure for both g and then for both the
regression function in the propensity score. To give you context, Chris, will present an
empirical example where and what he analyzes g_i, the case where g_i the treatment variable
is going to be for a 1K eligibility. The y is going to be net
savings and or total wealth. G_i's will be characteristics of the workload of the firm, and Alpha_naught will be
the average impact of for a 1K eligibility on
savings or total wealth. This gives you a
context to this setup. The way we proceed
with estimation, well, we need to figure
out the appropriate M_i that has identification property and the immunization property, and it's given by the Jin Hahn's efficient
score function. Jin Hahn worked out the
efficient square root function for estimating average
treatment effect. That turns out to be a function that automatically has
immunization property. What is this function? Well, it has a lot of
moving parts in it, but it's important
to recognize that it involves both the g,
regression function itself, and the propensity score. You need to model both
the regression function and the propensity score if you want to be robust with respect to model
selection mistakes. This regression function has
the immunization property, so it's a robust against perturbations in
g_naught and m_naught. It has this property you
can trivially verify it. In this context, this
property is sometimes called double robustness in the
parametric estimation context. Then the post-double
selection estimator here for average treatment
effect is going to be given by this expression. Essentially we solve
the empirical analogue of this equation, so we take the average over this and solve the
empirical analog, and we get this post-double
selection estimator Alpha check, which has this expression. Again involving both the
regression function estimates and the propensity
score estimates, and then we estimate g and m via post-selection methods,
and the result is, that is presented in our
updated CEMMAP paper. This paper. Inference on
treatment effect that the selection amongst
higher-dimensional controls. This result is similar to
the result that I've stated. Uniformly within a
rich class of models in which g and m, the regression function
and the propensity score admit approximately sparse form with the sparsity index
being this condition, s squared has to be
small compared to n, as well as we need other
practical conditions such as the ones that
I listed before. Under these conditions, we have that
post-double selection estimator minus estimate, times the scaling factor
is asymptotically normal. The scaling factor here
has this neat form, so this is just expected
value of M_i squared, Alpha_naught,
g_naught, m_naught. This is very easy to estimate
by the plugin principle. Moreover, this estimator
is semi-parametric efficient for estimating
Alpha_naught. You cannot do better under
the same set of assumptions. Here in model selection mistakes are also asymptotically
negligible due to the use of immunizing
moment equations. Let me now conclude. I talked today about
approximately sparse models. I argued that they correspond to the usual parsimonious
approach using economics. But we put the
specification searches and more rigorous footing. Then I talked about
estimating a prediction, estimating regression functions
arising in those models. We discussed this Lasso
method and Post-Lasso method. Then we presented a
series of applications. One application was
selection of instruments. Another application was
selection of controls in the partially linear model
where we did two things. We argued against the naive
or textbook selection, and we argued in favor
of double selection. If you have to do selection, do double selection, don't
do the naive selection. We argued that under certain assumption it protects against the omitted
variable bias. Then we argued for the use of immunized moment
equations more generally. Questions now. 