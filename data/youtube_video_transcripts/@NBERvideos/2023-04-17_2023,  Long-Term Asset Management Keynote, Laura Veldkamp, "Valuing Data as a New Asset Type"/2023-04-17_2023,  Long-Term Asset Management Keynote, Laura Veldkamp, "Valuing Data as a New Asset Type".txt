hello I'm delighted to be speaking here today at this year's long-term Asset Management conference I'm going to be talking about valuing data as a new asset type big data is rating raising some awfully big questions about the economy we know at some level that the most valuable firms in the world today are valued well they have glossy buildings and sure they've got talented workforces but they're really valued largely for their data and this raises a whole agenda of questions both theoretical empirical that touch on every aspect of finance so this data is a new asset class a recent events might ask us cause us to ask questions like is it long duration and therefore interest rate sensitive young data intensive firms also have losses so Amazon lost revenue for the first 26 quarters of its existence Uber has lost revenue for years and yet these firms seem to be highly valued so how should we value them and then there are entrepreneurship questions like are large terms of data an entry barrier for new firms what strategies might overcome this and and how do we assess these entry barriers what does Market power even look like but a lot of the new Digital Services we see are are free or are they really free they're at zero price so the the point here is that a lot of the ways in which we measure and think about Finance questions is based on industrial era economics tools those kinds of tools combine physical capital with labor to produce an output we might call it a widget that's typically a rival good either you can use it or I can use it um and that's not quite what's going on with the modern data economy so let's talk about ways in which we can update these tools so I won't be answering all of these big questions that I posed on the first slide today that will take a whole uh agenda of research but the question today is really how do we start on this agenda and I think most of us would agree it probably should start with measurement how do we measure the amount and the value of firm's data so I'm going to start with what do we mean by data and then I'm going to warm up with a little bit of data economy mechanics so we'll give us some tools to think about measurement we'll talk about data as a byproduct of transactions about buying and selling data about depreciating data and then we'll get into the business of measuring and valuing and I'll give you six approaches none of which are perfect but can be applied to various contexts and conclude with where next so what's data data is digitized information it's also an asset it's also an intangible it also might digitize ideas but at its heart it's digitized information and the data of interest in the economy today is really big data and that big data is typically generated by economic activity it's things like search histories and traffic patterns what did you buy or what machine did you buy it at what times of day what's your zip code and so forth and Ai and machine learning these are Innovations new Big Data Technologies and they're really the reason we're having this conversation now right data and bookkeeping or Millennia old but we're talking about this at this point in history because we've had these big breakthroughs and the nature of these Technologies is that they're prediction Technologies they're they might those predictions might be inputs into inventions and the inputs into profits and inputs into other things but their data used for prediction so at its heart I'm going to think of data as digitized information that's used to predict some random outcome and this data is then distinct from technology patents learning by doing or algorithms ideas and Technologies or procedures or concepts and we know data can be an input into that but many of us use data as an input into our research but the data we use is distinct from the ideas of our research itself learning by doing is similar to data and that both can be a byproduct of production and economic activity but learning by doing is human capital it's owned by a worker it's in your head and attached to you as a person and it's not directly tradable right I can convey some human capital to you through the process of teaching but that's costly for the teacher and the learner I can't directly download what's in what's in my head and transfer it to you and algorithms are certainly related but there are technologies that use data and are distinct from the data itself the data may help improve those algorithms but it's not the same okay now that we're clear on what we're talking about let's talk a little about the economics of data I think this cartoon from the New Yorkers or it's a nice way to characterize what's going on in the data economy so the kids have a lemonade stand and the adults are standing there talking and they say it's free but they sell your information right so what just happened here at the lemonade stand was this lemonade really free well it was transacted at a zero price but it was exchanged for something valuable it was exchanged for the adults data so this is a barter trade right and this was a valuable exchange it was a valuable exchange because lemonade has value to the adults who are enjoying it the data has value to the kids they're going to monetize it they're going to sell it but it had a price of zero so in GDP this transaction would count as zero there's a lot of these transactions in our economy I bet every single one of you has a cell phone with an app that was free in the same sense as the lemonade was you're paying for it but you're paying for it with your data and not only does this apply for zero price transactions there are a lot of partial barter trades out there that we might not be recognizing so if I go to Whole Foods and I let them scan a QR code they'll give me about five percent off my grocery order well in that instance I've just paid for my groceries 95 with money and five percent with my data so that was a partial barter trade and then if you think of any firm that values data and gets it by doing transactions that firm is going to want to do more transactions well everybody'd like to do more transactions how do you make that happen you make that happen by dropping your price and so firms might naturally in the business of being competitive want to lower their prices a little bit to generate more transaction to generate more data but that difference that lowering of the price is now the payment for data it's the partial barter trade reappearing so this kind of lemonade transaction may be happening in a widespread way in the modern economy so this gives rise to the data feedback loop if more transactions and customers generate more data and firms use data for some reason that helps them be profitable it helps them improve the quality of their good as they're innovating or the efficiency of their procurement or transformation Transportation or it helps them advertise to the right people so maybe they're producing the same good but if it gets to the right customer they value it a lot more could sell for a higher price that's going to show up as higher efficiency as well and all of those reasons that firm might be more profitable with more data would encourage the firm to grow bigger and do more transactions so that then in turn generates more data so more data here begets more data this is an increasing returns feedback loop and it's really easy to model this increasing returns feedback loop you could do it with a simple three equation a system like this we can certainly add more whistles and bells to it but it's hard what I'm telling you is that data tomorrow DT plus one might be probably a depreciated version of data today that's a stock of data plus some new inflows of the data and those new inflows of data are related to YT the number of transactions we've done with ZT just transforming units of output into units of data and with more data a firm could be more productive so we could write a productivity function a that's increasing in the stock of data and then more productivity produces more output both directly and because a more productive firm might choose more inputs like a higher K in this instance and so that simple three equation system could explain data fueled monopolies and that's an idea I explore and my work with Mariam farbuti and Juliana beginner so let's think about that right leg of the data feedback triangle how data creates higher quality how might that work well one way it works is certainly data helps raise current profits firms with more data choose better products they manage their inventory better they do better transportation they advertise to better customers it's a really nice review of this this literature and the ways in which firms use data to profit by Catherine Tucker but data also helps firms create Market power that doesn't have the same social value but it does profit the firms who are using the data so firms with more data can grow bigger and use data perhaps to out-compete their Rivals and extract Monopoly rents but the third piece of this that I think is less appreciated is that data reduces risk remember that data is digitized information and the nature of information is that it's a way to resolve uncertainty or reduce risk help us predict things better so we're less surprised by them and this is what makes Finance tools for this question really crucial we need to bring in our knowledge of how to model uncertainty and risk how to price risk how to measure risk and this effect could be big this isn't just a little add-on to other benefits of data do the following thought experiment think about the return on equity how much of that is compensation for risk and how much of that is the riskless return well typically we think of the risk compensation as being twice as big as the riskless return and that suggests the scope for using data to try to resolve some of that risk to try to shrink uncertainty could be really large economically okay so in what I've talked about so far we've sort of pretended as if data automatically generates productivity and this is kind of silly right it's it's almost as if you do some empirical work and the moment you get access to your data set the paper writes itself and we know that doesn't happen lots of Labor has to go into this and there are two types of Labor I'd like to focus on one is a type of Labor I'll call data manager labor and it's quite prevalent in the hiring data and these are people who turn raw data into structured data they structure the data sets they make a queryable database that other people can search easily to answer the questions they want but they also maintain the servers and the software and fix the broken links and so forth and so I'm going to model them as Lambda and think about combining Lambda the data management labor with some raw data lowercase D to produce structured data uppercase d and then there's this other group of people they take the structured ready to use data and they produce knowledge and I think of knowledge as really an action recommendation here are the assets we should invest in or here's the products we should produce or here's how we should do procurement and those analysts um those people who turned structured data into knowledge I'll call analysts and represent as capital LT so that we can think of knowledge omega as being a combination of structured data DT and analysts LT and we'll come back to this idea and use it to help and infer how much data a firm might have so we've talked about firms can get data as a byproduct of economic activity they could use labor to help create structured data sets but they can also of course buy it or sell it so as we start thinking about data markets we have to keep in mind a few special features of data first is it's non-rival right you could sell a copy of your data if you own it and you can keep a copy second is data leaks it leaks through market prices right if you're trading an asset some of what you know will affect the market price and other people will learn from that but it also leaks through your neighbor's transactions if my neighbor is similar to me and they like a particular product a a firm might rightly infer that I'll probably like that product as well even if I've never bought it through them and data known by others loses value this is the Strategic substitute ability and information acquisition that we talk about in financial markets but it also shows up in economic markets and oligopoly market for example but typically have the feature that the information everybody knows doesn't have a lot of competitive value but information that your firm has that's proprietary you can extract a lot more out of and so all of this implies that firms selling data might face a commitment problem this is a a question I'm looking into and work with sungma and Ernest Liu so most firms are monopolists over their particular data set nobody else has exactly the same data that they're selling and so to extract Monopoly rents they'd want to restrict access right reduce quantity sell the data to a few clients but the problem is as soon as a firm's done that tomorrow the data seller could come along and sell some more copies of that data and then the lower the price to broaden their Market but as soon as they do that they'll dilute the value of data for the initial people who bought it because remember the value of data is lower when others know it and so a firm May compete a lot of its own rents away and it's a lot a lot of its own incentive to acquire the data and the first in the first place they disappear along with it so these are some issues we're going to have to keep in mind as we think about how data markets might work we want to think about viewing data the depreciation rate is a key question for valuation so let's do a simple example to think about how this might work suppose you're using data to predict a simple ar-1 process right Theta t plus one is the persistence parameter rho times Theta t plus an innovation it's normal well I have some prior Precision I don't exactly know what today's State Theta T is I haven't observed it but I I have some forecast of it my expected squared forecast error is V of theta conditional on the data or the information set I have it I'm going to invert that to get a conditional precision and call it Omega and I'm going to put the term stock of knowledge on that and you'll see why in a second okay so this represents what I already knew about the time T State now I can take that and use it to say well what's my T variance of tomorrow's state so I'm just applying a variance operator to the left and the right side of the ar1 equation and I'll get out well as I take the variance I gotta Square the parameter Row the variance of theta T I've defined to be Omega T inverse and the variance of that Innovation I've defined to be Sigma Epsilon squared okay that's simple enough now if data forecasts tomorrow State then a data point is a noisy signal about tomorrow's State we could represent that as St in tomorrow's State plus noise and Bayes law for normal variables says the t plus one precision that's what I've called Omega t plus one should be the prior Precision plus the signal Precision the signal Precision here is the NS times the Precision of each of the data points Sigma s inverse and so what that means is that I can write tomorrow's precision as essentially a depreciated time T data term plus some new data inflows and this now looks pretty similar to how we modeled capital and a capital depreciation equation this is like KT plus one is one minus Delta KT plus the new Investments right my new data inflows or my new investments in the data stock except that I've got this funny sort of depreciation term that's not linear and the depreciation will be faster when data is abundant and when the environment has volatile Innovations right because if the world's changing a lot then the data I have about the way the world was yesterday is not as useful for predicting how the world will be today okay so that's how we can start to think about data depreciation all right so now let's use these foundational Concepts to turn and think about measuring and valuing a firm's data I'm going to consider six approaches to measuring data the first is cost accounting then complementary inputs value functions Revenue approach choice covariance and market prices let's get started a cost accounting so this would be a standard Book value approach right the way we might value the the building that that we're in uh is to think about what was the cost of building it and maybe you know what was the cost of maintaining it so the equivalent approach for data would be to accumulate the sum of costly investments in a data set well this is tricky because remember the lemonade stand most data is a byproduct of some other economic transaction it might have been Bartered there wasn't an explicit cost for it and accounting isn't going to help us very much here because Gap accounting rules only allow us to count data in the balance sheet of a firm if it was purchased that applies to some data sets but not a lot of them and so what you're going to have to do is think about the implicit cost remember the data exchange of the lemonade stand had value it was Bartered it just didn't have an explicit price and so what we might want to do is say well how valuable is that lemonade that the data was Bartered for right what's the market price of a cup of lemonade from a lemonade stand let's call that the price of the data that's the price the adults would have paid for the lemonade if they hadn't transferred their data instead so that would require us to go to go measure discounts in the price of a lot of goods remember a lot of these barters aren't zero price barters they're not complete barter trades they're partial barter trades and so we'd have to have a way of imputing what would be the zero data price of a good but if we could do that this could work because then we could accumulate up these implicit costs and impute the data barter discount and build it up to value a stock of data second approach complementary inputs so remember I had that knowledge triangle knowledge and structured data and unstructured data and I described to you about how that might use labor to create each of these asset structured data knowledge so let's think about knowledge as being produced using structured data that's the capital D and the analyst labor that's the capital L and I'm going to add to this we probably want some time fixed effects that's what the capital sign some firm fixed effects that's a little sign okay we can go and estimate this thing we can observe of course how firms hire workers right we could use something like burning glass data to go count hiring and accumulate those up to get some labor stocks and then we can think about new structured data as being added to the existing stock of structured data with data management labor right so here we want to think about data management labor didn't create the whole stock of data today they added to an existing stack of data that's got some depreciation rate that we talked about earlier so you could estimate the data stock capital D of a firm from observing L the number of workers analysts and Lambda the number of data management workers and the wages that those people are paid and we'd essentially be asking what amount of data would be they would make employing this many analysts and this many data managers optimal so that's something that I do in some work with Simone Abbeys that's forthcoming at the RFS but another way of doing this would be to say well it's not just labor that's complementary to data you also need computers right and so presidentson used this idea and they use it capital to proxy for the amount of data that a firm has approach three value function approach the same tools macro uses to Value Capital we could put to work to Value data we need a an adjusted law of accumulation so V here is a Bellman value equation uh this is about an equation the value of data is the maximum over some choices The Firm makes let's think about capital and labor and the firm produces some Revenue some productivity that depends on the data they have in their capital and labor inputs of course they've subtract off the cost of Labor subtract off the cost of the capital and then we have this last term which is the discounted value of the data that the firm will have tomorrow will take forward and accumulate more data now to make this work we're going to need a law of motion that Maps data at time T over here to data time t plus one well that law of motion is the depreciation equation that we talked about earlier I called this Omega now I'm just going to call this data at t plus one is this depreciation term for data at time t plus new data influx so what are these new data inflows well we need a theory of that we could think about new data inflows as being a byproduct of transactions right that was that Z times YT term and our data feedback loop triangle and or we could think about data being acquired by purchases of sales we'd have to add a new Choice variable for the firm how much data do they want to buy or sell but we could do that and then we'd subtract off the cost of data or add on the revenue from selling data to market clearing price and we could also include the idea that maybe just data alone doesn't really do much for us unless we combine it with some skilled labor so we can think about using labor to process raw data and turn it into structured data or knowledge so all of these would be theories of how do we create new inflows of data or knowledge and you can use any one or you could combine all three so what Simona and I do and in our RFS paper is actually a combination of uh the last two approaches that I just showed you we use labor complementary inputs and count up hiring by a bunch of financial analysis firms but we also use the Bellman approach because we want to account for the idea that that data is valuable not just today but it has this future value as well and what we find is a large increase shown here in the aggregate stock of value for these financial analysis firms from 2015 to the end of 2018 and it's about a 33 increase and data value here is growing rapidly for three reasons the first is firms simply are accumulating more data and more data is more valuable but second is they're also hiring more analysis workers data here is not crowding out labor data is crowding in labor with more data they want more people to work with it and that helps to make each data point more valuable and the third is we identify a subset of firms that are hiring analysis workers that have skills that particularly indicate Big Data Technologies right they have skills like being able being fluent in artificial intelligence or machine learning or random forests or they use uh plugins like tensorflow or keywords that indicate that this firm is interested in using some big data technologies that subset of firms is becoming particularly productive more productive than their peers and that's increasing their value of data as well fourth approach is revenue you might say well you know if we want to Value data why don't we just count up the present discounted revenue of that it generates right and that might be easy in some cases it can get complicated a lot of firms will use data for multiple purposes they might use data to manage human resources and also use it to help advertising that might use it for procurement as well it might be difficult to model and measure and isolate and break out exactly how much value that data is creating all these different roles but this is doable you need a really clear idea of how data generates Revenue essentially you need a model and a model here is particularly essential because when we ask what's the value of data we're asking again counter factual question how much revenue would this firm have had if it didn't have that data so another challenge here is that data has different values to different agents it's a private value Asset and that private value component I'm going to show you is is rather large essentially data is unlike financial securities we tend to think most people value a financial asset pretty similarly right we are going to use it for the same purpose to make money but data you might do something very different with it than what I would do with it and so you might extract a lot more value out of a data set than I would be able to and so your evaluation for it might be quite different so I'm going to show you an approach next of valuing data as a private asset using a revenue approach so in this example data is used for the particular purpose that it's used to inform the purchase of a portfolio of risky assets right so we're going to use our data to predict which assets will have higher return and buy more of those so the value of the data set I T from an equilibrium model with heterogeneous investors correlated information and learning from noisy prices we show depends on three sufficient statistics the expected return the variance of the return and the conditional variance of their turn given the data that you're valuing and given these three Moments One can value data now you can estimate these three moments expected return and the variance of return are relatively straightforward I won't say easy because I know there's a lot of disagreement among people in the field about how we should do that but we have tools for doing that the conditional variance is the tricky one but you want to recognize that the conditional variance is really an expected squared forecast error so what you need to do to figure out your conditional variance is run a forecasting regression and compute the mean squared errors and so when we do this our finding is that the same data is worth ten dollars to some investors and 1.2 million dollars to other investors depending on the Investor's wealth their investment style their price impact or their trading frequency so this is work with Mariam farbuti Drew singal and thank you thank you approach five choice covariance data is valuable to firms because it allows them to make better choices or better matches and better choices really means actions that Co vary with some objective or some payoff so I'll call the action queue and the payoff R and think about somebody who's trying to maximize the product of these two I want to do more higher Q when the payoff is high RT and so I can think of the expectation of that product of course as the product of the expectations plus the covariance and that covariance term is really interesting because an agent cannot achieve a high covariance without information about R if you're trying to choose an action queue and R is a stochastic process I can't achieve systematically could be lucky I can't systematically achieve a high covariance without having sufficient information at R it wouldn't be a feasible it wouldn't be a measurable strategy now an agent could have some really great data and be just terrible with it right they have no analyzing it and they could screw up and they could achieve a lower covariance than their data would enable so you really want to think of this as a lower bound on the amount of data a firm has but you could use this covariance to back out how much data an agent or a firm has an example of this is a portfolio Alpha right that's the excess return on the portfolio the expectation of two times R in excess of the Returns on the assets that that are contained in the portfolio a customer conversion rate right how many times do I serve and add to a customer that I actually click through and buy the product at hand is an example of a covariance that would be feasible only with some data which customers are more likely to buy but keep in mind that these covariances might also show up as aggregation effects so in my work with Yan acap we look at the difference between a firm markup and a product markup as a measure data why because the firm markup is the collection of product markups that Waits more heavily the goods The Firm produces more now firm wants to produce more of high markup I.E High profit products and data that forecasts those markups allows them to do just that so it creates an aggregation effect so the aggregation effects all over the place so we measure things differently and they might diverge they might grow with more data last approach market prices so we might think why don't we just look up the price for the data set right their data marketplaces out there snowflake data raid there are more and more of them every day keep in mind though that the price at which a data set sells is really the intersection of supply and demand so it's a lower bound on the bot on the buyer firm's private value right they wouldn't possibly buy the data set if they didn't evaluate at least that much but they might value it a lot more so market prices the intersection of supply and demand don't necessarily tell us about the demand curve the private values how much is somebody's willingness to pay for the data set but for many questions the market price would be a great answer to it for other questions we really want to dig deeper and understand the value of it to a buyer keeping in mind that that often depends on who else knows the same data we might also look at market prices for firms right this is what Donna Cruze and Eberly and Peters and Taylor do to think about intangible other kinds of intangible assets and and data is certainly one of those assets but this is tricky because that market to book sort of value is already used for lots of other intangible assets there it's been claimed by people who say it represents the value of branding the value of patents the value of organizational Capital so it raises the question how are we going to tease apart the value of data from these other kinds of intangible assets we might have to use an approach like this in conjunction with one of the others to isolate that effect and lastly market prices presume that market participants know how to Value data and a lot of context Market participants are great at determining the value of the wisdom of the crowd shines through but those are often contexts where these participants have had a lot of experience trading this asset and valuing it this might not be the case for a new asset like data or Market participants might be looking to us for guidance on how to Value the asset okay conclusions so data is one of the most important and highly valued Assets in the modern economy we know that at some deep level but we want to put an actual number on it we want to be able to say exactly what that value is and that's tricky because it's one of the hardest to observe assets it's hard to measure it's hard to put a price on but we cannot stick our heads in the sand with one of the most valuable assets in the economy and just say I don't know how to do it so let me just ignore it we're going to need different approaches for different situations we're going to have to refine these techniques and theory and measurement will have to work together here because a lot of these valuation questions are counter factuals because we're going to need the structure to help us impute things that we can't observe directly so next steps in this agenda I've talked to you mostly about the value of data which is a data demand how much am I willing to pay for it but we'd want to combine that with a theory of data Supply how do data markets work how do platforms work how does a data ownership matter and then we want to combine the demand estimates with the supply Theory and develop a theory of equilibrium prices of data right the valuations and how do they fluctuate are there data risk premium we want to we'd want to compare these sorts of equilibrium prices to what we see as transaction prices in data markets and this would give rise to a new asset pricing Theory but for data and so to help us all do that there's a textbook coming soon the day-to-day economy joint work with Isaac Bali it will hopefully be coming out in press in a few months with Princeton University Press thank you very much 