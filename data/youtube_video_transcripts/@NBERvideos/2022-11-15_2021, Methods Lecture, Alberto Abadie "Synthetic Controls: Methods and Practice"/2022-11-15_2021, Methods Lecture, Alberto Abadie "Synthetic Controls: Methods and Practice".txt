it's great to be here with a with ratio Matias talking about the synthetic controls and the and the regression discontinuity designed and um yes and the first lecture is going to be about a synthetic control so let's go for it um so synthetic controls were um originally developed to estimate the effects of a large aggregate events and in particular the effects of a policy interventions that are deployed at the are available interventions that may affect entire cities or or regions or countries if you think about that much of the available Machinery that we have in economics and in statistics to estimate treatment effects is designed with the idea of estimating an average treatment effect over some population and from that population we have extracted a you know a large sample of individuals one of the not prediction of they're not treated and we are going to compare it in some way however like many of the Polish interventions that we care about or many of the aggregate events are together but well they are not like that they do not happen at the individual level they happen at the level of a you know large jurisdictions as I said before like um like cities a religious regions or countries and even if you are in an experimental setting where the experimenter has the freedom of a you know like um deploying the intervention at a micro level or at a macro level or at an aggregate level the experimenter may not want to do it at the micro level because you know there may be like a fairness considerations for example if you have two individuals in the same location and one is sweeter than the other is not treated this could erase like um issues of fairness and also you may have like interference issues you may have like two individuals in the same locations one is streets thank God it's not treated but they are interacting and like this is creating a bias in the experimental estimate So to avoid all these type of issues you may want to implement your intervention not at the micro level but the level of a city or or a school district or a region synthetic controls have been applied to study a variety of you know problems empirically like a from a you know immigration policy to the effect of a corporate political connections uh they have been adopted as a as the main tool for data analysis uh you know across different sites of issues in in prominent polish debates on you know like immigration and minimum wages and and they are also applied to also at economics in the social sciences in biomedical disciplines especially in epidemiology in computer science and so on um they have also been widely adopted by multilateral organization symptoms like business analytics units Etc and um and for example a synthetic control method plays like a big role in the in the official evaluation of a massive FM educational program funded by the gate Foundation and you know while I think it is um fair to say that the econometrics methods are not a daily staple of a popular place and you know and I guess this is a huge understatement um synthetic controls methods on the application have a times I managed to capture the the attention of journalists and of Virginia public so what is my my goal for the talk today so I I want to provide an introduction to synthetic control methods describe some of the advantages of this methodology which my mind is explaining the popularity that I have described before and I also want to talk about a you know some recent developments and like in synthetic control estimation and I will finish with the some remarks about you know potential avenues for future research so that's that's that's what I'm going to uh that's going to be the next 50 minutes okay something that I should say is that many people many many people have contributed to our understandings of a synthetic control methods and related methods like a I don't know in economics like a uh with the imbalance Victor Sherman Hospital Bruno Furman uh Jesse rostin and colters and in statistics Abby Feller Adobe play and cultures in computer science you have Deborah prasano his group in the in MIT and you know that literature is very large and I won't have time to to cover all this and I will basically going to concentrate in the original formulation of synthetic controls although they are like a like a many you know important extensions many important uh like a related methods that they you know are out there in the literature and of course there are many many empirical applications that I will not be able to cover okay so let's just start about the you know what synthetic controls are about synthetic controls are based on the observations and when you have as I described before you know a few aggregate units are the units of the analysis then a combination of comparison units what we are going to call a synthetic control is often going to do a better job reproducing the characteristics of the treated unit than any single comparison unit alone okay I'm motivated by this consideration a synthetic control is going to be selected as the weighted average of four potential comparison units but we are going to choose this average in a way that bears resembles the characteristics of the treated unit and when I say characteristic of the unit train thinking about uh you know like a predictors of the outcome for interest you will see you will see in a moment okay so like to formalize this concept so let's say introduce a notation support suppose that we have like a j plus one unit say t period uh without a with a loss of generality unit one is exposed to the intervention of Interest after period t0 and you have like the remaining J units that are untreated and this is what we are going to say there are you know an untreated pressure about a potential controls we will call them the donor pool this is the set from which we are going to choose our potential comparison okay or at the comparison for for the created unit and this is going to be a framework of potential outcomes like we will have an yiit and why nit these are going to be the outcomes that will be observed for Unity with hand without the intervention respectively and what we aimed we aim to estimate we need to estimate the effect of the intervention of the on the treated unit in a post intervention period and this is going to be of course as always the comparison between the two potential outcomes you know the first one is easy without going we observe we don't have to estimate anything there but the second one is complicated and all our effort is going to be concentrated in you know in in estimating this um as the second object so as I said before I think that the control is going to be a weighted average so each potential synthetic control can be represented by a vector of weight okay one weight for each unit in the North Pole and you know all the weights are going to be like a non-negative and they are going to sum to one and we do that to avoid to avoid extrapolation although this restriction could be relaxed and in fact they have been relaxing in the literature okay so how are we going to choose among all these like potential synthetic controls well we are going to you know like um specify for Vector of M characteristics for the treated units and again these are predictors of the outcome variables interest is K times one vector and we are going to have a you know a matrix with the same the same values for the same characteristics for the units in the in the non report and our Vector of M you know synthetic control weights is going to be chosen to minimize the discrepancy between X1 the characteristics of the treated unit and x0 double due to characteristics of the synthetic control like a subject today with constraints that we have here that we described before okay and now once we have this W star like um uh the estimation of the treatment effect in the center control framework is is easy like your your estimate is going to be just the outcome for the treated unit and minus the outcome for the synthetic control and and you know this comparison you can do it period by period and like in after the intervention okay in the previous slide I use a a norm but I did not Define it and in fact you know there is a lot of flexibility in in what we can you can use there but most people uh use like some type of weighted euclidean Norm in which these weights here these constants are going to reflect the predictive power of each of the predictors and this uh this this constants V1 to VK that are you know like these weights for each of the predictors can be chosen by the analyst directly or Okay can be chosen in like a in a data driven way so let's look at the first application of a synthetic control estimation this is an application to the effects of the German unification on GDP per capita in West Germany so what we see in the figure on the left hand side is the series of GDP per capita for waste your money before and after their reunification in 1990 and we also say see the stain series for a sample of oecd countries and something that you notice here like almost immediately is that a here we don't have the usual conditions that we want to have to apply something like difference in differences we don't have parallel Trends in fact the trends and you know before the the German reunification for we share money and for other countries in the oecd are quite different however what this the the figure on the right hand side shows is that there is a combination of countries in the oecd that very well reproduces the trajectory of GDP per capita for waste your money before the unification and as we will see later it is also going to reproduce to other characteristics other predictor of the outcomes for way share money okay so like you can see the match is a is very very close and I guess as some of you may be thinking like okay you know what these guys did they they put all these all the values to get the feed that good you probably put all the values of the outcome variable in the pre-intervention period you put in all of them into X1 okay in order to have this fit and in fact you know we didn't do that we did something very different we just took you know the average over 10 years before the intervention and that's what we put in the X1 okay and then like other other predictors okay and and but you can see what when you fit this average you feed the whole trajectory very well and why is that well if you look at the raw data you can see that there there are some countries in this sample that they are moving very nicely they're moving very strongly and this core movement between countries is exactly what the synthetic control is trying to is trying to exploit and that's why we can fit uh this series you know quite well okay so what is X1 in this case so we have here X1 these are like a predictors of a you know economic growth that are typically used in the literature and these are the values that we observe for ceremony and these are the values here that we observed for the oecd sample and you can see that hey you know what German is not a is not a very close uh to the average of the oh the sample even when we are already using a sample of industrialized countries okay but however what we can see also in the second column is that there is a combination there is a convex a combination of weighted average of all these countries in the oecd that very very closely reproduces the characteristics of a ceremony okay and this is going to be our synthetic control this is going to be the comparison that we are going to use and if you are thinking about so what is how does what is this synthetic control for this for this problem so here we have like a you know like he said perhaps not pretty surprisingly as Austria carries a large weight also like a Japan the Netherlands Switzerland and the United States they carry positive weights and the rest of the countries have zero weights okay so like a as you can see this is this synthetic control is a sparse this is something that is a typical of synthetic controls and we will see why later and which makes the interpretation of synthetic controls particularly particularly okay so what are the properties of these estimators um one could ask to establish the properties of the estimator we are going to adopt some type of you know benchmark model for the uh for the general like a burst mode generative you know some model that is going to generate the outcomes okay of interest and by the way this is only the potential out Congress the treatment that I said before the outcome with treatment I observe so you know I don't have to you know I don't have to assume anything about it my only assumptions are going to be about that you know what happens with the treatment and in this this has you know simple linear Factor model in which the outcome is going to depend on characteristics of the units that we observe and characteristics of the units that we don't observe and they come with them with coefficients that vary with time arbitrarily and also we have like a an individual transitory shock Epsilon that we are going to model as a as random noise and suppose that we can choose W star such that you know like hey and we can you know reproduce the characteristics of the critic unit and also we can reproduce you know there and then the trajectory of the outcome for the treated Unity for intervention supporting fact that we can do that for you know like any um value of action and in practice they say you know like him he said this um restriction may only hold a approximately then we can establish a bound on you know like um on on the bias of the synthetic control estimator and here is the for completeness I have here like all the elements of the bound and like the case this is a complicated subject but I only want to focus on something that we have here inside this Max we have these ratios and these ratios are you know these two objects and these two objects are related to the scale of the transitory shocks Epsilon and in the denominator we have like a functions of a d0 that is the number of pre-interpretation periods and remember that you know to get this bound uh we need this to happen okay fine so what are the implications of all these what are the implications of this bound to the bias well the first thing that we have to remember again is that this bound is predicated on closed feed and you know it is so it is controlled by the ratio between the scale of the transitory shocks and the number of pre-intervention periods that means that the credibility of synthetic control is going to depend understand to which you cannot uh you know approximate the trajectory of the outcome variable for the greeted unit for an extended pre-intervention period so t0 has to be large you know the balance of action cannot be very large otherwise you know they are not going to be able to fit this trajectory there are no extante guarantees on the feed like sometimes you are able to fit sometimes say the unit may be very stream and you are not able to fit and if the feet is poor like we will recommend against using the CCTV controls or at least against using the you know this vanilla version of the synthetic controls there are some things that you can do about it like bias construction that I'm not going to do I won't have time to to discuss uh today okay we have to be aware that you know settings perhaps with a you know a small number of intervention periods large number of units in the in the North Pole and large noise like a large body of Epsilon they create substantial risk of overfeeding in the sense that you are not fitting the outcomes because the the characteristics of the units the CI and the mui are similar you are fitting the outcomes because you know like you have an operation in the epsilons and you know from this variation in the epsilons uh you know like a uh you know you can fit you give me enough noise and I can fit anything okay so to reduce to to so what we are going to do in this setting I think like a one one good and um piece of advice is to restrict the Donald pool to units that are similar to The treated unit as we did for the German unification example we you know like um look only at United countries in the oecd for example you wouldn't want to feed you know like um a concrete like a middle-income country with a you know a country that is like a very very um very very rich in a country that is very very poor because that will create interpolation biases but also like hey you may you know like be aware of the risk of overfitting and you know having like a lot of units in the North Pole uh you may be risking like head to insulin number 15 and therefore having like some bias so what about a you know like inference um inference in this in this setting like inferencing for the synthetic control method is Complicated by the fact that we are estimating the treatment effect typically for one unit so a standard asymptotic tools do not apply in this context and to address this problem we are going to base our influence on on implementation on permutation methods so like basically what we are going to do is like we are going to create a permutation distribution by iteratively reassigning the treatment to all the units in the North Pole and estimating you know these Placebo effects in each iteration and then the effect of the treatment on the unit that is affected by the intervention we are going to state that this effort is significant when the magnitude of the estimated fetish strain and you know relative to the permutation distribution this is the way in which we are going to do um you know like inference there are other ways to do inference in the control methods but again you know like I'm I am looking today only at the original formulation for example like a Matthias and rotio in fact have lunch on um a very nice work on like doing other modes of inferencing synthetic controls okay so let me uh you know try to explain with a sample how this like a permutation inference working practice so here we have like um uh our test statistics you know like a computed for we share money and completed for you know all the countries in the in the North Pole and we can see here is that the question is very very strained you know in this distribution and by the way and if you look at the statistic it's not just the treatment effect that we estimated it's like um what we have here is the pause period or post intervention routine square root divided by the pre-intervention Roman Square why is this well you know when you look at all these units in the donor pool some of them are can be fit uh you know you can you can feed them very well in the pre-intervention video tension of then you can fit the notion one in the intervention period so that's why we are going to take this room in the sclero that is the difference between the you know like the the trajectory of the outcome for you know like a for the treated unit in the post intervention period and the and the trajectory of the synthetic control for that uh the synthetic control that corresponds to the street units we are going to divide it by the same difference in the pre-intervention period okay that will be um I will explain this and this idea um you know like a little bit more later okay uh strong remarks about this type of permutation inference the first one is that the clearly like looking at the permutation distribution is a much more informative than mechanically looking at p-values because you can see you know how far away you know the test statistic is relative to the uh to the um you know like a permutation distribution p-valuation only gives you the the ranking um the second remark is that depending on the number of units that you have in the North Pole conventional significant levels maybe unrealistic or maybe impossible right if you have like um 19 units in the pool plus one treated unit you know like um then you can hope for you know having a babe you know like a p-value equal to 0.05 otherwise you know if you have letters you have fever you won't be able to have to attain uh you know like 0.05 uh p-value which uh you know is something that some people really insist in doing and and something else that you know it is going to be I hope a clear in a in a second example is that the one-sided inference is a um of the more relevant and each of them uh you know like a more powerful we will see that in in this second example this is an example about um applying synthetic controls to study the effects of um approved in California in 1988 and what we can see here is like at the series of a per capita cigarette sales in California and before and after the passage of this regulation the passage of these laws that were was um is referred to as proportion 99 and we have also you know the same series for the rest of the US and again this is associated in which a you wouldn't want to do like difference in difference right the trajectory of the outcome Parable is a completely different for you know California and the rest of the US if I do difference in difference between this point and this point in time I'm going to get something but a you know like there is no there is no intervention there okay however like as for the German ramification example it is a combination there is a weighted average of other states in the United States that is able to reproduce the trajectory of the consumption in California before the passage of proportion 99 and also other um I won't talk about this but also other predictors of the of the consumption and as before we we want to say you know that these you know this what we have said there is the effect the effect of the intervention okay so what we are going to do in order to understand the inferential procedure a little bit better it's like we are going to plot all these differences we are going to plot all these difference in time between uh you know California and the synthetic version in California and if you do that you obtain this graph right like we have pitted California very well you know in the pre-intervention period uh there is a gap um in the post intervention period okay so what is the permutation you know like inference based on the is based on the idea that now we are going to go to our data and artificially we are going to reassign you know the treatment to the other units in the in the North Pole and see what happens okay so here we are these are all the synthetic controls for all the units in the North Pole and something that you see here very clearly is that I'm not very surprisingly is that there's some units that they are so extreme that are that the stream of the distribution of the outcome that you can that you cannot that you cannot fit very well okay like um double consumption in Kentucky is these two height and to go to a consumption Utah is too low to be so that we can reproduce it used in a weighted average for their you know double consumption of regions so while while all of them and you know the um they estimate for California is to really extreme you know in this permutation distribution the comparison is not even fair right because it's like a you know we are you know including here like um units that we cannot fit very well you know like before um before the intervention okay if we take those away let's do this now we have like um only units that we can fit quite well like a unit at the minus Square prediction error the difference between the you know the you know the outcome variable for California and for the synthetic control um you know like it's not more than twice you know the sorry the units with minus where um um you know that is not twice the minus square up for California so the mean Square here is the difference between the outcome variable for California and there was sorry that was comparable for the treated unit and the alcohol variable for the photosynthetic control so we are only keeping we are only keeping those at a we fit almost as well as California again you can see that this you know California is is a stream in this distribution and you know in you know you know this um two-sided um sorry one-sided the inference is useful in the sense that you know if there is like a negative threat for California you know California may have been a you know like a in the synthetic control for you know a positive intervention creating positive fats and you know like adding to you know our statistical power if you don't like a uh this business often you know like taking away like a units that you cannot fit very well before the intervention you can do as before like looking at some Russian between the post intervention fit and the pre-intervention fit and you can see for this example California is like a very very extreme in this distribution okay so um let me talk about the you know a couple of remarks about um this permutation method the first one is that then it is the availability of a well-defined procedure to to select the comparison unit what makes this uh uh the calculation of all these plastic interventions visible if we are going to choose a comparison based on uh you know subjective measure of affinity between the units it will be very difficult to reproduce that when you have to do it for you know like in many possible interventions the second um point that is important to notice is that the permutation method that we we describe now it does not attempt to approximate the sampling distribution of the test statistics and in fact like sampling inference is going to be quite complicated in a synthetic control setting um many times because you know we have the atoms of uh you know like well-defined sampling mechanisms and often because you know the sample is the same as the population so it is very difficult to think about you know what sampling inference means um you know in this setting at least like again in this original formulation of the of the synthetic control model okay um many of you probably realize that this type of inference is going to reduce to classical ornamentation inference if the intervention is actively randomized but again remember that we are talking about a setting which you you have like a you know aggregate units like regions or countries and like a randomization in the settings is quite a um you know improbable it's like it's something that it doesn't happen like in general but more generally we can think about a this mode of inference as you know evaluating significance relative to a benchmark distribution um for the assignment mechanism one that we can Implement directly in the data and in what I have shown before this is a benchmark distribution is uniform but it doesn't need to be uniform like a you can you can use like um you know you can use departures from uniformity and you know like a serious Point possible and have like a paper about this in the in the Journal of a cultural inference many times people ask me like um why don't you use something like a propensity score made of and and that's kind of a you know like a complicated because it is a in this settings with a great unit it is often difficult to articulate the nature of the assignment mechanism uh or even the specific nature of the placebo intervention right I was uh I was discussing yesterday in like a in in another meeting about you know like think about the you know what would be a passive intervention in the you know German ramification example like a you know France is going to get reunified with whom exactly right you know like these um inferences relative to you know like a benchmark distribution but it's not based on active ornamentation in the field okay so like and now I will try to think about or and discuss what are the advantages of this entity control methods why and why me we may want to use them and you know in order to illustrate the advantages I'm going to compare synthetic controls to operational analysis that is a you know very familiar estimation framework and in fact you know years ago when we started talking about synthetic controls and we will go places are you know they to present this this work people will ask hey why don't you just like a run a relation and and instead of doing this thing of you know like a you know like a combination of waves at some to one uh can you do this using relation in fact you can do distribution progression right and then you know we have it here like you can take you know all the outcomes of the units internal pool in the post intervention periods these units are not are not treated in the post intervention period and we are going to regress them on the characteristics of these units and I put a bar because I have augmented this this metrics of of a characteristics uh with the road one because you know I want and you know I'm going to have like a an Intercept in my regression as usual but you know like once I have that I know how to you know estimate regression coefficients you did with the with the usual formula okay and each each column of this is going to be like the you know the regression of coefficients for one of the post intervention periods okay and now how how am I going to use this to calculate a regression based estimate of the counter factor of interest that is what would have happened with the treated unit in the post treatment period without the intervention so that's going to be you know okay you know my usual like a regression estimate that is going to be uh very hard to multiply by you know the characteristics of the unit okay so now something that is easy to see um given the formulas that we have here so far is that this regression page counterfactory in fact looks very very much like a synthetic control it's also like a combination of the outcomes of the untreated units in the post intervention period okay and you could calculate the weights at this weight you know we usually don't do it when we run a regression this way but you could calculate these weights explicitly and given the formula that we had for these weights you can also prove that this weight sum to one exactly like a for the synthetic control so what is the difference between you know like doing this with regression doing this with the controls what the difference is that this whites here there may be outside of the zero one interval and we know that we should allow that because you know integration extrapolates is going to extrapolate to fit the values of the characteristics of the treated unit okay it cannot be only based on interpolation okay and and also as we will see in a moment these weights are not going to be as far okay so a nice exercise something that we can do is to do to calculate these weights for one of the and you know like examples that we you know consider so far and you know these are the weights for the for the example of the German ramification these are the relation weights remember the synthetic control weights where is parsley where all all of them they were positive and Men sorry no negative and most of them were zero here you have like um and you have like some of them that they that are negative and this is like a little bit more difficult to interpret or much more difficult to interpret uh you know the nature of this uh estimated contrafactory because now perhaps you know the the contribution of Austria perhaps is moderated by you know the negative weight of a Italy or Greece okay and it's very difficult to figure out what's going on here and that's the cost that you pay for not having a sparsity so what are these advantages of a synthetic controls the first one is like a no extrapolation like a synthetic controls are preclude extrapolation by using weights that they you know are in this interval in between zero and one uh the second one is transparency of the feet like linear regression is going to extrapolate so you know you can show that you know it's going to fit the characteristics of the treated unit um perfectly even if the untreated units are completely similar in their characteristics and they are very far away from the critic unit in contrasting that the controls are going to make a transparent the actual discrepancy between the treated units and the compact screw of the units in the con in the donor pool also unlike an introduction analysis we don't need data on streaming outcomes in the same in the design phase of the study when we are trying to calculate this Vector W star to calculate double star we don't need any data on outcomes after the intervention therefore all design decisions like we can make all of them without knowing how they are going to affect the conclusions around the study not only that like this synthetic control weights can be calculated and registered before even you know post intervention outcomes are realized or before the actual intervention takes place okay and this provides a safeguard against you know specification searches and pick hacking something that they you know is worrying many people uh and now okay at the last two points are related the first one is transparency of the counterfactual synthetic controls make explicit the contribution of each comparison unit to the counter factor of interest you know I said you know weighted average is easy to interpret especially when it dispersed because um because the synthetic control coefficients are proper weights on the edge bars they allow you to have a precise interpretation of the nature of the estimate of the counter factor of intersection of potential biases for example if you think no no I think that outstream father was affected by the general unification you can immediately see how this is going to bias your estimate okay so um we have him mentioned a couple of times that a you know synthetic controls um RH bars and here we have the explanation like a a geometric interpretation of the ab sparsity you can think about a you know synthetic control what is this acidity control is a projection of X1 you know on the convex hole of the characteristics of the of the units in the North Pole and they like that we obtain the the synthetic control so if X1 does not belong to a compact school and this is typically the case because of the course of dimensionality then the synthetic control is going to be unique and expressed effects One belongs to the convex hole you know the synthetic control may not be unique and you will have candidates typically you will have candidates for a university control that may not be a sparse okay you typically you will have a infinite number of solutions although we know that you know like a data sparse Solutions among them okay and you know the question is like how we can decide among all these many solutions and and you know can you know can we get a one that is optimal in some way and this is the shop demand solution going to be as parse as before and this is related to one of the uh like a new techniques in the synthetic control literature which is called like a penalization to the control estimators and penalizing that the control estimators are going to be uh very much like the usual synthetic control transmitters but they are going to add this term to get the function and this term is penalizing the discrepancy between the characteristics of the treated unit and the characteristics of each of the unit that contribute solution to the controls so when this Lambda is positive like a it's going to control the trade-off between fitting well the treated and minimizing per wash machine discrepancies between the treated unit and the units in the in the synthetic controls if you take this estimator you calculate the estimator and make it Landa you know like going to zero and you know the resulting estimator in fact can be calculated um you know without an approximation that they're like a mathematical techniques to do that this is what we call the synthetic control and this is going to minimize the pair was much in discrepancies among all solutions for the Imperial estimator and not very surprisingly when you know if you make Lambda go to Infinity then only the pair was matching discrepancies matter and then you will you will get the nearest neighbor time matching estimator that would be the solution and so what is the what are the advantages of the penalization to the control estimator for Lambda bigger than zero the solution is going to be unique and and you will see that it's going to be sparse and and then there is we will see in an example that is going to be a sparse although you know this is like a general mathematical threshold that I'm not going to discuss and you know the presence of the pronunciation term is also going to reduce interpolation biases that of course when you you know like average units that are very far away from each other and you can show that has the same computational complexity as the ASM penalized estimate or so like a is not more complicated to to estimate to understand how the penalization that the controls Works in practice instead of giving you the formal result you know I I would like to and you know think about you know a very very simple example this is as simple as it it can get I mean like we have like a one treated unit three untreated unit a unit a unit unit one is a specific unit we have three untreated unit and X is one dimension we only have one dimension okay and so you can see here that the that the I'm penalizing that the control estimator has two spark Solutions the first one is the one that the you know your thing when you use X2 and X3 uh to interpolate X1 and they in the second one you obtain when you use X2 and X4 uh to interpolate X1 okay but you immediately see that you know the first solution dominates the second solution in terms of the matching discrepancy okay and it also is going to dominate an infinite number of known as part solutions that you obtain from convex combinations of The Spar Solutions okay but if you look at the original synthetic control estimators like all this infinite number of age Solutions like a look at the same I mean like in terms of the getting function okay we need a way uh to uh to discriminate among them okay however when you look at the penalization to the control estimator as long as the is going to have a unique solution and this is going to be yes parse in fact you can see that it never it never uses that much X4 okay you never use them as much a Slumdog goes to zero it converges to W1 start as Lambda goes to Infinity it converges to you know giving all the way to X2 that is the nearest neighbor estimator that's the way that the way this works in in practice okay so this was a um you know like a little bit of a short summary of a like a of one new development and like in the literature of synthetic controls and you know I want to talk about the second one and then the second one is like a using scene that controls for experimental designs and you know what is this about let me try let me let me try to explain this with a sample suppose that a you're in a ride sharing company like something like uber and you are thinking about him you know like um a different Compensation Plan for um for uh for workers for drivers okay and you think okay so like we have the the the usual composition scheme and a new one perhaps with higher incentives to drivers and which one we are going to use here in the you know business analytic unit of this of this company so uh something that you could do say is like okay so I will go to a city and you know I will you know assign some workers and you know like to the new compensation plans and some workers today all Compensation Plan and see what happens but as we described in the first slide this is going to be problematic because you have like a workers in the same cities like the you know like getting different amount of money for the same work and this in and that will be considered unfair and be some Fair also like the problem is that you know like if if um if um Rocio is like in the high Compensation Plan and in the low Compensation Plan probably trophy is going to work longer hours uh because half High has higher incentives as you may see maybe still in business for me so you know there will there will be all these type of interference issues that we that we uh you know make care about when um when doing an experiment so what we may decide is like no no what we have to do is to to you know to to deploy it or or or treatment at the level of the the entire Market okay and perhaps we cannot we don't want to do it like um you know treatment for 50 of the markets in the United States and no treatment for 50 of the markets in the United States that could be super costly perhaps and also like it will be like um you know a little bit of a nightmare if you know like if if you recite to roll uh outage you know or if you decide sorry if you decide not to implement a the the better Compensation Plan at the end of this experiment right so you may decide okay so I'm going to I'm going to apply to apply a pilot program in one market or in one city or any few marketing cities and I'm going to try to see what happens there okay so you know like a in this case you don't want to randomize okay and you don't want to randomize if you because if you are going to you know apply this program to one city like you know the city that you can get maybe one that is not a you know like a maybe not even not representative of the like a entire market for the United States or you know maybe one that you cannot reproduce very well that is you know like especially idiosyncratic and then you know randomization will give you like unbiased net section but I suppose you may be like in big big trouble so like you want to like use your experiment you want to do your intervention but it's not a non-randomized way okay and the question that you are going to ask yourself is which Market of Market should I treat and which Market or Market should I use as a comparison um or or control okay so like what they you know we have proposed in this setting is like a using static controls first you find a set of weights uh that they that make a synthetic unit that reproduces the characteristics of the population of interest for example like all the cities where I have a you know like a where my company is operating in the United States and then you're going to create a synthetic controls uh for the for the photosynthetic treated unit okay and and in contrast with the observational case here we are going to have like a two units once in the treated unit and one synthetic control units okay and you know like a these two objectives you can you know like operationalize like putting in a like in a nice like a objective function that you uh you need to maximize and this is what's going to give you like a optimal designs uh in synthetic control settings when you are doing experimentation no randomized experimentation with a great units and you know we have some work on this and also like a nickel chain Concours have been working have been working describe to techniques and I have to say something about this is that this is kind of new to Academia but it is not new to Industry I like many many like many many companies are applying this type of um you know synthetic control designs for experimentation exactly because of the type of issues uh that we were talking about in the previous slide on this slide and then what we are trying to do here in this work is I try to formalize uh you know many of the themes that are already been applied um you know like um in industry in in Academia sometimes we have like less capacity of randomizing our across bigger great units okay but but sometimes happens like sometimes we do that in like a with villages and so on okay so um I I have some time but you know since that I went there fast enough so I'm uh I'm you know like a ready to to to to to give closing remarks and what I've been trying to say is that synthetic controls like you know is you know they have gain popularity but the reason that why they have gained popularity might be used because they have a many practical advantages are designed for the estimation of three benefits especially in settings where you have like afia grade units some of them are you know exposed to intervention of interest and and some of them are not okay um I didn't have the time to talk carefully about a what are the requirements that we need for synthetic controls to work in practice uh we saw some of that when we talk about them by spawn formula as I said before like for as for any other statistical procedure and especially those uh that they that want to estimate Council Effect The credibility of the result that you're going to obtain depends very much on the on how diligent you are um you know like I applying these methods and you know the the type of data that you have and and contextual a characteristics of your problem that have to be that have to be met and you know uh and you know like I I have a paper a recent paper in the general economic leadership that covers uh cover all these aspects in quite a lot of the detail um so what are some open areas of a of research for synthetic controls like well it is quite a new method so there is a there is a lot that has not been done and then you know like some people have been working on a sampling based inference although like in my opinion there's still this um they share quite a big open field in in terms of a potential Improvement uh not much has been done in terms of external validity sensitivity to Modern restrictions uh there is some work with estimate estimation with multiple interventions uh and that also very interesting set of issues and also like a much more coordinates dividend on data driven selectors of BH you remember this where the the weights that we put to to the variables to the predictors that we are using for estimator and other things like mediation analysis and you know um you know like there is very little work done unfortunately controls only something that they uh my guess is that is going to happen in the next in the next few years and then and also another area of this favorite of mine as you could see before that is a of a height interest is the use of synthetic controls as a you know experimental designs and then there are like um many contexts especially like in industry but also in Academia where we need to you know like a carry out experiments at an aggregate level and we lack a lot of tools for doing that like most of the tools we have today is like for Education which we you know have many you know micro units that are created on many micro units that are not treated and then you know there's not this is not exactly the case and this is interesting because I make also like a this a case in which um the Avail the availability of them of a methods May restrict what you do in practice I mean like sometimes people tend to do what they what we know how to do and and and these are setting dishes are you know like a new set of you know potential um you know experimental designs that could be used quite a lot in practice um awesome computation and you know this important like a um we need many more results on robots and efficient computation of synthetic controls and then and then the good news about them about the this methodology is that on the empirical sites many many of the events samples interventions that we care about do take place at an aggregate level and affect the entire aggregate units so there is a lot of um you know like a you know many possibilities like many potential empathical settings where we can apply uh these methods so let me finish looking a little bit at um some resources that I have used in this presentation on some extra resources that they that you can get that you can use for synthetic controls um comes from my own work and and um and this is um and you have the reference here and in particular you know like it comes a um very directly from a recently um papered a in the Journal of economic literature on um on a you know feasibility data requirements a methodological aspect of synthetic controls and there is also like a uh some information here about it you know like a code for um you know like a calculating three control estimators calculating the penalizing the control estimator also calculate the penalizing the control estimators and as I said before like a many many people have contributed to the and to delete return synthetic controls and um and you know I wanted to provide some references here so like you could you can you can access this uh this material and I hope that these slides are going to be available after after the presentation okay and that's all I have thank you very much 