Jeffrey Wooldridge: All
methods and probably I will stop more or less at the usual time before I say something that I
regret at the end. I'm just going to run through some fairly basic stuff and then talk about things that are a little more recent and dealing
with endogeneity and some panel data issues with when
doing quantile regression, including least
absolute deviations. Let me just start
with some guess work. What I would call reminders about means and
medians and quantiles. This is often introduced
in the context of a linear model with of course some covariates
and then an error term. I want to focus on the case
where the error term here has a finite variance so that even though the distribution
could be spread out, it's not so spread
out that there's an infinite variance
in the error term. If we ask a basic question, because certainly least
absolute deviations is a method that has become
more and more popular, at least often as a
supplement to least squares, if not as the primary
method of interest. It's important to understand when we should expect
these to produce similar estimates
and when we should fully expect them not to and
not be worried about that. Of course, the
least-squares solves minimizing the sum of
squared residuals and two, and least absolute
deviations minimizes the sum of at least absolute
errors in three. When should we expect these to provide similar estimates? Which is basically
just saying when are the population parameters that solve these two
problems the same? Well, there are two cases
that are important. The first is when the distribution of u
in this equation is symmetric conditional on x and it might as will be
symmetric about 0 then. In that case, of course, the mean and the median
are the same and so we would expect at least under the case where there aren't influential
observations, that these will provide
similar estimates. Certainly, the
population values that solve these two equations
in the limit are the same. One condition is symmetry, the other case, and this, of course, allows the
distribution of u to depend on x. There could be
heteroscedasticity here. It's just that the expected
value of u given x is 0. The meeting interview
given x is 0 and so OLS and LAD have the same probability
limits in that case. Now, another case is
where this error term is independent of x and we have
to have some normalization. Let's just say the mean is 0, we couldn't say the median is 0, the point wouldn't change. If the error term is actually
independent of the x's, then the OLS and LAD estimators both
identify the slopes Beta. Of course, the intercept
will generally be different because if you have an asymmetric distribution then the intercept for lab will converge to some Alpha
plus eta and of course, the OLS estimate of the intercept will
converge to Alpha. The point of actually
going through these two cases is
that in lots of cases where one is interested in doing least
absolute deviations, you suspect that neither of
these assumptions is true. Independence is quite strong
because at a minimum, it rules out heteroscedasticity right in the error distribution. Symmetry is quite
strong also because often one of the reasons we apply least absolute
deviations is because we think the effect that the mean and the median might be different
than the mean effect. It's not so much that
when we find differences between OLS and least
absolute deviations that we should conclude that, oh, there must be an outlier
problem or something. A perfectly sensible
explanation is that the conditional median
and the conditional mean are different and they're
different in a way that's more than just this additive
constant on the end. This is a basic point but again, the focus often is on resiliency to outliers,
and then somehow, if you find a difference
between these two, that means something is
wrong with OLS when in fact, it just might be that they're approximating two
different objects. OLS of course is trying to
approximate the mean and least absolute deviation is trying to approximate
the median. Of course, neither of
those might be linear, in which case we still
don't expect them to be similar because they're
approximating different things. Of course, least absolute
deviations is more resilient to outliers or
changes in extreme values, because while the median is certainly a lot
less sensitive than the mean is too to such changes. That's just really in some ways, a consequence of the object that we're trying to estimate, the median is less sensitive to changes in the tails
of the distribution, and the mean, of course,
can be quite sensitive. That's just one point that certainly sometimes the
language and applications seems to be that it's really
just an issue of being resilient to outliers which is why people use least
absolute deviations. If you look at other so-called water-called robust
estimators that are intended to be insensitive to outliers. Virtually all of them do rely on a symmetric distribution
because they might do something like use
a squared residual up to a point and
then actually flatten out so that you don't give any additional weight
to large residuals. Again, those are
almost always derived under the assumption of
a symmetric distribution and so they're
robust in the sense that they're insensitive
to outlying data. But they're not what we would
call a robust estimator of the mean in general because they impose symmetry and of course, to estimate a conditional mean, we don't need a
symmetric distribution. Now, just a couple of other things that
will come up later. There are some advantages that medians have over means and some of these
carryover to quantiles as well. Then there are some advantages
means have over medians. One big advantage of the median operator is that it passes through
monotonic functions. If you start with
a model like this, say this isn't the most
interesting example, but we'll come to
those later on. We start with a model
for log of y linear and assume that the median of u is zero conditional on x, then of course, the
median of y itself is just the exponential
of the median of log y. This is not true of
course for the mean where the mean does not pass through nonlinear functions. In this particular case, you can characterize the mean even though you can't
necessarily find it. It's the exponential of the
mean of log y in this case, if u actually has a
zero conditional mean. But then there's this
extra part at the end which in general would
be a function of x. This basic point was key in the early development of least absolute deviations
for things like censored regression
models that you couldn't characterize the
mean without knowing a lot more about the distribution of the unobservable or
is the median was easily obtained because the
transformations are monotonic in the
underlying variable. But let's not forget about
some properties that the conditional mean has that
the median does not share. Two of those that turn out to
be fairly useful generally are the expected value is a linear operator
and the median is not. The other useful
probabilistic feature of expectations is the law
of iterated expectations. There's no law of
iterated medians, which is unfortunate actually, that makes quite a bit harder to figure out sensible ways to do things like least
absolute deviations when you have panel data with unobserved heterogeneity,
for example. We'll talk about that
in a little bit. But just to show you an example where maybe we don't think about these issues as
much as we should, if we started with a general random coefficient
model as in six, again, this intercept plays the role of the
usual additive error, and then we have these
random slope coefficients. Well, if we made the
typical assumption, which is that the
heterogeneity is independent of everything, well, what would we need to do
to consistently estimate something interesting
by least squares, say, well, if under
independence, then we have this
expression in seven so that the
regression of y on x, of course, identifies
the average, the population average defects. That's very basic. Of course this would imply some heteroscedasticity
in y given x. But we certainly don't need
to even know what that is in order to consistently
estimate Alpha and Beta. If we just take the
basic linear model, we add this feature,
then clearly, of course, OLS is
going to consistently estimate something
interesting in that case. By contrast, if we maintain independence between the heterogeneity in x, you really can't do
anything with the median unless you add some
more assumptions. In this particular case, you would say, well, if you just wanted to say, the key assumption is that heterogeneity is
independent of the covariates, you wouldn't be able to get
very far with the median. There are, of course, assumptions that one
can add to make sure that least absolute deviations does estimate
something interesting. The one issue is how one goes about defining symmetry in
a multivariate context or what assumptions are
sufficient in order to be able to figure out what medians
are of linear combinations. One general definition
that has come up is, I think I guess I cut
that off on this slide, is this notion of a centrally
symmetric distribution. Suppose we have a
random vector ui and whether we condition on
x's is not that important, although in this
context it is relevant. The distribution is
said to be centrally symmetric if the
distribution of u is the same as the
joint distribution of minus u, conditional on x. This has the implication
that if we take, because this is
conditional on x, then if we take a linear
combination of these errors, which could be functions
of x's then this has a univariate distribution
which is symmetric. In particular then
the expected value, it'd be symmetric about zero. The expected value of u has to be given x has to be
zero in this case also. If we start with that and then rewrite the
equation in this way, and we assume that
the heterogeneity consisting of A and B
is centrally symmetric, then this additional term does have a conditional median
that's equal to zero. Then we would actually
be estimating the same Alpha and Beta that we estimate
with least squares. Now, we'll come back
to this because while this example is not
especially interesting, I think this notion of centrally symmetric is a
pretty strong restriction. Yet it's hard to
relax it actually and get any usable results
in this particular case. Methods such as quantile methods generally least absolute
deviation, specifically, they don't naturally apply to cases where you suddenly
take basic models with a single additive
error and now replace that with models where
they're heterogeneous slopes. Let's talk a little more generally about
quantile estimation. Remember, generally this is the definition of a quantile. If we focus on the
continuous case, then we'll just say the
probability that yi less than Q of Tau is equal to Tau. What does quantile
estimation do? We just talked about the case of least absolute deviations
which identifies the median. In quantile regression, in
the leading case where we specify a linear function, well, we specify a linear function and the intercept and the slope explicitly are shown there as being a function of whatever
quantile we've chosen. Of course this has become
quite popular so that we can see how changes in the covariates change not just the mean of the
distribution of y or the median, but how it affects various other quantiles
and the distribution. The estimation method, there's a function that
identifies a quantile. I was mentioning before when we pick an object
from a distribution, either a full distribution or a median or a mean or whatever, there's an objective function
that for which it is a solution to a population
optimization problem. In the case of a quantile, it's the so-called
check function, which is written right here. I should have put a graph, but this is just
a function which is linear on each side of zero. It's slope is Tau to the
right and it's slope is, well, in absolute value
1 minus Tau to the left. This is the thing that
identifies the Tauth quantile, conditional quantile of the
distribution of y given x. If in fact the
quantile was linear, then solving this minimization problem
will consistently estimate it by the usual arguments because this is actually
a continuous function. It has the kink at zero, but it is a continuous function. The general consistency
results applied directly. The difficulty is that it is in doing the
large sample analysis, although it's pretty much
been worked through. You can look, say, at [inaudible] book on
analog estimation methods if you want to be convinced
that in fact the conditional median does minimize the expected value of this
objective function down here. Estimation is a bit tricky too, it's not as if you can just use standard methods for smooth
objective functions. But these things have been
programmed up now and they're doable for
even large data sets. The asymptotic normality, the reason it's tricky
is because if you try to differentiate this
objective function twice, it's not defined at zero, but everywhere else the
Hessian has got to be zero and so the usual methods
of expanding out in a Taylor series expansion just don't work anymore. But if you like those details, Roger Koenker recent book has references and
lots of conditions under which this holds. Something that's
more interesting is you see lots of studies where a sequence of quantiles is estimated for different
parts of the distribution. Of course, in almost all cases those quantiles are all
assumed to be linear, and one might ask, could all the
quantiles really be linear functions of
the same set of axis? Well, under strict assumptions, yes, but generally no. This has been recognized
for some time, but there has been some
recent work actually characterizing what
quantile regression is actually identifying
when in fact the quantile is misspecified. This is an easy exercise
in the case of regression because we know that if we have specified a
linear function for, in the case of least-squares, then it's easy to
show that in fact, if the regression function, the population regression is generally some function Mu of x, then the parameters that
we're estimating are the best linear approximation to Mu of x in the mean
squared sense. This gives us a very simple way to interpret what we've done, we're just choosing a
linear function that best approximates the true mean, where best is defined in terms of minimum mean squared error. By the way, there are some
assumptions under which the Betas that we get from least squares could actually be either the average
partial effects of the true conditional mean or at least proportional to them. This is work that goes back to the mid 1980s by [inaudible]
and others trying to characterize when linear
regression could actually estimate something
interesting in terms of the actual
conditional mean. But those assumptions are
actually quite restrictive, it's more of an aside here. As I mentioned,
this formulation of linear quantiles as
an approximation has certainly been
recognized for some time. But other than that recognition, there hasn't really been
much formal available, and until recently, Angrist Chernozhukov
and Fernandez Val, have this nice paper where they characterize the
probability limit of the quantile
regression estimator when the true quantile
is not actually linear. Just to simplify the
notation a little bit, put the intercept back into x so I can just write
Beta here and then this is the true conditional
quantile function, q_Tau of x. What they show is that the linear
approximation obtained from minimizing
this check function asymptotically has the
following interpretation, it actually minimizes a
weighted mean squared error away from the
actual quantile function. There are actually
a couple of ways they characterize this problem, one is, this weighting function that depends not just on x, but actually on Beta as well. So when you minimize this over Beta you're
actually choosing the weights as well as the discrepancy between the linear
approximation and the quantile. But the weight function
looks like this. What it's really doing, this is the density of y given x, and that's being evaluated
at some point on the line between the linear approximation and the actual quantile. Then there's this
weight out front, and so this is being averaged over all possible values of u, where you're giving more
weight to points that are closer to the true
quantile function. This does give a
characterization of what's happening
in this case, whether that speaks to
you or not, I don't know, but it is interesting that
the characterization is in terms of a weighted
mean squared error. It's not in terms of
an absolute value, even it's actually a mean
squared error that's weighted, and you can look
at their paper for other ways that they
use this result. They provide a way to
interpret partialing out in a quantile
regression and so on. But in any case, this is interesting
to show that there is an interpretation to what quantile regression is doing
in the misspecified case. Now, in terms of
computing standard error, I can go through fairly quickly. Mostly, the interesting
thing is to know which standard errors are valid under which
kinds of assumptions. If we assume that the quantile is actually correctly specified, then the two matrices that have to be estimated are
this A and B matrix. This is always easy, Tau of course we've
chosen because that's the quantile and so
then it's just the x prime x matrix divided by n. This turns out to be the
hard matrix to estimate. The one, interestingly, in the outer product of
the sandwich in this case. If we took this part outside, instead of having it summed
in with x_i prime x_i, this would be just a
histogram estimator. But unless you're
willing to assume that the error is
independent of the x, you generally have to have
it inside the function here. Of course, we have to actually
choose this parameter, h_n, which is shrinking to 0, but not too quickly. People have suggestions
for how one should choose these
so-called bandwidths. The main point I'm
putting this up is, this is not entirely automatic, but probably you're just going
to use the ones that are reported by whatever software
you're using or who knows, you might even
want to program it up yourself, I suppose. There is an interesting point here and how this
works is very similar to why the standard Eicker-White
heteroskedasticity robust standard errors work. It's not that this
is supposed to be estimating the
expected value of, if you go back here, it's supposed to be
estimating this, and it's not as if each one
of those terms gives us a good estimate of that
part but together, it gives us a good estimate
of the expected value here. As I said, if we assume independence between the
error term and the x, which again is a fairly
strong assumption, especially if you're doing it across different quantiles, you can't really have
independent of the errors across every quantile except in
very restrictive situation. That's what the formula
turns out to be. Mainly I'm pointing this out because this is, for example, in state and this
would be the default, which is to actually act as if the additive errors are
independent of the x's. What happens in this case is, the density is estimated
using this histogram. Of course, you can smooth
this out if you want by using a kernel estimator
of the density at 0. But since these are just
going into standard errors, it's not clear that one
wants to go through that. If the quantile function
is misspecified, then this estimator doesn't
work even if we estimate A in this way, the resulting variance
matrix is not robust to misspecification of
the quantile function. In that case, there's a different estimate of
B actually that needs to be used and this has
been worked out and it's also a simple adjustment
to the usual estimator. So the most robust
estimator uses B-hat in this formula and
A-hat in the formula that I just had up there in 18. Hopefully, these will become options to that are computed routinely when you do quantile
regression that actually, I haven't checked this for
sure in the state of 10, but with state and nine, you only have the
option of using the completely non-robust version of the variance matrix. Han did work out
that the bootstrap works in this case,
although it doesn't, as implemented on the
non-smooth objective function, it doesn't provide the
asymptotic refinements for t-tests and
confidence intervals. Joel Horowitz has
a nice chapter in the handbook of
econometrics that discusses how one can
do that adjustment. Let's see, just to have some
numbers to look this is just an example generated
from a dataset that Alberto, [inaudible] used in
a case where he was actually allowing
some endogeneity of a binary treatment. That's not done in this case. But you can see
that these when you apply quantile regression to something like a
wealth variable, this is net total
financial assets. You get fairly different
estimates depending on what point in the
distribution we're looking at. You can see that
the relationship between financial
wealth and income for the average is quite
strong and it's still quite a bit less
strong at the median, at the upper quantiles
of the distribution, it's strong and then it's very weak at the
lower quantiles. Some interests that was
given to this variable, which is eligibility
and a 401K plan. With the idea being
that while this has a large effect on average
financial wealth, this is in thousands of dollars. It's very different depending on which part of the
distribution you'll look at. In fact, if you keep going
up the distribution, the effect keeps getting
stronger and stronger, so clearly eligibility in
this particular pension plan has a much larger
impact in dollar terms, certainly at the upper
ends of the distribution. Unfortunately these
standard errors, these are the ones
reported that act as if these quantiles actually are
all linear in these x's. What about quantile regression with endogenous variables? Well, certainly some
stuff is known but it's not as clean when we use zero correlation assumptions between errors and instruments. Suppose we start with a model and we just
write this model down y_1 is a linear
function as usual. In fact it's not entirely
clear when we start talking about
quantile regression or even least
absolute deviations, what we generally mean when
we say y_2 is endogenous, we're not necessarily going to define it in terms
of a correlation between y_2 and [inaudible] although that certainly
is one possibility. But let's proceed with this
and see where it goes. The MME as two-stage least
absolute deviation estimator was the first one, I think, to deal with endogeneity in least absolute
deviation estimator and it adds to this standard linear reduced form so if you just
looked at this and this, you would just think
this is the usual setup. The question is,
how are we going to estimate the parameters? Well, this estimator is
done well in two steps, just like you can do
two-stage least squares. In this step, either OLS or at least absolute
deviations is used and then fitted values are gotten by using
the z pi 2 hats. Then y_2 hat is plugged
in up there and then least absolute
deviations is carried out on, the equation with
the fitted values. The question is, when does this give consistent estimates of parameters in the so-called
structural equation? Well, certainly some symmetry has to be floating around here. The easiest way to think
of it as that, well again, bivariate symmetry
between u_1 and v_2 is certainly bivariate symmetry in that distribution is
certainly enough. Because what happens is
implicitly, of course, when you plug in y_2 hat, there's a linear combination of this reduced form error
that gets put in there. The weakest assumption
you're making is that there's a linear
combination of u_1 and v_2 that has a
symmetric distribution. This is the linear combination that is being plugged in there. Of course, you could
do this as via a control function
approach under the same symmetry assumptions by writing u_1 as this linear function of v_2
and then e_1 would have a symmetric distribution
conditional on v1 and the exogenous variables. Instead of putting in y_2 hats, you can put in v_2 hats and do least absolute deviations
on y_2 and v_2 hat. Actually, I'm not sure
that I've seen this done, but I shouldn't say that
this is just something that occurred to me after thinking about
control function methods. Presumably a t-test
on vi_2 hat to test for endogeneity is a legitimate thing to do in this case. Although the asmptotics, I'm not sure, had been
formerly worked out. It's the interpretation of doing least absolute deviations in these cases is a
little strange because if you start off with a structural model
and you want to argue that the reason y_2 is endogenous is because it's correlated with some
omitted variable. Well you have to assume lots of symmetry to get
back to this equation where this error term here is
symmetrically distributed. If you just started off saying
what I'm interested in is the median of y_1
given z_1, y_2, and say some omitted
variable r_1 and because what I'd
like to do is be able to control for this r_1
and see what the effect of y_2 is on the median
this conditional median. Well, of course, if
you are going to interpret Alpha-1 in
that way, in the end, then you're going to have
to have some symmetry of the distribution of
the omitted variable. It's going to have to be
some sort of joint symmetry with the omitted variable
and then the extra noise. In the end it's not especially
clean in the context of estimating models by usual instrumental variables
with omitted variables, we don't have to assume things like the omitted variable has a symmetric distribution
in order to make sense of what we've
estimated in the end. Now, in the case of a
binary endogenous variable, there's some additional
work available in this. This looks at the general
quantile regression case. Their papers by our body and our body ingress and embeds
that look at this problem. It's in the context of
estimating treatment effects. [inaudible] actually looks
at various features of a distribution And
the AAAI paper focuses on quantile
treatment effects. The treatment in this case
is binary and call it D, be consistent with
their notation. This is set up in the potential outcomes framework that actually it's closely related to the local average
treatment effects setup where you have a
binary instrument z, which is easiest, most easily thought of as
eligibility for some program. Then D is actual participation and then there are
potential outcomes y sub D in both states with and without treatment or without and with treatment. Then of course, there's the counterfactual
treatment states to that treatment without eligibility and
with eligibility. The usual setup here is the observed data consists
of the covariates x, the eligibility z, and then d, and the observed outcome y. They study treatment
effects for compliers. That is, you've already
seen this before. These assumptions are actually fairly close
to what we saw before. This is the unconfoundedness assumption for the instrument conditional on X probability of being eligible conditional on X can be zero or one and so on. This is the
monotonicity assumption that is used in a lot
of this literature. Under these assumptions,
they show that treatment is unconfounded for the conditional on X for the
population of compliers. In other words, this
is usual thing. The distribution of the
counterfactual outcomes conditional on
treatment in the X's and for the population of
compliers is unconfounded. That means that the
treatment effects can be based on this distribution, which is the observed y given X , given observed treatment. Then the problem is that we can't observe
this subpopulation, which is for the compliers. But for the moment,
let's assume that we can and specify a
quantile function for the population of compliers
and say that it's linear in the treatment
and the covariates. Now, this is one of those cases where to consistently
estimate these parameters, what you would like to do
is use a subpopulation. But in fact, you can't identify what that subpopulation is. You're stuck with this larger population
and the question is, how can you identify these when you'd like to be able
to condition on a subgroup. One way to think about this is to define a dummy variable, which is one for compliers,
and zero otherwise. You'd like to be able
to observe this. Then of course you could
just do quantile regression on the population of compliers
and you would be done. You'd like to be able
to solve this problem where this objective function g is just the check function. Then c would actually select out the sub-population of interests that you can estimate
this quantile for. In some ways I talked about inverse probability
weighting earlier. This is in some ways
the reverse problem. Is there you're trying to take a selected subsample and make it representative
of the population. Here you have a population, but you would like
a sub population. You can't do this because
c is not observed. But interestingly, they show that they can actually estimate the conditional probability of being a complier conditional on everything else
that shows up here, Y, X, and D. It's as simple iterated expectations
argument to show that this expected value is
this because this is just the probability
of being a complier conditional on all of
these observed variables. Of course, you could
always say, well, I can turn this problem
into this problem. But if you can't estimate
this probability, then it doesn't do any good. Well, as it turns out, this, it can be
estimated, I see it. There's a notational problem that should have to
be consistent here, that should have a v subscript
and so should it there, because this is all
the same function. Under the assumptions,
what's interesting is that the probability takes
this form where Pi and v are estimable
because these are the probabilities of
eligibility conditional on everything else you observe. This is the probability of eligibility conditional
on the covariates. Once these two quantities
are estimated, then you can take this
population problem and turn it into a sample
problem in this way. Where this of course just
ensures that you don't use observations where this estimated probability
is actually negative. You can imagine even doing this in a fully parametric way where these probabilities
that underlie this are flexible logits or
something and you can refer to the [inaudible] et
al paper to see how you do it in a
series estimation way. Now more recently,
Chernozhukof and Hansen consider also
quantile estimation. They start off with this general notion of a
quantile treatment effect. The representation they use
it's critical is that again, this is in this
counterfactual setting. Is that the counterfactual
outcome on why is this quantile function
and the randomness comes through this uniform
random variable, U_D, which of course can
change across counterfactuals. Z is the instrument that's independent of the
unobservables. The key identifying condition, I'm going to have to run through this to get anywhere close to even talking about the
panel data case is, or I should say the key insight is that under the
assumptions they impose, they can actually show that the quantile function
Q satisfies this. What's essentially a
moment condition actually, it's easiest to see down here. One way to think about this is, well, they're trying to
estimate this function Q. Now it depends on
observables D and X. Of course it's a function of tau because we've chosen
the quantile. This is the indicator function. This moment restriction
right here. If D were exogenous, I guess that's the
easiest way to think. If D were exogenous and so we could drop z and put D in here, this equation would essentially define the conditional quantile. This with Z replaced by XD, where D is exogenous. This defines the
conditional quantiles. One way to think
about their work is that they basically provide justification for taking
the moment conditions that would occur under
exogenous treatment. Justify using essentially the analogous moment conditions under endogenous treatment where they have an instrument
for treatment. This is actually two-stage
least absolute deviations. Certain versions of it can be
motivated in the same way. There's something that they call a quantile regression instrument
or variables estimator, which in special
cases actually turns out to be the same thing as
two-stage least squares. If you were to replace
these moments with the zero conditional
mean assumption. I guess I'm not going to say too much about the
panel data case here, but let me just highlight
a couple of things. If we just want to do quantile regression
on panel data and not explicitly introduce
unobserved heterogeneity, then of course, using the
data pool to cross I and T is conceptually
quite straightforward. We might have, and we might
put in time effects or whatever to allow the
quantiles to shift over time. The one point to make I suppose, is that just as when we
do least squares with panel data and there might
be extra serial correlation, we really should adjust the asymptotic variance for possible serial correlation
in the quantiles as well. There is an assumption under which we don't
have to do that, but that means that
the quantile function has the dynamics
completely specified. In general though, you have
to use this typical formula where you have to
account for all of the pairwise correlations
and the score. Where this is the score
function which acts like the first derivative
of the quantile function. This is the formula
that shows up quite generally in nonlinear models. I think David Drucker at Stata interested in
programming these up because now if you try to
do quantile regression with a cluster option or something in Stata, it doesn't happen. Let me just spend maybe
just three minutes talking about the problems with
unobserved heterogeneity. I think it clearly makes
sense even if we only think linear quantile functions are an approximation to reality, I think it makes sense to
specify models like this, where the quantile is linear and the covariates and then an additive unobserved
heterogeneity. Since the Thetas would have a direct interpretation as being partial effects
on the quantiles, it would be ideal
if we could come up with estimation methods that don't restrict this
distribution in any way. That would be analogous to
the so-called fixed effects estimator for estimating
conditional means. 