what i'm going to do is give a little bit of an overview of how i have used the hpc resources to work on my own stuff and then i'm going to turn it over to mao so structural estimation is a really broad field it has many different flavors in many different areas of economics what the industrial organization folks do who looks nothing like what i do and yeah oh do i have to stay still oh no that's very hard but i will try okay okay so in any case um it is a very broad field and i obviously can't talk about it in half an hour so i'm just going to talk about what i know which are full solution estimation models of dynamic models of the firm or of dynamic industry equilibrium models so high performance computing is extremely useful for these full solution methods for now not when i started but now one simple estimation can take can you can do on a workstation and so you can test code on workstations and that's straightforward what's harder to do is to create an entire paper on a single workstation it might because you have to do it serially it might take years and so i think that high performance computing has allowed me to do things i could never have done otherwise so here's what i want to do is i want to talk a little bit about hpc for estimation and just give an overview of the sorts of methods that i've used give you some examples and then i want to talk about a paper that i utterly could not have done had i not had access to super computers which is talking about the finite sample performance of these simulation estimators which i think is interesting and i'll shut up so these are the models that i look like so you have a value of you typically a firm is a function of two things w and z w is a vector of exogenous or excuse me endogenous state variables like w might mean net wealth and then you have a vector of exogenous state variables that are also stochastic and that's z and that's just a bellman equation and so this is the maximum over the choice variables which are w prime and then the continuation value of the firm so those are the models that those are the definitions and this is a this encompasses a very large set of models that you could possibly put into a framework like this you might add an industry equilibrium condition on there but other than that this could you you can put anything in this framework okay the solution is actually fairly easy to parallelize when i you know so i have a literature background and i know that sounds strange i have a literature background and i learned a lot of this stuff later in life and i thought it was fairly straightforward to paralyze so you take these variables w and z and you discretize them into a finite number of feasible points and then there are a variety of methods you can use you can use value function iteration which is mighty slow and extremely reliable which is nice because when you do estimations you have to solve models a lot policy function iteration which is much faster and sometimes not so reliable especially depending on how aggressive you are with the thing then there are polymer you can use polynomial approximations to v which is i've used that in one of my papers which is much faster and a little bit squirrely sometimes it just doesn't work at all especially in models of corporate finance where you have these variables that have where the solution has kinks and that happens a lot in finance applications and so that's very fast and a little bit squirrely um then how do you parallelize these things in when you're using compiled languages which are super fast you can use openmp which is extremely intuitive or mpi which is unintuitive and much usually faster so how do those things work openmp is just a set of compiler directives in the code it just looks like something that says please do this loop all at once instead of in a row in a perhaps slightly more compact fashion but that's what it says and what's important about this is that all instances of the loop can share variables in memory and so that makes creating the code very straightforward you basically don't have to break your code you just add a few lines here a few lines there and everything works out fine that's not a problem mpi is a much more powerful method for parallelizing things that does not require shared memory and what happens there is an entire section of code runs as many identical copies and they're utterly independent of each other so my example i came up with last summer when i had ants in my kitchen was the following you have a hole and you have 10 ants and they all come out and they go off to a bunch of sugar they each gra and they don't talk to each other they just go out to the sugar and then they each grab the sugar and then they come back and they don't talk to each other until they get back to the hole and so they and they're all the same they're all ants they're all doing exactly the same thing and then they come back and but they can share inform so these processes can share information with each other and you can make them do that and that's why it's called mpi message passing interface to do that you basically have to break your code because it doesn't look the same and then what i've been using to estimate these models is a simulated minimum distance estimator so how does this work you compute some statistics in actual data and i'm glad that the colors are working on this projector which is good um so this is there are some statistics and i've used this function h very generally it could be something simple like a mean or it could be something less simple like an ols regression coefficient or it could be something even less simple like an estimator from a duration model it can be anything just some statistic that can be computed from the data that has ultimately is formed when you by taking averages so then you solve and you simulate data from the model and you compute the exact same statistics on simulated data which is the blue and the simulated data are functions of model parameters and so if the model parameters are this the data looks like something and if the model parameters are that the data looks like something else and then you just try to get these statistics as close together as possible so that's the objective function and it's just the difference between the real data statistics and the model based statistics which i said are a function of the model parameters and you just try to get those as close to possible as possible and so you just minimize second line a quadratic form in the in the object that where w is this weight matrix i'll talk about that in a little bit not not in as much detail as i make my phd students suffer but i will talk about that in a little bit i've tried many many minimization algorithms first you can't use gradient based methods because these models have no closed form solutions and so taking a gradient means taking a numerical gradient which might not be accurate and the whole thing just blows up and it doesn't work and it's slow so you need to rely on these heuristic methods which many very often have never been proved to reach a global minimum but if you eyeball the thing look like sometimes they really do it could not get closer so there's something called multi-start melder mead so neldar mead's just an algorithm where you stick a triangle on the side of a hill and you just flip it in the direction that gravity would flip it until it hits the bottom of the hill and that seems to work very well but it's also mighty talented at finding local minima and so it's useful to start it at many different starting values that can be paralyzed but it's somewhat unreliable in the sense that it needs a lot of human hand-holding if you want to make it work and there's simulated annealing which is just a generalization of a metropolis hastings algorithm it um uses randomization it picks a candidate parameter vector and then it either goes up or down and does that many times it's extremely slow and usually if you set the tuning parameters right hits the bottom of the hill and of course the you know the longer it takes the more likely it is to hit the bottom of the hill and but it's it's very slow then there are two um types of very closely related algorithms that can be parallelized there's differential evolution and particle swarms they're both rel they both rely on the same principle as simulated annealing you start somewhere you take a random gas and then you move except with particle swarms and um differential evolution the guesses are many guesses at once and that obviously can be parallelized so that's pretty straightforward the only difference between these two algorithms is how they update at the end of each of these swarms of either children or birds so those are the and those can be parallelized i have found differential evolution to also be mighty talented at hitting local minima and i've had better better luck with particle swarms these are these last two are very useful for papers that with models that take longer to solve so here are two examples i have this paper on financial constraints and wages and so this is a model that has both endogenous default and an endogenous wage bargain and so it takes a very it takes a long time to solve where long is about five minutes and so using something like a serial method just wouldn't work and so using these parallelized methods works pretty well the other paper i have had is about money demand and it has many state variables in endogenous default and it also takes forever to solve so that using a parallelizable method is very useful i don't think either i could do either of these papers on a workstation there's just no way so let me give you some examples so this is what i started doing i've actually been using distributed computing since not but it's not been high performance computing since 2003. um when i was doing these two papers i used a bunch of old unused pcs in the uw madison plasma physics lab which were that was where my husband used to work there no one else was using them and so but this this was not high performance this was not parallelization there were not low latency connections between the computers i just had to trudge through the snow every morning and check to see what was happening and then restart some stuff so that's that's what i that's what i did back when computers were slower and that wasn't inefficient but it was the only thing that i had available and then a kind ph who prefers to remain anonymous it was not at the university where i was working gave me access to his account on a high performance cluster and we got a revise and resubmit on this paper and the referee said i don't see any empirical work in here and i thought that's all i do and so i thought well i'll show them empirical work i so i estimated this paper or this model on data from 41 different industries and i'll talk about why that might be a useful thing on the second slide and so what's on this plot is on the horizontal axis is average debt to asset ratios and each dots in industry and on the vertical axis is at is model simulated debt to asset ratios and nothing is statistically significantly different from each other so if this were an asset pricing paper no one would believe it but since it's a structural estimation paper and since this this particular moment is estimated incredibly precisely you get that the only one that's off is railroads that's the dot that's above the line so that was my first introduction to high performance computing and then this phd student told me about exceed and so then i applied and they gave me units and then i did more things the models that i use are of a single firm or of an industry with very limited heterogeneity across firms and so if you want to estimate the parameters from a model like this you it kind of makes no sense to estimate them on data from the entire u.s economy because there's no such thing as an average u.s firm what is that does that even make sense there is such a thing as an average chemical firm maybe that's an easier thing to imagine or an average textiles firm or an average wholesaler something like that so those are much easier things to imagine and i have found access to hpc clusters to be invaluable for this so imagine that estimating a model takes i don't know six days and you have 42 industries you can see how that would be just crazy impossible to do even with if you had access to many plasma physics labs so i so with um my paper with boris nikolov we looked at heterogeneity in the dimension of governance and my paper with misakawi looked at heterogeneity in terms of old economy new economy in my paper with yuffan and xiaojin we looked also at industry different industries and so this was both of these things were important aspects of the paper and would have been impossible without access to high performance computing and i think that from you know the in corporate finance we're really doing applied microeconomics we're not doing macroeconomics and so looking at microeconomic heterogeneity is something that is typically a very very interesting question and then here's the fun part this is the finite sample performance so all of the econometric estimators i use are based on an analogy principle so all these gmm estimators m estimators minimum estimate minimum distance estimators were all examples and of course they're all asymptotically consistent and efficient but there have been studies of these estimators that have looked at simulated data to see if they have good finite sample performance and the answer is often not so much so there's the actually the old ariano and bond paper has it does some of those muddy carlos even though they only have 100 trials because it was 1991 don't look so great then there's the autongi and siegel paper which is highly cited and tells us that things don't often work very well or the hanson heat neurons same thing there have been very few evaluations of the finite sample properties of simulation estimators just a couple and in very different contexts so not in the context of things that i look at which are estimating models of the firm so we did it there we go that we used that at least we started on that that's the texas supercomputer which actually has been taken out of commission and which i no longer have access to so there was really nothing for this class of simulated minimum distance estimators used in corporate finance and so we evaluated the finite sample performance of these things and so that requires estimating the model thousands and thousands of times not not 40 or 50 or 10 or 12 but thousands and thousands of times and how do you do that well one use a relatively simple model that can that can solve fast and two use high performance computing and we found some really interesting results that at first make no sense and then if you think about it for about 10 minutes they do so these estimators of the parameters have really low root mean squared errors and bias so it's not nothing like what you've seen in many of the time series applications of gmm looks like it works pretty doggone well optimal weight matrices help a lot and so that's very different from altonji and siegel or from hanson heat in your own where they say that all the bad finite sample properties happen because there are parameters in the weight matrix now let me explain why you get these seemingly counterintuitive results all of these estimators of models are based on very simple statistics these statistics were matching our means it's really easy to estimate a mean or a regression coefficient it's maybe a little bit harder but not much or a variance so all of these statistics that we're estimating can be estimated extremely precisely the model parameters that we are estimating are just functions of these very precisely estimated statistics and as long as that mapping is monotonic invertible and what have you then the estimated parameters inherit these very very nice finite sample statistics and then the weight matrix we use is different it's not a function of the parameters of the model it's just a function of the data and so you don't have all those problems that you have with these finite sample properties of gmm estimators whether parameters are in the weight matrix so it works pretty well i thought that was interesting and that i could not have done in a million years without it high performance computing and i'm done so it's actually becoming more and more widespread like many universities have clusters my university charges the business school time and a half to use the cluster because they don't they think we're too rich so they they want to tax us more so that's that's a little so that's a lo that's not very great but in any case it is becoming more widespread and it seems like there's a lot of overhead you need to learn but remember i'm a literature major and so i learned all of this late in life you need some operating knowledge of unix or linux knowledge of a fast language so there are three of them somebody in this room doesn't like fortran but i do it's la as long as you get the loops in the right order it's still faster than c plus i know i know in any case learn parallelization paradigms those are actually pretty easy and it's really not as intimidating as it looks 