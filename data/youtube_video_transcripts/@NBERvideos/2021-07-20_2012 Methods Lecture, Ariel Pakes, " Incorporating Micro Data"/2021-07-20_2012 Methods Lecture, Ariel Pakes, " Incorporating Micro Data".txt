Ariel Pakes: It's
really true. We spent a lot of time on the
pricing equation, and it's really true that
the pricing equation fits very well in
the cross-section. But when you try to
predict prices over time, say there was a change
in the environment, you do much, much less well. If you have a price change
or induced tax change, and you go to the
next equilibrium, because the cross-section works, it must eventually
go to something like what we have in
the cross-section. But it must take some
time to get there, so you should be aware of that. I didn't want to leave
the impression that our pricing equations are great. They aren't great, at least for the time series variation. The purpose of this talk is to show you how to integrate microdata together
with aggregate data. I said this before, but I'll repeat it now. Almost always, you have
the market shares, what I call market level data, which is prices,
characteristics, and quantities. It's going to end up that
you're going to need them both when you have microdata.
I will tell you that. Now, I'll tell you
why as we go along, so that paper is going to
be structured for having both micro and aggregate data because that's what's available. When there's
microdata available, it's almost always
aggregate data available. Microdata matches
individual characteristics of the products the
individuals chose, so that's the definition
of microdata. I actually have the
individual characteristics. I know something
about the individual, and I know what
product it chose. It immediately gives
you information on the interactions between product characteristics
and the importance of different product
characteristics to different individual
characteristics. Immediately, you
can just look at it and look at the difference. Do families that are
large-sized buy minivans? It's in the data, and
you should be able to nail those coefficients
precisely. In addition to that, you usually, though not always, have data on which
individuals purchase a car, period, or purchase one of
the inside goods, period. That's helpful because it
tells you the distinction between who's buying this period and who's not
buying this period. Sometimes it also has second
and third choice data. You get the first choice out of by watching
what the guy bought. This is actually a survey. You might actually ask them. For example, in one of the papers that I'm
going to go over now, they did ask, what would you have chosen had
you not chosen this car? That's the second choice data. This is increasingly available, but particularly
in public finance. I think I mentioned
this, but school choice, you get a list of the schools that the
students preferred. Order 1, this is my
favorite choice, my second choice,
my third choice. If you're doing the matching and residency programs for doctors, you're going to get the
first choice of the guy, the second choice is the guy, that third choice to the guy, so it's increasingly available. It's an incredibly useful
source of information, and the reason it's an
incredibly useful source of information is
because it's going to give you information on the unobservable
characteristics of the individual as
well as the observed. The first choice data
is going to tell you about the observed
relationships. But when I look at
the second choice, if I would predict it from
just the first choice things, I would get a certain
prediction for the character. But if the characteristics
of the second choice are closer than that, so the first part is closer than you would predict from
the observable things, it's because of the
unobservable things. The new eyes; is that clear? It's almost direct
evidence on the importance of unobserved individual
characteristic interactions. Now, the only thing
that's a little bit close to that is panel data. In panel data, if you had
panel data over time, this is also helpful
in a particular way. You have to make an additional assumption if you
have panel data, and the additional
assumption is that preferences are
stable over time. Now, sometimes that makes sense, sometimes that
doesn't make sense. When the kids leave home, the preferences are
going to change the type of things you buy. But it's clearly good, at least in some instances. If you really are willing to assume stable
preferences over time, the other difference
between panel that in a second choice data is, in panel that the
whole choice set would typically change
from period to period. Where here, you're just taking one thing out of the choice. That, you take the
first choice out, and you look at what will I
do? What's the second choice? In panel data, you
have this problem of distinguishing effects. Because there's many
things going on at the same time. Here's the model. You guys are going to
get sick of this model. You've seen it a lot of
times, but I'm just going to rewrite it down on the board. I'm going to write it in a
slightly different form, the form that we typically use when you have
direct microdata; u i j is the utility of
individual I for good j. The difference is you can
do this in many ways, but this is the way it's
typically been done. Actually, it started
in this paper microbial P. Every x, j, k is the characteristics
of the products, and you let every household have different preferences
for each characteristic. The Beta now, is Beta i, k; is that clear? Then there's c, j, which is
this unobservable thing. Again, the unobservable
is picking up everything that we do
not put into the model. It's just like all
unobservables. Then there's the Epsilon i, j, which can cause problems. Now, what we're going to do
is we're just going to say, let's do a regression function. We have a bunch of
family characteristics , or household characteristics. Let's say, Beta i,
k is really Beta. Let's just do a
regression function on those characteristics;
is that clear? We're going to
allow for residual. What's the residual? The residual is all those
family characteristics that are making one x, j or more or less that
aren't included in the z; is that clear? That residual, by the
way, importantly, can have different variances
for different coefficients, for different cases;
is that clear? We can be very good at getting all the things that determine my preferences
for car size, and very bad at determining my preferences for
luxury in the vehicle or high-end leather
or sports car. That's when we
really don't know. The difference is that z, i, r are observed, and the new i, k unobserved determinants of
individual preferences. The way the notation
is going here is that, the Beta 0, are going to end
up being unobserved things. The Beta u, I'm going to come back to now. Well,
maybe I'll rewrite it. I think I wrote it here, so
what I'm going to do is, I'm going to just
substitute this into that. That's the extent of economic theory that
goes on in this stuff, which isn't a whole lot. I'm just going to
substitute the second equation in the first equation, and you get the
following equation, and reappears our Delta j, but now we know what it is. You just substitute in, and you get a constant term, which is a function
of all the x's. It's the constant term and the projection of x onto stuff. We did this projection. We need a constant term
in the projection. If that constant just
interacts with x, it's the same effect
on everybody, so it goes into the Delta j, as does the c, j there. Then on top of that,
I have two type of interactions between x and
household characteristics. I have the interactions of observable household
characteristics and the x's, and the interaction
of the unobservable household characteristics
and the x's. What I did is, the Beta u, there is really the
variance of the new i, l. Remember, there's one unobserved
characteristics per product, but they can have
different variances. I have to normalize it, but the variance on the
Beta U is the variance. The reason I did this
way, because Beta u, are the Betas on the unobserved
characteristics for u. The Beta 0 are the
unobserved characteristics. That's the easy way
to remember it. There's a Beta 0, k, and a Beta 0, r, k. The Beta 0, k is
from constant terms. If you remember and think
back to my first lecture, the whole problem or most
of the problems with own and cross-price elasticities emanate from the fact that, there's not enough
interactions between individual characters or household characteristics
and product characteristics. Because when I put
individual characters together with household
characteristics, particular individuals like
particular characteristics. When we increase the price,
they'll go to other places, other cars or other products with similar characteristics
to the one they bought. Now, the difference is, we have two sets of those
kinds of interactions. We have interactions
with observed z, and then interactions
with the unobserved z, so that's supposed
to fix this up. That's their role in this game. That's going to be the model
behind everything I do. That's it for the modeling. Now then come back, the
way you handle this. Go ahead. By the way,
questions are welcome. Dennis: [inaudible] Ariel Pakes: Not
always, but it works out that it's hard to
estimate in other ways. It's a good question.
Let's go back here. I could have said,
there's this news. They could well be correlated, and there's nothing in them all that says they
aren't correlated. You could model them as
a correlated matrix, so this goes back to what
Dennis asked earlier. But in principle, you
should be able to do it because it's not
j by j anymore. It's k by k. What I've seen, is, people haven't
had terrific success in estimating the
covariance terms. It's a precision. We should have done this in microbial
p because we had more data, you'll see, and I don't
remember we did it. You got to remember these papers were written quite a while ago, maybe today I would do that. I would always check
to see what happens. But I think the thing
you have to remember about estimating
variance covariances, is the variance of a variance
is a fourth order moment, it's something to the power of four and you don't estimate
those things very well. Because the variance of
it is, if you have 10, it's 10^4 and you have to
average out numbers like that. We didn't do this in
microbial public. You'll see that we had enough data that we
probably should have tried. In those days, it was a week run and now it's 15,
20 minutes to run. We should have tried it,
but that's a good question. There's no particular reason
to assume they're not correlated. Where was I? Types of microdata. You handle the different types of
microdata slightly differently, so I'm going to go over
a little bit of this. There's random
samples of consumers. The CEX is a random
sample of that. Consumer Expenditure Survey is supposed to be a random
sample of consumers because they're getting
the weights for the price index from it. That's very unusual structure, and the reason it's
an unusual structure is if I wanted to learn about a particular product and who likes a
particular product, what kind of people
that appeals to, it's a very inefficient
way of gathering data. No marketing firm would do this. The reason it's an
inefficient thing is because even if you do autos, which are a big purchase, now most people don't
purchase the good. Ninety percent of the
people that you go phone survey and talk to or give a present for
answering things, are giving you very
little information, just that they didn't
purchase anything. What all the marketing
firms do and indeed, what the statistical
agencies do when they don't have to
have a random sample, what they'll is they'll
do what's called choice-based sampling. What choice-based sampling is, is you sample from
a group of people who purchased a particular
product. Is that clear? Then you do inference
from this sample. What we're going to do, we're
going to carve the data we got directly from
General Motors. What they did is they told
this firm to go out and sample all registered cars in the
United States and they told them to get 40 Pontiacs, 50 Chryslers, etc. The question you're asking in the choice-based samples is, what does your model predict for the characteristics of
somebody who bought a Pontiac? Because that's what the data is. I'm going to have to transform the model to make
it look like that. That's what's called the
choice-based sample. Most of the samples that you're going to get that have
microdata look like this. The choice-based samples they come in very many
different ways. A marketing firm typically don't give you the individual stuff. Often, by the way, they
sign something that says, "We will not hand out
the individual data." Also, by the way, there's lot of data now on insurance companies. It's similar, health
insurance especially, they can't tell you
who the person is, where people go to hospitals. Healthcare is a big
deal at 17 percent of the economy and this is an industry which
is so regulated, there actually is a lot
of microdata around. Almost all insurance
stuff is regulated. Regulated industries often have some form of microdata around, because they have to report
things to the government. What they let out are things like the average income of
a consumer who did this, the average age, it was
insurance, what fraction smoke? Stuff like that. That's
what you're fitting. You're saying, people who
bought this insurance, I know number of
their average age. What does the model
predict for that? What does the model predict for the average age of
people who bought this type of product or the
average income of people. We're going to have
to do that and I'm going to do
that in a second. This is also the information. I'm going to start with [inaudible] but I'm
not going to do the tough that we split it up. I'm just going to
show you what you do. I'm not going to go
over how you transfer the model into choice-based
sampling moment. If the data comes is
choice-based sampling, you know the characters of
individuals who brought product j or the correlation between income and product
size from this thing. How would I actually
bring it to data? We already got the moments
from the aggregate data, so I'm not going to give
you the moments for the microdata that
you're going to add and you're going to
estimate them simultaneously. These moments are
going to depend on three things. It's
going to work out. They're going to depend
on the number of individuals in the
choice-based sample. There's 40 people who
bought a Pontiac, then that's little n for me. They're going to
depend on the number of simulation draws, because I'm going to have to do the aggregate shares also, and I'm going to do
that by simulation. I'll show you how to do that. They're going to depend on
the size of the market. Often, we forget about that because if it's 100
million people, that source of variance is
going to be averaged out. Here's how you do it. I'm going to say, let xkjb be the case characteristic
of the JF car. I'm going to look at
the people who said, I'm giving you the words
assuming we're doing cars, but you could substitute
anything else in here. I'm going to assume
that I know that car j is the first choice of
the [inaudible] individual. I'm going to say, what's
my moment going to be? If you look at the
interior term, what's in the data? That is the average
income of people or the average family size of
person who bought this car. Is that clear? That's
this term here. That mean zij. All those people bought
car j or bought product j, and what I know is
their average income or their average household size
or whatever it is with them. What I want to do
is I have a model, which tells me who
will buy good j, and I want to know what
that model predicts for the average income of
people who bought car j. Is that clear? That's what's on
the second term. Given the value of
the parameter vector, because that's going
to change with value parameter vector and
I want to find the value of the parameter vector
that makes that as close as possible
to the actual data. Then what I'm going to do, this typically what's done if
I have many products, is I'm going to interact it with a particular
characteristic, xkj, and then get a weighted average of
that over the sample points, nj over n. That's just the
correlation between income. This is family size and
xkj is parked car size. Typically, you'll have
one of these moments for every x and z that you
interact with in the data. If you're going to go
to a utility function, there's going to be
interacting with the x and z and what you'll do is
you'll typically have one of these moments for each
of those interactions. Everybody with me? The only thing that
I need to tell you, everything else is data, except for that ez, which I have to tell you how
to calculate. Is that clear? You can just assume
this is all sums. Forget about the integral sign. It's just the expectation of z given that the
individual chose j. Is that clear? Then
just use Bayes law. The probability of z given the y equals j is the
probability that y equals j times pd,z times probability
at random person does j. This is just using Bayes law. Everybody's familiar with
Bayes law? That's all I did. Px given y is Py given x
times Px divided by Py, that's what this equation
is. Now I'm in business. Why I'm in business?
Because our model tells us about this. That's what the model is, it tells you the
probability that y equals something
given x and Theta. I got to calculate that. Here's my calculation. Remember, we're going to use
aggregate data along with the microdata. For every Theta, because I'm doing
this inversion, I'm going to fit Pry equals
j conditioned on Theta, exactly, because that's
just the market share. For every Theta,
the inversion makes the Deltas make the shares, the observed shares equal to the actual shares.
Is that clear? This is just s and j and it's always going to be s and j no matter
what Theta you choose, because you're
always going to find the Delta that makes
everything equal. Is that clear? That's what
the contraction mapping does. This is approximately
equal to that. Now, this is a
complicated object, but the actual probability
is not so complicated. It's just a formula that [inaudible]
wrote up on the board seven times and I
wrote up four times. It's just that formula, but the problem is I have to
integrate out over zs and Nus because I need to find
the average. Is that clear? What I do is I just
take random draws from z and Nu distributions
and I integrate out, which you got to be
careful because this Pdz, that's the distribution in
the population at large. I need random draws from
something like the CPS. That's almost
always available in every country I've ever been in. It depends what's
in z by the way, you could define things
with standard demographics, that distribution is
always available. That's your approximation
to the moments. You just add that to
the aggregate moments, [inaudible] moments
and Omega moments, if you have a pricing equation and you're off and
running and you do method of moments
when you have a choice-based sample. That's
all you do to change things. Let me go back to [inaudible]. I'm going to just do
this very quickly because [inaudible]
did it, I think. When I do little stuff, it's because it's interesting, but it's not directed at
what we're supposed to do. The little stuff here is
the history of the minivan. You guys probably
don't know this but the minivan was first proposed by a guy
named Don DeLaRossa, who worked for Ford. He was one of the
design guys at Ford. He went to the head
of Ford and he said, "We should produce this thing." The guys at Ford said no. Why did they say no? They said,
"Even if it's successful, what it's going to do is
it's going to eat into the station wagon market and we control the station
wagon market." Ford had the biggest share
of the station wagon market, and it wasn't the
biggest company, so it was a big deal to them. Lee Iacocca quit. You heard of Lee
Iacocca? He quit. He went to Chrysler and he
took Don DeLaRossa with him. Don DeLaRossa built a
minivan at Chrysler. Took eight years for
Ford and GM to catch up. By the way, Chrysler was a
failing company before this. This is the first company that I think the government
walked in to support. It ended up being the most profitable car company
for the next 10 years. It was not only because of this, it was also because
they bought AMC, the jeep dealer at the same
time or shortly thereafter. Here's the question. Yeah. MALE_1: [inaudible] Ariel Pakes: Yeah.
That's probably right. MALE_1: [inaudible] Ariel Pakes: Yeah.
That's why we did it. You don't have to
do it that way. There's a lot of
intuition in that. MALE_1: [inaudible] Ariel Pakes: We did that
also because that will tell you the average height
of people who bought a car. You would average over
everybody who bought a car and you'd have to compare it to the
population at large. What's the goal of this? I'm going to concentrate on the economics then
I'm going to go back to micro BLP, where I do the
econometrics and modeling. The goal is to
quantify the impact of their minivan on
Chrysler's profits, the consumer welfare, and in the profits of
competing firms. This comparison is a comparison that was always of
interest to economics. Chrysler's profits is
the private return to R&D and consumer surplus is
the social return to R&D, or the sum of the two is
supposed to return to the R&D. The reason we have all
these patent laws and R&D tax rebates and everything
is because they are supposed to be different.
Is that clear? We might want to find out
just how different they are. Of course there's a problem in doing that here and you
should keep this in mind. We're going to find out
they're very different. Then in fact, the social
surplus from the minivan, at least the way Petrin
computes it and there are all these problems
that Aviv just went over. It's going to swamp
the private profits. I'm going to assume
that's true for a second, you got to worry a
little bit because there's lots of R&D that went into autos in these
days probably still now but in those days, it was one of the biggest
R&D sectors in the economy. There is maybe two innovation in the last 25 years that were
as important as the minivan. Somehow I got to weight. When you're going to do the
social versus private region, I got to weight the
unsuccessful innovations with the successful innovations which requires a way of
sampling innovations. It's much more
complicated than this. You shouldn't take this as
the last word on what's driving the difference between social and private rate of
return. Here's an example. It is an example of one
of the high-end things, but you'd want to mix it
with everything else. If you actually went
out to do something. That's the goal. The exercise is
estimated demand and cost side from the
observed data and then re-compute equilibrium
prices and quantities from that choice set that doesn't
include the minivan. I'm going to estimate
this model and I'm going to estimate a cost pricing
equation at the same time. I'm going to say, that's from the data when the
minivan were there. Now I'm going to say what
would have happened to profits and consumer welfare if the minivan weren't there? There are two problems
with that calculation other than the technical
ones that Aviv went over. If the minivan weren't there, the other cars
would have changed their pricing because now they're competing against
different kinds of goods. This he can solve if he puts
in the pricing equation, he can compute a new
equilibrium and he does. The other problem is, had the
minivan not been developed, different new vehicles
would have been introduced over the
time period study. Because if the
minivan wasn't there, there's an incentive to produce
different kinds of cars. That we can't control for
it without a dynamic model. Because the way you would do you'd have a
different incentives for producing different
characteristics of cars and you could run
that through a dynamic, in principle, you
could run that through a dynamic bond to figure out what happened, but
he didn't do that. In fact, it's pretty hard to do though we're making
a fair amount of progress on the dynamics. We're not going to
talk about it here, but that's one of the forefronts
of research right now. Aviv's going to talk at
least about dynamic demand. But this problem here is not
the dynamic demand side, it's a dynamic production side. You got to take that
with a grain of salt also for that reason. What Petrin did is to
the standard data, all he did is just
add the data from the CEX on these
characteristics, interaction between product characteristic and
consumer characteristics. He interacted one set
corresponding to purchase probabilities interacted with
individual characteristics, and one set of
characters interacted with car characteristics,
with individual character. Purchase characteristics
is whether you bought a car or not. This goes back to
something I didn't stress in the first lecture,
but I should say it. This came up in one of
the conversations also. When you're doing this,
you also have a UI 0. Remember? We forgot all about
this. Everybody with me? There's a UI 0 out there and we're going
to subtract it from UI J because we get this additive normalization.
Is that clear? The purchase characteristics
are going to be the things that are
added to the UI 0. The way we did in BLP, I can't remember what
he did in micro BLP is we said exactly the same characteristics that we interact with product
characteristics, we're going to leave
free on the UI 0 term. They're going to differ by county, state whatever you want, the distribution of them
so we're going to estimate separate coefficients
for the outside term and for the inside terms. I can't remember whether he
did that or not but we did. I remember that. I will show
you that when we get there. I just wanted to go
over a couple of the things from this just
to show you what goes on. This is what happens
when I take out the minivan and this is what happens when
I put in the minivan. This just goes back
to the thing on pricing equations for a second, just to show you
what's going on. These are the cars with the largest price decrease
when the minivan comes in. The minivan comes in and these cars
decrease their price, when the minivan comes in. You'll notice that they're all midsize cars and none
of them belong to Chrysler. What the competitors do, the midsize car are
the family sized cars. The people who are
buying family size cars, the minivan now comes in, the people who are
selling minivan, a medium-sized cars, GM, particularly what they
do and Ford is they respond by decreasing
the price of their family sized cars because they now
have a competitor which is better in some dimensions and in order to sell more family sized cars, I have to decrease my price. In contrast, if you look at
what the Chrysler cars do, they increase their price. The Chrysler family size cars. Why? We just got a
Chrysler minivan in. I'm going to increase my price. Some people will stay, but
some people will leave, but now most of the families
who leave their car, are going to go to the minivan
and they earn profits on the minivan anyway so we might as well hike
up our prices. This is exactly the logic of this multi-product firm thing
that we were doing before. It's the sense in which
economics at that level works. Welfare, profits and the
returns to research. Let me just say there's going
to be two sets of things. When he does these
welfare calculations, there's two things going on. There's an increment to
the people who bought a minivan and who
would have bought a family car had there not been any minivans and then there's increments also to everybody else or because there's been
a change in all the prices. If I bought a GM
family size car, I just gained because the
minivan came in because the prices of all GM family
size cars were reduced. Is that clear? There's
that source and then there's the source
to the individuals. The source to the individuals
for people who would have bought a minivan at the
highest price minivan. In some sense, we're waving our hands because we're
getting info marginal rents. Is that clear? From above that and there's nothing in the data that's telling us what it is. It's just a projection of these epsilons and
other things out there. But for everybody after that, if you believe the
model estimates, you have a level at
which they didn't buy and a level
of which they did buy and that bounds their
consumer surplus game. Because you know it's not greater than the
level of they didn't buy and it is greater than
the level they did buy. That part of it you could
have done non-parametrically. Actually, I would
have liked to have seen him do that
split, but he didn't. It's a very good
paper by the way, I don't mean to be negative, but we both talked about it. But to give you some
idea of what happened, if you look down
here, first of all, this is what
happened to profits. The minivan entered in 1984. Actually, Ford and GM immediately tried to do another something
that looked like it, they did something
called the Aerostar and the Astro or something. We'll see if that
helps. They built cars that drove like trucks. They thought people
just wanted the space, and the closest
thing to them were these Volkswagen
minibus-type things, and they drove trucks. It took them till 1990 to come in with something
that was really a competitor to these things and you can see what
happened when it came. By the way, Chrysler maintained their lead in this
till the mid-'90s, so it's 10, 15 years that they got a lead on everybody on this. You can see the industry
profits actually fell in the first two periods. Of course, Chrysler's profits, these are millions of dollars, went up quite a bit, but Ford and GM lost more
than Chrysler gained. Then eventually, industry
profits went up. Now we look back at the welfare and the
compensating variation games, the welfare change and the
change in producer profits. Let me just say, the investment in the minivan has
been estimated at three-quarters to $1 billion. That's around the range where it's supposed to
have been estimated at. You can see the rate of return for Chrysler was about
20 or 25 percent, down here it's up to
50 percent per year, which is a noticeable
rate of return. If you go down to
the welfare change, you can look at just the
compensating variation change. This is the stuff that
Chrysler didn't internalize, but the society
did, is that clear? This is a consumer variation.
This consumer surplus gain. You can see these are for
a $1 billion investment, consumer surplus returns are
about 50 percent per year. This is what set the trade, the trade guys, and
the growth guys going. It's numbers like this. They originally started
with numbers in hybrid seeds and
things like that. Why we got the
modern growth theory in R&D and all of this is
because of numbers like. But again, you have to be really a little bit
wary, because again, this industry does a lot
of R&D and there have been very few innovations that have been as successful
as the minivan. This changed the whole
industry around. You have to be a little
bit careful, but there is a potential there. This is one final thing. I'm now going to go through in much more detail
how you actually add consumer-level data
and how you do everything. This is from this
MicroBLP paper. MicroBLP uses the KMIP data. This gives you some idea of
what goes on in industry. KMIP is General
Motors' sample of, I can't remember,
37,000 people a year. I'm sure they have two
samples of 37,000 a year. They pay somebody
to do this sample. They tell them exactly, all new cars
registered this year, what I want you to do is, I want you to sample
20 Pontiac Grand Ams, 45 Cadillacs, 50 Volkswagens. You go sample them, and
then ask them questions, and then bring back the answers. It's 30,000 people a year; it's bigger than the CPS. It's more than 30,000
whatever it is, the CPS about 34 or five
if I remember right. What this taught me is if the government was a
private corporation, we'd have a lot
more data around. They never let it out before. I'll tell you one story. They never let it out before, they let it out after they saw BLP because they thought maybe, and they gave us old data. Then by the way, what
happened in the end, is they asked us to
rerun this stuff on new data and then get
rid of the programs. Give them to GM and
never come back to them again so that they
could do it by themselves and nobody
would know the answers. Actually, I didn't do it. This woman, Nadia Soboleva, actually redid the program
and interacted with them to make sure that they could
use it, and that was it. I never heard from them again. It's interesting, there's
a Vice President, Marina Von Neumann, who is Neumann's daughter. Jim was friendly with her. Jim Levinsohn, who's one of the co-authors on this paper, and was also a friend
of Mustafa Mohatarem, this guy who ran the
research program at GM. Anyway, when we went, he had to go ask Marina
whether she could give us the data, and she said, yes. It is a big deal
because they'd never let it out of GM before. They have one every year,
they've never let out. Mcfadden asked for it
once and they said no. It was BLP that they got to. But when most of us I said,
"What's Marina like?" She said, "She's about a third
as smart as Von Neumann, which makes her the
smartest person at GM." I don't know if you guys know this story is about Von Neumann, but they're amazing.
This is the dataset. It has vehicle characteristic
prices and sales. It has the same as the
aggregate we had in BLP. Actually, more detailed
because they know more, but it's the same kind of data. Household characteristic
by vehicle purchased. Again, they told them to
find out these people's age, their income, their family
size, education, you'll see. Then they asked a very
important question, which is, if you hadn't
chosen this vehicle, what vehicle would
you have chosen? That's the second
choice data. Go ahead. UNKNOWN_1: [inaudible]. Ariel Pakes: No, you
couldn't respond that. That's going to end
up being a problem. That's all very good, but unlike the aggregate study, and this is going to
end up being important, we only have one cross-section. The aggregates study, we
actually had 20 cross-sections of aggregate data:
market-level data. We're working here with
one cross-section. You're going to see where
that's going to impact things. Here's my UIJ again that you guys must be getting
bored of already. It's the same UIJ that
was there before. What's going to happen here? The benefit of the
first choice data is it's going to give
me the Beta 0 RK. At some level, it's
got to do that. If all the Beta 0 RK are zero and I only had
first choice data, you can show formally that
sufficient statistics for this problem are just
BLPs statistics, the market level data. If you only had first choice, and those characteristics
didn't matter, the observant,
you're back to BLP. That's where they're
going to hit us. The unobserved attributes
differentiates the model from the standard
micro-based logit model. If you just took this
as a micro-model, you're going to go do logits in any kind of choice thing with microdata,
it doesn't need to be cars. It can be from public finance, trade, whatever you want to do. The difference would be if you
just did a standard logit, there wouldn't be any new eyes, and you'd just do a logit and do maximum likelihood of
whatever you're doing. The new eyes do two things. There's a substantive
issue and what they're trying to put in there
is they're trying to say, look if I got all the
individual characteristics in my observed stuff
that are important in determining the preferences
for this characteristic. Because if I don't
I know I'm going to screw up on the
cross-price elasticities, because I don't
have enough people moving; is that clear? The new eyes should
correct that problem. That's what they're there
for. But it also generates a computational issue
that probability, is like Aviv showed you, if new eye were all zeros, so I didn't have to
worry about that, I could just do a straight logit and there's no problem
with estimating that. You just call a logit in Stata
or whatever you're using. Matlab probably has logit
now, and you run it. But once you have the new eyes, there's no analytic formula
for the probability, so I have to integrate out with simulation, or something else. Let me just say we went
through this before, but when there's only one
data set, one period, and one choice set, there's very little in
the data to tell you what the new eyes are, is that clear? If you had one cross-section, same household distribution, so you can't go over different
county distributions or household characteristics, you can't use that,
same choice sets, so I don't move the choice
set around; is that clear? There's very little in the
data that's going to tell you about what the distribution of the new eyes really is. Because it's just functional
form essentially, there's nothing sweeping it up. The advantage of the
second choice data is that it does have
something for that. I said that already, but
it's worth repeating. The second choice data tells
you if I take out a car, how close is it to the Xs of the cars that was
my first choice? In fact, closeness can't be predicted from the
observed sets, it's because of the new eyes. Now one other point
about microdata, which is often missed, once I have the microdata, I could actually estimate separate Delta Js for
every product point-wise. I have microdata, there's different constant term for
each product; is that clear? It's easy to do that
if you just call logit and let separate constant term. The Beta 0 RK and the Beta UK, I could estimate them without
any assumption on the CJ, which was the identifying
assumption in BLP. Everybody with me? Because I can
estimate the Delta's just point-wise and
everything else is just a logit for the
microdata. Everybody with me? But you got to remember
that the Delta Js are a function of the Xs
and one of the Xs is price. To get price substitutes, or cross-price elasticities, or cross-anything elasticities, I also have to run
this equation. Unless I'm going to assume that all the Beta 0K is a zero, which is typically
what was done, at least before we
got around to it. Because it's a nonlinear
function of the Deltas, it's E to the whatever it is. The derivative with
respect to the price also requires the price derivative, in the constant
term, is that clear? This is where there's
going to end up being problems because
you have microdata, which is a ton of data, but you still have
only whatever it is, however many products that
are observations on Delta J. You haven't solved that
problem yet, is that clear? The microdata tells
you how to solve the observed interaction terms, but it doesn't tell
you how to solve the problem of figuring out what the constant
is determined by. Because you have
the same amount of constants as you have products, just like we had in the market
level data. Again, I said this before, but I'll
say it once more. There are some issues
that can address without getting price elasticities
like price indexes. But many of the issues we want, we want price elasticities or characteristic
elasticities and for that I have to get
the Delta j's. Now let me tell you
how to do this. Remember, I'm going to try and estimate the Beta 0 and Beta u. Then I'm going to try and
partition the Delta as a function of the Betas of
the x's, including price. One of the x's now is price. Those are the three
things that I will do. You have two choices, I can either start by estimating
Beta Delta point-wise. Estimate every delta
separately and the Betas. The Beta is a bit of U and the coefficient of the observed and the coefficients
of the unobserved. I could do that point-wise. I could just estimate them all. Or I could use an assumption
identifying restriction on Csi [inaudible] Csi given X is 0 and estimate instead of Beta Delta estimate
Beta and Beta 0. Is that clear? If I have K
characteristics in 200 cars this is estimating
only five parameters and this is estimating
200 Deltas. We're always going to
estimate all the Betas used all the other Betas.
What's the trade-off? It's an obvious
trade-off actually, if I estimate Beta
Delta point-wise, those estimates don't depend on the assumptions that go into the instrumental
variables here. They can be wrong or right, the Beta Delta will
still be right. Is that clear? Because I'm not making an assumption
when I do that. The trade-off, of course, is if this assumption
really is right, I'm going to get more
efficient estimators this way. What we did is we weren't worried so much
about precision and the reason is we had datasets that have 30,000 observations. We did the Beta Delta pointwise. Then we're going to use different assumptions
on the Deltas to show you the impact of the different assumptions on the estimates of
the Delta equation. Because once I have
the Delta estimates fairly precisely, I can regress, I can do instrumental
variables in seconds, I can try to and see how
robust it is, everything. Computational choices
and the searcher chain. Again, we're going to do
the Beta Delta pointwise. How would you do that? I'm going to come back to it, I think. But I'm going to show
you how we do it. But you can think of this
as [inaudible] hold Delta fixed and estimate the Beta. I'll come back to
this in a second. So what I could do is I could do this search over Beta and
Delta together. Is that clear? Or I could use BLP's
contraction mapping to compute Delta as
a function of Beta. We know it's going
to fit exactly, get the Delta exactly, and then put those Betas
as a function of Delta back into the moment
equation. Is that clear? That's sometimes called
concentrating out. What's the difference here? The difference is really
a computational one. I don't know how many
of you guys have done complicated search procedures. But the Deltas have
200 parameters, and the Betas are about
30 or 40 in our model. Searching over 250 parameters is a pretty difficult
thing to get the computer to do right and to make sure
you're right. Is that clear? This we know we can
do this right away. The inverse right away. It's probably not as efficient as doing
everything together. But we know we get the 200 exactly right for every
Beta and then we're just searching over 20 or 30
parameters. Is that clear? We chose you solve BLP's
contraction mapping for Delta. You get Delta as a function of the market size and the number of simulation
draws and Beta. Then I'm going to
substitute this back into the moments that
are going to determine Beta. I have Delta of data and Beta in those moments and I'm going to maximize with respect to Beta. One comment about this. The first thing I
want to do is I want to just do the micro Data and get the estimates of Beta
given I've done the inverse, I got Delta inverse and I got to put this into micro
data and estimate it. The first thing
people would say is, why don't you try doing the likelihood
because it's a legit. Maximum likelihood is efficient. It's not efficient. It's not even a good idea. It's not efficient because for maximum likelihood
to be efficient, you have to have exact
estimates of the probabilities. If you look at the theorems, that's what the
theorems are saying. There is no simulation error, there's no approximation error from quadrature or whatever
sequences you're using. You got it exactly. What happens when you
don't have it exactly? What happens when
you don't have it exactly is there's a difference, I'm going to say
we did simulation, there's a difference
between the true P, which is a P
[inaudible] Beta Delta, and your estimate of
p, which is the PNS. Everybody with me? I'm going to call that difference U and S. That's the unobserved
part of that difference. I'm going to look at that difference and
it's approximately, if you do the expansion, just the Taylor
series expansion, the first term is going
to be U and S over P. The second term is going
to be U and S squared over two p squared and
then there's going to be terms of higher-order
in the Taylor series. Now I'm going to take the
expectation of the error. This error is actually, you're
not maximum likelihood, you're maximizing
the likelihood of the real P plus these errors. Is that clear? Because that's
a fact what you're doing. It's not log p, it's
logged P and S, which is really
log P plus log of p plus u. Is that clear? I'm just doing that expansion. You do that expansion and
then I'm going to take, remember it's going to be something like there's
going to be a moment in the first-order condition
I want to take what's the expectation now of this. The true thing, not [inaudible] setting the
expectation likelihood, which we know will come
if by the entropy thing, will go the right
thing of Theta 0, will give you a maximum Theta 0. I'm going to now take the
expectation of this thing. The entropy theorem
is what tells you that maximum
likelihood is efficient. If you take that, you get
a variance term here. The first term has a mean
of 0 if you've simulated. It's mean is 0, so it goes away. The second term is not, it's the variance and it's
a function of Beta and Delta. Is that clear? Now to the likelihood we're adding this other
function of Beta and Delta. Is that clear? No longer
you're maximizing likelihood, you're maximizing
something else. Now that other function
it works out is approximately 1 over
2 times n s times p. That's because of binomial formula for
the variance is p times 1 minus p
[inaudible] and if you stick that and you'll get this. Now let's think about
what's going on. If that number is large, it's larger than a
function of Beta and Delta. Is that clear? You've added something to the
likelihood which is large and it's not reflective of
what you're trying to do. Let's think of the
auto industry. Only one in 10
families buy a car so all of the Ps
together, sum to 10.1. Everybody with me?
There are 200 cars. There are many, many cars
with shares about 0.001. You can just look
in the data there , that's the shares of those cars. Is that clear? In order to get the variance just down to the level of P, I'd have to simulate
10,000 times for each individual and I
have 30,000 individuals. I would need to take 300
million simulation draws, hold them in the computer, fixed and reevaluate them
every different Theta. Your computational
burden is just crazy with that and you're
not going to do it. That's just to get P
on the same order. If you try to get the bias to be very small relative to P, you'd have to do it 10
times more than that, I guess, which
would make you into three billion simulation draws. You're getting two numbers
that are ridiculous. You don't want to do
maximum likelihood. I've seen people do this all the time and I just don't understand why
it doesn't catch hold. It makes a difference
by the way, because I've tried it both ways. It's a little bit
of a rather extreme case because the Ps are very small and the sample
size is very large. If I had Ps like 0.5, it wouldn't be such a problem. But in most cases, dealing with most markets, most people don't buy a
good in every period. One in 10 maybe, I don't know. But then if you have 150
goods or something like that, the numbers are going
to get like this. We don't want to estimate
the micro parameters. We don't want to use likelihood. What do we want to use? You can choose just
about anything, as long as you know
how to compute the model's implication to it. We chose something which we
thought was very intuitive. Every time we interacted a household characteristic with a car character's family
size and car size, we did the moment
I told you before, which is we looked at the correlation between
household size and car size and the data and the
prediction of the model for the correlation between
household size and car size. For every parameter like that, we add it with a
parameter like that. Also, for the characteristics of the households that bought
a car, we did the same. Then for the first and
second choice moments for every car
characteristic that's in, remember there's a new I for
every car characteristic. We added the correlation between the first choice and the
second choice, car care. I know your first choice x, the first car's car size, and your second choice car size. I'm going to do the
correlation between the first choice car size and the second choice car size, and we get the model's
prediction for that. Is that clear? I'm going to compare
that what's in the data because that's
supposed to pick up everything that gives you a correlation between
the two car sizes, the observable and
the unobservable. The observable is
going to be gotten from the first set of moments, and the difference
is going to be because of the unobservable
set of moments. That's the intuition
between what we're doing. Again, here's the
simulation problem. Here's how we estimate it. We start with the
market-level data. We set S Beta Delta P
and S to get equal S, and we use a BOP contraction mapping to do that, by the way, so why is a BOP contraction
mapping useful? Is because this is 200 equations and 200 unknowns for
every different Beta. It's a nonlinear
system, 200 equations and what the contraction
mapping says is look, I have it, it's a
contraction mapping. I know how to solve this quickly without any
search procedure. I'm going to give you 200. I'm just going to
be exactly right to whatever precision is you put in as Aviv said
in his lecture, by the way, you want
real precision. This is one thing we've
learned from all the stuff. You really want it
down to 10 to minus 12 or something like that. That gives me my
Delta. I'm going to substitute the estimate of Delta 4 Delta into these micro-moments that I gave you at the very beginning. I'm going to maximize a weighted average
of the difference between the prediction
for the micro-moments and the actual
micro-moments in the data. They're going to GMM.
That's what's going on. Here's some tables. Again, the K-map, this data set is a
random sample of 37,500, I said it was over
30,000, so I was right, and 34,500 they don't
have second choice data. This is a huge sample. The first thing I
want you to look at is K-map versus the CPS. I said you want to do correction for choice-based sampling. This is not a random
sample of people. It's a weighted average of a sample of people
who bought Pontiacs, a sample of people
who bought a car. If on the whole of that
K-map is the whole thing. If the characteristics on the K-map sample were
the same as the CPS, you'd worry less about
choice-based sampling. Is that clear? Because the whole issue
of choice-base is I've chosen particular people. Now I've chosen
one, basically the new eyes as well
as the zed eyes. But if it was uncorrelated
with the zed eyes, you might think it's not
so bad an assumption. Remember, what you're
doing is you're taking not a random
sample of people, but people who bought the car, who have particular
zed eyes and new eyes. If you look at it, here's the CPS group mean and
here's the K-map group mean. You can see that the income
stuff is really very clear. People who buy cars
not unsurprisingly, are wealthier than people
who don't buy cars. The average household income at this time in the CPS was 34,000, the average in the K-map
sample was 72,000. It's not a random
sample of consumers. If you look at other demographics,
the interesting thing, they aren't really
that different except for rural versus urban. Rural people buy a lot more
cars than urban people, which explains why GM and
everybody else was interested in light trucks and SUVs. The first time we
did this by the way, we ran it through with just the old BLT stuff,
which was only cars. Mustafa told us that they are not interested in
something that doesn't have the interactions with light trucks because it's 40 percent of the
market even then. Oh, no. Control. Does
somebody know what it is? MALE_2: Control Shift. Ariel Pakes: Control Shift what? Plus. Good man. Thanks. You have car characters, you have people characteristics. What you want to do is find out which interactions
are important. Is that clear? You do a bunch of tables of data and you try and
figure out what it is. This is the one that helped
us the most, actually. These are the household
characteristics, age, kids, family size, urban, suburban, rural, and income. You can see all the
things that you expect. It's really nice to look at
data sometimes because then you start believing
all this stuff they taught you about economics. Higher-income people buy
more high-priced cars, for example, which
should not surprise you, but it's nice to see
stuff like that. If you look at who's
buying pickup payload, who's buying trucks, they're
largely rural people. The thing that surprised me about this by there the
first time I look at it, I remember, was that
age mattered so much. Works out that age interacts with almost
everything and taste. It's a little disappointing, but what can you do? You can look at this
and you can preview it. If you do minivan and kids, you're going to get 2 plus where all the
minivans have sold. You look at it and you can
see that it all makes sense. Control Shift Plus. Good. When you do something new, typically it's usually
more complicated. The first thing you
do is you compare it to the old style stuff. We actually estimated
four models. We estimated straight logits. It was just logit, first and second choice, straight logits, straight BLP, and then the full model. Our full model, by the way, it's a combination of
the logits and the BLP, because we have unobserved
individual characteristics and observed individual
characteristics. The logits just had the
observed characteristics, the pure logit just had the
observed characteristics. BLP just has the unobserved
characteristics. The one way to look at
this model as we have both observed and
unobserved characteristics. We include both. Here's again, what we did. We did exactly what we said
it's separate price and income and we let
that be a spline. Spline we just said
there's one coefficient for the first quartile another coefficient for
the second quartile of income, third, and fourth. Because you're always interested in the price coefficient more so we freed it up and just said, there is a data set. The other thing you
should note is for all the individual
characteristics they entered interacted with
the product characters, but they also entered
for the outside good. They get a coefficient in the outside good
and there's also a new high for the outside
good Everybody with me? The only ones that
were important were income, family size, and the number of adults, and it's everything you
expect from the outside good. The outside good was more about, if you had more income your
outside good was better. Conditional on income, if you
increase your family size, the outside good went down
because your total income, your income per family
member went down and if it was adults,
it went down even more. This goes back to this question of taking care of the
outside good correctly. Originally, by the
way, they were all in here just that nothing else mattered except for these three. You look at the
price coefficient. The price coefficient,
we did it with wealth and we did one over it. You can see that
there's a difference between the first quartile
and the third quartile. Family size entered exactly the way it was
expected to enter. I'll let you preview
these numbers, but you'll see that almost
everything makes sense. In a way, it wasn't
a surprise to us. We had really good data, and if you can't get it with 37,000 observations, you
shouldn't be able to get it. Minivans interacted with kids, those part was age. SUV with age is negative. Rural dummy with
SUV is positive. Trucks are all rural. Stuff like that. You
can just go through this and you'll see
everything makes total sense. The surprise wasn't that, the surprise was the logits
made a lot of sense also. That was a surprise. I didn't expect that. But if you look down the
line for the logits, they're not that different. At least there's magnitude
differences a little bit, but signs aren't
really different. To go from the coefficients to the actual, it's not clear. You don't really have a big, I will show you the
implications and then you have some priors
on the implications. But in the actual
coefficients in this stuff, you don't have
very strong prior. You have priors on signs. The one that's a little
bit weird is, I think, trucks, if I remember
right trucks, or had a rural dummy and goes the wrong
way for the logits. But it's only one that
goes the wrong way. It was pickup payload
must be this one. Rural dummy, it goes down, it's just goes down a bit, it doesn't really screw up. You look at the
logits, it's really not hard to tell. I
know what was wrong. The outside good for the rurals, people with higher
income the logits we're predicting they had a
less valued outside good. But outside good is a weird animal anyway.
You might not kill it. You look at this
and you would say, even if one sign out
of 25 was wrong, you'd say that's not bad
for the way we do stuff. The logits looked okay. I'm going to come back
to the implications. Again, they look okay from the point of
view of parameters, but that's only because
the only thing we have priors over the
parameters are signs. Magnitudes we don't
have any prior. This is the same table. Now I'm going to look
at the table with the variance terms on
the unobserved stuff. Can't remember,
there was a lot of coefficient's going on because
we have a lot of data. There's the observed
interactions and the unobserved
UI interactions. If you look at this table, it's just striking how important the unobserved
interactions are. I think virtually
every one of them was significant and some had T values on the order of 20, 25. Many had T values on
the order of 20 or 25. These are standard
errors underneath them. This, for example, the price, we had four terms
interacting with price. In addition, there's
a random term on the price coefficient
and it has a variance, it has a T value of 22 or
20, something like that. It's clear that it matters, we haven't picked
up everything that affects price. Is that clear? Similarly, if you look
at number of passengers, it's got a T value of again, on the order of 20 something. We had many things
interacted with the number of passengers
which is size of the car. We had age, we had family size, we have lots of stuff in there. You're still left
over with a 20, [inaudible] had a T value of 20. You can go down the list and you can see almost all of
them look like that. Certainly those on
characteristics look like that. The BLP ones, by the way, so now in the first table I couldn't show
you anything about BLP because BLP had no
interactions with observables. In this table I can't
tell you anything about the logits because they have
unobservable interactions. But these are the BLP numbers which aren't so different
from our numbers by the way. I'm not finished.
I want to go to implications of the estimates, because you're going to see
that the logits screw up, and you're going to see why. But before I do that,
I can't do that. I can't get on and cross price elasticities until I tell you this projection
of the Delta J's. Because remember they include
also the constant terms. Everybody with me?
I have to do that. I've got this equation
for Delta J and I'm back to BLPs problem
because CJ is a correlated with PJ for the same reason it always
was correlated with P. I need estimates to
calculate this equation. Moreover, I'm in a much worse
situation than BLP was, because now I have
one cross section of these Deltas. Is that clear? But BLP at least I had
20 cross-sections, so the same car appeared
in different periods. I could see some variants there. But now I got one cross-section. There's going to be
a precision problem with estimating Beta 0. Again, we're
particularly interested in owning cross-price
elasticities, for that all I need
to know is Beta 0P, because they depend on Beta
0P and the Delta estimates. We tried fewer instruments,
we got nowhere. We added an assumption of marginal cost and the pricing
equation like BLP did. I'll show you those answers. Still you're going to get
very imprecise estimates. It works out for the
reasons you might think. We look for prior information. When I used to give this
at econometric seminars, everybody would start saying, "Where are you getting
this infinite?" But the truth is, all we're trying to do is find out
the answer to the question. If somebody else knows the
answer to this, you use it. In this case, GM thought they knew the aggregate
price elasticity. They said they'd
been doing this for years in their research program upstairs and it's almost exactly one and it always
predicts almost correctly. So we said, "Okay. Let's take our model, set the aggregate
price elasticity, the weighted average
of the price elasticity equal to one. What's the Beta 0
that that implies?" And use that. I'll show you
the differences in a second. The other thing we did is we
took what everybody else did before us when they had
micro data which said zero, and we did that
also, the Beta 0. I'm hoping yes. What we did to figure out whether
these made any sense or not, is we took the
semi elasticities. I think the semi elasticities, and just regressed them down on characteristics and see
if it made any difference between the various
products about whether the elasticities
are different when I use different
estimates of Beta 0. The BLP with instruments and pricing equation
zero and the other one. What you can see is the actual aggregate
mean elasticity matters. It matters what you use. I can't remember which one is which. Zero I know what it is. I think minus 11 with the
number Beta 0 comes out to be from our stuff and maybe minus 3.58 from using Beta
0 that GM gave us. The aggregate elasticity. The average elasticity
has changed a lot. But the ratio, but the regression functions
are almost the same. They're very similar.
What is this telling you? It's telling you we're
not going to get the aggregate elasticity right, but we're going to
get the differences across products
approximately right. We're going to get the cross substitution
patterns approximately right. That makes sense because
this is the same Beta 0 that goes into
every calculation. Every elasticity and every cross price has the same Beta 0. If I screw it up, I
move all of them, but I moved them all together. That's what's going on here. These are interesting
also because you can see where the markup
should be higher. They're going to be higher in the minivan where
it's more negative, where the elasticity is
more negative, lower. It's going to be higher in
the minivan and the SUV, and that's exactly
the incentive. Remember those are going
to be the markups. That's the incentive
for new car development because it's the
markups that give me the profits from the
development expenditures. That's where you
expect people to enter the market and that's where
they did enter the market. Let me go to Control Shift Plus. These are the elasticity. What we did in this paper
by the way is 200 products. I can't give you a
table with 200 products , you won't read anything. We took the two highest
selling vehicles in every class of vehicles, and we're giving
you the numbers for those two highest
selling vehicles. The Metro and the Cavalier
with sub-compacts, their price was
7,840 and $11,000. That's their semi-elasticity. The best substitute
is the Tercel and an escort which are
also in the same class, and the second best substitute is a Festiva and the tempo. What are the substitutes
we increased? I can't remember what we
increased but some we looked at some amount of
money, is that clear? We looked at where everybody
went as a result of that, and we got the first and
second best substitutes. That gives you the movers and the percentage outside good. You guys can look at
this at your leisure. But you can see that it
all makes a lot of sense. The Lexus 400 series, which was in the
high-end Lexus then. It's substitutes
with the Mercedes and the Lincoln Town Car. The Metro, satisfied with
Tercel and the Festiva, I go down to a Jeep
Grand Cherokee and it substitutes with the
Explorer and another Cherokee, which isn't the Grand Cherokee. You can just see they
all make a lot of sense. I'm not going to spend
too much time on it. Now I'm going to compare the best substitutes from our model to the best
substitutes to the logit model. The best substitute for
the logit model for every car from the first
choice data was the caravan, except for the caravan, which was the Voyager
and the economy. The caravan it's this
van, big heavy van. It's the best substitute for the Lexus 400 series as
well as everything else. If you use the first and
the second choice data, it's the Ford full-size pickup. Everybody, if they moved out
of a Mercedes 500 series, they're going to
move to the Ford. That's the model predicting. The Ford full-size pickup. If you'd moved from
a Tercel or a Metro, you're still going to move
to a Ford full-size pickup. What's going on here? Remember, what's happening is the substitution patterns are determined by the interaction between product characteristics and individual characteristics. The individual things, even
though they all came out significant and there were 30
of them or whatever it was. They were dominated by the micro, the unobservable
characteristics. Is that clear? When you put back in the unobservable
characteristics, they mattered a lot. You could tell they
mattered a lot because the people who bought a Mercedes really
did move to the Lincoln, we saw that in the last thing. But they moved to
the Lincoln not because of observable things. They move to the Lincoln
because of the unobservables. If I didn't have the unobservables there,
what would it say? Well there wasn't much variance explained by the observables, even though they were all significant and they
all made sense. There wasn't much of the
variance explained by them. The logit takes over what happens when the
logit takes over? It goes to the car that
has the biggest share. Because that's what logits do. The car with the biggest
share actually was between the caravan and
the Ford full-size pickup, because there were no other full-size pickups in the market. There were no other
vans than the Dodge, there was either very
small number compared to family size cars. Is that clear? Remember I said at the
beginning I said look, we get rid of some of this by having observable characters. But if you don't have enough
observable characteristics, it's still going to come
out in spades, and it did. It shows you the importance of allowing for unobserved
characteristics. MALE_3: [inaudible] Ariel Pakes: If you had the right observed
characteristics. For example, just think for a second what determines
whether you want to have a van or a you
want to have a minivan. One of the things that determine whether you have a minivan
is when you got to take 10 kids to the soccer game. That's just not in our data. Or one other thing that
determines whether you want a big car or not is
you have another car, is it a small car or a big
car? It's not in our data. Maybe if we had those things in our data then the
observables wouldn't matter. It's that logic that you
have to think through. MALE_3: [inaudible] Ariel Pakes: I think you'd
find that for a lot of things. The one thing you do pick up a fair amount on price itself. The income trends,
I actually got three or four different terms interacting with price and
we've got a lot of that. But once you get to the
other characteristics, you often don't have. Income really does
determine what people buy. But when you determine whether you're
getting a minivan or a Mini Cooper S depends whether
you have your other cars, is a big car and
whether you'd got to drive kids around
and stuff like that. That would just
isn't in the data. It's important, they can be, at least in this example, and they can be very important. Now I'm going to go back
to prediction exercise. Remember, one of the
things we want to do is find out crossing
own-price elasticity. Other thing we want to
do is do prediction. Remember one of the
reasons that you went to characteristics
based model? Because I want to predict
what the demand would be for a new car and I can't do that in product space. That clear. GM actually asked us to do the two prediction exercises that I'm going to
go through quickly. One was to use the demand system for the evaluation of potential
demand for a new model. They were deciding
where to build the high-end SUV and
they want to know what would happen if we put a
high-end SUV into this thing. That's going to be my
first prediction exercise. The second prediction exercise was they were thinking of shutting down the
Oldsmobile division. Why did Jim think about shutting down the Oldsmobile division? Because they had
four other divisions that were doing
family size cars. There was Buick,
there was Pontiac. I'll show you in a second.
They're all up here and they thought that
they were actually just cannibalizing themselves. If people didn't go
to the automobile, they were going to go to
one of the other GM cars, family size cars and there was a lot of cost to keeping the
Oldsmobile division going. That was the question
and the question they asked us is that we got rid
of the Oldsmobile division, the whole division,
there were five cars that they are
being produced. Well where would people go? Those are the two
questions we were asked. By the way, automobile division
has been discontinued. There is no more
automobile produced in this country anywhere actually. They actually went through
with it in the end. What we're going do is
we're going to put in a new high-end SUV and then we're going to take out the automobile
and we're just going to calculate stuff.
That's what we're doing. There's a couple of
problems with that. First of all, the major problem I think is the data
we're going to use to recalculate things is 1993
data and they're not going to kill the automobile until
1999 or something like that. I can't remember when
they actually killed it. They were going to
kill it for a while. The other goods you're
competing with in 1999 are different from the
goods that you were competing with in 1993. But you might get
some idea of what's going on by looking
at the 1993 data. The second problem
is that there's an issue of what the
prices would be with a high-end minivan
or high-end SUV, I can't remember which we did, I'll see on the next page, and what the prices would be
if there was no automobile. We could re-compute prices. We didn't do that. We actually did a
regression function for price like a hedonic function, because it didn't
seem to make much difference, whatever we did. Then the other thing is, the other cars are
going to be held constant in the 1993 cars, which definitely is wrong. This is going to be
particularly wrong when you go to the high-end SUV, because it's true that the
high-end SUV there was none. Ford Explorer was the highest
of the high-end SUVs. But everybody put
in a high-end SUV, not just GM, which
put it in a Cadillac. There's a Porsche Cayenne, there's the Lexus 300
series and then the 400s, it's different from
what they would get. But anyway, we'll
look at the answers. Just to show you the
problems you run into here; the two new SUVs we put in
were a Mercedes and a Toyota. The question is we're
going to put them in, we've got to put them
in at some set of characteristics. Is that clear? We put them in at the Ford
Explorer characteristics, but we didn't want the
quality to an XC to be the same as the Ford
Explorer because it was going to be a
Mercedes or a Toyota. We put it in as the average
XC of the Toyota cars in the market in that year. Then we did a price
regression on Xs and XCs. You could have taken
a random draw of XCs for Toyota and Mercedes. Is that clear from the
distribution of XCs? But you don't
believe Mercedes is a random draw from the
distribution of XCs. We put in the actual
average for the Toyota, and actually the low-end
of Mercedes one. The reason of the low-end one, because the high-end
one just was so much out-of-sample for the SUV stuff, we didn't believe it anymore. We're predicting quite a
bit out-of-sample because the Toyota's SUV
price was $30,240, we predicted, which was 4,500 more than not high-end SUV that
was in the market then. This is really high-end for
that and the Mercedes SUV was $8,000 higher than the highest-end
car SUV in the market. You're comparing a car
which was 25,000-33,000, which is different
private market. Remember this is whole dollars. This is a table for
putting in the Mercedes. FEMALE_1: I have a question. Ariel Pakes: Yeah. FEMALE_1: You only put there
an observable question which you were able to
explain [inaudible] but if you had to make an assumption
of the distribution of the XCs instead you wouldn't
be able to [inaudible] Ariel Pakes: Here's
what you could've done. I'm not sure I know
what you mean, but I'll tell you what we
did do first of all. There's a Delta there, so all we have to do
is add C to it. There's an issue. Again, an XC isn't unobservable. What I essentially said was
the assumption that XC given the characteristics is
0, it's probably wrong. I wasn't willing
to go with it for the Toyota and the Mercedes. Is that clear? Because it said the characteristic
Mercedes go with a high XC. But I didn't want
to go with that to taking a random draw. In reality, what's going on? The XCs are produced
along with the Xs. Is that clear? Even just from the cost of production side, they're going to be correlated. We don't know what XC is, but they're going
to be correlated. It's not like the
cost are additive and they're related
to the car size, the engine type, and
all that type of stuff. The hope in going into stuff like BOP and
things like that. The one that Aviv
showed you that I really like a lot is that the CT minus row CT minus 1, remember that assumption is 0. That made a lot of
sense to me and the reason it makes
a lot of sense to me is the Xs are fixed
for the product. Once it's introduced, XC can change because
people's perceptions of the car can change and new reports come out on the
car and everything like that. But those things
should be independent of the Xs because
they are already fixed at some level.
Is that clear? These are innovations that
people didn't know about when the Xs were
fixed. Is that clear? Because we have XC, which
was the initial perceptions, what might well be correlated
with X for many reasons. If Mercedes is pushing the car, it's probably a high-end car
with a lot of quality stuff. But now that I fixed
that and I look at the increment or decrement in the perception
of that over time, that may well be independent. Aviv showed you this. I think Andrew Sweden was
one of the first. We said this in VLP that
we should be doing this. But we didn't think
we had enough data in some footnote somewhere.
I don't remember. You can see what happened here. This is the high-end Mercedes. It did very well. It did very well in the market. The price was 33,000 and it
got a share of almost 0.1, which remember
only 10 percent of the people buy the car and there are 200
cars in the market. Got it mostly from
the Ford Explorer and the Jeep Grand Cherokee. Mostly from exactly who you
would expect it would get. That's what we're
predicting anyways, and people who switched
almost entirely, there was almost no gain in the total number of
people who bought cars. It was all from other cars. That was what
happened from there. I'm right on time, so
I'll just finish this. Now here's the
automobile division. These are the five
automobile cars and we just assume
they were shut down, and they were taken
out of the market and we recomputed the equilibrium. You can see why GM thought that it might be a
good thing to do. The Chevy Lumina, the [inaudible] Saber and
the Pontiac Grand AM, they're all GM cars. These are the cars that
gained the most share. The automobile goes
out, people switch to other cars and the question is, which cars did they switch to? In particular, did they switch
to GM cars or not GM cars? You can see what's
going on here. The first three are GM
cars and so is a Saturn. But in addition, there's
the Honda Accord, the Ford Taurus, and
Toyota Camry there. The rest of them,
those aren't GM cars, so some people are
switching to other cars. Ford Taurus was the
highest-selling car in those days. I think
it might still be. They lost all the automobiles but some people switched
out of cars entirely, but a very small
fraction of them. About 0.11 out of 0.237, which I guess is about 40
percent shifted out of GM cars, which is a big shift
out of GM cars. Again, they went ahead
and did this anyway, even though they were worried about the answer to this one. But they did it. They
took their time. They didn't vote 10
years after this. That's it for this lecture. 