foreign the topic this morning is control functions and related methods and I'll try to use my full allotment of time this time I think with Control Function methods like with much of econometrics part of it is just figuring out what the terminology is and I think what people mean by a Control Function method has evolved a little bit but I think it's more or less settling down and I'll try to um start with things that are probably familiar and then connect it to the more recent literature one thing to remember is that as I say in in topic three here about limitations of the Control Function approaches there's actually still a very broad class of models where we don't have any good robust approaches to estimation with endogenous explanatory variables so this is um just we're thinking of these as instrumental variables methods and they're usually applied in non-linear models although you know you know of some cases where they're applied in linear models as well so I'll briefly review that um so if we start with a linear model of course most of the time we estimate this in the simplest cross-sectional case with random sampling we usually use something fairly simple like two stagely squares or if we want to try to get a more efficient estimator and there's heteroskedasticity of unknown form we can use generalized method of moments but the point is we know that there there's a set of estimation methods that may or may not work well but that has to do with the quality of instruments and identifying assumptions the alternative approach that I'll quickly derive or or at least show you what the estimating equations are it would be surprising if they didn't rely on the same sort of identification schemes the identification gets a little different when we look at non-linear models but in terms of the linear model how one goes about identifying using a Control Function approach is the same as using standard instrumental variables approaches so let's um just start with equation one where the response variable is y1 and we have exogenous covariates Z1 and then for Simplicity I'm just considering the scalar case here where um Y2 is a scalar and then U1 is the error and then of course we need an exclusion restriction we need an instrument for Y2 and so this Vector Z has at least one more element than Z1 does now one thing to remember is if we estimate this equation by two stagely squares say then the the key condition I I didn't put the rank condition down on the on the slide the key exogeneity condition is given by two and if we estimate this equation by two stagely squares one thing to keep in mind is that it doesn't restrict the nature of Y2 at all okay Y2 could be a binary variable it could be continuous it could be have a tobit-like characteristics the the two stagely squares in GMM don't care about the nature of Y2 because in the first stage all of it's being imposed is this this reduced form which is a linear projection so equation three is is a is definitional okay I'm not worried about whether variances exist and so on for in any practical case we just assume that they do and so three is a definitional equation and so it holds no matter what the nature of Y2 is okay of course if we start making assumptions on V2 such as it's independent of Z or even just that it has a zero conditional mean then we start making substantive restrictions on the reduced form okay but the way that linear IV methods work is precisely not to make that sort of assumption okay that it's just it's just definitional okay but I the the point of this is is to try to connect that to Control Function methods in this simple case so everything so far is basically um you can think equation two of course is an exogeneity requirement but it's it's stated in terms of zero correlations that's actually going to play a role later on we have to in non-linear models state exogeneity in stronger terms than that but for now we we just state it like that and the Control Function method then proceeds by writing the structural error if you will U1 as a linear function of the reduced form error and then there's something left over again equation four is definitional okay so it doesn't matter that V2 might have some discrete characteristics for example it's just a linear projection where Row one is given there as the population regression coefficient so if you again when whenever we write down a linear projection by definition the error in that linear projection is always uncorrelated with with the thing we projected on and so E1 and V2 are uncorrelated well E1 is a linear combination of U1 and V2 so E1 is also uncorrelated with all of the exogenous variables Z which means that E1 is also uncorrelated with Y2 because Y2 is just a linear function of Z and V2 okay that's a lot of words to say something that's fairly obvious but the point is if we then essentially we take the the structural error and we replace it with this with this projection onto the reduced form error and we wind up with equation five so in this equation what have we accomplished well think of V2 now as a regressor we can't observe it of course but but we're going to estimate it but when we talk about identification schemes of course any parameter that can be consistently estimated we just treat as fixed so in equation five we have this um this new regressor V2 and now we have the error term is E1 but as we just went through E1 is now uncorrelated with everything on the right hand side including Y2 okay now of course if you look at that equation V2 it's a linear function of Y2 and all of the Z's if we didn't have extra Z's in equation 3 then V2 would be an exact linear combination of Z1 and Y2 and then that equation would be useless to us okay so so the fact that we can put V2 in as a regressor there means that we have to have something extra in Z that shows up actually in three this is exactly the the identification condition for two stage these squares okay so the Control Function approach says instead of essentially instrumenting for Y2 some people like to think of it as taking the fitted values from the first stage regression and plugging it in for Y2 that's that's certainly algebraically the same we leave Y2 in the equation and we add V2 as a regressor and that controls for the endogeneity of Y2 hence the name Control Function okay now to implement this of course we we need to do a two-step procedure and the first step is to estimate the reduced form just like you do with two stagely squares at least in the background if you are thinking of it that way and then in the Second Step you just add the V2 hats as a regressor and of course standard results on two-step estimation tell you that this is going to be consistent standard results on two-step estimation also tell you that the standard errors that you get from this regression are not going to be valid in general because of this generated regressors problem okay so V2 hat the way to think about it of course is what we've done is we've replaced Pi 2 with pi 2 hat which depends on the same sample and so there's sampling error in um in this regression due to the first stage estimation of Pi 2 hat and so you you could ask why would you do this regression instead of just using two stagely squares and the answer is you wouldn't but but the point is that when we get to other other applications this is quite convenient and they're really the the analogs of two stagely squares are are less preferable okay the re so there are two reasons why you okay there's one good reason for doing this regression so I shouldn't say you wouldn't the good reason is because this gives you a simple way to test whether Y2 is exogenous in fact this is equivalent to the Houseman test that compares the OLS estimate and the two stagely squares estimate is to just do a t-test on V2 hat that's valid because if you notice what happens when Row 1 is equal to zero the estimation error from PI to hat is no longer there so the the T statistic and typically you want to make this robust to heteroskedasticity but I should emphasize you don't make it robust to heteroskedasticity because a pi 2 hat okay that does not correct for the first stage estimation you want you make it robust to heteroskedasticity because the structural error might have heteroskedasticity okay so generally the adjustment you need to make if Row one is different from zero is not so straightforward as just making it robust to heteroskedasticity some time ago there was confusion on that point I don't think there's much anymore but just to emphasize that um okay so one reason to do it is a simple test for the null hypothesis that Y2 is exogenous the reason not to do it is the standard errors you can't just use the standard errors from this regression if Row one is different from zero and probably the even better reason is that this turns out to be identical to two stagely squares so you get this extra estimate estimate Row one hat which like I said can use be used to gauge whether um whether Y2 seems to be exogenous or endogenous but the coefficients Delta one and Alpha One hat Delta one hat those are the same as doing two stages least squares so probably that's why you don't see this discussed very much in the standard linear case because it reduces to something that we already know about okay um so as I said these are Control Function estimates and they really don't give us anything new what it what the approach does give us is a way to test for Exogen 80 housemen notice this in his his uh paper back in 1978 on specification testing so things actually get different once we change the model just a little bit so this is seven is still a linear model but it's that is it's linear in the parameters but now a non-linear function of the endogenous variable has been inserted or added so this is not a crazy model right we might want an endogenous variable to have a non-linear effect and the the nature of the exogeneity Assumption now is given an eight and this is actually critical for these kinds of models that it doesn't it doesn't um well I should be a little careful usually you need to to make an assumption like a conditional mean assumption um to uh to be able to to identify the coefficients there there are exceptions to that but um to be able to use a broad class of instrumental variables estimators for these kinds of models you you should replace the the zero correlation assumption by a zero conditional mean assumption as an eight okay so let's just take a simple case where we have one omitted variable Z2 that we've assumed is exogenous and is in the reduced form for Y2 okay whatever so so let's just say in the linear projection of Y2 on all the Z's Z2 appears there then what can we do to estimate the parameters of that equation well we need instruments for we need instruments for Y2 and Y2 squared and while this would be inefficient the um a valid instrument for Y2 squared is Z2 squared ruling out the case where Z2 is a is a dummy variable and then its Square would be itself so we would have to do something else in that case um so we could just estimate that equation by standard I I'll just say standard instrumental variables because what what one means by two stagely squares might might be wrong in this case but in any case we just list the instrumental variables as Z1 Z2 and Z2 squared and that will be consistent in general under the exogeneity assumption eight and a standard rank condition okay now what would the Control Function approach entail in this situation well now you you have to start making substantive restrictions if you're going to apply a similar method to the one we just did notice that equation 9 is now stated as a conditional mean and we're conditioning on the exogenous variables and the endogenous variable okay but in the end let's just assume for now that it takes on the same form as it did before that is it's a linear function of V2 so understanding this equation is actually critical to understanding the the recent uh Cutting Edge work on on these kinds of models because it's saying the following we know that Y2 is correlated with U1 okay or at least we think that's true of course we think that disease are correlated with Y2 also so U1 generally is correlated with everything okay if we just say U1 is correlated with everything we're not going to be able to get very far so we have to basically say that it's correlated with these things but in a specific way okay so this this is fairly specific it doesn't have to be this specific by any stretch but it can't be completely unrestricted okay so suppose that we say that well in fact it um it still is um depends on this reduced form error in a linear way V2 well now if we work through the the conditional expectation given in 10 and now notice this is not a linear projection this is a conditional expectation we get the following equation which looks pretty much like it did before both functions of the endogenous variable are still there and we have and we have this reduced form error excuse me and so the Control Function approach would be to estimate this equation by least squares after having done a first step estimation to get the V2 hats and plug them in okay now this is not the same thing as applying instrumental variables on that model seven where where you simply use say Z2 squared for Y2 squared in fact even coming up with other possible instruments of for Y2 squared is not going to produce the same thing as the Control Function estimates so now which what are what are the pros and cons of the difference between the Control Function approach and the instrumental variables approach okay the Control Function approach will be much less robust in the sense that there are now substantive restrictions imposed both on the structural errors and the reduced form errors see the linear the the IV stuff Y2 it wouldn't be of course a binary variable if we put a square in it but it could be some discrete variable with with distributions that that we don't even care to think about it could have a pile up at zero be continuous elsewhere anything basically goes as long as you take the equation and estimate it by Instrumental variables the Assumption in equation nine essentially for any kind of models you would write down for you want for let's say for v2 if Y2 were something like a discrete variable um account variable or something there's no way that you're going to get equation 9 out of that okay because it just to think of a sufficient condition for when you could at least get the first equality there is if the the structural and reduced form errors were jointly independent of the exogenous variables that's a really strong restriction when you write down a linear model for Y2 okay it pretty much rules out any kind of discreetness because you know so once a supportive of a of a random variable depends on the exogenous variables you can't have any kind of Independence assumption holding okay Independence is not necessary but something pretty strong is and so the Control Function approach um in this particular case it's less robust in the sense that it imposes significant restrictions on both the structural error and the reduced form error it's likely to be quite a bit more efficient too because all we what we've done is we've plugged in one function one Control Function to handle endogeneity of Y2 and Y2 squared at the same time the reason we're able to do that is because we've assumed that this linear reduced form has fairly strong properties okay so um how might we go about relaxing that well we could put in say V2 squared for example we could put it so those kinds of extensions where we allow the relationship in nine to be more flexible it turns out that's pretty much what a lot of the the recent literature is about okay it's about more than that in the sense that the reduced forms are going to be allowed to be a non-linear in disease as well but a lot of the recent work is basically basically rests on this notion that you can still get a relationship between the structural errors and the reduced form errors and and the exogenous variables don't show up there separately on their own okay so you you can fiddle around with that functional form okay and it's um and then so you would put in V hat and V hat squared V2 hat and V2 hat squared but the cost you the the reason we typically don't do that is because we usually think about um what are the weakest assumptions where we're going to get consistent estimates and the Control Function approach just applied to this simple extension of that model would be significantly less robust it really would only you you only have a shot if um if Y2 is continuous any discreetness and and we're going to uh probably not have a consistent procedure so just to show you that there that how this works when we explicitly recognize that Y2 has special features If instead of just using two stagely squares or the equivalent Control Function approach based on linear projections what happens if we actually recognize that Y2 is binary and we we want to use a Control Function approach based on that expectation in 11. well this is going to lead to something pretty familiar the expected value of U1 given Z and Y2 this can't be written as just a linear function of Z and Y2 anymore okay so this just shows you the probably the leading case once we deviate from a continuous endogenous variable what's going to happen but if we make some distributional assumptions of course we can compute that expectation because if we know a joint distribution for U1 and V2 then we know the distribution of U1 conditional on Z and V2 and from that we can get the distribution conditional and Y2 actually I'm jumping ahead a little bit there's really no V2 left here because I call it E2 down in the next line which is that Y2 follows a binary response model and so if we if we assume that U1 and E2 are independent the easiest thing to do is just to to make the joint normality assumption although as you can see I've written down somewhat weaker assumptions there um then you can def you can derive the expected value of U1 given Z and Y2 as in equation 12. often this is written out separately for Y2 equals 1 and Y2 equals zero I actually think it's quite informative to have it written out all at once because it makes you understand that this is a problem of endogeneity of Y2 it doesn't have anything to do with sample selection as uh as it does when we have missing data topic to be covered later so equation 12 derives the Control Function in this case this is sometimes called a generalized residual in um in literature on on limited dependent variables so this this generalized residual or function of the inverse Mills ratio gets added as an additional regressor and this controls for the endogeneity of Y2 okay so again and so of course this this leads to the celebrated two-step estimation method where in the first step you do a probit and then you compute inverse Mills ratios and again there's in this model you wouldn't do any you wouldn't do separate regressions for Y2 equals one and Y2 equals zero because that wouldn't impose the restrictions on the coefficients that they're the same in that that there's no interaction basically with Y2 so you get this method where you add this uh this generalized residual and that corrects for the endogeneity of Y2 so how does this compare with just estimating the original equation by Instrumental variables again this is much less robust because it takes the probate model for Y2 quite seriously and not only that it actually essentially makes the normality assumption on U1 which remember is the structural error term so U1 couldn't have heteroskedasticity because basically U1 and and E2 are both being assumed independent of Z so the Control Function method based on deriving this expectation in 11 . again it's going to be more efficient than using instrumental variables it uses a lot more information and in fact you could do the most efficient Thing by using a joint maximum likelihood procedure okay under joint normality but it's costly because if anything about the The Joint assumptions The Joint normality and Independence assumptions fails then this is no longer going to be consistent I noticed that stata does exactly this and what it calls its treat rig command but the question is why why do it um and the answer is well if you want a more efficient but not as robust estimator you would do it there are reasons that it will adjust this model a little bit and talk about where this sort of procedure becomes fairly indispensable if you want to to estimate an average treatment effect okay but in this particular case there is a procedure that's perfectly suitable and that is to just use two stage lease squares by the way um just to complete this thought I often get asked because when Y2 is binary people are just dying to do something about the fact that it's binary right that that there's this endogenous right hand side variable in its binary and two stagely squares just doesn't seem sexy enough or something that you know you're just using this linear reduced form for this thing that's why too and that's a linear probability model and isn't there something wrong with that well no of course there's nothing wrong with it but if you wanted to do something different then you could act as if the probit model is your best guess at what the the right model for Y2 is an estimated probate and get the fitted values and then use those as instruments not as regressors right you don't want to use those as regressors you want to use them as instruments so this gives you a way to to you know use the binary nature of Y2 if you must but in the end all that matters is you're only going to get a very different answer if the probit model predicts Y2 very differently from a linear probability model that often it it often doesn't actually make much of a difference okay but that's just as robust in the sense that as long as you use those as instruments and don't take the probate model seriously um it's fine and it turns out there's a set of assumptions under which it's the efficient IV estimator and those two assumptions are given at the top of the slide um okay so if the probate model happened to be true and the structural error was homoscedastic then this turns out to be the efficient instrumental variables estimator so that is one way you might want one reason you might want to do it okay so this um those examples are in the case where if you want to think about where the coefficients are homogeneous so in the treatment effect stuff that Guido talked about yesterday this is that's the case where you're assuming constant treatment effects so I'm taking a a less modern approach to this sort of stuff and saying well suppose we now write down a model explicitly with a random coefficient on the endogenous variable Heckman and vitlacill call this a correlated random coefficient model which is uh which is a useful terminology because the idea is that A1 is a random coefficient so this is written in the population where I'm I'm trying I try to stick to the convention that anything that is Greek is a parameter and anything that's not Greek is not a parameter okay so A1 is a random variable that interacts with Y2 and then its mean is Alpha One which would be the average pop the population averaged effect or if Y2 is binary it is the average treatment effect okay so what happens in in these kinds of models IV versus Control Function well this is a this is quite a bit harder to deal with as it turns out interestingly enough you would think this is a is a modest extension to the basic model but now anything you do doesn't come for free because you if you if you really want to estimate the average effect you have to start making some assumptions and so one way to see the problem is to write the equation as in 14. where now the the model has been written with a constant coefficient on Y2 so Alpha One is the parameter we hope to estimate but now there's this interaction term where V2 which think of as the unobserved part of the the individual specific deviation from the population mean interacts with the endogenous variable now if V2 sorry if Y2 were exogenous this would be a minor nuisance right this would just induce heteroskedasticity into the equation and you would you would ignore it right that we in fact um this the whole literature on random coefficient models once you assume that the coefficients are independent of the covariates you you have almost nothing right because it's just a particular kind of heteroskedasticity that's introduced it becomes much more interesting when that thing interacts with an endogenous variable and the coefficient is allowed to be correlated with the endogenous variable so if this is an earnings equation where y1 is log of earnings and Y2 is say schooling then this model allows say unobserved Talent OR ability to it allows the return to schooling to depend on unobservables like talent and ability and so on and it also allows those things to be correlated okay so that's this that that certainly is is an attractive setup if you can estimate it um and so the problem with applying standard instrumental variables methods to this to equation 14 or or thinking of it as 15 where you just say well leave that interaction in the error term and let's just move ahead um is that is that this term V v1y2 down at the very bottom is in general correlated with any function of the instruments that we can think of okay and so we have a situation where the error term is correlated with the instruments um so the um the problem is not just that the even if we make the Assumption in 16 which again is a standard kind of exogeneity assumption although it is stated in terms of conditional means this isn't enough for IV to be able to consistently estimate that average effect a sufficient condition though is that the covariance between V1 and Y2 so think of you can think of this in terms of actually the reduced form error for Y2 also if you want although um that you don't actually need to write down any kind of particular functional form to think of this condition a condition that is sufficient is that even though V1 and Y2 might be arbitrarily correlated so this does allow for correlation between the slope and the endogenous variable it rules out the case where the covariance is a function of Z okay now so this is this is an assumption that can hold under joint normality although as it turns out certainly just saying that yeah this is a sensible condition for cases where Y2 is continuous is is much too strong in fact David card showed examples where this condition clearly is not true in exactly the example that I pointed out the schooling equation the earnings equation with schooling endogenous so this assumption is hardly for free even if you're even if you're thinking of the case where Y2 is is roughly continuous we'll call it that nothing has ever really continuous so the question is is it continuous enough and certainly 17 fails in the case where Y2 is discrete that is if you think of Y2 being a binary variable that follows a probe at 17 simply is not true okay so um but it does give us something and it actually gives us something to compare with Control Function methods and other kinds of methods so in the notes um this Control Function approach that was suggested by uh Garen in 1984. under joint normality for everything so in other words the structural error U1 the random coefficient A1 and the reduced form error V2 if those are jointly normal then you can derive a Control Function approach for this problem actually you can relax those conditions a little bit of course as always but not a lot that that makes a big practical difference I think and the Control Function approach in that case is simply to allow to add not just the reduced form residuals but the reduced form residuals interacted with Y2 itself to the equation okay and that you can show under those assumptions that does produce a consistent estimate of the average treatment effect okay but again the key assumptions there are um the essentially putting a restriction on a very strong restriction on the reduced form of Y2 conditional normality with this with a linear mean and homoscedasticity okay so that's that turns out to be um those assumptions are quite a bit stronger than in in equation 17 which while ruling out discreteness and Y2 certainly does not require Y2 to be normally distributed okay and it doesn't require anything notice that the structural error U1 doesn't show up in that condition either so that that's pretty much unspecified except for 16. so basically the argument so far is that the the standard instrumental variables estimator it looks pretty good at least in terms of robustness now whenever you want you do these things of course you you want to compare precision and as I said the Control Function method assumes more so you you should get something for that but if you apply garen's method as at the bottom of page nine then of course you have to adjust the standard errors to make that comparison Fair okay now um so let's continue with this a little bit I I will get to stuff that's that's newer but I do think it's useful to see how all of these things fit into basically one framework and then the issue is are you able to use an IV method or do you really have to go to something like a Control Function method that imposes more assumptions and in the cases where there's no heterogeneity in the treatment effect I mean this is basically a conclusion from hedo's work and then then you don't have to worry about it right you just use standard IV methods and and move on but if you are worried about heterogeneity then the nature of Y2 suddenly plays a big role and so if we now changed the model sorry we don't change the model we we go we still start with 13. but now y1 is a binary treatment okay so this is a case explicitly with a binary endogenous variable but where the treatment effect varies by individual now what are we going to do with this well 17 can't be true okay and we know what the IV estimator estimates in general it estimates that the local average treatment effect okay might we be so heroic to think we could estimate the average treatment effect well this is you know this is the old-fashioned way of going about this the answer is yes if we make enough assumptions so that we can derive this Control Function method and that's given by this um unnumbered equation which I left unnumbered I think because it made it go off the end of the page although I see didn't have to do that um so so this is a an equation that you often see in the context of so-called switching of regression models and this H2 is again this so-called generalized residual function and now what happens is the Control Function approach leaves y2n it in it includes this generalized residual which is a function of the Mills ratio just as before but now you also have to interact that with Y2 itself okay and now this produces consistent estimates of the coefficient Alpha One on Y2 and so of course the two-step method is to do a probit and then again this is not the switching regression does this and in two steps where the implicit assumption is that Y2 interacts with the um with the exogenous variables as well and you can do that by of course just adding Z1 interacted with Y2 to this equation so this this I think is actually the cleanest way of stating what's going on it's no different from Computing e of Y one given Z of Y2 when Y2 is continuous in principle it just looks different because we have to make sensible assumptions about the distribution of Y2 so um one thing that I haven't actually seen in this uh using this approach is when when you take this as a Control Function method where it's being used to solve the problem of endogenous um random coefficients that are correlated with the endogenous variables then you might want to allow the coefficients on the exogenous variables to be random as well the switching regression framework doesn't usually doesn't seem to allow for that but interestingly it it's not the case where you just say well that that's that's on an exogenous variable I can just leave that in the error term you can't do that anymore because you're conditioning on Y2 in this expectation and so the most flexible thing to do I guess I didn't write it out but I oh there it is at the bottom is to interact this generalized residual with everything because that would allow the coefficients on Z1 to be random as well although there's a joint normality assumption underlying that for the random coefficients but anyway if you're if you're going to allow heterogeneity on Y2 why not on the Z ones as well okay okay so that's all for models that are linear in the parameters and with that background we can now turn to models that are where the structural model is not a linear model and um unfortunately we don't really have simple things like linear instrumental variables to apply in these cases actually I'll take that back we do have those things we can just say suppose y1 the response variable is binary then we can just estimate a linear probability model treating Y2 as as an exogenous variable Josh angris certainly has pushed this view that functional forms when you're interested in average effects they don't they don't seem to matter much anyway and so why not but again presumably not everybody is taking that that view so so the question is what else can be done and you'll see in some ways I think Josh's view on this is is basically frustration because there's actually very little that we know I mean that econometricians in a sense are nibbling on the edges of this problem because once you once you allow General discreteness in y1 and general discreetness and Y2 we don't really have good good answers basically you can always say model them jointly and do maximum likelihood right that that's always an answer but it's not um it's not perhaps what we're looking for so in this particular case let's just look at a binary response problem first and unlike in a linear model where the nature of Y2 wouldn't matter the nature of Y2 matters a lot it determines basically what we're going to do in terms of whether we can actually come up with a sensible Control Function procedure or or some sort of two-stage lease squares analog of a linear model so Blundell and Smith and rivers and Wong looked at this this model where Y2 is a continuous variable and in fact it's it's more than continuous there there have to be some restrictions put on it and again this is um a little stronger than necessary but the the way to think about the kinds of restrictions that are needed are that the reduced form of Y2 is this well it's it's a classical linear model and then jointly the U1 and the V2s are independent of Z and then um and bivariate normal in fact then when you work out what the Control Function is in this case which again so equation 22 this is exactly the sort of thing we derived before because y1 is binary this is the expected value of y1 given all the exogenous variables and Y2 and it does have a probate form but it has a probate form because of all the normality that's that's floating around in in this so and notice also of course when when you derive this equation 22 the coefficients all get Scaled by a conditional variance okay which is um which is 1 minus rho 1 squared which is the correlation between U1 and V2 okay and so the two-step method in this case the Control Function approach is to get the residuals from this linear reduced form and then do a probate where you add these residuals and again this controls for the endogeneity of Y2 the nice feature of this procedure is that like in the linear case this gives you a simple a simple test for endogeneity and in fact the test is valid quite generally you don't need to assume because under the null X2 sorry under the null Y2 is exogenous and so it doesn't really matter whether it has a normal distribution or anything like that this can be treated as um that this is just a a simple specification test the um so once you've done that it turns out you you certainly have enough information to recover the so-called the the structural parameters that show up in equation 18 which means that you can compute the the so the average partial effects where where U1 gets averaged out because it's not observed um but for reference later it's also possible to compute the averaged Effects by an equation like 23 which um so X1 it just contains Z1 and Y2 and so the the so-called average structural function which we're going to then take changes in derivatives with respect to the elements of Z1 and Y2 can be estimated as an equation 23 where notice what's happening is we're averaging out those reduced form residuals um and those are notice those are the scaled coefficients that are in that expression we we don't go back to the original coefficients and then compute this we actually use the scaled coefficients that are obtained from the second stage Control Function estimation the um one thing that's useful about this approach I don't know how many of you deal with so-called fractional response variables but the same sort of method works if you start with an omitted variables formulation as in 24 where y1 could be anything that sensibly has a mean that's bounded between zero and one that is some sort of fractional response and then if you go through the same kinds of manipulations you get exactly an expect you get a control function that looks exactly like the one we already derived the difference is that y1 no longer has a known distribution and so you're not doing a second stage maximum likelihood you're doing a second stage what's usually called quasi-maximum likelihood which is perfectly fine because um because the the Bernoulli log likelihood has this nice property that as long as you have the mean correctly specified you can you get consistent estimates even though the distribution is completely wrong so you can do that or or of course you could do non-linearly squares which is always um consistent for for conditional means so again the the point is that you can do all of this where y1 is just some any variable that's between zero and one it could have a pile up at one or zero or zero or both or somewhere in between well if you if you believe Model 24 basically and you're willing to make these kinds of normality assumptions the same thing goes through and you can estimate the the partial effects based on equation 23 as well okay okay and then in the a paper that um that I did you can make this more robust or sorry more flexible you can model variances and so on to allow heteroskedasticity um but I'll let you look at the notes for those sorts of extensions now this Control Function approach in this particular setup can be compared with another approach that is is commonly used and that is so that this is the attempt to mimic two stage lease squares that is in the first stage you regress Y2 on the Z's just as you do for the Control Function approach the difference is whether you you get the fitted values Y2 hat or the residuals V2 hat in the in the quote two-stagely squares approach you take Y2 and of course you replace it with Y2 hat that that's that's the analog um it's hard to figure out when that would be preferred to the Control Function approach for for a couple of different reasons first of all it it doesn't deliver a simple test of endogeneity for Y2 okay that's probably the the most important reason secondly the parameters that you get are a real mishmash of of the things that you wanted to begin with and there's no simple formula like equation 23 to get the average partial effects okay so um it so the question is do you get consistent estimates of the parameters after you undo everything the answer is yes but you can see that those um actually I didn't write down what this if you look a few lines down there's this there's this parameter Omega one and it's the scale factor that shows up in doing this procedure you know it's a complicated function of well it's a function of variances and covariances and so you have to to figure that out it certainly it's doable but it does give you different scaled coefficients too so you do want to be careful if you if you do the quote two stage these squares approach and you do the Control Function approach these these coefficient estimates have different scales so you shouldn't try to compare them and say G one of these is wrong they're just scaled by different things and my argument is that the scale that the Control Function method uses is more useful because of equation 23. okay plus I know no one in this crowd would ever do this but this this avoids the temptation of when you add Y2 squared to the model that you're not going to plug in Y2 hat and square it right and in the Control Function approach you you can't tempt yourself to do that because adding the V2 hat controls for endogeneity no matter how Y2 shows up okay that's another advantage of it is that I didn't emphasize this but if you really believe Y2 has this linear reduced form with an additive normal error then Y2 can show up in any way in the structural equation that is anyway interacted with say the um the exogenous variables or quadratic and so on it can't have a random coefficient but it can it can be a fairly uh complicated nonlinear function so um I think that's a clear Advantage for the Control Function approach in this case so you can see if you just simply added the extension that you wanted Y2 to show up as a quadratic or added or want want it to interact with the exogenous variables very sensible things then the Control Function approach while maintaining a lot of assumptions is easy and it doesn't maintain anything more than the quote two stage lease squares approach which is which is wrong okay it's not just a matter of is it less convenient it's just wrong in that in in those extensions Okay so um I don't want to talk about this too much but to show you the again the the somewhat sad State of Affairs in some ways about where we can what else we can do is that if you if you take this equation and you add a random coefficient to Y2 well you can you can work this all through again with Y2 as a um um uh as normally distributed and so on but when we go to to talk about extensions to um to cases where we say non-parametrically um we don't specify say the function fee in any particular way those sorts of extensions are um actually I take that back we we can't we can handle things like that up to a point as long as Y2 is not discrete I'm jumping ahead a little bit okay that when Y2 is continuous we can actually do a lot okay but um let's see okay so there is some work recently that allows us to estimate coefficients up to scale in this binary response model where this we just of course write y1 as this indicator function and then the goal is to estimate the coefficients Delta 1 and Alpha 1 up to some common scale so this is a case where the interest is clearly not in the magnitude of an effect but in the direction of the effect and how what its size is compared with other other explanatory variables and this is actually a hard problem in general the um and and lubel's approach it it assumes a fairly strong assumption and the assumption is that there's there's a special regressor in this in this model and that's the special regressor that's not my terminology that's actually his um the special regressor the easiest way to think about it is it it has to appear in the model and in fact it's it's coefficient is normalized to one and it it basically has to be independent of everything else okay so it has to be independent not just of the exogenous variables but also of the endogenous variable and you can think of some cases where that's true but you can't think of I think many cases where that's true so um even if you're just interested in the coefficients it's um it's a little hard to see how that would be generally applicable Okay so let's um let's return to this case where we actually have a parametric model and start talking about where where things stand in terms of relaxing distributional assumptions the um basically if y1 if Y2 is a discrete variable very little is known and um the the even the semi-parametric non-parametric stuff I'm going to mention really excludes that case so if we start even with a model which is parametric like at the top of that slide and then we say y1 Y2 is hidden in X1 somewhere in fact it could even be the standard additive structure but Y2 is just some discrete variable basically we've failed in providing good good estimators for that case there are of course some poor strategies that one might hope work but don't you know you can try you can try plugging in fitted values from a first stage estimation um so in the in the binary response case why you might model Y2 as a probate and then and then try to stick it inside a second stage probe it and we know that does not work although as recently as three weeks ago somebody contacted me wanting me to tell them that yes that did work it doesn't so that basically the strategies we have are to do a joint maximum likelihood because the control functions see the problem is there there's no simple way to derive a Control Function except by having a joint distribution okay so if you have the joint distribution then you might as well do maximum likelihood so what you would like of course is to be able to derive a Control Function approach that doesn't rely on distributions for the reduced form but we really don't have those methods available yet um by the way as a as a practical comment I think this is pretty pretty widely known now but again this this has come up fairly recently when you um if you have y1 and Y2 that are both binary and you're in the situation where it's an omitted variable context and why one is um assumed to be a function of Y2 and you think Y2 is correlated with some omitted factor and so Y2 you're you're willing to specify that that reduced form of Y2 follows a probit um the question is can you apply so-called bivariate probit to this case to estimate everything even though the bivariate probate model is typically written with exogenous explanatory variables and the answer is yes you can because of the way the likelihood function factors so you can actually include Y2 in a completely General way in in the structural model as long as its reduced form follows a probate and use the so-called bivariate probit approach to estimation okay um so one of the one of the um key features of the rivers Wong approach Control Function approach for the probate model is it has normality floating around all over it okay so or at least there's normality somewhere and this becomes a problem to apply to multiple multinomial responses because because multinomial probate is computationally harder the effect the two stagely squares estimate is minus .012 on the labor force participation probability where where other income is measured in thousands of dollars okay if you use the rivers of long approach you get minus .011 so you would hardly say that somehow two stage lease squares is leading you astray in that case and in fact now that's the average effect of effect of course so if you looked at extreme if you put in more extreme values of the income variable then you you should see differences but in terms of getting the average effect once again the linear model seems to do pretty well with that and in fact that turns out to be the case also that I I do have an example I didn't write it down for the exactly the case where you do a linear probability in in each stage and in fact um that does raise an important I I mentioned this before but I'll say it again the the lack of procedures that are short of just saying do full maximum likelihood when you have discreteness in y1 and Y2 I think it helps to explain why people are just doing these linear models and saying you know evidence shows that this gives a pretty good estimate of the average effect in lots of cases okay not always of course it might just be estimating the effect of the compliers for example but certainly when you compare non-linear procedures like the river's Wong with these things that are based on just linear models you often find this this close Association in the estimates okay so um so as I was saying when we turned to multinomial responses because multinomial probate gets difficult to estimate with with many Alternatives um although hedo will be talking about that a little bit um day after tomorrow I guess um the still clearly the simplest way to go is to use things like logit and nested logit the problem with trying to account for endogeneity in those models is that the normal distribution doesn't mix very well with the logistic distribution and so even if you start with an equation like 28 which I've now written Y2 to be a vector that it as long as everything is is normally distributed you don't really have to worry about having one or more of them the the problem is if you write down 28 and you start with a structural model which is a multinomial logit conditional on some unobserved thing and then you try to derive the estimating equation based on that you get a mess basically so there are some who are proposing to and I I think partly this is influenced by this recent recent literature on average structural functions and so on is basically they're saying well look whatever you start with in the structural model is pretty much arbitrary anyway so why don't you just do something which is simple once you get to the thing you can actually estimate which is in this particular case would be to model that probability that's two lines below 28 directly so in other words once you have established that you can actually identify the objects of interest from that probability then you can think about it as just choosing a a sensible approximation to that probability where in the end the V2s are going to have to be integrated out averaged out in order to get the partial effects but that's very easy if say you specify a multinomial logit or a nested logit at that particular stage now whether this is going to catch on or not I don't know but there's a paper by Petron and Trane for example that that does this and actually they compare it with the Barry Levinson and Pecos approach and actually get very similar estimates um and and that will be talked about tomorrow as well so I don't know if this is a general strategy that one should be pushing but the point is if we get to when we get to the semi-parametric approaches it's really quite in the spirit of that except that you're you're saying I'm going to use a parametric model instead of some really flexible non-parametric approach okay in terms of there are other selected cases let me just talk about this briefly where you can actually get somewhere with a Control Function approach but it again it's it's fairly Slim Pickens when Y2 is binary or or discrete in general one case is where y1 follows an exponential model and so why one could be a count variable or it could be just some other non-negative variable where you're willing to assume that the the mean response is an exponential form and again the way that this is this is usually done is by adding an omitted variable R1 so this is thinking of a case where again you would like to control for R1 you can't and R1 is correlated with Y2 so starting from here then there are then because the exponential function is relatively easy to work with right because of its its properties that the exponential of the sum is the product of the exponentials you can you can derive Control Function methods in this this case as well so why one can have some discreteness certainly like I said it could be a count variable why one could even be something that piles up at zero and is roughly continuous of course we usually model that as some sort of Tobit type model but there's nothing that says we have to if what we want is just a functional form that keeps the the expectations to be non-negative okay so there's nothing that says this this equation 29 can't be applied for basically any any non-negative response variable provided we've settled on being interested in the effects on the mean response okay so there are a couple of things that have been done um in the case of where Y2 is a continuous variable not surprisingly there's a set of assumptions under which adding reduced form residuals um to the exponential model and then using either non-linearly squares or some quasi-likelhood method does produce consistent estimates a terza worked through the case where the Y2 is actually binary and follows a probate and there is a Control Function that's written again it's the it's the equation above equation 30 that um well it's messy but certainly you can see that what you have to do in a first stage is a probate and then you have to do some sort of non-linear estimation in a second stage so that there are methods in the exponential case that I think the Control Function methods are in the exponential case though they're certainly going to be efficient lots of times or more efficient than an instrumental variables approach but there is an IV approach that works fairly generally and it was worked out it's actually a very simple thing to work out once you see it by malayhee and that is to basically just find moment restrictions that can be used in generalized method of moments and so 32 is a set of moment conditions that generally identifies the parameters and the important thing is it doesn't restrict the nature of Y2 so Y2 could be discrete it could be continuous it could be something else and the only thing that that needs to be assumed basically is Independence between the omitted variable R1 and the instruments okay but that's much weaker than imposing reduced form restrictions on Y2 okay which in general is you know if we have a continuous variable or binary we might be willing to do that but there's no reason to do it and this certainly Works more generally okay so I should at least talk about the semi-parametric and non-parametric approaches also um so let's just return to the binary response case and see what it is that has been been learned here if you write down the equation again as y1 is is a an indicator function then the question is where Y2 is actually inside X1 and it could be in there in in basically any way but maybe just think of it as showing up additively then Blundell and Powell have this paper where they showed how to how to estimate the so-called average structural function when Y2 takes on has a reduced form given a few lines down actually I guess I could use this right there where G2 is actually unspecified okay so you don't have to have a linear reduced form or even any parametric function you have to have this additive structure though so they consider the case where you have this additive this reduced form with an additive error that is independent of disease okay so if you're hoping that this somehow is going to apply to the case where Y2 is discrete it doesn't okay because one again no matter what you choose G2 to be tacking on an additive error and then saying it's independent of Z is imposing real restrictions that basically don't hold when we have discrete responses um but if you do assume that in fact basically you assume jointly that U1 and V2 are independent of Z then it turns out you can estimate uh the parameters up to scale and you can actually estimate the the distribution function for U1 they show how to do that and so you can estimate both parameters and you can estimate the average effects the partial effects so it's nice because you can get estimates of both things and you don't have to make a distributional assumption for U1 you don't have to make a distributional assumption for v2 but it doesn't help when Y2 is discrete so that's why I said there the the cases where we really don't have anything else we we still don't have anything else except maximum likelihood um okay you can you can go even um even further with this and not put parameters in at all so if you just write um let's see I haven't oh oops yeah if you look at um this equation here you can start off with an equation like that where y1 is some unknown function of the exogenous variables the endogenous variables and then this unobservable U1 if you still maintain this additive structure as in 36 then you can estimate the average structural function very generally which means you can compute partial effects so no functional forms are here anywhere but you still have this assumption so even though the you you one can appear in a nonlinear function in affecting y1 it's not allowed to appear the the reduced form errors here have to appear additively and be independent so again this certainly can give you an analysis that's more flexible than say doing a probate or the river's Wong approach but it doesn't get at the issue of discreteness and why too okay so remember I mentioned these this this Movement by people who are trying to come up with with simple estimates in these cases the fact that you can estimate something interesting by starting here and using this and then just estimating this function right here um it really is is in some ways justified by this uh the the approach of then doing parametric models in the second stage is is somewhat justified by this it's just saying that instead of doing full non-parametrics in the second stage you're going to do parametrics in the second stage okay but again we usually we're I think psychologically once we start doing a parametric model that can't be derived from an underlying structural model we start getting queasy about that even though as a practical matter it's not really clear why we we get so queasy I I think now in Ben's a Nui so the question is can you relax this additivity structure oops there in 36. it plays a key role in in the bundle and pal work where it's independent and um and clearly that doesn't cover all cases okay now can you can you relax that assumption it turns out you can um and in fact you can allow Y2 to be generated as in 37. as in um the case considered by imbenzanui now and this allows for interactions between the unobservables E2 and Z but it it does require a strictly monotonic function it has to be strictly monotonic as a function of E2 so this rules out and E2 here is a scalar so this this again rules out discreteness in Y2 unfortunate you can't so if Y2 followed a probe it then it's it's a monotone function but it's a step function of E2 so um but nevertheless this does relax that additivity restriction and um so far except for adding inverse Mills ratios and so on the the control functions we've seen are residuals from A reduced form an added a reduced form whether those are obtained from a linear regression or a non-parametric regression they're just additive residuals so the question is are there other control functions that would fall out depending on what the the framework is and in this case imbenzanui show that a Control Function it's it's interesting is is actually to evaluate the conditional CDF of Y2 given Z at the data and that that enforces this conditional Independence assumption that you need in order to implement the Control Function approach so that's a way to um to allow for a general unknown function G2 but with monotonicity in the unobservable okay um so that's pretty much is um I think this the State of Affairs for for Control Function methods for cross-sectional applications now for panel data one way to view these um the correlated random effects models that we talked about yesterday is that those are essentially Control Function methods also that what you're doing is you're adding the time averages of the covariates in the simplest case as a control for the unobserved heterogeneity so in a way you I I could have called those Control Function approaches and in fact altanji and matskin I believe do label them that way and so it does make sense that one could combine those sorts of approaches with approaches that allow for more traditional forms of endogeneity and so um General things are General results aren't really available so I'll just quickly take you through how one might do this in a a recent paper with Leslie papke on on essentially either you can apply this to a probate or a fractional response but the setup here is there's unobserved heterogeneity and a panel data and then there's a key explanatory variable that's thought to be correlated with not just the heterogeneity but also time varying omitted variables now in the in a linear case we would know exactly what to do we would use a fixed effects IV method so-called fixed effects two stagely squares because that would account for the unobserved heterogeneity by removing the unobserved effect and then we'd have an instrument and so we would just do linear linear IV estimation in this particular case the response variable is can be binary or actually in the particular application is is a fraction it's the fraction of students passing a test at the either the school or the district level so the idea is to instead of using a linear model for that seeing if some sort of non-linear model gives you very different answers and and allows you to look at effects at different at different points in the spending distribution so this is this sort of example where especially when y1 is is a fraction and can take on values zero and one so distributions for that you know you can try to use a two limit toe bit but you know maybe you don't want to impose that much structure if you're interested in a mean effect so basically this is just an exercise in showing how you can take a specific example and come up with an estimating equation that allows for these features okay it allows for them in a restrictive way but if what you're going to do is just do two stagely squares accounting for the for the unobserved heterogeneity then perhaps it's useful to apply that to some method that allows for for a non-linear response so the um so as I said the key is in this in this expectation 38 Y2 is the endogenous variable it varies across time and across individual it was it's an average spending measure actually in the application we did the z's are assumed to be strictly exogenous the heterogeneity so think of this as school level heterogeneity is ci1 and then there are these time varying unobservables That You Don't See but maybe School administrators see and maybe you know are fiddling around with spending based on the crop of students they see coming through um or you know there are lots of stories about why spending might be correlated with time varying things that you don't observe not just the so-called School fixed effect okay so this this setup as I mentioned um the the practice of using two-stage fixed effects two stagely squares has become fairly common as a tool for policy analysis and so this is supposed to be mimicking that but allowing for a functional form that's more consistent with the response variable that that we have okay so um this device the Chamberlain device in 39 is is one way to get started and that's to say we don't want to assume that the ins the exogenous variables are completely independent of the heterogeneity the instrumental variable is is what's known as a foundation allowance which is has to do with the way Michigan is funding its its schools currently I won't go into that but it depends exactly on initial conditions of where students where schools what schools we're spending before the policy change so you can't just say the school heterogeneity is completely independent of the instruments so what can you do well you could use the more the more General device where you allow um uh the heterogeneity to depend on the whole time time path of the instruments in an unrestricted way as it turns out that doesn't matter at all in this application so I'm writing it as a as a linear function of the time average and then um when you plug this in is you get the equation at the bottom of uh of page 39 and there's still an unobserved variable in there which is RIT but it's the um it's now the sum of two pieces and both of these pieces now are independent of the exogenous variables okay that's that's the key so by by doing this first step where you replace the heterogeneity by by this we now get this this leftover unobserved effect which is now independent of everything and so the method that works with at least if you have a continuous Y2 that you can think of as having a linear reduced form like that is to estimate this reduced form by actually by pooled OLS which is the same as doing fixed effects because I have the zi bar in there and then there are these residuals vi2 that are going to be inserted in the second stage so the question is when does when can you make all of this work well basically the uh when you make when you make the assumption that all of the unobservables are normally distributed you get an estimating equation that you can easily work with and the estimating equation is in 43 so what happens is each time you do one of these substitutions both to allow for the endogen for the heterogeneity and then secondly for the contemporaneous endogeneity you rescale the coefficients right in a linear model this wouldn't happen you would just be adding stuff in the coefficients of Interest would be staying the same but because of the non-linearity the the coefficients get rescaled well happily in the end it's exactly those scaled coefficients that you want to estimate in order to get the partial effects so after you know making certainly some strong assumptions you can get to equation 43 but now this leads immediately to a simple Control Function approach where you put in both the time averages and then also this um the these reduced form residuals from the equation for Y2 and since Y2 is a variable it's log of spending it's not crazy to to think of that as having this reduced form with an additive independent error it's not completely general of course so um again you can estimate this very easily using two steps you do an a fixed effects regression and the first step get the residuals and then you do something like a pooled probit which is um which is generally consistent even in the fractional case to get the scale parameters and then the um the average effects are given in equation 45. so this is a very straightforward procedure and um you know they're there there was nothing else out there that we could use except except we could do the a combination of the altanji madskin and bundle pal stuff and what that would do is in the first stage you would do something more flexible than just linear fixed effects and in the second stage you would do something more flexible than just a probe response function and and in the end you know you can ask whether that matters very much in this particular application um so I'll stop after just to give you some idea about whether this matters um the the fixed effect IV estimate of the spending effect so this is a 10 increase in average spending uh increases the pass rate by 5.6 percentage point so that's just using a linear model of course including aggregate time effects um controlling for poverty rates and and a couple of other things and you get you know a large average effect of course it's not measured that precisely it's it's standard error is about 2.2 if you do the non-linear model okay as I suppose and one one way to think of it is you're imposing a lot more in restrictions on the distribution of heterogeneity and you're trading that off for a more realistic functional form for the response and the question is do you get very different answers um no you get 5.8 instead of 5.6 and you get a standard error that's a little larger than you do for the linear two-stage lease squares now so so are these kinds of non-linear models a waste of time well of course what happens is if you look if you look across the distribution of spending you get different effects and so one thing is the average effect is about 5.8 but let's see I have this written down if you look at the upper and lower quartiles over different years the effect ranges from about 4.9 to 6.6 so you know 5.8 is basically in the middle of that and the question is you know do you you get much wider variation of course if you look at the the lower and upper deciles well not much wider you get somewhat wider variation so then the question is you know do you care about that sort of thing and the answer is well it's it's an alternative to the linear model and you can you can certainly try it so is it the final comment is this is exactly the sort of problem where this more recent work can be applied and you know with I don't know if you have 500 or so school districts you know is that sort of data set up to anything more than than what we did here I I don't know that certainly simple checks of putting in quadratics and interactions didn't make any difference and so you wonder whether doing something like allowing full non-parametrics is is going to make a difference either for for at least the average effect since since the linear model gives you basically what the nonlinear model does in that case okay so this is a good good area to keep an eye on to see what's going to happen with with um the case where Y2 is discrete because in this case this hinges critically on this linear reduced form per Y2 with an additive independent error as do do most of the other methods questions then yes to mentioned before not robust how far how often do you get other measures and uh yes well saying things like linear models yeah I should be a little careful with that right but it is true that that if you're talking about averaged effects it's not too surprising that a linear model often does pretty pretty well for that so if if the idea was to say apply let's just take this example I went through to apply a non-linear method to to get something that's more efficient okay now a couple of things about that we just did a pooled probe in the second stage we didn't try to do any kind of GMM procedure or anything like that okay so there may be some efficiency gains there but the same is true when we did the linear model we just did a pooled two stagely squares without trying to to do something more efficient than um you know like GMM or something like that well in that case the the estimate of the average effect is actually more precise for the linear model than than in the non-linear model so it's um and in that case you can't you know really say that that is an efficiency gain in cases where you're choosing between you know doing linear IV or doing the the Heckman you know adding the the generalized residual I don't know I guess I I don't know enough about various applications where that's been done but I you know you do have to correct for the estimation of the generalized residual so that's that certainly is going to change the comparisons um I don't know I you know when you're doing instrumental variables it's hard to eke out any additional efficiency right no matter what you're doing yes you know always Pickers or never thinkers right I was wondering in the random coefficient yeah that's a that's a very good question certainly there should be because if you right the question that I I think the the last part of the question is to in the reduced form right to allow for some sort of random coefficients or at least some heterogeneity and how the instrument affects the endogenous explanatory variable certainly you could do that and then of course the problem with applying the the general methods is that you no longer have an additive an error that's additive and independent of the instruments so in a parametric framework you could certainly do that but in a non-parametric framework that relies on this Independence assumption you you couldn't really do that but you might have to in a parametric framework let's see how would that work I guess you probably would have to make some second moment assumptions in order to to identify all of the the terms that you need to add to the Control Function approach I think because there's there's always this problem right that you really can't distinguish um in a linear model distinguishing between heterogeneity in coefficients and heteroskedasticity is basically impossible right so if you if you maintain that some underlying thing is independent of your instruments then if you make enough assumptions you should be able to to disentangle that and use it in in the reduced form and getting the control functions yes that's a good question 