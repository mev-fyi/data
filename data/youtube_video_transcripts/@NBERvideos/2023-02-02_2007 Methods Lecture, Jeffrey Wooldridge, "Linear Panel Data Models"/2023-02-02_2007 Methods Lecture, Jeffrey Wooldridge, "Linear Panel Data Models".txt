well um okay so this is um perhaps out of all the topics the most basic one linear panel data models so some of this material is is certainly not what you would call new um hopefully it's packaged in what we now think of as the modern approach to panel data models so I'll talk a little bit about the basic model emphasizing some things that at least it seems that that people are not doing as a routine matter that are now easy to do talk about some properties that um some some standard estimators have that are more robust than perhaps what we thought um talk about what happens when the so-called strict exogeneity assumption fails and and how to to use IV methods and then a topic that I've tried to pull together is um on pseudo-panel datas from pooled Cross sections so the um the starting point is to make sure we have a common way of stating assumptions because it affects both how we interpret the model as well as the estimation methods that we use so for this particular talk I'll be focusing mainly on the large cross-section and small time series case although I'll talk about some approximations that hold for large T in the talk on cluster sampling I'll turn more formally to what happens when you perhaps don't have a large cross section and have maybe several time periods that are comparable in size to the cross-section Dimension so the the model is stated as an equation one I'm using this um notation that Gary Chamberlain I think pioneered by treating the unobserved heterogeneity CI as a random variable with a small T the Ada sub T's are treated as time aggregate time effects or different different uh intercepts to be estimated because we can do so with a large cross section and a small number of time periods and in fact I think studies that don't include full aggregate time effects should probably be viewed suspiciously because if it changes the results then you're going to think that well a policy was implemented during a certain particular a particular period where some aggregate effects were also happening um okay so they're they're of course covariates there's the unobserved heterogeneity they're the idiosyncratic errors use of I.T and um the probably the most attractive assumption to start with at least if we're ruling out simultaneous equation models is what has come to be known as the contemporaneous exogeneity Assumption and this is conditional on the heterogeneity so it's that the idiosyncratic errors are in the conditional mean sense uncorrelated with the x's and the heterogeneity now this defines this in effect defines what we're interested in which is the regression of function the regression function in equation three which is the conditional mean so this makes it clear that we're interested in the partial effect on the outcome with respect to the X's holding of course the other x's and the heterogeneity fixed so this is really the equation that gives us the useful interpretation of the betas and unfortunately it's well known that the betas are not identified under this this assumption so you can't quite think of three as a definition because I've written it as a conditional expectation but if we wrote it as say a linear projection then it would be definitional okay but but the betas are still not identified unless we make some additional assumptions um so of course if we were to add these relative well the strong assumption that the x's and the heterogeneity are uncorrelated then we're in business because we could identify beta just using a cross-sectional regression right we're just assuming away the problem by by saying well there's this heterogeneity term that we're thinking of as constant over time but it's not really correlated with the covariates anyway so we'll throw it in the error term and and we don't even need a panel data set to estimate beta in that case we could of course increase our sample size by pooling but um certainly we wouldn't need to so I'm going to act for the most part as if we don't want to make that strong assumption because of course the the power of panel data is that we can hopefully control for unobserved heterogeneity that is not changing over time okay at least if we have enough variation in our covariates over time so where do we go from there what what additional assumptions allow us to identify the betas well a common one and again this is terminology that was really introduced by Gary Chamberlain strict exogeneity conditional on CI which you can call a latent effect or heterogeneity and that's written in terms of the errors as equation four and of course then the you can see looking at equation five what the real restriction is on the on the observable response variable y it says that once you control for the unobserved heterogeneity and the X's at time T whatever those are those could be lags of other conditioning variables none of the other x's in the other time period matter now the real restriction here in most applications is that this implies that the x's in future time periods cannot be correlated with the errors say in this time period so if you if you take that back to equation four you can't have it that the error term at time T is correlated with the X's at time t plus 1. I think we understand this this shortcoming of that assumption fairly well although in applications um where you can actually estimate beta because you have time varying X's we owe it to ourselves and to everyone else I think to ask well why is it the X's are changing over time okay if they're changing over time in a completely exogenous way hence this this name strict exogeneity then this assumption is going to be reasonable so if there's some policy maker out there ignoring what's going on with this particular response variable or things that might affect it and is just setting the covariates or covariate if it's a policy variable essentially independently of everything else then this assumption is a sensible assumption but that's actually a fairly small fraction I think of the kind of applications that we use for for panel data applications often the X's themselves are Choice variables of whatever units we're talking about and you have to wonder whether in any Dynamic view of the world that it makes sense that somebody's choice of a variable at time t plus one is not going to be affected by the shocks that affected them in the previous time period okay now I say this again because it is clearly part of this talk is about what we do when this assumption is violated and so there's clear recognition out there but still it's fairly common in applications to see this assumption not even discussed okay that that um something well I'm going to talk about the so-called fixed effects or first differencing estimators where this assumption assumption is actually critical okay so um once we have that assumption um let me go back so I'm going to call the the fixed effects estimator is just a note at Fe the first difference thing is FD remember the fixed effects estimator uh the way that we're going to think about it for these talks is that we're not going to think of it as treating the heterogeneity in equation 3 as parameters to estimate at least not not yet we're going to think of the transformation as subtracting off time averages to eliminate the heterogeneity which then gives us a a set of equations that we can apply a variety of methods to um because if we take out time averages CI doesn't change over time and so when you take out the averages CI is gone okay but it's exactly because we've taken out these averages and now we're using the data that have been de-meaned if you will that this strict exogeneity assumption comes into play okay first differencing is is actually a little more direct in that you just take two usually adjacent time periods although they don't have to be adjacent subtract one from the other and that eliminates C and then you have an equation in changes but what you've done is you've introduced an equation where you have the change in the regressors and the change in the errors and under the strict exotic and strict exogeneity in particular requires that the error at time T minus 1 and the exit time T are uncorrelated okay and that is restrictive in terms of of economic behavior in terms of we're not talking about regularity conditions or anything like that we're talking about assumptions that impinge on Behavior so that is um the so-called feedback effect that that I'm uh I'm describing but before getting to that once you have decided how you're going to use one of those transformations to estimate the parameters if you're maintaining the strict exogeneity assumption then then ever then a bunch of things are available one is just to use the standard fixed effects or within estimator one is to use a first differencing estimator um if the strict exogeneity assumption is really true those things should give you fairly similar estimates right because they're different transformations to eliminate the unobserved heterogeneity and they should they both work under strict exogeneity of course when T is two there's no difference between the two methods there really is only one thing to do um so typically the choice between those two if if you want to do something simple like that hinges on what you think are the second order serial correlation properties in the errors so if we go back to this slide if we think that um going to go back to if we think the error is in equation one are serially uncorrelated then the fixed effects estimator is essentially the best you can do if you also impose homoscedasticity if you think there's still a lot of persistence in those errors then differencing removes that persistence and then that becomes a more efficient thing to do you can avoid those choices entirely by applying GLS type procedures in both cases and they actually give you the same answer they asymptotically it doesn't matter if you use a full GLS procedure on the first differences or a full GLS procedure on the the within transformation those um can both be asymptotically efficient and then of course there's always generalized method of moments estimation which will will come to a little later when talking specifically about relaxing this strict exogeneity assumption so one thing that has been recognized for a long time is that if the model contains a lag dependent variable then then the strict exogenated assumption is a non-starter it simply can't be true because of course if exit time T is Y at time T minus 1 then exit time t plus 1 is y and you can't assume that the error at time T is uncorrelated with Y because it's directly affecting y so most of the emphasis up until well probably about 10 or 15 years ago was on looking at models where explicitly there was a lag dependent variable and then how are we going to estimate the coefficient on that lag dependent variable in the presence of unobserved heterogeneity now with this broader recognition of the so-called feedback effect attention has turned to more general settings where the X's you know if you're estimating a production function and on the right hand side are inputs like capital and labor those that's not lagged output so there's no issue that well my my model contains a lag dependent variable on the right hand side but the issue is do do firms say change their inputs in the next time period based on shocks that happened at time t I mean we would think they would right at least until you can show otherwise or have other evidence that they they wouldn't do such things so an assumption that is certainly more appealing than strict exogeneity is um again there are there are other names used for this I'm using a term that was introduced by Gary Chamberlain a while back in the 90s called sequential exogeneity um conditional again on the unobserved heterogeneity this um this label I think is preferred because actually keto mentioned that there are all these Notions floating around exogeneity definitions that primarily come from the angle Hendry and Richard paper back in the early 1980s weak exogeneity strong exogeneity I couldn't even list them all actually some people call this weak exogeneity so if you're looking in the literature and you're trying to pull everything together this is sometimes called weak exogeneity but actually that definition from its original source also restricts parameter dependence on the process that's driving X and we don't really care what's happening to we're not trying to model the X process unless of course it happens to be a lag dependent variable but in the case where we're estimating something like a production function we don't really think of modeling the input process as a function of of lagged output and other variables while we're interested in mainly is coming up with ways to estimate the production function that would recognize this possibility of feedback effects okay so when um so weak Exogen 80 adds some stuff that that really I think just gets in the way and sequential exogenity basically says that the Innovations are well uncorrelated I guess is the the sloppy way to say it but um with with the X's dated at time T and earlier okay so if um again if we maintain the linear model then then equation seven makes it pretty clear what the Restriction actually is and again this is this is not as much of a restriction as it looks like because you just put other stuff in X until essentially it's true so if you believed if you had enough time periods and you thought linearity was a good approximation you could in effect make this assumption true whereas with strict exogeneity there's no way to make it true in the sense that it depends on some behavioral aspects that you can't really um you can't really control okay so in um we're going to talk about estimation under that that weaker assumption momentarily but um so let me go back for a minute to what distinguishes so-called fixed effects and first differencing methods from so-called random effects methods and written in terms of conditional means the key restriction is eight which means that the unobserved heterogeneity is uncorrelated with um with the covariates in all time periods this assumption really makes no sense in models with lag dependent variables or anything so you only start talking about this assumption if you start off with strictly exogenous X's to begin with okay and then of course under under this assumption that the typical random effects estimator is um is uh certainly has some nice properties if we maintain certain second moment assumptions that is something to emphasize um I wasn't going to spend much time talking about so-called random effects methods although they will creep up again in the crop up again in the cluster sampling lectures but this is the key assumption eight for random effects to be consistent there of course are other assumptions imposed that have to do with the serial correlation properties of the idiosyncratic errors their their homoscedasticity properties and I already wrote the assumption that basically implies that the idiosyncratic errors and the heterogeneity are uncorrelated but those assumptions only restrict the form of the variance covariance Matrix used in the GLS estimation okay we now know if that's if that's wrong the estimator is still going to be consistent but you have to adjust your inference appropriately so some people say well if you're doing random effects that's GLS so why would you ever make your inference robust well the answer is of course because your second moment Matrix might not be the right one and that that's a topic that comes up a lot in the so-called generalized estimating equations literature as well okay but it's uh it's it certainly should be familiar to us us too um now for the for linear model estimation in practice things like fixed effects and first differencing are used mostly but there are some other possibilities that turn out to be very important for nonlinear models uh models and those are so-called correlated random effects assumptions and in a way this is a middle ground between random effects and fixed effects in that we're not willing to assume that there's no relationship between the heterogeneity and the X's as an equation eight but for reasons that we'll see later on we can't actually leave the the relationship between the heterogeneity and the X's completely unrestricted so a middle ground is to say well let's specify some simple relationship between the heterogeneity and the covariates now in the in the linear case um Chamberlain introduced this equation nine where L denotes linear projection so this is not an assumption this is a definition it's just writing the linear projection of the heterogeneity on all of the x's in all time periods the reason he did this is because if any of those second moment assumptions fail that I was talking about so if any of those second moment assumptions that I mentioned are violated you can get more efficient estimators by using this linear projection procedure and then using so-called minimum distance estimation the munlock discussed a more restrictive version of this assumption in terms of equation 10 which is stated as a conditional mean so that's certainly a restriction and then simplifies things by saying basically the X's get all equal weights in this representation in equation nine this conserves on degrees of freedom and can actually be somewhat important for non-linear models one thing that's interesting is if you if you plug this representation in to equation 11 you get equation 11 and then if you apply say just pooled OLS on this recognizing that the new error term by construction is now uncorrelated with everything on on the the x i t and the x i bar then this actually equals the fixed effects estimator it's also true if you use Chamberlain's representation oddly enough this is actually fairly easy to show using um the so-called two-step projection theorem but the point is that taking this approach in the linear model doesn't lead anywhere differently from just the usual fixed effects estimator okay when you plug it in and then do a say you're going to analyze equation 11. so we will see this somewhat later on when uh when we talk about the um some non-linear models equation 11 leads to fairly simple so-called Houseman tests for comparing random effects and fixed effects and one thing that um probably everyone's aware of this but since stata when it does this calculation gets it wrong I'll assume that maybe there are a few people in the room that that aren't aware of this and that concerns what the proper degrees of freedom are in Computing the so-called Hausman tests which Compares random effects and fixed effects and this doesn't this isn't so obvious because it has to do with the aggregate time effects so in models where you have aggregate time effects there's nothing you can do to compare random effects and fixed effects for those particular coefficients okay the only coefficients that you can compare are on variables that change both across individual as well as across time so in equation 12 where I've separated things out into aggregate variables GI GT time constant variable zi and then the wits which are changing across both both indices you can only test the coefficients on Delta okay and if you apply packaged programs it will include the estimates on the aggregate time effects and this is why you sometimes get you know negative test statistics because there's this there's this degeneracy in the the test statistic that isn't readily apparent until you actually do fixed effects and random effects on just aggregate variables and see that they're they're identical okay it doesn't doesn't come out when you so clearly when you add extra controls so that's one comment the other comment is there still seems to be a little misunderstanding about when the Hausman principle actually applies and it's commonly stated that you should have an estimator that's consistent and efficient under the null hypothesis and then an estimator which is consistent but not efficient under the alternative well that certainly was the original motivation but these days we recognize that if you impose that efficiency condition on one of the estimators what you're doing is you're adding assumptions under the null hypothesis that you really can't test anyway not with that particular test statistic so the Hausman test statistic is has power against this the alternative that eight is not true it doesn't have any systematic power for testing whether the other random effects assumptions are true that is that there's no serial correlation in the uit or that those are homoscedastic the traditional statistic maintains those under the null but they're not testable based on the use of the Hausman statistic there are other ways to test those assumptions of course but rarely are they tested these days because we like to use things like fully robust standard errors right so in the case where you have a large cross-section and the small number of time periods we there's really little excuse to to not use the so-called cluster robust standard errors now because they allow for any kind of Serial correlation pattern in any kind of heteroskedasticity pattern yet you'll see in the same paper where people do that they then apply a form of the Houseman test which is not robust to violations of those assumptions so you can of course try to get the Hausman test the hard way which is figuring out what the variance Matrix is when one of the estimators is not efficient or you can turn to a regression form which makes it a trivial exercise these days and the regression form is basically in equation 11 except that you can't include everything that you might think you can only include the time averages of the variables again that change across I and across t so if you estimate this equation by pooled OLS it's very easy to make the the test is is of joint significance of the wi bars in this regression okay it's a very simple thing to do and um I suppose so far I'm emphasizing simple things to do um Okay so enough about that the um so as I mentioned there there are there are some issues still with um with degrees of freedom if you if you use things like stata which have these things pre-programmed in addition to being non-robust okay so um the next thing I'm going to turn to is looking at the properties of familiar estimators when we relax some of the assumptions under which they're usually derived and this is really quite simple stuff but perhaps seems to have gone unnoticed um at least at at some level of generality so we usually write down the unobserved effects models with just this additive heterogeneity but of course nothing and people have looked at certainly models where the slopes are also individual specific and as long as you have enough time periods there are ways to estimate those models allowing B sub I to be different for everyone but if x is has a large Dimension then you need a large time period number of time periods to be able to do that so one question is well suppose we act as if bi is actually beta there that there is no individual heterogeneity in the slopes so we just estimate equation 14. sweeping out C sub I and ignoring B sub I well I mean in in principle we do this all the time right because we don't know what our models are so our model might be best described in this in this way 14 even though we usually act as if it's um it's the traditional model so 15 is now just the way we would State these strict exogeneity assumption or at least one way to State it and so to apply fixed effects the the usual fixed effects to this problem we just need two time periods but of course we can have more um so the question is when when might doing this seemingly incorrect thing still estimate something interesting and that the interesting thing is um the so-called population average effect or if if we were in a policy evaluation environment this would be called the average treatment effect because in this model in the true model if x let's just take the case X is a scalar by the way the fact that I've left off aggregate time things here means that I've just absorbed them into the X's now but if x i t is just a scalar policy parameter then then if 14 is true then this is a case with heterogeneous treatment effects the treatment can differ by essentially arbitrarily by individual or more generally unit and so again with a small number of time periods we can't actually estimate each of the bis very well for each eye but we can still hope to get a good estimate of the average effect and so that's what this beta at the top of the slide is it's the population averaged effect okay well it turns out a sufficient condition there are there are some weaker conditions too but this is probably the easiest to to interpret is um is given by equation 16. and that is that the the average value of the slopes doesn't systematically depend in a conditional mean Sense on the time d mean covariates so in other words the x i t double dot these are the X's at time t for individual I but with the time average taken out so in in a sense we've removed the time constant component of the X's now we've done it in a fairly simple way but nevertheless this assumption allows for bi to be essentially correlated with um with the time averages of the X's so as just a simple example if we write this out explicitly where x i t has some permanent component F sub I and then it has some rits which may or may not be serially correlated it doesn't matter then this condition in 16 holds if if b i is essentially unpredictable given the idiosyncratic movements where it could be arbitrarily correlated with f sub I so this shows that by the way one of these interesting Curiosities about well about probability is it's not enough to assume zero correlation in 16 if you just go through the three line proof it um it's clear that zero correlation is not enough so it has to be a conditional mean restriction okay so this shows that the the standard fixed effects estimator has some robustness properties for this model and you can extend this to models where you want to take out more that is you don't want to just take out time averages of the covariates but maybe they're trending and those Trends are individual specific so you may have X's for unit I growing at different rates than they are for a different um unit I and so what you might do in that case is instead of just having one additive unobserved effect which I called CI you might add one more term which is a linear trend for each individual eye and then when you do that the transformation is no longer just to subtract off the time average but it's to essentially de-trend the data linearly detrim the data for each cross-sectional unit okay so you're taking out not just a level effect but essentially a growth effect this shows up in the the so-called Heckman and hot's random growth model where you add this additional piece essentially allowing in a policy in evaluation framework what you're doing is you're allowing the um um policy determination or assignment to depend not just on a level effect but on what might have been happening before the um before the policy is implemented and you could certainly come up with reasons for why that not that why that might be true uh you might you know assign certain areas as empowerment zones not just because they have high levels of unemployment but because those levels have been increasing for recent years as well and then this can can at least help to get at that so if you write that in equation 17 where again these W T's are deterministic functions of time the dimension is restricted of course by the number of time periods we have then the the result still carries over it's just that now since we're taking more out of x more of these permanent features of X we're taking more out it makes it I think even more likely that they're going to be uncorrelated with these individual specific slopes of course as you do that you reduce the variation in X so there's of course a trade-off between robustness and efficiency here if you take out Trends from the x's and you didn't have to then potentially you reduce the variation in them by quite a bit more than than just doing the usual within transformation um you can also do the same game and and ask about well what happens in models that have actually become fairly popular lately not so much maybe in micro econometrics although certainly I've seen people estimating them but also but in Factor models for asset prices and so on so that would be equation 18 where the Ada T's are the so-called time varying Factor loads and if you work this through you can show that um in this case a zero covariance assumption does work so even if you ignore the fact that those Adas are changing over time if your main interest is an estimating beta it turns out that ignoring that those factors are changing over time still we leaves you with an estimator that is you know not entirely robust certainly but it's it's 19 is not a crazy condition because you're taking out stuff that um either a mean or or perhaps even more okay now there's an extension of a sort of these results to instrumental variables problems and um if you now suspect some of the X's as not satisfying the strict exogeneity assumption so they could either be contemporaneously endogenous they could fail the strict exogeneity assumption but we have some strictly exogenous instruments for them then a method that has become quite popular is so-called fixed effects instrumental variables or fixed effects to stagely squares where not only do you take out the heterogeneity but you instrument for the time varying X's because you're worried that there may be some correlation leftover correlation with idiosyncratic things certainly Steve Levitt has used this a lot in his papers right where the idea is to get both take out some sort of time constant heterogeneity and then to use some sort of exogenous policy change as instruments so the question is in this case does this estimator have any efficiency robustness properties and it's kind of interesting the answer is yes you have to add an assumption but again it's an assumption that might help explain why allowing for heterogeneity and slopes sometimes doesn't seem to matter or maybe often doesn't seem to matter in these kinds of setups so clearly we would use 21 because that's just replacing the exogenous X's with the exogenous instruments and then the additional assumption is 22. which is an assumption on the conditional covariate covariances so the key thing about this assumption is that it um it doesn't actually restrict at all the correlation between the slopes and these D Trend that are deem indexes that's allowed to be completely unrestricted the Restriction is that that the conditional covariance does not depend on the the demeaned instruments now it's a little hard to figure out when that might be true without just assuming Independence but you can certainly write down mechanisms where the x's and the z's are both functions of um persistent factors and idiosyncratic shocks where 22 holds it certainly holds under joint normality always right and then there are many other distributions where it would hold the case where it's not a reasonable assumption is is when the endogenous elements of X are actually discrete so this is actually related to a result that I showed in a cross-sectional context about robustness of the standard IV estimator in the context of random coefficients but it doesn't apply to cases where there are heteroskedasticity and it doesn't apply to to discrete outcomes so one thing that this implies is that this is another good reason for always including aggregate time effects because the Matrix on the right hand side would generally change across time so this covariance term so but that can be cleaned up by putting in the aggregate time effects and then applying fixed effects instrumental variables consistently estimates the average effect okay even if those these bis are changing across individual so these are results in some ways that say well doing what we usually do is more robust than maybe we we understood and um of course as econometricians aren't so necessarily happy about these sorts of things because you're supposed to be coming up with new ways to to solve problems not old ways okay um so let's turn more explicitly to the problem of how do these estimators behave without the strict exogeneity assumption um and again this is this is something that has certainly been well known for the dynamic model for a long time there's a paper by nickel that that in fact establishes exactly what the probability limits of the so-called fixed effects estimator are in the case where you have a single lag dependent variable and you estimate it by fixed effects or the so-called within estimator and the bias is on the order of t to the minus 1. as it turns out it's a fairly straightforward proof to show that this is a general feature of the fixed effects estimator provided we maintain the so-called contemporaneous exogeneity assumption so this these representations don't work in the case where you at time T is correlated with the X's at time t these work in the case where a Time series regression would work that is where the x is at time T are uncorrelated with the errors at time t so one implication of 23 and 24. so what 24 means is that the bias let's call it the asymptotic bias which is of course a bit sloppy is no systematic function of time it's just there okay it's and you can't you can figure out what it is in simple cases and in general it would depend on the process driving the X's okay but what you can show is that if um if the error at time T especially is on is correlated with the exit time t plus one there's no averaging out that happens with the differencing or as with the within transformation there is an averaging out that helps you and that's why you get the the term that's of order t to the minus 1 in equation 23. so so the proof where the X's are so-called weekly dependent okay so now we have to learn you know a little bit about time series that means that there there are no unit routes that the correlation is dying out as you get farther and farther apart in time that's it's really just an application of the koshy Schwartz inequality interestingly you can also show and now you have to get further into the so-called unit root literature but you can also show that this same expression holds if the X's have unit roots okay as long as the errors do not okay so taken together because these X's these time series processes are often very well predicted by just the lag that that you really want to know that this result carries through for persistent covariates so 23 and 24 do provide some evidence some rationale for preferring fixed effects over first differencing if you don't think you have a better way to solve the problem other than just doing one of these two methods course there are two catches one is we don't really know what the what the other factors involved in that big o t to the minus one term are at least not in general so of course that could that those could be big enough to swamp the big o1 terms although with a with a large number of time periods that seems unlikely but the other catch is that we we can't have it both ways if we start appealing to large time periods as a waste as a way to choose or prefer fixed effects over first differencing now we have to now we've entered the realm of well what happens if we have say a spurious regression and that in fact there is no well-defined model here and that the errors really aren't errors on what in the way we usually think they actually are moving around quite persistently as well well then of course first differencing eliminates that problem right or at least it gives you something sensible to estimate it gives you a model and differences and now fixed effects um well no longer has this desirable property in 23. so so it's always difficult to know what you're going to do if you do fixed effects and first differencing and they give you very different estimates okay you can't just you shouldn't just choose the one you like best right well you can and then pretend you didn't do the second one but um the same conclusions actually hold for instrumental variables so because the um so with small T of course you're going to be worried that both of these estimators might have a bias okay and so what can you do about that well you can apply methods that we'll talk about in the next section but there are some simple things that can be done and this really comes back to this idea that we should ask if the X's are changing over time why are they changing or is it a completely exogenous thing or can we find a simple way to actually see whether they're changing in response to what's happened to the response variable so a simple test and this is just proposed in my MIT press book but I I wanted to put it down because the couple of times I've tried it it's it's rejected the strict exogeneity assumption and one wonders if this wouldn't often be true is to Simply say well we're really checking whether the lead value of the X's or at least some of the X's which I put in wit maybe there's just one One X that we're concerned about you just lead that one period as long as you have more than two time periods and you estimate the resulting equation by fixed effects or first differencing and the estimate of Delta should not be statistically different from zero and it it shouldn't be large in an economic sense either um if you try this with say union membership or marital status and a wage equation forget it um there's there's clear evidence that Union status at time t plus one uh in in the few data sets where you might actually have variation in Union status seem to clearly be reacting to shocks in um in earnings in the previous period so fixed effects estimates are first differencing estimates if they give you fairly different answers there there's a story why it's not a story for for which one you prefer except this this previous story I gave but it is a story for why the the economic differences might be fairly large um just on this slide let's see I added a comment on how it's also easy to test for contemporaneous endogeneity and I'll probably just skip over that it's again one thing to emphasize here is that people sometimes complain about regression based tests because they tend to make you focus on the value of the test statistic instead of whether you're getting important differences in the estimates but I would prefer a regression-based tests that doesn't add a bunch of assumptions under the null to a test based on differences in the estimates if one if if one is maintaining that one of the estimators is efficient under the null okay so so these regression based tests these days can be made quite quite robust completely robust to extra assumptions and so that's more or less why I'm I'm inserting these tests as we go along and basically showing or at least describing how they can be implemented in a very in a completely robust way I have to say I was disappointed to see that stata I think even State at 10 did not allow for a robust option in the in the fixed effects IV command I don't know if anybody else has noticed that but they they do for fixed effects but not for fixed effects IV so hoping that will will change at some point in the future um okay so let's go back to this um this problem where we're not willing to we're not willing to entertain any kind of asymptotic bias even though it might be a function of one over the number of time periods so this this comes back to the estimation under sequential uh exogeneity and the weakest form of the Assumption is stated two lines below equation 26 and that is just as a covariance assumption that the X's dated at the same time period um as the use and then previous time periods are uncorrelated this turns out to be enough technically to identify the parameters because it gives us some moment conditions we can use after we eliminate the unobserved effect so because of the sequential nature of these assumption this assumption the transformation that is convenient or at least the one I'll talk about here is differencing there are some forward filtering Transformations as well that Hayashi and Ariano and others have have worked with but I'll just focus on the on the uh the the simple differencing then we get a set of moment conditions as in 27 and then of course when we put the parameters back in we can apply generalized method of moments so under just the Assumption of sequential exogeneity what happens is we can use as instruments at time T the entire past history of the X's Okay so just having um just having contemporaneous exogeneity isn't enough but sequential exogenity is and in fact once we impose that we have in principle lots of lots of extra instruments that we can use and so if we set this up in a GMM problem what happens is the The Matrix of instruments is this diagonal matrix and what's happening is of course as you go further and further in time the number of instruments you get is growing because you have more lags to use and then allow GMM to essentially choose the most efficient combination of those instruments and in and you're done basically there are some other simpler strategies too which involve just um essentially differencing the equation as in 29 and then instead of doing a full GMM just estimate a reduced form for each time period separately this is a good way to get starting values to get an initial estimator for the GMM weighting Matrix too you need something to in order to do that and so the um if you get the fitted values from the change in the x's on the um on the instruments which I should have mentioned of course you have to lag them once once you've taken changes for them to be valid instruments so when you get those fitted values that that takes care of the problem that your instruments are growing with there are different numbers in each time periods because of course the fitted values are going to have the same Dimension as X and so you can now just use those as instruments in the pooled regression and that's it's it's an inefficient estimator but it's something you can do right away uh without having to go to full GMM um of course this if we only use the sequential exogeneity restriction this procedure often suffers from the so-called weak instrument problem which Guido will be talking about later and that's that's because if the X's are persistent and have so-called unit Roots then what we're doing is we're trying to use lagged values of the levels right which includes lag changes to predict the change in X from time period T minus 1 to T well one case where there's no predictive power is when X is actually a random walk because it really is just its last Value Plus a shock and by definition that shock is uncorrelated with everything prior and so this method um you can it GMM doesn't really help in this case it in fact just sort of serves to mask the problem if you look at this method where you explicitly do the reduced form regressions so in each time period you regress the changes on the X's up through the previous time period you can see whether in fact there is significant correlation or not and if there's not you shouldn't probably go on because um what it means is that you really don't have in usable instruments for this problem so so that certainly has been noticed in models with lag dependent variables when the the coefficient on the lag is close to one so that you have a unit root in the in the model itself or close to one and it's also been noticed in applications such as production function estimation where you have to difference something like the inputs log of Labor input or log of capital input and then predict the difference which is a growth rate based on lags of those things and often the lags of the things don't do a very good job so more recently people have been looking at other sorts of restrictions that one can impose and one restriction would be in in equation 30 which is much stronger than just saying the U's are uncorrelated with current and past X's you can write it in two different ways it says that user also uncorrelated with pass-wise or that user also uncorrelated with past values of itself okay which imposes a no serial correlation assumption in in the original model so one would hope that if you're going to add an extra assumption like that that you would get more out of it and you certainly get more by way of moment conditions now in turn in the case of the dynamic um the simple ar-1 model this has been looked at a lot and I think people disagree on whether that model by definition really should have serially uncorrelated errors but usually it's studied under that assumption in which case 30 adding the equation the conditions 30 to the usual sequential exogeneity assumption really are no different okay so there becomes more of an issue if you're estimating something like a production function where you don't really you don't have lagged output or anything like that you're basically estimating a static production function should you be adding additional assumptions that say that the productivity shocks are serially uncorrelated and I think many people would be uncomfortable making that sort of assumption at least as a general identifying scheme but if you make the Assumption then you get lots of extra moment conditions and and these have been been looked at by um by Anand Schmidt for example in the uh in the auto regressive case and they you can show that the moment conditions that it adds are given by equation 31. so interestingly what equation 31 does is it adds a set of conditions that involve the levels of the equation as opposed to just the first differences so before we first differenced and then used essentially lags lag levels as instruments 31 essentially considers the levels equation and then uses lag differences as instruments with one small computational issue and that is that the moment conditions are now non-linear in the parameters beta okay so this probably has been in turn empirically this has been applied a lot less than just the standard estimator which involves linear restrictions I think partly because this is a lot harder and partly computationally a lot is an exaggeration it's harder than just you know pulling up your canned program and doing it and it also does a impose that these extra assumptions when you might not think that the model has the Dynamics entirely complete so others have looked at different conditions Ariana and bover suggested just saying well let's assume that the change in the X's is uncorrelated with the heterogeneity now interestingly this is very much in the spirit of that assumption that I talked about when the usual fixed effects estimator has robustness properties in the presence of random slopes this just states that as a first difference condition instead of instead of time demeaning so the idea here is that if you think the X's are basically a random walk and the Innovations are uncorrelated with the heterogeneity but the initial condition might be correlated with heterogeneity then equation 32 is reasonable because it Nets out that the permanent component if you will from the X's if you add this assumption well then it's to the sequential exogeneity assumption then you now get a set of linear moment restrictions in the um uh in the parameters so you get 33 in addition to the moment conditions that we already had now this as it turns out works pretty well okay Blundell and co-authors have used this for the exactly the problem of estimating production functions and if you just use the initial sequential exogeneity restrictions you get very imprecise estimates which reflects this weak instrument problem on the other hand if you add these equations in 33 suddenly it ties things down much much more tightly of course you've added some assumptions and just as I mentioned when talking about the robustness of the fixed effects estimator that's hardly for free this assumption 32 is hardly for free okay but if you are stuck with um a case where you can't learn anything using the weakest set of assumptions you might be willing to to add something in the um in the case of the simple Dynamic model and bond actually showed that this condition that was generally used by Ariana and bond could be distilled into one extra assumption okay and the extra assumption I'll just go to oops I'll just go to the bottom line is basically given by equation 39 . so this this random variable ri0 if you assume that at time 0 the process is in its steady state plus some departure from that so the steady state is CI divided by 1 minus rho and then plus this this deviation from it ri0 bundle and bond basically showed that these extra moment conditions are the same as assuming that the deviation from the steady state is uncorrelated with the steady state so you know when you start imposing restrictions on steady states which are untestable you might wonder whether you should continue but as I said this there's no question that the additional moment restrictions that it adds which are linear in the parameters have been shown to help a lot both in simulations and in empirical work the more supportable conditions which impose uh in in this ar1 model which impose these non-linear restrictions have been used very little even though they they are less restrictive so in a sense this probably has won out because of simplicity at least in terms of the estimation and it's it's fairly transparent in the in the ar-1 case okay so of course even though a lot of the theoretical work has been on the very simple ar1 model the um in practice you might want to estimate models that have both a lag dependent variable and some other covariates and the an example of that would be in equation 40. now again if the zits are actually changing over time we might sensibly ask whether they are strictly exogenous but it's it's fair to say that most of the time when we look at equation 40 we would our I would be taken toward the lag dependent variable as the first thing that we should be worrying about because it clearly is not strictly exogenous in this equation but you know whether the other covariates are or not is an empirical issue the other one is not an empirical issue okay that it can't be strictly exogenous so the the methods that um I just talked about can be added can be applied to this case as well the easiest thing is to difference and then to start counting moment conditions and decide which ones you're going to use um if the z's are in fact assumed to be strictly exogenous then it gives you lots of extra potential moment conditions right because by definition of strict exogeneity they are uncorrelated with all errors in all time periods so in principle you could add just a ton of moment conditions in practice um I think in fact the Ariano and bond approach generally would just use the um the changes in the in the zits as instruments for themselves okay instead of trying to add on these um these extra orthogonality conditions there's an example that's covered in a bit more detail in the notes just to see what happens in a simple example they are data from on airline routes in the United States for a four-year period um 1997 to 2000 there's not that much information except there is a concentration ratio um for the routes there and so these estimates that are reported at the bottom of the slide are they're actually the estimates on the lag dependent variable even though one might be interested in the estimates on the concentration ratio as well that the dependent variable is the is the log of the airfare between the two the two points um with four time periods and a lag dependent variable you difference once and then you have two time periods with which to work with and you can see that clearly um doing OLS on the first differences when you have a lag dependent variable is shown it that this example pretty much shows that's a bad idea you get a negative estimate if you implement that IV procedure that I mentioned where you regress the change in the lag dependent variable on changes T minus 2 or earlier you get a more sensible estimate and if you implement the Ariano and bond GMM estimator which essentially uses the efficient weighting Matrix you get a higher estimate and a lower standard error so that's of course what you would hope from a procedure that is supposed to be more efficient you can look in the notes for what happens to the coefficient on the concentration ratio the route concentration ratio again in that case you do get a more precise estimate using the full GMM procedure um okay there there is uh something interesting that happens with the the large sample properties it wouldn't be applicable in the example I just talked about because it has to do with when the number of time periods is getting getting large with um and and uh Ariana and Alvarez show that the GMM estimator that accounts for the particular structure of Serial correlation that's induced by the first differencing does have some desirable properties as the um as the time number of time periods grows along with the cross-sectional sample size okay the last topic um I'm going to talk about is um analyzing so-called pseudo-panel data sets which are constructed from pooled Cross sections and I should say in in looking through the the recent literature it seems that there are really only a couple of estimators being used but they're they're analyzed from every possible angle you can imagine so as I find in these cases it's not never a bad idea to go back to the original source which uh in this case is a paper by Angus Deaton um in 1985 and um there I think I think he lays out the problem pretty clearly the idea is we don't observe the a panel data set on individuals for some well-defined population where the assumption is there's stability in this population as we as we March through time but we can collect pretty good data set on on Independent cross sections so at each time period a new random sample is taken and it would be a fluke if somebody showed up in the sample more than once and even if they did you wouldn't be able to use that information in any useful way so so what what is the setup um well the the idea is at least as stated by Deaton is can we identify the population parameters in this model at the individual level using data that have um are on individuals but not on the same individuals as we move through time it's a little surprising what comes out I think or at least I was surprised when I finally forced myself to go through this um so that the model is stated in 42 and for some I've used um I've used F instead of C as the so-called unobserved effect or fixed effect and I I haven't put I subscripts on 42 because I want us to think of this as a population representing a population of the same group of individuals as we move through time okay because the ETA T's are unrestricted time intercepts then we're just going to assume that the expected value of f is that is just zero and um and then we'll assume that all the X's have time variation because there's going to be a um well a a fixed effects type transformation is going to to show up now beta of course doesn't have perhaps too much of an interpretation in this model um generally although if we impose the contemporaneous exogeneity assumption down in equation 43 it certainly does because now this again takes us back to this first equation that I wrote down which is that ideally what we would do is estimate the partial effects of these x's on the average value of y holding this heterogeneity fixed interestingly in the pseudo-panel literature this assumption isn't used except for one thing and that is what it implies about the relationship between the idiosyncratic errors and the heterogeneity which is equation 44. Okay so the um so again this is this is pretty much this is just taking from Deaton's original setup the um the the way to think about this I think is that when you when you add on this heterogeneity it's some aggregate of everything that affects the response variable that you can't control for but is constant across time and so anything that's constant across time should have a zero conditional mean when you condition on the errors as well and so if you let G is going to denote group designation or cohort which is what the pseudo panels are based on putting individuals into cohorts often based on birth year or something like that then the way to view 44 is that it's going to imply that the mean of the individual specific effects is is also conditional on group membership is zero remember we're taking out aggregate time effects already in the in the underlying model so as I said the the implication of this is now if we have a random sample um at each time period Then the implication is given in in 45 that the idiosyncratic shocks are uncorrelated or mean Independence more precisely of the um of cohort or group designation so the idea is to divide the population up into cohorts again birth year of birth is a common one to use and then to see how those cohorts using the cohort data over time can be used to identify the parameters in the underlying model so if we use that then we get this equation 46 which is really this which is really uh Deaton's starting point that if you believe this underlying individual specific model then there's a relationship between the cohort means of the response variable the cohort means of the covariates and of course that has a Time component as well because we're view we're observing different samples in different time periods Moffitt also basically starts from this equation where in fact XT contains a lag dependent variable okay as it turns out the motivation of how to estimate beta is not really dependent on whether XT contains a lag dependent variable okay um so if we just Define now all of the population means which um so the unobserved effect has a mean for each group which is called Alpha sub G the then there are the means the cohort means for the Y's in each time period so those are mu y g t and the cohort means for the X's then this equation 46 can be written like a regression equation but it's in the population means as opposed to real data at least so far so the interesting thing about this equation is that it it looks like a panel data model right it is in a sense it just it's in terms of population means so if we have um you know 10 groups defined by cohorts and 10 time periods then equation 48 is basically for a 10 by 10 panel data set now I said that this talk was only going to focus on cases where we have a large cross-section and a short number of time periods at least for the most part usually that's violated by this kind of setup but also there's nothing about data here yet okay so the question is how do you turn this equation 48 into something that is estimable based on data well in fact what people did actually what Deaton did was then to plug in the sample averages for each of these and then treat it as a panel and then study the properties as G gets large by the way before I talk about that hmm there's something suspicious about this about this equation 48 it it's derived without assuming any exogeneity assumptions at all on the X's the only the only assumption it used was that the means the idiosyncratic means conditional on cohort are zero after you take out the aggregate time effects this should make us a little suspicious I think because we just I just went through what would happen with individual data if we have lag dependent variable or or other variables that are not exogenous and contemporaneous exogenicity wasn't even enough in those cases to consistently estimate the parameters but now we have an equation that looks like you can estimate these parameters basically without any assumption on the X's at all and you you see this in the recent work this is actually highlighted as um as a good thing and and it is a good thing but then the question is what's the source of the identification here right that's so so how can you get away with this when you can't get away with it on individual level data well the the source of the identification is that you can't have any cohort slash time period effects in the individual level model so if there are effects that depend on cohort and time interacted and they should have been in the original model to begin with then that assumption on the error term that I made is no longer going to be true okay so it's kind of an interesting trade-off between assuming exogeneity assumptions about your X's which you pretty much have to do in the OR at least in some instruments in the individual level case and here basically making exclusion restrictions on the individual level model and that actually shows up in some common estimation strategies which are based on for example equation 49 so when you look at this let me go back up again you look at this equation 48 and what does it look like it looks like it has aggregate time effects it has group effects Alpha G it has some covariates which are the MU x g sub T and we're trying to learn about beta well that looks like a standard linear panel data model and in fact the pot in the population of course if you do the usual time demeaning and subtracting and and taking out the aggregate effects you get well this isn't an estimator this is a representation of the population parameters so again 49 is still a population equation that is one way you can represent beta just in terms of the means of the covariates and the means of y's where we've taken out those other aggregate effects and group effects so in fact if you look at equation 49 you would say well how would you estimate beta well let's get the get the cohort or group time period means for each GT pair and plug them in and that becomes an estimate of beta it's a standard method of moments estimator where you have a population Vector in turn that you don't know in terms of some moments that you don't know but can estimate and Away you go and effect in fact in most of these applications you do have a fair number of of observations within each cell because you the idea is you hope to be able to get a big random sample from each time period you're putting each unit or person into cohorts but you know they're not really small cohorts so that you have two people and then a huge number of cohorts that can be done but often it's just a handful of cohorts and you have a lot of people per cohort so you can estimate those means fairly precisely okay this then becomes Ace Ace not quite standard but fairly standard problem in minimum distance estimation where you view the sampling error as coming through estimating these population means now the recent work on this has actually not taken this approach it have used it it views the sampling it it typically does large G asymptotics and then just kind of tax on an error term and then treats it as a fairly standard fixed effects model but if you if you take Deaton's starting point then it looks to me like this is clearly the intended way to go is it turns out that that estimator using using 49 as the basis for estimation you might not be surprised to learn as inefficient because it seems that something about the cohort the cell sizes as well as the variability within cells should should Place some role if you're going to efficiently estimate and in fact not surprisingly it does um so as I I already basically gave you this this discussion of why it is we're able to do this um and the the point is that we're excluding group time or cohort time effects from the structural model yet at the same time we have to have the covariates themselves have some variation within group time structure if the covariates if you can write them if you look at the second bullet on the slide I did have a laser pointer here but too late to use it now there if um you look at that representation if if that's the way you think the X's essentially evolve over time and group then clearly that formula that I wrote down is not going to be well defined and again this is this is all in the population this doesn't have anything to do with data yet so 49 doesn't exist because there's no variation in those um those variables with two dots over them if you um if everything if all the variation in the X's can be explained by two additive effects a Time effect and a group effect so that's the identifying restriction and of course if we had individual level data we could allow group time we could allow cohort time interactions and so the fact that we're not doing that is basically where we're getting the identification from okay so I mentioned that so so people have applied this so-called fixed effects estimate to the to this to these sample averages and that turns out to be an inefficient minimum distance estimator so the question is how can one get the efficient minimum distance estimator and actually if you look at this problem you might as well use this now um the moment restrictions that it imposes has this form where the thetas are the parameters that we want to estimate which actually are beta the time effects and the group effects we don't know what those are those are are to be estimated with the data the pies on the other hand are these are all the these are the all of the means within group cohort dependent variable independent variables so it's actually a fairly big vector and you can write the restrictions like this now this isn't the standard separable minimum distance problem where you have essentially the thetas being pulled out from this and equaling some function of the pies but of course you can work it through and when I saw this and wondered if somebody had worked it through I emailed Gary Chamberlain and with a within a half an hour he sent me some lecture notes where he had worked exactly this problem through with the efficient weighting Matrix and what the asymptotic variance is it turns out in this case it's fairly it's easy because for a given value of pi the restrictions are linear in the parameters of Interest so this does have a closed form solution it should because because I already showed you the fixed the so-called fixed effects estimator can be um or I asserted it could be could be derived in this form so when you work it through you get something which is not at all surprising that the efficient minimum distance estimator looks like a weightedly squares estimator but remember the asymptotics here is done not off of the dimension of the parameter um the group sizes or the number of time periods it's being done off the sizes of the of the cells so it you can't derive the properties of this estimator as if it's really a weightedly squares estimator you have to derive the properties as if it's a minimum distance estimator and the um the the way you get the efficient weights if you will is also not surprising is you want to give observations with relative you want to give cells with relatively more observations more weight and you want to give cells with relatively less variation in the um in the regression residuals more weight as well okay so to get the efficient estimator you do you run the regressions within um within each group cohort and you estimate the the residual variances that way so you actually have to go back to you the efficient decimator requires you to go back to the individual level data more than just Computing the averages on Y and X as I said if you want an inefficient estimator then you can use this um the so-called fixed effects one but of course it's variance Matrix is more complicated as well so um as I said so there's some actually pretty recent literature on this that and they all more or less come up with the same estimator although not not this efficient version of the of the minimum distance estimator but again the asymptotics is done in different ways so let me just spend the last couple of minutes discussing discussing the Deaton approach to this problem except with the only exception that the again the sampling is assumed to be large cell sizes with without trying to do the fiction that somehow the number of cohorts is getting large because as I said it if you believe the identifying restrictions then it it changes with very little modification if you have lag dependent variables the only difference now is that if as an equation 52 you have y on the right hand side you really don't have as many moments to estimate as you originally thought you did because one of the moments is is uh is from uh from a previous previous time period so it's very easy to adjust the the set of moment restrictions so that you don't get this degeneracy from basically estimating the same moments a couple of times the other thing that's interesting about this is that you can you can extend this to fairly common extensions when you have individual level data so if you wanted to add one of the if you wanted to look at the so-called random Trend model that as I said has been you been used more and more especially with more than a few time periods then all that does to the moment conditions is add another term which of course is a linear time Trend now you still have you have more parameters to estimate but remember you have these you have a huge number of hopefully of observations and moment conditions to do that and then um again I mentioned these models with uh with time varying Factor loads which would be in 55 and then you would get the moment conditions in 56. so again there are more parameters added and one might ask whether there's really sufficient variation in these moments across cohort and time period to be able to get precise estimates I think the answer is generally no but um it is the method that is implied by this framework as a comment I'm not really I I haven't seen a what I would call a completely honest simulation of these methods that has aggregate time effects allows for group effects and then generates the data in a way that doesn't rig the results so that it's going to work well so I think this is still waiting for a fairly honest evaluation um so the last thing that I'll leave you with I know I'm supposed to be answering questions and not asking them but um you know just to show you that maybe somebody who who thinks about these problems a little differently should should weigh in surely in many of these cases we're willing to assume at the individual level that some of the X's are exogenous in the tip in the very weak sense that they're uncorrelated with the idiosyncratic errors the methods that have been used up to now in this literature don't use those assumptions at all okay which is again so you're trading off these different identifying restrictions but it if you're willing to make some assumption that you have some set of Z's that are exogenous it seems like you should be able to use that that information and one way would be to use as an equation 57 where you're just basically saying that you know the the these this Vector of Z's is exogenous in the sense that they're uncorrelated with the idiosyncratic errors at time t if you look at that equation potentially there are a bunch of extra moment conditions there that should help to estimate beta but the question is what you do with the very last term as it turns out because that's the one you need restrictions on to do this so um I will stop there and I'm afraid right one minute over or something but if there are any questions I'm certainly happy to field a couple yes foreign yeah I I don't think there's a lot of evidence on that actually there clearly there is some trade-off there because yes if you if you do choose a large number of groups then it makes much more sense to treat it as if the resulting model is a panel data model and then what usually is done is the the means are that the sample means are viewed as having measurement error in them and then you try to come up with a way to solve that measurement error but the solution that's been offered in those cases basically is to use cohort time dummy variables as instrumental variables and that leads exactly back to that inefficient fixed effects estimate that I wrote down so I don't know generally how one does that clearly if you want to treat this in in a framework where there's a clear answer to the efficiency question is what's the most efficient way to use this structure then that's you know arguing for larger group cell sizes but of course the more you do that the the less variation you're going to have in those cells so I I unfortunately I think that's kind of an open question and and part of that could be answered if with a simulation study that again allows for all the features that are evident in empirical work 