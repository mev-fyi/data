Mark Watson: It turns out that when you want to do test for these things or lots of other situations that
we're going to see, they end up with statistics
that are not nice. They're nice, but at first
sight they're not nice. You met people like this. But then after a while, you get all that
guy's a nice guy. I thought he was a jerk,
but he's a nice guy. The statistics that you have to look at
here are like that. When you first see him go, God, I don't want to deal with that, and then it turns
out to be just okay. What we're going to do
is we're going to think about our first tool. Again, many of you know this, so this will be a review. But for those of you
that haven't seen this, I'll give a brief introduction to this functional
central limit theorem, which is an example of using empirical process
theory to solve a problem. Here's going to be the problem. Anyway, that's
what I'm going to do. Let's just go. What do we know? We're really good and
we're pretty good at this. I should be writing
bigger, sorry. This is Epsilon t as IID, mean 0, variance Sigma
Epsilon squared. Let me compute an
average of those guys, which I might know it as an
average based on a sample of size t, which is that. We're pretty good at
dealing with this. Law of large number. This guy is going to converge
in probability to zero, scale it by the square
root of t, so I get 1 over the square root of t times sum of the Epsilons. That's a normal zero
Sigma Epsilon squared, so we use a central
limit theorem. We've got, laws
of large numbers, central limit theorems, no muss, no fuss. Stat 101. Now as it turns out, not too surprisingly
when we're dealing with highly serially
correlated data, our data aren't going
to be IID like this. They're going to be more
complicated and in an extreme, and we're going to see
an expression like this that shows up
a lot of times. What we're going to have is like this will be dealing
with x's here. I've written it
as a random walk. X depends on lag, the x and Epsilon. If you will, xt is the
summation i goes from one to t, Epsilon i, assuming
x_0 is equal to zero, so I'll leave out that
initial condition. The question here is, g, x is a nice sum of the Epsilons, no muss, no fuss. We can handle that. But now what if you want
something like this? Well, you want to study the average of
these guys, of the x's. That's a bit more
complicated because these guys are highly
serially correlated. The law of large numbers central limit theorem isn't going
to apply here immediately. We need something else. That's what this FCLT
thing is all about. You're dealing with highly
serially correlated data, sums of things that are nice, sums of things that are nice. We're going to end up looking at functions of these as well, not just this simple
function. What do you do? What we're going to do is
we're going to just extend this CLT in a pretty
straightforward way so that we can use it over here. That's all that stuff is about. On this I must say,
this gives you some more intuition. Here we go. Stat 101 review is this. I've got a sequence of
random variables like this. This is a sequence because I
could imagine computing it for capital T is equal to one
If my sample size was one, my sample size was two, I can imagine computing it
for sample size of two, sample size of three, etc. I can think about a
sequence of these guys, and then I can ask, can I approximate the
probability distribution of some element in the sequence like Epsilon for a
sample size of 150? Can I think about
approximating that using another maybe simple
probability distribution? What do we do? Well, we say, Epsilon t converges
in distribution Epsilon. By that we mean the probability density of this converges to the
probability density of this, as t gets big. That is, if t is pretty big, we feel pretty good approximating the probability
distribution of the object we're interested in by some simple probability
distribution. The CLT didn't assume
these guys were normal. That's one thing we
know. That's Stat 101. An example of that is the central limit theorem,
convergence distribution. I could have written down a
law of large numbers too, but it turns out we're not
going to need that I think. The other thing we're going to need is we're going to need, since we're not always
going to be looking at simple functions, sometimes we're going
to square things, or do something else to them. We're interested in functions. If g is a continuous function, this guy converges to this guy, then g of this guy
converges to g of this guy. That is the probability
density of this. Distribution of this can be approximated by the probability
distribution of this. But the standard
example is, well, if you look at the
usual t-statistic from a regression or something, as the sample size gets large, that converges to
a standard normal. Take that t-statistic
squared and it converges a probability distribution to a standard normal squared
or a Chi-squared 1. That's just what this is.
What do we want to do? We want to extend this so
that it works over here. What we're going to do
is we're going to extend these guys to random functions. I got to tell you what random functions I'm going
to talk about. That's what I have to
do. What's nice here? Central limit theorem. The nice limiting random
variable is a standard normal. What we're going to do is
we're going to think about a standard normal but a function. Let's think about
a Wiener process. This Wiener process, it's
going to be a function. Here I'll just draw it. I won't draw,  I'll
try and draw it. Let's think about
it on just for fun. Let's think about it,
this Wiener process on. We're going to go from
time 0 to time 1. This is like over
the course of a day. This probability distribution is going to look like this. It's going to start
here and then it's going to be a Gaussian random walk and continuous time. It's just a Gaussian
random walk. It's just going to do
whatever random walk does. It just looks like that. It's going to have
Gaussian increments. It's just going to wander, whatever it is, the
drunk guy or something. It starts at zero, and it's a random walk, so it's increments are
uncorrelated with one another. It's Gaussian, so uncorrelated
means independent. The increments are going to be independent of one another. It's standardized so that its variance at time 1 is unity. It's like a standard normal. If you will, this
Wiener process, I'll denote it by W at time s, where s is between zero and one. We're only looking at
this guy over one day. At the end of the day, the displacement from the original
point which was zero, this guy is a standard normal. These increments
are all going to be normally distributed
with standard deviation, which is given by the distance in time
between these guys. This is the standard
normal thing. This is a random function
because if I ran it again, I'd get another sample path. If I ran it again, I get
another sample path. Thinking about this as
a function of time, it's a random function. It looks different
from day to day, and the way it looks is random. I'm going to draw
another random function. Here's another random function. I'm going to take this 0, 1, let me break it
into five points. Here's the function
I'm going to do. This is what I'm going to do. I'm going to go to my
random number generator, and I'm going to
draw an Epsilon, a standard normal, and I'm going to
call that standard normal the first
standard normal I draw. Then I'm going to take that guy and I'm going to divide it by the square root of five and I'm going to
plot it right here. That's that. Then we go to my random number generator
again and I'm going to draw another Epsilon
independent of this guy. I'm going to draw Epsilon 2, and then I'm going to plot this. Epsilon 1 plus Epsilon 2 divided by the square root of 5. That's going to be here. This increment is just
the Epsilon 2 bit. Then I'll do that for Epsilon 3, Epsilon 4, Epsilon 5, so I get five points. That's not quite a random function yet because I haven't told you what I'm doing in-between. Let
me do this in-between. Many of you have children, had small children or
have small children, and if you give them this like a restaurant and
you give them a crayon, what do they do? This should be at
zero. They go, boom, boom, boom, boom, boom. That's a random function. I've told you what the
value of this guy is at all points between
zero and one. We could figure out what
its probability law is because I told you
how I drew these. The way I cook this as I
drew these Epsilons from a normal random
number generator. Let's look at this
last point here. This last point is
at 55. What is that? That last point is 1 over
the square root of 5, Epsilon 1 plus Epsilon 2 plus Epsilon 3 plus Epsilon
4 plus Epsilon 5, so that guy is a normal
with mean 0, variance 1. At this point, the
probability density of this function is the same as the probability
at that function. At this point, this
is a normal 4/5. At this point, the probability
density here is the same as the probability
density here. Similar to this point. As a matter of fact,
the joint distribution of these points, at the black points, is the same as the joint
probability density over here at those points. It's just the in-between
stuff where I cheated here, and I didn't cheat here. What might you
think about doing? Let's suppose I did this. Suppose I did this again, but instead of looking
at five points, I looked at 10. Then I do better, if instead of I looked at 10, I looked at a hundred
or a thousand, or a million, or a gazillion, so a gazillion, enter
the big number, if I look at a gazillion, really, basically I got this. That's what we're going to do. This guy evaluated
a gazillion points, connect the dots is a function with the same characteristics
as this wiener process. I guess I need to be
a little bit careful, so we'll try and be a
little bit careful. I can approximate functions
that are going too wild, if this was discontinuous, I got to make sure this one
doesn't get too weird as I change the grid points. The other thing I
guess I want to do is, as I thought about doing this, I drew these guys
from normals so that the probability density here is exactly the same as the probability density here. But at the end of the
day I'm averaging here, I'm like some of these guys, so I could draw these
guys not from normals. I could draw them from
other densities and the averaging would give me a normal probability law even if the underlying
Epsilons weren't normal. I could use the central
limit theorem there. I got the functional stuff going on that's connecting the dots, and I got the central
limit theorem thing saying that's the averaging. That's all we're going
to do. We're going to do connect the dots, and then we're going to need one other thing because it turns out connect the dots isn't
quite what we need. We have to use a slightly
different approximation, but I'll get to
that in a minute. Let's go through the
connect the dots stuff. That must be with this shares. This is blah, blah, blah,
blah, blah, blah, blah. That's what this is. Connect the dots.
That's all that says. Here's two books, classic book near
very good book. This is Andrew's handbook
of econometrics article on empirical process
theory, which is great. These guys live on 0.1
and they're continuous, so you could call them that. They're continuous on 0.1. I want to say guys
are getting close. If you think seeing
guys are getting close, you have to define what
do you mean by close. Maybe that could
be a definition of close here on this
space of functions. Now, I'm going to show
you three theorems. Here's just three key
theorems that we need. Here's theorem 1. Theorem 1 says, if I want to show the probability
density of this guy, behaves like the probability
density of this guy, this is what I have to do. I have to do a couple of things. Number 1, I have to show that
if I look at these points, the joint probability
distribution of these points converges to the joint
probability distribution of this function at
the same points. I have to do this
for these points. Choose a set of
points, 1 over 5, 2 over 5, 3 over 5, and show that that works. For any set of arbitrary points, you got to be able to show that, so you get the
joint distribution of points. That's
straightforward. That's a straightforward
application of convergence in
distribution for a vector. Then what do you have to show? Well, you have to show
that this isn't perverse, because this guy is continuous, but of course now I'm
letting t get big. As I let t gets big, this could stay continuous, but it could get wild in places, so I want be able to
control the smoothness of this thing uniformly along the sequence as I
like t get big. You say, don't go crazy, man. You say you can't get crazy. That's what this says. It says, don't go crazy, it says stay tight, man. That's the jargon. You
say this is tight. That's nice, isn't it? You see that's tight.
That's what this is. It says it doesn't go crazy. This is continuous, but if
you will thinking about continuity with
Epsilons and Deltas, you need those to be the same, you need to be able to
keep the same Epsilons and Deltas as you move
along the sequence. You need that to
work uniformly in t. That's what this sense. If you've got that, if you've got this guy converging with this this guy at
these points and yet this guy being tight, being cool, then
everything's okay. Then the probability
distribution of this is converging to the
probability distribution of this in the sense that if I'm interested in making
probability statements about what's going on here, I could use this probability
distribution over here. Same thing as convergence in distribution as we normally do. Step 2 is the continuous mapping
theorem continues to work. If I'm interested in
functions of this, so mappings from the space of continuous functions on
01 to the real line, and I've got convergence
of these probability laws, I get convergence here. So continuous mapping
theorem works. Then I've got the
central limit theorem. The central limit theorem says, theorem 1 was about general
convergence of functions, theorem 2 is the
central limit theorem. If I start with some
IID Epsilons here, I wrote it as Martin Gil,
different sequences, and I do this connect
the dots stuff, this exactly this connect the dots staff, then
everything's okay. That's what this is.
These theorems are, you can see these in
Hall and Heyde and you can see these in Davidson, you can see these elsewhere. This has these Epsilons being martingale
difference sequence is sometimes we'll want
more persistence that more dependence, and Davidson does that nicely
in one of his chapters. There's one little subtlety. Let's hit "Okay", so here let's go through an example. Here we go. Let's go to this example and I'll show you where this
subtlety comes in. Let's do this. Start
with some nice guys, write some nice Epsilons. Then I look at them. I look at that. What is this? This is the height of one of these dots where I used
capital T grid points. If I put five here, would have been this plot. Let's suppose that what we want to
do is we'll enter, and what do we know about this? We know these grid points, if you will, that this
guy is converging nicely. Fine, I'm just going
to write this. Suppose I'm interested in this. Let's suppose you're sitting at dinner with your child, and you give them these dots. I'm going to draw
these dots again. You give your child those dots. I'm going to put
the dots and again, 1 over 5, 2 over 5, 3 over 5, 4 over 5, 5 over 5. I've tried to put them
in the same place, dot, dot, dot, dot. You give those to your child, and your child does something
weird. Some kids do this. Maybe you have two kids, and one kid connects the dots, and the other kid
says, you know what, I'm going to approximate this Wiener process in
a different way. She says, you know what? Let me do this, I'm going to use
like a step function because that's cooler. Instead of connecting the dots, this other child uses
this step function. For thinking about this problem, this step function is actually
a bit more convenient. Why? Because what is this? What's 1 over t? 1 over t is like 1 over 5. It's these widths.
What are these? These are all these heights. This is really just
the area under this, it's just the integral. We deal with objects,
and we're going to deal with objects like
this all the time. It would be nice if, what went on with this connect
the dots approximation, also worked for these
step functions. It turns out that it does. Going back here, these results
can be extended to this. This is just notation for this step function
approximation. At any point between
grid points, I use the last grid point. That's all this says. This extension is really useful for the things
we're going to do. Let's go ahead to our examples. Here you go. Excuse me, I've changed notation slightly because I'm just
making this up as I go along. But now x is a sum
of these Epsilons, there I divide it by
the square root of t. But you'll see
that comes back in. These Epsilons are like IID or martingale
different sequences. Here's my step function
approximation. It's the red line here. It's the step function
approximation to that wiener process. Then what I wrote
down over there, I only had a T. Somehow, I think if I look at something, you guys will all look at
it to even but so I should. It's funny actually, I don't
know why that happens, but see I have square root of t, that's t to the 3/2. That's what I have
here in these notes, and blah, blah, blah. Then this just says, look, all I did was integrate
under this thing. Integrals of
continuous function, apply the continuous
mapping theorem, so that the probability
distribution of this thing which
looks like a mess, is the same, or can be approximated by the probability
distribution of this. If I know the probability
distribution of this, I know quantiles of
it or something. I can use that to
approximate that. That's what this
trick is going to be. We'll use this trick,
a zillion times 20. This slide work through
this on your own. This just says, if these x's
are formed as partial sums, not of Epsilons, but of a's, where a's are nice moving
averages of the epsilons. This is important, so I'm
going to go through this. Now x depends on a,
depends on Epsilon. X is going to be
some of these a's. A is just Epsilon, i and Epsilon i minus 1. Rearranged that some. That just says any of these Epsilons is
being added in twice. It's being added in here, and then it's being
added in with a weight of minus Theta. That shows up there, and then there's
little endpoint stuff, to get rid I did
double counting twice. But this stuff is just tiny. Because it's got divided by
t. That little thing is it. If I think about the probability
distribution of this guy, it's this some of the
Epsilons times 1 minus Theta. When I work out the
limit of this guy, it's going to be this wiener process times Sigma Epsilon. But now I have to put this
constant in front of it. This constant, 1 minus Theta
times Sigma Epsilon square, that, that's the
long-run variance of a. This long-run variance
is going to keep showing up whenever we're
interested in sums of things. That's the important
part of this slide. This long-run variance
then keeps showing up. This just says this generalizes. This long-run variance thing is going to be the scale
that we're going to need, not Sigma Epsilon anymore, but this long-run variance. That's another reason
why this thing is key. What does all this mean? You guys can read this. You run a regression and
you want to look up, you've got a t-statistic
and you want to see whether it's
significant or not, you compare it to 1.96. Why do you compare it to 1.96? Well, because someone
at some point, figured out the probability
distribution for a standard normal and the
critical values are 1.96, and you said my
t-statistic is basically an average of student
dies the average so I can use 1.96 as an approximation. This says the same
thing that you can use critical values computed
from standard normals. In the t-statistic, if you wanted to look at
the critical value for the t-statistic squared,
what would you do? Well, you'd look up the
Chi-square 1 table. Let's suppose you
had some statistic that was the t-statistic and a regression and you needed the critical value for the t-statistic raised
to the 13th power. How would you find the
critical value for that? You could go back to really old statistics books where they did that or you could generate a
bunch of standard normals, raise them to the 13th power, and compute a histogram, and that would give you
the critical value, do that for a zillion
draws, and you'd have it. That's what people
tend to do here, you can generate
approximations to this wiener process
doing what your kids would have done with a
whole bunch of grid points, and then integrate them or
square them or do whatever, and then via Monte
Carlo figure out what sensible critical values
are for the statistic that you're interested in so
that's what this is. Can we now take a breath?
Was that hard work? I'm glad I'm getting you guys in the morning because it's going to be hard around
4:30 this afternoon, I'm going to be by the pool so you guys are
going to be in week mini instrument land looking at concentration parameters
and it's going to be interesting as it can
be I can tell you. Actually, I'm going
to be preparing my talk for Wednesday, but that's another matter, I would rather be here
listening to Jim. We're going to shift gears
now and I'm going to start talking about a set of models that we're going to
use for the next two-and-a-half lecture. Here's what we're going
to do, I'm going to think about this in the
context of regression model, but this thing can be re-interpreted lots of different
ways and we'll see it. Here's the regression model, y is equal to x Beta
plus error term, now I'm calling the
error term Epsilon and the only change is now I'm going to put a
time subscript on Beta. We're going to think about
a couple of problems, we're going to think
about Question 1 is their time variation
so how do you do test? Question 2 is going to be, suppose you've got
time variation, how do you estimate Beta as
it evolves through time? That's going to be the
estimation question, then we're going to think about the forecasting question, if you've got a model like
this and you're forecasting, you need to know
Beta in the future to think about doing forecasts so how do you do forecast? We're going to think about
all three of those problems. I'm going to spend almost all of my time talking
about this model, so x is equal to one and it's a scalar because generally, if you understand
what's going on here, you can do the regression
model or the IV model, or the non-linear
GMM model, etc. I mean, the trick
in all of those things is to linearize, so if we understand this, we're about 95 percent
of the way there. Let's see if we can understand some of the issues
in this model. This model in the
jargon science, this is called a local
level model so you see that in the literature. Beta is like the
local mean of y. If we're thinking
about doing any of these things, testing, estimation, I have to tell
you how Beta evolves, so how does Beta evolve? There's one way for Beta to be constant there's lots of
ways for Beta to change. There's lots of ways we could
all invent our own model. Here I'm going to just go
through some popular ones, so Beta can break, it was, and then it is. It breaks at time tau and here some of the interesting
issues are going to be, can you date the break? Did it break and can
you date the break? Think about volatility,
declines in the macro economy was there
a decline in volatility? When did it happen? We'll talk about
great moderation, if you will, stuff in the
context of this model. You can put into breaks
and if you can put into, you can put in 83, anyway, I was going to say probably
wouldn't make any sense, but in many contexts it
won't make any sense, I'll come back to them. You could do mark off switching so you can
have these guys, when you're in Regime 0, Beta t is equal to Beta, when you're in Regime 1, Beta t is equal to Beta plus Delta and follow
a Markov process. You can put in multiple regimes, you can put in other
stochastic breaks, I'm going to talk about
these in the context of forecasting because
they're interesting. You could allow
continuous evolution so I'll talk about this model a lot so this is a model in which you've
got Martin Gil variation. Your Beta today is
the same as your Beta yesterday plus an increment, which might be IID so that's
what this Beta thing is, you could sum this up and make
this a ream or something. These have names going
back to these guys, I have 1995 here, but I think I have a
version of this book in 1976 or something and of course, this is the one
model that gives you the usual forecast function, it looks like this,
which has some name, I forgot what it is discounted this thing
you guys know this. There's different models here, I want to talk about
what's important and what isn't important. This model looks continuous, it changes every period, where's the discrete
models look well discrete, but heck, I didn't tell you what the
probability distribution of this Eta was. If this Eta is zero, lots of summer time periods and then non-zero occasionally, write a model like
this will generate long periods of time in which Beta is constant
and then it jumps. The continuous, discrete thing, isn't something that
you should think about and there's
this great paper by Graham Elliott
and Eric Mueller that makes this clear and I'll talk about
that a little bit, there's some long
memory stuff to. What is important
particularly for forecasting, is whether you think
when these breaks occur there's going to be mean reversion like in a
Markov switching model, you've got a two statements,
sometimes you're here, sometimes you're
here but once you go from here to here,
where can you go? You can only go back to the
first one, back to the set. In those models you've got
mean reversion in, of course, if you're thinking
about forecasting, mean reversion is your friend, because you know in the future you can't be too
far away from that whereas you've got these
drifting guys is important low-frequency
action then that's important. Deterministic verses stochastic, this is also going
to be important for forecasting because in thinking about what's going to
happen out-of-sample. Case of this, so there's a couple of
things I want to say that were useful at least when I started
thinking about this problem, which is, these are just
example calculations but what does it
mean if you've got time-varying parameters in
ARMA model or something, ARMA model so I
thought about this in the context of
vector auto regression. Here's the thing. Suppose you start with a vector
auto regression that's got 15 variables in it and those variables
are covariant stationary. Maybe not 15, maybe
1,500, 15,000, they're covariant stationary and then choose a subset of them, like one, and look at it. Well, it's going to
have a nice little ARMA representation
which you could work out pencil-and-paper and its coefficients are
going to be stable. If I look at individual
series and I see that there's some instability
in the individual series, then I must learn that, that's part of a bigger model, but it must be there's some instability going
on in that bigger model. It's interesting to test for time variation even using
a small number of series. What's less obvious or important is that
if you think about, suppose you have two variables, we just think of as
two, two variables. They follow a VAR. VAR coefficient is stable, doesn't change, but
the variance changes. Sigma Epsilon changes from Sigma Epsilon 1 to
Sigma Epsilon 2. If I look at those
two variables, if I look at a VAR including
those two variables, and I ask, did the AR
coefficient change? I'll find out it didn't
because it didn't. If I look at one of the series, one of the series is going
to have a mixture of these Epsilons in it
and as it turns out, its univariate marginal
auto regressive parameters, if you will, will
change because they're going to be a function of the relative variances as well. If you'll look at a small
number of series and you see that there is some instability
in the VAR coefficients, does that tell you that there's really an underlying change in mean parameters or in
variance parameters? Well, you can't tell. Interpreting time variation
is a little subtle. You can see this, for example, undoubtedly discussed in one of these [inaudible] papers,
which is really nice. Here's some quick evidences
from an old paper by Jim Stock in
which he looked at 5700 VARs for usual series. Take 300 macro
series from the US and look at every
possible bivariate VAR, you get a whole bunch of them, so that's what he did and ask, is that stable or unstable? Well, the answer is, over the post-war period apparently there's lots of instabilities. This is important enough. What does is that
cause? Was that caused by the
coefficients really changing in some macro model
or because of volatility? Well, he didn't say. I want to talk
about that testing. I'm going to talk
about testing for time variation and let me talk about it in the simplest model. This model is going
to be hard enough, as it turns out, or
interesting enough. Simple model. There's a break at time Tau. I want to know whether there
was stable or unstable, so I want to know whether
Delta is equal to zero. No muss, no fuss. What do you do? You
do a Chow test, so that's what
we're going to do. If you're doing a Chow test, you have to know
when break date was. Suppose you know when
the break date was. Break date was in 1984. Things are always in 1984, 1983, quarter four, 1984 quarter two made little
uncertainty, but not much. You know the break gate,
so what do you do? You do a Chow test. What's the Chow test here? You compute the mean
in the second period. That's an estimate
of Beta plus Delta. You estimate the mean in the first period, that's
an estimate of Beta. You subtract them,
you get Delta hat, and then you do a t-test. Here I've written
it as a Wald test, so the t-statistic squared, so that would be
a Chi-squared 1, under the null of no break. If where was a break, this guy would tend to be big and you would
reject the null and assume and conclude that
there was some instability. Well, the challenge here of course is this says
you need to know the break date and
if you don't know the break date, what do you do? The suggestion really goes back, at least in 1960 and I've
never been able to figure out. I should read these
papers. I never actually read these papers, but don't tell anyone. Quandt says, or this is
what I'm going to say. Quandt says. Now you guys know that I've never
read the paper, who knows, Quandt may have said, Watson's an idiot in a footnote. But he wouldn't
have known me then. But anyway, what Quandt says is, what if I computed
the Chow statistic for a whole bunch of dates, reasonable dates, and then I
looked at the biggest one, the one that was most extreme? That might be called, let's call it that Quandt test, and it just says compute the Chow statistic
over a whole bunch of break dates and look
at the biggest one. What I've never been
able to figure out is Quandt's papers 1960 and
I think Gregory's paper, Gregor Charles'
papers 1961 or two, so it seemed like
this part should come before this part, but maybe not. Anyway. Now the challenge is we know if you looked
at one break date, this guy would be a
standard normal squared, under the null it would
be a Chi-squared 1, and since you're
looking at the maximum, a whole bunch of this, the
challenge is to figure out what critical value should you use for the Quandt statistic. You don't reject
in 1984 so you do the test in 1985, do
you reject there? If you will, you've
got to think of it as data mining problem,
the max problem. We want to figure out the probability distribution
of this and for that, we're going to use
this stuff over here. It turns out to be pretty easy, so this is just algebra. There's no way around it. Let's suppose we're
figuring out, we're trying to get
critical values. What are critical values? Critical values are quantiles of the probability
distribution under the null. Under the null, Delta is equal to zero. Then if I look at
y-bar 1 minus y-bar 2, the Mus cancel and
that's the same as looking at the differences
in the Epsilons. I've written this in terms of
the equals under H naught, so I put Epsilons here.
What do you have here? This first line is just what the Chow statistic looks like under the null with slightly
different notation. Here's the average
in the first period. Here's the average in
the second period, number of observations in the first period and
the second period. This I've divided
by Sigma squared, so this guy has a limiting
Chi-squared distribution. All I'm doing here is
just rewriting this, blah, blah, blah,
blah, blah, blah. Each of these guys, look at each of these guys, they're starting to
look like these guys. I'm just going to
call these guys, all algebraic
expressions are guys, because it wouldn't be
polite to say these gals. I can think about this guy as an approximate Wiener process, so that's this notation. Similarly here, and
what's going to happen as the sample
size gets large? This thing now viewed as a
function of this break date, this S is like the brake date, is converging to
this Wiener process as a function of the break date. This function now as a function of the break date is converging to this function. Looking at the maximum of this, as Quandt is doing, is like looking at
the maximum of this. If we can figure out what
the maximum of this guy is, we've figured out what the critical values are to use for the Quandt test, so boom. That's that and
someone figured out these critical values.
Who figured them out? This guy, Don Andrews did this. This calculation is
Don's calculation. His was more elegant. People often look at all possible break dates like in the middle 70 percent
of the sample, you need enough pre
sample periods and post-break periods that
you're doing averaging, because all of this is central limit theorem stuff so you need normality work
in all of these, so you need these
samples to be big enough so this is
a standard thing. The critical values here, this is a Wald test
with one degree of freedom or like 12, 8, and 6, think of this 8.68 or if you were doing
like the just the Chow test, it would be 1.96 squared, like four that would be
the five percent critical. It jumps up to about 8.5, so that's the data
mining problem. These critical values for different numbers of regressors, degrees of freedom can be found in any good
undergraduate textbook. Don't go and look at Andrew's 93 because he wrote down the wrong numbers.
This is a funny story. When Jim and I wrote this book, we wanted to put this test in because undergraduates
should know this. We compute it as critical values and they were different than what he had. I think we wanted
to put them in like F statistic forms
so we divided by k. He had them in Wald
statistic in Chi-squared form. When you divided them by k, we just had different numbers. We went back to Dawn and
said, what's up basically? What mistaken is Watson
making when he's doing his programming which
is usually the case. As it turned out in this case, it was Dawn had could dig out of his files the output from when he did this because he did this by Monte Carlo. He copied down the wrong column. We felt pretty good. Anyway, that's it. This is icing on the cake. Icing on the cake is Gregory
Chow says do a Chow test. Declan says, do a soup test or Quandt Likelihood Ratio sometimes is called
the QLR tests. Quandt likelihood ratio test. No one uses that for 33 years because no one can figure out the
critical values. Dawn figures out the critical
values using this stuff. Connect the dots stuff
and life is good. Until Dawn says but is
this an optimal test? This is only something that econometrician would care about. We should all care about it. We're econometrician
you want to squeeze as much out of the
data as you can. Is there a more
powerful procedure this quad thing is ad
hoc? It makes sense. Dawn writes a couple of papers, the most famous and
beautiful paper is this paper with
Werner Ploberger, which of course he
doesn't just study this problem at least studies an entire
class of problems. The problem that he's
faced with is this, he's interested in
testing for a break. In this data testing whether
Deltas equal to zero. Where you have known break date. But under the null if
Delta is equal to zero, the break date doesn't matter
because it didn't break, it shifted from here to here. This break date Tau thing, does it affect anything if
the null hypothesis is true? In the jargon in that Tau thing, that parameter is
important under the alternative and identified
under the alternative, but not under the null. It turns out this is a
problem if you think about basic Neyman-Pearson
likelihood ratio testing that we all learned in Stat 101, how to construct hypothesis, you need a simple null and a simple alternative and you
form the likelihood ratio. You've got a parameter
that's important here, that's not important here that isn't like the
parameter of interest. You've got this Tau
thing floating around. The question is, in general, how do you handle this? This is what Dawn and Verner do. It's just beautiful. What they do is this. Here's what they do. I got to do this because
it's just beautiful. What they do is one way
to interpret what they do is pretty simple. Is what they do is say, Gee, I don't know Tau, let me view Tau as a random variable with some
probability distribution. When the breakdown
occurred, it's random. Here's my distribution for it. I think it looks like this. Under the null, that
distribution doesn't matter but under the
alternative it does. What's the probability
distribution under the null? That's just what it was. The probably distribution
or the alternative is now a mixture. It's the probability
distribution of a null for a fixed break date times the probability
that was the break date. See you get this mixture out. The probability
distribution under the alternative becomes this. Then it turns out with a
little bells and whistles, you can construct the
Neyman-Pearson task, which you know is going to
be the most powerful test. All you've done is if
you will change what the alternative is by thinking about a
random, a break in. Then they have some
interesting interpretations of that this test turns out to be. Anyway, you can just read this. This now is a standard way to solve the break
tests problem. It's also will see
it in Markov's switching models and in other problems where you've got unidentified
parameters under the null, this Andrews-Ploberger scheme. To implement this
you've got to say, what's the right
probability distribution of this break date under
the alternative? This guy said, why don't
we say we don't know. Here what is I don't know mean, it means maybe it might
mean uniform. I don't know. They said put a
uniform weight on that and then it turns out
that the best test, dependent on how big your
thought the break would be. Is it a small break
or big break? It turns out those give
you different tests. The small break test turns out to be it's really interesting. The small break tests
turns out to be this. What you want to do is construct the Chow test at
all of the dates. Quandt said, choose
the biggest one. The most powerful thing to
do if you think the break is small is don't choose the
biggest one, average them. Turns out who would
have guessed? If there is a break,
you think it's big, then it turns out what
you don't want to do is you don't want to
average the Chow tests. You want to take the Chow
statistics and you'll want to exponentiate them and
then average those. That's sometimes called
the exponential Wald test because that's what you do. This looks like a normal. E it's got a 1/2  in it and
stuff so it shouldn't be too weird, but now here
this is interesting. If you've got a whole
bunch of Chow test, here's a whole bunch
of Chow tests. You construct these Chow tests at a whole bunch of
different dates. Sometimes they're here,
sometimes they're here, sometimes they're here,
sometimes they're here. What would Quandt's say? Quandt would say,
choose the biggest one. That's my statistic right
here. That's the biggest one. What would this say? It
would say take all of these guys and exponentiate
them and average them. When you take these guys and exponentiate them,
what do you get? You get a bunch of little guys here and then one
really big guy, it looks like that
because you've exponentiated this
really big guy. As it turns out,
when you compute the average of the exponentials, that's dominated
by this big guy. As it turns out, in practice, this optimal test turns out to reject when the
ratio test rejects. In practice, just looking
at the biggest one of these guys is just fine. It's approximately this. That's really nice because Quandt is a nice guy. Some of you know him and
it's nice to use his test. Anyway, it's nice. I'm going to do a
couple more things. I'm going to do a
couple of more things so that you can see some
of these other tests so when you see them written down, you
know what they are. If there's a discrete break, what do you do if there's
a discrete break? Suppose you are interested
not in discrete breaks, but you were interested
in Martingale variation, so you thought maybe the
coefficient evolved smoothly. Today is like yesterday, this is my life for many of us. Today is like yesterday, sometimes something happens but today's a lot like yesterday. You want to see whether
this is constant. What does that mean
if it's constant? It means, well
that term is zero, so let me just do this. Let me put a little
coefficient Gamma here, let me normalize this so that Epsilon and e have
the same variance, just a normalization, and multiply e by Gamma so that this variance
can be different than this, and then I can write
the null hypothesis of constant-coefficient as
Gamma is equal to zero. Gamma is equal to zero, beta t is exactly equal to beta t minus 1, no time variation. How do you think about
in this simple model and how do you think
about an optimal test? Well, it's straightforward. If these guys are
Gaussian, for example, you write down the
likelihood under the null, you write down the likelihood
under the alternative. Neyman-Pearson tells you
the best thing to do is to form the ratio of those, we're in the densities, you're throwing the data that you have, that's the likelihood and that's it you can't
do any better. All you have to do
is to write down the density under the null and density
under the alternative. Under the null, in this simple model,
it's pretty easy. Why is IID mean Mu here I made it zero
just for simplicity, variant Sigma squared I under the alternative Y has a variance component
from this as Sigma squared I component, but also a random walk component in its covariance matrix. That random walk part of the covariance matrix
it's called A, so A's got ones and stuff in it. This is just random walk
covariance matrix thing. Then it's this I guess. Then what's the
likelihood ratio? It's just going to be that. You're going to reject for
that if that guy is big , so that's straightforward. The problem, I guess is this. In the olden days, meaning I don't know when, but in the olden days, if T was big, this is a T by T matrix, so I guess in the olden days, it was hard to do that
inverse, I don't know. Guys tried to figure ways
not to do the inverse, and so here's a famous way
not to do the inverse. This isn't too much
of an olden days because we had Gauss then. But anyway, he's living
in the past thing. What you have to do
is do this inverse it's that, and then
you could say, well, that's right, and then you could say, well, here's an approximation. This approximation is right
if Gamma is really small. I think I got that right. This guy is really small, so we'll use this approximation then the test turns
out to look like this. There's actually no Gamma A in here and that's
also a good thing. Then it turns out you
muck around and you do some algebra
and you can write the test statistic like this because its A has
this really simple form. This as it turns out, this divided by T is
called Nyblom statistic. I got four minutes left
and that's right and then you can work out its
probability distribution. This is cool, again, notice what we have under the null these y's
are sums of Epsilons. What do I have here? I have a sum of the sums
of these y's squared, the sums of the y's are
the sums of epsilons, so this is a mess. But on the other hand, I know that this guy is like one of these approximate wiener
process guys and I know this divided by T
is just integrating, so figuring out a limiting
representation for this, something that I can use to simulate critical
values is simple, it turns out just to be this
if you do the calculation. This integral right
here is just that sum, and the squared thing
is just this guy right here and I've
deviated from means, I guess, and there's
a typo here. It's something like this, there should at least be an
S here or something else. I'm going to skip
past that right away. If you've got regressors, you can handle regressors
and that's what this says, and there's a great
paper by Bruce Hansen , which does this. This is pretty
good algebra here, I hate to skip it because
it's pretty good, but it doesn't look good, but this is very
thoughtful algebra. You can do hack corrections, I'm going to come back to this. This is what I want to end with because this is important. There's this great paper by Graham Elliott
and Ulrich Mueller, apparently in the review of economic studies where I copied this chart from, Page 928. This is what they do. The paper's about comparing
models with discrete breaks, best test for discrete
break situations to best test for
Martingale variation, discrete versus
continuous if you will. The insight is that g
this continuous stuff. Again, if you have
error terms which are zeros and then
occasionally jump, that's going to give
you discrete behavior. If I work at a optimal test
for Martingale variation, which works well regardless of the distribution of the
change in the Betas, that should be approximately
an optimal test for discrete breaks as well, because those are
the same thing. That's what this paper
does. In the end, it shows that these guys are
in some sense equivalent. Here's a Monte Carlo
which is just wonderful and here's power for a
whole bunch of tests. The key thing here is you notice all of these curves, there's a whole
bunch of tests here, all of these curves
look the same. The way they generated
the data here is they put one break in
the model and then they computed the best test for one break and then they did a whole bunch of other tests, including the best test for
Martingale time variation. What did they conclude? Well, the best test
for one break and the best test for
Martingale variation have basically the same power. If you had a single break and
you did one of these tests for stochastic time variation
and you rejected the null, you shouldn't conclude
that there is smooth variation because that test has a lot of power against a single break, it's
essentially optimal. Similarly, here's a picture from an old paper from Stock and Watson and this
does the reverse. It says, here we generated
data in which there was Martingale smooth variation and we looked at the
power of the contest, which just looks
for a single break, and compare that to the power of optimal tests for
Martingale variation. What do you see? Well again, all these power curves
basically look the same. This is just another way of saying when you reject the null, what do you conclude? What's never a good idea is to conclude that because the null isn't consistent with the data, the alternative must be. Here, just because you reject
stability using a test for one time variation
or instability doesn't mean that that
instability is right, it just means things
aren't constant. See you then have to
think of it more deeply, how do I want to model
this, and blah, blah, blah. My daughter is an
elementary school teacher, and she learned how to teach. She went to school and
they taught, it's amazing. People learn how to teach, and this is what she told me, she said dad, what
you're supposed to do is when you go into class, you're supposed to
tell the students where you're going to go. I did that, I had an outline then you're
occasionally supposed to remind them where you are, put up the outline with
things bolded or something. I didn't do that. Then you're supposed to allow
them to practice. I should have given you
some problems to practice, which will make you drive
it into your brain, and then I have to tell
you where we've been. That's what this slide is, but we didn't have
time for this, good pedagogy, we
just need to move on. That's it. I think
we get to have lunch now and then we get to
hear Jim, so thank you. 