i'm going to start with briefly talk about what is super computing and then talk quite a bit about an overview of the exceed resources that are available with keeping in mind what is a relevant for computational economics okay and then i'll talk briefly about how to access and use exceed resources there is actually a lot of detail in the slides but i'll probably skip over some of them but i'll make the slides available through now and of course there's much more details and help available if you actually want to get onto one of these exceed resources okay so what is a supercomputer you might have seen pictures of that some of you have actually used it and are experts on them but that's maybe how that's how one looks like so it is essentially a bunch of commodity computing components uh plus some enhancements so um uh it is um so you can consider a supercomputer to be made up of a large number of so-called nodes each node you can think of as just a laptop or a workstation in fact if you just bought a brand new workstation in your lab yesterday it might be actually faster than a node of let's say the bridges supercomputer at psc which is two years which was obtained two years before but there's many and many of them in the supercomputer and then what's new what you probably wouldn't have is that these large number of nodes are connected by a very fast interconnect for very efficient data movement between these different nodes um so in your home the kind of data connection you might have is ethernet this is way faster than ethernet so that's one of the new things that a supercomputer has that you probably don't have and then the other thing is a very fast parallel large file system for um reading and writing lots and lots of data so those are the two new additional things that a supercomputer often has that you may not have and talking then about a few different flavors of super computing uh the one that has existed for a while is high performance super compute high-performance computing or hpc which typically means you're running a single application across many many many processors in a very tightly coupled way the other one is high throughput computing where you're kind of running smaller applications but running many of them at once one instance and that is very common maybe in economics as well as engineering which is my background is just doing parameter sweeps so uh so high throughput computing uh requires a little less data movement because each of these applications is smaller and but high performance computing is where where you have a single application running across many nodes you have to do a lot of work to make the data movement efficient so that uh the processing elements are fed data um at a high rate and that might require a lot of work then fairly recently what has become very uh important is also data intensive computing where basically the ratio of data to computing has is much larger than what used to be before and that requires uh more focus on data movement and data storage uh particularly input output operations per second okay so since this workshop is about data intensive computing and big data i'm going to focus a little bit more on that so again this is a fairly new field of super computing it was brought on by a data deluge from large numbers of sensors that have been deployed recently as well as a large-scale genomics gene sequencing and uh large telescopes in astronomy and things like that um now some of the in some situations there may have been a lot of data but people didn't you think of using them because it was just impractical and as you have said but now you can do that more and more the strategies here again is actually going from hard disk spinning hard disk drives to solid state drives where the input output operations are much faster as well as file systems in memory so in ram you can you can create file systems in ram um and and that uh increases the input output operations per second quite a bit and then there are software solutions for doing data analysis in very large large scale distributed fashion for example the map reduce algorithm that came out of google of which hadoop is an open implementation and then there is another implementation called spark which is uh different from it's not it's just very efficient data analytics and hadoop and spark for example often work together to really uh speed up data analytics one of the examples here is in information retrieval so that's watson when watson won the geoparty in in 2012 that was a big leap in in machines efficiently retrieving information from a large database of information a subset of big data is machine learning which has become really really uh important currently and so machine learning is not just doing simple data analytics but actually using data to improve its performance towards some task such as image recognition for example or prediction of market prices based on historical information time series information so the emphasis again uh now comes back here to flops because in machine learning you have to train a model to do the job properly uh using data and you do that so you do the training with the data so there's still lots and lots of data involved but the training itself requires now lots more computations so there's still a lot of data but computations uh uh again become important compared to data and for these kinds of computations actually accelerators such as gpus and fpgas do a very good job because a lot of them are sort of matrix matrix operations and you can really optimize them well on gpus for example but then then you still have to deal with a lot of data for the training and so solid state drives and memory drives also help okay as to algorithms for machine learning so there's there's two categories in my mind one is traditional machine learning where for example the algorithms could be regression all kinds of regression linear logistics lesser rage as mao has mentioned then there's other things such as support vector machines decision trees and random forests and they can look into non-linear interactions in data and things like that as to software a very popular software is scikit-learn where a lot of these algorithms are implemented already and that's python based then there's also apache apache spark ml lib and that can actually work with python or java or scala programs there's matlab has the statistics and machine learning toolbox i think a lot of people here use matlab so that's another good place to look for algorithms already implemented there is a java java implementation called weka 3 there are several others and then so these are the traditional machine learning tools and algorithms they work usually well with low dimension low size data i mean relatively low dimension low size data and then if you have very high dimensional data such as images where each pixel may be a dimension right or uh and so and uh and very large number of them they're currently deep learning or new uh techniques involving neural networks have have have really uh have really started doing very very well there uh so it was around the middle of this last decade where deep learning uh methods came into being and then they really did they really the performance of these methods just took a big jump so neural networks have been around for a while but they were not doing that well compared to some of these other methods and then deep learning came in deep learning essentially is many many layers of neural networks not a few but several layers uh in a neural network and they started doing really well towards the middle of this decade some uh very popular neural network configurations or convolutional neural networks or cnns recurrent neural networks generative adversarial neural uh networks and so on uh there are many uh open source frameworks uh for uh youtube player around with neural networks one of the most popular is tensorflow from google then there is a framework called keras which is actually a higher level framework that can sit on top of tensorflow then there's other frameworks such as cafe and cafe 2 apache mxnet l band from lawrence livermore and a few others so on exceed resources a lot of these are already available and several different versions this is actually a very fast moving field so i mean new versions come out pretty often every few months and we try it's hard to keep up to date with that but we try our best okay so those were the different flavors of super computing that are current and now let me talk a little bit about different programming paradigms for different parallel programming paradigms so the first is distributed memory computing and this has been there for a long time this is really i think how parallel programming started uh and basically it's uh you have many processors and each processor has its own memory and it works on data in its memory but uh it but processors may also need data that resides in other processors memory and if that if if data is needed from another processor's memory then uh that has to be explicitly requested and received from the other processor and basically the user has to put in explicit function calls in their code to actually do this data exchange the way to do this is mpi and you can do it very very efficiently if you spend enough time but it is hard if you have a serial code and you want to parallelize with mpi it might not be very easy in the beginning so ah after a while another parallel programming paradigm came along which is shared memory parallel programming also called symmetric multiprocessing where every processor can now access all parts of the memory so if if i am a processor i can just and i want to compute on data here in memory or here i can just grab that data and do some computation on it um for example your laptop is a single a small shared memory machine because all the cores in your laptop can access all of the ram okay the way to do this usually now is openmp and basically in openmp if you have a serial code you can just put in uh so-called directives openmp directives which are essentially just comments uh and those comments serve as hints to the compiler to parallelize certain parts of the code for example a loop or uh right and and so you can just take your serial code and stick in these comments and then tell the compiler okay this is an openmp code which has this openmp directives or comments and use those to create a parallel application and the compiler will then do it for you one important thing is that when you insert these comments if you if you then compile in a serial fashion it will still work because then the compiler just treats those comments as i mean just ignores comments right so so you cannot probably you still won't break your cdl code if you stick in openmp directory so that makes things a lot easier and a compiler then is doing a lot of the parallelization work for you then there's of course hybrids which are combinations of distributed and shared memory programming where you do shared memory within a node a node as i said is a laptop which you can think of as a small shared memory machine and then as you go across nodes you can do distributed memory again a quick pros and cons of these two methods so you can actually because you have a lot of control over how you are parallelizing exchanging data with a lot of work you can scale up to very high processor counts 10 000 to 100 000 cores and even beyond and mpi has been used for a long time but basically the downside is that if you're new to parallel computing and you want to get your code to use a large parallel systems it is initially very hard to get there there is a steep learning curve there whereas in shared memory programming you can actually get your code running parallel in parallel pretty quickly by inserting these comments in your code and the compiler will then do the rest of the work for you but uh the amount of parallelism that you can get is limited so around 100 100 cores is the limit beyond that if you if you go from 100 to 200 course maybe you will not see a benefit from your shared memory programming and then that's where hybrid comes in where you can initially begin you know converting to a shared memory code and then as you want to scale beyond one node uh which is usually less than 100 course then you can at that point you can invest more time into uh distributed memory programming okay then uh at the turn of the century i think uh another new kid came to town which is uh accelerators uh so the most popular accelerator now is uh graphics gpus graphics uh processing unit they have been there for a long time for video processing but around uh 2000 or so they became also ubiquitous for general purpose scientific computing and uh so they work well really well for some certain kinds of problems one of them is large-scale data computing especially things like machine learning they were initially a bit hard to program for example using nvidia's cuda but now there are software frameworks to make programming gpus much easier for example open sec which is essentially like openmp where you stick in again directives which are comments that the compiler can use to move parts of your code onto a gpu then there's something called feed programmable gate areas or fpgas which have a lot of potential for amazing performance at low power these are especially important for on the field computing such as on robots for example so you might for example train a machine learning algorithm using gpus on some supercomputer but then once the model is trained you can deploy it on fvgs to do inferencing in the field and because it's low power that so if it is employed on mobile platforms like robots that's really advantageous so that was a quick overview of just super computing in general i would say as a few take away messages is that a lot of people in computational finance use things like matlab or r large shared memory systems that we have in uh in in exceed for example share up to 12 terabytes of shared memory so and 352 cores at psc uh so matlab and r are very good candidates for for these kinds of platforms and then there's a lot of gpu resources on exceed and machine learning algorithms are a very good fit for that there's also other things such as molecular dynamics etc that can make good use of gpus but from a data perspective machine learning if you're going to do machine learning you should go for uh the gpu resources available on exceed okay so now a little bit more detail into exceed john towns already gave a very nice short overview it stands for extreme science and engineering discovery environment so it's an integrated set of leading edge computational networking data software and most importantly support services to facilitate science and by science i mean not just the physical sciences but social sciences including economics finance things like that people around the world actually use this most of the user community is us-based but there's international users also and they are using exceed resources to improve our planet and the human condition funded by the national science foundation and most importantly it is free for all open research for big allocations you do have to spend some time writing a proposal to get access to the resources uh which is only fair but no dollars involved okay it's already paid for by tax dollars here's a set of uh so so exceed consists of many partners some of which are resource providers i think those are the ones that are important to you they are the ones that host big machines and and then exceed users can request time on those machines the lead organization is the national center for super computing applications at illinois john towns is the pi of the project he is at ncsa and then i circle some of the big resource providers currently on on exceed but then there's many other partners providing various other things such as management or training uh et cetera uh who can access exceed resources so pis of exceed grants have to be researchers or educators at u.s academic nonprofit institutions that includes postdocs but not regular graduate students however if you're a graduate student who is an nsf fellow you can also be a pi if you are not then you have to ask your advisor to apply uh for uh exceed grant if you want to use the hpc resources in exceed or you can ask a postdoc in your lab to do that right additional users are the pi's collaborators which could be foreign outside of u.s or their students and then there's need to be added to the pi's exceed grant so students graduate students cannot or undergrads cannot directly be pis but their advisors if they are pis they can just add them to the grant and we i'll talk about that briefly in a few slides later what resources are available so there are compute resources there are the special purpose resources such as gpus there's a lot of networking and storage resources there are resources called science gateways which are essentially web portals uh that allow people in a community to very easily do some uh computing by submitting some input and getting results and the web portals the science gateways hide all the complexities that go behind actually running a code on a large resource parallel resource you know getting the data in getting the data out things like that so within compute resources there are massively parallel resources of the orders of hundreds of thousands of cores uh and then there are large shared memory resources for example up to 12 terabytes at psc's uh bridges resource there's high throughput computing resources such as the open science grid and cloud computing resources at indiana ps there's several gpu resources available for example sdsc psc uh stanford uh and each of these large computing resources also go with a lot of these parallel file systems or storage resources now on some of these i think on tax range at texas you can actually just request data resources to just store your data at other places you have to get both computing and and storage and then last but not but very important is there are a lot of human resources so if you are about to start using exceed and you so you get a grant but then you are not sure how to get your code to run on these exceed resources um you can request something called extended collaborative support surface where somebody like me or bob uh or some other person who has a lot of experience uh on these machines ah can will work with you very closely to get your code running efficiently on one of these resources and bob will talk more about this in his talk okay here's a quick snapshot of the different machines that are there these are the compute resources storage resources and high throughput computing which is osg and cloud resources which is jet stream at illinois you can actually go to that link up there to see that for yourself and i believe you do not actually have to have an exceed account to actually see this okay and just a little bit more detail about the different data resources since this is a big data workshop so as john said petabytes is i mean uh uh so is now possible malta there where there are multi-petabyte resources available uh so that's kind of you know you should kind of start thinking about that maybe that kind of opens up the thought process to what might be possible what kind of data sets you might now be able to analyze okay so it's uh so if you have these large big data resources it's important to get data in and out of it and there are several tools that are available on exceed to do that efficiently and easily there are command line tools such as scp gsi scp and rsync but then there's also this web best tool called globus online and that basically uses very fast grid xtp grid ftp amongst different uh uh machines so you can both move data between exceed resources using these tools as well as move data in and out from your system to the exceed resources what software is available there's a lot of software available on the different supercomputers a lot of them are open source but there are also some commercial softwares available for commercial softwares the different supercomputing centers sometimes provide the licenses but at other instances you might have to bring your own license but but if you have any questions or if there's some software that you want that may not currently be on one of these machines you can just talk to us and we install new software all the time the link there if you go there it will take you to the comprehensive software search page where you can actually type in the software you need and you can see which resources in exceed have that software so if you're thinking of applying for a new grant to exceed and you've never used exceed before and you're wondering okay do i have the software somewhere that's where you would go to take a look but again if it is not there please talk to us and we install software all the time some of the software that is important for computational finance or computational economics such as matlab r python those are probably more popular in this community and they are actually i think pretty ubiquitous i think most of the resources or all the resources have these uh including uh you know package packages under python for example such as scipy numpy a whole bunch of machine learning packages they're now available on all the different resources if you're a little bit more adventurous you might get get up to cc plus fortran together with mpi and openmp as you've mentioned those are of course those have always been there of course now there are some new things such as vms and containers so that will allow you to for example just maybe take a snapshot of your laptop or workstation and put it onto a supercomputer so it's so you do not have to leave the familiar confines of your laptop environment you can just take it to the supercomputer and use it okay so here's a look at uh the webpage for software uh so for comprehensive software search so you can basically type in a software and as i said matlab is available on almost all of the resources over there i think r would be similar and then i think you can click on them and you can see what versions are available for each of them so that's a nice tool so quick comment so that's about the different resources that are available quick comments so if you're going to use if you're going to run on exceed resources you want to run in parallel which you may or may not already be doing locally so and since the two common uh software tools that this uh community may be using on matlab and parallel matlab and r i just uh have a slide on each for the different ways you can do that in matlab and r so you can do job level parallelism you can have a serial matlab code and then you can have a script run each uh learn many instances of that one on each core okay using some script and that's job level parallelism you don't have to really touch your matlab code at all there in matlab you have inherently multi-threaded functions so a lot of the matrix operations are just inherently multi-threaded if you just launch it on for example your mult four core laptop it will by default try to run those functions on all of the cores of your laptop uh a way to control this is there's something called maxnum com that's in matlab where you can control this behavior the default is to just use all the cores that matlab sees that can give you quite a bit of parallelism without you really doing anything just run it on a multi-core system and that will happen okay then if you want to be a little bit more adventurous and then you can maybe think of either the parallel computing toolbox or mdcs matlab distributed computing server and within each of these there are several ways you can parallelize your code the first one is called par 4 which is just parallelizing a construct to parallelize big loops so instead of 4 so if you have a big loop and you in your original code it was 4 something you just change that 4 to power 4. and now if there are more cores available then that loop will be parallelized across multiple cores without you doing anything else there are certain restrictions for using par 4 for example you know the order of execution of the different iterations must be independent and things like that but it's pretty easy in general to just use par 4 to parallelize your matlab code then there is something called spmd which is more mpi like single program multiple data that's really kind of matlab's implementation of mpi where you kind of control all the data movement and in matlab you can also do gpu based computing with constructs such as gpu arrays you can do all the same things in something called mdcs the difference between parallel computing toolbox and the distributed computing server is that pct has to stay within one node however we have some very large nodes in exceed for example there are nodes with 352 cores and 12 terabytes and fram on bridges they're called the extreme shared memory nodes and you can run matlab on that node across all the i mean pct on all the 352 cores so you can do a lot with pct and then there's but if you want to go across nodes then you have to use ndcs sdsc for example has mdcs and then you can just basically the same code but now you can use multiple nodes and run your code across multiple nodes okay using r in parallel so again the ideas are pretty similar to matlab there's you know inherent threaded code that you can use for you can for example you can just compile your r code the r installation with nmkl library with threading turned on and that will just parallelize uh those functions that use mkl that make calls to mkl functions mkl is math kernel library from intel there's pn math which uses basically openmp versions of standard math libraries which are already parallelized with openmp and so if you're making calls to those functions from those libraries those will be parallelized then you can do distributed uh parallel computing yourself in r including both multi-core and distributed and mpi like parallelization with rmpi and there is also this for each construct which is uh similar to the par 4 construct for parallelizing loops okay so i want to then give some examples so one great example of course of a great success in computation and finance with large resources is mouse work and you you already heard a lot about that and then bob will talk a little bit more detail in his talk about because he worked closely with mao and that using ecss here's another example that was recently done on bridges and that uses machine learning and so this project was about looking at the security and uncertain quality so sorry looking at the economic impacts of images and natural language in e-commerce specifically looking at the impact uh verified photos high quality photos can have on um airbnb uh business so uh so this was uh done on bridges and the pi was at carnegie mellon stepper school of business and the project in the project seventeen thousand properties were analyzed uh for four months the four months data for seventeen thousand bnb prop airbnb properties were analyzed using bridges gpu nodes using computer vision algorithms and so the findings were that so first of all rooms with verified photos were booked nine percent more often and then after separating the effects of photo verification from photo quality and room reviews uh it was found that high quality photos just being high quality results in 2 500 of additional yearly earnings for those properties so these might make sense but this study kind of uh did some quantitative analysis of the effects of having high quality verified photos uh on airbnb business and interestingly they found some spillover effects such as on the neighborhood level if there's uh a lot of rooms in a certain neighborhood that have verified high quality photos then in general that neighborhood gets a lot of business so anyway this wouldn't have been possible easily without machine learning applied on gpu nodes in exceed okay so then i will very quickly go over how to get allocations uh again we already talked about who can get allocations and then you can go to something called the exceed user portal to actually request an allocation which is in web-based portal not just for requesting access but also managing your grant getting help and things like that here's how the exceed portal looks like there's the portal.exceed.org is the address for that and so basically if you are new you can just create an account anybody can create an account my mom can create an account but then if you want to actually use the exceed resources you have to have your account added to some grant that has already been approved so not anybody can actually use the exit resources okay so here's the sign in and then once you sign in you will see something like this where there are lots of tabs the two important tabs are my exceed and allocations my exceed is where you manage your existing allocation or grant allocations is where you request or renew your grant and then there are some other very important tabs like training and help and documentation okay uh so basically if you go to allocations and then go to submit review requests that's that will take you to where you submit your allocation and there are different kinds of allocations you can request if you are about to start and start on exceed the thing you need to do is request a startup allocation and that will give you a fairly up to 200 000 uh service units for example which equates to uh for example 22 years on of computing on a single core so anyway it's a it's a significant amount and it might take just about half an hour to request that for bigger allocations you have to do something called x rack but for that you must first have a startup allocation and then have explored how you are going to do your how you are going to run your code on one of these large resources and use that information to then go for an extract account that requires a little bit more effort it's a proper proposal uh but that can get you millions of service units if you are thinking of just teaching a course using one of these big resources you can request an educational allocation and you don't have to have a startup to go for education you can just directly go for educational allocations i'll talk about campus champions a little later okay so again you can just go to startup allocations we have already talked about a little bit about who can request and i mentioned how much you can get quite a significant amount through a startup allocation up to 200 000 service units a service unit used to be core hour think uh think think of core hours as service units uh okay and and then this is how you request a service unit basically you need to fill out a form with some basic information you need a title and an abstract and then you need your cv and then you just need to upload all of that information through this web tool and click submit and you will get probably the allocation within one to two weeks so it's fairly easy it's probably half an hour on your part and then a weight of a week or two and then you would get it here are the different resources that you can request for compute resources you need to specify service units again think of core hours but now that there are other kinds of resources such as gpu for gpu resources it's actually gpu hours uh or for large memory resources it's terabyte hours and when you click on one of them it will explain uh what the su means for that particular compute resource okay you can also ask for storage resources in gigabytes and advanced services such as human expertise to help you run your code and that's just a yes no and that's pretty much it so here's for example how so if you wanted to request all the different parts of bridges there's uh regular memory gpu large memory and storage you click on them and then you have to enter how much you want on each of them and here's the explanation of what one service unit means on bridges large on gpu on regular memory so it's all very nice and explanatory and i already mentioned you'll get it in two weeks uh in up in within two weeks um and i have mentioned this so a startup can get you up to 22 years on a single core or more importantly now that you have these big resources you can now start thinking about how your how your research can expand given that now you have you can start experimenting with that that's the more important part okay adding users again the users should just such as your students they can just go on the exceed portal and create their own accounts and then tell you their username and then you have to just add them to your grant through the web portal if students are done you can go to the web portal and remove them from your again grant so all that management you can do yourself through the web portal or you can actually assign a grant manager who can do it on your behalf like a postdoc for example or a phd student who is going to be there for a while okay and then if you go again on the exceed portal clicking on accounts it will show all the uh machines where you have accounts i have accounts on all of the resources and each resource has a local username it will also appear here so if you want to directly connect to the machine you will need the local username connecting to the machine again you can connect directly through an ssh client from your such as party but for that you need to know the local username which you can get from the web portal shown on the previous slide and then you also need a local password which you can sometimes set for example at psc you can just go to this website apr stands for automatic password reset to set up your initial local password or if you forget it you can reset it and for some other resource providers you have to email them and ask them for a password or contact them to ask them for a password the other way is a single sign-on so that's this so you can go to the web portal and from there using two-factor authentication now you can sign on to different exceed resources so you can go through the web portal to the different machines or you can just do directly running jobs on exceed you can run it in batch mode or interactive mode i'll kind of quickly flip through this but you can i'm around you can ask me for more details or later through email while running since this is big data it's important to know about different file systems that are available some are small and permanent some are big and permanent or big and temporary and then as i said you can run your code in batch mode where you submit your job go away there will run sometime and then you come back and get the results you have to write a little script usually this is an example for writing a script to do this batch processing you can also do things interactively which might be actually exciting for this or useful for this community to know so you can just get onto a compute node and then just type typing commands and seeing results and you know just working interactively okay this is a way to manage software environments on exceed called modules i won't go into the details right now there's a lot of ways to get help again i'm going to share these slides with mao bob will talk next about extended collaborative support services in quite a bit of detail now on the exceed portal there's a lot of training that exceed does okay so if you do not know about mpi and want to learn there's workshops on mpi or openmp or gpu programming and you can actually go to the training tab and get a calendar of the upcoming training uh events and there's a lot of online training also that you can participate in so that's an important thing to remember you can yes this for free right up to close to half of the folks that participate in training uh offered through exceed do not have allocations on our systems so if you have students that are coming up in your group and they need to learn these things you don't have to have an allocation you can send them here you can create your account you can sign up for training there's online and in-person training there's extensive training that we provide and i want to make sure people understand that that part does not require yeah excellent point finally there's something called campus champions so they are faculty or staff at a particular institution for example with an interest in helping others in that institution start using exceed resources and so they receive specialized training from us at exceed and so and then they can transfer that knowledge uh to others in the institutions so you can find out whether there's a campus champion by going to the campus champion website and then there's a current champions button and uh in there and that will tell you whether there is a champion or if you are interested in becoming a campus champion there's also a link to send a request for that if you want to know more about that get in touch with us and we can tell you more about that 