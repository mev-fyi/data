Ian Schmutte: For this last
talk, I'm going to discuss formal privacy as it's being implemented in some
census data products. On this first slide, I'm going
to reveal my preference, I guess, for Lego's over
the budget metaphor. I had the advantage of
seeing Dan's slides before I prepared mine and
also I'm committed to using PowerPoint, which makes it maybe a
little bit too easy to just slap screenshots
into the thing. But I wanted to pick up on
the idea that basically these tools give us
an opportunity to build some pretty cool stuff. I'm going to discuss how these differential
privacy tools have actually been used in a few data products at the Census Bureau. I'm going to start
by talking about the Opportunity Atlas. Then I'm going to
move on to talk about the LEHD Origin-Destination
Statistics. Then if I've got time, I'll talk about some data
that I've been working on, the Innovation
Measurement Initiative which is a partnership with the Institute for Research
on Innovation and Science at the University of
Michigan to develop some data that are going
out to universities there. We'll see how far I get. I want to start first
by talking about protection system
that was used for the Opportunity Atlas data. This is the paper
that I mentioned in the first talk by Raj
Chetty and John Friedman, which describes the
method that they use to get the Census Bureau to be willing to
release these data. If you're not
familiar with them, these are data that provide information on social
mobility outcomes for individuals who are in
their 30s right now based on the characteristics of the households that
they grew up in. What's particularly
remarkable about these Opportunity Atlas data is
that they allow you to look at summaries of these social mobility outcomes
disaggregated by race, gender, and census tract. Census tract we can think of as mapping on to roughly the
level of the neighborhood. Here's a map, it's generated from the Opportunity Atlas data, which allows us to
see, for instance, that black men who grew up in Watts 30 years ago have mean household income around $7,286. Whereas men who grew up in
nearby Compton ended up having mean household incomes more than twice as
higher, $19,000. These data are available for download for
researchers to use, but they also are the background for sort of mapping application. So I can plug in my neighborhood here in Athens, Georgia. Then it'll tell me that
children that grew up in households and
this neighborhoods, say 30 years ago, ended up making around $43,000 here focusing on white males. I can get these estimates
not only separated by tract, but for looking at households where household
incomes were either low at the 25th percentile, the median, or high incomes. These are remarkably rich data. The question is, how exactly did they persuade the
Census Bureau to let them get these
data out the door? Because I'm not aware of
any other data that provide quite this richer detail on individual outcomes
that are quite is high level of
geographic resolution. It was possible by using
a form of privacy system. The Opportunity Atlas data
are based on tax records, but you can sort of
think about the raw data a little bit more abstractly. We've got data for individuals
who are in some race, gender group, and who grew up in a particular
census tract. We'll think about all of
that as being in group G, and these groups G
partition all of the data. Every individual in
the data is going to fall into exactly
one of these groups. For those individuals, for that group, we have a database that tells us a set of social mobility outcomes
that we're interested in. For instance, your
income rank as adulthood and they give you information on the income rank, of the household
that you grew up in, the income rank of your parents. What they do with these data is fit least squares
regression models. This is a little bit
of a simplification, but not much of one. Fit least squares
regression models, one for every group. The data are completely
desegregated, so we're fitting a regression
for each group where I'm going to predict the
outcome that I'm interested in as a function of the
household income rank. Then the queries that their interests are the things
that they're publishing, the things that they want to get out and put into
the Opportunity Atlas are essentially just predicted values at each income rank. In particular, the
25th percentile, the 50th and the 75th. Those are the queries
of interests. Now, here's the issue. These are very small cells. Once we get down to
the track level, the population is
small to begin with, and we're just aggregating
by race and gender. These predictions, these predicted values have potentially
high sensitivity. Nevertheless, the basic idea, the basic building
block here, so think about calibrating the
noise to sensitivity. What you could try
to do is to publish this statistic that
you're interested in by essentially taking
the true statistic and adding some Laplace
distributed noise. Where the noise is scaled
according to the sensitivity of the statistic that
we're interested in, here, these predicted values. If you do that, we know
from Dan's talk that this is going to satisfy Epsilon
differential privacy. Because the group's
partition the data, we're going to get a
parallel composition results that tells us that we can actually publish these for many, many
different groups. But actually at a relatively
low level of privacy laws. This isn't going to work though. Recall that differential privacy depends on how much
the output can change when it's evaluated on any two different datasets. The problem is that when
we're thinking about this global sensitivity
calculation, we have to think of all of the potential inputs
that could be fed in, not just the ones
that we observe. Global sensitivity
here is just too high. For the Opportunity Atlas,
again, the sensitivity is how much the conditional
mean of say, the child's rank in the earnings distribution
could change if you add or remove any legal value for
any conceivable dataset. The answer is a lot, so they show this
hypothetical example of a case where we have a particular group
where there's not very much variation in
the parent's income rank. If we add an outlier, say somebody who grew up
in the poorest household possible and ended up being
in the highest income rank, then that's going to
cause the slope and all of the predicted values
to change considerably. Our sensitivity is
going to be high if we focus on the global sensitivity. Well, so that's not going to work so
what else might we try? Well, we can think about using something called
local sensitivity. Global requirement is overkill. Because in this case, we know
we have access to the data already and we're going to apply this method to this
particular dataset. We don't need to think about all of the potential
datasets that we have. We need to think
about protecting deviations from the dataset
that we actually have. This gives us the concept
of local sensitivity. How much could the
conditional mean of child earnings rank change if I remove or add any legal value from the observed data set? The answer there is not as much. If I look at the regression in the data
that I actually have, this is the same issue
that comes up in general, we're thinking about sensitivity of regression estimates. Most of the time, we don't
expect our regression to be particularly
sensitive to an outlier. But it's only in these
extreme cases where we get high global
sensitivity results. This seems pretty promising. This suggests a new method
where we could just publish the true prediction plus some Laplace distributed
noise where the variance of Laplace noise is now scaled according to
the local sensitivity. The problem with
this thing is it's no longer going to satisfy epsilon differential
privacy for reasons that Dan alluded to in the
talk that he just gave. This is frustrating. This privacy aware analysis requires that not just that we publish the query results
that we're interested in, the predicted values
plus the noise. But to do the privacy
aware analysis, we also need to publish information about the variance of the noise distribution. In this case, the variance of the noise distribution is
based on local sensitivity, which is a function
of the true dataset. That makes it also a
population statistic. That means that it also
has a privacy cost, so we can't publish
the local sensitivity without adding to
the privacy cost. A third approach, and this is the innovation in this Chetty Friedman method, is to think about constructing an upper bound on the
local sensitivity that is not disclosive of the
information in each cell. But that does not require adding
quite as much noise as the global sensitivity. The way that this works is you could just calibrate it to the maximum observed sensitivity that you ever see in any cell. This is a plot of
the sensitivity of the 25th percentile
predictions for household against the number
of individuals in attract, both plotted on a log scale. You see is that the
sensitivity tends to be higher in tracks that have small populations and decreases linearly and the logarithms
as the population increases. You could calibrate the noise according to the maximum
observed sensitivity, which we'll put
it at this level. That turns out to also
add too much noise and so instead, they calibrate relative to the scaling parameter deflated
by the population size. The sensitivity is going to
be now be a function just of the size or the number of
individuals in each group. This is a Goldilocks solution. You can use Laplace
mechanism published the true query plus
Laplace distributed noise, where the Laplace distribution is scaled according
to this maximum observed sensitivity
envelope is now a function of the
size of the cell. Where the scaling parameter is constructed by just taking
the maximum over all groups. The product of the
number of individuals in the group and the local
sensitivity of that group. This unfortunately is not
epsilon differentially private because technically when we're publishing this
scaling parameter, it is a function of the data and it is published in a
non-undistorted fashions to be able to adjust inference. However, if we're
willing to condition on the existence of this
scaling parameter, then the publications from
the for the Opportunity Atlas satisfy the differential
privacy guarantee and satisfy parallel
composition across the groups. The are some full
implementation details, but I'll skip through these. One interesting not those
that according to the paper, they said the privacy
loss parameter Epsilon 8 and argue that this is based on an accuracy measure that was framed in terms of the
probability of correctly classifying tracks
in either the top or bottom tail in terms of
social mobility measures. But they don't get as far into the details of exactly how that process worked
as I would like, that this is exactly
the problem that is at the heart of the
first part of my talk. Thinking about how
more information about how these privacy laws, parameters gets set, I think is important
to be discussed. Takeaways from this first
application, this MOSE hack, maximum observe sensitivity
envelope hacks was called solves the issue of
high global sensitivity. It's hard to imagine these
data being published under conventional statistical
disclosure limitation. As I showed in the first talk, if they had been allowed to do it through cell suppression, it would give far
worse inferences. Of course, we'd
not be able to do privacy aware analysis
and the same way. There's new research, actually published an archive just this month that gets
full differential privacy results for
this class of problems. There's progress currently
being made on this problem. Now, I want to pivot
to talking about a different type of formal
privacy model applied to publication of summaries of linked
employer-employee data. This is a paper by Haney,
Machanavajjhala, John Abowd, Graham, Kurtzbach, Vilhuber. Question here is
how do you protect data that are known as LODES? The LODES are the LEHD origin destination
employment statistics. The data that
underlie the LODES. The LODES are essentially
a tabulation of jobs that are in
the LEHD database. These are jobs that are subject to unemployment
insurance taxation. They include information on the characteristics
of the workplace, including its location,
the industry, and the type of ownership and characteristics
of the workers, including their
residential location, age, race, ethnicity,
and gender. These data can be used to track commuting patterns as
well as to understand the distribution of employment by different demographic
characteristics across space. Once again, the data here sparse because of
course we're looking at employers geo-located
down to the block level. Employment data can be skewed
and so that means that there's the potential
for a lot of disclosure of employer
characteristics. Furthermore, there is a
statutory requirement to protect both the
workers and employers. This calls into
question what exactly is the nature of the
underlying database? How exactly should we
think about neighbors? Should we think about neighbors as neighboring databases
as being one where I add or remove a
worker or one where I add or remove an employer
or something different. It turns out that it's
something different. There's an approach that they introduce in this paper is based on privacy protection
notion called pufferfish. The basic idea is that first decide what needs to
be protected and then define neighboring databases in terms of those protected
characteristics. Then devise provably
private algorithms on the basis of that, essentially what's
going to happen is through the application of a clever definition of
neighboring databases, you can apply most of the differential
privacy technology to this problem in a way that deals with the highest
sensitivity of the data. The first question and this is an important
one to start asking. If you're end up in this situation of designing
one of these things, what exactly is it
that needs to be protected because you
don't want to add more noise than you need to protect things that don't necessarily need
to be protected. They argue first, based on statutory requirements, the data need to satisfy
the condition that there's no potential for
re-identification of individuals. In particular, it shouldn't be possible
to learn too much about whether or not a
particular individual was in the database or not, whether they work for
a specific type of employer where they have a particular set of
demographic characteristics. This is the same type
of privacy guarantee we've been thinking
about all along. However, for employer
establishments, the situation is different. The privacy requirement there
is that we only need to protect against precise
inference of establishment size. In particular, the fact
that an establishment, an employer business exists is not considered to be private. The industry and location where the employer business
operates are also not considered
to be private. Finally, the core
size's private, just not the exact size. In other words, we need
to protect against the ability of an
adversary to learn about an exact
establishment size. But it's okay if they
learned some information about the fact that the
employer is large or small. Finally, there
shouldn't be an ability to make precise inferences
about workforce composition. That is, it shouldn't
be possible to infer things like the share of workers that are female in a
particular establishment. I'm going to focus
on points 1 and 2. Point 3 involves more
technical detail than I'm inclined to get into. There's a formalization here
which is to try to think about exactly who are
we trying to protect? Who are we trying to
protect the data from? Here, it introduces the
concept of an adversary. In this setting, we're
going to imagine an adversary who
knows the set of all employee or establishments E and their public attributes. They know the set of all workers that could appear
in the database, but they don't know which
ones are in the database. For each worker who
could be appearing, they have private attributes. But again, the
adversary doesn't know what those private
attributes are. One of those private
attributes is where they work and whether or not they
appear in the data at all. The adversary is potentially trying to learn all of
these characteristics. The adversary has beliefs that are consist of a distribution over the
workers attributes. Then those can just be
composed together to give prior beliefs about all of the workers that might
appear in the data. On the basis of this
adversary concept, they define two concepts
I'm going to discuss. One is an employee
privacy requirement and the second is an employer
privacy requirement. So employee privacy
requirement just says that on the basis of
the published data, it should not be possible for the adversary to update the
log of the base factor by more than a factor of Epsilon in differentiating between
whether a worker had characteristics, say A or B. This is the privacy guarantee, the requirement that
any mechanism should satisfy the privacy
requirements. Employers get a guarantee of protection of
the employer size, and here, they add the concept of an additional
privacy parameter, Alpha. So Alpha is a parameter
that's going to make precise exactly how fine grained the attacker's inferences about the employer size could be. Employers size requirement
says that an attacker's posterior inference about
whether the employer has size x or has size 1 plus Alpha x is going to be bounded
by a factor Epsilon. This is telling me that the privacy requirement is specifically
that I'm going to shield employers against an
attacker's ability to learn whether or not their
size is within a factor of Alpha
from its true value. To operationalize this, we need a new concept of
neighboring databases. The first option we can think about the neighboring databases of being ones where
we add or remove a single worker or
a single employee. In that case, all of our queries in this context are counts, and so you could be operating low-cost mechanism
with sensitivity one. But if we do that, then
we're going to fail to satisfy the employer
size requirement. Option 2, we can just think
about adding or removing an entire employer as being the neighboring
database concept, and that would mean
that the sensitivity of any calculation would basically be bounded by the maximum size of any
employer and the database. Our queries could include
the sums of all workers. This would satisfy all
the privacy requirements. But it's going to give us
atrocious data quality because the sensitivity
would be too high. Once again, there's a
Goldilocks solution and that's to construct a
definition of neighbors, which they call strong
Alpha neighbors. We're going to say two
databases, D and D prime are strong Alpha
neighbors. If they differ in the
employment attribute of exactly one employer record e, and if we let x be the number
of workers at e. This is the size of the
employer establishment in the initial database D, and x prime is the
number of workers in establishment e in the
neighboring database D prime. We just require that the
number of workers in the neighboring database
be bounded between the initial level x and
1 plus Alpha times x. In other words, neighboring databases
are going to be those where I take a single employer and replace their employment
with a value that could be up to 1 plus
Alpha times its value. This leads to new
privacy concept, which is Alpha Epsilon
-ER -EE privacy. Here, the privacy definition is exactly the same
privacy definition that we saw for
differential privacy. All that we've changed is
our definition of neighbors. Now we're saying that
for every pair of strong Alpha
neighboring databases, the ratio of the probability of seeing any output
when the mechanism is operated on the data set D relative to its
strong Alpha neighbor, D prime is going to be
bounded by e to the Epsilon. This is sufficient for worker and establishment
size requirements, and it's going to inherit all of the nice composition
properties that we get from the standard
differential privacy results. The trick here is
essentially to just define the neighboring
databases in terms of exactly the quantities
that need to be protected. Now, in application,
the global sensitivity here can still be high. The key query is
total employment. If q of D is such a counting query
than the L1 sensitivity, still be the difference between operating that counting query
on the initial database, and implementing it on
strong Alpha neighbor. The sensitivity
can be as high as the maximum over
all employers of Alpha times the
level of employment. If employment can
potentially be very large, then this sensitivity
can still be Alpha times something
very large. This could still give
problems with sensitivity, at least global sensitivity. Global sensitivity here
is essentially unbounded. However, if instead you look at the sensitivity
of the log of the counts, that's going to be
bounded by 1 plus Alpha. They build a mechanism
called Log-Laplace which adds noise to the log of the count and then takes
an exponential to back out to get the publish count. Log-Laplace mechanism satisfies strong Alpha Epsilon privacy
for employer attributes, but does have the result
that it's biased. However, through
post-processing, you can correct for the bias that is introduced
by taking these logarithms. They also introduce
some other mechanisms using an approach called
smooth sensitivity, and it's a
complimentary approach to the Goldilocks problem. Here, the idea is like in
the Chetty Friedman case. You want to find some bound
on local sensitivity and use that as the
noise scaling factor in operating, say,
Laplace mechanism. Because the idea is to
derive a function S, it's a function of the data such that that function is always above the local sensitivity, like in the maximum of
zero sensitivity envelope. But it satisfies the property that that function is itself
not particularly sensitive. If I operate this function on the initial database versus
any one of its neighbors, it's going to be
bounded above by a factor e to the Alpha. What this means is that operating with the
smooth sensitivity, in this particular use
case, you can add noise. It's proportional to the
maximum of Alpha times the employer size
over just the set of employers that are in
the initial data set. On the basis of that degenerate, two additional algorithms,
one called smooth Gamma, which basically adds
Gamma distributed noise calibrated to this local or calibrated to the
smooth sensitivity, which is unbiased
and smooth Laplace, which satisfies
an approximate of privacy guarantee,
is also unbiased. Something I like
about this paper is that they compare the effects of this form of
privacy protection with the existing privacy
protection mechanisms. They're looking at the L1
error on a set of counts, which are margins across
all different places since there's places make sectors
and ownership categories, and compare the L1 error using the original system relative
to the proposed system. When this L1 error
ratio is above one, it means that the
proposed mechanisms, these formerly
private mechanisms, are doing worse than the preceding mechanism and when it falls below one,
they're performing better. What we see is that the
Log-Laplace and smooth Gamma, so we have Epsilon on
the horizontal axis, and each of the lines here corresponds to different
settings for Alpha. As Alpha changes, we're
changing what it is that we're providing a
privacy guarantee of. Say, when Alpha is
set equal to 0.2, we're telling employers
that it's not going to be possible for an adversary to learn their
true employment to within a factor of 20 percent. Whereas if Alpha
is equal to 0.01, we're telling employers that
it will be impossible for an adversary to learn
their employment within a factor of plus
or minus one percent. Those are very
different promises. The factor Epsilon tells us how much an attacker
is going to be able to learn about whether or not
their employment is within, say, for the factor of Alpha. What we see is that we need to, if we set the factor of
Alpha relatively low, then we're providing a relatively weak
guarantee of privacy. But we can get
acceptable quality. However, as the degree of coarseness with
which we're willing to allow adversaries to make
inference increases, the quality deteriorates
and you need a greater degree
of privacy loss of greater Epsilon to
counterbalance that. So there's a trade-off here
between Alpha and Epsilon. This smooth Laplace, because it's an
approximate method, tends to do better on all
these different measures. To wrap up, I'll spend some time talking about work that
I've been doing at the Census Bureau
with the Umetrics. This is the partnership between the Institute for Research and Innovation
and Science, IRIS, and the Census Bureau. The goal of this
partnership is to track employment and
earnings outcomes for grant funded employees. There's more research
going on here in the innovation
measurement initiative. This particular data product is something called an
employee profile report. The idea is to report back to university partners
information about the employment outcomes for individuals who were
funded on grants, who attended those institutions. IRIS is receiving data from different university partners
that provide information on individuals that
were funded through grants and vendors that were supported with grant funding. The partnership links
data on the outcomes for individuals that come from W2 records information
in the LEHD program, then in the business register. The desired outputs here is, the IRIS wants to be
able to report to its university partners
characteristics of workers. So in so particular,
employment outcomes and average wages disaggregated by an individual's
title when they were at the university so whether they were a faculty
member or grad student. Their sector of employment after leaving the university
so whether or not they ended up in academic environment or
non-academic environment. They want to track
those outcomes for up to 10 years after
leaving the university. Furthermore, there's one
table for each university. Privacy requirements here are to protect the university
employees against re-identification on the basis of whether or not
they're included in the data at all and all of the attributes of their
employment history. That means the
neighboring databases are going to be
those that add or remove a single employee and their entire
employment history. Just note that this would be a simpler problem if all we were worried about was just
protecting a single job. If it were, say, for instance, okay to know that individual
worked for the university, we just don't want to disclose too much information about their earnings or employment in any single year then this
problem would be different. The methods that we use
to do this publication, first, take the
Laplace mechanism to generate the
employment counts, which have sensitivity 1. We used a modified version of the maximum observe
sensitivity envelope to be able to publish the
average earnings using the Chetty-Friedman method where we calculate the maximum observed sensitivity
scaling factor at a job title by sector level. This places an upper bound
on the sensitivity envelope. Something that made
this implementation a little bit different
is that we have an accuracy requirement due to pre-existing agreements
with the IRIS, they had an expectation
of a threshold level of data accuracy measured in terms of an average
percent deviation. On average, the
difference between the true and noisy values
scaled by the truth need to match some preset level. The thing I want
to focus on here is the privacy analysis, where we want to understand after we do all
of these calculations, What's the total privacy loss? You can reason through the different composition
possibilities. Each worker appears only in one job title
by sector pair. That means that across
those groupings, we can do parallel composition. However, each worker can appear in multiple
years of the data. That means that every time we're publishing information
one-year out versus two years out, those privacy losses have to be accounted for using
serial composition. Finally, each
record is used both to compute total employment, the employment count,
and the earnings levels. We once again have
serial composition. If we define Epsilon emp ts, this is the privacy loss parameter for
employment queries, t years after leaving university s and the second privacy loss parameter for the
average earnings queries t years out after
leaving university s, then the total privacy loss
associated with publishing this employee report is gonna
be the sum over all years, say one up to 10, because of the serial
composition result of the privacy loss associated with publishing
employment for that cell and the privacy loss associated with publishing
earnings for that cell. If we set this privacy
loss parameter the same for each job title by sector group parallel
composition tells us that the total
privacy loss is still just Epsilon s. This is our total privacy
loss associated with publishing the data for
a single university. Something to highlight
then in thinking about these privacy losses is
that this means that universities that
are particularly small or have fewer years
of data end up with a different allocation of the privacy budget than
universities that have many years of data or
have much larger cells. The distortions that
are introduced by this process reduce the quality more for small universities than for larger universities. Ideally, these privacy
losses could be tailored to individual
universities. Other examples that
I won't go into of formal privacy
systems being used at the Census Bureau are data in the post-secondary
employment outcomes, where there's a
clever method for publishing the entire
distribution of earnings for individuals after completing university
education and the veterans
employment outcomes. You can read up on
the documentation for these methods in this paper which is linked here and the slides that will be available on the website
after this talk. 