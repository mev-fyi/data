foreign thank you for those of you who are not leaving to go take an early lunch no just kidding I love you too um Christopher I work on Anova um and it's yeah the talk is very opportunely timed I guess what I'm talking about here just to situate it a little bit also Public Safety advisory I wrote These slides at like five in the morning I was still kind of jet-lagged they might not be pedagogically optimized if you you know like the talk but you think the slides are confusing or you didn't like the talk because you thought the slides were confusing in either case feel free to come find me afterwards and complain about it um so I think it might be helpful just for this crowd here to situate anoma in relation to say a project like Suave I guess we kind of you know we envision our role in a broader ecosystem is sort of doing things which are complementary like doing things maybe which other people aren't doing based on what constraints you know we we might have that are different in a particular one thing which is different about anoma is compared to kind of projects in the ethereum ecosystem is basically that we are willing to ditch backwards compatibility as in that like you know I've worked with ethereum and I've worked with Cosmos and both of those systems are great and they're working in the real world and they have real users and those things are amazing and I'm thrilled that people are building on those systems uh on the other hand I think those systems also embed a lot of design assumptions and many of them are kind of old like they're designed assumptions that were made based on our understanding of the world you know five years ago and some of those assumptions I think have proved to be right and some of them I think have proved to be you know if not wrong at least like kind of orthogonal to what has actually happened so in that Spirit I'm going to explain a little bit about how taking this kind of intense and so we you know has been working on this intense Centric idea often phrased in kind of vague terms for a little while and now it's kind of permeated its way through all the layers of the protocol stack to what you might call a via and this talk is going to be about that so to kind of related to the previous one this is like a proposal for an intent language or so to speak and I'm not you know what is an intent that's a great question maybe intensor decided based on extensional quality there's no one true answer I'm not going to try and give it you know the one true holy answer to what is an intent this is kind of our answer to how can intents be represented and it's an answer that we think satisfies you know design criteria that many of these projects may actually share um so we think it's it's interesting for this audience so this talk was inspired by basically a tweet and a meme I would like to give credit uh so the Tweet uh thank you Andrew Miller who I haven't seen here but maybe he will somehow see this later intents and transactions aren't really different let's fight uh well well I mean if you like squint really hard intense transactions there's like some data you send it to the distributed system like something happens but I think I think that's like squinting slightly too hard um and I'll get into why oh you can't see this oops well anyway um this one uh thanks Sheen from flashblocks for this this meme so uh I don't even know what this meme is called the Vlad CT 43 37 crowd andoma they have no idea and Tesla higher order commitment commitments with extensive literature defining its semantics well well you know yes no so interestingly the extension of literature is full filled with what are called folk theorems I I.E where people seem to all know something but it took a while for someone to write it down and I think intents are kind of you know in fact they're kind of the same thing from the research literature also have this folk theorem property you see stuff like account abstraction and these kind of user Gas payment systems kind of approaching the concept of an intent but in this domain specific way so this talk is about a kind of General intense Centric via and let me clarify a little bit what I mean by VM because that word is used to refer to many things which I think are not really in the same category so a long time ago that people were building computers and they wanted those computers to execute programs and those computers had Hardware constraints they had like a processing unit they had some instructions that processing unit could execute they had you know some kind of like various levels of memory at different speed to sort of durability trade-offs they had registers and a stack they had volatile memory slightly less fast volatile and they had non-volved storage and they wanted to like run programs I mean very reasonable this is like 1960 no one's thinking about Mev they wanted to execute programs sequentially so if you go on the Wikipedia page to where I got it from you're not going to make up my own diagram and look at vondo I'm in architecture is you'll see something like this diagram and basically the distinction between control and arithmetic logic units doesn't matter anymore but the same kind of you know there's a communication system between the CPU it's executing instructions sequentially and as a result you kind of run through your programs accessing memory input output when you need to right so bonus credit anyone knows where this screenshot is from but the evm is in the grand Von Neumann style of Von Neumann via it has a program counter it has an instruction set it has different kind of layers of memory and storage and it goes through and it runs programs sequentially great um and I'm going to talk about a VM which is designed to do something else so it's not like not really competing with the ebm um but it is uh doing something which I think is perhaps more suitable to this intense Centric World in particular this VM is designed to match conditional commitments atomically and commitments involve programs so you might say well of course you just need something that will execute programs why don't you just use a Von Neumann VM and my answer is that yes you can do that but it's like not quite the relevant problem because what you want to do in matching commitments is to execute several commitments atomically and in intense Centric world is all kind of get to and more specificity later in the talk you care about whether the result of executing those intents or those commitments is satisfactory to all involve parties maybe you care a little bit less about the specific path of execution you took to get there right so there's a slightly different design constraint an evm also includes a few other things which are like not exactly Von Neumann things it includes message passing between program that includes like scheduling those are relevant I think and I will get to them later sometimes something people bring up in sort of the broad discourse is like true and completeness well if you want something why don't you just emulate it on this string complete VM which we already have so bonus credit number two anyone who knows where this is from I think this is harder actually but you definitely earn OG if you recognize this quote I'll tell you at the end of the talk um and my answer is yes you can emulate the thing which I'm talking about upon the evm I think that's great but you know the question I'm interested in is like what does the execution environment look like for intents and where how you kind of emulate that as a sort of you know interesting from a performance standpoint but kind of separate question like you could also emulate it on some other you know non-evm Bond women machine and nothing would change that much so I want to go just to kind of go back a little bit into the research literature uh just to add another answer to the question of what is an intent this meme is my own unfortunately you can't see the bottom but it says intents are cybernetic commitments to the Future human LL um synthesis something like that it doesn't matter because I'm not going to talk about that I'm mostly going to talk about the third thing so what are end times intents are just transactions intensive binding conditional conditional commitments from 50 old game three right so here's the 50 year old game theory um the first kind of folk theorem to be written down by Friedman in 1971 was this uh equilibrium for super games and super games I'm very sad that that term has kind of gone out of fashion I think it's a more fun term than like repeated games super games who doesn't want to play the super game it's like the infinite super game uh anyway the result established in this paper was just the repeated interaction can result in any feasible individually rational payoff and then a bunch of recent work I'm just going to cite one there are like 15 papers that talk probably more I just haven't read them but the talk about different variants of uh this idea is program equilibria I think this paper is particularly clear so I recommend it and the idea of program equilibrium is basically that you can achieve this same result you can get any feasible individually rational payoff if you have users instead of just taking actions themselves use what are called commitment devices or use programs and commit themselves to a strategy and because this is because this provides like a credible guarantee of how users will act then you could do stuff like this in the prisoner's dilemma if my program is the same as the other players program then cooperate else defect seems like very straightforward right so program equilibria I think are you know what you might call the closest like a near-term or like relatively Recent research basis for intents and I think the research literature here at least like I'll talk in just a sec about what I think we need to change but it describes the problem pretty well from a mathematical perspective in terms of what we want to get out of these systems and why it's interesting so from that perspective I want to ask the kind of like different direction of questions and forget blockchains like whatever what if we just wanted to send around commitments and then we want to like settle them somewhere you do need public knowledge to make it credible uh let's just say commitments are functions we publish them to a blockchain and the blockchain like calculates this equilibrium um uh which is sort of individually rational and feasible um I argue that this runs into basically four challenges so if you just try and take the research literature and implement it um I think you run into all of these challenges um you know I'm not maybe they're three not four it's not exact mathematical characterization some of them are kind of related to each other and depend on the particular language but I want to go over each of these in turn the first is sort of termination the second is function introspection the third is this difference between nominal and structural identities and the fourth is unclear information flow so um right a termination fixed Point finding so if you use the kind of most powerful representation in the research literature which gives you the best game theoretic results um you need to use higher order commitments as in commitments which are dependent on other players commitments and you want eventually your execution to terminate which basically means that you need to somehow you know if your commitments are dependent on what the other players do and what the other players do depending on what you're committing to do that system runs forever unless someone short circuits somehow and I've seen kind of two approaches in the research literature to short circuiting um one of them is to use Randomness so if you have kind of a shared random speaker and then you can say like oh well you know one percent of the time I'll just cooperate else I'll like call the other function and that function will call me and I will call that other function until some Randomness happens and then we'll both cooperate and then it will terminate right so it gets you like basically the same results modulus on Epsilon but I think this is like a more serious implementation problem than it might appear to be for two reasons one Randomness although it's possible to get these kind of distributed systems you know threshold BLS signatures or something it adds an assumption which like it seems like you maybe don't want unless you really absolutely need it and there's this kind of annoying trade-off that you have to now run for a lot of time until your Randomness causes the system to terminate and when your execution is replicated that seems like a very expensive thing to do another way to do this is that you can add a separate payoff basically bribe someone um uh to make you terminate and that also works but I think it sort of creates kind of unnecessary Mev um so program equilibria and the reason I said these were kind of related is the program equilibria deals with this problem by like checking by reading the source code for The Other Guys program so if we go back here okay so in the program equilibrium paper they write the program like this there's some very simple syntax but this is just the example if my program equals the other player program then do cooperate else dude effect um that's great the problem is that this equal sign is like open type Theory research problem in the general case um in particular if you want to deal with sort of programmatic preferences which are going to encode complex things and you care about like specific sort of structures of if you do this kind of thing what does your counterpart do uh you would need like full dependent types like you basically need to prove arbitrary properties of these other functions to figure out what your fixed point is um so this is you know seems very heavyweight of course there are dependently typed languages but ideally we wouldn't need to embed dependent types as a design requirement for our intent Centric architecture third challenge is what I call you know I don't know if this is exactly the right term but what I call nominal versus structural identity which is that in these formalisms uh all of the players are known and they're numbered so they're referred to each other like by index and if you have a formalism I think like the one from 2011 that uses commitment devices these devices are referred to each other by index there's kind of a closed World model where you can say oh well I care about like what this specific player does and I know beforehand who all the players are I don't think this matches what we need in the kind of distributed typical distributed blockchain setting in particular we want we're dealing with this open world where we don't necessarily know I mean we definitely don't know all the identities of the players we don't want to know for privacy reasons and the set is like very large um and anyways we typically don't care because we want to be able to find counterparties on the basis of you know structural identity like what capabilities they have where something like owning a token is a capability you know some maybe we're actually trying to use these systems to avoid war and then we want like credible attestations that this party can actually take some action in the external world something like this so we need to change the kind of basis of identity from this literature to include all actions that matter in order in our context in the my sort of fourth fourth thing that I think we need to change is you know not precisely characterized but typically these uh constructions from the research literature require or rely on this like magic logically centralized commitment executing computer and it needs to take all these commitments and like calculate the equilibrium somehow or run them all or do something fancy and then it needs to you know reach the uh you know output that state right this is fine uh but unfortunately it requires this like single logical point and it's not you know it doesn't give you this kind of fine-grained information control like you might want to know things like oh I have a bunch of these commitments and some of them are not codependent so in fact I can like deal with them separately and this uh you know this this framework doesn't natively give you a way to check that okay so now I'm going to talk about the kind of specific construction that we have in mind which we call a resource logic and the reason we ended up choosing this term is that there seem to be a bunch of similarities to kind of distributed linear logic we care about you know no double spends in the blockchain context um and we like the idea of modeling everything as resources and there's a really basic kind of trick that resource logic uses to solve or address several of these concerns and that trick is to forget some information or to not care about some information and that information is the path of execution so in a typical kind of commitment scheme you have like commitments you you sort of do one commitment then you check the next commitment then you check the next commitment it's like executing small state transitions one at a time and if you look at this in kind of a branching flow you know maybe depending on what player one does if player one is the red arrow you go to S1 or S1 Prime then depending on what player 2 does you go to S2 S2 prime a it's two prime B Etc until you reach some final terminating state um and my argument is that we actually have This brilliant property which allows us to not need to do this which is atomicity and we can get this by changing the type slightly so instead of having commitments be kind of higher order functions we can instead just pretend that we already executed everything we can say the commitments will take some final State including some you know specific strategies which are played by particular players and commitments will say yes I'm okay with that or no I'm not so still they're issued by particular parties who have the ability to authorize particular actions you know this is like a very high level view but um if we do this then we can not care about the path so I sort of um way to to bring this into a practical context is that the earlier talk talked a little bit about these different sort of intent-centric vertical specialized systems like account abstraction or something like this and one thing which these systems often differ in is like where the execution is specifically happening so there are some you know still some ultimately some verification on chain but Sometimes some you know someone is executing to do meta transactions Sometimes some Searchers some Builder is executing the execution is happening somewhere and if we can architect our VM in a way which doesn't care about where the execution happens as long as the final result is in line with everyone's preferences it still satisfies the kind of state transition constraints of the system then we sort of generalize all of these cases as in like people can choose a specific topology for where the specific components of execution will happen at runtime and all that we need to check you know on the final blockchain that we've agreed to trust for sort of uh you know consensus and stakehospitality purposes is that um you know everyone is hap everyone who took an action in this transaction is happy with the final outcome so how do we encode this um in a number we encode this and something that we call resources and resources are uh you know they're they're like a little bit like smart contracts but not quite uh you could if you squint you could also call them smart contracts but that term has come to be associated with many things and some of those things resources are not um in particular resources do not encode imperative execution logic so smart contracts typically encode like start at State one do some computation ended state two right resources don't encode this instead resources encode constraints so in a Noma each resource has what we call a logic where a logic is this kind of predicate function over partial transactions which I'll talk about in a second maybe some arguments prefix suffix quantity which is like like if you have different tokens it's just for the linear logic balance so if you have different tokens you might have one unit of a token two units of a token you want to encode fungibility into your based system because it gives you a lot of capabilities and some kind of arbitrary value so if you want to like squint and think about them in smart contracts the logic is the code and the value is like the state in the ethereum system but we have this built-in way to encode fungibility into the VM and resources don't specify any kind of execution logic oops okay then the units of the system are kind of the closest thing to an intent how you might encode an intent let's say although it could also contain subsided information is what we call a partial transaction a partial transaction includes two sets of resources it includes a set of resources which were consumed and a set of new resources which were created and it includes some like arbitrary extra data such as signatures and the idea behind partial transactions is that we can have this kind of resource validity check at the partial transaction level so partial transaction may not be entirely balanced so we might spend resources that we don't actually have or we might create resources that aren't yet consumed but we can check if all the predicates are satisfied just by mapping over them right then we can separate out what we call the balance check so the balance check relies on you know maybe you can that this kind of fungibility of resources question is not so important for the order of execution point but you could think of there as being different denominations of resources the denomination of resources calculated based on the logic prefix kind of the static data and then uh what we require in order to actually have a transaction be balanced is that there's no change basically the sum of the sort of Delta of all the created resources less the sum of the Delta of all the consumed resources is zero so this is the like linear logic check if we think about the system in linear logic terms now this has several nice properties one is that we kind of uh partial transactions compose so if you just take two partial transactions and they're valid and you join they're created and consumed resources you end up with another valid partial transaction so the system is compositional uh now they might like conflict with each other or something it has nothing to do with you know ordering but the partial transactions themselves compose also a transaction like a valid full transaction is just a partial transaction that in fact happens to be balanced so there's no sort of hard distinction between intents and transactions anymore and when you create these partial transactions and compose them and append to them you can do all of that execution kind of wherever you want it just needs to end in this valid full transaction which you published for the blockchain so you could take so for example if you think about the um uh specific prisoner's dilemma case it's almost like you just pretend that you can do some execution which you actually can't do so you pretend that you're the other player and you create a resource that says that um your counterparty cooperates um and you you know also uh I'm sorry consumer resource that says your account that your counterparty cooperates and you create a resource that says that you cooperate which you have the permissions to do um you don't have the permissions to create a resource that says that your counterparty cooperates but you can consume one that says that your counterparty cooperates where then this balance check sort of uh defers the check of the um uh like validity of the whole thing to the transaction level instead of the partial transaction level so symmetric partial transactions if you create a partial transaction that says that you cooperate and consume a resource that says your counterparty cooperates and your counterparty creates just the inverse then these will balance because the denominations Council cancel out for something like a token Swap and you just consume the tokens resource representing the tokens that you already have and want to pay create a resource of new tokens assigned to you and similarly symmetric partial transactions will balance you know maybe with some slack that's like an Mev question which I think is very interesting but orthogonal to this VM design question so I'm not going to talk about it um and I will argue that this model addresses these concerns with kind of faithfully translating this uh conditional commitment uh Concept in particular IT addresses this kind of like termination and introspection difficulty by just splitting computation from verification so if we think if we go back I think this slide is helpful if we go back to um the kind of path in the configuration space of potential State changes that we can make when you make a partial transaction you can like make some State changes which you don't even have the permission to do like they're far down the line right but then you can send that partial transaction to someone else who could make the state changes which if the whole system were sequential would have happened first but they don't need to happen first anymore because we've made our VM agnostic to the actual ordering of computation all it needs to care about is that everything checks out okay um and verification uh then to deal with nominal and structural identities when we specify interactions on the basis of resources instead of on the basis of like you know this one may be simpler like blockchain to kind of already do this just to be comprehensive um resources already connote the ability to do something because they're owned so inclusion of other players is explicit in this model if you want you know to check that your country party cooperates you just put that check right in the like validity check of the partial transaction right you consume the resource that says that your counterparty cooperates and they can only you know the whole transaction is only valid if your counterparty in fact see this and creates the resource that says that they cooperate then um addressing information flow so this you know this is a VM it's not like a language for information flow um but because validity conditions are separate from the balance check this helps a lot with building a kind of good substrate for information flow in particular it means that you can make the proofs of validity separately and prior to checking the balance so if I like spend my tokens I mean a partial transaction and the whole thing is on balanced but I can go ahead and make the proof for that spend I can like hide the note where they came from I can send the partial transaction to you and you can see what constraints have to be satisfied in order to balance it without seeing the path of execution that allowed me to like create that potential state in the first place so the validity constraints or so to speak or forward it and they can kind of be satisfied at any point during execution as long as they're all satisfied at the end right okay so the spicy take which I do not offer a like comprehensive mathematical defensive in this talk but uh I haven't heard any good counter arguments yet um is that this research structure is kind of inevitable I think as in that what you really want in order to generalize intents is to build a kind of VM or an execution environment which is agnostic to where execution happens you don't want it to care about the path you only wanted to care about the end result you can still use the evm to do execution I think that's fine but I think you would end up with something on top of this that will kind of do this sort of intent matching and that will end up splitting out the validity and balance Checks In This Way um then I argue if you end up with this then like the evm does things that maybe you don't need like in particular it does this sequential message passing execution which is fine but I just don't think it solves the problem that like matching commit matching conditional commitments wants to solve because it is path dependent and we don't care about the path cool so just the last part here kind of to touch upon the first talk um in going towards uh more privacy Center world is how I think this research resource model can act as a substrate for information flow control so I want to First clarify what I don't mean by substrate for information flow control there's this great paper called Viaduct by Val fructose some other Cornell which I think is a good resource for people trying to think about um information flow control in an mbb context blockchain context and that paper includes a light witch for information flow control this is not language for information flow control it's kind of like a runtime so the way this paper feedback structures things is that they have a high level Source language that describes in a pretty declarative way information flow control policies like constraints on who is trusted that they sort of be or not a includes like Integrity assumptions and information assumptions um and uh maybe such a language could include cryptographic assumptions that you're willing to make other kinds of constraints and then that language is like compiled somehow into a bunch of instructions which are run using actual cryptographic Primitives the runtime execution engine is responsible for running The Primitives and the correct sequence with real data and this is what I think the resource model is kind of suitable for as that it could be a part of at least with specific cryptographic Primitives this kind of runtime that executes uh partial transactions with real data and you know if it's constructed correctly can um enforce information flow control policies by calling The Primitives in the right order right so this is not you know you could have many different higher level languages which compiled and run on something like this um but I think this system is helpful because it is very amenable to what I call the least powerful primitive the rules should be in quotes really because it's not a rule it's just a heuristic but um there's kind of this power ranking of cryptography Primitives where you have very powerful Primitives that do everything but extremely slow and you have very fast Primitives that do only one thing like hash functions or even slightly in the middle zero knowledge proofs but are much faster and typically you want to architect your system so that you can use the least powerful primitive you need for the specific task you're trying to do Under the specific information flow constraints that you have because it will make the thing more efficient and faster you know just amenable to kind of complicated verification in cases it could be stuff like this so an example of how you would kind of use this runtime this resource electric runtime to provide the kind of information flow control people might want um is let's say you wanted to do kind of solver selection let's say you know in this system you still have to reveal some information to the solvers in order for them to match uh your intents or for them to put together partial transactions but maybe you care about which solvers you send those intents to maybe you trust some maybe they're your friends and in particular because we have this separation of validity for balance once you send let's say you send your intent to a solver with the instruction like please find me a counterparty and don't forward it don't reveal it to somebody else because we have the separation they can do that and once they find you a counterparty they can create another partial transaction which already kind of has because it has some execution it makes some proofs already doesn't reveal any of your personal data anymore let's say they take like you know you have an a for B trade and they take an overp trade and B for C trade and C for D trade and make an a for D trade and maybe under your information flow constraints this is like a fine amount to reveal because it no longer reveals that not you and not even that there wasn't a for B trading in the first place so because we have compositionality we can get these kind of nice information flow control properties um another example is batching or kind of like if you were to implement something like penumbra on the resource logic model how would you do it um you would simply consume tokens to create in this case threshold encrypted resources which would be queued in some kind of batch perhaps a per block batch those resources are threshold decrypted next round next block and they have logic such that the their logic allows them to only be consumed in this batch so basically the validators you still have to have the validators attest to like what was actually included in the batch right they're providing data availability but the validator is attest to what was included in the batch and the logic checks and all the resources are consumed that the fairness conditions a optimal Arbitrage um was satisfied another thing you might want to do is something like aggregate statistics so in a kind of privacy first world where all of your transactions are private sometimes you might want information flow control to uh some decrypt like Aggregates that still allow you to reason about aggregate properties of user interactions without de-anonymizing specific users for example um if you have a kind of private bridging system often you might want to be able to reason about Economic Security which requires that you know how much of say some assets are secured by the proof of stake asset of some particular chain and in order to do that in a world where the bridges are private you need to decrypt like some aggregate amounts of assets right but let's say you have information flow control policy that says that oh like this counter of how many assets are on this particular chain will be decrypted like every day or something like this or even every 10 blocks um resources don't magically solve any of the cartography problems for you but they create this nice separation of different parts of State in different parts of logic and then you can enforce different information flow control policies on those different parts of state so that's why I think it's kind of like an amendable substrate um right so uh kind of in summary the case with the resource model and why I think it might be interesting to you if you're thinking about intents an intense Center of architectures is that resources package data and logic together very cleanly they separate out execution so that you don't need to worry about execution paths you can just worry about results they make State inclusion very explicit instead of having implicit Global state which is access to like some pattern that you don't know you access only state that you specifically need to validate for the purposes of like your application um and yeah no path dependence which is also helpful for information flow control because once you know some variable X you like f of x for some other arbitrary functions you also don't care about um conclusion of future directions uh three points if you only remember three points from this talk make it these three one uh in 10 Centric VM design is not a Von Neumann problem we're trying to build something else um it's it's not you know competing it's just like orthogonal um two speculative execution or this kind of like executing things that you can't in fact authorize yourself but you want to happen as part of a transaction plus atomicity is a very powerful tool because users care about equilibria you know to harken back to the previous talk users care about the results and if you kind of translate that philosophy into the design of the VM users care about the final output State they care about you know the game theoretic equilibrium they don't care about the path it took to get there in fact you don't even need to compute the path it took to get there in many cases if you can just you know find some equilibrium that satisfies all of the constraints and Third Kind of the spiciest spiciest perhaps controversial take is that the resource model is probably inevitable you can call it something else you can like change you know there are some specific decisions about which uh you know variables encode different parts of static or dynamic data you can change these decisions but the kind of atomic execution of conditional commitments in the separation out of the execution path so that it can be determined at runtime I think are things that any intense centered architecture is really going to want some open questions I think there are also some on the website uh please find me if you understand these things better formal languages for information flow control particularly amenable to conditional disclosure more suitable abstract intermediate representations for programs solver privacy improvements maybe particularly using like tees in combination with ckps in efficient ways I think it's interesting many more uh you can find me at cwgoz while Twitter lasts maybe we'll go away soon I'm also on Blue Sky but I haven't posted there very much but that's only because I'm lazy and I should or preferably even here thank you [Applause] 