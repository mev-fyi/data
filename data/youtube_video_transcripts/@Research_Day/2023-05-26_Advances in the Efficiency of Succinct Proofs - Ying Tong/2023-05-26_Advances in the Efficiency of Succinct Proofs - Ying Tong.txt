foreign so this talk is not going to be as scary as you might think um basically how I would frame this is that uh Tyrone just now told us about how zero knowledge can provide better guarantees and mechanism design but he used your knowledge as a black box and pretty much all the talks we've heard today use DK as a black box whereas this talk actually opens up the black box and the motivation for doing this is that first of all a lot of the recent advances and the efficiency of DK proofs come come from opening the black box and secondly a lot of features such as collaborative proving depend on us knowing where to combine NPC with their knowledge proofs and so overall I think what I want you to take away from this is some high level understanding of how zero knowledge proofs work and hopefully um which parts of the zero knowledge stack are useful for your protocol so we're going to start off basically at a high level and then we'll go down to some details of constructions and then finally we'll end up at an example which is the lurk programming language that compiles to azir knowledge back end well I want to start off with some motivating applications so today I've heard um quite a few for example the Z L1 zkavm and for example collaborative proving between Builders and Searchers um so I wanted to bring up a few that are maybe less well-known because they were they were conceived at a time where zero knowledge proofs were not efficient and were not performant enough to actually make these applications interesting so for example this paper from 2015 suggests a solution for verifiable cloud computing and specifically they focus on mapreduce which is useful for a great variety of programs like machine learning bioinformatics and this solution makes use of a primitive called proof carrying data which allows us to enforce some predicate locally um and which allows us to enforce a local predicate at on a global scale so at every point in this computation each cluster does not need to trust any of the other clusters rather it only needs to verify a single proof that the whole history of computation up to that point was correct another use case that I think many of us are familiar with is the zkml so I won't say too much about this and lastly of course um virtual machines like L1 ZK VMS and risk0 so now that I've hopefully motivated this talk we'll go into the high level overview um and this is basically talking about the proof system stack and a taxonomy of zero knowledge proofs so the proof system stack can be divided into these components we start off with some computation and we arithmetize it in order to compile it to a constraint satisfaction problem that we can efficiently check with a few probabilistic algebraic checks and then to instantiate this in the real world in an efficient and secure way we pick some kind of cryptographic compiler that introduces a cryptographic assumption for example discrete lock hardness or for example um Collision resistant hash functions and we finally arrive at our proof system so some popular arithmetizations right now are rank one constraint system um algebraic intermediate representation as well as Planck um so on a high level what we desire from an arithmetization is three features the first is uh support for lookup arguments and this comes in useful when we want to offload very expensive computation to the pre-processing or offline phase so instead of constraining like bit shifts or um hash functions inside the circuit we pre-compute them outside the circuit and simply look them up during proof generation time and this is what makes zkevms feasible today the second desirable feature is support for high degree constraints so for example plonky2 has the fastest Poseidon implementation that we know of and this is because it squeezes the whole Poseidon function into one single constraint a very high degree constraint and lastly we want arithmetizations to be compatible with folding schemes and we'll talk more about this in the second part of this talk so these three features that I've listed are basically what everyone's aiming for right now but there's still a lot of um unknowns in this optimization space for example we can achieve lots of useful functionality with just low degree Gates and if we don't if we can get rid of one of these requirements then basically we can optimize for a specific problem a lot better so one of the latest advancements I think it came out maybe a few weeks ago is some something that claims to unify all these three arithmetizations and it's called the customizable constraint system so yeah it claims to basically um be able to express any arithmetization with no overhead and it also claims to achieve a foster approver for air specifically a foster prover then the Stark prover B so um yeah air was made famous by Starks and so CCs is now claiming that they have a foster approvert and some things something interesting about CCs is that it takes a unifying approach so it wants to make a universal arithmetization and this is as opposed to another Paradigm of proof composition known as commit and prove so some of you may have heard of Lego snark basically instead of trying to unify different arithmetizations Lego snark says that some arithmetizations are better suited for some computations and if we're trying to express a heterogeneous computation we might as well use these bespoke arithmetizations and then later on combine them by committing to the same witness but by proving that we use the same Witness um across all the all the proof systems and it's it's still not clear to me which approach is more efficient um but my intuition is that there there is value in thinking about composing heterogeneous proof systems there's also been a recent line of work in optimal lookup arguments and we've basically gotten to the point where um the overhead of a lookup argument is independent of the lookup table size and it only it is a function only off the number of lookup inputs and this is good for most use cases such as range constraints where the number of possible legal values is far greater than the number of values we actually need to look up so around the same time as ccs this proof system called protostar came out and they managed to introduce a lookup argument that was at the same time compatible with a folding scheme sorry for the alignment of the slides um so um on the next level of the stack we have sort of the information theoretic level where we express um the where we express our computation as a constraint satisfaction problem and we actually engage in a protocol to probabilistically check that it's satisfied um so the trend here of note is the return of the sum check so the subject protocol is very old um relative in this industry it's considered really old it's from the it's from 1990. and it has many desirable features such as avoiding expensive operations like phosphoria transforms it's also very compatible with folding schemes because it incurs very little overhead um in particular it doesn't produce error terms in the group and here's another tweet from Arielle I think I have another tweet from Arielle left in this presentation he just tweets really cutting edge research and we can get it for free so the next step is to instantiate um um to instantiate this information theoretic model with a cryptographic commitment and to make it efficient and secure in the real world so I think something to note here is um I want to highlight that kzg commitment scheme this is used in dunk sharding which um is ethereum's way of providing data availability guarantees so um I highlight this as an example of how different components in the proof system set can be separated and are interesting in and of themselves and the point of framing proof systems in a stack is to show that they are modular and that we can mix and match different components according to um our system's requirements and finally we get to our proof systems so I think it's less trending now actually to present a whole full stack proof system um but basically these are these are the popular proof systems that um are deployed today so now that we have a high level overview of proof systems and um we can sort of classify the popular Protocols of today let's go in and see some of the recent advancements in efficiency that have been made possible and as I said before these advancements are possible because we went into different layers of the proof system stack um and applied basically recursive techniques there so these advancements lead us to the Primitive called proof carrying data and this is a very powerful primitive because it allows mutually distrusting parties to collaborate and to prove the validity of a whole chain of computation and basically it enforces um it enforces a local compliance predicate on a global level and how we get to prove carrying data is by um instantiating recursive proof composition at various levels of the proof system stack so this is sort of a tech tree of techniques for recursive proof composition that lead us to the desired primitive proof carrying data and we're going to go through them pretty much in chronological order so the classic instantiation of proof carrying data is actually a special case called incrementally verifiable computation and how you can think of this is um a dag is to a linked list as proof carrying data is to IVC um so for example plonky 2 uses full recursive proof composition in which each recursive step fully verifies the proof generated by the previous step and so we embed the full verifier of the protocol and each recursive step so ponkey2 is highly performant and the reason is that the verifier is sublinear um in the circuit size however this this sort of classic method is restrictive because there are not many systems like plonky2 um and there there may be some other systems that we wish to use but that don't have succinct verifiers so this motivated us to um weaken the requirements on the proof system such that we don't uh we don't need a strictly succinct verifier and instead we only need a succinct accumulation verifier what this means is that at each recursive step we defer the expensive part of the verification to what's known as an accumulator and instead of checking it in the recursive step we simply accumulate all the expensive computations and check it at one go at the very end so what Atomic accumulation achieves is basically a weakening of requirements on our proof system and this is what allowed Halo 2 to achieve IVC without a trusted setup and you can see this as an amortization of costs because when I say expensive verification I mean linear time so this decider at the very end performs a linear time check and this is only worth it if you amortize it across a huge batch of recursive steps so the latest development in IVC is something called split accumulation also known as folding schemes so split accumulation further reduces the recursive threshold by making it a constant size how it does this is it it reduces it removes the requirement for even a succinct accumulator instead it only needs a succinct representation of the witness of of the accumulator Witness um so concretely for example if your witness is some r1cs assignment as long as you're able to commit to it um using a constant size group element for example using a Patterson commitment then this is eligible for split accumulation so what happens is that instead of accumulating the whole proof we now accumulate only the instance only the commitment to the witness and the split accumulation verifier is Tiny it only concerns itself with the constant size commitments to the witness um yeah so what split accumulation has done is it's given us recursion with a highly efficient proverb and this is what's going to make things like lurk possible this is what's going to make things like l1zkevm possible um any application where you need to produce proofs at almost the same rate as the computation um you want something like split accumulation so yeah basically the gains inefficiency of ZK proofs have made a whole class of applications possible that were not before um so now folding schemes are spiritually very similar to split accumulation but they differ in one um pretty technical way so instead of full they basically fold at an earlier stage instead of folding at the poly commitment claim they vote directly at their relation so what this means is that the priver does not need to do this processing step of committing of computing a quotient polynomial or of computing any kind of proof about their relation instead they can directly take the relation be it an r1cs or planckish relation and just accumulate instances of it and what this lets us do is save on expensive computations like ffts so in um in folding schemes the prover only needs to commit to their witness and nothing else so the drawback of folding schemes is that first of all um it's not zero knowledge and the reason what I mean by that is that each recursive step the proverb basically needs to receive the whole Witness whereas if you compare it to Atomic accumulation at each recursive step the proverb only needs the proof so therefore folding scheme is more suitable for a single priver um Computing a large number of recursive steps and the second drawback is that because we're folding so early we need to make sure that we're always voting the exact same relation and this removes a lot of flexibility um whereas again if you compare to say split accumulation you're just folding evaluation claims so anything that looks like a commitment evaluation point and a value can be folded can be accumulated so in summary um folding schemes are pretty much the trendiest thing in zero knowledge land right now I think it's slightly boring by now but um basically all the papers that have come out recently are about folding schemes um so it's still worth actually understanding um the features of these protocols and what they're useful for and what they're not useful for so to summarize folding can happen at pretty much any level of the proof system stack so the earlier your fault the less work your prover has to do but that comes with drawbacks such as losing flexibility and losing zero knowledge um whereas if you vote later on your perver needs to at each step compute um maybe some ffts um but um keep in mind also that there's a parallel effort to um um accelerate these very common operations and Hardware so if ffts and multi-scalar multiplications become cheap enough then we might actually prefer to progress down the proving stack a bit further before folding if it can preserve attractive properties like zero knowledge um that make it easier for collaborative grouping um sort of the last Innovation I want to mention is something called non-uniform IVC so whereas in IVC um we had the same computation at each recursive step a non-uniform IVC allows us to pick one out of L predefined um circuits so this is useful for example in the context of virtual machines you can imagine these L circuits as the L opcodes of the virtual machine um so the idea here is that you don't need uh the same huge circuit expressing the whole virtual machine logic in every step so instead you can just pick the specific op code at each step um yeah and and save a lot of save save on recursive over overhead you do have to pay in the sense that um you need to maintain L accumulators but if you recall that these accumulators are constant sized and they're very succinct commitments um then that seems like a very good trade-off so now we'll we'll end up with actually a case study um that I think is a great application of proof carrying data so this is the lurk programming language so they've designed it from first principles to compile to approved carrying data backend and how they do this is they they conceive of um their state machine as um a chain um they conceive of their state machine as a chain of recursive steps so basically each operation is one step and they don't attempt to process all n operations let's say in one shot instead they break it up into what they call continuations so given this tree of operations um they process basically each subtree one by one and each time they sort of save the unprocessed part on the continuation stack the continuation tag so right now lurk um actually repeats the same state machine at each step so yeah they're doing uniform IVC currently but as we saw earlier this can be made more efficient with non-uniform IVC where instead of paying for the whole state machine you only need to pay for a cheap binary operation in this case so yeah as we go through this tree of operations you can see how when we end up at a self-evaluating value we can replace um or we can simplify the continuation deck and basically um yeah process this step by step and when we when we get to the outermost note on the continuation deck this is when we terminate the computation and return the results of um return the final proof in the PCD yeah that's all I had thank you [Applause] 